<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 2]
- [cs.IT](#cs.IT) [Total: 7]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies](https://arxiv.org/abs/2601.00510)
*Jetlir Duraj,Ishita Khan,Kilian Merkelbach,Mehran Elyasi*

Main category: cs.IR

TL;DR: 本文提出了一种结合树搜索与LLM语义评分的Chain-of-Thought方法，用于电商搜索查询分类，相比基于嵌入的方法表现更好，并能检测层次分类体系中的问题。


<details>
  <summary>Details</summary>
Motivation: 电商搜索依赖于结构化的库存分类体系，正确分类用户查询不仅能定位正确的库存空间，还能开启多种意图理解能力。然而，在实际电商分类体系中实现准确查询分类具有挑战性，需要实用且准确的解决方案来提升搜索结果的相关性。

Method: 探索了一种新颖的Chain-of-Thought范式，将简单的树搜索与LLM语义评分相结合。同时提出了具有相同思路但能扩展到百万级查询规模的LLM-based方法。

Result: CoT方法在人工评判的查询-类别对、相关性测试和基于LLM的参考方法评估中，表现优于基于嵌入的查询类别预测基准。该方法还能检测层次分类体系中的问题。

Conclusion: Chain-of-Thought方法为电商查询分类提供了有效的解决方案，不仅提升了分类准确性，还能识别分类体系的结构问题，同时提出了可扩展的LLM-based方法以适应大规模应用场景。

Abstract: Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries.

</details>


### [2] [Improving Scientific Document Retrieval with Academic Concept Index](https://arxiv.org/abs/2601.00567)
*Jeyun Lee,Junhyoung Lee,Wonbin Kweon,Bowen Jin,Yu Zhang,Susik Yoon,Dongha Lee,Hwanjo Yu,Jiawei Han,Seongku Kang*

Main category: cs.IR

TL;DR: 本文提出学术概念索引来改进科学领域检索器，通过概念覆盖查询生成和概念聚焦上下文增强，解决现有方法忽略学术概念多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 科学领域检索器适配面临两大挑战：领域相关标注数据稀缺，以及词汇和信息需求不匹配。现有基于LLM的方法（合成查询生成和辅助上下文生成）忽略了科学文档中嵌入的多样学术概念，导致查询冗余和概念覆盖狭窄。

Method: 1. 构建学术概念索引：从论文中提取关键概念，按学术分类体系组织；2. 概念覆盖查询生成（CCQGen）：基于未覆盖概念自适应调节LLM，生成互补查询以扩大概念覆盖；3. 概念聚焦上下文增强（CCExpand）：利用文档片段作为概念感知查询的简洁响应。

Result: 实验表明，将学术概念索引融入查询生成和上下文增强能产生更高质量的查询、更好的概念对齐，并显著提升检索性能。

Conclusion: 学术概念索引能有效改进科学领域检索器的适配，通过结构化概念组织提升查询多样性和上下文相关性，为科学信息检索提供新思路。

Abstract: Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [3] [A repair scheme for a distributed storage system based on multivariate polynomials](https://arxiv.org/abs/2601.00120)
*Hiram H. López,Gretchen L. Matthews,Daniel Valvo*

Main category: cs.IT

TL;DR: 将基于Reed-Solomon码的精确修复方案扩展到基于Reed-Muller码的分布式存储系统，支持单节点和多节点故障修复


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统需要高效的数据恢复机制，现有基于单变量多项式的Reed-Solomon码修复方案需要扩展到基于多变量多项式的Reed-Muller码系统

Method: 扩展GW论文中的精确修复方案，将其应用于基于Reed-Muller码的分布式存储系统，利用多变量多项式的特性设计修复算法

Result: 提出的修复方案能够修复任何单节点故障和满足特定条件的多节点故障，扩展了现有修复方案的应用范围

Conclusion: 成功将精确修复方案从Reed-Solomon码扩展到Reed-Muller码，为基于多变量多项式编码的分布式存储系统提供了有效的故障修复机制

Abstract: A distributed storage system stores data across multiple nodes, with the primary objective of enabling efficient data recovery even in the event of node failures. The main goal of an exact repair scheme is to recover the data from a failed node by accessing and downloading information from the rest of the nodes. In a groundbreaking paper, ~\cite{GW} developed an exact repair scheme for a distributed storage system that is based on Reed-Solomon codes, which depend on single-variable polynomials. In these notes, we extend the repair scheme to the family of distributed storage systems based on Reed-Muller codes, which are linear codes based on multivariate polynomials. The repair scheme we propose repairs any single node failure and multiple node failures, provided the positions satisfy certain conditions.

</details>


### [4] [The permutation group of Reed-Solomon codes over arbitrary points](https://arxiv.org/abs/2601.00122)
*Eduardo Camps-Moreno,Jun Bo Lau,Hiram H. López,Welington Santos*

Main category: cs.IT

TL;DR: 证明了Reed-Solomon码的置换群由保持评估点集不变的一次多项式构成


<details>
  <summary>Details</summary>
Motivation: 理解Reed-Solomon码的置换群结构，为已知特例提供简洁证明

Method: 通过数学证明，建立Reed-Solomon码置换群与保持评估点集不变的一次多项式之间的等价关系

Result: 证明了Reed-Solomon码的置换群恰好由那些保持评估点集不变的一次多项式构成

Conclusion: 该结果为理解Reed-Solomon码的对称性提供了统一框架，并简化了已知特例的证明

Abstract: In this work, we prove that the permutation group of a Reed-Solomon code is given by the polynomials of degree one that leave the set of evaluation points invariant. Our results provide a straightforward proof of the well-known cases of the permutation group of the Reed-Solomon code when the set of evaluation points is the whole finite field or the multiplicative group.

</details>


### [5] [Evolution of UE in Massive MIMO Systems for 6G: From Passive to Active](https://arxiv.org/abs/2601.00251)
*Kwonyeol Park,Hyuckjin Choi,Geonho Han,Gyoseung Lee,Yeonjoon Choi,Sunwoo Park,Junil Choi*

Main category: cs.IT

TL;DR: 该论文回顾了从5G到6G演进中用户设备(UE)在mMIMO系统中的角色转变，从被动接收器变为主动参与者，分析了3GPP标准演进、设备实现挑战和架构创新。


<details>
  <summary>Details</summary>
Motivation: 无线网络对低延迟和高可靠性的严格要求，以及高度动态的信道条件，暴露了传统gNB中心化mMIMO架构的局限性，需要重新思考UE在系统中的角色。

Method: 通过回顾3GPP Release 15-19的标准演进，分析UE功能从基本CSI报告到AI/ML增强CSI和UE发起波束管理的进展，并基于数字孪生评估验证新兴UE中心化功能的影响。

Result: UE发起的波束报告在现实移动场景中提高了吞吐量，多面板UE架构相比单面板UE增强了链路鲁棒性，验证了UE中心化功能的实际价值。

Conclusion: UE正在从被动收发器转变为主动参与系统性能优化的实体，这一转变对于满足6G网络的严格性能要求至关重要，需要继续推进标准化、设备实现和架构创新。

Abstract: As wireless networks continue to evolve, stringent latency and reliability requirements and highly dynamic channels expose fundamental limitations of gNB-centric massive multiple-input multiple-output (mMIMO) architectures, motivating a rethinking of the user equipment (UE) role. In response, the UE is transitioning from a passive transceiver into an active entity that directly contributes to system-level performance. In this context, this article examines the evolving role of the UE in mMIMO systems during the transition from fifth-generation (5G) to sixth-generation (6G), bridging third generation partnership project (3GPP) standardization, device implementation, and architectural innovation. Through a chronological review of 3GPP Releases 15 to 19, we highlight the progression of UE functionalities from basic channel state information (CSI) reporting to artificial intelligence (AI) and machine learning (ML)-based CSI enhancement and UE-initiated beam management. We further examine key implementation challenges, including multi-panel UE (MPUE) architectures, on-device intelligent processing, and energy-efficient operation, and then discuss corresponding architectural innovations under practical constraints. Using digital-twin-based evaluations, we validate the impact of emerging UE-centric functionalities, illustrating that UE-initiated beam reporting improves throughput in realistic mobility scenarios, while a multi-panel architecture enhances link robustness compared with a single-panel UE.

</details>


### [6] [Semantic Transmission Framework in Direct Satellite Communications](https://arxiv.org/abs/2601.00381)
*Chong Huang,Xuyang Chen,Jingfu Li,Pei Xiao,Gaojie Chen,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 提出用于直接卫星通信的语义传输框架，通过REINFORCE++算法优化语义效率指标，解决链路预算不足问题


<details>
  <summary>Details</summary>
Motivation: 当前卫星通信中直接接入的链路预算不足已成为瓶颈问题，需要有效解决方案

Method: 开发直接卫星通信语义传输框架，引入带优化权重的语义效率指标，提出决策辅助的REINFORCE++算法，联合优化传输模式选择、卫星-用户关联、ISL任务迁移、去噪步骤和自适应权重

Result: 数值结果表明，所提算法相比基线方法实现了更高的语义效率

Conclusion: 语义传输框架是解决卫星通信链路预算不足问题的有效可行方案，REINFORCE++算法能有效优化语义效率

Abstract: Insufficient link budget has become a bottleneck problem for direct access in current satellite communications. In this paper, we develop a semantic transmission framework for direct satellite communications as an effective and viable solution to tackle this problem. To measure the tradeoffs between communication, computation, and generation quality, we introduce a semantic efficiency metric with optimized weights. The optimization aims to maximize the average semantic efficiency metric by jointly optimizing transmission mode selection, satellite-user association, ISL task migration, denoising steps, and adaptive weights, which is a complex nonlinear integer programming problem. To maximize the average semantic efficiency metric, we propose a decision-assisted REINFORCE++ algorithm that utilizes feasibility-aware action space and a critic-free stabilized policy update. Numerical results show that the proposed algorithm achieves higher semantic efficiency than baselines.

</details>


### [7] [On the burst-covering radius of binary cyclic codes](https://arxiv.org/abs/2601.00435)
*Gabriel Sac Himelfarb,Moshe Schwartz*

Main category: cs.IT

TL;DR: 该论文研究突发覆盖码，为循环码提供更强的突发覆盖半径界限，并提出高效的突发覆盖循环码算法。


<details>
  <summary>Details</summary>
Motivation: 研究突发覆盖码是为了在通信和存储系统中处理突发错误。突发错误是连续多个位置同时出错的情况，需要专门的编码方案来有效覆盖和纠正这类错误。

Method: 1. 定义突发覆盖码并建立一般参数界限；2. 利用线性反馈移位寄存器序列为循环码提供更强的突发覆盖半径界限；3. 对BCH码证明LFSR序列中模式频率的新界限；4. 基于这些工具分析二进制原始BCH码和Melas码的覆盖半径；5. 提出高效的突发覆盖循环码算法。

Result: 1. 建立了突发覆盖码参数与突发覆盖半径之间的一般连接界限；2. 为循环码获得了更强的突发覆盖半径界限；3. 证明了BCH码中LFSR序列模式频率的新界限；4. 分析了二进制原始BCH码和Melas码的覆盖半径；5. 开发了高效的突发覆盖循环码算法。

Conclusion: 该论文系统研究了突发覆盖码的理论性质，特别针对循环码提供了改进的界限分析工具，并开发了实用算法，对纠错编码理论和实际应用都有重要意义。

Abstract: We define and study burst-covering codes. We provide some general bounds connecting the code parameters with its burst-covering radius. We then provide stronger bounds on the burst-covering radius of cyclic codes, by employing linear-feedback shift-register (LFSR) sequences. For the case of BCH codes we prove a new bound on pattern frequencies in LFSR sequences, which is of independent interest. Using this tool, we can bound the covering-radius of binary primitive BCH codes and Melas codes. We conclude with an efficient algorithm for burst-covering cyclic codes.

</details>


### [8] [CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge](https://arxiv.org/abs/2601.00549)
*Zhiheng Guo,Zhaoyang Liu,Zihan Cen,Chenyuan Feng,Xinghua Sun,Xiang Chen,Tony Q. S. Quek,Xijun Wang*

Main category: cs.IT

TL;DR: CoCo-Fed是一个面向O-RAN的联邦学习框架，通过双重维度梯度降维投影解决本地内存瓶颈，并通过正交子空间叠加协议减少全局通信开销，在无线感知任务中实现高效收敛。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构中大规模神经网络部署面临两个关键瓶颈：1) 资源受限的gNB上本地训练所需的内存占用过大；2) 高维模型更新在带宽受限的回程链路上进行全局聚合时导致带宽饱和。

Method: 提出CoCo-Fed框架：本地采用双重维度梯度降维投影，使优化器能在低秩结构上运行而不增加推理参数/延迟；全局采用基于正交子空间叠加的传输协议，将层间更新投影并叠加为单个矩阵，大幅减少回程流量。

Result: 在到达角估计任务上的大量仿真表明，CoCo-Fed在内存和通信效率方面显著优于现有基线方法，同时在非独立同分布设置下保持稳健收敛。

Conclusion: CoCo-Fed通过统一本地内存效率和全局通信减少，为O-RAN中的边缘智能提供了有效的解决方案，并建立了严格的理论基础，证明了即使在无线感知任务的无监督学习条件下也能收敛。

Abstract: The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings.

</details>


### [9] [Universal Outlier Hypothesis Testing via Mean- and Median-Based Tests](https://arxiv.org/abs/2601.00712)
*Bernhard C. Geiger,Tobias Koch,Josipa Mihaljević,Maximilian Toller*

Main category: cs.IT

TL;DR: 该论文研究了通用离群值假设检验问题，提出在离群序列数量随序列长度增长的情况下，分别使用均值法和中位数法估计典型分布，并证明了两种方法都能达到已知分布的最大似然检验的错误指数。


<details>
  <summary>Details</summary>
Motivation: 研究通用离群值假设检验问题，其中大多数序列服从典型分布π，少数服从离群分布μ。与之前工作不同，本文假设观测序列数量和离群序列数量都随序列长度增长，需要在这种情况下开发有效的检验方法。

Method: 提出两种方法：1）当离群序列数量相对于总序列数量为次线性时，使用均值法估计π；2）当离群序列数量与总序列数量成比例时，使用中位数法估计π。两种方法都旨在达到已知π和μ的最大似然检验的错误指数。

Result: 均值法在离群序列数量次线性时能实现最大似然检验的错误指数；中位数法在离群序列数量成比例时能以概率趋近于1实现最大似然检验的错误指数。为此引入了典型错误指数的概念。

Conclusion: 在不同离群序列数量增长模式下，分别使用均值法和中位数法可以有效估计典型分布，达到与已知分布的最大似然检验相同的错误指数性能，为通用离群值假设检验提供了实用的解决方案。

Abstract: Universal outlier hypothesis testing refers to a hypothesis testing problem where one observes a large number of length-$n$ sequences -- the majority of which are distributed according to the typical distribution $π$ and a small number are distributed according to the outlier distribution $μ$ -- and one wishes to decide, which of these sequences are outliers without having knowledge of $π$ and $μ$. In contrast to previous works, in this paper it is assumed that both the number of observation sequences and the number of outlier sequences grow with the sequence length. In this case, the typical distribution $π$ can be estimated by computing the mean over all observation sequences, provided that the number of outlier sequences is sublinear in the total number of sequences. It is demonstrated that, in this case, one can achieve the error exponent of the maximum likelihood test that has access to both $π$ and $μ$. However, this mean-based test performs poorly when the number of outlier sequences is proportional to the total number of sequences. For this case, a median-based test is proposed that estimates $π$ as the median of all observation sequences. It is demonstrated that the median-based test achieves again the error exponent of the maximum likelihood test that has access to both $π$ and $μ$, but only with probability approaching one. To formalize this case, the typical error exponent -- similar to the typical random coding exponent introduced in the context of random coding for channel coding -- is proposed.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means](https://arxiv.org/abs/2601.00094)
*Ali Dasdan*

Main category: cs.DS

TL;DR: 利用最优环均值（最小和最大环均值）为有向图中最长简单环问题提供严格代数界限和启发式近似，平衡计算效率与精度


<details>
  <summary>Details</summary>
Motivation: 在有向图中寻找最长简单环是NP难问题，在计算生物学、调度和网络分析中有重要应用。现有多项式时间近似算法仅适用于受限图类，一般界限要么松散要么计算成本高。

Method: 利用可在强多项式时间内计算的最优环均值（最小和最大环均值），推导最长简单环权重和长度的严格代数界限及启发式近似。严格分析这些均值统计量与最长环性质之间的代数关系，并给出最短环的对偶结果。

Result: 在ISCAS基准电路上的实验评估显示：严格代数下界通常较松散（中位数低于真实值85-93%），但启发式近似的中位数误差仅为6-14%。同时观察到最大权重环和最大长度环经常重合，表明长环倾向于积累大权重。

Conclusion: 严格界限适用于分支定界算法中的搜索空间剪枝，而启发式近似能提供精确的目标值估计。最大权重环和最大长度环的频繁重合揭示了有向图中环结构的有趣特性。

Abstract: The problem of finding the longest simple cycle in a directed graph is NP-hard, with critical applications in computational biology, scheduling, and network analysis. While polynomial-time approximation algorithms exist for restricted graph classes, general bounds remain loose or computationally expensive. In this paper, we exploit optimum cycle means (minimum and maximum cycle means), which are computable in strongly polynomial time, to derive both strict algebraic bounds and heuristic approximations for the weight and length of the longest simple cycle. We rigorously analyze the algebraic relationships between these mean statistics and the properties of longest cycles, and present dual results for shortest cycles. While the strict bounds provide polynomial-time computable constraints suitable for pruning search spaces in branch-and-bound algorithms, our proposed heuristic approximations offer precise estimates for the objective value. Experimental evaluation on ISCAS benchmark circuits demonstrates this trade-off: while the strict algebraic lower bounds are often loose (median 85--93% below true values), the heuristic approximations achieve median errors of only 6--14%. We also observe that maximum weight and maximum length cycles frequently coincide, suggesting that long cycles tend to accumulate large weights.

</details>


### [11] [Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search](https://arxiv.org/abs/2601.00272)
*Alexandr Andoni,Themistoklis Haris,Esty Kelman,Krzysztof Onak*

Main category: cs.DS

TL;DR: 该论文研究自适应对抗环境下的近似最近邻搜索问题，针对高维和低维两种场景分别提出新算法，结合公平性、差分隐私和鲁棒性技术来对抗控制数据集和查询序列的强对抗者。


<details>
  <summary>Details</summary>
Motivation: 研究在强大自适应对抗者控制下的近似最近邻搜索问题，对抗者既能控制数据集又能控制查询序列。现有方法在这种强对抗环境下存在安全性和性能限制，需要开发能够同时保证隐私、公平性和效率的新算法。

Method: 针对高维场景(d=ω(√Q))：1) 建立自适应安全与公平性的联系，利用公平ANN搜索隐藏内部随机性；2) 使用差分隐私机制在LSH数据结构上实现鲁棒决策原语；3) 提出新颖的同心环LSH构造，结合公平性和差分隐私技术，通过鲁棒释放时序信息来突破√n查询时间障碍。

针对低维场景(d=O(√Q))：提出专门算法，提供"对所有查询"的强保证，引入新的度量覆盖构造来简化和改进汉明空间和ℓ_p空间中的ANN方法。

Result: 为高维场景开发了一系列渐进增强保证的算法序列，成功突破了√n查询时间障碍，同时改进了公平ANN的现有结果。为低维场景提供了在所有可能查询上都具有高概率正确性的强保证算法，在汉明空间和ℓ_p空间中实现了简化和改进的度量覆盖构造。

Conclusion: 该论文通过创新性地结合公平性、差分隐私和鲁棒性技术，成功解决了自适应对抗环境下的近似最近邻搜索问题，针对不同维度场景提供了具有理论保证的高效算法，为对抗性机器学习环境中的相似性搜索问题提供了新的解决方案。

Abstract: We study the Approximate Nearest Neighbor (ANN) problem under a powerful adaptive adversary that controls both the dataset and a sequence of $Q$ queries.
  Primarily, for the high-dimensional regime of $d = ω(\sqrt{Q})$, we introduce a sequence of algorithms with progressively stronger guarantees. We first establish a novel connection between adaptive security and \textit{fairness}, leveraging fair ANN search to hide internal randomness from the adversary with information-theoretic guarantees. To achieve data-independent performance, we then reduce the search problem to a robust decision primitive, solved using a differentially private mechanism on a Locality-Sensitive Hashing (LSH) data structure. This approach, however, faces an inherent $\sqrt{n}$ query time barrier. To break the barrier, we propose a novel concentric-annuli LSH construction that synthesizes these fairness and differential privacy techniques. The analysis introduces a new method for robustly releasing timing information from the underlying algorithm instances and, as a corollary, also improves existing results for fair ANN.
  In addition, for the low-dimensional regime $d = O(\sqrt{Q})$, we propose specialized algorithms that provide a strong ``for-all'' guarantee: correctness on \textit{every} possible query with high probability. We introduce novel metric covering constructions that simplify and improve prior approaches for ANN in Hamming and $\ell_p$ spaces.

</details>


### [12] [Deterministic Coreset for Lp Subspace](https://arxiv.org/abs/2601.00361)
*Rachit Chhaya,Anirban Dasgupta,Dan Feldman,Supratim Shit*

Main category: cs.DS

TL;DR: 提出了第一个迭代算法，用于构建确定性ℓp子空间嵌入的ε-coreset，解决了长期存在的对数因子问题


<details>
  <summary>Details</summary>
Motivation: 现有的ℓp子空间嵌入coreset构造方法通常包含对数因子，这增加了coreset的大小，且缺乏确定性保证。本文旨在设计一个确定性算法，消除这些对数因子，提供最优大小的coreset

Method: 提出了一种迭代算法，在每次迭代中确保维护集上的损失在原数据集损失的上界和下界之间。算法通过加权选择矩阵X的行来构建coreset，保证ℓp子空间嵌入性质

Result: 算法在O(poly(n,d,ε⁻¹))时间内返回大小为O(d^{max{1,p/2}}/ε²)的确定性ε-coreset，消除了对数因子，与下界匹配，是最优的

Conclusion: 该工作解决了ℓp子空间嵌入coreset构造中长期存在的对数因子问题，提供了确定性保证和最优大小的coreset，可用于确定性ℓp回归问题的近似求解

Abstract: We introduce the first iterative algorithm for constructing a $\varepsilon$-coreset that guarantees deterministic $\ell_p$ subspace embedding for any $p \in [1,\infty)$ and any $\varepsilon > 0$. For a given full rank matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $n \gg d$, $\mathbf{X}' \in \mathbb{R}^{m \times d}$ is an $(\varepsilon,\ell_p)$-subspace embedding of $\mathbf{X}$, if for every $\mathbf{q} \in \mathbb{R}^d$, $(1-\varepsilon)\|\mathbf{Xq}\|_{p}^{p} \leq \|\mathbf{X'q}\|_{p}^{p} \leq (1+\varepsilon)\|\mathbf{Xq}\|_{p}^{p}$. Specifically, in this paper, $\mathbf{X}'$ is a weighted subset of rows of $\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\ell_p$ subspace embedding. For an error parameter $\varepsilon$, our algorithm takes $O(\mathrm{poly}(n,d,\varepsilon^{-1}))$ time and returns a deterministic $\varepsilon$-coreset, for $\ell_p$ subspace embedding whose size is $O\left(\frac{d^{\max\{1,p/2\}}}{\varepsilon^{2}}\right)$. Here, we remove the $\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\ell_p$ regression problem in a deterministic manner.

</details>


### [13] [Mind the Gap. Doubling Constant Parametrization of Weighted Problems: TSP, Max-Cut, and More](https://arxiv.org/abs/2601.00768)
*Mihail Stoian*

Main category: cs.DS

TL;DR: 提出一种新方法，将无权重问题的算法应用于有权重问题，通过构造性Freiman定理将权重转换为多项式有界整数，避免伪多项式因子


<details>
  <summary>Details</summary>
Motivation: 当前有权重NP难问题难以获得超多项式改进，而无权重版本已有显著加速。现有方法通过多项式嵌入权重会引入伪多项式因子，不适用于任意权重实例

Method: 使用Randolph和Węgrzycki的构造性Freiman定理将输入权重转换为多项式有界整数，然后应用多项式嵌入，最后使用无权重问题的算法

Result: 当输入权重集合具有小加倍性质时，TSP、Weighted Max-Cut、Edge-Weighted k-Clique等问题的复杂度与其无权重版本成正比

Conclusion: 提出了一种元算法，能够有效将有权重问题转化为无权重问题求解，避免了伪多项式因子，为有权重NP难问题提供了新的求解思路

Abstract: Despite much research, hard weighted problems still resist super-polynomial improvements over their textbook solution. On the other hand, the unweighted versions of these problems have recently witnessed the sought-after speedups. Currently, the only way to repurpose the algorithm of the unweighted version for the weighted version is to employ a polynomial embedding of the input weights. This, however, introduces a pseudo-polynomial factor into the running time, which becomes impractical for arbitrarily weighted instances.
  In this paper, we introduce a new way to repurpose the algorithm of the unweighted problem. Specifically, we show that the time complexity of several well-known NP-hard problems operating over the $(\min, +)$ and $(\max, +)$ semirings, such as TSP, Weighted Max-Cut, and Edge-Weighted $k$-Clique, is proportional to that of their unweighted versions when the set of input weights has small doubling. We achieve this by a meta-algorithm that converts the input weights into polynomially bounded integers using the recent constructive Freiman's theorem by Randolph and Węgrzycki [ESA 2024] before applying the polynomial embedding.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [14] [Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\ell_1$ Relaxations](https://arxiv.org/abs/2601.00329)
*Angshul Majumdar*

Main category: cs.GT

TL;DR: 研究联盟结构生成问题，其中联盟价值不是给定的，而是需要从观察中学习。提出稀疏线性回归模型，分析两种估计方法：贝叶斯贪婪联盟追踪和ℓ₁惩罚估计，并比较它们与传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的联盟结构生成假设联盟价值是已知的，但在实际应用中，这些价值通常需要从观察数据中学习。本文旨在解决当联盟价值未知且需要从稀疏观测中学习时的联盟结构生成问题。

Method: 将每个观察视为稀疏线性回归问题，提出两种估计方案：1) 贝叶斯贪婪联盟追踪，模仿正交匹配追踪的贪婪过程；2) ℓ₁惩罚估计器。在特定条件下分析这两种方法的理论性能。

Result: 贝叶斯贪婪联盟追踪在相干条件和最小信号假设下，当T ≳ K log m时能以高概率恢复盈利联盟的真实集合，从而产生福利最优结构。ℓ₁惩罚估计器在受限特征值条件下获得误差界限和福利差距保证。

Conclusion: 稀疏概率CSG在某些情况下优于传统方法，而密集情况下经典最小二乘方法具有竞争力。研究确定了稀疏概率CSG优越的机制以及传统方法有竞争力的密集机制。

Abstract: We study coalition structure generation (CSG) when coalition values are not given but must be learned from episodic observations. We model each episode as a sparse linear regression problem, where the realised payoff \(Y_t\) is a noisy linear combination of a small number of coalition contributions. This yields a probabilistic CSG framework in which the planner first estimates a sparse value function from \(T\) episodes, then runs a CSG solver on the inferred coalition set. We analyse two estimation schemes. The first, Bayesian Greedy Coalition Pursuit (BGCP), is a greedy procedure that mimics orthogonal matching pursuit. Under a coherence condition and a minimum signal assumption, BGCP recovers the true set of profitable coalitions with high probability once \(T \gtrsim K \log m\), and hence yields welfare-optimal structures. The second scheme uses an \(\ell_1\)-penalised estimator; under a restricted eigenvalue condition, we derive \(\ell_1\) and prediction error bounds and translate them into welfare gap guarantees. We compare both methods to probabilistic baselines and identify regimes where sparse probabilistic CSG is superior, as well as dense regimes where classical least-squares approaches are competitive.

</details>


### [15] [Unifying Proportional Fairness in Centroid and Non-Centroid Clustering](https://arxiv.org/abs/2601.00447)
*Benjamin Cookson,Nisarg Shah,Ziqi Yu*

Main category: cs.GT

TL;DR: 提出半质心聚类新范式，结合质心与非质心损失，针对比例公平性核心准则设计多项式时间常数近似算法


<details>
  <summary>Details</summary>
Motivation: 现有比例公平聚类研究分为质心聚类（损失由到质心距离决定）和非质心聚类（损失由到簇内最远点距离决定）两个独立范式，需要统一框架来结合两者的优势

Method: 提出半质心聚类新范式，将每个数据点的损失定义为质心损失和非质心损失的组合，研究核心准则及其松弛形式FJR，设计多项式时间常数近似算法

Result: 主要成果：即使质心和非质心损失使用不同距离度量，也能在多项式时间内实现核心准则的常数近似；对更受限损失函数和较弱FJR准则获得改进结果；建立各情况下的下界

Conclusion: 成功统一质心和非质心聚类范式，提出的半质心聚类框架在比例公平性方面取得理论突破，为公平聚类提供了新的理论工具和算法保证

Abstract: Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study centroid clustering, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study non-centroid clustering, in which each data point's loss is determined by its maximum distance to any other data point in its cluster.
  We generalize both paradigms to introduce semi-centroid clustering, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria -- the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case.

</details>


### [16] [The CoinAlg Bind: Profitability-Fairness Tradeoffs in Collective Investment Algorithms](https://arxiv.org/abs/2601.00523)
*Andrés Fábrega,James Austgen,Samuel Breckenridge,Jay Yu,Amy Zhao,Sarah Allen,Aditya Saraf,Ari Juels*

Main category: cs.GT

TL;DR: 集体投资算法（CoinAlgs）存在根本性的盈利性与公平性权衡（CoinAlg Bind）：无法同时保证经济公平性而不被套利侵蚀利润。


<details>
  <summary>Details</summary>
Motivation: 随着集体投资算法（CoinAlgs）的普及，这些系统为投资者社区提供共享交易策略，旨在民主化复杂的AI投资工具。但研究发现这些系统存在根本性的盈利性与公平性矛盾。

Method: 1. 建立CoinAlgs的形式化模型，定义隐私性（算法不完全披露）和经济公平性（对抗性内部人员的价值提取）
2. 证明两个互补结果：隐私性是内部攻击经济公平性的前提；缺乏隐私性（透明性）则使套利者侵蚀CoinAlg的盈利能力
3. 使用Uniswap去中心化交易所数据进行实证研究，量化透明CoinAlgs的套利影响，并展示私有CoinAlg的风险

Result: 1. 理论证明：隐私性与经济公平性存在根本冲突（CoinAlg Bind）
2. 实证研究：透明CoinAlgs面临套利侵蚀，私有CoinAlgs即使低带宽隐蔽信道信息泄露也会导致不公平价值提取
3. 量化分析：展示了两种攻击方式的实际影响

Conclusion: 集体投资算法面临根本性的盈利性与公平性权衡，无法同时保证经济公平性而不损失利润给套利者。这为CoinAlg设计提出了基本限制，需要在隐私性和透明度之间做出权衡。

Abstract: Collective Investment Algorithms (CoinAlgs) are increasingly popular systems that deploy shared trading strategies for investor communities. Their goal is to democratize sophisticated -- often AI-based -- investing tools. We identify and demonstrate a fundamental profitability-fairness tradeoff in CoinAlgs that we call the CoinAlg Bind: CoinAlgs cannot ensure economic fairness without losing profit to arbitrage. We present a formal model of CoinAlgs, with definitions of privacy (incomplete algorithm disclosure) and economic fairness (value extraction by an adversarial insider). We prove two complementary results that together demonstrate the CoinAlg Bind. First, privacy in a CoinAlg is a precondition for insider attacks on economic fairness. Conversely, in a game-theoretic model, lack of privacy, i.e., transparency, enables arbitrageurs to erode the profitability of a CoinAlg. Using data from Uniswap, a decentralized exchange, we empirically study both sides of the CoinAlg Bind. We quantify the impact of arbitrage against transparent CoinAlgs. We show the risks posed by a private CoinAlg: Even low-bandwidth covert-channel information leakage enables unfair value extraction.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories](https://arxiv.org/abs/2601.00002)
*Tarek Al Mustafa*

Main category: cs.DB

TL;DR: 该研究提出语义单元（SUs）作为知识图谱中语义显著的有名子图，旨在提升用户认知互操作性，并在生物多样性知识图谱中实现首个语义单元实现，同时探索LLMs在元数据提取中的应用。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）在生态学和生物多样性研究中具有巨大潜力，但SPARQL查询语言对许多用户群体来说难以使用，且用户需求与KGs的机器互操作性、推理任务等技术需求之间存在根本性脱节，导致KGs中的许多陈述对最终用户没有语义意义。

Method: 1）提出语义单元（SUs）作为知识图谱中有语义意义的有名子图；2）从德国生物多样性探索平台（BE）的出版物和数据集元数据构建知识图谱；3）在知识图谱上实现首个语义单元；4）使用大语言模型（LLMs）从标题和摘要中提取结构化元数据类别；5）使用嵌入模型通过潜在信息丰富元数据。

Result: 1）实现了知识图谱上首个语义单元实现；2）研究了语义单元如何影响知识图谱查询；3）展示了LLMs能够从出版物和数据集标题、摘要中提取结构化元数据类别；4）嵌入模型能够用潜在信息丰富元数据，支持创建结构化且符合FAIR原则的元数据。

Conclusion: 语义单元是解决知识图谱用户交互挑战的有效方法，能够增强用户认知互操作性，同时LLMs和嵌入模型在元数据提取和丰富方面显示出潜力，有助于创建更结构化、可查找、可访问、可互操作和可重用的生物多样性研究数据。

Abstract: Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata.

</details>


### [18] [Database Theory in Action: Yannakakis' Algorithm](https://arxiv.org/abs/2601.00098)
*Paraschos Koutris,Stijn Vansummeren,Qichen Wang,Yisu Remy Wang,Xiangyao Yu*

Main category: cs.DB

TL;DR: 本文简要综述了使Yannakakis算法更实用的最新进展，包括效率和实现便利性方面，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Yannakakis算法在理论上是无环连接的最优算法，但由于实际性能不佳而未被广泛采用，因此需要研究如何使其更实用。

Method: 采用文献综述方法，调查分析近年来在优化Yannakakis算法效率和实现便利性方面的各种技术进展。

Result: 总结了使Yannakakis算法更实用的多种技术方法，包括性能优化和实现简化等方面的最新成果。

Conclusion: Yannakakis算法仍有改进空间，本文指出了几个未来研究方向，旨在推动该算法在实际系统中的更广泛应用。

Abstract: Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research.

</details>


### [19] [Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More](https://arxiv.org/abs/2601.00208)
*David Lomet,Rui Wang*

Main category: cs.DB

TL;DR: 提出一种基于notices的无锁技术，通过delta记录更新减少浪费工作，避免线程切换或停滞，提高键值存储性能


<details>
  <summary>Details</summary>
Motivation: 键值存储中线程切换或停滞的高成本是性能主要瓶颈，传统基于锁的方法通过阻塞线程处理资源争用，无锁技术可以避免阻塞但可能产生浪费工作

Method: 提出notices新方法，结合delta记录更新的无锁技术，减少浪费工作，解决B树索引维护问题，避免线程切换或停滞

Result: notices方法能显著减少浪费工作，有效解决B树索引维护问题，避免线程切换或停滞，提高键值存储性能

Conclusion: 基于notices的无锁技术通过delta记录更新能有效减少浪费工作，避免线程切换或停滞，为键值存储提供高性能解决方案，并有其他应用机会

Abstract: A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed.

</details>


### [20] [Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches](https://arxiv.org/abs/2601.00304)
*Mouna Ammar,Marvin Hofer,Erhard Rahm*

Main category: cs.DB

TL;DR: 该论文对结合图数据和时序数据的现有系统进行了全面综述，将其分为四类架构，并分析这些系统如何满足不同需求、展现不同实现特性，以统一支持两种数据类型。


<details>
  <summary>Details</summary>
Motivation: 随着图数据和时序数据应用的日益增长，需要能够同时处理这两种数据类型的系统。然而，现有系统在架构、集成程度和特性上各不相同，缺乏系统的分类和比较分析，使得用户难以选择和评估合适的解决方案。

Method: 作者对现有系统进行了全面的文献综述和系统分析，将系统分为四个架构类别，然后分析每个类别如何满足不同的需求，并比较它们在跨模型集成程度、成熟度、开放性等方面的实现特性。

Result: 提出了一个系统的分类框架，将现有系统分为四类架构，并详细分析了每类系统的特点、优势和局限性。识别了不同系统在跨模型集成程度、成熟度、开放性等方面的权衡，为读者提供了评估现有选项的实用指南。

Conclusion: 该综述为理解和评估结合图与时序数据的系统提供了有价值的框架，帮助读者根据具体需求选择合适的架构，并指出了该领域未来的研究方向和发展趋势。

Abstract: We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness.

</details>


### [21] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: KELP是一种基于进化分组树的高吞吐量日志解析器，通过在线聚类处理动态日志模式，解决了传统静态模板解析器在生产环境中的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线日志解析器基于静态模板模型，无法适应生产环境的动态变化，微小的模式漂移就会导致解析管道静默失效，造成警报丢失和运维负担。

Method: 提出KELP解析器，采用新颖的进化分组树数据结构，将模板发现视为连续在线聚类过程。随着日志到达，树结构动态演化，节点根据变化的频率分布进行分裂、合并和根节点重新评估。

Result: KELP在反映现代生产系统结构模糊性的新基准测试中保持高准确性，而传统启发式方法失败，同时不牺牲吞吐量。

Conclusion: KELP通过进化分组树实现了对动态生产环境的适应性，解决了传统静态解析器的脆弱性问题，为实时日志分析提供了更可靠的解决方案。

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


### [22] [DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data](https://arxiv.org/abs/2601.00695)
*Chuanyi Lv,Huan Li,Dingyu Yang,Zhongle Xie,Lu Chen,Christian S. Jensen*

Main category: cs.DB

TL;DR: DeXOR是一种用于流式浮点数压缩的新框架，通过十进制XOR编码最长公共前缀和后缀，在22个数据集上实现了比现有方案高15%的压缩率和快20%的解压速度。


<details>
  <summary>Details</summary>
Motivation: 随着流式浮点数应用日益广泛，需要高效压缩方案来利用连续数值的相似性（平滑性），同时应对高精度值或缺乏平滑性等极端条件。

Method: 1) 采用十进制XOR程序编码十进制空间的最长公共前缀和后缀；2) 引入缩放截断与容错舍入处理二进制-十进制转换误差；3) 针对十进制XOR优化的比特管理策略；4) 鲁棒的异常处理器管理浮点数指数。

Result: 在22个数据集评估中，DeXOR超越现有最优方案，实现15%更高的压缩率、20%更快的解压速度，同时保持有竞争力的压缩速度，在极端条件下表现出良好的可扩展性和鲁棒性。

Conclusion: DeXOR通过创新的十进制XOR框架和容错机制，在流式浮点数压缩中实现了优越的性能，特别是在极端条件下仍能保持高压缩比和稳定性。

Abstract: With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail.

</details>
