<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.GT](#cs.GT) [Total: 8]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.IT](#cs.IT) [Total: 21]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.IR](#cs.IR) [Total: 19]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment](https://arxiv.org/abs/2511.17676)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Annie Wang,Weizhe Wang*

Main category: cs.DB

TL;DR: 论文探讨了生成式AI和智能体技术如何变革企业数据管理，重点关注SQL生成、RAG和向量数据库等技术在企业数据分析中的应用，以及相关的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和智能体技术的快速发展，传统企业数据管理系统面临根本性变革。企业需要新的AI驱动工具来支持语义查询、降低数据访问门槛，同时确保数据安全和合规性。

Method: 采用基于大语言模型的SQL生成、检索增强生成(RAG)和向量数据库技术，构建支持复杂查询理解、多智能体协作、安全验证和计算效率的创新框架。

Result: 开发了能够连接自然语言与结构化数据的企业数据分析系统，通过代表性用例展示了在分布式部署、数据安全和SQL生成任务挑战方面的解决方案。

Conclusion: AI驱动的数据管理工具正在重塑企业数据分析格局，SQL生成成为连接自然语言与结构化数据的关键桥梁，但需要解决分布式部署、数据安全和SQL生成固有困难等挑战。

Abstract: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.

</details>


### [2] [Efficient Partition-based Approaches for Diversified Top-k Subgraph Matching](https://arxiv.org/abs/2511.19008)
*Liuyi Chen,Yuchen Hu,Zhengyi Yang,Xu Zhou,Wenjie Zhang,Kenli Li*

Main category: cs.DB

TL;DR: 提出了DTkSM问题，通过最大化匹配子图之间的拓扑距离来提升多样性，并开发了PDD框架和优化方法，在真实数据集上实现了显著的速度提升和高质量的多样性结果。


<details>
  <summary>Details</summary>
Motivation: 现有的top-k多样化方法主要关注顶点覆盖度，但结果往往集中在同一区域，缺乏拓扑多样性，无法充分反映图的全局结构。

Method: 提出了DTkSM问题定义，引入PDD框架进行图分区并从远距离区域检索多样化匹配，采用嵌入驱动分区过滤和基于分区邻接图的密集分区选择优化。

Result: 在12个真实数据集上的实验表明，相比基线方法实现了高达4个数量级的速度提升，95%的结果达到80%的最优距离多样性，100%的覆盖多样性。

Conclusion: DTkSM问题能有效捕捉图的全局结构，PDD框架及其优化方法在保证多样性的同时显著提升了计算效率。

Abstract: Subgraph matching is a core task in graph analytics, widely used in domains such as biology, finance, and social networks. Existing top-k diversified methods typically focus on maximizing vertex coverage, but often return results in the same region, limiting topological diversity. We propose the Distance-Diversified Top-k Subgraph Matching (DTkSM) problem, which selects k isomorphic matches with maximal pairwise topological distances to better capture global graph structure. To address its computational challenges, we introduce the Partition-based Distance Diversity (PDD) framework, which partitions the graph and retrieves diverse matches from distant regions. To enhance efficiency, we develop two optimizations: embedding-driven partition filtering and densest-based partition selection over a Partition Adjacency Graph. Experiments on 12 real world datasets show our approach achieves up to four orders of magnitude speedup over baselines, with 95% of results reaching 80% of optimal distance diversity and 100% coverage diversity.

</details>


### [3] [A General Framework for Per-record Differential Privacy](https://arxiv.org/abs/2511.19015)
*Xinghe Chen,Dajun Sun,Quanqing Xu,Wei Dong*

Main category: cs.DB

TL;DR: 提出了一个通用实用的框架，使任何标准差分隐私机制都能支持按记录差分隐私(PrDP)，通过隐私指定域划分和查询增强技术，在保护隐私预算的同时实现高效用。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私假设所有记录具有统一的隐私预算，而实际应用中不同数据值的隐私需求可能不同。按记录差分隐私虽然解决了这个问题，但隐私预算与数据值的依赖关系带来了保护预算本身隐私的挑战。

Method: 提出隐私指定域划分技术来准确估计数据集中的最小隐私需求而不泄露信息，并在本地差分隐私设置中通过隐私指定查询增强技术扩展框架。

Result: 实验结果表明，该机制在基本任务（如计数、求和、最大值估计）中实现了高效用，显著优于现有的个性化差分隐私方法。

Conclusion: 该框架为按记录差分隐私提供了通用解决方案，在保护隐私预算的同时保持了高数据效用，优于现有的个性化差分隐私方法。

Abstract: Differential Privacy (DP) is a widely adopted standard for privacy-preserving data analysis, but it assumes a uniform privacy budget across all records, limiting its applicability when privacy requirements vary with data values. Per-record Differential Privacy (PrDP) addresses this by defining the privacy budget as a function of each record, offering better alignment with real-world needs. However, the dependency between the privacy budget and the data value introduces challenges in protecting the budget's privacy itself. Existing solutions either handle specific privacy functions or adopt relaxed PrDP definitions. A simple workaround is to use the global minimum of the privacy function, but this severely degrades utility, as the minimum is often set extremely low to account for rare records with high privacy needs. In this work, we propose a general and practical framework that enables any standard DP mechanism to support PrDP, with error depending only on the minimal privacy requirement among records actually present in the dataset. Since directly revealing this minimum may leak information, we introduce a core technique called privacy-specified domain partitioning, which ensures accurate estimation without compromising privacy. We also extend our framework to the local DP setting via a novel technique, privacy-specified query augmentation. Using our framework, we present the first PrDP solutions for fundamental tasks such as count, sum, and maximum estimation. Experimental results show that our mechanisms achieve high utility and significantly outperform existing Personalized DP (PDP) methods, which can be viewed as a special case of PrDP with relaxed privacy protection.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [The Impacts of Increasingly Complex Matchup Models on Baseball Win Probability](https://arxiv.org/abs/2511.17733)
*Tristan Mott,Caleb Bradshaw,David Grimsman,Christopher Archibald*

Main category: cs.GT

TL;DR: 本研究开发了四种层次贝叶斯模型来预测棒球比赛中的击球结果，并将其应用于博弈论框架中优化比赛策略。模拟显示更准确的模型可以显著提高胜率，相当于每162场比赛多赢一场。


<details>
  <summary>Details</summary>
Motivation: 棒球比赛中的战略决策（如投手调度、代打和故意保送）需要根据比赛状态调整。管理者需要工具来预测击球结果概率，从而制定最佳获胜策略。

Method: 开发了四种逐步复杂的层次贝叶斯模型，结合投手和击球手信息、惯用手、近期数据以及基于球员偷垒倾向的跑垒概率。在博弈论框架中近似子博弈完美纳什均衡来解决比赛中的决策问题。

Result: 对2024年MLB季后赛的模拟显示，更准确的对抗模型可以在胜率上带来实质性收益——相当于每162场比赛多赢一场。最详细模型生成的胜率预测与实际市场预期一致。

Conclusion: 先进的对抗模型在场上策略和预测方面具有强大的潜力和应用价值，能够显著提升棒球比赛的战略决策效果。

Abstract: Baseball is a game of strategic decisions including bullpen usage, pinch-hitting and intentional walks. Managers must adjust their strategies based on the changing state of the game in order to give their team the best chance of winning. In this thesis, we investigate how matchup models -- tools that predict the probabilities of plate appearance outcomes -- impact in-game strategy and ultimately affect win probability. We develop four progressively complex, hierarchical Bayesian models that predict plate appearance outcomes by combining information from both pitchers and batters, their handedness, and recent data, along with base running probabilities calibrated to a player's base-stealing tendencies.
  Using each model within a game-theoretic framework, we approximate subgame perfect Nash equilibria for in-game decisions, including substitutions and intentional walks. Simulations of the 2024 MLB postseason show that more accurate matchup models can yield tangible gains in win probability -- as much as one additional victory per 162-game season. Furthermore, employing the most detailed model to generate win predictions for actual playoff games demonstrates alignment with market expectations, underscoring both the power and potential of advanced matchup modeling for on-field strategy and prediction.

</details>


### [5] [TimeBoost: Do Ahead-of-Time Auctions Work?](https://arxiv.org/abs/2511.18328)
*Akaki Mamageishvili,Christoph Schlegel,Ko Sunghun,Jinsuk Park,Ali Taslimi*

Main category: cs.GT

TL;DR: TimeBoost拍卖中，快速通道交易的出价与未来提取价值的预测能力较弱，但长期聚合数据相关性更高，表明投标人更擅长检测趋势而非精确识别高套利价值时机。


<details>
  <summary>Details</summary>
Motivation: 评估TimeBoost拍卖中投标出价对未来提取价值的预测准确性，了解投标人如何利用时间优势获取价值。

Method: 比较快速通道交易的累计固定时间标记收益与快速通道出价，分析获胜出价与标记收益之间的相关性，并考察不同时间间隔的聚合效果。

Result: 获胜出价与标记收益的相关性较弱，表明出价是提取价值的噪声预测指标；使用支付出价（第二高出价）相关性略有改善；长期聚合数据相关性显著提高。

Conclusion: 投标人更擅长检测长期趋势而非精确识别高套利价值时机，前一分钟的标记收益被用于预测下一分钟的价值，这可能是相关性存在的原因。

Abstract: We study the performance of the TimeBoost auction, by comparing cumulative fixed time markout of fast lane trades over the TimeBoost interval to bids for the fast lane. Such comparison allows us to assess how well bids predict future extracted value from the time advantage. The correlation between winning bids and markouts is weak across bidders, suggesting that bids are a noisy predictor of extracted value. The correlation slightly improves when comparing paid bids (the second highest bid) instead of winning bids to markouts, which we attribute to the fact that the auction is more of a common value type. In all settings, the relative order of the most frequent bidder performance remains the same, together with their absolute profits. Bids and markouts aggregated over long time intervals exhibit much higher correlation, indicating that bidders detect trends much better than identify when the high arbitrage value is exactly available. One possible explanation for this is the fact that the correlation between previous minute markouts and current minute bids is significant, suggesting that the previous minute markouts is used to predict the next minute value when bidding.

</details>


### [6] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part B: Stochastic Stability in Weakly Acyclic Games](https://arxiv.org/abs/2511.18418)
*Georgios C. Chasparis*

Main category: cs.GT

TL;DR: 本文提出了一种基于期望的扰动学习自动机(APLA)来解决多玩家战略博弈中强化学习收敛到纯纳什均衡的问题，特别是在弱非循环博弈中提供了首个强化学习收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在分布式多玩家战略博弈中收敛到纯纳什均衡存在局限性，特别是在非势博弈和非协调博弈中，且强收敛保证主要限于双玩家博弈。

Method: 扩展了扰动学习自动机(PLA)，引入基于期望的扰动学习自动机(APLA)，通过重复选择和捕获玩家满意度水平的期望因子来强化行动选择概率分布。

Result: 在存在噪声观测的多玩家正效用博弈中进行了随机稳定性分析，为弱非循环博弈提供了首个强化学习收敛到纯纳什均衡和支付优势均衡的保证。

Conclusion: APLA是首个在弱非循环博弈中实现收敛的强化学习方案，通过经典Stag-Hunt博弈的仿真研究验证了结果。

Abstract: Reinforcement-based learning dynamics may exhibit several limitations when applied in a distributed setup. In (repeatedly-played) multi-player/action strategic-form games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. Furthermore, strong convergence guarantees (i.e., almost sure convergence or weak convergence) are mostly restricted to two-player games. To address this main limitation of reinforcement-based learning in repeatedly-played strategic-form games, this paper introduces a novel payoff-based learning scheme for distributed optimization in multi-player/action strategic-form games. We present an extension of perturbed learning automata (PLA), namely aspiration-based perturbed learning automata (APLA), in which each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This paper is the second part of this study that analyzes stochastic stability in multi-player/action weakly-acyclic games in the presence of noisy observations. We provide conditions under which convergence is attained (in weak sense) to the set of pure Nash equilibria and payoff-dominant equilibria. To the best of our knowledge, this is the first reinforcement-based learning scheme that addresses convergence in weakly-acyclic games. Lastly, we provide a specialization of the results to the classical Stag-Hunt game, supported by a simulation study.

</details>


### [7] [Understanding Optimal Portfolios of Strategies for Solving Two-player Zero-sum Games](https://arxiv.org/abs/2511.18658)
*Karolina Drabent,Ondřej Kubíček,Viliam Lisý*

Main category: cs.GT

TL;DR: 本文为基于投资组合的策略近似建立了理论基础，证明了寻找最优投资组合是NP难问题，并展示了常见启发式方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 在大规模博弈中，使用小型投资组合来近似对手策略空间是常见且强大的技术，但现有方法依赖领域知识或启发式方法，缺乏理论保证。

Method: 定义了两玩家零和博弈中的最优投资组合问题，证明其NP难性，提出了分析框架来评估投资组合质量，并评估了多种启发式方法。

Result: 证明最优投资组合问题是NP难的，发现直观启发式方法（如使用纳什均衡支持集或增量构建）可能产生高度次优解，启发式方法的成功严重依赖于具体游戏。

Conclusion: 该问题的困难性凸显了对稳健、经验验证的启发式方法的需求，提出了评估启发式方法的框架，并公开了代码。

Abstract: In large-scale games, approximating the opponent's strategy space with a small portfolio of representative strategies is a common and powerful technique. However, the construction of these portfolios often relies on domain-specific knowledge or heuristics with no theoretical guarantees. This paper establishes a formal foundation for portfolio-based strategy approximation. We define the problem of finding an optimal portfolio in two-player zero-sum games and prove that this optimization problem is NP-hard. We demonstrate that several intuitive heuristics-such as using the support of a Nash Equilibrium or building portfolios incrementally - can lead to highly suboptimal solutions. These negative results underscore the problem's difficulty and motivate the need for robust, empirically-validated heuristics. To this end, we introduce an analytical framework to bound portfolio quality and propose a methodology for evaluating heuristic approaches. Our evaluation of several heuristics shows that their success heavily depends on the specific game being solved. Our code is publicly available.

</details>


### [8] [Bipartiteness in Progressive Second-Price Multi-Auction Networks with Perfect Substitute](https://arxiv.org/abs/2511.19225)
*Jordana Blazek,Frederick C. Harris*

Main category: cs.GT

TL;DR: 该论文为去中心化渐进第二价格拍卖开发了基于投影的影响框架，通过活跃投标索引集的投影定义主要和扩展影响集，展示了投标价格的部分排序如何控制分配、市场转移和饱和单跳壳的出现。


<details>
  <summary>Details</summary>
Motivation: 研究在买卖双方构成的二分网络中，卖方运行本地独立的渐进第二价格拍卖，买方可能参与多个拍卖形成完美替代品的多拍卖市场，旨在理解去中心化环境中拍卖的稳健性。

Method: 使用基于投影的影响框架，通过活跃投标索引集的投影定义主要和扩展影响集，利用投标价格的部分排序分析分配和市场动态，并建模协调异步卖方更新的轮内动态。

Result: 结果显示渐进第二价格拍卖在去中心化环境中具有稳健性，通过饱和组件和结构化框架支持多拍卖动态的相变，确保策略空间的确定性覆盖，实现稳定和真实的嵌入。

Conclusion: 该框架解释了局部互动如何在拍卖间传播，为无需全局信息或集中控制的连贯均衡提供了前提，证明了渐进第二价格拍卖在去中心化多拍卖市场中的有效性。

Abstract: We consider a bipartite network of buyers and sellers, where the sellers run locally independent Progressive Second-Price (PSP) auctions, and buyers may participate in multiple auctions, forming a multi-auction market with perfect substitute. The paper develops a projection-based influence framework for decentralized PSP auctions. We formalize primary and expanded influence sets using projections on the active bid index set and show how partial orders on bid prices govern allocation, market shifts, and the emergence of saturated one-hop shells. Our results highlight the robustness of PSP auctions in decentralized environments by introducing saturated components and a structured framework for phase transitions in multi-auction dynamics. This structure ensures deterministic coverage of the strategy space, enabling stable and truthful embedding in the larger game. We further model intra-round dynamics using an index to capture coordinated asynchronous seller updates coupled through buyers' joint constraints. Together, these constructions explain how local interactions propagate across auctions and gives premise for coherent equilibria--without requiring global information or centralized control.

</details>


### [9] [On Altruism and Spite in Bimatrix Games](https://arxiv.org/abs/2511.19307)
*Michail Fasoulakis,Leonidas Bakopoulos,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.GT

TL;DR: 本文研究双矩阵博弈中利他主义和恶意行为的算法影响，包括纳什均衡的复杂性和质量，并探讨学习对手行为程度的方法。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设玩家只优化自身收益，但现实中玩家可能表现出利他或恶意行为。现有经济学研究多关注解释这些行为，而较少关注其在博弈中的算法影响。

Method: 放松"自利"假设，理论分析和实验研究双矩阵博弈在利他/恶意行为下的算法特性，包括学习对手行为程度的方法。

Result: 提供了利他/恶意行为对双矩阵博弈纳什均衡复杂性和质量影响的理论和实验结果。

Conclusion: 证明了学习对手利他/恶意行为程度的潜力，可用于对手选择和知识迁移，拓展了博弈论的算法研究范围。

Abstract: One common assumption in game theory is that any player optimizes a utility function that takes into account only its own payoff. However, it has long been observed that in real life players may adopt an altruistic or even spiteful behaviour. As such, there are numerous attempts in the economics literature that strive to explain the fact that players are not entirely selfish, but most of these works do not focus on the algorithmic implications of altruism or spite in games. In this paper, we relax the aforementioned ``self-interest'' assumption, and initiate the study of algorithmic aspects of bimatrix games -- such as the complexity and the quality of their (approximate) Nash equilibria -- under altruism or spite. We provide both a theoretical and an experimental treatment of these topics. Moreover, we demonstrate the potential for learning the degree of an opponent's altruistic/spiteful behaviour, and employing this for opponent selection and transfer of knowledge in bimatrix games.

</details>


### [10] [Disc Game Dynamics: A Latent Space Perspective on Selection and Learning in Games](https://arxiv.org/abs/2511.19346)
*Pablo Lechon-Alonso,Andrew Dennehy,Ruizheng Bai,Nicolas Sanchez,Derek K. Wise,David Sewell,David Rosenbluth,Alexander Strang*

Main category: cs.GT

TL;DR: 本文通过公理化方法推导出对称零和博弈的潜在空间表示，将复杂博弈表示为简单圆盘游戏的线性组合，简化了学习动态分析。


<details>
  <summary>Details</summary>
Motivation: 传统进化博弈论中，玩家属性与结果之间的函数关系往往很复杂，阻碍了数学进展。本文旨在寻找一种坐标空间表示，使代理对对手的最优训练方向仅取决于对手的坐标。

Method: 通过公理化推导出圆盘游戏嵌入方法，将原始博弈表示为新坐标空间中简单圆盘游戏的线性组合。该方法将经典进化过程简化为潜在空间中的约束振荡方程。

Result: 连续复制方程简化为哈密顿耦合振荡系统，表现出庞加莱回归。该方法在有限秩博弈中实现精确闭包，在其他情况下提供最优近似，并建立了连续复制方程与自适应动态之间的精确等价关系。

Conclusion: 圆盘游戏嵌入为对称双人零和博弈中的学习和选择提供了一个组织范式，提供了数值方法，可将模拟成本与定义代理的属性数量解耦。

Abstract: Evolutionary game theory studies populations that change in response to an underlying game. Often, the functional form relating outcome to player attributes or strategy is complex, preventing mathematical progress. In this work, we axiomatically derive a latent space representation for pairwise, symmetric, zero-sum games by seeking a coordinate space in which the optimal training direction for an agent responding to an opponent depends only on their opponent's coordinates. The associated embedding represents the original game as a linear combination of copies of a simple game, the disc game, in a new coordinate space. In this article, we show that disc-game embedding is useful for studying learning dynamics. We demonstrate that a series of classical evolutionary processes simplify to constrained oscillator equations in the latent space. In particular, the continuous replicator equation reduces to a Hamiltonian system of coupled oscillators that exhibit Poincaré recurrence. This reduction allows exact, finite-dimensional closure when the underlying game is finite-rank, and optimal approximation otherwise. It also establishes an exact equivalence between the continuous replicator equation and adaptive dynamics in the transformed coordinates. By identifying a minimal rank representation, the disc game embedding offers numerical methods that could decouple the cost of simulation from the number of attributes used to define agents. These results generalize to metapopulation models that mix inhomogeneously, and to any time-differentiable dynamic where the rate of growth of a type, relative to its expected payout, is a nonnegative function of its frequency. We recommend disc-game embedding as an organizing paradigm for learning and selection in response to symmetric two-player zero-sum games.

</details>


### [11] [Black-Box Lifting and Robustness Theorems for Multi-Agent Contracts](https://arxiv.org/abs/2511.19358)
*Paul Dütting,Tomer Ezra,Michal Feldman,Thomas Kesselheim*

Main category: cs.GT

TL;DR: 本文分析了多智能体契约设计中超越纯纳什均衡的更广泛均衡概念，包括混合纳什均衡和（粗）相关均衡。主要结果表明，对于子模和XOS奖励函数，复杂推荐策略最多只能带来常数倍的增益。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体契约设计主要关注纯纳什均衡，但现实中智能体可能采用更复杂的策略，包括相关策略和学习动态。本文旨在扩展分析范围，研究更一般均衡概念下的契约设计问题。

Method: 通过理论分析，比较不同均衡概念（纯纳什均衡、混合纳什均衡、相关均衡）下的契约效用，特别关注子模和XOS奖励函数的情况。

Result: 对于子模和XOS奖励函数，复杂推荐策略最多带来常数倍增益；提供了黑盒提升方法，可将纯纳什均衡的结果扩展到相关均衡；对于子模奖励，设计了多项式时间算法实现常数近似。

Conclusion: 本文扩展了多智能体契约设计的分析框架，证明了在子模和XOS奖励下，纯纳什均衡与更复杂均衡概念之间的差距是有限的，为契约设计提供了更强的理论基础和实用算法。

Abstract: Multi-agent contract design has largely evaluated contracts through the lens of pure Nash equilibria (PNE). This focus, however, is not without loss: In general, the principal can strictly gain by recommending a complex, possibly correlated, distribution over actions, while preserving incentive compatibility. In this work, we extend the analysis of multi-agent contracts beyond pure Nash equilibria to encompass more general equilibrium notions, including mixed Nash equilibria as well as (coarse-)correlated equilibria (CCE). The latter, in particular, captures the limiting outcome of agents engaged in learning dynamics.
  Our main result shows that for submodular and, more generally, XOS rewards, such complex recommendations yield at most a constant-factor gain: there exists a contract and a PNE whose utility is within a constant factor of the best CCE achievable by any contract. This provides a black-box lifting: results established against the best PNE automatically apply with respect to the best CCE, with only a constant factor loss. For submodular rewards, we further show how to transform a contract and a PNE of that contract into a new contract such that any of its CCEs gives a constant approximation to the PNE. This yields black-box robustness: up to constant factors, guarantees established for a specific contract and PNE automatically extend to the modified contract and any of its CCEs. We thus expand prior guarantees for multi-agent contracts and lower the barrier to new ones. As an important corollary, we obtain poly-time algorithms for submodular rewards that achieve constant approximations in any CCE, against the best CCE under the best contract. Such worst-case guarantees are provably unattainable for XOS rewards. Finally, we bound the gap between different equilibrium notions for subadditive, supermodular, and general rewards.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [12] [Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation](https://arxiv.org/abs/2511.18415)
*Wei Yang,Yiran Zhu,Zilin Li,Xunjia Zhang,Hongtao Wang*

Main category: cs.MM

TL;DR: 提出SEKD方法，通过自蒸馏让视觉语言模型学习多步推理能力，无需人工标注或外部工具，显著提升层次理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在层次理解任务中表现不佳，主要问题在于无法保持跨层级状态一致性，而非缺乏分类学知识。

Method: SEKD方法：让同一个VLM进行逐步推理作为教师模型，输出硬标签、软分布和解码器隐藏状态，然后让单次推理的学生模型蒸馏这些信号。

Result: 学生模型接近多步教师模型的准确率，域内路径一致性提升29.50个百分点，零样本性能从4.15%提升到42.26%，在数学基准上也有提升。

Conclusion: SEKD提供了一种无需标注成本的实用方法，可为紧凑的VLM注入依赖感知的多步推理能力，具有良好的可扩展性。

Abstract: Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks, where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels. We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher. It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route to imbue compact VLMs with dependency-aware multi-step reasoning.

</details>


### [13] [When Top-ranked Recommendations Fail: Modeling Multi-Granular Negative Feedback for Explainable and Robust Video Recommendation](https://arxiv.org/abs/2511.18700)
*Siran Chen,Boyu Chen,Chenyun Yu,Yi Ouyang,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.MM

TL;DR: 提出了Agentic ENF框架，通过三个智能体分析用户行为和视频内容，预测负面反馈并生成解释，显著提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统主要依赖ID嵌入和协同过滤，难以捕捉深度视频语义，且无法有效处理用户偏见行为（如误点击、快速跳过），导致兴趣建模不准确和负面反馈频发。

Method: 构建了ENF框架，包含三个核心组件：Profile Agent（从用户历史数据提取心理和个性特征）、Video Agent（多模态视频分析）、Reason Agent（综合信息预测用户参与度并生成解释）。使用S-GRPO算法进行强化微调。

Result: 在收集的数据集上，该方法在负面反馈预测和原因解释方面显著优于现有基线，在原因分类上比GPT-4o提升8.6%。业务平台部署显示：用户平均观看时间增加6.2%，快速跳过率降低9.4%，用户满意度显著提升。

Conclusion: Agentic ENF框架通过多智能体协同分析和强化学习，有效解决了视频推荐中的负面反馈问题，提升了推荐准确性和用户满意度。

Abstract: Existing video recommendation systems, relying mainly on ID-based embedding mapping and collaborative filtering, often fail to capture in-depth video content semantics. Moreover, most struggle to address biased user behaviors (e.g., accidental clicks, fast skips), leading to inaccurate interest modeling and frequent negative feedback in top recommendations with unclear causes. To tackle this issue, we collect real-world user video-watching sequences, annotate the reasons for users' dislikes, and construct a benchmark dataset for personalized explanations. We then introduce the Agentic Explainable Negative Feedback (ENF) framework, which integrates three core components: (1) the Profile Agent, extracting behavioral cues from users' historical data to derive psychological and personality profiles; (2) the Video Agent, performing comprehensive multimodal video analysis; and (3) the Reason Agent, synthesizing information from the other two agents to predict user engagement and generate explanations. Additionally, we propose the S-GRPO algorithm, enabling the model to progressively address complex tasks during reinforcement fine-tuning. Experimental results on the collected dataset show that our method significantly outperforms state-of-the-art baselines in negative feedback prediction and reason explanation. Notably, it achieves an 8.6% improvement over GPT-4o in reason classification. Deployment on the business platform further validates its benefits: increasing average user watch time by 6.2%, reducing the fast-skip rate by 9.4%, and significantly enhancing user satisfaction.

</details>


### [14] [Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach](https://arxiv.org/abs/2511.19080)
*Fan Nie,Jiangqun Ni,Jian Zhang,Bin Zhang,Weizhe Zhang,Bin Li*

Main category: cs.MM

TL;DR: 提出了一种基于变分贝叶斯的多模态深度伪造检测框架FoVB，通过变分贝叶斯估计音频-视觉相关性，并分解为模态特定和相关性特定的变量，有效检测跨模态不一致性。


<details>
  <summary>Details</summary>
Motivation: AIGC内容的广泛应用带来了安全风险，如音视频深度伪造。需要开发有效且泛化性强的多模态深度伪造检测方法，利用音频-视觉相关性学习暴露跨模态不一致性。

Method: 采用变分贝叶斯估计音频-视觉相关性作为高斯分布潜变量。使用差异卷积和高通滤波器提取局部和全局伪造痕迹，通过变分贝叶斯估计相关性潜变量，并分解为模态特定和相关性特定变量。

Result: 在多个基准测试中，FoVB优于其他最先进的方法。

Conclusion: FoVB框架通过变分贝叶斯方法有效学习音频-视觉相关性，在多模态深度伪造检测中表现出色。

Abstract: The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [15] [How to Expand a Self-orthogonal Code](https://arxiv.org/abs/2511.17503)
*Jon-Lark Kim,Hongwei Liu,Jinquan Luo*

Main category: cs.IT

TL;DR: 本文展示了如何扩展欧几里得/埃尔米特自正交码并保持其正交性质，证明了每个k维埃尔米特自正交码都包含在(k+1)维埃尔米特自正交码中，并给出了欧几里得自正交码的扩展条件。


<details>
  <summary>Details</summary>
Motivation: 研究自正交码的扩展问题，旨在找到保持正交性质的码扩展方法，这对于编码理论和实际应用具有重要意义。

Method: 提出了两种可行的算法来实现自正交码的扩展过程，针对欧几里得和埃尔米特自正交码分别给出了扩展条件和证明。

Result: 证明了每个k维埃尔米特自正交码都包含在(k+1)维埃尔米特自正交码中；对于k<n/2-1，每个[n,k]欧几里得自正交码都包含在[n,k+1]欧几里得自正交码中；对于k=n/2-1的情况，在p=2时总能完成扩展，在p为奇素数时需要满足额外条件。

Conclusion: 成功建立了自正交码的扩展理论框架，提供了具体的扩展算法，并明确了不同参数条件下的扩展可行性，为自正交码的构造和应用提供了理论基础。

Abstract: In this paper, we show how to expand Euclidean/Hermitian self-orthogonal code preserving their orthogonal property. Our results show that every $k$-dimension Hermitian self-orthogonal code is contained in a $(k+1)$-dimensional Hermitian self-orthogonal code. Also, for $k< n/2-1$, every $[n,k]$ Euclidean self-orthogonal code is contained in an $[n,k+1]$ Euclidean self-orthogonal code. Moreover, for $k=n/2-1$ and $p=2$, we can also fulfill the expanding process. But for $k=n/2-1$ and $p$ odd prime, the expanding process can be fulfilled if and only if an extra condition must be satisfied. We also propose two feasible algorithms on these expanding procedures.

</details>


### [16] [Covert Communication and Key Generation Over Quantum State-Dependent Channels](https://arxiv.org/abs/2511.17504)
*Hassan ZivariFard,Rémi A. Chou,Xiaodong Wang*

Main category: cs.IT

TL;DR: 本文研究了在量子态相关信道上实现正速率的隐蔽通信和隐蔽密钥生成，提出了两种安全度量下的可实现方案，并在特定情况下证明了结果的最优性。


<details>
  <summary>Details</summary>
Motivation: 现有隐蔽通信研究主要关注经典信道，而量子信道上的隐蔽通信和隐蔽密钥生成尚未得到充分研究。本文旨在探索在量子态相关信道上同时实现隐蔽通信和隐蔽密钥生成的可能性。

Method: 使用完全量子态相关信道模型，发射机与信道共享纠缠态。研究两种安全度量：一是隐蔽通信同时生成隐蔽密钥，二是隐蔽传输安全消息同时生成隐蔽密钥。提出了一次性和渐近可达的正隐蔽-密钥速率对。

Result: 获得了两种安全度量下的一次性和渐近可达正隐蔽-密钥速率对。在经典信道且状态在发射机和接收机处非因果可用的情况下，证明了结果的最优性。

Conclusion: 这是首次在量子信道上实现正速率的隐蔽通信，也是首次实现正速率的隐蔽密钥生成。结果在特定情况下是最优的，并为量子隐蔽通信领域提供了新的理论基础。

Abstract: We study covert communication and covert secret key generation with positive rates over quantum state-dependent channels. Specifically, we consider fully quantum state-dependent channels when the transmitter shares an entangled state with the channel. We study this problem setting under two security metrics. For the first security metric, the transmitter aims to communicate covertly with the receiver while simultaneously generating a covert secret key, and for the second security metric, the transmitter aims to transmit a secure message covertly and generate a covert secret key with the receiver simultaneously. Our main results include one-shot and asymptotic achievable positive covert-secret key rate pairs for both security metrics. Our results recover as a special case the best-known results for covert communication over state-dependent classical channels. To the best of our knowledge, our results are the first instance of achieving a positive rate for covert secret key generation and the first instance of achieving a positive covert rate over a quantum channel. Additionally, we show that our results are optimal when the channel is classical and the state is available non-causally at both the transmitter and the receiver.

</details>


### [17] [Unified Error Analysis for Synchronous and Asynchronous Two-User Random Access](https://arxiv.org/abs/2511.17718)
*Nazanin Mirhosseini,Jie Luo*

Main category: cs.IT

TL;DR: 该论文研究了一个双用户随机接入系统，其中每个用户独立选择编码方案，接收机旨在解码用户1的消息，但也可在有益时解码用户2的消息。论文提出了同步和异步场景下的解码策略，并推导了广义错误性能的可实现上界。


<details>
  <summary>Details</summary>
Motivation: 研究双用户随机接入系统中，在没有用户间协调的情况下，如何设计解码策略来可靠地解码目标用户消息，同时处理用户间的潜在干扰。

Method: 在同步设置中使用两个并行子解码器：一个专门解码用户1消息，另一个联合解码两个用户消息。在异步设置中，使用2^{2L}个并行子解码器，每个负责解码消息-代码索引对的子集。所有子解码器将编码空间划分为操作区、边界区和碰撞区。

Result: 推导了同步和异步场景下广义错误性能（错误解码、碰撞和漏检概率的加权和）的可实现上界。

Conclusion: 提出的解码策略能够在双用户随机接入系统中有效处理用户间干扰，为广义错误性能提供了理论保证。

Abstract: We consider a two-user random access system in which each user independently selects a coding scheme from a finite set for every message, without sharing these choices with the other user or with the receiver. The receiver aims to decode only user 1 message but may also decode user 2 message when beneficial. In the synchronous setting, the receiver employs two parallel sub-decoders: one dedicated to decoding user 1 message and another that jointly decodes both users messages. Their outputs are synthesized to produce the final decoding or collision decision. For the asynchronous setting, we examine a time interval containing $L$ consecutive codewords from each user. The receiver deploys $2^{2L}$ parallel sub-decoders, each responsible for decoding a subset of the message-code index pairs. In both synchronous and asynchronous cases, every sub-decoder partitions the coding space into three disjoint regions: operation, margin, and collision, and outputs either decoded messages or a collision report according to the region in which the estimated code index vector lies. Error events are defined for each sub-decoder and for the overall receiver whenever the expected output is not produced. We derive achievable upper bounds on the generalized error performance, defined as a weighted sum of incorrect-decoding, collision, and miss-detection probabilities, for both synchronous and asynchronous scenarios.

</details>


### [18] [Multi-Port Selection for FAMA: Massive Connectivity with Fewer RF Chains than Users](https://arxiv.org/abs/2511.17897)
*Hanjiang Hong,Kai-Kit Wong,Xusheng Zhu,Hao Xu,Han Xiao,Farshad Rostami Ghadi,Hyundong Shin*

Main category: cs.IT

TL;DR: 本文提出了三种端口选择方法（EPS、IPS、DPS）来提升多活动端口慢速流体天线多址接入的性能，其中IPS在保持可管理计算复杂度的同时实现了接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 从单活动端口过渡到多活动端口可以显著增强慢速FAMA的复用能力，但这一机制尚未得到充分理解。

Method: 提出了三种端口选择方法：最优穷举搜索端口选择（EPS）作为性能上界，以及两种次优低复杂度算法——增量端口选择（IPS）和减量端口选择（DPS）。

Result: 仿真结果表明，所提方法优于当前最先进的多端口FAMA技术，特别是IPS在保持可管理计算复杂度的同时实现了接近最优的性能。

Conclusion: 这项研究为FAMA系统中的端口选择提供了一个更通用的框架。

Abstract: Fluid antenna multiple access (FAMA) is an emerging technology in massive access designed to meet the demands of future wireless communication networks by naturally mitigating multiuser interference through the utilization of the fluid antenna system (FAS) at RF-chain-limited mobile device. The transition from single-active-port to multi-active-port on a shared RF chain for slow FAMA can greatly enhance its multiplexing capability but is not well understood. Motivated by this, this paper proposes and studies three port selection methods: the optimal exhaustive-search port selection (EPS) as a performance upper bound, and two suboptimal, low-complexity algorithms, namely incremental port selection (IPS) and decremental port selection (DPS). Then the performance of multi-active-port slow FAMA is analyzed, and the complexity of the proposed methods is compared. Simulation results indicate that the proposed methods outperform current state-of-the-art multi-port FAMA techniques. In particular, IPS achieves near-optimal performance while maintaining manageable computational complexity. This research provides a more general framework for port selection in FAMA systems.

</details>


### [19] [Asymptotic Performance Analysis of Fluid Antenna Systems: An Extreme Value Theory Perspective](https://arxiv.org/abs/2511.17916)
*Yi Zhang,Jintao Wang,Zheng Shi,Xu Wang,Guanghua Yang,Shaodan Ma,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本文使用极值理论分析流体天线系统的性能缩放规律，发现中断概率随天线端口数近似指数衰减，遍历容量呈双对数缩放规律，并证明空间相关性会降低系统性能。


<details>
  <summary>Details</summary>
Motivation: 为了量化流体天线系统在大规模天线端口下的性能缩放规律，需要分析其渐近性能表现。

Method: 利用极值理论对流体天线系统的中断概率和遍历容量进行渐近分析，建立上下界，并通过Gumbel分布模式的对数缩放特性重新验证缩放规律。

Result: 中断概率随天线端口数近似指数衰减，遍历容量呈双对数缩放规律，空间相关性会同时降低中断概率和遍历容量性能。

Conclusion: 流体天线系统在大规模配置下展现出优异的性能缩放特性，但空间相关性会对其性能产生负面影响，所有理论分析结果均通过数值模拟得到验证。

Abstract: Fluid antenna systems (FAS) allow dynamic reconfiguration to achieve superior diversity gains and reliability. To quantify the performance scaling of FAS with a large number of antenna ports, this paper leverages extreme value theory (EVT) to conduct an asymptotic analysis of the outage probability (OP) and ergodic capacity (EC). The analysis reveals that the OP decays approximately exponentially with the number of antenna ports. Moreover, we establish upper and lower bounds for the asymptotic EC, uncovering its double-logarithmic scaling law. Furthermore, we re-substantiate these scaling laws by exploiting the fact that the mode of the Gumbel distribution scales logarithmically. Besides, we theoretically prove that spatial correlation among antenna ports degrades both OP and EC. All analytical findings are conclusively validated by numerical results.

</details>


### [20] [A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference](https://arxiv.org/abs/2511.17931)
*Jaswanth Bodempudi,Batta Siva Sairam,Madepalli Haritha,Sandesh Rao Mattu,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 提出了一种基于强化学习的上行载波聚合资源分配方案，使用复合动作演员-评论家算法来处理离散和连续变量的混合优化问题，有效解决非线性自干扰约束。


<details>
  <summary>Details</summary>
Motivation: 在移动网络中，上行载波聚合需要为功率受限用户分配多个载波和功率，但传统方法难以处理混合变量优化和自干扰约束。当用户上行载波的谐波落在下行频率时，会导致接收机灵敏度下降。

Method: 采用强化学习框架，使用复合动作演员-评论家算法，设计新颖的奖励函数来有效处理自干扰约束，在线学习和分配合适的载波。

Result: 数值结果表明，与简单方案相比，所提出的RL方案能够实现更高的总吞吐量，且奖励函数使CA2C算法在有无自干扰情况下都能自适应优化。

Conclusion: 基于强化学习的载波聚合资源分配方案能有效处理自干扰约束，在动态环境中实现高性能的载波选择和功率分配。

Abstract: Carrier aggregation (CA) is a technique that allows mobile networks to combine multiple carriers to increase user data rate. On the uplink, for power constrained users, this translates to the need for an efficient resource allocation scheme, where each user distributes its available power among its assigned uplink carriers. Choosing a good set of carriers and allocating appropriate power on the carriers is important. If the carrier allocation on the uplink is such that a harmonic of a user's uplink carrier falls on the downlink frequency of that user, it leads to a self coupling-induced sensitivity degradation of that user's downlink receiver. In this paper, we model the uplink carrier aggregation problem as an optimal resource allocation problem with the associated constraints of non-linearities induced self interference (SI). This involves optimization over a discrete variable (which carriers need to be turned on) and a continuous variable (what power needs to be allocated on the selected carriers) in dynamic environments, a problem which is hard to solve using traditional methods owing to the mixed nature of the optimization variables and the additional need to consider the SI constraint. We adopt a reinforcement learning (RL) framework involving a compound-action actor-critic (CA2C) algorithm for the uplink carrier aggregation problem. We propose a novel reward function that is critical for enabling the proposed CA2C algorithm to efficiently handle SI. The CA2C algorithm along with the proposed reward function learns to assign and activate suitable carriers in an online fashion. Numerical results demonstrate that the proposed RL based scheme is able to achieve higher sum throughputs compared to naive schemes. The results also demonstrate that the proposed reward function allows the CA2C algorithm to adapt the optimization both in the presence and absence of SI.

</details>


### [21] [Block Length Gain for Nanopore Channels](https://arxiv.org/abs/2511.18027)
*Yu-Ting Lin,Hsin-Po Wang,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: 本文扩展了Geno-Weaving方法，将其从仅处理替换错误扩展到处理删除错误，并展示了该方法在DNA数据存储中的两个优势：消除有限长度惩罚和在现实删除率下表现良好。


<details>
  <summary>Details</summary>
Motivation: DNA作为数据存储介质具有极高的数据密度和长久耐久性，但当前技术只能合成200-300核苷酸长的链，导致内码速率受到显著有限长度惩罚。需要一种方法来克服这一限制并处理DNA存储中的删除错误。

Method: 扩展Geno-Weaving方法，使用单一编码保护多个链中的相同位置，这种方法理论上能够达到替换错误下的容量，现在扩展到处理删除错误。

Result: Geno-Weaving方法在删除错误下表现良好，由于链数量比链长度大3-4个数量级，有限长度惩罚消失。在0.1%-10%的现实删除率下，为BSC设计的Geno-Weaving方法经验上表现良好。

Conclusion: Geno-Weaving方法不仅能够有效处理DNA数据存储中的删除错误，还避免了为删除信道定制设计的需要，在现实条件下具有实用价值。

Abstract: DNA is an attractive candidate for data storage. Its millennial durability and nanometer scale offer exceptional data density and longevity. Its relevance to medical applications also drives advances in DNA-related biotechnology.
  To protect our data against errors, a straightforward approach uses one error-correcting code per DNA strand, with a Reed--Solomon code protecting the collection of strands. A downside is that current technology can only synthesize strands 200--300 nucleotides long. At this block length, the inner code rate suffers a significant finite-length penalty, making its effective capacity hard to characterize.
  Last year, we proposed $\textit{Geno-Weaving}$ in a JSAIT publication. The idea is to protect the same position across multiple strands using one code; this provably achieves capacity against substitution errors. In this paper, we extend the idea to combat deletion errors and show two more advantages of Geno-Weaving: (1) Because the number of strands is 3--4 orders of magnitude larger than the strand length, the finite-length penalty vanishes. (2) At realistic deletion rates $0.1\%$--$10\%$, Geno-Weaving designed for BSCs works well empirically, bypassing the need to tailor the design for deletion channels.

</details>


### [22] [Average Secrecy Capacity Maximization of Rotatable Antenna-Assisted Secure Communications](https://arxiv.org/abs/2511.18097)
*Pengchuan Jiang,Quanzhong Li,Lifeng Mai,Qi Zhang*

Main category: cs.IT

TL;DR: 本文研究了可旋转天线辅助安全通信系统的平均保密率最大化问题，证明了目标函数关于天线调整因子是拟凹的，提出了二分搜索最优解和闭式近优解，并在高信噪比下分析了系统保密中断概率。


<details>
  <summary>Details</summary>
Motivation: 可旋转天线能够动态调整偏转角，有望在无线通信中实现更好的物理层安全性能。考虑到实际场景中天线调整非实时的特点，需要研究平均保密率最大化问题。

Method: 理论证明平均保密率最大化问题的目标函数关于天线调整因子是拟凹的，使用二分搜索寻找最优解，并推导了仅考虑视距信道分量时的闭式最优偏转角作为近优解。

Result: 仿真结果表明，近优解实现了与最优解几乎相同的平均保密容量，在高信噪比下理论保密中断概率与仿真结果匹配。

Conclusion: 提出的闭式近优解在性能上接近最优解，且在高信噪比下理论分析准确，为可旋转天线安全通信系统的设计提供了有效解决方案。

Abstract: A rotatable antenna, which is able to dynamically adjust its deflection angle, is promising to achieve better physical layer security performance for wireless communications. In this paper, considering practical scenarios with non-real-time rotatable antenna adjustment, we investigate the average secrecy rate maximization problem of a rotatable antenna-assisted secure communication system. We theoretically prove that the objective function of the average secrecy rate maximization problem is quasi-concave with respect to an adjustment factor of the rotatable antenna. Under this condition, the optimal solution can be found by the bisection search. Furthermore, we derive the closed-form optimal deflection angle for the secrecy capacity maximization problem, considering the existence of only line-of-sight components of wireless channels. This solution serves as a near optimal solution to the average secrecy rate maximization problem. Based on the closed-form near optimal solution, we obtain the system secrecy outage probability at high signal-to-noise ratio (SNR). It is shown through simulation results that the near optimal solution achieves almost the same average secrecy capacity as the optimal solution. It is also found that at high SNR, the theoretical secrecy outage probabilities match the simulation ones.

</details>


### [23] [On the Hamming Weight Functions of Linear Codes](https://arxiv.org/abs/2511.18250)
*Dongmei Huang,Qunying Liao,Sihem Mesnager,Gaohua Tang,Haode Yan*

Main category: cs.IT

TL;DR: 提出了一种基于权重函数的线性码二次构造新方法，通过固定汉明权重的码字集合构建新码，分析了维度、权重数和权重分布，并与原始码的可扩展性建立联系。


<details>
  <summary>Details</summary>
Motivation: 现有的线性码二次构造技术主要是删减、缩短和扩展，需要新的构造方法来探索码的固有组合和几何结构。

Method: 基于权重函数的通用框架，从给定码中固定汉明权重的码字集合构造新线性码。

Result: 建立了双权重码最小权重的上界并刻画了达到该界的所有双权重码，推导了双权重码参数的若干可除性性质。

Conclusion: 该方法不仅能生成新的线性码族，还为探索现有码的内在组合和几何结构提供了有力工具。

Abstract: Currently known secondary construction techniques for linear codes mainly include puncturing, shortening, and extending. In this paper, we propose a novel method for the secondary construction of linear codes based on their weight functions. Specifically, we develop a general framework that constructs new linear codes from the set of codewords in a given code having a fixed Hamming weight. We analyze the dimension, number of weights, and weight distribution of the constructed codes, and establish connections with the extendability of the original codes as well as the partial weight distribution of the derived codes. As a new tool, this framework enables us to establish an upper bound on the minimum weight of two-weight codes and to characterize all two-weight codes attaining this bound. Moreover, several divisibility properties concerning the parameters of two-weight codes are derived. The proposed method not only generates new families of linear codes but also provides a powerful approach for exploring the intrinsic combinatorial and geometric structures of existing codes.

</details>


### [24] [Function-Correcting Codes With Data Protection](https://arxiv.org/abs/2511.18420)
*Charul Rajput,B. Sundar Rajan,Ragnar Freij-Hollanti,Camilla Hollanti*

Main category: cs.IT

TL;DR: 本文提出了一个同时保护数据和函数值的函数纠错码通用框架，针对函数值需要比数据更强保护的场景，给出了构造方法、冗余度界限和具体实例。


<details>
  <summary>Details</summary>
Motivation: 现有的函数纠错码通常只保护函数值而不保护底层数据，本文旨在构建一个能同时保护数据和函数值的通用框架。

Method: 提出了两步构造方法，建立了最小距离图概念，研究了线性函数纠错码的结构特性，并推广了经典编码理论中的Plotkin和Hamming界限。

Result: 展示了在不增加冗余度的情况下为现有函数纠错码添加数据保护的可能性，为局部有界函数和汉明权重函数等特定函数族提供了显式构造。

Conclusion: 这是首个具有线性结构的函数纠错码研究，证明了完美码和MDS码无法为函数值提供超出数据保护水平的额外保护。

Abstract: Function-correcting codes (FCCs) are designed to provide error protection for the value of a function computed on the data. Existing work typically focuses solely on protecting the function value and not the underlying data. In this work, we propose a general framework that offers protection for both the data and the function values. Since protecting the data inherently contributes to protecting the function value, we focus on scenarios where the function value requires stronger protection than the data itself. We first introduce a more general approach and a framework for function-correcting codes that incorporates data protection along with protection of function values. A two-step construction procedure for such codes is proposed, and bounds on the optimal redundancy of general FCCs with data protection are reported. Using these results, we exhibit examples that show that data protection can be added to existing FCCs without increasing redundancy. Using our two-step construction procedure, we present explicit constructions of FCCs with data protection for specific families of functions, such as locally bounded functions and the Hamming weight function. We associate a graph called minimum-distance graph to a code and use it to show that perfect codes and maximum distance separable (MDS) codes cannot provide additional protection to function values over and above the amount of protection for data for any function. Then we focus on linear FCCs and provide some results for linear functions, leveraging their inherent structural properties. To the best of our knowledge, this is the first instance of FCCs with a linear structure. Finally, we generalize the Plotkin and Hamming bounds well known in classical error-correcting coding theory to FCCs with data protection.

</details>


### [25] [Aerial Semantic Relay-Enabled SAGIN: Joint UAV Deployment and Resource Allocation](https://arxiv.org/abs/2511.18456)
*Yanbo Yin,Dingzhu Wen,Changsheng You,XiaoWen Cao,Tat-Ming Lok,Dusit Niyato*

Main category: cs.IT

TL;DR: 提出了一种多集群无人机辅助的空天地一体化网络语义通信架构，支持语义用户和传统用户，通过联合优化功率、带宽和无人机位置来最大化系统和速率。


<details>
  <summary>Details</summary>
Motivation: 空天地一体化网络在6G系统中面临卫星到地面链路严重受损的挑战，虽然无人机可以作为中继节点，但卫星到无人机的链路仍是瓶颈。语义通信通过传输关键语义信息来提高频谱效率。

Method: 设计多集群无人机辅助的语义通信架构，在卫星到无人机链路使用语义通信，无人机实施智能自适应中继策略，可将语义数据直接转发给语义用户或转换为比特级数据给传统用户。采用交替优化算法解决非凸的联合优化问题。

Result: 数值结果表明，所提算法在各种信道条件和用户分布下，在系统和速率和频谱效率方面显著优于基线方案。

Conclusion: 该设计在保证语义通信高效率优势的同时，实现了更大覆盖区域的网络接入，联合资源分配和智能无人机部署对系统性能至关重要。

Abstract: Space-Air-Ground Integrated Networks (SAGINs) are pivotal for enabling ubiquitous connectivity in 6G systems, yet they face significant challenges due to severe satellite-to-ground link impairments. Although Unmanned Aerial Vehicles (UAVs) can function as relay nodes to compensate for air-to-ground channel degradation, the satellite-to-UAV link remains a critical bottleneck. Semantic Communication (SemCom) emerges as a promising solution to enhance spectral efficiency by transmitting essential semantic information. This paper proposes a novel multi-cluster UAV-aided SAGIN SemCom architecture that supports both semantic users (SemUsers) and conventional users (ConUsers). While SemCom is employed in the satellite-to-UAV link to improve transmission efficiency, the UAVs implement an intelligent adaptive relay strategy, capable of either directly forwarding semantic data to SemUsers or converting it into bit-level data for ConUsers. Compared to existing similar schemes, this design guarantees the high-efficiency advantages of SemCom while enabling network access for larger coverage area. A joint optimization problem is formulated to maximize the system's sum-rate through coordinated allocation of power, bandwidth, and UAV positions. To address this non-convex problem, we develop an efficient alternating optimization (AO) algorithm, which decomposes the original problem into tractable subproblems. Numerical results demonstrate that the proposed algorithm significantly outperforms baseline schemes in terms of both sum-rate and spectral efficiency across various channel conditions and user distributions, underscoring the importance of joint resource allocation and intelligent UAV deployment.

</details>


### [26] [Performance Evaluation of Dual RIS-Assisted Received Space Shift Keying Modulation](https://arxiv.org/abs/2511.18610)
*Ferhat Bayar,Haci Ilhan,Erdogan Aydin*

Main category: cs.IT

TL;DR: 提出了一种新型双RIS辅助架构，用于智能室内无线信号路由，其中第二个RIS基于源数据比特动态配置，将信号导向特定接收器或室内区域。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要探索单RIS部署和静态/半静态反射控制，需要更动态的智能信号路由方案来提升无线环境的重塑能力。

Method: 采用双RIS架构：RIS1被动反射信号，RIS2配备轻量级控制器执行比特驱动的空间调制，基于SSK进行信号检测，开发了完整的端到端系统模型。

Result: 在不同RIS间距离和载波频率下评估了可实现容量和中断概率的性能表现。

Conclusion: 双RIS架构通过动态配置实现了数据依赖的方向选择，为智能室内无线信号路由提供了有效解决方案。

Abstract: Reconfigurable intelligent surfaces (RISs) are gaining traction for their ability to reshape wireless environments with low energy consumption. However, prior studies primarily explore single-RIS deployments with static or semi-static reflection control. In this paper, we propose a novel dual-RIS-assisted architecture for smart indoor wireless signal routing, wherein the second RIS (RIS$_2$) is dynamically configured based on source data bits to steer signals toward specific receivers or indoor zones. The first RIS (RIS$_1$), positioned near a fed antenna or access point, passively reflects the incident signal. RIS$_2$, equipped with a lightweight controller, performs bit-driven spatial modulation to enable data-dependent direction selection at the physical layer. We develop a complete end-to-end system model, including multi-hop channel representation, RIS phase configuration mapping, and signal detection based on space shift keying (SSK). Performance analysis is evaluated in terms of achievable capacity and outage probability under varying inter-RIS distances and carrier frequencies.

</details>


### [27] [Understanding the Role of Phase and Position Design in Fluid Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2511.18663)
*J. D. Vega-Sánchez,V. H. Garzón Pacheco,N. V. Orozco Garzón,H. R. Carvajal Mora,F. J. López-Martínez*

Main category: cs.IT

TL;DR: 本文通过对比FRIS与传统RIS的性能，发现当使用最优波束成形和相位设计时，FRIS的位置优化优势消失，但FRIS在空间相关性和较小孔径方面仍优于紧凑型RIS。


<details>
  <summary>Details</summary>
Motivation: 澄清FRIS性能提升是否完全归因于空间灵活性，还是等效孔径或相位设计的差异。

Method: 在两种实际场景中基准测试FRIS与传统RIS：传统RIS（相同数量的有源元件和相同总体孔径）和紧凑型RIS（相同数量的有源元件，但具有亚波长元件间距的较小孔径）。

Result: 统计分析表明：(i) 在没有相位偏移设计的情况下，FRIS中的空间位置优化比传统RIS有明显增益；(ii) 当FRIS和传统RIS采用最优波束成形和相位偏移设计时，这种优势消失；(iii) 由于空间相关性和较小孔径，FRIS在优化波束成形和相位偏移设计方面始终优于紧凑型RIS。

Conclusion: FRIS的位置优化在无相位设计时有价值，但在最优波束成形和相位设计下变得无关紧要；FRIS在空间相关性和孔径优势方面持续优于紧凑型RIS。

Abstract: Fluid Reconfigurable Intelligent Surfaces (FRISs) are gaining momentum as an improved alternative over classical RIS. However, it remains unclear whether their performance gains can be entirely attributed to spatial flexibility, or instead to differences in equivalent aperture or phase design. In this work, we shed light onto this problem by benchmarking FRIS vs. RIS performances in two practical scenarios: conventional RIS (same number of active elements and same overall aperture) and compact RIS (same number of active elements, and smaller aperture with sub-λ inter-element spacing). Statistical analysis demonstrates that: (i) spatial position optimization in FRIS provides noticeable gains over conventional RIS in the absence of phase-shift design; (ii) such benefits vanish when FRIS and conventional RIS employ optimal beamforming (BF) and phase shift (PS) design, making position optimization irrelevant; (iii) FRIS consistently outperforms compact RIS with optimized BF and PS design, owing to spatial correlation and smaller aperture.

</details>


### [28] [Study of Iterative Dynamic Channel Tracking for Multiple RIS-Assisted MIMO Systems](https://arxiv.org/abs/2511.18669)
*Roberto C. G. Porto,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出了一种基于LDPC码的迭代信道估计方案，用于多RIS系统的6G网络，通过利用信道相干时间和迭代处理来提高估计精度并减少导频开销。


<details>
  <summary>Details</summary>
Motivation: 6G网络中多RIS部署面临信道估计效率低和导频开销大的挑战，需要开发更有效的估计方法来支持实际部署。

Method: 采用LDPC编码导频，结合信道相干时间和迭代处理，利用导频和校验位进行联合估计，并整合先前估计结果来降低开销。

Result: 在sub-6 GHz场景下，无论是LOS还是NLOS条件下，该方法均优于现有方法，在显著降低导频开销的同时获得明显性能增益。

Conclusion: 所提出的迭代信道估计方案能够有效解决多RIS系统的信道估计问题，为6G网络中多RIS的实际部署提供了可行方案。

Abstract: The use of multiple Reconfigurable Intelligent Sur- faces (RIS) has gained attention in 6G networks to enhance coverage. However, the feasibility of deploying multiple RIS relies on efficient channel estimation and reduced pilot overhead. To address these challenges, this work proposes an iterative channel estimation scheme that exploits low-density parity-check (LDPC) codes, channel coherence time, and iterative processing to improve estimation accuracy while minimizing pilot length. Encoded pilots are used to strengthen the iterative processing, leveraging both pilot and parity bits, while previous estimates are incorporated to further reduce overhead. Simulations consider a sub-6 GHz scenario with non-sparse channels and multiple RIS under both LOS and NLOS conditions. The results show that the proposed method outperforms existing approaches, achieving significant gains with substantially lower pilot overhead.

</details>


### [29] [Exploring Spatial Flexibility and Phase Design in Fluid Reconfigurable Intelligent Surfaces: A Physical Layer Security Perspective](https://arxiv.org/abs/2511.18675)
*J. D. Vega-Sánchez,V. H. Garzón Pacheco,N. V. Orozco Garzón,D. A. Riofrío Almeida,D. P. Moya Osorio*

Main category: cs.IT

TL;DR: 本文分析了流体可重构智能表面(FRIS)的保密中断概率(SOP)性能，并与传统平面RIS和紧凑型RIS架构进行对比。通过MLE方法建模FRIS信道，使用Q学习算法优化元素位置。结果显示FRIS在元素位置优化后SOP显著改善，但与传统RIS采用优化波束成形和相移控制时差距缩小。


<details>
  <summary>Details</summary>
Motivation: 研究不同RIS架构对无线通信安全性能的影响，特别是FRIS在保密中断概率方面的表现，探索空间多样性在物理层安全中的重要性。

Method: 采用最大似然估计(MLE)方法建模FRIS端到端信道，使用Q学习算法自适应选择FRIS元素的空间位置，并与传统平面RIS和紧凑型RIS进行性能对比。

Result: FRIS在元素位置优化后SOP显著改善，但与传统RIS采用优化波束成形和相移控制时性能差距缩小。FRIS在空间相关性方面优于紧凑型RIS设计。减小元素间距对SOP产生负面影响。

Conclusion: 空间多样性对RIS的保密性能至关重要，FRIS在降低空间相关性方面具有优势，但传统RIS通过优化波束成形和相移控制也能达到相近性能。

Abstract: This work examines the secrecy outage probability (SOP) in Fluid Reconfigurable Intelligent Surfaces (FRIS) and contrasts their performance against two alternative RIS architectures: a traditional planar RIS and a compact RIS layout. To characterize the end-to-end FRIS channel, a maximum likelihood estimation (MLE) approach is introduced, while a Q-learning algorithm is employed to adaptively select the spatial positions of FRIS elements. Numerical evaluations show that optimizing element placement in FRIS significantly improves SOP compared to conventional RIS without phase adaptation. However, these improvements become less evident once the conventional RIS implements optimized beamforming (BF) and phase-shift (PS) controlling. In addition, FRIS maintains a clear advantage over compact RIS designs with optimized BF and PS, mainly due to its lower spatial correlation. Results further indicate that reducing the inter-element distance negatively impacts SOP, highlighting the importance of spatial diversity.

</details>


### [30] [On Construction of Linear (Euclidean) Hull Codes over Finite Extensions Binary Fields](https://arxiv.org/abs/2511.18779)
*Sanjit Bhowmick,Deepak Kumar Dalai,Sihem Mesnager*

Main category: cs.IT

TL;DR: 本文研究了线性码的hull（核）维度问题，特别关注一维hull和LCD码的关系，并提出了从ℓ维hull构造ℓ+1维hull的方法。


<details>
  <summary>Details</summary>
Motivation: 线性码的hull在检查置换等价性和计算自同构群算法中起关键作用，这些算法在hull尺寸较小时更有效。LCD码具有最小的hull，而一维hull是第二小的。

Method: 首先探索有限域上线性码的一维hull，证明在弱条件下，任何最小距离≥2的扩展二进制域LCD码等价于某线性码的一维hull，并提出从ℓ维hull构造ℓ+1维hull的方法。

Result: 建立了LCD码与一维hull码的等价关系，实现了hull维度的扩展构造，并推导出多个线性码ℓ维hull的构造方法。

Conclusion: 本文解决了hull维度变化问题，为线性码的hull结构提供了新的理论框架和构造方法，特别在维度扩展方面取得了重要进展。

Abstract: The hull of a linear code is defined as the intersection of the code and its dual. This concept was initially introduced to classify finite projective planes. The hull plays a crucial role in determining the complexity of algorithms used to check the permutation equivalence of two linear codes and compute a linear code's automorphism group. Research has shown that these algorithms are very effective when the hull size is small. Linear complementary dual (LCD) codes have the smallest hulls, while codes with a one-dimensional hull have the second smallest.
  A recent notable paper that directs our investigation is authored by H. Chen, titled ``On the Hull-Variation Problem of Equivalent Linear Codes", published in IEEE Transactions on Information Theory, volume 69, issue 5, in 2023. In this paper, we first explore the one-dimensional hull of a linear code over finite fields. Additionally, we demonstrate that any LCD code over an extended binary field \( \FF_q \) (where \( q > 3 \)) with a minimum distance of at least $2$ is equivalent to the one-dimensional hull of a linear code under a specific weak condition. Furthermore, we provide a construction for creating hulls with \( \ell + 1 \)-dimensionality from an \( \ell \)-dimensional hull of a linear code, again under a weak condition. This corresponds to a particularly challenging direction, as creating \( \ell \)-dimensional hulls from \( \ell + 1 \)-dimensional hulls. Finally, we derive several constructions for the \( \ell \)-dimensional hulls of linear codes as a consequence of our results.

</details>


### [31] [Detection of Number of Subcarriers of OFDM Systems using Eigen-Spectral Analysis](https://arxiv.org/abs/2511.19020)
*Vishnu Priya Chekuru,Ganapathiraju S S Ananya Varma,Arti Yardi,Praful Mankar*

Main category: cs.IT

TL;DR: 提出一种基于协方差矩阵特征谱分析的盲OFDM参数估计方法，无需先验知识即可准确估计子载波数量


<details>
  <summary>Details</summary>
Motivation: 在认知无线电等非协作场景中，接收机缺乏OFDM参数先验知识，需要盲估计子载波数量等关键参数

Method: 通过分析接收数据协方差矩阵的特征值谱特性，利用正确符号分段下协方差矩阵的独特秩特性来估计子载波数量

Result: 数值结果表明，即使在低信噪比条件下，该方法也能以高概率准确检测子载波数量

Conclusion: 该方法比现有方法更通用，能够检测任意数量的子载波，且性能与调制方案无关

Abstract: Orthogonal Frequency-Division Multiplexing (OFDM) is widely used in modern wireless communication systems due to its robustness against time-dispersive channels. In this work, we consider a non-cooperative scenario where the receiver does not have prior knowledge of the OFDM parameters such as the number of subcarriers and the aim is to estimate them using the received data. Such a setup has applications in cognitive radio networks. For this blind OFDM parameter estimation problem, we provide a novel method based on eigen-spectral analysis of the covariance matrix corresponding to the received data. In particular, we show that the covariance matrix exhibits a distinctive rank property under correct segmentation of the received symbols, reflecting a characteristic behavior in its eigenvalue spectrum that facilitates accurate estimation of the number of subcarriers. The proposed method is more general than existing approaches in the literature, as it can detect an arbitrary number of subcarriers and its performance remains independent of the modulation scheme. The numerical results show that the proposed method accurately detects the number of subcarriers with high probability even at low SNR.

</details>


### [32] [On the Tail Transition of First Arrival Position Channels: From Cauchy to Exponential Decay](https://arxiv.org/abs/2511.19074)
*Yen-Chi Lee*

Main category: cs.IT

TL;DR: 该论文分析了非零漂移分子通信系统中首次到达位置(FAP)通道的统计特性，揭示了从柯西分布到指数衰减的转变过程，并提出了"截断柯西"模型来描述这一行为。


<details>
  <summary>Details</summary>
Motivation: 虽然零漂移FAP通道被严格证明服从柯西分布，但实际分子通信系统通常在非零漂移条件下运行。需要理解漂移如何影响通道统计特性，特别是从重尾柯西行为到轻尾指数衰减的转变。

Method: 通过渐近分析识别临界空间尺度n_c=σ²/v，该尺度区分了扩散主导和漂移主导的两种机制。提出了"截断柯西"模型来描述通道的有效行为。

Result: 数值结果显示，在低漂移条件下，高斯近似严重低估了信道容量。零漂移情况为漂移辅助粒子传输的系统提供了适当的性能下限。

Conclusion: 该研究为分子通信系统的性能分析提供了重要见解，表明在非零漂移条件下，通道统计特性从柯西分布向指数衰减过渡，临界尺度n_c是理解这一转变的关键参数。

Abstract: While the zero-drift First Arrival Position (FAP) channel is rigorously known to be Cauchy-distributed, practical molecular communication systems typically operate with non-zero drift. This letter characterizes the transition from heavy-tailed Cauchy behavior to light-tailed exponential decay. Through asymptotic analysis, we identify a critical spatial scale $n_c=σ^2/v$ separating diffusion- and drift-dominated regimes, revealing that the channel effectively behaves as a ``Truncated Cauchy'' model. Numerical results show that Gaussian approximations severely underestimate capacity at low drift, while the zero-drift case provides the appropriate performance lower bound for systems where drift assists particle transport.

</details>


### [33] [Directional Pinching-Antenna Systems](https://arxiv.org/abs/2511.19133)
*Runxin Zhang,Yulin Shao,Yuanwei Liu*

Main category: cs.IT

TL;DR: DiPASS是一个将PASS建模从理想化抽象转向物理一致性的框架，提出了首个准确捕捉捏合天线定向辐射的通道模型，包含1.3 dB/m的波导衰减和随机视距阻塞。


<details>
  <summary>Details</summary>
Motivation: 将PASS建模从理想化抽象转向物理一致性，解决实际部署中的关键障碍。

Method: 引入"等配额分配"功率分配策略保证预定耦合长度，推导单PA场景下天线放置和方向的闭式解，开发多PA系统的可扩展优化框架。

Result: 波导多样性在提升系统容量方面超过天线密度，DiPASS为未来6G网络提供了现实的性能基准。

Conclusion: DiPASS从根本上重塑了对未来PASS支持的6G网络的理解和设计原则。

Abstract: We propose a directional pinching-antenna system (DiPASS), a comprehensive framework that transitions PASS modeling from idealized abstraction to physical consistency. DiPASS introduces the first channel model that accurately captures the directional, pencil-like radiation of pinching antennas, incorporates a practical waveguide attenuation of 1.3 dB/m, and accounts for stochastic line-of-sight blockage. A key enabler of DiPASS is our new "equal quota division" power allocation strategy, which guarantees predetermined coupling lengths independent of antenna positions, thereby overcoming a critical barrier to practical deployment. Our analysis yields foundational insights: we derive closed-form solutions for optimal antenna placement and orientation in single-PA scenarios, quantifying the core trade-off between waveguide and free-space losses. For multi-PA systems, we develop a scalable optimization framework that leverages directional sparsity, revealing that waveguide diversity surpasses antenna density in enhancing system capacity. Extensive simulations validate our analysis and demonstrate that DiPASS provides a realistic performance benchmark, fundamentally reshaping the understanding and design principles for future PASS-enabled 6G networks.

</details>


### [34] [Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints](https://arxiv.org/abs/2511.19156)
*Jianfeng Xu,Zeyan Li*

Main category: cs.IT

TL;DR: 本文提出了一个统一的理论框架，通过引入推导熵来量化信息处理的热力学成本，揭示了存储与计算之间的相变点，为设计节能AI架构提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型的快速扩展揭示了模型容量（存储）与推理效率（计算）之间的基本矛盾。经典信息理论缺乏统一的物理框架来量化从压缩定律生成信息与从内存检索信息的热力学成本。

Method: 提出了一个理论框架，将信息处理视为从本体状态到载体状态的映射。引入了推导熵这一新指标，量化从给定逻辑深度计算目标状态所需的有效功。分析了香农熵（存储）与计算复杂度（时间/能量）之间的相互作用。

Result: 证明了存在一个临界相变点：低于该阈值时，内存检索在热力学上更有利；高于该阈值时，生成计算成为最优策略。这种'能量-时间-空间'守恒定律为生成模型的效率提供了物理解释。

Conclusion: 推导熵的最小化是生物和人工智能演化的支配原则，为设计下一代节能AI架构提供了严格的数学界限。

Abstract: The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This "Energy-Time-Space" conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence.

</details>


### [35] [Stitched Polar Codes](https://arxiv.org/abs/2511.19249)
*Yuan Li,Zicheng Ye,Huazi Zhang,Jun Wang,Wen Tong,Guiying Yan,Zhiming Ma*

Main category: cs.IT

TL;DR: 提出缝合极化码，作为Arikan正则极化码的新颖推广，通过添加结构增强不可靠信息比特的可靠性，保持相同复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决正则极化码在速率匹配场景下的性能下降问题，通过重新配置极化过程来增强不可靠信息比特的可靠性。

Method: 在基础极化过程中缝合额外结构，保持极化变换结构和编码解码复杂度不变，通过灵活配置提升性能。

Result: 缝合极化码在性能上持续优于正则极化码，有效解决了速率匹配场景下的性能下降问题。

Conclusion: 缝合极化码是极化码的有效推广，在保持相同复杂度的同时显著提升性能，并通过理论分析证明了其优越性。

Abstract: In this paper, we introduce stitched polar codes, a novel generalization of Arıkan's regular polar codes. Our core methodology reconfigures the fundamental polarization process by stitching additional structures to enhance the reliability of less reliable information bits in the original code. This approach preserves the polar transformation structure and maintains the same encoding and decoding complexity. Thanks to the flexible configuration, stitched polar codes consistently outperform regular polar codes, effectively solving the performance degradation issue in rate-matched scenarios. Furthermore, we provide theoretical analysis on the weight spectrum and the polarization speed of stitched polar codes to prove their superiority.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [36] [Reconstructing Sets of Strings from Their k-way Projections: Algorithms & Complexity](https://arxiv.org/abs/2511.17707)
*Elise Tate,Joshua A. Grochow*

Main category: cs.DS

TL;DR: 本文引入了字符串集重构问题，研究如何从k-way投影中重建字符串集合，并分析了该问题的计算复杂性、近似性和参数化复杂性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多现象涉及超越简单成对关系的高阶依赖关系，需要新的组合模型来从算法和计算复杂性角度捕捉这些高阶依赖。

Method: 提出了基于遗传重构算法中重叠图的改进算法，关键区别在于k-mer不需要连续。还考虑了单字符串重构和最大k-wise独立性问题。

Result: 给出了问题的复杂性结果，包括硬度结果、不可近似性和参数化复杂性。通过实验展示了算法效率，并通过解析近似支持复杂度分析。

Conclusion: 该模型为分析高阶依赖关系提供了有效的算法框架，在多种参数下表现出良好的可扩展性，并揭示了k-way投影在信息重建中的极限。

Abstract: Graphs are a powerful tool for analyzing large data sets, but many real-world phenomena involve interactions that go beyond the simple pairwise relationships captured by a graph. In this paper we introduce and study a simple combinatorial model to capture higher order dependencies from an algorithms and computational complexity perspective. Specifically, we introduce the String Set Reconstruction problem, which asks when a set of strings can be reconstructed from seeing only the k-way projections of strings in the set. This problem is distinguished from genetic reconstruction problems in that we allow projections from any k indices and we maintain knowledge of those indices, but not which k-mer came from which string. We give several results on the complexity of this problem, including hardness results, inapproximability, and parametrized complexity.
  Our main result is the introduction of a new algorithm for this problem using a modified version of overlap graphs from genetic reconstruction algorithms. A key difference we must overcome is that in our setting the k-mers need not be contiguous, unlike the setting of genetic reconstruction. We exhibit our algorithm's efficiency in a variety of experiments, and give high-level explanations for how its complexity is observed to scale with various parameters. We back up these explanation with analytic approximations. We also consider the related problems of: whether a single string can be reconstructed from the k-way projections of a given set of strings, and finding the largest k at which we get no information about the original data set from its k-way projections (i.e., the largest $k$ for which it is "k-wise independent").

</details>


### [37] [From Hop Reduction to Sparsification for Negative Length Shortest Paths](https://arxiv.org/abs/2511.18253)
*Kent Quanrud,Navid Tajkhorshid*

Main category: cs.DS

TL;DR: 本文改进了负权最短路径问题的随机算法，将运行时间从之前的O(mn^{3/4})提升到O(mn^{0.7193})或O((mn)^{0.8620})，具体取决于图的稀疏程度。


<details>
  <summary>Details</summary>
Motivation: 传统实权单源最短路径算法需要O(mn)时间，近期突破性工作将其改进到O(mn^{3/4})。本文旨在进一步优化这一时间界限。

Method: 结合了分层稀疏化、递归和稀疏引导三种技术：将之前的"跳数缩减器"重新用于"负边稀疏化"；基于分层跳数缩减器的简单递归算法；用新的稀疏捷径图替换密集捷径图来改进引导结构。

Result: 获得了新的上界：当m≥n^{1.03456}时为O(mn^{0.7193})随机时间，当m≤n^{1.03456}时为O((mn)^{0.8620})随机时间，均优于之前的结果。

Conclusion: 通过将跳数缩减器重新用于负边稀疏化，并结合分层稀疏化、递归和稀疏引导技术，显著改进了负权最短路径问题的随机算法运行时间。

Abstract: The textbook algorithm for real-weighted single-source shortest paths takes $O(m n)$ time on a graph with $m$ edges and $n$ vertices. A recent breakthrough algorithm by [Fin24] takes $\tilde{O}(m n^{8/9})$ randomized time. The running time was subsequently improved to $\tilde{O}(mn^{4/5})$ [HJQ25] and then $\tilde{O}(m n^{3/4} + m^{4/5} n)$ [HJQ26].
  We build on the algorithms of [Fin24, HJQ25, HJQ26] to obtain faster strongly-polynomial randomized-time algorithms for negative-length shortest paths. An important new technique in this algorithm repurposes previous "hop-reducers" from [Fin24, HJQ26] into "negative edge sparsifiers", reducing the number of negative edges by essentially the same factor by which the "hops" were previously reduced. A simple recursive algorithm based on sparsifying the layered hop reducers of [Fin24] already gives an $\tilde{O}(m n^{\smash{\sqrt{3}}-1}) < O(mn^{.7321})$ randomized running time, improving [HJQ26] uniformly.
  We also improve the construction of the bootstrapped hop reducers in [HJQ26] by proposing new sparse shortcut graphs replacing the dense shortcut graphs in [HJQ26]. Integrating all three of layered sparsification, recursion, and sparse bootstrapping into the algorithm of [HJQ26] gives new upper bounds of $O(mn^{.7193})$ randomized time for $m \geq n^{1.03456}$ and $O((mn)^{.8620})$ randomized time for $m \leq n^{1.03456}$.

</details>


### [38] [Approximating maximum properly colored forests via degree bounded independent sets](https://arxiv.org/abs/2511.18263)
*Yuhang Bai,Kristóf Bérczi,Johanna K. Siemelink*

Main category: cs.DS

TL;DR: 提出了最大规模度有界拟阵独立集问题的一般框架，应用于最大规模适当着色森林问题，在多重图上实现了2/3近似比，改进了之前的5/9近似比。


<details>
  <summary>Details</summary>
Motivation: 研究最大规模适当着色森林问题，通过引入更一般的问题框架来改进现有近似算法。

Method: 引入最大规模度有界拟阵独立集问题，设计其近似算法，保证仅依赖于最大度数Δ，并将该框架应用于最大规模适当着色森林问题。

Result: 在多重图上获得了2/3近似比，优于之前的5/9近似比。

Conclusion: 通过一般化的问题框架，为最大规模适当着色森林问题提供了更好的近似算法。

Abstract: In the Maximum-size Properly Colored Forest problem, we are given an edge-colored undirected graph and the goal is to find a properly colored forest with as many edges as possible. We study this problem within a broader framework by introducing the Maximum-size Degree Bounded Matroid Independent Set problem: given a matroid, a hypergraph on its ground set with maximum degree $Δ$, and an upper bound $g(e)$ for each hyperedge $e$, the task is to find a maximum-size independent set that contains at most $g(e)$ elements from each hyperedge $e$. We present approximation algorithms for this problem whose guarantees depend only on $Δ$. When applied to the Maximum-size Properly Colored Forest problem, this yields a $2/3$-approximation on multigraphs, improving the $5/9$ factor of Bai, Bérczi, Csáji, and Schwarcz [Eur. J. Comb. 132 (2026) 104269].

</details>


### [39] [Steiner Forest: A Simplified Better-Than-2 Approximation](https://arxiv.org/abs/2511.18460)
*Anupam Gupta,Vera Traub*

Main category: cs.DS

TL;DR: 本文改进了Steiner Forest问题的近似算法，将近似比从之前的2-ε提升到1.994，这是对该问题超过20年未突破的2-近似比的重要改进。


<details>
  <summary>Details</summary>
Motivation: Steiner Forest问题的最优近似比在Agrawal等人于1991年提出的2-近似算法后，20多年来一直未能突破。直到最近Ahmadi等人给出了2-ε的近似算法，本文旨在进一步改进这一结果。

Method: 结合了Ahmadi等人的方法（如扩展的moat-growing原始对偶算法、识别autarkic pairs）与其他技术：用于Steiner树的相对贪心算法中的子模最大化组件收缩方法，以及autarkic triples的使用。

Result: 成功获得了1.994的近似比，显著改进了之前的结果。

Conclusion: 本文提出的更简洁的抽象方法有望为Steiner Forest问题的进一步改进开辟道路。

Abstract: In the Steiner Forest problem, we are given a graph with edge lengths, and a collection of demand pairs; the goal is to find a subgraph of least total length such that each demand pair is connected in this subgraph. For over twenty years, the best approximation ratio known for the problem was a $2$-approximation due to Agrawal, Klein, and Ravi (STOC 1991), despite many attempts to surpass this bound. Finally, in a recent breakthrough, Ahmadi, Gholami, Hajiaghayi, Jabbarzade, and Mahdavi (FOCS 2025) gave a $2-\varepsilon$-approximation, where $\varepsilon \approx 10^{-11}$.
  In this work, we show how to simplify and extend the work of Ahmadi et al. to obtain an improved $1.994$-approximation. We combine some ideas from their work (e.g., an extended run of the moat-growing primal-dual algorithm, and identifying autarkic pairs) with other ideas -- submodular maximization to find components to contract, as in the relative greedy algorithms for Steiner tree, and the use of autarkic triples. We hope that our cleaner abstraction will open the way for further improvements.

</details>


### [40] [Weighted Chairman Assignment and Flow-Time Scheduling](https://arxiv.org/abs/2511.18546)
*Siyue Liu,Victor Reis*

Main category: cs.DS

TL;DR: 本文提出了一种加权分配问题的确定性舍入方法，将分数分配转换为整数分配，同时保证累积误差小于最大权重。这推广了Tijdeman的无加权版本结果，并验证了Morell和Skutella单源不可分流猜想的一个特例。


<details>
  <summary>Details</summary>
Motivation: 解决加权分配问题中的舍入误差控制问题，为调度问题提供理论基础。该工作推广了经典的chairman assignment问题，并验证了单源不可分流猜想的一个特例。

Method: 提出一种确定性舍入算法，将分数分配x转换为整数分配y，使得对于每个i和t，累积加权误差小于最大权重。该方法基于Tijdeman无加权版本的推广。

Result: 证明了存在整数分配y，使得对于所有i∈[m]和t∈[n]，累积加权误差严格小于最大权重。作为应用，给出了调度问题中最大流时间最小化的3-近似算法。

Conclusion: 该工作成功推广了Tijdeman的经典结果到加权情况，验证了单源不可分流猜想的一个特例，并为调度问题提供了有效的近似算法。

Abstract: Given positive integers $m, n$, a fractional assignment $x \in [0,1]^{m \times n}$ and weights $d \in \mathbb{R}^n_{>0}$, we show that there exists an assignment $y \in \{0,1\}^{m \times n}$ so that for every $i\in[m]$ and $t\in [n]$, \[ \Big|\sum_{j \in [t]} d_j (x_{ij} - y_{ij}) \Big| < \max_{j \in [n]} d_j. \] This generalizes a result of Tijdeman (1973) on the unweighted version, known as the chairman assignment problem. This also confirms a special case of the single-source unsplittable flow conjecture with arc-wise lower and upper bounds due to Morell and Skutella (IPCO 2020). As an application, we consider a scheduling problem where jobs have release times and machines have closing times, and a job can only be scheduled on a machine if it is released before the machine closes. We give a $3$-approximation algorithm for maximum flow-time minimization.

</details>


### [41] [Online Smoothed Demand Management](https://arxiv.org/abs/2511.18554)
*Adam Lechowicz,Nicolas Christianson,Mohammad Hajiesmaili,Adam Wierman,Prashant Shenoy*

Main category: cs.DS

TL;DR: OSDM是一个在线平滑需求管理问题，针对数据中心等大型能源消费者的电网集成和储能需求。该问题涉及能源采购和交付决策，包含基础需求和灵活需求，目标是优化包含采购成本、交付成本和平滑性惩罚的总成本。


<details>
  <summary>Details</summary>
Motivation: 受电网集成和能源存储范式转变的推动，特别是针对数据中心等大型能源消费者。现有文献未能完全建模实际应用需求，需要解决在线决策中的能源采购、存储管理和需求平滑问题。

Method: 提出了PAAD（分区核算与聚合决策）竞争算法，并开发了一个新颖的学习框架，该框架在保证最坏情况竞争比的同时，允许在历史实例上进行端到端可微分学习。

Result: PAAD算法实现了最优竞争比。在电网集成数据中心与电池存储的案例研究中，PAAD有效解决了问题，端到端学习相比PAAD实现了显著的性能提升。

Conclusion: OSDM问题是对现有在线算法文献的重要推广，PAAD算法和端到端学习框架为解决这类问题提供了有效方法，在保证鲁棒性的同时显著提升了性能。

Abstract: We introduce and study a class of online problems called online smoothed demand management $(\texttt{OSDM})$, motivated by paradigm shifts in grid integration and energy storage for large energy consumers such as data centers. In $\texttt{OSDM}$, an operator makes two decisions at each time step: an amount of energy to be purchased, and an amount of energy to be delivered (i.e., used for computation). The difference between these decisions charges (or discharges) the operator's energy storage (e.g., a battery). Two types of demand arrive online: base demand, which must be covered at the current time, and flexible demand, which can be satisfied at any time steps before a demand-specific deadline $Δ_t$. The operator's goal is to minimize a cost (subject to the constraints above) that combines a cost of purchasing energy, a cost for delivering energy (if applicable), and smoothness penalties on the purchasing and delivery rates to discourage fluctuations and encourage ``grid healthy'' decisions. $\texttt{OSDM}$ generalizes several problems in the online algorithms literature while being the first to fully model applications of interest. We propose a competitive algorithm called $\texttt{PAAD}$ (partitioned accounting \& aggregated decisions) and show it achieves the optimal competitive ratio. To overcome the pessimism typical of worst-case analysis, we also propose a novel learning framework that provides guarantees on the worst-case competitive ratio (i.e., to provide robustness against nonstationarity) while allowing end-to-end differentiable learning of the best algorithm on historical instances of the problem. We evaluate our algorithms in a case study of a grid-integrated data center with battery storage, showing that $\texttt{PAAD}$ effectively solves the problem and end-to-end learning achieves substantial performance improvements compared to $\texttt{PAAD}$.

</details>


### [42] [Overlap Analysis of the Shortest Path Problem: Local Search, Landscapes, and Franz--Parisi Potential](https://arxiv.org/abs/2511.18666)
*Frederic Koehler,Joonhyung Shin*

Main category: cs.DS

TL;DR: 该论文研究了优化问题的多项式时间可解性预测方法，特别是通过几何方法分析优化景观。研究发现最短路径问题虽然多项式时间可解，但重叠间隙属性和Franz-Parisi势能预测局部搜索会失败，而在最短路径树问题上局部搜索应该成功，这与子模最小化的结果类似。


<details>
  <summary>Details</summary>
Motivation: 研究如何预测优化问题在多项式时间内的可解性，特别是在平均情况下缺乏强硬度证据的问题。探索几何方法（如优化景观分析）在预测算法可解性方面的有效性。

Method: 使用重叠间隙属性(OGP)和Franz-Parisi势能等几何方法分析优化景观，研究局部搜索算法在不同优化问题上的表现。通过分析最短路径和最短路径树问题的优化景观特性来预测算法性能。

Result: 发现OGP和Franz-Parisi势能正确预测了：最短路径问题的局部搜索会失败（尽管该问题多项式时间可解），而最短路径树问题的局部搜索应该成功。这与子模最小化问题的结果类似。

Conclusion: 几何方法如OGP和Franz-Parisi势能可以有效地预测局部搜索算法在优化问题上的表现，即使对于多项式时间可解的问题也能提供有价值的见解。这些方法揭示了优化景观特性与算法性能之间的深刻联系。

Abstract: Two directions in algorithms and complexity involve: (1) classifying which optimization problems can be solved in polynomial time, and (2) understanding which computational problems are hard to solve \emph{on average} in addition to the worst case. For many average-case problems, there does not currently exist strong evidence via reductions that they are hard. However, we can still attempt to predict their polynomial time tractability by proving lower bounds against restricted classes of algorithms.
  Geometric approaches to predicting tractability typically study the \emph{optimization landscape}. For optimization problems with random objectives or constraints, ideas originating in statistical physics suggest we should study the \emph{overlap} between approximately-optimal solutions. Formally, properties of \emph{Gibbs measures} and the \emph{Franz--Parisi potential} imply lower bounds against natural local search algorithms, such as Langevin dynamics. A related theory, the \emph{Overlap Gap Property (OGP)}, proves rigorous lower bounds against classes of algorithms which are stable functions of their input.
  A remarkable recent work of Li and Schramm showed that the shortest path problem in random graphs admits lower bounds against a class of stable algorithms, via the OGP. Yet this problem is polynomial time tractable. We further investigate this. We find that both the OGP and the Franz--Parisi potential predict that: (1) local search will fail in the optimization landscape of shortest paths, but (2) local search should succeed in the optimization landscape for shortest path \emph{trees}, which is true. Using the Franz--Parisi potential, we explain an analogy with results from combinatorial optimization -- submodular minimization is tractable via local search on the Lovász extension, even though ``naive'' local search over sets or the multilinear extension provably fails.

</details>


### [43] [A sufficient condition for characterizing the one-sided testable properties of families of graphs in the Random Neighbour Oracle Model](https://arxiv.org/abs/2511.19027)
*Christine Awofeso,Patrick Greaves,Oded Lachish,Amit Levi,Felix Reidl*

Main category: cs.DS

TL;DR: 本文研究了在随机邻居预言机模型中的图性质测试，提出了一个基于r-可允许性的充分条件来判定图族是否是H-可测试的，并获得了多个图族的新特征化结果。


<details>
  <summary>Details</summary>
Motivation: Czumaj和Sohler在STOC 2019中引入了随机邻居预言机模型，并证明了平面图和任何无minor图族是H-可测试的。本文旨在提供一个更广泛的充分条件来判定图族是否是H-可测试的，从而获得更多图族的特征化结果。

Method: 基于Nesetril和Ossona de Mendez发起的稀疏图族理论中的r-可允许性图度量，提出了一个充分条件来判定图族是否是H-可测试的。

Result: 获得了多个图族的新特征化结果，包括：在拓扑minor或浸入下封闭的图族、低密度对象的几何交集图、有界团数的欧几里得最近邻图、有界交叉数（每边）的图、有界队列数和栈数的图等。

Conclusion: 本文提出的基于r-可允许性的充分条件为判定图族是否是H-可测试的提供了一个广泛的标准，是朝着完全特征化H-可测试图族目标迈出的重要一步。

Abstract: We study property testing in the \emph{random neighbor oracle} model for graphs, originally introduced by Czumaj and Sohler [STOC 2019]. Specifically, we initiate the study of characterizing the graph families that are $H$-\emph{testable} in this model. A graph family $\mathcal{F}$ is $H$-testable if, for every graph $H$, $H$-\emph{freeness} (that is, not having a subgraph isomorphic to $H$) is testable with one-sided error on all inputs from $\mathcal{F}$.
  Czumaj and Sohler showed that for any $H$-testable family of graphs $\mathcal{F}$, the family of testable properties of $\mathcal{F}$ has a known characterization, a major goal in the study of property testing. Consequently, characterizing the collection of $H$-testable graph families will not only result in new characterizations, but will also exhaust this method of characterizing testable properties. We believe that our result is a substantial step towards this goal.
  Czumaj and Sohler further showed that the family of planar graphs is $H$-testable, as is any family of minor-free graphs. In this paper, we provide a sufficient and much broader criterion under which a family of graphs is $H$-testable. As a corollary, we obtain new characterizations for many families of graphs including: families that are closed under taking topological minors or immersions, geometric intersection graphs of low-density objects, euclidean nearest-neighbour graphs with bounded clique number, graphs with bounded crossing number (per edge), graphs with bounded queue- and stack number, and more.
  The criterion we provide is based on the \emph{$r$-admissibility} graph measure from the theory of sparse graph families initiated by Nesetril and Ossona de Mendez. Proving that specific families of graphs satisfy this criterion is an active area of research, consequently, the implications of this paper may be strengthened in the future.

</details>


### [44] [New Algorithms and Hardness Results for Connected Clustering](https://arxiv.org/abs/2511.19085)
*Jan Eube,Heiko Röglin*

Main category: cs.DS

TL;DR: 本文研究了连通聚类问题，证明了连通k-中心问题的Ω(log*(k))近似下界，并在有界树宽图上给出了精确算法和常数近似算法，同时改进了最小和半径与最小和直径目标的近似保证。


<details>
  <summary>Details</summary>
Motivation: 连通聚类问题在社区检测和大地测量等领域有重要应用，但之前对连通k-中心问题的近似性仍为开放问题，且连通k-中位数已被证明难以近似。

Method: 使用计算复杂性理论证明连通k-中心问题的近似下界，针对有界树宽图设计精确和近似算法，并为最小和半径与最小和直径目标设计近似算法。

Result: 证明了连通k-中心问题存在Ω(log*(k))近似下界；在有界树宽图上给出了多项式时间精确算法和FPT时间常数近似算法；将最小和直径的近似比从(6+ε)改进到(4+ε)。

Conclusion: 连通聚类问题在一般图上具有较高的计算复杂性，但在有界树宽图上可以有效求解，且某些目标函数可以获得较好的近似保证。

Abstract: Connected clustering denotes a family of constrained clustering problems in which we are given a distance metric and an undirected connectivity graph $G$ that can be completely unrelated to the metric. The aim is to partition the $n$ vertices into a given number $k$ of clusters such that every cluster forms a connected subgraph of $G$ and a given clustering objective gets minimized. The constraint that the clusters are connected has applications in many different fields, like for example community detection and geodesy.
  So far, $k$-center and $k$-median have been studied in this setting. It has been shown that connected $k$-median is $Ω(n^{1- ε})$-hard to approximate which also carries over to the connected $k$-means problem, while for connected $k$-center it remained an open question whether one can find a constant approximation in polynomial time. We answer this question by providing an $Ω(\log^*(k))$-hardness result for the problem. Given these hardness results, we study the problems on graphs with bounded treewidth. We provide exact algorithms that run in polynomial time if the treewidth $w$ is a constant. Furthermore, we obtain constant approximation algorithms that run in FPT time with respect to the parameter $\max(w,k)$.
  Additionally, we consider the min-sum-radii (MSR) and min-sum-diameter (MSD) objective. We prove that on general graphs connected MSR can be approximated with an approximation factor of $(3 + ε)$ and connected MSD with an approximation factor of $(4 + ε)$. The latter also directly improves the best known approximation guarantee for unconstrained MSD from $(6 + ε)$ to $(4 + ε)$.

</details>


### [45] [Fast and Flexible Flow Decompositions in General Graphs via Dominators](https://arxiv.org/abs/2511.19153)
*Francisco Sena,Alexandru I. Tomescu*

Main category: cs.DS

TL;DR: 提出了一个基于支配树框架的混合整数线性规划方法，用于解决一般有环图上的流分解问题，相比传统DAG方法实现了千倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有流分解方法主要局限于有向无环图，导致要么使用启发式方法，要么需要将有环图预处理为DAG，这限制了方法的准确性和适用性。

Method: 利用支配树理论识别安全边序列，构建高效的MILP模型，通过变量预设置减少模型规模和计算复杂度。

Result: 在四个细菌数据集上的实验表明，预处理可实现高达千倍的加速，许多原本超时的实例能在30秒内解决。

Conclusion: 基于支配树的MILP简化框架和配套软件库有望成为多组装应用的基础构建模块。

Abstract: Multi-assembly methods rely at their core on a flow decomposition problem, namely, decomposing a weighted graph into weighted paths or walks. However, most results over the past decade have focused on decompositions over directed acyclic graphs (DAGs). This limitation has lead to either purely heuristic methods, or in applications transforming a graph with cycles into a DAG via preprocessing heuristics. In this paper we show that flow decomposition problems can be solved in practice also on general graphs with cycles, via a framework that yields fast and flexible Mixed Integer Linear Programming (MILP) formulations.
  Our key technique relies on the graph-theoretic notion of dominator tree, which we use to find all safe sequences of edges, that are guaranteed to appear in some walk of any flow decomposition solution. We generalize previous results from DAGs to cyclic graphs, by showing that maximal safe sequences correspond to extensions of common leaves of two dominator trees, and that we can find all of them in time linear in their size. Using these, we can accelerate MILPs for any flow decomposition into walks in general graphs, by setting to (at least) 1 suitable variables encoding solution walks, and by setting to 0 other walks variables non-reachable to and from safe sequences. This reduces model size and eliminates costly linearizations of MILP variable products.
  We experiment with three decomposition models (Minimum Flow Decomposition, Least Absolute Errors and Minimum Path Error), on four bacterial datasets. Our pre-processing enables up to thousand-fold speedups and solves even under 30 seconds many instances otherwise timing out. We thus hope that our dominator-based MILP simplification framework, and the accompanying software library can become building blocks in multi-assembly applications.

</details>


### [46] [PTF Testing Lower Bounds for Non-Gaussian Component Analysis](https://arxiv.org/abs/2511.19398)
*Ilias Diakonikolas,Daniel M. Kane,Sihan Liu,Thanasis Pittas*

Main category: cs.DS

TL;DR: 本文首次针对一系列统计任务建立了非平凡的PTF（多项式阈值函数）测试下界，特别是证明了NGCA（非高斯分量分析）的近乎最优PTF测试下界，填补了该领域的重要空白。


<details>
  <summary>Details</summary>
Motivation: 统计问题中存在信息-计算差距，现有文献主要分析低度多项式测试，但这类测试排除的算法范围有限。更自然和强大的PTF测试下界证明一直具有挑战性，此前文献中缺乏非平凡的PTF测试下界。

Method: 利用PTF伪随机生成器的最新工作和技术，开发了新的结构结果来分析低度多项式在随机方向上的行为，建立了多个独立有用的技术工具。

Result: 证明了NGCA的近乎最优PTF测试下界，该下界可推广到其他多个统计问题，填补了PTF测试下界研究的空白。

Conclusion: 本文首次实现了非平凡的PTF测试下界，为理解统计问题的信息-计算差距提供了更强大的工具，所开发的技术方法具有独立价值。

Abstract: This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.
  In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [47] [Token-Controlled Re-ranking for Sequential Recommendation via LLMs](https://arxiv.org/abs/2511.17913)
*Wenxi Dai,Wujiang Xu,Pinhuan Wang,Dimitris N. Metaxas*

Main category: cs.IR

TL;DR: COREC是一个基于LLM的重新排序框架，通过令牌增强机制实现细粒度用户控制，在保持个性化推荐的同时满足用户特定的属性要求。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的重新排序器缺乏细粒度用户控制机制，难以平衡用户固有偏好与多属性约束，导致用户只能被动接受推荐结果。

Method: 提出COREC框架，通过令牌增强机制整合用户明确的属性信号，学习平衡用户指令与潜在偏好，实现协作式推荐排序。

Result: 实验表明COREC在推荐效果上超越现有最佳基线，并在属性要求遵循方面表现优异，实现了可预测的细粒度排名操控。

Conclusion: COREC成功填补了用户控制机制的空白，使推荐系统从被动接受转向主动协作，实现了个性化与可控性的平衡。

Abstract: The widespread adoption of Large Language Models (LLMs) as re-rankers is shifting recommender systems towards a user-centric paradigm. However, a significant gap remains: current re-rankers often lack mechanisms for fine-grained user control. They struggle to balance inherent user preferences with multiple attribute-based constraints, often resorting to simplistic hard filtering that can excessively narrow the recommendation pool and yield suboptimal results. This limitation leaves users as passive recipients rather than active collaborators in the recommendation process. To bridge this gap, we propose COREC, a novel token-augmented re-ranking framework that incorporates specific user requirements in co-creating the recommendation outcome. COREC empowers users to steer re-ranking results with precise and flexible control via explicit, attribute-based signals. The framework learns to balance these commands against latent preferences, yielding rankings that adhere to user instructions without sacrificing personalization. Experiments show that COREC: (1) exceeds state-of-the-art baselines on standard recommendation effectiveness and (2) demonstrates superior adherence to specific attribute requirements, proving that COREC enables fine-grained and predictable manipulation of the rankings.

</details>


### [48] [Save, Revisit, Retain: A Scalable Framework for Enhancing User Retention in Large-Scale Recommender Systems](https://arxiv.org/abs/2511.18013)
*Weijie Jiang,Armando Ordorica,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: 提出了一种轻量级、可解释的框架来建模用户重新访问行为并优化长期用户留存，通过在Pinterest的搜索推荐场景中定义代理归因过程来减少噪声，并实现了0.1%的活跃用户提升。


<details>
  <summary>Details</summary>
Motivation: 用户留存是在线平台的关键目标，但建模和优化重新访问行为面临归因困难和时间跨度大的挑战，现有方法无法有效捕捉长期重新访问模式。

Method: 引入代理归因过程将保存行为与后续重新访问关联，减少因果关系的噪声；构建可扩展的事件聚合管道分析用户重新访问模式；增强排名系统展示高留存价值内容的能力。

Result: 在Pinterest的Related Pins功能中部署，服务超过5亿用户，实现了0.1%的活跃用户提升，且没有增加额外计算成本。

Conclusion: 该框架成功解决了重新访问建模中的归因和时间跨度问题，有效提升了用户留存，证明了在大型推荐系统中优化长期用户行为的可行性。

Abstract: User retention is a critical objective for online platforms like Pinterest, as it strengthens user loyalty and drives growth through repeated engagement. A key indicator of retention is revisitation, i.e., when users return to view previously saved content, a behavior often sparked by personalized recommendations and user satisfaction. However, modeling and optimizing revisitation poses significant challenges. One core difficulty is accurate attribution: it is often unclear which specific user actions or content exposures trigger a revisit, since many confounding factors (e.g., content quality, user interface, notifications, or even changing user intent) can influence return behavior. Additionally, the scale and timing of revisitations introduce further complexity; users may revisit content days or even weeks after their initial interaction, requiring the system to maintain and associate extensive historical records across millions of users and sessions. These complexities render existing methods insufficient for robustly capturing and optimizing long-term revisitation. To address these gaps, we introduce a novel, lightweight, and interpretable framework for modeling revisitation behavior and optimizing long-term user retention in Pinterest's search-based recommendation context. By defining a surrogate attribution process that links saves to subsequent revisitations, we reduce noise in the causal relationship between user actions and return visits. Our scalable event aggregation pipeline enables large-scale analysis of user revisitation patterns and enhances the ranking system's ability to surface items with high retention value. Deployed on Pinterest's Related Pins surface to serve 500+ million users, the framework led to a significant lift of 0.1% in active users without additional computational costs.

</details>


### [49] [Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems](https://arxiv.org/abs/2511.18024)
*Dor Arviv,Yehonatan Elisha,Oren Barkan,Noam Koenigstein*

Main category: cs.IR

TL;DR: 提出一种从推荐系统用户和物品嵌入中提取单义神经元的方法，使用稀疏自编码器揭示预训练表示中的语义结构，并通过预测感知训练目标保持用户-物品交互关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从推荐系统的用户和物品嵌入中提取可解释的单义概念，同时保持用户-物品交互关系，需要开发专门的方法来实现可解释和可控的个性化推荐。

Method: 使用稀疏自编码器（SAE）结合预测感知训练目标，通过冻结的推荐器反向传播，使学习到的潜在结构与模型的用户-物品亲和度预测对齐。

Result: 提取的神经元能够捕捉类型、流行度、时间趋势等属性，支持目标过滤和内容推广等后处理控制操作，无需修改基础模型。

Conclusion: 该方法在不同推荐模型和数据集上具有通用性，为可解释和可控的个性化推荐提供了实用工具。

Abstract: We present a method for extracting \emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \emph{prediction aware} training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.

</details>


### [50] [Fidelity-Aware Recommendation Explanations via Stochastic Path Integration](https://arxiv.org/abs/2511.18047)
*Oren Barkan,Yahlly Schein,Yehonatan Elisha,Veronika Bogina,Mikhail Baklanov,Noam Koenigstein*

Main category: cs.IR

TL;DR: SPINRec是一个模型无关的推荐系统解释方法，通过随机基线采样和路径积分技术，解决了推荐数据稀疏性和隐式反馈带来的解释保真度问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的解释保真度（解释准确反映模型真实推理的程度）严重不足，现有方法在稀疏和隐式的推荐数据上效果有限。

Method: 采用随机基线采样：从经验数据分布中采样多个合理的用户画像，选择最忠实的归因路径，捕捉观察和未观察交互的影响。

Result: 在三个模型（MF、VAE、NCF）、三个数据集（ML1M、Yahoo! Music、Pinterest）和多种反事实指标上，SPINRec始终优于所有基线方法。

Conclusion: SPINRec为推荐系统建立了新的忠实可解释性基准，提供了更稳定和个性化的解释。

Abstract: Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.

</details>


### [51] [ProHD: Projection-Based Hausdorff Distance Approximation](https://arxiv.org/abs/2511.18207)
*Jiuzhou Fu,Luanzheng Guo,Nathan R. Tallent,Dongfang Zhao*

Main category: cs.IR

TL;DR: ProHD是一种投影引导的近似算法，通过将数据投影到少数信息方向来识别候选极值点子集，从而显著加速Hausdorff距离计算，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 在大规模高维数据集上精确计算Hausdorff距离计算成本过高，需要一种快速且准确的近似方法。

Method: 通过将数据投影到质心轴和主要主成分等少数信息方向上，识别候选极值点子集，在该子集上计算Hausdorff距离。

Result: 在图像、物理和合成数据集（最多200万点，维度256）上的实验显示，ProHD比精确算法快10-100倍，比基于随机采样的近似方法误差低5-20倍，通常能达到与精确值相差几个百分点的结果。

Conclusion: ProHD使得在大规模向量数据库和流数据等场景中实现实用化的Hausdorff距离计算成为可能，提供了快速可靠的集合距离估计。

Abstract: The Hausdorff distance (HD) is a robust measure of set dissimilarity, but computing it exactly on large, high-dimensional datasets is prohibitively expensive. We propose \textbf{ProHD}, a projection-guided approximation algorithm that dramatically accelerates HD computation while maintaining high accuracy. ProHD identifies a small subset of candidate "extreme" points by projecting the data onto a few informative directions (such as the centroid axis and top principal components) and computing the HD on this subset. This approach guarantees an underestimate of the true HD with a bounded additive error and typically achieves results within a few percent of the exact value. In extensive experiments on image, physics, and synthetic datasets (up to two million points in $D=256$), ProHD runs 10--100$\times$ faster than exact algorithms while attaining 5--20$\times$ lower error than random sampling-based approximations. Our method enables practical HD calculations in scenarios like large vector databases and streaming data, where quick and reliable set distance estimation is needed.

</details>


### [52] [LLM Reasoning for Cold-Start Item Recommendation](https://arxiv.org/abs/2511.18261)
*Shijun Li,Yu Wang,Jin Wang,Ying Li,Joydeep Ghosh,Anne Cocos*

Main category: cs.IR

TL;DR: 提出针对Netflix冷启动推荐场景的LLM推理策略，通过监督微调、强化学习微调和混合方法显著提升冷启动推荐性能，在某些情况下比Netflix生产排名模型表现提升8%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注有丰富用户-物品交互数据的暖启动场景，而交互稀疏的冷启动场景研究不足，传统协同过滤方法在此类场景中效果受限。

Method: 利用LLM的先进推理能力推断用户偏好，特别是对新引入或交互稀少的物品。系统评估监督微调、强化学习微调以及两者结合的混合方法。

Result: 在真实数据上的广泛实验显示，在冷启动推荐场景中方法有效性和实际性能均有显著提升。基于推理的微调模型在某些情况下比Netflix生产排名模型表现提升8%。

Conclusion: 提出的LLM推理策略能有效解决冷启动推荐问题，为推荐系统在数据稀疏场景下的性能提升提供了新思路。

Abstract: Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.

</details>


### [53] [Democratic Recommendation with User and Item Representatives Produced by Graph Condensation](https://arxiv.org/abs/2511.18279)
*Jiahao Liang,Haoran Yang,Xiangyu Zhao,Zhiwen Yu,Guandong Xu,Wanyu Wang,Kaixiang Yang*

Main category: cs.IR

TL;DR: 提出DemoRec框架，利用图压缩技术生成用户和物品代表，通过构建紧凑交互图来提升推荐系统的计算效率和性能


<details>
  <summary>Details</summary>
Motivation: 解决大规模用户-物品交互图带来的计算效率低下和信息传播不足问题，现有方法存在泛化能力差和信息损失等局限性

Method: 基于民主原则，使用图压缩技术生成用户和物品代表，构建紧凑交互图并聚类具有共享特征的节点

Result: 在四个公共数据集上的实验表明，DemoRec在推荐性能、计算效率和鲁棒性方面相比SOTA方法有显著提升

Conclusion: DemoRec通过图压缩有效解决了大规模二分图推荐系统中的计算复杂性和高阶信息过度依赖问题

Abstract: The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods.

</details>


### [54] [BioArtlas: Computational Clustering of Multi-Dimensional Complexity in Bioart](https://arxiv.org/abs/2511.19162)
*Joonhyung Bae*

Main category: cs.IR

TL;DR: BioArtlas是一个分析81件生物艺术作品的系统，使用13个维度进行多轴分类，通过代码本方法将相关概念聚类，解决了文化术语的多义性问题。最佳聚类方案是k=15的凝聚聚类结合4D UMAP，揭示了四种组织模式。


<details>
  <summary>Details</summary>
Motivation: 生物艺术的混合性质跨越艺术、科学、技术、伦理和政治，无法用传统的单轴分类方法进行有效分析，需要开发新的多维度分析方法。

Method: 使用代码本方法将相关概念聚类为统一集群，分析81件生物艺术作品在13个维度上的特征，评估了800种表示空间-算法组合，找到最优聚类方案。

Result: 发现四种组织模式：艺术家特定的方法一致性、基于技术的分割、时间上的艺术演变和跨时间的概念亲和性。最佳聚类方案在轮廓系数、可信度和连续性方面表现优异。

Conclusion: 通过将分析优化与公共传播分离，BioArtlas提供了严谨的分析和可访问的探索，通过交互式网页界面公开数据集，为生物艺术的多维度分析提供了有效工具。

Abstract: Bioart's hybrid nature spanning art, science, technology, ethics, and politics defies traditional single-axis categorization. I present BioArtlas, analyzing 81 bioart works across thirteen curated dimensions using novel axis-aware representations that preserve semantic distinctions while enabling cross-dimensional comparison. Our codebook-based approach groups related concepts into unified clusters, addressing polysemy in cultural terminology. Comprehensive evaluation of up to 800 representation-space-algorithm combinations identifies Agglomerative clustering at k=15 on 4D UMAP as optimal (silhouette 0.664 +/- 0.008, trustworthiness/continuity 0.805/0.812). The approach reveals four organizational patterns: artist-specific methodological cohesion, technique-based segmentation, temporal artistic evolution, and trans-temporal conceptual affinities. By separating analytical optimization from public communication, I provide rigorous analysis and accessible exploration through an interactive web interface (https://www.bioartlas.com) with the dataset publicly available (https://github.com/joonhyungbae/BioArtlas).

</details>


### [55] [Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation](https://arxiv.org/abs/2511.18282)
*Jiahao Liang,Haoran Yang,Xiangyu Zhao,Zhiwen Yu,Mianjie Li,Chuan Shi,Kaixiang Yang*

Main category: cs.IR

TL;DR: 提出InvGCLLM框架，结合数据驱动模型和知识驱动LLMs，通过因果置信度评分指导图结构优化，解决图推荐系统中的OOD泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在分布偏移下性能显著下降，因为它们学习的是虚假的环境相关性而非稳定的因果关系。LLMs虽然具有丰富的世界知识和推理能力，但如何将其与特定图的细粒度拓扑结构有效整合来解决OOD问题仍具挑战。

Method: 首先使用数据驱动的不变性学习模型生成用户-物品交互的因果置信度评分；然后利用LLMs进行有针对性的图优化，基于其世界知识修剪虚假连接并增强缺失的因果链接；最后通过因果引导的对比学习目标从结构纯化的图中学习鲁棒表示。

Result: 在四个公共数据集上的实验表明，InvGCLLM在OOD推荐方面取得了显著改进，持续优于最先进的基线方法。

Conclusion: InvGCLLM成功整合了数据驱动模型和知识驱动LLMs的优势，通过因果学习框架有效解决了图推荐系统中的OOD泛化挑战，学习了对虚假相关性具有鲁棒性的表示。

Abstract: Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\textbf{Inv}$ariant $\textbf{G}$raph $\textbf{C}$ontrastive Learning with $\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines.

</details>


### [56] [UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning](https://arxiv.org/abs/2511.18342)
*Jiaming Zhang,Yuyuan Li,Xiaohua Feng,Zhifei Ren,Li Zhang,Chaochao Chen*

Main category: cs.IR

TL;DR: 本文提出UFO框架，通过自我博弈机制解决基于大语言模型的推荐系统中的项目侧不公平问题，发现不公平不仅来自监督微调，还来自预训练阶段。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将不公平归因于监督微调阶段缺乏公平约束，但研究发现不公平也源于预训练阶段的内在偏见，且现有方法难以同时保持推荐性能。

Method: 提出UFO框架，采用自我博弈机制，将不公平缓解建模为双玩家游戏：评判者识别预训练和监督微调中的不公平，校正者调整系统解决不公平同时保持推荐性能。

Result: 大量实验表明，UFO能有效缓解不公平问题，同时提升推荐性能。

Conclusion: UFO框架通过迭代优化完全解决了不公平问题，证明了同时处理预训练和监督微调阶段偏见的重要性。

Abstract: Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \textit{judger}, which identifies unfairness from both pre-training and SFT, and the \textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance.

</details>


### [57] [Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs](https://arxiv.org/abs/2511.18347)
*Haoyan Fu,Zhida Qin,Shixiao Yang,Haoyao Zhang,Bin Lu,Shuang Li,Tianyu Huang,John C. S. Lui*

Main category: cs.IR

TL;DR: TGODE模型通过构建用户时间图和物品演化图，结合时间引导扩散生成器和用户兴趣截断因子，解决了序列推荐中用户兴趣不连续和物品分布不均匀的问题，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法忽视了两个关键因素：用户交互间的不规则兴趣变化和物品随时间的高度不均匀分布。前者导致历史交互可能与当前购买行为无关，后者由于季节性趋势或促销活动可能不符合用户真实兴趣。

Method: 构建用户时间图和物品演化图；设计时间引导扩散生成器来自动增强时间感知用户图；开发用户兴趣截断因子识别稀疏时间间隔；使用图神经常微分方程对齐用户偏好和物品分布的演化。

Result: 在五个数据集上的实验结果显示，TGODE相比基线方法有10%到46%的性能提升。

Conclusion: TGODE通过有效处理用户兴趣不连续性和物品分布不均匀性，显著改善了序列推荐系统的性能，证明了考虑时间动态性的重要性。

Abstract: Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%.

</details>


### [58] [A Recommender System Based on Binary Matrix Representations for Cognitive Disorders](https://arxiv.org/abs/2511.18645)
*Raoul H. Kutil,Georg Zimmermann,Christian Borgelt*

Main category: cs.IR

TL;DR: 开发了一个基于二元矩阵的认知障碍诊断推荐系统，通过分析症状与障碍的关联来推荐最有效的诊断路径


<details>
  <summary>Details</summary>
Motivation: 认知障碍诊断复杂且需要专业知识，症状重叠使得基于症状的诊断导航困难，需要工具来辅助识别相关障碍并指导后续症状调查

Method: 使用二元矩阵表示障碍与症状组合，基于患者当前症状过滤矩阵行和列，识别潜在障碍并推荐最具信息量的下一个症状

Result: 原型系统成功从初始症状识别出合理障碍，推荐了进一步诊断的症状，并提供了症状-障碍关系的额外背景信息

Conclusion: 该推荐系统有潜力作为临床支持工具，帮助精神健康专业人员更高效识别相关障碍，指导症状特异性随访调查以提高诊断准确性

Abstract: Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.

</details>


### [59] [When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation](https://arxiv.org/abs/2511.18717)
*Jin Chai,Xiaoxiao Ma,Jian Yang,Jia Wu*

Main category: cs.IR

TL;DR: 提出了PASRec框架，通过扩散模型联合预测用户兴趣时间(ToI)和兴趣物品(IoI)，实现主动推荐而非被动响应


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐系统多为被动模式，只在用户打开应用时响应，错过用户关闭应用后的推荐机会。需要研究主动推荐，预测下次交互时间并主动推送物品

Method: 提出PASRec框架，基于扩散模型，通过联合目标函数对齐兴趣时间(ToI)和兴趣物品(IoI)的预测

Result: 在五个基准数据集上测试，采用留一法和时间分割策略，优于八个最先进的基线方法

Conclusion: PASRec框架在主动推荐任务中表现出色，能有效预测用户兴趣时间和兴趣物品，提升推荐效果

Abstract: Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.

</details>


### [60] [Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation](https://arxiv.org/abs/2511.18740)
*Yu Wang,Yonghui Yang,Le Wu,Yi Zhang,Richang Hong*

Main category: cs.IR

TL;DR: 提出了HaNoRec框架，通过动态调整优化权重和引入高斯扰动分布优化，解决多模态推荐中的样本难度不平衡和跨模态语义偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐方法仅使用文本模态，忽略了视觉信号对用户细粒度兴趣的影响；而多模态LLM方法存在样本难度不平衡和跨模态语义偏差的挑战。

Method: HaNoRec框架包含：1）基于样本估计难度和模型实时响应动态调整优化权重，优先处理困难样本；2）在输出logits上引入高斯扰动分布优化，增强跨模态语义一致性。

Result: 该方法能够更有效地建模用户偏好，提高推荐性能，特别是在处理长序列时减少模态偏差。

Conclusion: HaNoRec通过硬度感知和噪声正则化的偏好优化，有效解决了多模态推荐中的关键挑战，提升了推荐系统的性能。

Abstract: Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model.

</details>


### [61] [STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models](https://arxiv.org/abs/2511.18805)
*Yi Xu,Chaofan Fan,Jinxin Hu,Yu Zhang,Zeng Xiaoyi,Jing Zhang*

Main category: cs.IR

TL;DR: STORE是一个基于token的排序框架，通过语义token化、正交旋转变换和高效注意力机制，解决了推荐系统中高基数异构稀疏特征带来的表示瓶颈和计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统中的排序模型面临高基数、异构和稀疏特征空间的挑战，存在表示瓶颈（低秩表示导致One-Epoch和Interaction-Collapse现象）和计算瓶颈（特征token爆炸导致注意力机制计算量大且易分散）。

Method: 1) 语义token化：将高基数稀疏特征分解为紧凑的稳定语义token；2) 正交旋转变换：旋转低基数静态特征的子空间以促进更有效的特征交互；3) 高效注意力：过滤低贡献token以提高计算效率。

Result: 离线实验和在线A/B测试显示，该框架持续提升了预测准确性（在线CTR提升2.71%，AUC提升1.195%）和训练效率（吞吐量提升1.84倍）。

Conclusion: STORE框架通过创新的语义token化、正交旋转变换和高效注意力机制，有效解决了推荐系统排序模型中的表示和计算瓶颈，实现了准确性和效率的显著提升。

Abstract: Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like "One-Epoch" and "Interaction-Collapse," ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput).

</details>


### [62] [Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation](https://arxiv.org/abs/2511.18997)
*Chenhao Zhai,Chang Meng,Xueliang Wang,Shuchang Liu,Xiaolong Hu,Shisong Tang,Xiaoqiang Feng,Xiu Li*

Main category: cs.IR

TL;DR: 提出了一个新颖的异构多处理提升建模框架，用于短视频推荐中的权衡优化，通过离线混合提升建模和在线动态决策模块，有效解决了多策略冲突和个性化权衡问题。


<details>
  <summary>Details</summary>
Motivation: 短视频推荐面临用户偏好多样、不同策略指标冲突的挑战，现有提升模型难以处理异构多处理场景，传统固定权重方法缺乏个性化且可能导致决策偏差。

Method: HMUM框架包含离线混合提升建模模块和在线动态决策模块，前者捕捉多策略的协同和个体效应，后者实时估计不同用户响应的价值权重进行个性化决策。

Result: 在两个公共数据集、工业数据集和快手平台的在线A/B实验中，模型表现出优越的离线性能，关键指标显著提升，已在平台全面部署，服务数亿用户。

Conclusion: 该框架有效解决了短视频推荐中的多策略权衡优化问题，实现了更好的个性化推荐效果，具有重要的实际应用价值。

Abstract: The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.

</details>


### [63] [What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models](https://arxiv.org/abs/2511.19324)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 本文系统评估了跨语言信息检索(CLIR)中的四种干预方法，发现专门为CLIR训练的密集检索模型始终优于词汇匹配方法，且对比学习能显著改善语言偏见，特别是在低资源和跨文字语言对中效果最明显。


<details>
  <summary>Details</summary>
Motivation: 跨语言信息检索面临资源差异、文字系统不同以及嵌入模型跨语言语义对齐弱等挑战，现有方法依赖翻译和单语检索启发式方法，增加了计算开销和噪声，降低了性能。

Method: 系统评估了四种干预类型：文档翻译、使用预训练编码器的多语言密集检索、在词、短语和查询-文档级别的对比学习，以及交叉编码器重排序，在三个基准数据集上进行测试。

Result: 专门为CLIR训练的密集检索模型始终优于词汇匹配方法，从文档翻译中获益甚微。对比学习能减轻语言偏见，对初始对齐较弱的编码器带来显著改进，重排序有效但依赖于交叉编码器训练数据的质量。

Conclusion: 跨语言搜索系统应优先考虑语义多语言嵌入和有针对性的基于学习的对齐，而不是基于翻译的流程，特别是对于跨文字和资源不足的语言。

Abstract: Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.

</details>


### [64] [Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval](https://arxiv.org/abs/2511.19325)
*Olivia Macmillan-Scott,Roksana Goworek,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 本文评估了多语言大语言模型在跨语言查询扩展中的表现，发现查询长度决定提示技术的有效性，语言差异显著影响检索性能，微调仅在训练和测试数据格式相似时有效。


<details>
  <summary>Details</summary>
Motivation: 研究多语言大语言模型如何通过生成伪文档来进行跨语言查询扩展，以改善密集检索性能，特别是解决短查询与长文档之间的差距问题。

Method: 评估了多种多语言大语言模型及其微调变体，比较了不同的生成式扩展策略，分析了提示技术、语言差异和微调效果对跨语言检索性能的影响。

Result: 查询长度是决定提示技术有效性的关键因素；语言差异显著影响检索性能，不同文字系统间的检索效果尤其差；微调仅在训练和测试数据格式相似时带来性能提升。

Conclusion: 需要更平衡的多语言和跨语言训练与评估资源，以解决当前存在的语言差异和检索性能不平衡问题。

Abstract: Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.

</details>


### [65] [Revisiting Feedback Models for HyDE](https://arxiv.org/abs/2511.19349)
*Nour Jedidi,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究发现，使用Rocchio等传统反馈模型可以显著提升基于LLM的伪相关反馈方法HyDE的效果，而不是简单地拼接查询和LLM生成内容。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的PRF方法通常只是简单地将查询与LLM生成的扩展内容拼接，而没有充分利用Rocchio、RM3等成熟的反馈模型来优化稀疏检索器的查询扩展效果。

Method: 重新评估传统反馈模型在HyDE方法中的应用，使用Rocchio等算法从LLM生成的假设文档中提取和加权扩展词项。

Result: 实验表明，通过Rocchio反馈算法提取和加权扩展词项，可以显著提升HyDE方法的有效性。

Conclusion: 利用传统反馈算法是进一步提升基于LLM的PRF方法准确性的简单有效方式。

Abstract: Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.

</details>
