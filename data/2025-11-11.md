<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 13]
- [cs.IR](#cs.IR) [Total: 25]
- [cs.DS](#cs.DS) [Total: 16]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [CommUNext: Deep Learning-Based Cross-Band and Multi-Directional Signal Prediction](https://arxiv.org/abs/2511.05860)
*Chi-Jui Sung,Fan-Hao Lin,Tzu-Hao Huang,Chu-Hsiang Huang,Hui Chen,Chao-Kai Wen,Henk Wymeersch*

Main category: cs.IT

TL;DR: 提出了CommUNext深度学习框架，利用低频覆盖数据和目标频段的部分测量来预测高频信号强度，解决6G网络全频段认知中的计算和测量间隙问题。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要在FR1到FR3频段实现全频段认知，但面临两个挑战：基于物理的射线追踪计算成本过高，以及5G系统的测量间隙会降低吞吐量和增加延迟。

Method: 提出了CommUNext统一深度学习框架，包括两个互补架构：Full CommUNext用于大规模离线建模替代昂贵的射线追踪模拟，Partial CommUNext用于实时操作中重建不完整的低频地图以减轻测量间隙。

Result: 实验结果表明，CommUNext即使在稀疏监督下也能提供准确稳健的高频信号强度预测，显著减少了模拟和测量开销。

Conclusion: CommUNext框架为6G网络的全频段认知提供了一种有效的数据驱动解决方案，能够克服传统方法的计算和测量限制。

Abstract: Sixth-generation (6G) networks are envisioned to achieve full-band cognition by jointly utilizing spectrum resources from Frequency Range~1 (FR1) to Frequency Range~3 (FR3, 7--24\,GHz). Realizing this vision faces two challenges. First, physics-based ray tracing (RT), the standard tool for network planning and coverage modeling, becomes computationally prohibitive for multi-band and multi-directional analysis over large areas. Second, current 5G systems rely on inter-frequency measurement gaps for carrier aggregation and beam management, which reduce throughput, increase latency, and scale poorly as bands and beams proliferate. These limitations motivate a data-driven approach to infer high-frequency characteristics from low-frequency observations. This work proposes CommUNext, a unified deep learning framework for cross-band, multi-directional signal strength (SS) prediction. The framework leverages low-frequency coverage data and crowd-aided partial measurements at the target band to generate high-fidelity FR3 predictions. Two complementary architectures are introduced: Full CommUNext, which substitutes costly RT simulations for large-scale offline modeling, and Partial CommUNext, which reconstructs incomplete low-frequency maps to mitigate measurement gaps in real-time operation. Experimental results show that CommUNext delivers accurate and robust high-frequency SS prediction even with sparse supervision, substantially reducing both simulation and measurement overhead.

</details>


### [2] [Average AoI in Pinching Antenna-assisted WPCNs with Probabilistic LoS Blockage](https://arxiv.org/abs/2511.05947)
*Huimin Hu,Ruihong Jiang,Yanqing Xu,Jiarui Ma,Fang Fang*

Main category: cs.IT

TL;DR: 分析了夹持天线辅助无线供能通信网络中考虑概率视距阻塞的年龄信息性能，推导了平均AoI的闭式表达式，并通过优化天线位置最小化AoI。


<details>
  <summary>Details</summary>
Motivation: AoI是评估物联网网络状态更新新鲜度的关键指标，其优化对时间关键应用性能至关重要。在存在概率视距阻塞的无线供能通信网络中，需要分析AoI性能。

Method: 考虑物联网设备从配备夹持天线的基站收集能量并传输数据包的场景，推导电容充电时间、传输成功概率和成功更新到达间隔时间的闭式表达式，通过一维搜索优化天线位置。

Result: 仿真结果显示最优天线位置最靠近物联网设备，这一结论可扩展到多设备FDMA场景，基于夹持天线的系统显著优于传统固定天线系统。

Conclusion: 夹持天线辅助系统在无线供能通信网络中能有效优化年龄信息性能，最优部署策略是将天线靠近物联网设备。

Abstract: This paper analyzes the age of information (AoI) for a pinching antenna (PA)-assisted wireless powered communication network (WPCN) with probabilistic line-of-sight (LoS) blockage. AoI is a key metric for evaluating the freshness of status updates in IoT networks, and its optimization is crucial for ensuring the performance of time-critical applications. To facilitate analysis and gain useful insights, we consider a representative scenario, where an IoT device harvests energy from a base station (BS) equipped with a PA and transmits data packets to it. The IoT device harvests energy via the PA until its capacitor is fully charged, then transmits status updates using all stored energy. We derive closed-form expressions for the average AoI by analyzing the capacitor charging time, transmission success probability, and inter-arrival time of successful updates. To minimize the average AoI, we formulate an optimization problem of PA position, and propose a one-dimensional search to solve it. The simulation results show that the optimal PA position is the one closest to the IoT device, and this conclusion can be extended to the multi-IoT devices frequency division multiple access (FDMA) scenario. The PA-based systems significantly outperform the conventional fixed-antenna systems.

</details>


### [3] [Necessary and Sufficient Conditions for Capacity-Achieving Private Information Retrieval with Adversarial Servers](https://arxiv.org/abs/2511.06003)
*Atsushi Miki,Toshiyasu Matsushima*

Main category: cs.IT

TL;DR: 本文证明了实现容量最优私有信息检索(PIR)方案的必要和充分条件，为系统构建达到理论上界的PIR方案提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然证明了各种PIR方案的信息理论上界，但缺乏系统构建达到这些上界的PIR方案的方法。为了系统构建容量最优PIR方案，需要明确查询应满足的条件。

Method: 通过理论分析，推导出容量最优PIR方案的必要和充分条件，重点关注查询应满足的数学条件。

Result: 成功证明了容量最优PIR方案的必要和充分条件，为系统构建这类方案提供了明确的指导原则。

Conclusion: 本文建立的必要和充分条件为系统构建容量最优PIR方案奠定了理论基础，填补了该领域的重要空白。

Abstract: Private information retrieval (PIR) is a mechanism for efficiently downloading messages while keeping the index of the desired message secret from the servers. PIR schemes have been extended to various scenarios with adversarial servers: PIR schemes where some servers are unresponsive or return noisy responses are called robust PIR and Byzantine PIR, respectively; PIR schemes where some servers collude to reveal the index are called colluding PIR. The information-theoretic upper bound on the download efficiency of these PIR schemes has been proved in previous studies. However, systematic ways to construct PIR schemes that achieve the upper bound are not known. In order to construct a capacity-achieving PIR schemes systematically, it is necessary to clarify the conditions that the queries should satisfy. This paper proves the necessary and sufficient conditions for capacity-achieving PIR schemes.

</details>


### [4] [Capacity Analysis of Cascaded BD-RIS Assisted MIMO Systems](https://arxiv.org/abs/2511.06089)
*M. S. S. Manasa,Praful D. Mankar,Sundaram Vanka*

Main category: cs.IT

TL;DR: 本文研究了级联超对角可重构智能表面(BD-RIS)在MIMO系统中的部署，推导了与SVD注水算法和均匀功率分配预编码策略的最优闭式解，发现级联RIS能降低发射机复杂度，并分析了容量与功率、RIS尺寸之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 探索级联BD-RIS部署如何增强MIMO系统性能，特别是在降低发射机复杂度和提高系统容量方面的潜力。

Method: 推导了级联RIS与SVD注水算法和均匀功率分配预编码策略的联合最优闭式解，分析了近似遍历容量及其高信噪比近似。

Result: 级联RIS与均匀功率分配能达到与SVD注水算法相当的性能，同时降低发射机复杂度；建立了发射功率、RIS尺寸和可实现容量之间的明确权衡关系。

Conclusion: 级联RIS在高信噪比MIMO系统中具有实际部署价值，能在降低发射机复杂度的同时保持良好性能，为实际部署提供了重要指导。

Abstract: This paper examines the cascaded deployment of beyond diagonal (BD) reconfigurable intelligent surfaces (RISs) and explores its potential to enhance the performance of MIMO systems. We first derive the jointly optimal closed form solutions for the RISs in cascade with SVD water filling (SVD WF) and uniform power allocation (UPA) precoding strategies. The optimally configured cascaded-RIS with UPA is shown to achieve performance comparable to that with the SVD WF approach, suggesting that cascaded-RISs can also aid in reducing transmitter complexity. Furthermore, the approximate ergodic capacity for UPA is derived, along with its high SNR approximation which provides multiple useful insights into the dimension and deployment of cascaded RISs. The analytical results establish a clear tradeoff among transmit power, RIS size, and achievable capacity, providing insights for practical deployment in high SNR cascaded RIS MIMO systems.

</details>


### [5] [Towards Optimal Constellation Design for Digital Over-the-Air Computation](https://arxiv.org/abs/2511.06372)
*Saeed Razavikia,Deniz Gündüz,Carlo Fischione*

Main category: cs.IT

TL;DR: 提出了一种针对AWGN信道上计算优化的数字调制框架，通过求解非线性方程组获得最小化均方误差的最优星座设计，并在高信噪比下得到闭式解。


<details>
  <summary>Details</summary>
Motivation: 传统空中计算依赖模拟幅度调制，但受限于噪声敏感性和硬件约束，因此需要研究数字调制方案来提高性能。

Method: 将设计表述为加性映射问题，在发射功率约束下确定最小化均方误差的最优星座，建立非线性方程组并证明解的唯一性条件。

Result: 在高信噪比下通过广义Lambert函数得到最优调制参数的闭式表达式，并将框架扩展到高维网格、非高斯噪声和实数域计算。

Conclusion: 该数字调制框架为空中计算提供了优化的调制方案，在AWGN信道上实现了更好的计算性能。

Abstract: Over-the-air computation (OAC) has emerged as a key technique for efficient function computation over multiple-access channels (MACs) by exploiting the waveform superposition property of the wireless domain. While conventional OAC methods rely on analog amplitude modulation, their performance is often limited by noise sensitivity and hardware constraints, motivating the use of digital modulation schemes. This paper proposes a novel digital modulation framework optimized for computation over additive white Gaussian noise (AWGN) channels. The design is formulated as an additive mapping problem to determine the optimal constellation that minimizes the mean-squared error (MSE) under a transmit power constraint. We express the optimal constellation design as a system of nonlinear equations and establish the conditions guaranteeing the uniqueness of its solution. In the high signal-to-noise-ratio (SNR) regime, we derive closed-form expressions for the optimal modulation parameters using the generalized Lambert function, providing analytical insight into the system's behavior. Furthermore, we discuss extensions of the framework to higher-dimensional grids corresponding to multiple channel uses, to non-Gaussian noise models, and to computation over real-valued domains via hybrid digital-analog modulation.

</details>


### [6] [Differential Space-Time Block Coding for Phase-Unsynchronized Cell-Free MIMO Downlink](https://arxiv.org/abs/2511.06510)
*Marx M. M. Freitas,Giovanni Interdonato,Stefano Buzzi*

Main category: cs.IT

TL;DR: 提出了一种用于无小区大规模MIMO系统的差分空时块编码方法，无需接入点相位同步即可实现性能提升


<details>
  <summary>Details</summary>
Motivation: 解决无小区大规模MIMO系统中地理分布接入点相位同步的技术挑战，避免相位失准对系统性能的严重影响

Method: 采用差分空时块编码传输方案，无需信道状态信息，也不需要接入点之间的任何相位同步形式

Result: 数值仿真表明，所提出的DSTBC方案成功缓解了相位失准的影响，性能可与完全同步系统相媲美

Conclusion: DSTBC方法为无小区大规模MIMO系统提供了一种有效的相位同步替代方案，在保持性能的同时显著降低了系统复杂度

Abstract: In the downlink of a cell-free massive multiple-input multiple-output (CF-mMIMO) system, spectral efficiency gains critically rely on joint coherent transmission, as all access points (APs) must align their transmitted signals in phase at the user equipment (UE). Achieving such phase alignment is technically challenging, as it requires tight synchronization among geographically distributed APs. In this paper, we address this issue by introducing a differential space-time block coding (DSTBC) approach that bypasses the need for AP phase synchronization. We first provide analytic bounds to the achievable spectral efficiency of CF-mMIMO with phase-unsynchronized APs. Then, we propose a DSTBC-based transmission scheme specifically tailored to CF-mMIMO, which operates without channel state information and does not require any form of phase synchronization among the APs. We derive a closed-form expression for the resulting signal-to-interference-plus-noise ratio (SINR), enabling quantitative comparisons among different DSTBC schemes. Numerical simulations confirm that phase misalignments can significantly impair system performance. In contrast, the proposed DSTBC scheme successfully mitigates these effects, achieving performance comparable to that of fully synchronized systems.

</details>


### [7] [Events Meet Phase-Shifting Digital Holography: Practical Acquisition, Theory, and Algorithms](https://arxiv.org/abs/2511.06591)
*Ittetsu Uchiyama,Chihiro Tsutake,Keita Takahashi,Toshiaki Fujii*

Main category: cs.IT

TL;DR: 提出了一种基于混合事件视觉传感器的相位移动数字全息方法，通过单次曝光期间的相位移动实现高效全息重建


<details>
  <summary>Details</summary>
Motivation: 传统相位移动数字全息方法需要多次曝光，效率较低。本文旨在开发一种在单次曝光内完成相位移动的高效全息方法

Method: 利用混合事件视觉传感器记录相位移动过程中模糊的全息图和对应模糊变化的事件信号，通过分析和优化方法重建完整复波前

Result: 实验结果表明，该方法在重建质量上与传统的相位移动数字全息方法相当，同时显著提高了采集效率

Conclusion: 基于混合事件视觉传感器的相位移动数字全息方法能够实现高效的全息重建，为数字全息技术提供了新的解决方案

Abstract: We introduce a novel phase-shifting digital holography (PSDH) method leveraging a hybrid event-based vision sensor (EVS). The key idea of our method is the phase shift during a single exposure. The hybrid EVS records a hologram blurred by the phase shift, together with the events corresponding to blur variations. We present analytical and optimization-based methods that theoretically support the reconstruction of full-complex wavefronts from the blurred hologram and events. The experimental results demonstrate that our method achieves a reconstruction quality comparable to that of a conventional PSDH method while enhancing the acquisition efficiency.

</details>


### [8] [The Inaccessible Game](https://arxiv.org/abs/2511.06795)
*Neil D. Lawrence*

Main category: cs.IT

TL;DR: 提出了一个基于四个公理构建的信息论动态系统——不可访问游戏，其中第四个信息隔离公理是新颖的，使系统独立于观察者且可交换，并保持总边际熵守恒。


<details>
  <summary>Details</summary>
Motivation: 构建一个观察者独立且可交换的信息论动态系统，研究在信息隔离条件下的最大熵产生和系统动力学结构。

Method: 基于四个公理构建不可访问游戏系统：前三个定义信息损失，第四个是信息隔离公理，确保系统独立于观察者。分析最大熵产生下的动力学，展示其具有类似GENERIC的结构。

Result: 在信息隔离公理下，总边际熵守恒（∑hi = C）。系统动力学表现出结合可逆和不可逆分量的GENERIC-like结构。

Conclusion: 不可访问游戏提供了一个信息隔离条件下的信息论动态系统框架，其熵守恒特性和GENERIC-like结构为研究观察者独立的系统动力学提供了新视角。

Abstract: In this paper we introduce the inaccessible game, an information-theoretic dynamical system constructed from four axioms. The first three axioms are known and define \emph{information loss} in the system. The fourth is a novel \emph{information isolation} axiom that assumes our system is isolated from observation, making it observer-independent and exchangeable. Under this isolation axiom, total marginal entropy is conserved: $\sum_i h_i = C$. We consider maximum entropy production in the game and show that the dynamics exhibit a GENERIC-like structure combining reversible and irreversible components.

</details>


### [9] [Code Equivalence, Point Set Equivalence, and Polynomial Isomorphism](https://arxiv.org/abs/2511.06843)
*Martin Kreuzer*

Main category: cs.IT

TL;DR: 本文证明了线性码等价问题与点集等价问题等价，并通过代数方法将问题转化为多项式同构问题，在某些温和假设下可在多项式时间内解决。


<details>
  <summary>Details</summary>
Motivation: 研究线性码等价问题的复杂性，寻找更高效的解决方法，通过代数几何方法简化问题。

Method: 将线性码等价问题转化为点集等价问题，然后通过齐次坐标环和规范理想构造Artinian Gorenstein代数，利用Macaulay逆系统进一步简化为多项式同构问题。

Result: 证明了线性码等价问题与点集等价问题的等价性，并给出了在温和假设下多项式时间求解的方法。

Conclusion: 通过代数几何方法成功简化了线性码等价问题的复杂性，为不可分解同构对偶码提供了有效的求解途径。

Abstract: The linear code equivalence (LCE) problem is shown to be equivalent to the point set equivalence (PSE) problem, i.e., the problem to check whether two sets of points in a projective space over a finite field differ by a linear change of coordinates. For such a point set $\mathbb{X}$, let $R$ be its homogeneous coordinate ring and $\mathfrak{J}_{\mathbb{X}}$ its canonical ideal. Then the LCE problem is shown to be equivalent to an algebra isomorphism problem for the doubling $R/\mathfrak{J}_{\mathbb{X}}$. As this doubling is an Artinian Gorenstein algebra, we can use its Macaulay inverse system to reduce the LCE problem to a Polynomial Isomorphism (PI) problem for homogeneous polynomials. The last step is polynomial time under some mild assumptions about the codes. Moreover, for indecomposable iso-dual codes we can reduce the LCE search problem to the PI search problem of degree 3 by noting that the corresponding point sets are self-associated and arithmetically Gorenstein, so that we can use the isomorphism problem for the Artinian reductions of the coordinate rings and form their Macaulay inverse systems.

</details>


### [10] [Rate-Optimal Streaming Codes Under an Extended Delay Profile for Three-Node Relay Networks With Burst Erasures](https://arxiv.org/abs/2511.06882)
*Zhipeng Li,Wenjie Ma*

Main category: cs.IT

TL;DR: 本文提出了一种扩展延迟配置文件方法，用于三节点中继网络中的流码设计，在更宽松的约束条件下实现了最优速率，覆盖了先前工作的限制条件。


<details>
  <summary>Details</summary>
Motivation: 研究三节点中继网络中在突发包擦除和延迟约束下的流码设计，旨在在更宽松的条件下实现最优传输速率，突破现有构造的限制。

Method: 采用扩展延迟配置文件方法，在约束条件$\frac{T - u - v}{2u - v} \leq \left\lfloor \frac{T - u - v}{u} \right\rfloor$下设计流码，其中$u = \max\{b_1, b_2\}$，$v = \min\{b_1, b_2\}$。

Result: 提出的方法在更宽松的约束条件下实现了最优速率，严格覆盖了先前工作$u\mid (T-u-v)$的限制条件。

Conclusion: 扩展延迟配置文件方法为三节点中继网络中的流码设计提供了更通用的解决方案，在更宽松的条件下实现了最优性能。

Abstract: This paper investigates streaming codes for three-node relay networks under burst packet erasures with a delay constraint $T$. In any sliding window of $T+1$ consecutive packets, the source-to-relay and relay-to-destination channels may introduce burst erasures of lengths at most $b_1$ and $b_2$, respectively. Let $u = \max\{b_1, b_2\}$ and $v = \min\{b_1, b_2\}$. Singhvi et al. proposed a construction achieving the optimal rate when $u\mid (T-u-v)$. In this paper, we present an extended delay profile method that attains the optimal rate under a relaxed constraint $\frac{T - u - v}{2u - v} \leq \left\lfloor \frac{T - u - v}{u} \right\rfloor$ and it strictly cover restriction $u\mid (T-u-v)$. %Furthermore, we demonstrate that the optimal rate for streaming codes is not achievable when $0< T-u-v<v$ under the convolutional code framework.

</details>


### [11] [Experimental Validation of Reflective Near-Field Beamfocusing using a b-bit RIS](https://arxiv.org/abs/2511.06994)
*Emil Björnson,Murat Babek Salman*

Main category: cs.IT

TL;DR: 本文首次通过实验验证了使用可重构智能表面(RIS)的反射式近场波束聚焦技术，在28 GHz频段使用1024单元1位RIS进行了室内测量，证实了近场波束聚焦的实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然波束聚焦已被理论证明是大孔径RIS的关键特性，但其实际实现一直未被探索。本文旨在通过实验验证近场波束聚焦的可行性。

Method: 推导了b位RIS在近场视距场景下阵列增益的新解析表达式，并通过28 GHz频段的1024单元1位RIS在室内办公环境进行了一系列测量实验。

Result: 实验证实了近场波束聚焦可以动态实现，并且所提出的解析模型能够准确预测性能，即使在存在硬件缺陷和多径传播的情况下。

Conclusion: 近场波束聚焦是RIS辅助无线通信中一个鲁棒且实际可行的特性。

Abstract: This paper presents the first experimental validation of reflective near-field beamfocusing using a reconfigurable intelligent surface (RIS). While beamfocusing has been theoretically established as a key feature of large-aperture RISs, its practical realization has remained unexplored. We derive new analytical expressions for the array gain achieved with a $b$-bit RIS in near-field line-of-sight scenarios, characterizing both the finite depth and angular width of the focal region. The theoretical results are validated through a series of measurements in an indoor office environment at 28 GHz using a one-bit 1024-element RIS. The experiments confirm that near-field beamfocusing can be dynamically achieved and accurately predicted by the proposed analytical model, despite the presence of hardware imperfections and multipath propagation. These findings demonstrate that near-field beamfocusing is a robust and practically viable feature of RIS-assisted wireless communications.

</details>


### [12] [A Copula-based Semantics-Structure Minimization Framework for QoS Guaranteed Wireless Communications](https://arxiv.org/abs/2511.07145)
*Xinke Jian,Zhiyuan Ren,Wenchi Cheng*

Main category: cs.IT

TL;DR: 该论文为语义通信建立了完整的理论公理基础，提出了四个公理，证明了pairwise rank-Copulas是最小结构语义的最小充分表示，构建了基于Jensen-Shannon散度的语义失真度量，并建立了样本复杂度边界、率失真边界、端到端SLA定理和语义源信道分离定理。


<details>
  <summary>Details</summary>
Motivation: 当前基于经验的语义通信研究缺乏统一的理论基础，无法提供可量化的服务质量保证，特别是在紧急场景下传输最小结构语义时。这限制了其发展成为可预测的工程科学。

Method: 提出四个公理，严格证明pairwise rank-Copulas族是最小结构语义的最小充分表示；构建基于Jensen-Shannon散度的语义失真度量；建立样本复杂度边界、率失真边界、端到端SLA定理和语义源信道分离定理。

Result: 通过解耦实验验证了框架的有效性，实证表明核心度量严格遵循基础公理，而标准感知度量则无法做到这一点。

Conclusion: 该研究为语义通信建立了完整的理论公理基础，提供了可证明的服务质量保证，推动了语义通信向可预测工程科学发展。

Abstract: Current empirically driven research on semantic communication lacks a unified theoretical foundation, preventing quantifiable Quality of Service guarantees, particularly for transmitting minimal structural semantics in emergency scenarios. This deficiency limits its evolution into a predictable engineering science. To address this, we establish a complete theoretical axiomatic basis for this problem. We propose four axioms and rigorously prove that the family of pairwise rank-Copulas is the minimal sufficient representation for minimal structural semantics. Based on this, we construct a semantic distortion metric, centered on the Jensen-Shannon divergence. We then establish the core theoretical boundaries of the framework: sample complexity bounds; rate-distortion bounds; an end-to-end Service Level Agreements theorem; and a semantic source-channel separation theorem, which provides a provable Quality of Service guarantee. Finally, we validate our framework through decoupled experiments, empirically demonstrating that our core metric strictly adheres to our foundational axioms while standard perceptual metrics fail to do so.

</details>


### [13] [Frequency Diverse (FD)-RIS-Enhanced Covert Communications: Defense Against Wiretapping via Joint Distance-Angle Beamforming](https://arxiv.org/abs/2511.07309)
*Han Xiao,Xiaoyan Hu,Wenjie Wang,Kai-Kit Wong,Kun Yang,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出了一种基于频率分集智能表面(FD-RIS)的隐蔽通信方案，通过联合距离-角度波束成形能力解决传统RIS在隐蔽通信中的安全盲区问题。


<details>
  <summary>Details</summary>
Motivation: 传统RIS辅助的隐蔽通信系统存在安全盲区挑战，而FD-RIS的联合距离-角度波束成形能力具有解决这些局限性的潜力。

Method: 开发了FD-RIS的信号处理模型，利用时延技术控制谐波信号；构建多看守场景下的FD-RIS辅助隐蔽通信系统；推导隐蔽约束的近似闭式表达式；提出迭代算法联合优化时延和调制频率。

Result: 仿真结果表明FD-RIS能显著提升隐蔽性能，特别是在角度重叠场景中，传统RIS性能严重下降时FD-RIS仍能保持良好性能。

Conclusion: FD-RIS在具有挑战性的空间环境下能有效增强隐蔽通信的鲁棒性。

Abstract: In response to the security blind zone challenges faced by traditional reconfigurable intelligent surface (RIS)-aided covert communication (CC) systems, the joint distance-angle beamforming capability of frequency diverse RIS (FD-RIS) shows significant potential for addressing these limitations. Therefore, this paper initially incorporates the FD-RIS into the CC systems and proposes the corresponding CC transmission scheme. Specifically, we first develop the signal processing model of the FD-RIS, which considers effective control of harmonic signals by leveraging the time-delay techniques. The joint distance-angle beamforming capability is then validated through its normalized beampattern. Based on this model, we then construct an FD-RIS-assisted CC system under a multi-warden scenario and derive an approximate closed-form expression for the covert constraints by considering the worst-case eavesdropping conditions and utilizing the logarithmic moment-generating function. An optimization problem is formulated which aims at maximizing the covert user's achievable rate under covert constrains by jointly designing the time delays and modulation frequencies. To tackle this non-convex problem, an iterative algorithm with assured convergence is proposed to effectively solve the time-delay and modulation frequency variables. To evaluate the performance of the proposed scheme, we consider three communication scenarios with varying spatial correlations between the covert user and wardens. Simulation results demonstrate that FD-RIS can significantly improve covert performance, particularly in angular-overlap scenarios where traditional RIS experiences severe degradation. These findings further highlight the effectiveness of FD-RIS in enhancing CC robustness under challenging spatial environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [14] [GreyShot: Zeroshot and Privacy-preserving Recommender System by GM(1,1) Model](https://arxiv.org/abs/2511.05493)
*Hao Wang*

Main category: cs.IR

TL;DR: 提出了一种名为GreyShot的零样本隐私保护推荐系统算法，使用GM(1,1)灰系统模型解决冷启动问题


<details>
  <summary>Details</summary>
Motivation: 推荐系统面临冷启动问题，传统方法如迁移学习和元学习存在局限，需要零样本且保护隐私的解决方案

Method: 利用在线评分数据的泊松-帕累托特性，采用GM(1,1)灰系统模型构建算法，无需输入数据

Result: 能够生成准确且公平的推荐结果

Conclusion: 推荐系统的零样本问题可以通过GM(1,1)等灰系统方法有效解决

Abstract: Every recommendation engineer needs to face the cold start problem when building his system. During the past decades, most scientists adopted transfer learning and meta learning to solve the problem. Although notable exceptions such as ZeroMat etc. have been invented in recent years, cold-start problem remains a challenging problem for many researchers. In this paper, we build a zeroshot and privacy-preserving recommender system algorithm GreyShot using GM(1,1) model by taking advantage of the Poisson-Pareto property of the online rating data. Our approach relies on no input data and is effective in generating both accurate and fair results. In conclusion, zeroshot problem of recommender systems could be effectively solved by grey system methods such as GM(1,1).

</details>


### [15] [IMDMR: An Intelligent Multi-Dimensional Memory Retrieval System for Enhanced Conversational AI](https://arxiv.org/abs/2511.05495)
*Tejas Pawar,Sarika Patil,Om Tilekar,Rushikesh Janwade,Vaibhav Helambe*

Main category: cs.IR

TL;DR: IMDMR是一个创新的多维度记忆检索系统，通过六种不同的记忆维度（语义、实体、类别、意图、上下文和时间）显著提升了对话AI的记忆能力，相比现有基线系统性能提升3.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统在维持跨多轮对话的连贯上下文记忆方面存在局限，难以提供个性化和上下文相关的响应。

Method: IMDMR采用多维度搜索架构，包括智能查询处理与动态策略选择、跨记忆实体解析和高级记忆集成技术，涵盖六个记忆维度。

Result: 在五个基线系统（包括LangChain RAG、LlamaIndex等）的评估中，IMDMR实现了0.792的整体性能，比最佳基线（0.207）提升3.8倍。消融研究显示多维度搜索比单维度方法性能提升23.3%。

Conclusion: IMDMR代表了对话AI记忆系统的重大进步，为增强用户交互和个性化体验提供了坚实基础。

Abstract: Conversational AI systems often struggle with maintaining coherent, contextual memory across extended interactions, limiting their ability to provide personalized and contextually relevant responses. This paper presents IMDMR (Intelligent Multi-Dimensional Memory Retrieval), a novel system that addresses these limitations through a multi-dimensional search architecture. Unlike existing memory systems that rely on single-dimensional approaches, IMDMR leverages six distinct memory dimensions-semantic, entity, category, intent, context, and temporal-to provide comprehensive memory retrieval capabilities. Our system incorporates intelligent query processing with dynamic strategy selection, cross-memory entity resolution, and advanced memory integration techniques. Through comprehensive evaluation against five baseline systems including LangChain RAG, LlamaIndex, MemGPT, and spaCy + RAG, IMDMR achieves a 3.8x improvement in overall performance (0.792 vs 0.207 for the best baseline). We present both simulated (0.314) and production (0.792) implementations, demonstrating the importance of real technology integration while maintaining superiority over all baseline systems. Ablation studies demonstrate the effectiveness of multi-dimensional search, with the full system outperforming individual dimension approaches by 23.3%. Query-type analysis reveals superior performance across all categories, particularly for preferences/interests (0.630) and goals/aspirations (0.630) queries. Comprehensive visualizations and statistical analysis confirm the significance of these improvements with p < 0.001 across all metrics. The results establish IMDMR as a significant advancement in conversational AI memory systems, providing a robust foundation for enhanced user interactions and personalized experiences.

</details>


### [16] [DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows](https://arxiv.org/abs/2511.05496)
*Hao Zhang,Qinghua Lu,Liming Zhu*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are "good enough" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation.

</details>


### [17] [Socially Aware Music Recommendation: A Multi-Modal Graph Neural Networks for Collaborative Music Consumption and Community-Based Engagement](https://arxiv.org/abs/2511.05497)
*Kajwan Ziaoddini*

Main category: cs.IR

TL;DR: 提出了一个多模态图神经网络框架，用于社交感知的音乐推荐，通过融合歌词、音频和视觉数据来增强个性化推荐和社区参与度。


<details>
  <summary>Details</summary>
Motivation: 现有的音乐推荐系统往往忽视社交关系和情感因素，无法有效整合多模态数据和用户社交网络信息，限制了推荐的准确性和社区互动性。

Method: 采用融合自由的深度互学习策略，对齐歌词、音频和视觉的模态特定表示；构建异构图结构捕获用户-歌曲交互和用户-用户社交关系；引入基于声学和文本信号的情感感知嵌入。

Result: 在基准数据集上的实验表明，MM-GNN在各项性能指标上显著优于现有最先进方法，消融研究验证了各模型组件的关键作用。

Conclusion: 该框架能够提供准确且具有社交情境的音乐推荐，证明了多模态数据融合和社交关系整合在音乐推荐系统中的重要性。

Abstract: This study presents a novel Multi-Modal Graph Neural Network (MM-GNN) framework for socially aware music recommendation, designed to enhance personalization and foster community-based engagement. The proposed model introduces a fusion-free deep mutual learning strategy that aligns modality-specific representations from lyrics, audio, and visual data while maintaining robustness against missing modalities. A heterogeneous graph structure is constructed to capture both user-song interactions and user-user social relationships, enabling the integration of individual preferences with social influence. Furthermore, emotion-aware embeddings derived from acoustic and textual signals contribute to emotionally aligned recommendations. Experimental evaluations on benchmark datasets demonstrate that MM-GNN significantly outperforms existing state-of-the-art methods across various performance metrics. Ablation studies further validate the critical impact of each model component, confirming the effectiveness of the framework in delivering accurate and socially contextualized music recommendations.

</details>


### [18] [Biomedical Hypothesis Explainability with Graph-Based Context Retrieval](https://arxiv.org/abs/2511.05498)
*Ilya Tyagin,Saeideh Valipour,Aliaksandra Sikirzhytskaya,Michael Shtutman,Ilya Safro*

Main category: cs.IR

TL;DR: 提出了一种基于假设生成上下文检索框架的生物医学假设生成系统可解释性方法，结合语义图检索和数据限制训练，通过检索增强生成与LLMs集成，并采用反馈循环迭代修正解释。


<details>
  <summary>Details</summary>
Motivation: 为生物医学假设生成系统提供可解释性，模拟真实世界发现约束，通过科学文献提供上下文证据来解释假设。

Method: 结合语义图检索和数据限制训练，通过检索增强生成与LLMs集成，采用反馈循环迭代识别和修正LLM生成解释中的缺陷部分。

Result: 在多个大语言模型上展示了方法性能，通过专家评估和大规模自动分析评估了解释和上下文检索质量。

Conclusion: 该方法有效提升了生物医学假设生成系统的可解释性和可靠性，代码已开源。

Abstract: We introduce an explainability method for biomedical hypothesis generation systems, built on top of the novel Hypothesis Generation Context Retriever framework. Our approach combines semantic graph-based retrieval and relevant data-restrictive training to simulate real-world discovery constraints. Integrated with large language models (LLMs) via retrieval-augmented generation, the system explains hypotheses with contextual evidence using published scientific literature. We also propose a novel feedback loop approach, which iteratively identifies and corrects flawed parts of LLM-generated explanations, refining both the evidence paths and supporting context. We demonstrate the performance of our method with multiple large language models and evaluate the explanation and context retrieval quality through both expert-curated assessment and large-scale automated analysis. Our code is available at: https://github.com/IlyaTyagin/HGCR.

</details>


### [19] [Weightless Neural Networks for Continuously Trainable Personalized Recommendation Systems](https://arxiv.org/abs/2511.05499)
*Rafayel Latif,Satwik Behera,Ali Al-Ebrahim*

Main category: cs.IR

TL;DR: 该论文探索使用无权重神经网络(WNNs)构建小型个人推荐模型，相比传统基于聚合用户数据的推荐系统，能够实现连续学习并更快适应用户实时反馈。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖大规模分布式系统和预训练权重，难以实时适应用户反馈且缺乏透明度。需要更灵活、可解释的个性化推荐方法。

Method: 使用无权重神经网络(WNNs)为每个用户训练个人模型，WNNs作为状态机而非预训练权重系统，支持连续学习。与经典加权系统和标准协同过滤方法进行对比。

Result: 在MovieLens数据集子集上实现了具有竞争力的准确率水平。

Conclusion: 无权重系统可以增强集中式推荐系统，通过让终端用户更直接地调整推荐器来实现更高的主观准确性。

Abstract: Given that conventional recommenders, while deeply effective, rely on large distributed systems pre-trained on aggregate user data, incorporating new data necessitates large training cycles, making them slow to adapt to real-time user feedback and often lacking transparency in recommendation rationale. We explore the performance of smaller personal models trained on per-user data using weightless neural networks (WNNs), an alternative to neural backpropagation that enable continuous learning by using neural networks as a state machine rather than a system with pretrained weights. We contrast our approach against a classic weighted system, also on a per-user level, and standard collaborative filtering, achieving competitive levels of accuracy on a subset of the MovieLens dataset. We close with a discussion of how weightless systems can be developed to augment centralized systems to achieve higher subjective accuracy through recommenders more directly tunable by end-users.

</details>


### [20] [Predicting Oscar-Nominated Screenplays with Sentence Embeddings](https://arxiv.org/abs/2511.05500)
*Francis Gross*

Main category: cs.IR

TL;DR: 使用现代语言模型预测奥斯卡剧本提名，通过结合剧本、摘要和标题特征，最佳模型达到0.66的宏F1分数和0.79的ROC-AUC。


<details>
  <summary>Details</summary>
Motivation: 奥斯卡提名对电影行业至关重要，能提升电影的知名度和商业成功，但缺乏预测剧本提名的合适数据集。

Method: 创建Movie-O-Label数据集，将电影剧本与奥斯卡记录结合；使用E5句子嵌入模型编码文本块，然后用逻辑回归模型分类剧本嵌入。

Result: 最佳模型在结合剧本、摘要和标题三个特征时，宏F1分数达0.66，精确率召回率AP为0.445（基线0.19），ROC-AUC为0.79。

Conclusion: 基于现代文本嵌入的简单模型在奥斯卡剧本提名预测中表现良好，可作为未来研究的起点。

Abstract: Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.

</details>


### [21] [SARCH: Multimodal Search for Archaeological Archives](https://arxiv.org/abs/2511.05667)
*Nivedita Sinha,Bharati Khanijo,Sanskar Singh,Priyansh Mahant,Ashutosh Roy,Saubhagya Singh Bhadouria,Arpan Jain,Maya Ramanath*

Main category: cs.IR

TL;DR: 开发了一个多模态考古文档搜索系统，用于搜索扫描的考古书籍和报告PDF，提取并索引文本、图像和表格，评估了多种检索策略。


<details>
  <summary>Details</summary>
Motivation: 考古文档以扫描PDF形式数字化，但扫描质量差异很大，需要开发专门的多模态搜索系统来处理这类文档。

Method: 设计多模态考古文档处理流程，提取并索引文本、图像（分类为地图、照片、布局等）和表格，评估关键词搜索、嵌入模型和混合检索策略。

Result: 报告了初步结果并进行了分析，展示了不同检索策略在考古文档搜索中的表现。

Conclusion: 这是一个令人兴奋的垂直领域，讨论了未来工作方向。

Abstract: In this paper, we describe a multi-modal search system designed to search old archaeological books and reports. This corpus is digitally available as scanned PDFs, but varies widely in the quality of scans. Our pipeline, designed for multi-modal archaeological documents, extracts and indexes text, images (classified into maps, photos, layouts, and others), and tables. We evaluated different retrieval strategies, including keyword-based search, embedding-based models, and a hybrid approach that selects optimal results from both modalities. We report and analyze our preliminary results and discuss future work in this exciting vertical.

</details>


### [22] [A Representation Sharpening Framework for Zero Shot Dense Retrieval](https://arxiv.org/abs/2511.05684)
*Dhananjay Ashok,Suraj Nair,Mutasem Al-Darabsah,Choon Hui Teo,Tarun Agarwal,Jonathan May*

Main category: cs.IR

TL;DR: 提出了一种无需训练的表征锐化框架，通过增强文档表征来区分相似文档，在零样本密集检索中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 零样本密集检索中，预训练检索器由于未在目标语料上训练，难以区分相似文档的语义差异。

Method: 开发了无需训练的表征锐化框架，通过增强文档表征来区分相似文档，并设计了索引时近似方法以平衡性能与成本。

Result: 在20多个多语言数据集上表现优于传统检索方法，在BRIGHT基准测试中达到新SOTA，且与现有方法兼容并能持续提升性能。

Conclusion: 表征锐化框架有效解决了零样本密集检索中相似文档区分问题，在保持性能增益的同时控制了推理成本。

Abstract: Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost.

</details>


### [23] [User Hesitation and Negative Transfer in Multi-Behavior Recommendation](https://arxiv.org/abs/2511.05808)
*Cheng Li,Yong Xu,Suhua Tang,Wenqiang Lin,Xin He,Jinde Cao*

Main category: cs.IR

TL;DR: 提出HNT框架，通过识别和利用弱信号（犹豫倾向和负面干扰）来改进多行为推荐，在三个真实数据集上显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理仅产生辅助行为而未触发目标行为的交互时缺乏深入建模，这些弱信号包含丰富的潜在信息，可分为犹豫倾向的正面弱信号和操作噪声的负面弱信号。

Method: HNT从正负两个维度建模弱信号特征：通过识别类似辅助行为构建犹豫集作为弱正样本来增强偏好建模；在辅助特征融合时引入潜在负迁移效应建模来抑制负面表示的干扰。

Result: 在三个真实数据集上，HNT相比最佳基线方法将HR@10和NDCG@10分别提升了12.57%和14.37%。

Conclusion: HNT框架能够有效识别和利用多行为推荐中的弱信号，显著提升推荐性能，证明了弱信号学习在多行为推荐中的重要性。

Abstract: Multi-behavior recommendation aims to integrate users' interactions across various behavior types (e.g., view, favorite, add-to-cart, purchase) to more comprehensively characterize user preferences. However, existing methods lack in-depth modeling when dealing with interactions that generate only auxiliary behaviors without triggering the target behavior. In fact, these weak signals contain rich latent information and can be categorized into two types: (1) positive weak signals-items that have not triggered the target behavior but exhibit frequent auxiliary interactions, reflecting users' hesitation tendencies toward these items; and (2) negative weak signals-auxiliary behaviors that result from misoperations or interaction noise, which deviate from true preferences and may cause negative transfer effects. To more effectively identify and utilize these weak signals, we propose a recommendation framework focused on weak signal learning, termed HNT. Specifically, HNT models weak signal features from two dimensions: positive and negative effects. By learning the characteristics of auxiliary behaviors that lead to target behaviors, HNT identifies similar auxiliary behaviors that did not trigger the target behavior and constructs a hesitation set of related items as weak positive samples to enhance preference modeling, thereby capturing users' latent hesitation intentions. Meanwhile, during auxiliary feature fusion, HNT incorporates latent negative transfer effect modeling to distinguish and suppress interference caused by negative representations through item similarity learning. Experiments on three real-world datasets demonstrate that HNT improves HR@10 and NDCG@10 by 12.57% and 14.37%, respectively, compared to the best baseline methods.

</details>


### [24] [Retrieval Quality at Context Limit](https://arxiv.org/abs/2511.05850)
*Max McKinnon*

Main category: cs.IR

TL;DR: Gemini 2.5 Flash 在长文本检索中表现出色，没有出现'迷失在中间'效应，能够准确回答针在干草堆问题，无论文档位置如何。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型在长上下文中的信息检索能力，特别是针对之前研究中发现的'迷失在中间'效应。

Method: 使用针在干草堆测试方法，评估Gemini 2.5 Flash在不同文档位置的事实检索准确性。

Result: Gemini 2.5 Flash能够以高精度回答针在干草堆问题，即使文档接近输入上下文限制，也没有出现检索准确率下降。

Conclusion: Gemini 2.5 Flash在长上下文检索方面有显著改进，'迷失在中间'效应在简单事实问答中不存在。

Abstract: The ability of large language models (LLMs) to recall and retrieve information from long contexts is critical for many real-world applications. Prior work (Liu et al., 2023) reported that LLMs suffer significant drops in retrieval accuracy for facts placed in the middle of large contexts, an effect known as "Lost in the Middle" (LITM). We find the model Gemini 2.5 Flash can answer needle-in-a-haystack questions with great accuracy regardless of document position including when the document is nearly at the input context limit. Our results suggest that the "Lost in the Middle" effect is not present for simple factoid Q\&A in Gemini 2.5 Flash, indicating substantial improvements in long-context retrieval.

</details>


### [25] [A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2511.05885)
*Qiyong Zhong,Jiajie Su,Ming Yang,Yunshan Ma,Xiaolin Zheng,Chaochao Chen*

Main category: cs.IR

TL;DR: Speeder是一个高效的多模态大语言模型范式，用于序列推荐系统，通过压缩表示、增强位置感知和渐进优化，显著提升训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在序列推荐中存在冗余和效率问题，需要更高效的范式来处理复杂的序列依赖关系。

Method: 提出三个关键组件：多模态表示压缩(MRC)减少冗余、序列位置感知增强(SPAE)捕捉复杂依赖、模态感知渐进优化(MPO)集成不同模态。

Result: 在VHR@1指标和计算效率上优于基线，训练速度达到SOTA模型的250%，推理速度达到400%。

Conclusion: Speeder在序列推荐中表现出卓越的性能和效率，未来可考虑集成实时反馈机制。

Abstract: In this paper, we proposed Speeder, a remarkably efficient paradigm to multimodal large language models for sequential recommendation. Speeder introduces 3 key components: (1) Multimodal Representation Compression (MRC), which efficiently reduces redundancy in item descriptions; (2) Sequential Position Awareness Enhancement (SPAE), which strengthens the model's ability to capture complex sequential dependencies; (3) Modality-aware Progressive Optimization (MPO), which progressively integrates different modalities to improve the model's understanding and reduce cognitive biases. Through extensive experiments, Speeder demonstrates superior performance over baselines in terms of VHR@1 and computational efficiency. Specifically, Speeder achieved 250% of the training speed and 400% of the inference speed compared to the state-of-the-art MLLM-based SR models. Future work could focus on incorporating real-time feedback from real-world systems.

</details>


### [26] [Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance](https://arxiv.org/abs/2511.05991)
*Tiago da Cruz,Bernardo Tavares,Francisco Belo*

Main category: cs.IR

TL;DR: 研究不同知识图谱构建策略对检索增强生成(RAG)系统性能的影响，发现基于关系数据库构建的本体指导知识图谱在性能上与文本构建方法相当，但成本更低且复杂度更小。


<details>
  <summary>Details</summary>
Motivation: RAG系统的性能严重依赖于外部知识的表示方式，本研究旨在探索不同知识图谱构建策略如何影响RAG性能，特别关注成本效益和实现复杂度。

Method: 比较多种方法：标准向量检索RAG、GraphRAG，以及基于关系数据库或文本语料库构建的本体指导知识图谱检索。

Result: 本体指导知识图谱结合块信息实现了与最先进框架相当的竞争性能，显著优于向量检索基线。基于关系数据库构建的本体指导知识图谱与文本构建方法性能相当，但具有一次性本体学习过程和避免本体合并复杂性的双重优势。

Conclusion: 基于关系数据库构建的本体指导知识图谱为RAG系统提供了成本效益高且实现简单的替代方案，在保持竞争力的同时大幅降低LLM使用成本和实现复杂度。

Abstract: Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches.

</details>


### [27] [Time Matters: A Novel Real-Time Long- and Short-term User Interest Model for Click-Through Rate Prediction](https://arxiv.org/abs/2511.06213)
*Xian-Jin Gui*

Main category: cs.IR

TL;DR: 提出了一种时间感知的长短期用户兴趣建模方法，通过周期性模式和时间点模式来捕捉用户兴趣与时间的关系，解决传统CTR预测方法忽略兴趣与时间相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CTR预测方法忽略了用户激活兴趣与发生时间之间的相关性，导致学习到的是用户在所有时间表达的兴趣混合，而非特定预测时间的实时兴趣。

Method: 从整个时间线角度研究用户兴趣演化，发现周期性模式和时间点模式，基于这两种模式提出时间感知的长短期用户兴趣建模方法。

Result: 在公共数据集和工业数据集上的大量实验验证了利用这两种模式的有效性，并证明了所提方法相比其他最先进方法的优越性。

Conclusion: 通过捕捉用户兴趣与时间的相关性，能够更准确地建模用户在不同时间的动态兴趣，提升CTR预测性能。

Abstract: Click-Through Rate (CTR) prediction is a core task in online personalization platform. A key step for CTR prediction is to learn accurate user representation to capture their interests. Generally, the interest expressed by a user is time-variant, i.e., a user activates different interests at different time. However, most previous CTR prediction methods overlook the correlation between the activated interest and the occurrence time, resulting in what they actually learn is the mixture of the interests expressed by the user at all time, rather than the real-time interest at the certain prediction time. To capture the correlation between the activated interest and the occurrence time, in this paper we investigate users' interest evolution from the perspective of the whole time line and develop two regular patterns: periodic pattern and time-point pattern. Based on the two patterns, we propose a novel time-aware long- and short-term user interest modeling method to model users' dynamic interests at different time. Extensive experiments on public datasets as well as an industrial dataset verify the effectiveness of exploiting the two patterns and demonstrate the superiority of our proposed method compared with other state-of-the-art ones.

</details>


### [28] [LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation](https://arxiv.org/abs/2511.06254)
*Teng Shi,Chenglei Shen,Weijie Yu,Shen Nie,Chongxuan Li,Xiao Zhang,Ming He,Yan Han,Jun Xu*

Main category: cs.IR

TL;DR: LLaDA-Rec是一个基于离散扩散的生成式推荐框架，通过并行语义ID生成解决传统自回归模型的单向约束和错误累积问题，在三个真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归推荐模型存在两个固有局限：(1) 单向约束限制了全局语义建模；(2) 固定从左到右生成顺序导致早期预测错误传播。

Method: 提出离散扩散框架，包含三个关键设计：并行分词方案、用户历史和下一项目级别的双重掩码机制、适配的束搜索策略。

Result: 在三个真实世界数据集上的实验表明，LLaDA-Rec持续优于基于ID的方法和最先进的生成式推荐器。

Conclusion: 离散扩散为生成式推荐建立了新的范式，能够更有效地建模项目间和项目内依赖关系并缓解错误累积。

Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.

</details>


### [29] [Exploiting Inter-Session Information with Frequency-enhanced Dual-Path Networks for Sequential Recommendation](https://arxiv.org/abs/2511.06285)
*Peng He,Yanglei Gan,Tingting Dai,Run Lin,Xuexin Li,Yao Liu,Qiao Liu*

Main category: cs.IR

TL;DR: FreqRec是一个频率增强的双路径序列推荐网络，通过可学习的频域多层感知机联合捕捉会话间和会话内行为，并使用结合交叉熵和频域一致性损失的复合目标函数进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有频率感知方法孤立处理每个会话且仅在时域优化，忽略了跨会话的频谱依赖性和预测频谱与实际频谱的对齐，导致宝贵的频率信息未被充分利用。

Method: 提出FreqRec模型，包含可学习的频域多层感知机来联合捕捉会话间和会话内行为，使用结合交叉熵和频域一致性损失的复合目标函数进行优化。

Result: 在三个基准数据集上的广泛实验表明，FreqRec超越了强基线模型，并在数据稀疏和噪声日志条件下保持稳健性能。

Conclusion: FreqRec通过有效利用跨会话频谱依赖性和显式对齐预测与真实频谱特征，显著提升了序列推荐的性能。

Abstract: Sequential recommendation (SR) aims to predict a user's next item preference by modeling historical interaction sequences. Recent advances often integrate frequency-domain modules to compensate for self-attention's low-pass nature by restoring the high-frequency signals critical for personalized recommendations. Nevertheless, existing frequency-aware solutions process each session in isolation and optimize exclusively with time-domain objectives. Consequently, they overlook cross-session spectral dependencies and fail to enforce alignment between predicted and actual spectral signatures, leaving valuable frequency information under-exploited. To this end, we propose FreqRec, a Frequency-Enhanced Dual-Path Network for sequential Recommendation that jointly captures inter-session and intra-session behaviors via a learnable Frequency-domain Multi-layer Perceptrons. Moreover, FreqRec is optimized under a composite objective that combines cross entropy with a frequency-domain consistency loss, explicitly aligning predicted and true spectral signatures. Extensive experiments on three benchmarks show that FreqRec surpasses strong baselines and remains robust under data sparsity and noisy-log conditions.

</details>


### [30] [HyMoERec: Hybrid Mixture-of-Experts for Sequential Recommendation](https://arxiv.org/abs/2511.06388)
*Kunrong Li,Zhu Sun,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 提出了HyMoERec框架，通过混合专家架构解决现有序列推荐模型中位置前馈网络处理所有用户交互和物品时忽略异质性的问题


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型对所有用户交互和物品采用统一处理方式，忽略了用户行为模式的异质性和物品复杂度的多样性

Method: 引入混合专家架构，结合共享和专用专家分支，采用自适应专家融合机制来捕获不同用户和物品的多样化推理

Result: 在MovieLens-1M和Beauty数据集上的实验表明，HyMoERec持续优于最先进的基线方法

Conclusion: HyMoERec通过混合专家架构有效解决了序列推荐中的异质性问题，在多个数据集上表现出优越性能

Abstract: We propose HyMoERec, a novel sequential recommendation framework that addresses the limitations of uniform Position-wise Feed-Forward Networks in existing models. Current approaches treat all user interactions and items equally, overlooking the heterogeneity in user behavior patterns and diversity in item complexity. HyMoERec initially introduces a hybrid mixture-of-experts architecture that combines shared and specialized expert branches with an adaptive expert fusion mechanism for the sequential recommendation task. This design captures diverse reasoning for varied users and items while ensuring stable training. Experiments on MovieLens-1M and Beauty datasets demonstrate that HyMoERec consistently outperforms state-of-the-art baselines.

</details>


### [31] [TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation](https://arxiv.org/abs/2511.06405)
*Dongsheng Wang,Shen Gao,Chengrui Huang,Yuxi Huang,Ruixiang Feng,Shuo Shang*

Main category: cs.IR

TL;DR: Tool4POI是一个基于工具增强的LLM框架，用于解决POI推荐中的历史依赖和可扩展性问题，通过外部检索和推理实现开放集推荐，无需特定任务微调。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的POI推荐方法存在两个关键限制：(1) 对用户历史上下文完整性的强依赖，在历史外场景表现差；(2) 受限于LLM的上下文窗口，难以处理大量候选POI。

Method: 包含三个核心模块：偏好提取模块总结长期用户兴趣，多轮候选检索模块与外部工具交互检索相关POI，重排序模块基于近期行为优化最终推荐。

Result: 在三个真实数据集上的实验表明，Tool4POI显著优于现有最优方法，在挑战性历史外场景中达到40%准确率，在Acc@5和Acc@10上分别平均提升20%和30%。

Conclusion: Tool4POI提供了一个无需特定任务微调的即插即用框架，有效解决了LLM在POI推荐中的历史依赖和可扩展性问题，在开放集推荐场景表现优异。

Abstract: Next Point-of-Interest (POI) recommendation is a fundamental task in location-based services. While recent advances leverage Large Language Model (LLM) for sequential modeling, existing LLM-based approaches face two key limitations: (i) strong reliance on the contextual completeness of user histories, resulting in poor performance on out-of-history (OOH) scenarios; (ii) limited scalability, due to the restricted context window of LLMs, which limits their ability to access and process a large number of candidate POIs. To address these challenges, we propose Tool4POI, a novel tool-augmented framework that enables LLMs to perform open-set POI recommendation through external retrieval and reasoning. Tool4POI consists of three key modules: preference extraction module, multi-turn candidate retrieval module, and reranking module, which together summarize long-term user interests, interact with external tools to retrieve relevant POIs, and refine final recommendations based on recent behaviors. Unlike existing methods, Tool4POI requires no task-specific fine-tuning and is compatible with off-the-shelf LLMs in a plug-and-play manner. Extensive experiments on three real-world datasets show that Tool4POI substantially outperforms state-of-the-art baselines, achieving up to 40% accuracy on challenging OOH scenarios where existing methods fail, and delivering average improvements of 20% and 30% on Acc@5 and Acc@10, respectively.

</details>


### [32] [Can LLM Annotations Replace User Clicks for Learning to Rank?](https://arxiv.org/abs/2511.06635)
*Lulu Yu,Keping Bi,Jiafeng Guo,Shihao Liu,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: 本文比较了LLM标注和点击数据在排序学习中的效果，发现点击数据在高频查询上表现更好，而LLM标注在中低频查询上更有效。提出了两种整合两种监督信号的训练策略，都能提升整体排序性能。


<details>
  <summary>Details</summary>
Motivation: 获取高质量人工标注成本高昂，而点击数据和LLM标注是两种低成本替代方案。本文旨在研究LLM标注是否能替代点击数据用于排序学习。

Method: 在公共数据集TianGong-ST和工业数据集Baidu-Click上进行综合比较实验，分析两种监督信号在不同查询频率下的表现差异，并探索了数据调度和频率感知多目标学习两种整合策略。

Result: 实验表明：点击监督模型在高频查询上表现更好，LLM标注监督模型在中低频查询上更有效；点击监督模型更擅长捕捉文档级信号，LLM标注监督模型在语义匹配和区分相关/非相关文档方面更优。整合两种监督信号的策略能提升所有频率查询的排序性能。

Conclusion: LLM标注和点击数据各有优势，可以互补使用。通过数据调度和频率感知多目标学习整合两种监督信号，能显著提升排序模型的整体性能，其中频率感知多目标学习方法效果更好。

Abstract: Large-scale supervised data is essential for training modern ranking models, but obtaining high-quality human annotations is costly. Click data has been widely used as a low-cost alternative, and with recent advances in large language models (LLMs), LLM-based relevance annotation has emerged as another promising annotation. This paper investigates whether LLM annotations can replace click data for learning to rank (LTR) by conducting a comprehensive comparison across multiple dimensions. Experiments on both a public dataset, TianGong-ST, and an industrial dataset, Baidu-Click, show that click-supervised models perform better on high-frequency queries, while LLM annotation-supervised models are more effective on medium- and low-frequency queries. Further analysis shows that click-supervised models are better at capturing document-level signals such as authority or quality, while LLM annotation-supervised models are more effective at modeling semantic matching between queries and documents and at distinguishing relevant from non-relevant documents. Motivated by these observations, we explore two training strategies -- data scheduling and frequency-aware multi-objective learning -- that integrate both supervision signals. Both approaches enhance ranking performance across queries at all frequency levels, with the latter being more effective. Our code is available at https://github.com/Trustworthy-Information-Access/LLMAnn_Click.

</details>


### [33] [When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare](https://arxiv.org/abs/2511.06668)
*Saeedeh Javadi,Sara Mirabi,Manan Gangar,Bahadorreza Ofoghi*

Main category: cs.IR

TL;DR: 该研究评估了5个LLM在医学信息检索增强生成(RAG)中的表现，发现相似但矛盾的检索内容会降低模型回答的准确性和一致性，需要矛盾感知的过滤策略。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，LLM可能产生幻觉或错误信息，RAG通过外部文档来缓解此问题，但当源文档包含过时或矛盾信息时会引入新错误。

Method: 创建基于澳大利亚TGA药品信息文档的基准数据集，将标题重新用作自然语言问题；按出版年份分层检索PubMed摘要以进行时间控制评估；分析过时或矛盾内容对模型回答的影响。

Result: 高度相似但矛盾的摘要确实会降低性能，导致模型回答不一致和事实准确性下降。

Conclusion: 仅依赖检索相似性不足以实现可靠的医疗RAG，需要矛盾感知的过滤策略来确保高风险领域的可信回答。

Abstract: In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.

</details>


### [34] [Learning to Fast Unrank in Collaborative Filtering Recommendation](https://arxiv.org/abs/2511.06803)
*Junpeng Zhao,Lin Li,Ming Li,Amran Bhuiyan,Jimmy Huang*

Main category: cs.IR

TL;DR: 提出了L2UnRank方法，通过降低目标物品的排名位置来实现推荐系统的快速去学习，相比现有方法提速50倍，同时保持推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有推荐去学习方法存在效率低下和性能下降问题，无法满足实时去学习需求，需要一种既能保护用户隐私又能保持推荐质量的快速去学习方法。

Method: L2UnRank通过三阶段实现：基于交互的p跳传播识别影响范围，计算该范围内实体的结构和语义影响，基于影响信息进行高效的排名感知参数更新。

Result: 在多个数据集和骨干模型上的实验表明，L2UnRank具有模型无关性，实现了最先进的去排名效果，推荐质量与重新训练相当，速度比现有方法快50倍。

Conclusion: L2UnRank为推荐系统提供了一种高效、有效的去学习方法，能够在保护用户隐私的同时维持推荐性能，满足实时去学习需求。

Abstract: Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank.

</details>


### [35] [Have We Really Understood Collaborative Information? An Empirical Investigation](https://arxiv.org/abs/2511.06905)
*Xiaokun Zhang,Zhaochun Ren,Bowei He,Ziqiang Cui,Chen Ma*

Main category: cs.IR

TL;DR: 该论文对推荐系统中的协同信息进行了系统性研究，提出了量化定义，分析了其分布特征，并评估了对推荐算法性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前对推荐系统中协同信息的理解有限，缺乏量化定义，不清楚其在用户-物品交互中的表现形式，以及其对推荐性能的影响。

Method: 基于物品共现模式澄清协同信息，识别其主要特征并提出量化定义；从多个方面估计协同信息的分布；评估协同信息对各种推荐算法性能的影响。

Result: 建立了实证框架，揭示了关于协同信息的许多有见地的观察结果，阐明了协同信息在实际中的结构化方式。

Conclusion: 该研究推进了对协同信息的理解，为开发更有效的推荐系统提供了有价值的指导方针，并指出了未来研究的方向。

Abstract: Collaborative information serves as the cornerstone of recommender systems which typically focus on capturing it from user-item interactions to deliver personalized services. However, current understanding of this crucial resource remains limited. Specifically, a quantitative definition of collaborative information is missing, its manifestation within user-item interactions remains unclear, and its impact on recommendation performance is largely unknown. To bridge this gap, this work conducts a systematic investigation of collaborative information. We begin by clarifying collaborative information in terms of item co-occurrence patterns, identifying its main characteristics, and presenting a quantitative definition. We then estimate the distribution of collaborative information from several aspects, shedding light on how collaborative information is structured in practice. Furthermore, we evaluate the impact of collaborative information on the performance of various recommendation algorithms. Finally, we highlight challenges in effectively capturing collaborative information and outlook promising directions for future research. By establishing an empirical framework, we uncover many insightful observations that advance our understanding of collaborative information and offer valuable guidelines for developing more effective recommender systems.

</details>


### [36] [Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization](https://arxiv.org/abs/2511.06937)
*Yu Hou,Hua Li,Ha Young Kim,Won-Yong Shin*

Main category: cs.IR

TL;DR: ReFiT是一个将强化学习微调集成到基于扩散的推荐系统中的新框架，通过将去噪轨迹建模为马尔可夫决策过程并使用协作信号感知奖励函数，实现了对扩散推荐器的有效后训练微调。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推荐系统中表现出色，但从头训练计算成本高且收敛后收益递减。现有强化学习方法依赖外部奖励模型，存在噪声反馈问题。

Method: 将去噪轨迹建模为马尔可夫决策过程，设计协作信号感知奖励函数直接反映推荐质量，使用策略梯度优化最大化观测交互的精确对数似然。

Result: 在多个真实数据集上，ReFiT相比强基线性能提升显著（序列推荐最高提升36.3%），具有线性复杂度，在多种扩散推荐场景中泛化良好。

Conclusion: ReFiT框架通过任务对齐的强化学习微调，有效提升了扩散推荐系统的性能，同时保持了计算效率。

Abstract: Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.

</details>


### [37] [Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation](https://arxiv.org/abs/2511.07028)
*Huayang Xu,Huanhuan Yuan,Guanfeng Liu,Junhua Fang,Lei Zhao,Pengpeng Zhao*

Main category: cs.IR

TL;DR: 提出Wavelet Enhanced Adaptive Frequency Filter方法，通过动态频域滤波和小波特征增强来解决序列推荐中静态滤波器无法处理个性化行为模式以及全局傅里叶变换模糊非平稳信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有频域方法使用静态固定滤波器，忽略了行为模式的个性化特点；全局离散傅里叶变换虽然擅长建模长程依赖，但会模糊非平稳信号和短期波动。

Method: 包含两个核心模块：动态频域滤波（根据行为序列动态调整滤波操作提取个性化全局信息）和小波特征增强（集成小波变换重构序列，增强模糊的非平稳信号和短期波动）。

Result: 在四个广泛使用的基准数据集上的大量实验证明了该方法的优越性。

Conclusion: 该方法在长序列推荐场景中实现了全面的性能和效率优化。

Abstract: Sequential recommendation has garnered significant attention for its ability to capture dynamic preferences by mining users' historical interaction data. Given that users' complex and intertwined periodic preferences are difficult to disentangle in the time domain, recent research is exploring frequency domain analysis to identify these hidden patterns. However, current frequency-domain-based methods suffer from two key limitations: (i) They primarily employ static filters with fixed characteristics, overlooking the personalized nature of behavioral patterns; (ii) While the global discrete Fourier transform excels at modeling long-range dependencies, it can blur non-stationary signals and short-term fluctuations. To overcome these limitations, we propose a novel method called Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation. Specifically, it consists of two vital modules: dynamic frequency-domain filtering and wavelet feature enhancement. The former is used to dynamically adjust filtering operations based on behavioral sequences to extract personalized global information, and the latter integrates wavelet transform to reconstruct sequences, enhancing blurred non-stationary signals and short-term fluctuations. Finally, these two modules work to achieve comprehensive performance and efficiency optimization in long sequential recommendation scenarios. Extensive experiments on four widely-used benchmark datasets demonstrate the superiority of our work.

</details>


### [38] [Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models](https://arxiv.org/abs/2511.07295)
*Tianrui Song,Wen-Shuo Chao,Hao Liu*

Main category: cs.IR

TL;DR: LLMHNI框架利用大语言模型生成的两个辅助用户-物品相关性信号来区分困难样本和噪声样本，解决推荐系统中隐式反馈的噪声问题。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈在训练推荐系统时不可避免地面临噪声问题，而传统方法难以区分噪声样本和困难样本，导致硬噪声混淆问题。

Method: 使用LLM获取用户-物品语义相关性进行负采样，提出目标对齐策略将LLM嵌入投影到用户-物品相关性建模空间；利用LLM推断的逻辑相关性识别样本，通过图对比学习抑制不可靠边。

Result: 实证结果表明LLMHNI显著提高了去噪和推荐性能。

Conclusion: LLMHNI框架通过利用LLM生成的辅助信号有效解决了硬噪声混淆问题，提升了推荐系统的性能。

Abstract: Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [39] [Efficient Dynamic MaxFlow Computation on GPUs](https://arxiv.org/abs/2511.05895)
*Shruthi Kannappan,Ashwina Kumar,Rupesh Nasre*

Main category: cs.DS

TL;DR: 提出了两种基于Push-Relabel的GPU算法，用于处理动态图中的最大流问题，能够高效处理批量边容量增减更新。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图是动态的且不断演化，由于计算复杂度高，为实际应用中的图计算最大流非常耗时，需要并行解决方案。

Method: 开发了两种基于Push-Relabel的GPU算法，能够高效处理批量边容量的增减变化。

Result: 实验证明，对于小规模更新，动态重新计算比基于GPU的静态最大流计算显著更快。

Conclusion: 提出的动态GPU算法能够有效处理动态图中的最大流问题，特别适用于小规模更新的场景。

Abstract: Maxflow is a fundamental problem in graph theory and combinatorial optimisation, used to determine the maximum flow from a source node to a sink node in a flow network. It finds applications in diverse domains, including computer networks, transportation, and image segmentation. The core idea is to maximise the total flow across the network without violating capacity constraints on edges and ensuring flow conservation at intermediate nodes. The rapid growth of unstructured and semi-structured data has motivated the development of parallel solutions to compute MaxFlow. However, due to the higher computational complexity, computing Maxflow for real-world graphs is time-consuming in practice. In addition, these graphs are dynamic and constantly evolve over time. In this work, we propose two Push-Relabel based algorithms for processing dynamic graphs on GPUs. The key novelty of our algorithms is their ability to efficiently handle both increments and decrements in edge capacities together when they appear in a batch. We illustrate the efficacy of our algorithms with a suite of real-world graphs. Overall, we find that for small updates, dynamic recomputation is significantly faster than a static GPU-based Maxflow.

</details>


### [40] [A Better-Than-2 Approximation for the Directed Tree Augmentation Problem](https://arxiv.org/abs/2511.06162)
*Meike Neuwohner,Olha Silina,Michael Zlatin*

Main category: cs.DS

TL;DR: 提出了加权有向树增广问题(WDTAP)，这是加权树增广问题(WTAP)的有向版本，旨在用最小成本的有向链接覆盖树的所有基本有向割。


<details>
  <summary>Details</summary>
Motivation: WDTAP能够捕捉用有向链接覆盖交叉无关集族的问题，也可用于解决加权多重2-TAP问题，其中需要至少两次覆盖无向树的边。

Method: 使用标准技术可获得2-近似算法，对于有界成本链接的情况，通过发现"柳树"实例类（其自然集合覆盖LP是整数规划）和引入"可见k-宽"实例概念（可用动态规划精确求解），结合复杂的树结构分析，开发了改进的近似算法。

Result: 为有界成本链接的WDTAP提供了(1.75+ε)-近似算法，改进了之前的2-近似结果。

Conclusion: 通过识别可精确求解的实例类并利用树的结构特性，成功提升了WDTAP的近似比，为这一有向树增广问题提供了更好的算法保证。

Abstract: We introduce and study a directed analogue of the weighted Tree Augmentation Problem (WTAP). In the weighted Directed Tree Augmentation Problem (WDTAP), we are given an oriented tree $T = (V,A)$ and a set of directed links $L \subseteq V \times V$ with positive costs. The goal is to select a minimum cost set of links which enters each fundamental dicut of $T$ (cuts with one leaving and no entering tree arc). WDTAP captures the problem of covering a cross-free set family with directed links. It can also be used to solve weighted multi $2$-TAP, in which we must cover the edges of an undirected tree at least twice. WDTAP can be approximated to within a factor of $2$ using standard techniques. We provide an improved $(1.75+ \varepsilon)$-approximation algorithm for WDTAP in the case where the links have bounded costs, a setting that has received significant attention for WTAP. To obtain this result, we discover a class of instances, called "willows'', for which the natural set covering LP is an integral formulation. We further introduce the notion of "visibly $k$-wide'' instances which can be solved exactly using dynamic programming. Finally, we show how to leverage these tractable cases to obtain an improved approximation ratio via an elaborate structural analysis of the tree.

</details>


### [41] [No Price Tags? No Problem: Query Strategies for Unpriced Information](https://arxiv.org/abs/2511.06170)
*Shivam Nadimpalli,Mingda Qiao,Ronitt Rubinfeld*

Main category: cs.DS

TL;DR: 本文提出了一个处理未知变量成本的定价查询模型变体，证明了成本不确定性会带来不可避免的开销，并设计了接近下界的竞争性策略。


<details>
  <summary>Details</summary>
Motivation: 传统定价查询模型假设完全知晓查询成本，这在许多现实场景中不成立。需要处理未知变量成本的情况。

Method: 基于定价查询策略与布尔函数分析的联系，结合在线算法技术设计策略。

Result: 证明了成本不确定性会带来不可避免的开销，但设计的策略能接近下界并与最佳成本感知策略竞争。

Conclusion: 成功建立了处理未知成本的定价查询模型，提供了接近最优的竞争性策略。

Abstract: The classic *priced query model*, introduced by Charikar et al. (STOC 2000), captures the task of computing a known function on an unknown input when each input variable can only be revealed by paying an associated cost. The goal is to design a query strategy that determines the function's value while minimizing the total cost incurred. However, all prior work in this model assumes complete advance knowledge of the query costs -- an assumption that fails in many realistic settings.
  We introduce a variant of the priced query model that explicitly handles *unknown* variable costs. We prove a separation from the traditional priced query model, showing that uncertainty in variable costs imposes an unavoidable overhead for every query strategy. Despite this, we design strategies that essentially match our lower bound and are competitive with the best cost-aware strategies for arbitrary Boolean functions. Our results build on a recent connection between priced query strategies and the analysis of Boolean functions, and draw techniques from online algorithms.

</details>


### [42] [Spanning and Metric Tree Covers Parameterized by Treewidth](https://arxiv.org/abs/2511.06263)
*Michael Elkin,Idan Shabat*

Main category: cs.DS

TL;DR: 本文研究了图上的树覆盖问题，提出了在具有平衡递归分隔符或有限树宽度的图上，拉伸度与树数量之间的平滑权衡关系，并应用于路径报告稀疏器、距离标记和路由方案等场景。


<details>
  <summary>Details</summary>
Motivation: 树覆盖在图算法中具有重要应用，如稀疏器、距离标记和路由方案。现有研究在拉伸度和树数量之间存在权衡，但缺乏对具有特定结构（如平衡递归分隔符或有限树宽度）图的系统分析。本文旨在填补这一空白。

Method: 针对具有平衡递归分隔符大小s(n)或树宽度t(n)的图，对于任意参数k，构造树覆盖和HST覆盖，其中拉伸度为O(k)，树数量分别为O(k²log n/log s(n)·s(n)^{1/k})或O(k log n·t(n)^{1/k})。同时构造具有相同参数但拉伸度为O(k log log n)的生成树覆盖。

Result: 证明了在具有平衡递归分隔符或有限树宽度的图上，存在拉伸度与树数量之间的平滑权衡。具体地，对于任意k，可获得拉伸度O(k)的树覆盖，树数量与s(n)^{1/k}或t(n)^{1/k}相关。同时为一般图构造了拉伸度O(k log log n)、平均重叠度O(n^{1/k})的生成树覆盖。

Conclusion: 本文提出的树覆盖构造方法在拉伸度和树数量之间实现了优化的权衡，显著改进了现有结果。这些构造可直接应用于改进路径报告稀疏器、低跳度量稀疏器、距离标记方案和路由方案等图算法问题。

Abstract: Given a graph $G=(V,E)$, a tree cover is a collection of trees $\mathcal{T}=\{T_1,T_2,...,T_q\}$, such that for every pair of vertices $u,v\in V$ there is a tree $T\in\mathcal{T}$ that contains a $u-v$ path with a small stretch. If the trees $T_i$ are sub-graphs of $G$, the tree cover is called a spanning tree cover. If these trees are HSTs, it is called an HST cover.
  In a seminal work, Mendel and Naor [2006] showed that for any parameter $k=1,2,...$, there exists an HST cover, and a non-spanning tree cover, with stretch $O(k)$ and with $O(kn^{\frac{1}{k}})$ trees. Abraham et al. [2020] devised a spanning version of this result, albeit with stretch $O(k\log\log n)$. For graphs of small treewidth $t$, Gupta et al. [2004] devised an exact spanning tree cover with $O(t\log n)$ trees, and Chang et al. [2-23] devised a $(1+ε)$-approximate non-spanning tree cover with $2^{(t/ε)^{O(t)}}$ trees.
  We prove a smooth tradeoff between the stretch and the number of trees for graphs with balanced recursive separators of size at most $s(n)$ or treewidth at most $t(n)$. Specifically, for any $k=1,2,...$, we provide tree covers and HST covers with stretch $O(k)$ and $O\left(\frac{k^2\log n}{\log s(n)}\cdot s(n)^{\frac{1}{k}}\right)$ trees or $O(k\log n\cdot t(n)^{\frac{1}{k}})$ trees, respectively. We also devise spanning tree covers with these parameters and stretch $O(k\log\log n)$. In addition devise a spanning tree cover for general graphs with stretch $O(k\log\log n)$ and average overlap $O(n^{\frac{1}{k}})$.
  We use our tree covers to provide improved path-reporting spanners, emulators (including low-hop emulators, known also as low-hop metric spanners), distance labeling schemes and routing schemes.

</details>


### [43] [Coloring Reconfiguration under Color Swapping](https://arxiv.org/abs/2511.06473)
*Janosch Fuchs,Rin Saito,Tatsuhiro Suga,Takahiro Suzuki,Yuma Tamura*

Main category: cs.DS

TL;DR: 研究了图着色重构问题中的新规则"颜色交换"，建立了关于k值的复杂度二分法：k≤2时多项式时间可解，k≥3时PSPACE完全。在二分图、分裂图等受限图类上仍保持PSPACE完全性，但在路径、分裂图、补图等特定图类上有多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 引入新的颜色交换规则来研究图着色重构问题，该规则允许相邻顶点交换颜色，扩展了传统的单顶点重着色和Kempe链重着色规则的研究范围。

Method: 使用复杂度理论分析，通过归约证明PSPACE完全性，并为特定图类设计多项式时间算法。

Result: 建立了k值的复杂度二分：k≤2多项式时间可解，k≥3 PSPACE完全；在二分图、分裂图、有界度平面图上仍PSPACE完全；在路径(k=3)、分裂图(固定k)、补图(任意k)上有多项式时间算法。

Conclusion: 颜色交换规则下的着色重构问题具有丰富的复杂度结构，在一般图上困难但在特定图类上可高效求解。

Abstract: In the \textsc{Coloring Reconfiguration} problem, we are given two proper $k$-colorings of a graph and asked to decide whether one can be transformed into the other by repeatedly applying a specified recoloring rule, while maintaining a proper coloring throughout. For this problem, two recoloring rules have been widely studied: \emph{single-vertex recoloring} and \emph{Kempe chain recoloring}. In this paper, we introduce a new rule, called \emph{color swapping}, where two adjacent vertices may exchange their colors, so that the resulting coloring remains proper, and study the computational complexity of the problem under this rule. We first establish a complexity dichotomy with respect to $k$: the problem is solvable in polynomial time for $k \leq 2$, and is PSPACE-complete for $k \geq 3$. We further show that the problem remains PSPACE-complete even on restricted graph classes, including bipartite graphs, split graphs, and planar graphs of bounded degree. In contrast, we present polynomial-time algorithms for several graph classes: for paths when $k = 3$, for split graphs when $k$ is fixed, and for cographs when $k$ is arbitrary.

</details>


### [44] [UAIC_Twin_Width: An Exact yet Efficient Twin-Width Algorithm](https://arxiv.org/abs/2511.06486)
*Andrei Arhire,Matei Chiriac,Radu Timofte*

Main category: cs.DS

TL;DR: 本文提出了两种计算图twin-width的算法（精确和启发式），在PACE 2023挑战赛中表现优异，其中精确算法获得学生最佳成绩并总排名第四。


<details>
  <summary>Details</summary>
Motivation: twin-width是衡量图结构复杂度的新指标，需要为不同图结构建立统一基准并收集最优算法。PACE 2023挑战赛旨在满足这一需求。

Method: 开发了两种算法：一种精确计算twin-width的方法，一种启发式近似方法。两种方法都能高效处理任意结构的图。

Result: 在PACE 2023竞赛中表现突出，精确算法获得学生组最佳成绩，总排名第四。代码已公开以支持实际应用和进一步研究。

Conclusion: 提出的算法能有效计算图的twin-width，在竞赛中验证了其优越性能，为相关研究和应用提供了实用工具。

Abstract: Twin-width is a recently formulated graph and matrix invariant that intuitively quantifies how far a graph is from having the structural simplicity of a co-graph. Since its introduction in 2020, twin-width has received increasing attention and has driven research leading to notable advances in algorithmic fields, including graph theory and combinatorics. The 2023 edition of the Parameterized Algorithms and Computational Experiments (PACE) Challenge aimed to fulfill the need for a diverse and consistent public benchmark encompassing various graph structures, while also collecting state-of-the-art heuristic and exact approaches to the problem. In this paper, we propose two algorithms for efficiently computing the twin-width of graphs with arbitrary structures, comprising one exact and one heuristic approach. The proposed solutions performed strongly in the competition, with the exact algorithm achieving the best student result and ranking fourth overall. We release our source code publicly to enable practical applications of our work and support further research.

</details>


### [45] [Improved Approximation for Ranking on General Graphs](https://arxiv.org/abs/2511.06504)
*Mahsa Derakhshan,Mohammad Roghani,Mohammad Saneian,Tao Yu*

Main category: cs.DS

TL;DR: 本文改进了Ranking算法在一般图上的近似比，从0.526提升到0.5469，超越了之前已知的最佳结果。


<details>
  <summary>Details</summary>
Motivation: Ranking算法在二分图上的近似比已有深入研究(0.696-0.727)，但在一般图上的近似比仍不够明确，之前最佳下界仅为0.526。

Method: 基于标准的原始-对偶分析，引入顶点备份的新概念，通过分析顶点匹配的秩分布来消除限制先前工作的不良事件。

Result: 成功将Ranking算法在一般图上的近似比提升到0.5469，超过了Tang等人0.531的无意识匹配问题最佳结果。

Conclusion: 通过引入备份概念和新的结构性质分析，显著改进了Ranking算法在一般图上的性能保证。

Abstract: In this paper, we study Ranking, a well-known randomized greedy matching algorithm, for general graphs. The algorithm was originally introduced by Karp, Vazirani, and Vazirani [STOC 1990] for the online bipartite matching problem with one-sided vertex arrivals, where it achieves a tight approximation ratio of 1 - 1/e. It was later extended to general graphs by Goel and Tripathi [FOCS 2012]. The Ranking algorithm for general graphs is as follows: a permutation $σ$ over the vertices is chosen uniformly at random. The vertices are then processed sequentially according to this order, with each vertex being matched to the first available neighbor (if any) according to the same permutation $σ$.
  While the algorithm is quite well-understood for bipartite graphs-with the approximation ratio lying between 0.696 and 0.727, its approximation ratio for general graphs remains less well characterized despite extensive efforts. Prior to this work, the best known lower bound for general graphs was 0.526 by Chan et al. [TALG 2018], improving on the approximation ratio of 0.523 by Chan et al. [SICOMP 2018]. The upper bound, however, remains the same as that for bipartite graphs.
  In this work, we improve the approximation ratio of \textsc{Ranking} for general graphs to 0.5469, up from 0.526. This also surpasses the best-known approximation ratio of $0.531$ by Tang et al. [JACM 2023] for the oblivious matching problem. Our approach builds on the standard primal-dual analysis. The novelty of our work lies in proving new structural properties of Ranking by introducing the notion of the backup for vertices matched by the algorithm. For a fixed permutation, a vertex's backup is its potential match if its current match is removed. This concept helps characterize the rank distribution of the match of each vertex, enabling us to eliminate certain bad events that constrained previous work.

</details>


### [46] [The Harmonic Policy for Online Buffer Sharing is (2 + ln n)-Competitive: A Simple Proof](https://arxiv.org/abs/2511.06514)
*Vamsi Addanki,Julien Dallot,Leon Kellerhals,Maciej Pacut,Stefan Schmid*

Main category: cs.DS

TL;DR: 本文提出了一种简化的Harmonic策略，在保持(2+ln n)竞争比的同时，将每包阈值检查次数从n次减少到常数次，并提供了更简洁的证明方法。


<details>
  <summary>Details</summary>
Motivation: 原始Harmonic策略虽然理论性能优秀，但每包需要n次阈值检查，在实际网络交换机中成本过高。同时原始证明复杂冗长，需要简化。

Method: 提出简化的Harmonic策略实现，将阈值检查次数减少到常数级别；采用3分组方法重新证明竞争比。

Result: 简化后的Harmonic策略实现了与广泛部署的Dynamic Threshold策略相同的常数级别阈值检查，同时保持了(2+ln n)的竞争比。

Conclusion: 该工作使Harmonic策略在保持理论优势的同时具备了实际部署的可行性，为在线缓冲区共享问题提供了更实用的解决方案。

Abstract: The problem of online buffer sharing is expressed as follows. A switch with $n$ output ports receives a stream of incoming packets. When an incoming packet is accepted by the switch, it is stored in a shared buffer of capacity $B$ common to all packets and awaits its transmission through its corresponding output port determined by its destination. Each output port transmits one packet per time unit. The problem is to find an algorithm for the switch to accept or reject a packet upon its arrival in order to maximize the total number of transmitted packets.
  Building on the work of Kesselman et al. (STOC 2001) on split buffer sharing, Kesselman and Mansour (TCS 2004) considered the problem of online buffer sharing which models most deployed internet switches. In their work, they presented the Harmonic policy and proved that it is $(2 + \ln n)$-competitive, which is the best known competitive ratio for this problem. The Harmonic policy unfortunately saw less practical relevance as it performs $n$ threshold checks per packets which is deemed costly in practice, especially on network switches processing multiple terabits of packets per second. While the Harmonic policy is elegant, the original proof is also rather complex and involves a lengthy matching routine along with multiple intermediary results.
  This note presents a simplified Harmonic policy, both in terms of implementation and proof. First, we show that the Harmonic policy can be implemented with a constant number of threshold checks per packet, matching the widely deployed \emph{Dynamic Threshold} policy. Second, we present a simple proof that shows the Harmonic policy is $(2 + \ln n)$-competitive. In contrast to the original proof, the current proof is direct and relies on a 3-partitioning of the packets.

</details>


### [47] [Approximating the Average-Case Graph Search Problem with Non-Uniform Costs](https://arxiv.org/abs/2511.06564)
*Michał Szyfelbein*

Main category: cs.DS

TL;DR: 本文研究了带权顶点查询的图搜索问题，提出了针对一般图和树的近似算法。


<details>
  <summary>Details</summary>
Motivation: 经典二分搜索问题在图上的推广，考虑不同顶点查询成本和权重的实际情况，目标是找到最小化加权平均查询成本的策略。

Method: 建立搜索与顶点分离之间的联系，利用这一连接设计近似算法。

Result: 为一般图提供了O(√log n)近似算法，为树提供了(4+ε)近似算法。

Conclusion: 通过连接搜索与顶点分离，成功解决了带权顶点查询图搜索问题的近似算法设计。

Abstract: Consider the following generalization of the classic binary search problem: A searcher is required to find a hidden target vertex $x$ in a graph $G$. To do so, they iteratively perform queries to an oracle, each about a chosen vertex $v$. After each such call, the oracle responds whether the target was found and if not, the searcher receives as a reply the connected component in $G-v$ which contains $x$. Additionally, each vertex $v$ may have a different query cost $c(v)$ and a different weight $w(v)$. The goal is to find the optimal querying strategy which minimizes the weighted average-case cost required to find $x$. The problem is NP-hard even for uniform weights and query costs. Inspired by the progress on the edge query variant of the problem [SODA '17], we establish a connection between searching and vertex separation. By doing so, we provide an $O(\sqrt{\log n})$-approximation algorithm for general graphs and a $(4+ε)$-approximation algorithm for the case when the input is a tree.

</details>


### [48] [Improved Tree Sparsifiers in Near-Linear Time](https://arxiv.org/abs/2511.06574)
*Daniel Agassy,Dani Dorfman,Haim Kaplan*

Main category: cs.DS

TL;DR: 提出了一种近线性时间算法，为无向带容量图构建质量O(log²n log log n)的树割稀疏化器，这几乎匹配了已知多项式时间构造的最佳质量O(log¹.⁵n log log n)。


<details>
  <summary>Details</summary>
Motivation: 改进树割稀疏化器的构造质量，使其更接近理论最优值，同时保持近线性时间复杂性。

Method: 基于扩展器分解算法，构建改进的树割稀疏化框架，并引入近线性时间细化阶段来控制子簇边界边的负载。

Result: 实现了质量O(log²n log log n)的树割稀疏化器，比之前RST构造的O(log⁴n)质量有显著改进。

Conclusion: 该算法在近线性时间内实现了接近最优的树割稀疏化器质量，为树流稀疏化提供了更好的基础。

Abstract: A \emph{tree cut-sparsifier} $T$ of quality $α$ of a graph $G$ is a single tree that preserves the capacities of all cuts in the graph up to a factor of $α$. A \emph{tree flow-sparsifier} $T$ of quality $α$ guarantees that every demand that can be routed in $T$ can also be routed in $G$ with congestion at most $α$.
  We present a near-linear time algorithm that, for any undirected capacitated graph $G=(V,E,c)$, constructs a tree cut-sparsifier $T$ of quality $O(\log^{2} n \log\log n)$, where $n=|V|$. This nearly matches the quality of the best known polynomial construction of a tree cut-sparsifier, of quality $O(\log^{1.5} n \log\log n)$ [Räcke and Shah, ESA~2014]. By the flow-cut gap, our result yields a tree flow-sparsifier (and congestion-approximator) of quality $O(\log^{3} n \log\log n)$. This improves on the celebrated result of [Räcke, Shah, and Täubig, SODA~2014] (RST) that gave a near-linear time construction of a tree flow-sparsifier of quality $O(\log^{4} n)$.
  Our algorithm builds on a recent \emph{expander decomposition} algorithm by [Agassy, Dorfman, and Kaplan, ICALP~2023], which we use as a black box to obtain a clean and modular foundation for tree cut-sparsifiers. This yields an improved and simplified version of the RST construction for cut-sparsifiers with quality $O(\log^{3} n)$. We then introduce a near-linear time \emph{refinement phase} that controls the load accumulated on boundary edges of the sub-clusters across the levels of the tree. Combining the improved framework with this refinement phase leads to our final $O(\log^{2} n \log\log n)$ tree cut-sparsifier.

</details>


### [49] [Acceleration for Distributed Transshipment and Parallel Maximum Flow](https://arxiv.org/abs/2511.06581)
*Christoph Grunau,Rasmus Kyng,Goran Zuzic*

Main category: cs.DS

TL;DR: 本文提出了并行算法，以$\tilde{O}(1/\varepsilon)$深度和$\tilde{O}(m/\varepsilon)$工作量解决$(1+\varepsilon)$-转运问题和$(1+\varepsilon)$-最大流问题，通过开发并行线性成本近似器并结合加速连续优化框架实现。


<details>
  <summary>Details</summary>
Motivation: 结合多项最新进展，开发高效的并行算法来解决转运和最大流问题，特别是在分布式环境中实现确定性和高效的近似解。

Method: 开发并行线性成本近似器，结合Jambulapati等人提出的box-simplex游戏加速连续优化框架。对于最大流问题，增强Agarwal等人的线性成本近似器；对于转运问题，构建确定性和分布式近似器。

Result: 实现了$\tilde{O}(1/\varepsilon)$深度和$\tilde{O}(m/\varepsilon)$工作量的并行算法；在CONGEST模型中，一般网络需要$\tilde{O}(\varepsilon^{-1}(D + \sqrt{n}))$轮，minor-free网络需要$\tilde{O}(\varepsilon^{-1}D)$轮来计算$(1+\varepsilon)$-近似解。

Conclusion: 通过开发新的并行线性成本近似器和利用加速优化框架，成功实现了高效的并行算法来解决转运和最大流问题，在分布式环境中取得了显著的性能提升。

Abstract: We combine several recent advancements to solve $(1+\varepsilon)$-transshipment and $(1+\varepsilon)$-maximum flow with a parallel algorithm with $\tilde{O}(1/\varepsilon)$ depth and $\tilde{O}(m/\varepsilon)$ work. We achieve this by developing and deploying suitable parallel linear cost approximators in conjunction with an accelerated continuous optimization framework known as the box-simplex game by Jambulapati et al. (ICALP 2022). A linear cost approximator is a linear operator that allows us to efficiently estimate the cost of the optimal solution to a given routing problem. Obtaining accelerated $\varepsilon$ dependencies for both problems requires developing a stronger multicommodity cost approximator, one where cancellations between different commodities are disallowed. For maximum flow, we observe that a recent linear cost approximator due to Agarwal et al. (SODA 2024) can be augmented with additional parallel operations and achieve $\varepsilon^{-1}$ dependency via the box-simplex game.
  For transshipment, we also obtain construct a deterministic and distributed approximator. This yields a deterministic CONGEST algorithm that requires $\tilde{O}(\varepsilon^{-1}(D + \sqrt{n}))$ rounds on general networks of hop diameter $D$ and $\tilde{O}(\varepsilon^{-1}D)$ rounds on minor-free networks to compute a $(1+\varepsilon)$-approximation.

</details>


### [50] [Revisiting Chazelle's Implementation of the Bottom-Left Heuristic: A Corrected and Rigorous Analysis](https://arxiv.org/abs/2511.07008)
*Stefan Michel*

Main category: cs.DS

TL;DR: 本文对Chazelle提出的Bottom-Left启发式算法实现进行了严格修正和验证，解决了原实现中的形式细节缺失和运行时分析缺陷，确认了其二次时间复杂度。


<details>
  <summary>Details</summary>
Motivation: Chazelle在1983年提出的Bottom-Left启发式算法实现虽然被广泛使用，但存在形式细节描述不完整的问题，且原始运行时分析存在关键缺陷，在某些情况下会导致立方级运行时间。

Method: 对Chazelle的原始实现进行严格修正，填补形式细节的缺失，解决运行时分析中的缺陷，提供经过形式验证的版本。

Result: 得到了一个经过严格验证的Chazelle算法实现版本，确认了其O(n²)的时间复杂度和O(n)的空间复杂度。

Conclusion: 本文提供了Bottom-Left启发式算法的正确实现和严格分析，解决了原实现中的问题，为实际应用提供了可靠的理论基础。

Abstract: The Strip Packing Problem is a classical optimization problem in which a given set of rectangles must be packed, without overlap, into a strip of fixed width and infinite height, while minimizing the total height of the packing. A straightforward and widely studied approach to this problem is the Bottom-Left Heuristic. It consists of iteratively placing each rectangle in the given order at the lowest feasible position in the strip and, in case of ties, at the leftmost of those. Due to its simplicity and good empirical performance, this heuristic is widely used in practical applications. The most efficient implementation of this heuristic was proposed by Chazelle in 1983, requiring $O(n^2)$ time and $O(n)$ space to place $n$ rectangles. However, although Chazelle's original description was largely correct, it omitted several formal details. Furthermore, our analysis revealed a critical flaw in the original runtime analysis, which, in certain cases, results in $Ω(n^3)$ running time. Motivated by this finding, this paper provides a rigorous and corrected presentation of the implementation, addressing the imprecise arguments and resolving the identified flaw. The resulting analysis establishes a formally verified version of Chazelle's implementation and confirms its quadratic time complexity.

</details>


### [51] [Polynomial-time algorithms for PATH COVER and PATH PARTITION on trees and graphs of bounded treewidth](https://arxiv.org/abs/2511.07160)
*Florent Foucaud,Atrayee Majumder,Tobias Mömke,Aida Roshany-Tabrizi*

Main category: cs.DS

TL;DR: 本文研究了PATH COVER问题，设计了树上的线性时间算法和树宽有界图上的多项式时间动态规划算法，并扩展到PATH PARTITION问题。


<details>
  <summary>Details</summary>
Motivation: PATH COVER问题（用最少数量的路径覆盖图的所有顶点）与PATH PARTITION问题（路径需顶点不相交）相比研究较少，本文旨在填补这一研究空白。

Method: 使用动态规划方法，针对树宽有界图设计了XP时间算法，对于PATH PARTITION问题还应用了Cut&Count技术获得更优的时间复杂度。

Result: 在树上实现了线性时间算法；在树宽为t的图上，PATH COVER可在n^{t^{O(t)}}时间内求解；PATH PARTITION可在2^{O(t)}n随机时间内求解。

Conclusion: 本文为PATH COVER问题提供了高效的算法解决方案，特别是在树和树宽有界图上，并展示了这些方法同样适用于诱导路径和边不相交路径的变体。

Abstract: In the PATH COVER problem, one asks to cover the vertices of a graph using the smallest possible number of (not necessarily disjoint) paths. While the variant where the paths need to be pairwise vertex-disjoint, which we call PATH PARTITION, is extensively studied, surprisingly little is known about PATH COVER. We start filling this gap by designing a linear-time algorithm for PATH COVER on trees.
  We show that PATH COVER can be solved in polynomial time on graphs of bounded treewidth using a dynamic programming scheme. It runs in XP time $n^{t^{O(t)}}$ (where $n$ is the number of vertices and $t$ the treewidth of the input graph) or $κ^{t^{O(t)}}n$ if there is an upper-bound $κ$ on the solution size. A similar algorithm gives an FPT $2^{O(t\log t)}n$ algorithm for PATH PARTITION, which can be improved to (randomized) $2^{O(t)}n$ using the Cut\&Count technique. These results also apply to the variants where the paths are required to be induced (i.e. chordless) and/or edge-disjoint.

</details>


### [52] [A Fully Polynomial-Time Algorithm for Robustly Learning Halfspaces over the Hypercube](https://arxiv.org/abs/2511.07244)
*Gautam Chandrasekaran,Adam R. Klivans,Konstantinos Stavropoulos,Arsen Vasilyan*

Main category: cs.DS

TL;DR: 本文提出了首个在超立方体均匀分布下处理污染数据的多项式时间半空间学习算法，能在对抗性污染下实现η^O(1)+ε的误差保证


<details>
  <summary>Details</summary>
Motivation: 解决在存在对抗性污染（部分样本和标签被任意篡改）的情况下学习半空间的问题，此前的方法要么对1/ε有超多项式依赖，要么只能在连续分布下工作

Method: 开发了新的广义线性模型学习算法，仅对激活函数的Lipschitz常数有多对数依赖，避免了传统方法对连续分布结构特性的依赖

Result: 实现了η^O(1)+ε的误差保证，其中η是噪声率，这是即使在仅标签被污染的非确定设置下也未知的结果

Conclusion: 表明相对于离散分布的监督学习并不像之前认为的那样困难，为处理污染数据提供了新的理论框架

Abstract: We give the first fully polynomial-time algorithm for learning halfspaces with respect to the uniform distribution on the hypercube in the presence of contamination, where an adversary may corrupt some fraction of examples and labels arbitrarily. We achieve an error guarantee of $η^{O(1)}+ε$ where $η$ is the noise rate. Such a result was not known even in the agnostic setting, where only labels can be adversarially corrupted. All prior work over the last two decades has a superpolynomial dependence in $1/ε$ or succeeds only with respect to continuous marginals (such as log-concave densities).
  Previous analyses rely heavily on various structural properties of continuous distributions such as anti-concentration. Our approach avoids these requirements and makes use of a new algorithm for learning Generalized Linear Models (GLMs) with only a polylogarithmic dependence on the activation function's Lipschitz constant. More generally, our framework shows that supervised learning with respect to discrete distributions is not as difficult as previously thought.

</details>


### [53] [A Learning Perspective on Random-Order Covering Problems](https://arxiv.org/abs/2511.07283)
*Anupam Gupta,Marco Molinaro,Matteo Russo*

Main category: cs.DS

TL;DR: 该论文建立了随机顺序在线集合覆盖问题与随机镜像下降/在线凸优化之间的具体联系，将后者的遗憾界限转化为前者的竞争比，并扩展到覆盖整数规划、集合多重覆盖和非度量设施选址问题。


<details>
  <summary>Details</summary>
Motivation: 解决随机顺序模型是否能规避对抗顺序模型下O(log m log n)竞争比限制的问题，并建立随机顺序集合覆盖与在线学习理论之间的系统联系。

Method: 通过建立随机顺序集合覆盖与随机镜像下降/在线凸优化的具体连接，将后者的加性/乘性遗憾界限转化为前者的竞争比，提供了一种清晰的转换方法。

Result: 实现了与之前LearnOrCover框架相同的应用效果，但提供了更简单的证明，并能扩展到覆盖整数规划、集合多重覆盖和非度量设施选址问题。

Conclusion: 该工作为随机顺序在线覆盖问题提供了一个统一的框架，通过连接在线学习理论简化了分析过程，并扩展了应用范围。

Abstract: In the random-order online set cover problem, the instance with $m$ sets and $n$ elements is chosen in a worst-case fashion, but then the elements arrive in a uniformly random order. Can this random-order model allow us to circumvent the bound of $O(\log m \log n)$-competitiveness for the adversarial arrival order model? This long-standing question was recently resolved by Gupta et al. (2021), who gave an algorithm that achieved an $O(\log mn)$-competitive ratio. While their LearnOrCover was inspired by ideas in online learning (and specifically the multiplicative weights update method), the analysis proceeded by showing progress from first principles.
  In this work, we show a concrete connection between random-order set cover and stochastic mirror-descent/online convex optimization. In particular, we show how additive/multiplicative regret bounds for the latter translate into competitiveness for the former. Indeed, we give a clean recipe for this translation, allowing us to extend our results to covering integer programs, set multicover, and non-metric facility location in the random order model, matching (and giving simpler proofs of) the previous applications of the LearnOrCover framework.

</details>


### [54] [Dynamic Set Cover with Worst-Case Recourse](https://arxiv.org/abs/2511.07354)
*Shay Solomon,Amitai Uzrad*

Main category: cs.DS

TL;DR: 本文提出了一种转换方法，能将任意集合覆盖算法的近似比和更新时间转换为具有有界最坏情况补救的算法，近似比为(2+ε)α，更新时间为O(T+αC)，最坏情况补救为O(αC)。


<details>
  <summary>Details</summary>
Motivation: 现有动态集合覆盖算法要么具有低更新时间但无有界最坏情况补救，要么具有有界补救但更新时间高。本文旨在同时实现低更新时间和有界最坏情况补救。

Method: 提出一种黑盒转换方法，将任意集合覆盖算法作为输入，输出具有有界最坏情况补救的新算法。该方法通过维护两个解并交替使用来实现补救控制。

Result: 对于常数C，转换后的算法在近似比、更新时间和补救之间实现了平衡，填补了动态集合覆盖算法在有界最坏情况补救方面的空白。

Conclusion: 该转换方法为动态集合覆盖问题提供了第一个同时具有低更新时间和非平凡最坏情况补救的算法，解决了该领域长期存在的问题。

Abstract: In the dynamic set cover (SC) problem, the input is a dynamic universe of at most $n$ elements and a fixed collection of $m$ sets, where each element belongs to at most $f$ sets and each set has cost in $[1/C, 1]$. The objective is to efficiently maintain an approximate minimum SC under element updates; efficiency is primarily measured by the update time, but another important parameter is the recourse (number of changes to the solution per update). Ideally, one would like to achieve low worst-case bounds on both update time and recourse.
  One can achieve approximation $(1+ε)\ln n$ (greedy-based) or $(1+ε)f$ (primal-dual-based) with worst-case update time $O(f\log n)$ (ignoring $ε$ dependencies). However, despite a large body of work, no algorithm with low update time (even amortized) and nontrivial worst-case recourse is known, even for unweighted instances ($C = 1$)!
  We remedy this by providing a transformation that, given as a black-box a SC algorithm with approximation $α$ and update time $T$, returns a set cover algorithm with approximation $(2 + ε)α$, update time $O(T + αC)$, and worst-case recourse $O(αC)$. Our main results are obtained by leveraging this transformation for constant $C$:...

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [55] [A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs](https://arxiv.org/abs/2511.06455)
*Milena Trajanoska,Riste Stojanov,Dimitar Trajanov*

Main category: cs.DB

TL;DR: 提出了一种使用知识图谱和大型语言模型集成多个数据库的新方法，通过将数据库表列映射到Schema.org词汇来实现数据互操作性。


<details>
  <summary>Details</summary>
Motivation: 企业通常维护多个孤立的数据库系统，导致数据效率低下和互操作性挑战，需要集成不同数据源以充分发挥数据潜力。

Method: 在关系型数据库表上构建语义层，使用多个LLM代理将表和列映射到Schema.org术语，利用现有词汇表连接结构化数据。

Result: 该方法在多个领域实现了超过90%的映射准确率。

Conclusion: 基于知识图谱和LLM的语义代理方法能够有效解决数据库集成问题，提高数据互操作性。

Abstract: Enterprises often maintain multiple databases for storing critical business data in siloed systems, resulting in inefficiencies and challenges with data interoperability. A key to overcoming these challenges lies in integrating disparate data sources, enabling businesses to unlock the full potential of their data. Our work presents a novel approach for integrating multiple databases using knowledge graphs, focusing on the application of large language models as semantic agents for mapping and connecting structured data across systems by leveraging existing vocabularies. The proposed methodology introduces a semantic layer above tables in relational databases, utilizing a system comprising multiple LLM agents that map tables and columns to Schema.org terms. Our approach achieves a mapping accuracy of over 90% in multiple domains.

</details>


### [56] [OntoTune: Ontology-Driven Learning for Query Optimization with Convolutional Models](https://arxiv.org/abs/2511.06780)
*Songhui Yue,Yang Shao,Sean Hayes*

Main category: cs.DB

TL;DR: OntoTune是一个基于本体的平台，通过连接SQL查询、数据库元数据和统计信息来增强查询优化的学习效果，在保持关系和关键信息的同时嵌入本体，并应用于树基和图基卷积网络等学习算法。


<details>
  <summary>Details</summary>
Motivation: 本体作为结构化、信息丰富的知识表示，可以为学习问题提供上下文，特别是在查询优化领域。

Method: 开发了一个连接SQL查询、数据库元数据和统计信息的本体，并开发了一种在保持关系和关键信息的同时嵌入本体的方法，然后将其输入到树基和图基卷积网络等学习算法中。

Result: 案例研究表明，与数据库系统默认查询执行相比，OntoTune的本体驱动学习能够带来性能提升。

Conclusion: OntoTune基于本体的平台在查询优化中显示出潜力，能够捕获查询性能的关系和重要决定因素，并通过本体驱动学习实现性能增益。

Abstract: Query optimization has been studied using machine learning, reinforcement learning, and, more recently, graph-based convolutional networks. Ontology, as a structured, information-rich knowledge representation, can provide context, particularly in learning problems. This paper presents OntoTune, an ontology-based platform for enhancing learning for query optimization. By connecting SQL queries, database metadata, and statistics, the ontology developed in this research is promising in capturing relationships and important determinants of query performance. This research also develops a method to embed ontologies while preserving as much of the relationships and key information as possible, before feeding it into learning algorithms such as tree-based and graph-based convolutional networks. A case study shows how OntoTune's ontology-driven learning delivers performance gains compared with database system default query execution.

</details>


### [57] [Trading Vector Data in Vector Databases](https://arxiv.org/abs/2511.07139)
*Jin Cheng,Xiangxiang Dai,Ningning Ding,John C. S. Lui,Jianwei Huang*

Main category: cs.DB

TL;DR: 提出了一种用于向量数据交易的分层多臂老虎机框架，联合优化检索配置和定价策略，解决在线学习环境下的不确定成本和随机反馈问题。


<details>
  <summary>Details</summary>
Motivation: 向量数据交易对于跨领域学习至关重要，但目前研究较少。卖家面临不确定的检索成本，买家对定价提供随机反馈，且配置和定价决策存在固有耦合关系。

Method: 采用分层老虎机框架：第一阶段使用基于置信度的上下文聚类学习有效配置；第二阶段采用基于区间的价格选择与局部泰勒近似来估计买家响应。

Result: 在四个真实数据集上验证，相比现有方法在累积奖励和遗憾减少方面表现一致更好，具有多项式时间复杂度。

Conclusion: 该框架能够有效解决向量数据交易中的配置学习和定价学习挑战，提供理论保证并在实际数据上表现优异。

Abstract: Vector data trading is essential for cross-domain learning with vector databases, yet it remains largely unexplored. We study this problem under online learning, where sellers face uncertain retrieval costs and buyers provide stochastic feedback to posted prices. Three main challenges arise: (1) heterogeneous and partial feedback in configuration learning, (2) variable and complex feedback in pricing learning, and (3) inherent coupling between configuration and pricing decisions.
  We propose a hierarchical bandit framework that jointly optimizes retrieval configurations and pricing. Stage I employs contextual clustering with confidence-based exploration to learn effective configurations with logarithmic regret. Stage II adopts interval-based price selection with local Taylor approximation to estimate buyer responses and achieve sublinear regret. We establish theoretical guarantees with polynomial time complexity and validate the framework on four real-world datasets, demonstrating consistent improvements in cumulative reward and regret reduction compared with existing methods.

</details>


### [58] [MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces](https://arxiv.org/abs/2511.06179)
*Joel Ward*

Main category: cs.DB

TL;DR: MemoriesDB是一个统一的数据架构，旨在避免长期计算记忆中时间、意义和关系的去相干。它将每个记忆构建为时间-语义-关系实体，结合了时间序列数据库、向量数据库和图系统的特性。


<details>
  <summary>Details</summary>
Motivation: 解决长期计算记忆中时间、意义和关系三个维度的去相干问题，实现跨时间的一致性记忆表示。

Method: 基于PostgreSQL和pgvector扩展，采用仅追加模式。每个记忆表示为带微秒时间戳的顶点，包含低维和高维归一化嵌入来捕获语义上下文。记忆之间通过带标签和元数据的有向边形成关系。

Result: 原型展示了可扩展的回忆和上下文增强能力，支持高效的时间有界检索、混合语义搜索和轻量级结构推理。

Conclusion: MemoriesDB提供了一个统一的记忆表示框架，支持跨时间的一致性记忆管理，并讨论了向列式后端、分布式聚类和涌现主题建模的扩展方向。

Abstract: We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [59] [The Complexity of Stackelberg Pricing Games](https://arxiv.org/abs/2511.05700)
*Christoph Grüne,Dorothee Henke,Eva Rotenberg,Lasse Wulf*

Main category: cs.GT

TL;DR: 本文证明了Stackelberg定价博弈（领导者设定价格，追随者解决组合优化问题）对于50多个NP完全问题（包括TSP、顶点覆盖、团、子集和等）是Σ^p_2-完全的，解决了之前关于该问题是否属于多项式层次结构第二层的猜想。


<details>
  <summary>Details</summary>
Motivation: 解决Pferschy等人和Böhnlein等人提出的关于Stackelberg定价博弈是否属于Σ^p_2-完全类的猜想，并扩展这一结果到更广泛的NP完全问题类别。

Method: 通过理论证明和复杂性分析，展示Stackelberg定价博弈在多项式层次结构中的位置，特别关注追随者解决各种NP完全问题的情况。

Result: 证明了Stackelberg定价博弈对于超过50个NP完全问题是Σ^p_2-完全的，这包括旅行商问题、顶点覆盖、团问题、子集和等经典问题。

Conclusion: 该研究确立了Stackelberg定价博弈在多项式层次结构第二层的完全性地位，与Grüne和Wulf关于多项式层次结构中更高复杂性的元定理相一致。

Abstract: We consider Stackelberg pricing games, which are also known as bilevel pricing problems, or combinatorial price-setting problems. This family of problems consists of games between two players: the leader and the follower. There is a market that is partitioned into two parts: the part of the leader and the part of the leader's competitors. The leader controls one part of the market and can freely set the prices for products. By contrast, the prices of the competitors' products are fixed and known in advance. The follower, then, needs to solve a combinatorial optimization problem in order to satisfy their own demands, while comparing the leader's offers to the offers of the competitors. Therefore, the leader has to hit the intricate balance of making an attractive offer to the follower, while at the same time ensuring that their own profit is maximized.
  Pferschy, Nicosia, Pacifici, and Schauer considered the Stackelberg pricing game where the follower solves a knapsack problem. They raised the question whether this problem is complete for the second level of the polynomial hierarchy, i.e., $Σ^p_2$-complete. The same conjecture was also made by Böhnlein, Schaudt, and Schauer. In this paper, we positively settle this conjecture. Moreover, we show that this result holds actually in a much broader context: The Stackelberg pricing game is $Σ^p_2$-complete for over 50 NP-complete problems, including most classics such as TSP, vertex cover, clique, subset sum, etc. This result falls in line of recent meta-theorems about higher complexity in the polynomial hierarchy by Grüne and Wulf.

</details>


### [60] [Sequential Causal Normal Form Games: Theory, Computation, and Strategic Signaling](https://arxiv.org/abs/2511.06934)
*Dennis Thumm*

Main category: cs.GT

TL;DR: 将因果博弈扩展到顺序设置的研究发现，基于理性选择的经典博弈论框架与因果推理优势存在根本性不兼容，因果均衡无法提供比经典均衡更好的福利改进。


<details>
  <summary>Details</summary>
Motivation: 研究经典博弈论框架是否能扩展到捕捉AI智能体的有限理性和因果推理能力，特别是在顺序交互场景中。

Method: 引入顺序因果多智能体系统(S-CMAS)，将Pearl因果层次结构整合到领导者-跟随者交互中，通过理论证明和实证分析（50+蒙特卡洛模拟和手工构建的合成示例）进行验证。

Result: 实证研究表明S-CNE在所有测试场景中相比经典Stackelberg均衡提供零福利改进，即使放松理性假设也无法产生战略优势。

Conclusion: 基于理性选择的经典博弈论扩展与因果推理优势根本不相容，需要超越标准纳什均衡的新理论框架来建模AI智能体。

Abstract: Can classical game-theoretic frameworks be extended to capture the bounded rationality and causal reasoning of AI agents? We investigate this question by extending Causal Normal Form Games (CNFGs) to sequential settings, introducing Sequential Causal Multi-Agent Systems (S-CMAS) that incorporate Pearl's Causal Hierarchy across leader-follower interactions. While theoretically elegant -- we prove PSPACE-completeness, develop equilibrium refinements, and establish connections to signaling theory -- our comprehensive empirical investigation reveals a critical limitation: S-CNE provides zero welfare improvement over classical Stackelberg equilibrium across all tested scenarios. Through 50+ Monte Carlo simulations and hand-crafted synthetic examples, we demonstrate that backward induction with rational best-response eliminates any strategic advantage from causal layer distinctions. We construct a theoretical example illustrating conditions where benefits could emerge ($ε$-rational satisficing followers), though implementation confirms that even relaxed rationality assumptions prove insufficient when good instincts align with optimal play. This negative result provides valuable insight: classical game-theoretic extensions grounded in rational choice are fundamentally incompatible with causal reasoning advantages, motivating new theoretical frameworks beyond standard Nash equilibrium for agentic AI.

</details>


### [61] [Fair Societies: Algorithms for House Allocations](https://arxiv.org/abs/2511.07022)
*Hadi Hosseini,Sanjukta Roy,Aditi Sethia*

Main category: cs.GT

TL;DR: 本文研究房屋分配问题中的公平性算法，提出了两种处理计算复杂性的方法：在初始分配基础上通过有限重分配减少嫉妒代理数量，以及在单峰偏好域中寻找最小化嫉妒代理数量的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 房屋分配涉及单边偏好的匹配问题，代表如器官、课程座位、公共住房等不可分割资源的分配。最小化嫉妒代理数量已知是NP完全问题，需要寻找可行的计算方法来确保公平性。

Method: 1. 在初始分配基础上，通过重分配有限数量的房屋来减少嫉妒代理数量；2. 针对单峰偏好域，开发多项式时间算法寻找最小化嫉妒代理数量的分配，并可扩展至满足帕累托效率。

Result: 提出了高效算法：当代理表达有限房屋偏好时，可有效减少嫉妒代理数量；在单峰偏好域中，可在多项式时间内找到最小化嫉妒代理数量的分配。算法还可扩展到其他嫉妒度量如总嫉妒或最大嫉妒。

Conclusion: 本文提供了处理房屋分配公平性计算复杂性的实用方法，通过有限重分配和利用特殊偏好结构，实现了公平性与福利之间的权衡，并通过实证分析验证了算法的有效性。

Abstract: House Allocations concern with matchings involving one-sided preferences, where houses serve as a proxy encoding valuable indivisible resources (e.g. organs, course seats, subsidized public housing units) to be allocated among the agents. Every agent must receive exactly one resource. We study algorithmic approaches towards ensuring fairness in such settings. Minimizing the number of envious agents is known to be NP-complete (Kamiyama et al. 2021). We present two tractable approaches to deal with the computational hardness. When the agents are presented with an initial allocation of houses, we aim to refine this allocation by reallocating a bounded number of houses to reduce the number of envious agents. We show an efficient algorithm when the agents express preference for a bounded number of houses. Next, we consider single peaked preference domain and present a polynomial time algorithm for finding an allocation that minimize the number of envious agents. We further extend it to satisfy Pareto efficiency. Our former algorithm works for other measures of envy such as total envy, or maximum envy, with suitable modifications. Finally, we present an empirical analysis recording the fairness-welfare trade-off of our algorithms.

</details>


### [62] [The Landscape of Almost Equitable Allocations](https://arxiv.org/abs/2511.07395)
*Hadi Hosseini,Vishwa Prakash HV,Aditi Sethia,Jatin Yadav*

Main category: cs.GT

TL;DR: 本文研究了公平分配中的EQ1（equitability up to one item）概念，在代理人对总物品束价值符号一致的温和假设下，证明了EQ1分配的存在性并提供了高效算法。


<details>
  <summary>Details</summary>
Motivation: EQ1是公平分配中公平性的重要松弛概念，但在非单调估值下可能不存在。本文旨在探索在温和假设下EQ1分配的存在性和可计算性。

Method: 采用理论证明和算法设计的方法，针对不同估值类别（双单调、子模/超模、非负/非正估值）分别研究EQ1分配的存在性和计算复杂性。

Result: 证明了在代理人同意总物品束价值符号的假设下，对于两个代理人的一般估值存在EQ1分配；对于更多代理人，在双单调、子模/超模等特定估值类别下也存在EQ1分配。

Conclusion: EQ1分配在温和假设下对多种估值类别存在且可高效计算，但对于一般估值和超过两个代理人的情况，EQ1分配可能不存在且判定其存在性是计算难解的。

Abstract: Equitability is a fundamental notion in fair division which requires that all agents derive equal value from their allocated bundles. We study, for general (possibly non-monotone) valuations, a popular relaxation of equitability known as equitability up to one item (EQ1). An EQ1 allocation may fail to exist even with additive non-monotone valuations; for instance, when there are two agents, one valuing every item positively and the other negatively. This motivates a mild and natural assumption: all agents agree on the sign of their value for the grand bundle. Under this assumption, we prove the existence and provide an efficient algorithm for computing EQ1 allocations for two agents with general valuations. When there are more than two agents, we show the existence and polynomial-time computability of EQ1 allocations for valuation classes beyond additivity and monotonicity, in particular for (1) doubly monotone valuations and (2) submodular (resp. supermodular) valuations where the value for the grand bundle is nonnegative (resp. nonpositive) for all agents. Furthermore, we settle an open question of Bil`o et al. by showing that an EQ1 allocation always exists for nonnegative(resp. nonpositive) valuations, i.e., when every agent values each subset of items nonnegatively (resp. nonpositively). Finally, we complete the picture by showing that for general valuations with more than two agents, EQ1 allocations may not exist even when agents agree on the sign of the grand bundle, and that deciding the existence of an EQ1 allocation is computationally intractable.

</details>
