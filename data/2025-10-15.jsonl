{"id": "2510.12265", "categories": ["cs.MM", "cs.AI", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12265", "abs": "https://arxiv.org/abs/2510.12265", "authors": ["Sami Khairy", "Gabriel Mittag", "Vishak Gopal", "Ross Cutler"], "title": "Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication", "comment": "Accepted for publication in the proceedings of the AAAI Conference on\n  Artificial Intelligence 2026 (IAAI Technical Track on Deployed Highly\n  Innovative Applications of AI)", "summary": "The quality of experience (QoE) delivered by video conferencing systems is\nsignificantly influenced by accurately estimating the time-varying available\nbandwidth between the sender and receiver. Bandwidth estimation for real-time\ncommunications remains an open challenge due to rapidly evolving network\narchitectures, increasingly complex protocol stacks, and the difficulty of\ndefining QoE metrics that reliably improve user experience. In this work, we\npropose a deployed, human-in-the-loop, data-driven framework for bandwidth\nestimation to address these challenges. Our approach begins with training\nobjective QoE reward models derived from subjective user evaluations to measure\naudio and video quality in real-time video conferencing systems. Subsequently,\nwe collect roughly $1$M network traces with objective QoE rewards from\nreal-world Microsoft Teams calls to curate a bandwidth estimation training\ndataset. We then introduce a novel distributional offline reinforcement\nlearning (RL) algorithm to train a neural-network-based bandwidth estimator\naimed at improving QoE for users. Our real-world A/B test demonstrates that the\nproposed approach reduces the subjective poor call ratio by $11.41\\%$ compared\nto the baseline bandwidth estimator. Furthermore, the proposed offline RL\nalgorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond\nbandwidth estimation."}
{"id": "2510.12445", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.12445", "abs": "https://arxiv.org/abs/2510.12445", "authors": ["Xiangyu Li", "Ran Su", "Liangliang Liu"], "title": "M3ST-DTI: A multi-task learning model for drug-target interactions based on multi-modal features and multi-stage alignment", "comment": null, "summary": "Accurate prediction of drug-target interactions (DTI) is pivotal in drug\ndiscovery. However, existing approaches often fail to capture deep intra-modal\nfeature interactions or achieve effective cross-modal alignment, limiting\npredictive performance and generalization. To address these challenges, we\npropose M3ST-DTI, a multi-task learning model that enables multi-stage\nintegration and alignment of multi modal features for DTI prediction. M3ST-DTI\nincorporates three types of features-textual, structural, and functional and\nenhances intra-modal representations using self-attention mechanisms and a\nhybrid pooling graph attention module. For early-stage feature alignment and\nfusion, the model in tegrates MCA with Gram loss as a structural constraint. In\nthe later stage, a BCA module captures fine-grained interactions between drugs\nand targets within each modality, while a deep orthogonal fusion module\nmitigates feature redundancy.Extensive evaluations on benchmark datasets\ndemonstrate that M3ST-DTI consistently outperforms state-of-the art methods\nacross diverse metrics"}
{"id": "2510.12050", "categories": ["cs.DS", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2510.12050", "abs": "https://arxiv.org/abs/2510.12050", "authors": ["Mohit Daga"], "title": "Thin Trees via $k$-Respecting Cut Identities", "comment": null, "summary": "Thin spanning trees lie at the intersection of graph theory, approximation\nalgorithms, and combinatorial optimization. They are central to the\nlong-standing \\emph{thin tree conjecture}, which asks whether every\n$k$-edge-connected graph contains an $O(1/k)$-thin tree, and they underpin\nalgorithmic breakthroughs such as the $O(\\log n/\\log\\log n)$-approximation for\nATSP. Yet even the basic algorithmic task of \\emph{verifying} that a given tree\nis thin has remained elusive: checking thinness requires reasoning about\nexponentially many cuts, and no efficient certificates have been known.\n  We introduce a new machinery of \\emph{$k$-respecting cut identities}, which\nexpress the weight of every cut that crosses a spanning tree in at most $k$\nedges as a simple function of pairwise ($2$-respecting) cuts. This yields a\ntree-local oracle that, after $O(n^2)$ preprocessing, evaluates such cuts in\n$O_k(1)$ time. Building on this oracle, we give the first procedure to compute\nthe exact $k$-thinness certificate $\\Theta_k(T)$ of any spanning tree for fixed\n$k$ in time $\\tilde O(n^2+n^k)$, outputting both the certificate value and a\nwitnessing cut.\n  Beyond general graphs, our framework yields sharper guarantees in structured\nsettings. In planar graphs, duality with cycles and dual girth imply that every\nspanning tree admits a verifiable certificate $\\Theta_k(T)\\le k/\\lambda$ (hence\n$O(1/\\lambda)$ for constant $k$). In graphs embedded on a surface of genus\n$\\gamma$, refined counting gives certified (per-cut) bounds $O((\\log\nn+\\gamma)/\\lambda)$ via the same ensemble coverage."}
{"id": "2510.12014", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12014", "abs": "https://arxiv.org/abs/2510.12014", "authors": ["Eric He", "Akash Gupta", "Adian Liusie", "Vatsal Raina", "Piotr Molenda", "Shirom Chabra", "Vyas Raina"], "title": "Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval", "comment": null, "summary": "Text--image retrieval is necessary for applications such as product\nrecommendation. Embedding-based approaches like CLIP enable efficient\nlarge-scale retrieval via vector similarity search, but they are primarily\ntrained on literal caption-like text--image pairs and often fail to capture\nabstract or persona-driven attributes common in product recommendation\napplications (e.g., ``a gift for a mother who loves gardening''). In contrast,\nstate-of-the-art vision--language models (vLLMs) can align text with images in\na flexible manner, but their limited context window prevents them from directly\nhandling retrieval over large catalogs. We propose a framework that distills\nthe preference rankings of a powerful vLLM into an embedding-based system,\ntransferring its nuanced alignment abilities while maintaining the\ninference-time scalability of an embedding-based approach. Experiments on\npersona-driven product recommendation tasks demonstrate that our method\nsignificantly outperforms existing embedding-based baselines, providing an\nefficient solution for personalized text--image retrieval."}
{"id": "2510.12065", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.12065", "abs": "https://arxiv.org/abs/2510.12065", "authors": ["Soma Furusawa", "Taisei Kato", "Ryo Hayakawa", "Kazunori Hayashi"], "title": "Approximate Proximal Operators for Analog Compressed Sensing Using PN-junction Diode", "comment": null, "summary": "In order to realize analog compressed sensing, the paper considers\napproximate proximal operators of the $\\ell_1$ and minimax concave penalty\n(MCP) regularization functions. Specifically, we propose to realize the\napproximate functions by an electric analog circuit using forward\nvoltage-current (V-I) characteristics of the PN-junction diodes. To confirm the\nvalidity of the proposed approach, we employ the proposed approximate proximal\noperators for the $\\ell_1$ and MCP regularization functions in compressed\nsensing with the proximal gradient method. The sparse reconstruction\nperformance of the algorithms using the proposed approximate proximal operators\nis demonstrated via computer simulations taking into account the impact of\nadditive noise introduced by analog devices."}
{"id": "2510.12642", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12642", "abs": "https://arxiv.org/abs/2510.12642", "authors": ["Meihui Zhang", "Liming Wang", "Chi Zhang", "Zhaojing Luo"], "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis", "comment": null, "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."}
{"id": "2510.11866", "categories": ["cs.GT", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11866", "abs": "https://arxiv.org/abs/2510.11866", "authors": ["Michael Crystal", "Guy Goren", "Scott Duke Kominers"], "title": "Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network", "comment": "23 pages, 1 figure", "summary": "Decentralized storage is one of the most natural applications built on\nblockchains and a central component of the Web3 ecosystem. Yet despite a decade\nof active development -- from IPFS and Filecoin to more recent entrants -- most\nof these storage protocols have received limited formal analysis of their\nincentive properties. Claims of incentive compatibility are sometimes made, but\nrarely proven. This gap matters: without well-designed incentives, a system may\ndistribute storage but fail to truly decentralize it.\n  We analyze Shelby -- a storage network protocol recently proposed by Aptos\nLabs and Jump Crypto -- and provide the first formal proof of its incentive\nproperties. Our game-theoretic model shows that while off-chain audits alone\ncollapse to universal shirking, Shelby's combination of peer audits with\noccasional on-chain verification yields incentive compatibility under natural\nparameter settings. We also examine coalition behavior and outline a simple\nmodification that strengthens the protocol's collusion-resilience."}
{"id": "2510.12232", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.12232", "abs": "https://arxiv.org/abs/2510.12232", "authors": ["Jonathan Dransfeld", "Marvin Künnemann", "Mirza Redzic", "Marcus Wunderlich"], "title": "Engineering Dominating Patterns: A Fine-grained Case Study", "comment": null, "summary": "The \\emph{Dominating $H$-Pattern} problem generalizes the classical\n$k$-Dominating Set problem: for a fixed \\emph{pattern} $H$ and a given graph\n$G$, the goal is to find an induced subgraph $S$ of $G$ such that (1) $S$ is\nisomorphic to $H$, and (2) $S$ forms a dominating set in $G$. Fine-grained\ncomplexity results show that on worst-case inputs, any significant improvement\nover the naive brute-force algorithm is unlikely, as this would refute the\nStrong Exponential Time Hypothesis. Nevertheless, a recent work by Dransfeld et\nal. (ESA 2025) reveals some significant improvement potential particularly in\n\\emph{sparse} graphs.\n  We ask: Can algorithms with conditionally almost-optimal worst-case\nperformance solve the Dominating $H$-Pattern, for selected patterns $H$,\nefficiently on practical inputs? We develop and experimentally evaluate several\napproaches on a large benchmark of diverse datasets, including baseline\napproaches using the Glasgow Subgraph Solver (GSS), the SAT solver Kissat, and\nthe ILP solver Gurobi.\n  Notably, while a straightforward implementation of the algorithms -- with\nconditionally close-to-optimal worst-case guarantee -- performs comparably to\nexisting solvers, we propose a tailored Branch-\\&-Bound approach --\nsupplemented with careful pruning techniques -- that achieves improvements of\nup to two orders of magnitude on our test instances."}
{"id": "2510.12054", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12054", "abs": "https://arxiv.org/abs/2510.12054", "authors": ["Wenjin Xie", "Tao Jia"], "title": "MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation", "comment": null, "summary": "With the rapid expansion of scientific literature, scholars increasingly\ndemand precise and high-quality paper recommendations. Among various\nrecommendation methodologies, graph-based approaches have garnered attention by\neffectively exploiting the structural characteristics inherent in scholarly\nnetworks. However, these methods often overlook the asymmetric academic\ninfluence that is prevalent in scholarly networks when learning graph\nrepresentations. To address this limitation, this study proposes the\nMutual-Influence-Aware Recommendation (MIARec) model, which employs a\ngravity-based approach to measure the mutual academic influence between\nscholars and incorporates this influence into the feature aggregation process\nduring message propagation in graph representation learning. Additionally, the\nmodel utilizes a multi-channel aggregation method to capture both individual\nembeddings of distinct single relational sub-networks and their interdependent\nembeddings, thereby enabling a more comprehensive understanding of the\nheterogeneous scholarly network. Extensive experiments conducted on real-world\ndatasets demonstrate that the MIARec model outperforms baseline models across\nthree primary evaluation metrics, indicating its effectiveness in scientific\npaper recommendation tasks."}
{"id": "2510.12078", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.12078", "abs": "https://arxiv.org/abs/2510.12078", "authors": ["Sijing Xie", "Dingzhu Wen", "Changsheng You", "Qimei Chen", "Mehdi Bennis", "Kaibin Huang"], "title": "FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning", "comment": null, "summary": "Fine-tuning (FT) large language models (LLMs) is crucial for adapting\ngeneral-purpose models to specific tasks, enhancing accuracy and relevance with\nminimal resources. To further enhance generalization ability while reducing\ntraining costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a\nnew framework that applies dropout to the rows and columns of the trainable\nmatrix in Federated LoRA. A generalization error bound and convergence analysis\nunder sparsity regularization are obtained, which elucidate the fundamental\ntrade-off between underfitting and overfitting. The error bound reveals that a\nhigher dropout rate increases model sparsity, thereby lowering the upper bound\nof pointwise hypothesis stability (PHS). While this reduces the gap between\nempirical and generalization errors, it also incurs a higher empirical error,\nwhich, together with the gap, determines the overall generalization error. On\nthe other hand, though dropout reduces communication costs, deploying FedLoDrop\nat the network edge still faces challenges due to limited network resources. To\naddress this issue, an optimization problem is formulated to minimize the upper\nbound of the generalization error, by jointly optimizing the dropout rate and\nresource allocation subject to the latency and per-device energy consumption\nconstraints. To solve this problem, a branch-and-bound (B\\&B)-based method is\nproposed to obtain its globally optimal solution. Moreover, to reduce the high\ncomputational complexity of the B\\&B-based method, a penalized successive\nconvex approximation (P-SCA)-based algorithm is proposed to efficiently obtain\nits high-quality suboptimal solution. Finally, numerical results demonstrate\nthe effectiveness of the proposed approach in mitigating overfitting and\nimproving the generalization capability."}
{"id": "2510.12158", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.12158", "abs": "https://arxiv.org/abs/2510.12158", "authors": ["Kevin Hsu"], "title": "Fair Division of Indivisible Items", "comment": "105 pages, PhD dissertation", "summary": "We study the fair division of indivisible items. In the general model, the\ngoal is to allocate $m$ indivisible items to $n$ agents while satisfying\nfairness criteria such as MMS, EF1, and EFX. We also study a\nrecently-introduced graphical model that represents the fair division problem\nas a multigraph, in which vertices correspond to agents and edges to items. The\ngraphical model stipulates that an item can have non-zero marginal utility to\nan agent only if its corresponding edge is incident to the agent's\ncorresponding vertex. We study orientations (allocations that allocate each\nedge to an endpoint) in this model, as they are particularly desirable.\n  Our first contribution concerns MMS allocations of mixed manna (i.e. a\nmixture of goods and chores) in the general model. It is known that MMS\nallocations of goods exist when $m \\leq n+5$. We generalize this and show that\nwhen $m \\leq n+5$, MMS allocations of mixed manna exist as long as $n \\leq 3$,\nthere is an agent whose MMS threshold is non-negative, or every item is a\nchore. Remarkably, our result leaves only the case where every agent has a\nnegative MMS threshold unanswered.\n  Our second contribution concerns EFX orientations of multigraphs of goods. We\nshow that deciding whether EFX orientations exist for multigraphs is\nNP-complete, even for symmetric bi-valued multigraphs. Complementarily, we show\nsymmetric bi-valued multigraphs that do not contain non-trivial odd multitrees\nhave EFX orientations that can be found in polynomial time.\n  Our third contribution concerns EF1 and EFX orientations of graphs and\nmultigraphs of chores. We obtain polynomial-time algorithms for deciding\nwhether such graphs have EF1 and EFX orientations, resolving a previous\nconjecture and showing a fundamental difference between goods and chores\ndivision. In addition, we show that the analogous problems for multigraphs are\nNP-hard."}
{"id": "2510.12552", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.12552", "abs": "https://arxiv.org/abs/2510.12552", "authors": ["Nicolas El Maalouly", "Kostas Lakis"], "title": "Exact Matching and Top-k Perfect Matching Parameterized by Neighborhood Diversity or Bandwidth", "comment": null, "summary": "The Exact Matching (EM) problem asks whether there exists a perfect matching\nwhich uses a prescribed number of red edges in a red/blue edge-colored graph.\nWhile there exists a randomized polynomial-time algorithm for the problem, only\nsome special cases admit a deterministic one so far, making it a natural\ncandidate for testing the P=RP hypothesis. A polynomial-time equivalent\nproblem, Top-k Perfect Matching (TkPM), asks for a perfect matching maximizing\nthe weight of the $k$ heaviest edges.\n  We study the above problems, mainly the latter, in the scenario where the\ninput is a blown-up graph, meaning a graph which had its vertices replaced by\ncliques or independent sets. We describe an FPT algorithm for TkPM\nparameterized by $k$ and the neighborhood diversity of the input graph, which\nis essentially the size of the graph before the blow-up; this graph is also\ncalled the prototype. We extend this algorithm into an approximation scheme\nwith a much softer dependency on the aforementioned parameters, time-complexity\nwise. Moreover, for prototypes with bounded bandwidth but unbounded size, we\ndevelop a recursive algorithm that runs in subexponential time. Utilizing\nanother algorithm for EM on bounded neighborhood diversity graphs, we adapt\nthis recursive subexponential algorithm to EM.\n  Our approach is similar to the use of dynamic programming on e.g. bounded\ntreewidth instances for various problems. The main point is that the existence\nof many disjoint separators is utilized to avoid including in the separator any\nof a set of ``bad'' vertices during the split phase."}
{"id": "2510.12211", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12211", "abs": "https://arxiv.org/abs/2510.12211", "authors": ["Junfei Tan", "Yuxin Chen", "An Zhang", "Junguang Jiang", "Bin Liu", "Ziru Xu", "Han Zhu", "Jian Xu", "Bo Zheng", "Xiang Wang"], "title": "Reinforced Preference Optimization for Recommendation", "comment": null, "summary": "Recent breakthroughs in large language models (LLMs) have fundamentally\nshifted recommender systems from discriminative to generative paradigms, where\nuser behavior modeling is achieved by generating target items conditioned on\nhistorical interactions. Yet current generative recommenders still suffer from\ntwo core limitations: the lack of high-quality negative modeling and the\nreliance on implicit rewards. Reinforcement learning with verifiable rewards\n(RLVR) offers a natural solution by enabling on-policy sampling of harder\nnegatives and grounding optimization in explicit reward signals. However,\napplying RLVR to generative recommenders remains non-trivial. Its unique\ngeneration space often leads to invalid or repetitive items that undermine\nsampling efficiency, and ranking supervision is sparse since most items receive\nidentical zero rewards. To address these challenges, we propose Reinforced\nPreference Optimization for Recommendation (ReRe), a reinforcement-based\nparadigm tailored to LLM-based recommenders, an important direction in\ngenerative recommendation. ReRe incorporates constrained beam search to improve\nsampling efficiency and diversify hard negatives, while augmenting rule-based\naccuracy rewards with auxiliary ranking rewards for finer-grained supervision.\nExtensive experiments on three real-world datasets demonstrate that ReRe\nconsistently outperforms both traditional and LLM-based recommenders in ranking\nperformance. Further analysis shows that ReRe not only enhances performance\nacross both base and SFT-initialized models but also generalizes robustly\nacross different backbone families and scales. Beyond empirical gains, we\nsystematically investigate the design space of RLVR in recommendation across\ngeneration, sampling strategy, reward modeling, and optimization algorithm,\noffering insights for future research."}
{"id": "2510.12406", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.12406", "abs": "https://arxiv.org/abs/2510.12406", "authors": ["Zahra Mobini", "Hien Quoc Ngo", "Ardavan Rahimian", "Anvar Tukmanov", "David Townend", "Michail Matthaiou", "Simon L. Cotton"], "title": "Hybrid centralized-distributed precoding in fronthaul-constrained CF-mMIMO systems", "comment": null, "summary": "We investigate a fronthaul-limited cell-free massive multiple-input\nmultiple-output (CF-mMIMO) system and propose a hybrid centralized-distributed\nprecoding strategy that dynamically adapts to varying fronthaul and spectral\nefficiency (SE) requirements. The proposed approach divides users into two\ngroups: one served by centralized precoding and the other by distributed\nprecoding. We formulate a novel optimization problem for user grouping and\npower control aimed at maximizing the sum SE, subject to fronthaul and\nper-access point (AP) power constraints. To tackle the problem, we transform it\ninto a tractable form and propose efficient solution algorithms. Numerical\nresults confirm the hybrid scheme's versatility and superior performance,\nconsistently outperforming fully centralized and distributed approaches across\ndiverse system configurations."}
{"id": "2510.12641", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.12641", "abs": "https://arxiv.org/abs/2510.12641", "authors": ["Martin Bullinger", "Adam Dunajski", "Edith Elkind", "Matan Gilboa"], "title": "Single-Deviation Stability in Additively Separable Hedonic Games with Constrained Coalition Sizes", "comment": null, "summary": "We study stability in additively separable hedonic games when coalition sizes\nhave to respect fixed size bounds. We consider four classic notions of\nstability based on single-agent deviations, namely, Nash stability, individual\nstability, contractual Nash stability, and contractual individual stability.\nFor each stability notion, we consider two variants: in one, the coalition left\nbehind by a deviator must still be of a valid size, and in the other there is\nno such constraint. We provide a full picture of the existence of stable\noutcomes with respect to given size parameters. Additionally, when there are\nonly upper bounds, we fully characterize the computational complexity of the\nassociated existence problem. In particular, we obtain polynomial-time\nalgorithms for contractual individual stability and contractual Nash stability,\nwhere the latter requires an upper bound of 2. We obtain further results for\nNash stability and contractual individual stability, when the lower bound is at\nleast 2."}
{"id": "2510.12598", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.12598", "abs": "https://arxiv.org/abs/2510.12598", "authors": ["Shuyi Yan"], "title": "Lossless Derandomization for Undirected Single-Source Shortest Paths and Approximate Distance Oracles", "comment": null, "summary": "A common step in algorithms related to shortest paths in undirected graphs is\nthat, we select a subset of vertices as centers, then grow a ball around each\nvertex until a center is reached. We want the balls to be as small as possible.\nA randomized algorithm can uniformly sample $r$ centers to achieve the optimal\n(expected) ball size of $\\Theta(n/r)$. A folklore derandomization is to use the\n$O(\\log n)$ approximation for the set cover problem in the hitting set version\nwhere we want to hit all the balls with the centers.\n  However, the extra $O(\\log n)$ factor is sometimes too expensive. For\nexample, the recent $O(m\\sqrt{\\log n\\log\\log n})$ undirected single-source\nshortest path algorithm [DMSY23] beats Dijkstra's algorithm in sparse graphs,\nbut the folklore derandomization would make it dominated by Dijkstra's.\n  In this paper, we exploit the fact that the sizes of these balls can be\nadaptively chosen by the algorithm instead of fixed by the input. We propose a\nsimple deterministic algorithm achieving the optimal ball size of $\\Theta(n/r)$\non average. Furthermore, given any polynomially large cost function of the ball\nsize, we can still achieve the optimal cost on average. It allows us to\nderandomize [DMSY23], resulting in a deterministic $O(m\\sqrt{\\log n\\log\\log\nn})$ algorithm for undirected single-source shortest path.\n  In addition, we show that the same technique can also be used to derandomize\nthe seminal Thorup-Zwick approximate distance oracle [TZ05], also without any\nloss in the time/space complexity."}
{"id": "2510.12299", "categories": ["cs.IR", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.12299", "abs": "https://arxiv.org/abs/2510.12299", "authors": ["Zhi Li", "Yanan Wang", "Hao Niu", "Julio Vizcarra", "Masato Taya"], "title": "An Empirical Study for Representations of Videos in Video Question Answering via MLLMs", "comment": "6 pages, 3 figures", "summary": "Multimodal large language models have recently achieved remarkable progress\nin video question answering (VideoQA) by jointly processing visual, textual,\nand audio information. However, it remains unclear which video representations\nare most effective for MLLMs, and how different modalities balance task\naccuracy against computational efficiency. In this work, we present a\ncomprehensive empirical study of video representation methods for VideoQA with\nMLLMs. We systematically evaluate single modality inputs question only,\nsubtitles, visual frames, and audio signals as well as multimodal combinations,\non two widely used benchmarks: VideoMME and LongVideoBench. Our results show\nthat visual frames substantially enhance accuracy but impose heavy costs in GPU\nmemory and inference latency, while subtitles provide a lightweight yet\neffective alternative, particularly for long videos. These findings highlight\nclear trade-offs between effectiveness and efficiency and provide practical\ninsights for designing resource-aware MLLM-based VideoQA systems."}
{"id": "2510.12427", "categories": ["cs.IT", "math.IT", "94A15, 94A40"], "pdf": "https://arxiv.org/pdf/2510.12427", "abs": "https://arxiv.org/abs/2510.12427", "authors": ["Jonas Stapmanns", "Catarina Dias", "Luke Eilers", "Tobias Kühn", "Jean-Pascal Pfister"], "title": "Phase Transitions of the Additive Uniform Noise Channel with Peak Amplitude and Cost Constraint", "comment": "This work was presented in part at the IEEE International Symposium\n  on Information Theory (ISIT) 2025. 18 pages, 8 figures", "summary": "Under which condition is quantization optimal? We address this question in\nthe context of the additive uniform noise channel under peak amplitude and cost\nconstraints. We compute analytically the capacity-achieving input distribution\nas a function of the noise level, the average cost constraint, and the\ncurvature of the cost function. We find that when the cost function is concave,\nthe capacity-achieving input distribution is discrete, whereas when the cost\nfunction is convex and the cost constraint is active, the support of the\ncapacity-achieving input distribution spans the entire interval. For the cases\nof a discrete capacity-achieving input distribution, we derive the analytical\nexpressions for the capacity of the channel."}
{"id": "2210.12543", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2210.12543", "abs": "https://arxiv.org/abs/2210.12543", "authors": ["Shuyi Yan"], "title": "Edge-weighted Online Stochastic Matching: Beating $1-\\frac1e$", "comment": null, "summary": "We study the edge-weighted online stochastic matching problem. Since Feldman,\nMehta, Mirrokni, and Muthukrishnan proposed the $(1-\\frac1e)$-competitive\nSuggested Matching algorithm, there has been no improvement for the general\nedge-weighted online stochastic matching problem. In this paper, we introduce\nthe first algorithm beating the $1-\\frac1e$ barrier in this setting, achieving\na competitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we\ndesign an algorithmic preprocessing, dividing all edges into two classes. Then\nbased on the Suggested Matching algorithm, we adjust the matching strategy to\nimprove the performance on one class in the early stage and on another class in\nthe late stage, while keeping the matching events of different edges highly\nindependent. By balancing them, we finally guarantee the matched probability of\nevery single edge."}
{"id": "2510.12619", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.12619", "abs": "https://arxiv.org/abs/2510.12619", "authors": ["Sepehr Assadi", "Soheil Behnezhad", "Sayan Bhattacharya", "Martín Costa", "Shay Solomon", "Tianyi Zhang"], "title": "Vizing's Theorem in Deterministic Almost-Linear Time", "comment": "SODA 2026", "summary": "Vizing's theorem states that any $n$-vertex $m$-edge graph of maximum degree\n$\\Delta$ can be edge colored using at most $\\Delta + 1$ different colors.\nVizing's original proof is easily translated into a deterministic $O(mn)$ time\nalgorithm. This deterministic time bound was subsequently improved to $\\tilde\nO(m \\sqrt n)$ time, independently by [Arjomandi, 1982] and by [Gabow et al.,\n1985].\n  A series of recent papers improved the time bound of $\\tilde O(m\\sqrt{n})$\nusing randomization, culminating in the randomized near-linear time\n$(\\Delta+1)$-coloring algorithm by [Assadi, Behnezhad, Bhattacharya, Costa,\nSolomon, and Zhang, 2025]. At the heart of all of these recent improvements,\nthere is some form of a sublinear time algorithm. Unfortunately, sublinear time\nalgorithms as a whole almost always require randomization. This raises a\nnatural question: can the deterministic time complexity of the problem be\nreduced below the $\\tilde O(m\\sqrt{n})$ barrier?\n  In this paper, we answer this question in the affirmative. We present a\ndeterministic almost-linear time $(\\Delta+1)$-coloring algorithm, namely, an\nalgorithm running in $m \\cdot 2^{O(\\sqrt{\\log \\Delta})} \\cdot \\log n =\nm^{1+o(1)}$ time. Our main technical contribution is to entirely forego\nsublinear time algorithms. We do so by presenting a new deterministic\ncolor-type sparsification approach that runs in almost-linear (instead of\nsublinear) time, but can be used to color a much larger set of edges."}
{"id": "2510.12325", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12325", "abs": "https://arxiv.org/abs/2510.12325", "authors": ["Jie Yang", "Chenyang Gu", "Zixuan Liu"], "title": "Causal Inspired Multi Modal Recommendation", "comment": null, "summary": "Multimodal recommender systems enhance personalized recommendations in\ne-commerce and online advertising by integrating visual, textual, and user-item\ninteraction data. However, existing methods often overlook two critical biases:\n(i) modal confounding, where latent factors (e.g., brand style or product\ncategory) simultaneously drive multiple modalities and influence user\npreference, leading to spurious feature-preference associations; (ii)\ninteraction bias, where genuine user preferences are mixed with noise from\nexposure effects and accidental clicks. To address these challenges, we propose\na Causal-inspired multimodal Recommendation framework. Specifically, we\nintroduce a dual-channel cross-modal diffusion module to identify hidden modal\nconfounders, utilize back-door adjustment with hierarchical matching and\nvector-quantized codebooks to block confounding paths, and apply front-door\nadjustment combined with causal topology reconstruction to build a deconfounded\ncausal subgraph. Extensive experiments on three real-world e-commerce datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines while maintaining strong interpretability."}
{"id": "2510.12739", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.12739", "abs": "https://arxiv.org/abs/2510.12739", "authors": ["Mohanad Obeed", "Ming Jian"], "title": "CoNet-Rx: Collaborative Neural Networks for OFDM Receivers", "comment": null, "summary": "Deep learning (DL) based methods for orthogonal frequency division\nmultiplexing (OFDM) radio receivers demonstrated higher signal detection\nperformance compared to the traditional receivers. However, the existing\nDL-based models, usually adapted from computer vision, aren't well suited for\nwireless communications. These models require high computational resources and\nmemory, and have significant inference delays, limiting their use in\nresource-constrained settings. Additionally, reducing network size to ease\nresource demands often leads to notable performance degradation. This paper\nintroduces collaborative networks (CoNet), a novel neural network (NN)\narchitecture designed for OFDM receivers. CoNet uses multiple small ResNet or\nCNN subnetworks to simultaneously process signal features from different\nperspectives like capturing channel correlations and interference patterns.\nThese subnetworks fuse their outputs through interaction operations (e.g.,\nelement-wise multiplication), significantly enhancing detection performance.\nSimulation results show CoNet significantly outperforms traditional\narchitectures like residual networks (ResNets) in bit error rate (BER) and\nreduces inference delay when both nets have the same size and the same\ncomputational complexity."}
{"id": "2105.07161", "categories": ["cs.GT", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2105.07161", "abs": "https://arxiv.org/abs/2105.07161", "authors": ["Jochen Koenemann", "Justin Toth", "Felix Zhou"], "title": "On the Complexity of Nucleolus Computation for Bipartite b-Matching Games", "comment": null, "summary": "We explore the complexity of nucleolus computation in b-matching games on\nbipartite graphs. We show that computing the nucleolus of a simple b-matching\ngame is NP-hard even on bipartite graphs of maximum degree 7. We complement\nthis with partial positive results in the special case where b values are\nbounded by 2. In particular, we describe an efficient algorithm when a constant\nnumber of vertices satisfy b(v) = 2 as well as an efficient algorithm for\ncomputing the non-simple b-matching nucleolus when b = 2."}
{"id": "2510.12327", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12327", "abs": "https://arxiv.org/abs/2510.12327", "authors": ["Benjamin Clavié", "Sean Lee", "Rikiya Takehi", "Aamir Shakir", "Makoto P. Kato"], "title": "Simple Projection Variants Improve ColBERT Performance", "comment": null, "summary": "Multi-vector dense retrieval methods like ColBERT systematically use a\nsingle-layer linear projection to reduce the dimensionality of individual\nvectors. In this study, we explore the implications of the MaxSim operator on\nthe gradient flows of the training of multi-vector models and show that such a\nsimple linear projection has inherent, if non-critical, limitations in this\nsetting. We then discuss the theoretical improvements that could result from\nreplacing this single-layer projection with well-studied alternative\nfeedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU\nblocks, and skip-connections, could alleviate these limitations. Through the\ndesign and systematic evaluation of alternate projection blocks, we show that\nbetter-designed final projections positively impact the downstream performance\nof ColBERT models. We highlight that many projection variants outperform the\noriginal linear projections, with the best-performing variants increasing\naverage performance on a range of retrieval benchmarks across domains by over 2\nNDCG@10 points. We then conduct further exploration on the individual\nparameters of these projections block in order to understand what drives this\nempirical performance, highlighting the particular importance of upscaled\nintermediate projections and residual connections. As part of these ablation\nstudies, we show that numerous suboptimal projection variants still outperform\nthe traditional single-layer projection across multiple benchmarks, confirming\nour hypothesis. Finally, we observe that this effect is consistent across\nrandom seeds, further confirming that replacing the linear layer of ColBERT\nmodels is a robust, drop-in upgrade."}
{"id": "2510.12641", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.12641", "abs": "https://arxiv.org/abs/2510.12641", "authors": ["Martin Bullinger", "Adam Dunajski", "Edith Elkind", "Matan Gilboa"], "title": "Single-Deviation Stability in Additively Separable Hedonic Games with Constrained Coalition Sizes", "comment": null, "summary": "We study stability in additively separable hedonic games when coalition sizes\nhave to respect fixed size bounds. We consider four classic notions of\nstability based on single-agent deviations, namely, Nash stability, individual\nstability, contractual Nash stability, and contractual individual stability.\nFor each stability notion, we consider two variants: in one, the coalition left\nbehind by a deviator must still be of a valid size, and in the other there is\nno such constraint. We provide a full picture of the existence of stable\noutcomes with respect to given size parameters. Additionally, when there are\nonly upper bounds, we fully characterize the computational complexity of the\nassociated existence problem. In particular, we obtain polynomial-time\nalgorithms for contractual individual stability and contractual Nash stability,\nwhere the latter requires an upper bound of 2. We obtain further results for\nNash stability and contractual individual stability, when the lower bound is at\nleast 2."}
{"id": "2510.12369", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12369", "abs": "https://arxiv.org/abs/2510.12369", "authors": ["Yang Xiang", "Li Fan", "Chenke Yin", "Chengtao Ji"], "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning", "comment": null, "summary": "Recent progress in language and vision foundation models demonstrates the\nimportance of discrete token interfaces that transform complex inputs into\ncompact sequences for large-scale modeling. Extending this paradigm to graphs\nrequires a tokenization scheme that handles non-Euclidean structures and\nmulti-scale dependencies efficiently. Existing approaches to graph\ntokenization, linearized, continuous, and quantized, remain limited in\nadaptability and efficiency. In particular, most current quantization-based\ntokenizers organize hierarchical information in fixed or task-agnostic ways,\nwhich may either over-represent or under-utilize structural cues, and lack the\nability to dynamically reweight contributions from different levels without\nretraining the encoder. This work presents a hierarchical quantization\nframework that introduces a self-weighted mechanism for task-adaptive\naggregation across multiple scales. The proposed method maintains a frozen\nencoder while modulating information flow through a lightweight gating process,\nenabling parameter-efficient adaptation to diverse downstream tasks.\nExperiments on benchmark datasets for node classification and link prediction\ndemonstrate consistent improvements over strong baselines under comparable\ncomputational budgets."}
{"id": "2510.12461", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12461", "abs": "https://arxiv.org/abs/2510.12461", "authors": ["Andrei Chernov", "Haroon Wahab", "Oleg Novitskij"], "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance", "comment": null, "summary": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}."}
{"id": "2510.12604", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12604", "abs": "https://arxiv.org/abs/2510.12604", "authors": ["Qihang Zhao", "Zhongbo Sun", "Xiaoyang Zheng", "Xian Guo", "Siyuan Wang", "Zihan Liang", "Mingcan Peng", "Ben Chen", "Chenyi Lei"], "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch", "comment": null, "summary": "With the rise of modern search and recommendation platforms, insufficient\ncollaborative information of cold-start items exacerbates the Matthew effect of\nexisting platform items, challenging platform diversity and becoming a\nlongstanding issue. Existing methods align items' side content with\ncollaborative information to transfer collaborative signals from\nhigh-popularity items to cold-start items. However, these methods fail to\naccount for the asymmetry between collaboration and content, nor the\nfine-grained differences among items. To address these issues, we propose\nSMILE, an item representation enhancement approach based on fused alignment of\nsemantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and\ncollaborative information, followed by a two-step alignment: RQ encoding\ntransfers shared collaborative signals across items, while OPQ encoding learns\ndifferentiated information of items. Comprehensive offline experiments on\nlarge-scale industrial datasets demonstrate superiority of SMILE, and rigorous\nonline A/B tests confirm statistically significant improvements: item CTR\n+1.66%, buyers +1.57%, and order volume +2.17%."}
{"id": "2510.12668", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12668", "abs": "https://arxiv.org/abs/2510.12668", "authors": ["Minghao Tang", "Shiyu Ni", "Jingtong Wu", "Zengxin Han", "Keping Bi"], "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG."}
{"id": "2510.12709", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12709", "abs": "https://arxiv.org/abs/2510.12709", "authors": ["Lin Lin", "Jiefeng Long", "Zhihe Wan", "Yuchi Wang", "Dingkang Yang", "Shuang Yang", "Yueyang Yao", "Xu Chen", "Zirui Guo", "Shengqiang Li", "Weiran Li", "Hanyu Li", "Yaling Mou", "Yan Qiu", "Haiyang Yu", "Xiao Liang", "Hongsheng Li", "Chao Feng"], "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model", "comment": "Technical Report", "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain."}
