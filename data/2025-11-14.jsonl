{"id": "2511.10063", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.10063", "abs": "https://arxiv.org/abs/2511.10063", "authors": ["Yiwen Wang", "Vivek Shah", "Marcos Antonio Vaz Salles", "Claudia Bauzer Medeiros", "Julio Cesar Dos Reis", "Yongluan Zhou"], "title": "Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management", "comment": null, "summary": "Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency."}
{"id": "2511.10418", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.10418", "abs": "https://arxiv.org/abs/2511.10418", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Mark Birkin", "Man Luo"], "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain."}
{"id": "2511.09882", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09882", "abs": "https://arxiv.org/abs/2511.09882", "authors": ["Yaron Salman", "Tamir Tassa", "Omer Lev", "Roie Zivan"], "title": "Truth, Justice, and Secrecy: Cake Cutting Under Privacy Constraints", "comment": "This is the full version of our paper published in the Proceedings of AAAI 2026", "summary": "Cake-cutting algorithms, which aim to fairly allocate a continuous resource based on individual agent preferences, have seen significant progress over the past two decades. Much of the research has concentrated on fairness, with comparatively less attention given to other important aspects. Chen et al. (2010) introduced an algorithm that, in addition to ensuring fairness, was strategyproof -- meaning agents had no incentive to misreport their valuations. However, even in the absence of strategic incentives to misreport, agents may still hesitate to reveal their true preferences due to privacy concerns (e.g., when allocating advertising time between firms, revealing preferences could inadvertently expose planned marketing strategies or product launch timelines). In this work, we extend the strategyproof algorithm of Chen et al. by introducing a privacy-preserving dimension. To the best of our knowledge, we present the first private cake-cutting protocol, and, in addition, this protocol is also envy-free and strategyproof. Our approach replaces the algorithm's centralized computation with a novel adaptation of cryptographic techniques, enabling privacy without compromising fairness or strategyproofness. Thus, our protocol encourages agents to report their true preferences not only because they are not incentivized to lie, but also because they are protected from having their preferences exposed."}
{"id": "2511.10138", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.10138", "abs": "https://arxiv.org/abs/2511.10138", "authors": ["Jun Zhang", "Yi Li", "Yue Liu", "Changping Wang", "Yuan Wang", "Yuling Xiong", "Xun Liu", "Haiyang Wu", "Qian Li", "Enming Zhang", "Jiawei Sun", "Xin Xu", "Zishuai Zhang", "Ruoran Liu", "Suyuan Huang", "Zhaoxin Zhang", "Zhengkai Guo", "Shuojin Yang", "Meng-Hao Guo", "Huan Yu", "Jie Jiang", "Shi-Min Hu"], "title": "GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation", "comment": "12 pages, 5 figures", "summary": "As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR."}
{"id": "2511.09707", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2511.09707", "abs": "https://arxiv.org/abs/2511.09707", "authors": ["Ajaykrishnan E S", "Robert Ganian", "Daniel Lokshtanov", "Vaishali Surianarayanan"], "title": "A Quasi-Polynomial Time Algorithm for 3-Coloring Circle Graphs", "comment": "19 pages, 6 figures, Best Paper Award at SOSA 2026", "summary": "A graph $G$ is a circle graph if it is an intersection graph of chords of a unit circle. We give an algorithm that takes as input an $n$ vertex circle graph $G$, runs in time at most $n^{O(\\log n)}$ and finds a proper $3$-coloring of $G$, if one exists. As a consequence we obtain an algorithm with the same running time to determine whether a given ordered graph $(G, \\prec)$ has a $3$-page book embedding. This gives a partial resolution to the well known open problem of Dujmović and Wood [Discret. Math. Theor. Comput. Sci. 2004], Eppstein [2014], and Bachmann, Rutter and Stumpf [J. Graph Algorithms Appl. 2024] of whether 3-Coloring on circle graphs admits a polynomial time algorithm."}
{"id": "2511.09929", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.09929", "abs": "https://arxiv.org/abs/2511.09929", "authors": ["Zhentian Zhang", "David Morales-Jimenez", "Hao Jiang", "Christos Masouros"], "title": "A Universal Block Error Rate Bound for Fluid Antenna Systems", "comment": null, "summary": "Fluid antenna systems (FASs) offer genuine simplicity for communication network design by eliminating expensive hardware overhead and reducing the complexity of access protocol architectures. Through the discovery of significant spatial diversity within a compact antenna space, FASs enable the implementation of reconfigurable-antenna-based architectures. However, current state-of-the-art studies rarely investigate the impact of finite blocklength constraints on FAS-based designs, leaving a gap in both analytical modeling and the establishment of a solid, universally applicable performance metric for finite blocklength fluid antenna systems (FBL-FAS). In this work, we focus on the study of FBL-FAS and, more importantly, derive a block error rate (BLER) bound that serves as a general and practical performance benchmark across various FAS architectures. The proposed BLER bound is computable both with and without an explicit statistical model, meaning that the BLER performance can be characterized analytically or empirically under model-aware or model-free system scenarios. Moreover, when the statistical model is known, the analytical results derived from the proposed BLER bound exhibit strong alignment with the empirical findings, demonstrating the remarkable simplicity, accuracy, and universality of the proposed BLER bound."}
{"id": "2511.10245", "categories": ["cs.MM", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.10245", "abs": "https://arxiv.org/abs/2511.10245", "authors": ["Rizal Khoirul Anam"], "title": "Robustness and Imperceptibility Analysis of Hybrid Spatial-Frequency Domain Image Watermarking", "comment": null, "summary": "The proliferation of digital media necessitates robust methods for copyright protection and content authentication. This paper presents a comprehensive comparative study of digital image watermarking techniques implemented using the spatial domain (Least Significant Bit - LSB), the frequency domain (Discrete Fourier Transform - DFT), and a novel hybrid (LSB+DFT) approach. The core objective is to evaluate the trade-offs between imperceptibility (measured by Peak Signal-to-Noise Ratio - PSNR) and robustness (measured by Normalized Correlation - NC and Bit Error Rate - BER). We implemented these three techniques within a unified MATLAB-based experimental framework. The watermarked images were subjected to a battery of common image processing attacks, including JPEG compression, Gaussian noise, and salt-and-pepper noise, at varying intensities. Experimental results generated from standard image datasets (USC-SIPI) demonstrate that while LSB provides superior imperceptibility, it is extremely fragile. The DFT method offers significant robustness at the cost of visual quality. The proposed hybrid LSB+DFT technique, which leverages redundant embedding and a fallback extraction mechanism, is shown to provide the optimal balance, maintaining high visual fidelity while exhibiting superior resilience to all tested attacks."}
{"id": "2511.09934", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.09934", "abs": "https://arxiv.org/abs/2511.09934", "authors": ["David X. Lin", "Giannis Fikioris", "Siddhartha Banerjee", "Éva Tardos"], "title": "Robust Resource Allocation via Competitive Subsidies", "comment": null, "summary": "A canonical setting for non-monetary online resource allocation is one where agents compete over multiple rounds for a single item per round, with i.i.d. valuations and additive utilities across rounds. With $n$ symmetric agents, a natural benchmark for each agent is the utility realized by her favorite $1/n$-fraction of rounds; a line of work has demonstrated one can robustly guarantee each agent a constant fraction of this ideal utility, irrespective of how other agents behave. In particular, several mechanisms have been shown to be $1/2$-robust, and recent work established that repeated first-price auctions based on artificial credits have a robustness factor of $0.59$, which cannot be improved beyond $0.6$ using first-price and simple strategies. In contrast, even without strategic considerations, the best achievable factor is $1-1/e\\approx 0.63$.\n  In this work, we break the $0.6$ first-price barrier to get a new $0.625$-robust mechanism, which almost closes the gap to the non-strategic robustness bound. Surprisingly, we do so via a simple auction, where in each round, bidders decide if they ask for the item, and we allocate uniformly at random among those who ask. The main new ingredient is the idea of competitive subsidies, wherein we charge the winning agent an amount in artificial credits that decreases when fewer agents are bidding (specifically, when $k$ agents bid, then the winner pays proportional to $k/(k+1)$, varying the payment by a factor of 2 depending on the competition). Moreover, we show how it can be modified to get an equilibrium strategy with a slightly weaker robust guarantee of $5/(3e) \\approx 0.61$ (and the optimal $1-1/e$ factor at equilibrium). Finally, we show that our mechanism gives the best possible bound under a wide class of auction-based mechanisms."}
{"id": "2511.10492", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10492", "abs": "https://arxiv.org/abs/2511.10492", "authors": ["Yunkai Zhang", "Qiang Zhang", "Feng", "Lin", "Ruizhong Qiu", "Hanchao Yu", "Jason Liu", "Yinglong Xia", "Zhuoran Yu", "Zeyu Zheng", "Diji Yang"], "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding", "comment": null, "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes."}
{"id": "2511.09842", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.09842", "abs": "https://arxiv.org/abs/2511.09842", "authors": ["Bingbing Hu", "Jakob Nogler", "Barna Saha"], "title": "Hardness of Dynamic Tree Edit Distance and Friends", "comment": "ITCS 2026", "summary": "String Edit Distance is a more-than-classical problem whose behavior in the dynamic setting, where the strings are updated over time, is well understood. A single character substitution, insertion, or deletion can be handled in $\\tilde{\\mathcal{O}}(n \\cdot \\min(\\sqrt{n},w))$ time [Charalampopoulos, Kociumaka, Mozes, CPM 2020], where $w$ is the maximum operation weight. This bound is optimal [Cassis, Kociumaka, Wellnitz, FOCS 2023] and provides a substantial improvement over the static $\\mathcal{O}(n^2)$ algorithm when few characters of the input string are updated.\n  In contrast, for inherently related problems such as Tree Edit Distance, Dyck Edit Distance, and RNA Folding, it has remained unknown whether it is possible to devise dynamic algorithms with an advantage over the static algorithm. In this paper, we resolve this question by showing that (weighted) Tree Edit Distance, Dyck Edit Distance, and RNA Folding admit no dynamic speedup: under well-known fine-grained assumptions we show that the best possible algorithm recomputes the solution from scratch after each update. Furthermore, we prove a quadratic per-update lower bound for unweighted Tree Edit Distance under the $k$-Clique Conjecture. This provides the first separation between dynamic unweighted String Edit Distance and unweighted Tree Edit Distance, problems whose relative difficulty in the static setting is still open."}
{"id": "2511.10052", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.10052", "abs": "https://arxiv.org/abs/2511.10052", "authors": ["Yiwei Liao", "Shurui Tu", "Yujie Zhou", "Dongzi Jin", "Yong Xiao", "Yingyu Li"], "title": "Implicit Semantic Communication Based on Bayesian Reconstruction Framework", "comment": null, "summary": "Semantic communication is a novel communication paradigm that focuses on the transportation and delivery of the \\emph{meaning} of messages. Recent results have verified that a graphical structure provides the most expressive and structurally faithful formalism for representing the relational semantics in most information sources. However, most existing works represent the semantics based on pairwise relation-based graphs, which cannot capture the higher-order interactions that are essential for some semantic sources. This paper proposes a novel Bayesian hypergraph inference-based semantic communication framework that can directly recover implicit semantic information involving high-order hyperedges at the receiver based on the pairwise relation-based explicit semantics sent by the transmitter. Experimental results based on real-world datasets demonstrated that the proposed SBRF achieves up to 90\\% recovery accuracy of the high-order hyperedges based on the pairwise relation-based explicit semantics."}
{"id": "2511.10325", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.10325", "abs": "https://arxiv.org/abs/2511.10325", "authors": ["Yan Zhuang", "Minhao Liu", "Yanru Zhang", "Jiawen Deng", "Fuji Ren"], "title": "TMDC: A Two-Stage Modality Denoising and Complementation Framework for Multimodal Sentiment Analysis with Missing and Noisy Modalities", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Sentiment Analysis (MSA) aims to infer human sentiment by integrating information from multiple modalities such as text, audio, and video. In real-world scenarios, however, the presence of missing modalities and noisy signals significantly hinders the robustness and accuracy of existing models. While prior works have made progress on these issues, they are typically addressed in isolation, limiting overall effectiveness in practical settings. To jointly mitigate the challenges posed by missing and noisy modalities, we propose a framework called Two-stage Modality Denoising and Complementation (TMDC). TMDC comprises two sequential training stages. In the Intra-Modality Denoising Stage, denoised modality-specific and modality-shared representations are extracted from complete data using dedicated denoising modules, reducing the impact of noise and enhancing representational robustness. In the Inter-Modality Complementation Stage, these representations are leveraged to compensate for missing modalities, thereby enriching the available information and further improving robustness. Extensive evaluations on MOSI, MOSEI, and IEMOCAP demonstrate that TMDC consistently achieves superior performance compared to existing methods, establishing new state-of-the-art results."}
{"id": "2511.10228", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10228", "abs": "https://arxiv.org/abs/2511.10228", "authors": ["Thanasis Lianeas", "Marios Mertzanidis", "Aikaterini Nikolidaki"], "title": "Facility Location for Congesting Commuters and Generalizing the Cost-Distance Problem", "comment": null, "summary": "In Facility Location problems there are agents that should be connected to facilities and locations where facilities may be opened so that agents can connect to them. We depart from Uncapacitated Facility Location and by assuming that the connection costs of agents to facilities are congestion dependent, we define a novel problem, namely, Facility Location for Congesting (Selfish) Commuters. The connection costs of agents to facilities come as a result of how the agents commute to reach the facilities in an underlying network with cost functions on the edges. Inapproximability results follow from the related literature and thus approximate solutions is all we can hope for. For when the cost functions are nondecreasing we employ in a novel way an approximate version of Caratheodory's Theorem [5] to show how approximate solutions for different versions of the problem can be derived. For when the cost functions are nonincreasing we show how this problem generalizes the Cost-Distance problem [38] and provide an algorithm that for this more general case achieves the same approximation guarantees."}
{"id": "2511.10036", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.10036", "abs": "https://arxiv.org/abs/2511.10036", "authors": ["Yotam Kenneth-Mordoch", "Robert Krauthgamer"], "title": "Faster All-Pairs Minimum Cut: Bypassing Exact Max-Flow", "comment": null, "summary": "All-Pairs Minimum Cut (APMC) is a fundamental graph problem that asks to find a minimum $s,t$-cut for every pair of vertices $s,t$. A recent line of work on fast algorithms for APMC has culminated with a reduction of APMC to $\\mathrm{polylog}(n)$-many max-flow computations. But unfortunately, no fast algorithms are currently known for exact max-flow in several standard models of computation, such as the cut-query model and the fully-dynamic model.\n  Our main technical contribution is a sparsifier that preserves all minimum $s,t$-cuts in an unweighted graph, and can be constructed using only approximate max-flow computations. We then use this sparsifier to devise new algorithms for APMC in unweighted graphs in several computational models: (i) a randomized algorithm that makes $\\tilde{O}(n^{3/2})$ cut queries to the input graph; (ii) a deterministic fully-dynamic algorithm with $n^{3/2+o(1)}$ worst-case update time; and (iii) a randomized two-pass streaming algorithm with space requirement $\\tilde{O}(n^{3/2})$. These results improve over the known bounds, even for (single pair) minimum $s,t$-cut in the respective models."}
{"id": "2511.10066", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.10066", "abs": "https://arxiv.org/abs/2511.10066", "authors": ["Buket Özkaya"], "title": "Generalized Spectral Bound for Quasi-Twisted Codes", "comment": null, "summary": "Semenov and Trifonov [22] developed a spectral theory for quasi-cyclic codes and formulated a BCH-like minimum distance bound. Their approach was generalized by Zeh and Ling [24], by using the HT bound. The first spectral bound for quasi-twisted codes appeared in [7], which generalizes Semenov-Trifonov and Zeh-Ling bounds, but its overall performance was observed to be worse than the Jensen bound. More recently, an improved spectral bound for quasi-cyclic codes was proposed in [15], which outperforms the Jensen bound in many cases. In this paper, we adopt this approach to quasi-twisted case and we show that this new generalized spectral bound provides tighter lower bounds on the minimum distance compared to the Jensen and Ezerman et. al. bounds."}
{"id": "2511.10202", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.10202", "abs": "https://arxiv.org/abs/2511.10202", "authors": ["Athanasios L. Konstantinidis", "Charis Papadopoulos", "Georgios Velissaris"], "title": "Algorithms and Complexity of Hedge Cluster Deletion Problems", "comment": null, "summary": "A hedge graph is a graph whose edge set has been partitioned into groups called hedges. Here we consider a generalization of the well-known \\textsc{Cluster Deletion} problem, named \\textsc{Hedge Cluster Deletion}. The task is to compute the minimum number of hedges of a hedge graph so that their removal results in a graph that is isomorphic to a disjoint union of cliques. We show that for graphs that contain bounded size of vertex-disjoint 3-vertex-paths as subgraphs, \\textsc{Hedge Cluster Deletion} can be solved in polynomial time. Regarding its approximability, we prove that the problem is tightly connected to the related complexity of the \\textsc{Min Horn Deletion} problem, a well-known boolean CSP problem. Our connection shows that it is NP-hard to approximate \\textsc{Hedge Cluster Deletion} within factor $2^{O(\\log^{1-ε} r)}$ for any $ε>0$, where $r$ is the number of hedges in a given hedge graph.\n  Based on its classified (in)approximability and the difficulty imposed by the structure of almost all non-trivial graphs, we consider the hedge underlying structure. We give a polynomial-time algorithm with constant approximation ratio for \\textsc{Hedge Cluster Deletion} whenever each triangle of the input graph is covered by at most two hedges. On the way to this result, an interesting ingredient that we solved efficiently is a variant of the \\textsc{Vertex Cover} problem in which apart from the desired vertex set that covers the edge set, a given set of vertex-constraints should also be included in the solution. Moreover, as a possible workaround for the existence of efficient exact algorithms, we propose the hedge intersection graph which is the intersection graph spanned by the hedges. Towards this direction, we give a polynomial-time algorithm for \\textsc{Hedge Cluster Deletion} whenever the hedge intersection graph is acyclic."}
{"id": "2511.10181", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.10181", "abs": "https://arxiv.org/abs/2511.10181", "authors": ["Eeshan Modak", "Mayank Bakshi", "Bikash Kumar Dey", "Vinod M. Prabhakaran"], "title": "Sequential Adversarial Hypothesis Testing", "comment": "Shorter version of this work was published at ISIT 2024", "summary": "We study the adversarial binary hypothesis testing problem in the sequential setting. Associated with each hypothesis is a closed, convex set of distributions. Given the hypothesis, each observation is generated according to a distribution chosen (from the set associated with the hypothesis) by an adversary who has access to past observations. In the sequential setting, the number of observations the detector uses to arrive at a decision is variable; this extra freedom improves the asymptotic performance of the test. We characterize the closure of the set of achievable pairs of error exponents. We also study the problem under constraints on the number of observations used and the probability of error incurred."}
{"id": "2511.10230", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.10230", "abs": "https://arxiv.org/abs/2511.10230", "authors": ["Samuel Humeau", "Mamadou Moustapha Kanté", "Daniel Mock", "Timothé Picavet", "Alexandre Vigny"], "title": "Testing H-freeness on sparse graphs, the case of bounded expansion", "comment": "19 pages", "summary": "In property testing, a tester makes queries to (an oracle for) a graph and, on a graph having or being far from having a property P, it decides with high probability whether the graph satisfies P or not. Often, testers are restricted to a constant number of queries. While the graph properties for which there exists such a tester are somewhat well characterized in the dense graph model, it is not the case for sparse graphs. In this area, Czumaj and Sohler (FOCS'19) proved that H-freeness (i.e. the property of excluding the graph H as a subgraph) can be tested with constant queries on planar graphs as well as on graph classes excluding a minor.\n  Using results from the sparsity toolkit, we propose a simpler alternative to the proof of Czumaj and Sohler, for a statement generalized to the broader notion of bounded expansion. That is, we prove that for any class C with bounded expansion and any graph H, testing H-freeness can be done with constant query complexity on any graph G in C, where the constant depends on H and C, but is independent of G.\n  While classes excluding a minor are prime examples of classes with bounded expansion, so are, for example, cubic graphs, graph classes with bounded maximum degree, graphs of bounded book thickness, or random graphs of bounded average degree."}
{"id": "2511.10291", "categories": ["cs.IT", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.10291", "abs": "https://arxiv.org/abs/2511.10291", "authors": ["Aswin Arun", "Christo Kurisummoottil Thomas", "Rimalpudi Sarvendranath", "Walid Saad"], "title": "Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access", "comment": null, "summary": "Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems."}
{"id": "2511.10310", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.10310", "abs": "https://arxiv.org/abs/2511.10310", "authors": ["Honghao Wang", "Qingqing Wu", "Yifan Jiang", "Ziyuan Zheng", "Ziheng Zhang", "Yanze Zhu", "Ying Gao", "Wen Chen", "Guanghai Liu", "Abbas Jamalipour"], "title": "Reconfigurable Airspace: Synergizing Movable Antenna and Intelligent Surface for Low-Altitude ISAC Networks", "comment": null, "summary": "Low-altitude unmanned aerial vehicle (UAV) networks are integral to future 6G integrated sensing and communication (ISAC) systems. However, their deployment is hindered by challenges stemming from high mobility of UAVs, complex propagation environments, and the inherent trade-offs between coexisting sensing and communication functions. This article proposes a novel framework that leverages movable antennas (MAs) and intelligent reflecting surfaces (IRSs) as dual enablers to overcome these limitations. MAs, through active transceiver reconfiguration, and IRSs, via passive channel reconstruction, can work in synergy to significantly enhance system performance. Our analysis first elaborates on the fundamental gains offered by MAs and IRSs, and provides simulation results that validate the immense potential of the MA-IRS-enabled ISAC architecture. Two core UAV deployment scenarios are then investigated: (i) UAVs as ISAC users, where we focus on achieving high-precision tracking and aerial safety, and (ii) UAVs as aerial network nodes, where we address robust design and complex coupled resource optimization. Finally, key technical challenges and research opportunities are identified and analyzed for each scenario, charting a clear course for the future design of advanced low-altitude ISAC networks."}
