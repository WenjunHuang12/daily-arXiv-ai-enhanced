{"id": "2511.08941", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08941", "abs": "https://arxiv.org/abs/2511.08941", "authors": ["Chenhao Wang", "Shanshan Feng", "Lisi Chen", "Fan Li", "Shuo Shang"], "title": "Efficient Model-Agnostic Continual Learning for Next POI Recommendation", "comment": "This paper was accepted by ICDE2026", "summary": "Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption."}
{"id": "2511.09250", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.09250", "abs": "https://arxiv.org/abs/2511.09250", "authors": ["Jiyuan Wang", "Li Zhang", "Haipeng Lin", "Qile Liu", "Gan Huang", "Ziyu Li", "Zhen Liang", "Xia Wu"], "title": "NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning", "comment": null, "summary": "Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics."}
{"id": "2511.09329", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.09329", "abs": "https://arxiv.org/abs/2511.09329", "authors": ["Andreas Konstantin Kruff", "Christin Katharina Kreutz", "Timo Breuer", "Philipp Schaer", "Krisztian Balog"], "title": "Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction", "comment": null, "summary": "Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR community. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simulation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query predictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench."}
{"id": "2511.09545", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.09545", "abs": "https://arxiv.org/abs/2511.09545", "authors": ["Etienne Dallaire"], "title": "Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs", "comment": null, "summary": "This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build and audit golden sets. Third, leaderboards exist but lack end-to-end, on-corpus benchmarking that reflects production trade-offs. Fourth, how state-of-the-art embedding models handle proper-name identity signals and conversational noise remains opaque. To address these, we contribute: (1) RA-nWG@K, a rarity-aware, per-query-normalized set score, and operational ceilings via the pool-restricted oracle ceiling (PROC) and the percentage of PROC (%PROC) to separate retrieval from ordering headroom within a Cost-Latency-Quality (CLQ) lens; (2) rag-gs (MIT), a lean golden-set pipeline with Plackett-Luce listwise refinement whose iterative updates outperform single-shot LLM ranking; (3) a comprehensive benchmark on a production RAG (scientific-papers corpus) spanning dense retrieval, hybrid dense+BM25, embedding models and dimensions, cross-encoder rerankers, ANN (HNSW), and quantization; and (4) targeted diagnostics that quantify proper-name identity signal and conversational-noise sensitivity via identity-destroying and formatting ablations. Together, these components provide practitioner Pareto guidance and auditable guardrails to support reproducible, budget/SLA-aware decisions."}
{"id": "2511.08801", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.08801", "abs": "https://arxiv.org/abs/2511.08801", "authors": ["Mahsa Derakhshan", "Mohammad Roghani", "Mohammad Saneian", "Tao Yu"], "title": "A Simple Analysis of Ranking in General Graphs", "comment": null, "summary": "We provide a simple combinatorial analysis of the Ranking algorithm, originally introduced in the seminal work by Karp, Vazirani, and Vazirani [KVV90], demonstrating that it achieves a $(1/2 + c)$-approximate matching for general graphs for $c \\geq 0.005$."}
{"id": "2511.08765", "categories": ["cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.08765", "abs": "https://arxiv.org/abs/2511.08765", "authors": ["Rustam Galimullin", "Munyque Mittelmann", "Laurent Perrussel"], "title": "Formal Verification of Diffusion Auctions", "comment": "This is an extended version of the paper with the same title that will appear in the proceedings of AAAI 2026. This version contains a technical appendix with proof details that, for space reasons, do not appear in the AAAI 2026 version", "summary": "In diffusion auctions, sellers can leverage an underlying social network to broaden participation, thereby increasing their potential revenue. Specifically, sellers can incentivise participants in their auction to diffuse information about the auction through the network. While numerous variants of such auctions have been recently studied in the literature, the formal verification and strategic reasoning perspectives have not been investigated yet.\n  Our contribution is threefold. First, we introduce a logical formalism that captures the dynamics of diffusion and its strategic dimension. Second, for such a logic, we provide model-checking procedures that allow one to verify properties as the Nash equilibrium, and that pave the way towards checking the existence of sellers' strategies. Third, we establish computational complexity results for the presented algorithms."}
{"id": "2511.08826", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.08826", "abs": "https://arxiv.org/abs/2511.08826", "authors": ["Zonglin Guo", "Tony Givargis"], "title": "FlashMap: A Flash Optimized Key-Value Store", "comment": "6 pages, 2 figures, 3 tables", "summary": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server."}
{"id": "2511.08978", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08978", "abs": "https://arxiv.org/abs/2511.08978", "authors": ["Jingtian Ma", "Jingyuan Wang", "Wayne Xin Zhao", "Guoping Liu", "Xiang Wen"], "title": "Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding", "comment": null, "summary": "Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy."}
{"id": "2511.08756", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08756", "abs": "https://arxiv.org/abs/2511.08756", "authors": ["Al Nahian Mugdho", "Md. Ibrahim", "A. S. M. Badrudduza", "Md. Abdur Rakib", "Imran Shafique Ansari"], "title": "Design and Performance Analysis of Hybrid FSO/THz Relay with Aerial RIS for Future NTN-Integrated 6G Wireless Communications", "comment": null, "summary": "In the context of emerging sixth-generation (6G) wireless networks, reconfigurable intelligent surfaces (RISs) are gaining prominence for their ability to intelligently control electromagnetic wave propagation and enhance backhaul communication performance. In this paper, we propose a novel dual-hop wireless network, where the first hop consists of a hybrid free-space optics (FSO) / terahertz (THz) link, and the second hop incorporates an aerial RIS-based radio frequency (RF) link. To provide a comprehensive performance evaluation, a comparative analysis of two switching strategies is conducted: (1) hard switching and (2) soft switching. Novel closed-form expressions are derived for key performance metrics, including outage probability and bit error rate. These expressions are then utilized to investigate the impact of various system parameters. Our proposed hybrid model demonstrates a 52.54% performance improvement over the traditional RF-FSO framework. Moreover, the integration of an aerial RIS in the second hop enhances system performance by 41.39%. Numerical findings suggest that strategically placing the aerial RIS at a lower altitude and maintaining an equal, shorter distance from both communication endpoints significantly improves overall system performance. To analyze the response under high signal-to-noise ratio (SNR) conditions, asymptotic analysis is performed, and the diversity order of the system is determined. Finally, the analytical results are validated through a Monte Carlo simulation."}
{"id": "2511.08958", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.08958", "abs": "https://arxiv.org/abs/2511.08958", "authors": ["Md. Tanzeem Rahat", "Md. Manzurul Hasan"], "title": "Space-Efficient and Output-Sensitive Algorithms for the Longest Common Bitonic Subsequence", "comment": "12 pages, 2 figres, In the process of submission to 37th Annual Symposium on Combinatorial Pattern Matching", "summary": "The longest common bitonic subsequence (LCBS) of two sequences A and B is the longest subsequence that increases to a single peak and then decreases while appearing, in order, in both inputs. Although LCBS naturally models rise-fall patterns in bioinformatics, finance, and signal analysis, the only previously documented solution was a quadratic dynamic program that needs θ(nm) time and space. We show that this space barrier is not inherent: a refined rolling-row implementation evaluates the same recurrence in θ(nm) time with only θ(min(n, m)) additional memory. By isolating the M symbol matches and their C bitonic-compatible pairs, we cast LCBS as a longest-path problem in a sparse DAG and solve it in O((n + m) log n + M log M) time and O(M) space, which is asymptotically faster than the quadratic baseline whenever M << n m. These results make exact LCBS computation practical for inputs that were previously out of reach and expose a new fine-grained complexity landscape that invites further exploration."}
{"id": "2511.09021", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.09021", "abs": "https://arxiv.org/abs/2511.09021", "authors": ["Aloïs Duguet", "Tobias Harks", "Martin Schmidt", "Julian Schwarz"], "title": "Minimal Regret Walras Equilibria for Combinatorial Markets", "comment": null, "summary": "We consider combinatorial multi-item markets and propose the notion of a $Δ$-regret Walras equilibrium, which is an allocation of items to players and a set of item prices that achieve the following goals: prices clear the market, the allocation is capacity-feasible, and the players' strategies lead to a total regret of $Δ$. The regret is defined as the sum of individual player regrets measured by the utility gap with respect to the optimal item bundle given the prices. We derive necessary and sufficient conditions for the existence of $Δ$-regret equilibria, where we establish a connection to the duality gap and the integrality gap of the social welfare problem. For the special case of monotone valuations, the derived necessary and sufficient optimality conditions coincide and lead to a complete characterization of achievable $Δ$-regret equilibria. For general valuations, we establish an interesting connection to the area of sensitivity theory in linear optimization. We show that the sensitivity gap of the optimal-value function of two (configuration) linear programs with changed right-hand side can be used to establish a bound on the achievable regret. Finally, we use these general structural results to translate known approximation algorithms for the social welfare optimization problem into algorithms computing low-regret Walras equilibria. We also demonstrate how to derive strong lower bounds based on integrality and duality gaps but also based on NP-complexity theory."}
{"id": "2511.09001", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.09001", "abs": "https://arxiv.org/abs/2511.09001", "authors": ["Yuka Haruki", "Shigeru Ishikura", "Kazuya Demachi", "Teruaki Hayashi"], "title": "Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration", "comment": "10 pages", "summary": "As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications."}
{"id": "2511.09448", "categories": ["cs.MM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09448", "abs": "https://arxiv.org/abs/2511.09448", "authors": ["Lipisha Chaudhary", "Trisha Mittal", "Subhadra Gopalakrishnan", "Ifeoma Nwogu", "Jaclyn Pytlarz"], "title": "MCAD: Multimodal Context-Aware Audio Description Generation For Soccer", "comment": null, "summary": "Audio Descriptions (AD) are essential for making visual content accessible to individuals with visual impairments. Recent works have shown a promising step towards automating AD, but they have been limited to describing high-quality movie content using human-annotated ground truth AD in the process. In this work, we present an end-to-end pipeline, MCAD, that extends AD generation beyond movies to the domain of sports, with a focus on soccer games, without relying on ground truth AD. To address the absence of domain-specific AD datasets, we fine-tune a Video Large Language Model on publicly available movie AD datasets so that it learns the narrative structure and conventions of AD. During inference, MCAD incorporates multimodal contextual cues such as player identities, soccer events and actions, and commentary from the game. These cues, combined with input prompts to the fine-tuned VideoLLM, allow the system to produce complete AD text for each video segment. We further introduce a new evaluation metric, ARGE-AD, designed to accurately assess the quality of generated AD. ARGE-AD evaluates the generated AD for the presence of five characteristics: (i) usage of people's names, (ii) mention of actions and events, (iii) appropriate length of AD, (iv) absence of pronouns, and (v) overlap from commentary or subtitles. We present an in-depth analysis of our approach on both movie and soccer datasets. We also validate the use of this metric to quantitatively comment on the quality of generated AD using our metric across domains. Additionally, we contribute audio descriptions for 100 soccer game clips annotated by two AD experts."}
{"id": "2511.08788", "categories": ["cs.IT", "cs.CC", "math.AG"], "pdf": "https://arxiv.org/pdf/2511.08788", "abs": "https://arxiv.org/abs/2511.08788", "authors": ["Gil Cohen", "Dean Doron", "Noam Goldgraber", "Tomer Manket"], "title": "Tracing AG Codes: Toward Meeting the Gilbert-Varshamov Bound", "comment": null, "summary": "One of the oldest problems in coding theory is to match the Gilbert-Varshamov bound with explicit binary codes. Over larger-yet still constant-sized-fields, algebraic-geometry codes are known to beat the GV bound. In this work, we leverage this phenomenon by taking traces of AG codes. Our hope is that the margin by which AG codes exceed the GV bound will withstand the parameter loss incurred by taking the trace from a constant field extension to the binary field. In contrast to concatenation, the usual alphabet-reduction method, our analysis of trace-of-AG (TAG) codes uses the AG codes' algebraic structure throughout - including in the alphabet-reduction step.\n  Our main technical contribution is a Hasse-Weil-type theorem that is well-suited for the analysis of TAG codes. The classical theorem (and its Grothendieck trace-formula extension) are inadequate in this setting. Although we do not obtain improved constructions, we show that a constant-factor strengthening of our bound would suffice. We also analyze the limitations of TAG codes under our bound and prove that, in the high-distance regime, they are inferior to code concatenation. Our Hasse-Weil-type theorem holds in far greater generality than is needed for analyzing TAG codes. In particular, we derive new estimates for exponential sums."}
{"id": "2511.09531", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.09531", "abs": "https://arxiv.org/abs/2511.09531", "authors": ["Gregory Kehne", "Thomas Kesselheim"], "title": "Prophet and Secretary at the Same Time", "comment": null, "summary": "Many online problems are studied in stochastic settings for which inputs are samples from a known distribution, given in advance, or from an unknown distribution. Such distributions model both beyond-worst-case inputs and, when given, partial foreknowledge for the online algorithm. But how robust can such algorithms be to misspecification of the given distribution? When is this detectable, and when does it matter? When can algorithms give good competitive ratios both when the input distribution is as specified, and when it is not?\n  We consider these questions in the setting of optimal stopping, where the cases of known and unknown distributions correspond to the well-known prophet inequality and to the secretary problem, respectively. Here we ask: Can a stopping rule be competitive for the i.i.d. prophet inequality problem and the secretary problem at the same time? We constrain the Pareto frontier of simultaneous approximation ratios $(α, β)$ that a stopping rule can attain.\n  We introduce a family of algorithms that give nontrivial joint guarantees and are optimal for the extremal i.i.d. prophet and secretary problems. We also prove impossibilities, identifying $(α, β)$ unattainable by any adaptive stopping rule. Our results hold for both $n$ fixed arrivals and for arrivals from a Poisson process with rate $n$. We work primarily in the Poisson setting, and provide reductions between the Poisson and $n$-arrival settings that may be of broader interest."}
{"id": "2511.09062", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.09062", "abs": "https://arxiv.org/abs/2511.09062", "authors": ["Zhendong Guo", "Wenchao Bai", "Jiahui Jin"], "title": "Pricing Online LLM Services with Data-Calibrated Stackelberg Routing Game", "comment": "Accepted to AAAI-26 (Main Track). 9 pages main + appendix", "summary": "The proliferation of Large Language Models (LLMs) has established LLM routing as a standard service delivery mechanism, where users select models based on cost, Quality of Service (QoS), among other things. However, optimal pricing in LLM routing platforms requires precise modeling for dynamic service markets, and solving this problem in real time at scale is computationally intractable. In this paper, we propose \\PriLLM, a novel practical and scalable solution for real-time dynamic pricing in competitive LLM routing. \\PriLLM models the service market as a Stackelberg game, where providers set prices and users select services based on multiple criteria. To capture real-world market dynamics, we incorporate both objective factors (\\eg~cost, QoS) and subjective user preferences into the model. For scalability, we employ a deep aggregation network to learn provider abstraction that preserve user-side equilibrium behavior across pricing strategies. Moreover, \\PriLLM offers interpretability by explaining its pricing decisions. Empirical evaluation on real-world data shows that \\PriLLM achieves over 95\\% of the optimal profit while only requiring less than 5\\% of the optimal solution's computation time."}
{"id": "2511.09052", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.09052", "abs": "https://arxiv.org/abs/2511.09052", "authors": ["Yu Wang", "Hui Wang", "Jiake Ge", "Xin Wang"], "title": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking", "comment": "10 pages", "summary": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching."}
{"id": "2511.09054", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.09054", "abs": "https://arxiv.org/abs/2511.09054", "authors": ["Y. Tian", "C. Yue", "P. Cheng", "G. Pang", "B. Vucetic", "Y. Li"], "title": "Policy-Guided MCTS for near Maximum-Likelihood Decoding of Short Codes", "comment": null, "summary": "In this paper, we propose a policy-guided Monte Carlo Tree Search (MCTS) decoder that achieves near maximum-likelihood decoding (MLD) performance for short block codes. The MCTS decoder searches for test error patterns (TEPs) in the received information bits and obtains codeword candidates through re-encoding. The TEP search is executed on a tree structure, guided by a neural network policy trained via MCTS-based learning. The trained policy guides the decoder to find the correct TEPs with minimal steps from the root node (all-zero TEP). The decoder outputs the codeword with maximum likelihood when the early stopping criterion is satisfied. The proposed method requires no Gaussian elimination (GE) compared to ordered statistics decoding (OSD) and can reduce search complexity by 95\\% compared to non-GE OSD. It achieves lower decoding latency than both OSD and non-GE OSD at high SNRs."}
{"id": "2511.09435", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09435", "abs": "https://arxiv.org/abs/2511.09435", "authors": ["Francesco Morri", "Hélène Le Cadre", "David Salas", "Didier Aussel"], "title": "Steering Noncooperative Games Through Conjecture Design", "comment": null, "summary": "In dynamic noncooperative games, each player makes conjectures about other players' reactions before choosing a strategy. However, resulting equilibria may be multiple and do not always lead to desirable outcomes. These issues are typically addressed separately, for example, through opponent modelling and incentive design. Drawing inspiration from conjectural variations games, we propose an incentive design framework in which a coordinator first computes an equilibrium by optimizing a predefined objective function, then communicates this equilibrium as a target for the players to reach. In a centralized setting, the coordinator also optimizes the conjectures to steer the players towards the target. In decentralized settings, players independently compute conjectures and update their strategies based on individual targets. We provide a guarantee of equilibrium existence in both cases. This framework uses conjectures not only to guide the system towards desirable outcomes but also to decouple the game into independent optimization problems, enabling efficient computation and parallelization in large-scale settings. We illustrate our theoretical results on classical representative noncooperative games, demonstrating its application potential."}
{"id": "2511.09262", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09262", "abs": "https://arxiv.org/abs/2511.09262", "authors": ["Jiaping Cao", "Ting Sun", "Man Lung Yiu", "Xiao Yan", "Bo Tang"], "title": "CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System", "comment": null, "summary": "Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS."}
{"id": "2511.09070", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.09070", "abs": "https://arxiv.org/abs/2511.09070", "authors": ["Wing Shing Wong", "Chung Shue Chen", "Yuan-Hsun Lo"], "title": "Color Multiset Codes based on Sunmao Construction", "comment": "This work has been submitted for possible publication", "summary": "We present results on coding using multisets instead of ordered sequences. The study is motivated by a moving object tracking problem in a sensor network and can find applications in settings where the order of the symbols in a codeword cannot be maintained or observed. In this paper a multiset coding scheme is proposed on source data that can be organized as a flat or cyclic multi-dimensional integer lattice (grid). A fundamental idea in the solution approach is to decompose the original source data grid into sub-grids. The original multiset coding problem can then be restricted to each of the sub-grid. Solutions for the sub-grids are subsequently piece together to form the desired solution. We name this circle of idea as sunmao construction in reference to woodwork construction method with ancient origin. Braid codes are specific solutions defined using the sunmao construction. They are easy to define for multi-dimensional grids. Moreover for a code of a given code set size and multiset cardinality, if we measure coding efficiency by the number of distinct symbols required, then braid codes have asymptotic order equal to those that are optimal. We also show that braid codes have interesting inherent error correction properties."}
{"id": "2511.09479", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.09479", "abs": "https://arxiv.org/abs/2511.09479", "authors": ["Niclas Boehmer", "Lara Glessen", "Jannik Peters"], "title": "Understanding the Impact of Proportionality in Approval-Based Multiwinner Elections", "comment": "Accepted to AAAI '26", "summary": "Despite extensive theoretical research on proportionality in approval-based multiwinner voting, its impact on which committees and candidates can be selected in practice remains poorly understood. We address this gap by (i) analyzing the computational complexity of several natural problems related to the behavior of proportionality axioms, and (ii) conducting an extensive experimental study on both real-world and synthetic elections. Our findings reveal substantial variation in the restrictiveness of proportionality across instances, including previously unobserved high levels of restrictiveness in some real-world cases. We also introduce and evaluate new measures for quantifying a candidate's importance for achieving proportional outcomes, which differ clearly from assessing candidate strength by approval score."}
{"id": "2511.09221", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09221", "abs": "https://arxiv.org/abs/2511.09221", "authors": ["Vukan Ninkovic", "Dejan Vukobratovic"], "title": "Learning Binary Autoencoder-Based Codes with Progressive Training", "comment": "Invited paper at TELFOR 2025", "summary": "Error correcting codes play a central role in digital communication, ensuring that transmitted information can be accurately reconstructed despite channel impairments. Recently, autoencoder (AE) based approaches have gained attention for the end-to-end design of communication systems, offering a data driven alternative to conventional coding schemes. However, enforcing binary codewords within differentiable AE architectures remains difficult, as discretization breaks gradient flow and often leads to unstable convergence. To overcome this limitation, a simplified two stage training procedure is proposed, consisting of a continuous pretraining phase followed by direct binarization and fine tuning without gradient approximation techniques. For the (7,4) block configuration over a binary symmetric channel (BSC), the learned encoder-decoder pair learns a rotated version (coset code) of the optimal Hamming code, naturally recovering its linear and distance properties and thereby achieving the same block error rate (BLER) with maximum likelihood (ML) decoding. These results indicate that compact AE architectures can effectively learn structured, algebraically optimal binary codes through stable and straightforward training."}
{"id": "2511.09531", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.09531", "abs": "https://arxiv.org/abs/2511.09531", "authors": ["Gregory Kehne", "Thomas Kesselheim"], "title": "Prophet and Secretary at the Same Time", "comment": null, "summary": "Many online problems are studied in stochastic settings for which inputs are samples from a known distribution, given in advance, or from an unknown distribution. Such distributions model both beyond-worst-case inputs and, when given, partial foreknowledge for the online algorithm. But how robust can such algorithms be to misspecification of the given distribution? When is this detectable, and when does it matter? When can algorithms give good competitive ratios both when the input distribution is as specified, and when it is not?\n  We consider these questions in the setting of optimal stopping, where the cases of known and unknown distributions correspond to the well-known prophet inequality and to the secretary problem, respectively. Here we ask: Can a stopping rule be competitive for the i.i.d. prophet inequality problem and the secretary problem at the same time? We constrain the Pareto frontier of simultaneous approximation ratios $(α, β)$ that a stopping rule can attain.\n  We introduce a family of algorithms that give nontrivial joint guarantees and are optimal for the extremal i.i.d. prophet and secretary problems. We also prove impossibilities, identifying $(α, β)$ unattainable by any adaptive stopping rule. Our results hold for both $n$ fixed arrivals and for arrivals from a Poisson process with rate $n$. We work primarily in the Poisson setting, and provide reductions between the Poisson and $n$-arrival settings that may be of broader interest."}
{"id": "2511.09251", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.09251", "abs": "https://arxiv.org/abs/2511.09251", "authors": ["Lan Ma", "Qifu Tyler Sun", "Shaoteng Liu", "Liyang Zhou"], "title": "Generic Construction of Optimal-Access Binary MDS Array Codes with Smaller Sub-packetization", "comment": null, "summary": "A $(k+r,k,l)$ binary array code of length $k+r$, dimension $k$, and sub-packetization $l$ is composed of $l\\times(k+r)$ matrices over $\\mathbb{F}_2$, with every column of the matrix stored on a separate node in the distributed storage system and viewed as a coordinate of the codeword. It is said to be maximum distance separable (MDS) if any $k$ out of $k+r$ coordinates suffice to reconstruct the whole codeword. The repair problem of binary MDS array codes has drawn much attention, particularly for single-node failures. In this paper, given an arbitrary binary MDS array code with sub-packetization $m$ as the base code, we propose two generic approaches (Generic Construction I and II) for constructing binary MDS array codes with optimal access (or repair) bandwidth for single-node failures. For every $s\\leq r$, a $(k+r,k,ms^{\\lceil \\frac{k+r}{s}\\rceil})$ code $\\mathcal{C}_1$ with optimal access bandwidth can be constructed by Generic Construction I. Repairing a failed node of $\\mathcal{C}_1$ requires connecting to $d = k+s-1$ helper nodes, in which $s-1$ helper nodes are designated and $k$ are free to select. $\\mathcal{C}_1$ generally achieves smaller sub-packetization and provides greater flexibility in the selection of its coefficient matrices. For even $r\\geq4$ and $s=\\frac{r}{2}$ such that $s+1$ divides $k+r$, a $(k+r, k,ms^{\\frac{k+r}{s+1}})$ code $\\mathcal{C}_2$ with optimal repair bandwidth can be constructed by Generic Construction II, with $\\frac{s}{s+1}(k+r)$ out of $k+r$ nodes having the optimal access property. To the best of our knowledge, $\\mathcal{C}_2$ possesses the smallest sub-packetization among existing binary MDS array codes with optimal repair bandwidth known to date."}
{"id": "2511.09384", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.09384", "abs": "https://arxiv.org/abs/2511.09384", "authors": ["Matteo Nerini", "Bruno Clerckx"], "title": "Enabling Smart Radio Environments in the Frequency Domain With Movable Signals", "comment": "Submitted to IEEE for publication", "summary": "Smart radio environments (SREs) enhance wireless communications by allowing control over the channel. They have been enabled through surfaces with reconfigurable electromagnetic (EM) properties, known as reconfigurable intelligent surfaces (RISs), and through flexible antennas, which can be viewed as realizations of SREs in the EM domain and space domain, respectively. However, these technologies rely on electronically reconfigurable or movable components, introducing implementation challenges that could hinder commercialization. To overcome these challenges, we propose a new domain to enable SREs, the frequency domain, through the concept of movable signals, where the signal spectrum can be dynamically moved along the frequency axis. We first analyze movable signals in multiple-input single-output (MISO) systems under line-of-sight (LoS) conditions, showing that they can achieve higher average received power than quantized equal gain transmission (EGT). We then study movable signals under non-line-of-sight (NLoS) conditions, showing that they remain effective by leveraging reflections from surfaces made of uniformly spaced elements with fixed EM properties, denoted as fixed intelligent surfaces (FISs). Analytical results reveal that a FIS-aided system using movable signals can achieve up to four times the received power of a RIS-aided system using fixed-frequency signals."}
{"id": "2511.09412", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.09412", "abs": "https://arxiv.org/abs/2511.09412", "authors": ["Jonathan E. W. Huffmann", "Holger Boche"], "title": "Computability of the Optimizer for Rate Distortion Functions", "comment": null, "summary": "Rate distortion theory treats the problem of encoding a source with minimum codebook size while at the same time allowing for a certain amount of errors in the reconstruction measured by a fidelity criterion and distortion level. Similar to the channel coding problem the optimal rate of the codebook with respect to the blocklength is given by a convex optimization problem involving information theoretic quantities like mutual information. The value of the rate in dependence of the distortion level as well as the optimizer used in the codebook construction are of theoretical and practical importance in communication and information theory. In this paper the behavior of the rate distortion function regarding the computability of the optimizing test channel is investigated. We find that comparable with known results about the optimizer for other information theoretic problems a similar result is found to be true also regarding the computability of the optimizer for rate distortion functions.\n  It turns out that while the rate distortion function is usually computable the optimizer for this problem is in general non-computable even for simple distortion measures."}
