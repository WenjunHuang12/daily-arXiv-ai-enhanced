<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics](https://arxiv.org/abs/2601.15484)
*Philipp Eibl,Erica Coppolillo,Simone Mungari,Luca Luceri*

Main category: cs.IR

TL;DR: 比较分析维基百科和Grokipedia在政治争议话题上的语义框架、政治倾向和内容优先级差异，发现两者语义相似度随文章章节递减，在争议话题上分歧更大，且两者都主要呈现左倾框架，但Grokipedia呈现更明显的双峰分布，右倾内容更突出。


<details>
  <summary>Details</summary>
Motivation: 在线百科全书是现代信息基础设施的核心，已成为意识形态偏见辩论的焦点。维基百科长期被指责存在左倾偏见，而xAI推出的AI生成百科全书Grokipedia则被视为右倾替代品。本研究旨在比较分析这两个平台在政治争议话题上的差异。

Method: 对维基百科和Grokipedia在已确立的政治争议话题上进行比较分析，具体考察语义框架、政治倾向和内容优先级的差异。分析语义相似度随文章章节的变化，比较争议话题与随机抽样话题的差异。

Result: 1. 两个平台间的语义相似度随文章章节递减；2. 在争议话题上的分歧比随机抽样话题更大；3. 两个百科全书都主要呈现左倾框架；4. Grokipedia呈现更明显的双峰分布，右倾内容更突出。

Conclusion: 研究揭示了维基百科和Grokipedia在政治争议话题处理上的系统性差异，表明AI生成的百科全书可能强化而非消除意识形态偏见，为理解在线信息生态系统中的偏见动态提供了实证基础。

Abstract: Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available.

</details>


### [2] [DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking](https://arxiv.org/abs/2601.15518)
*Wenxin Zhou,Ritesh Mehta,Anthony Miyaguchi*

Main category: cs.IR

TL;DR: 开发两阶段检索系统，结合多种检索方法与学习型重排器和LLM重排，用于TREC Tip-of-the-Tongue任务，通过混合检索和LLM重排达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 解决TREC Tip-of-the-Tongue（ToT）任务的检索挑战，该任务需要从记忆中检索难以回忆的信息，需要结合多种检索方法提高召回率和准确性

Method: 两阶段检索系统：第一阶段采用混合检索，结合LLM检索、稀疏检索（BM25）和稠密检索（BGE-M3），并引入主题感知多索引稠密检索；第二阶段使用训练好的LambdaMART重排器和LLM重排。使用LLM生成5000个合成ToT查询支持模型训练

Result: 最佳系统在测试集上达到0.66的召回率和0.41的NDCG@1000，通过混合检索与Gemini-2.5-flash重排相结合，证明了融合检索的有效性

Conclusion: 两阶段检索系统结合多种互补检索方法和LLM重排能有效解决ToT任务，混合检索与LLM重排的组合表现出色，为难以回忆的信息检索提供了有效解决方案

Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.

</details>


### [3] [Blockchain-Based Spectrum Resource Securitization via Semi-Fungible Token-Lock](https://arxiv.org/abs/2601.15594)
*Zhixian Zhou,Bin Chen,Zhe Peng,Zhiming Liang,Ruijun Wu,Chen Sun,Shuo Wang*

Main category: cs.IR

TL;DR: 本文提出SFT Lock方法，通过锁/解锁机制替代铸造/销毁操作，在保持NFT身份和历史可追溯性的同时实现频谱资产的分割所有权和可转让性，显著降低链上开销。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络发展，频谱资产需要灵活、动态、高效的利用，推动了基于区块链的频谱证券化。现有的ERC404混合代币模型在资产转移时频繁进行铸造和销毁操作，这会破坏代币身份连续性并增加链上开销。

Method: 提出Semi Fungible Token Lock (SFT Lock)方法，采用锁/解锁机制替代铸造/销毁操作，通过确定性状态转换保持NFT身份和历史可追溯性。设计了模块化智能合约架构支持频谱授权、证券化和共享，并引入质押机制增强资产流动性。

Result: 在私有以太坊网络上的实验结果表明，相比ERC404混合代币模型，所提方法实现了显著的气体节省，同时保持了功能正确性和可追溯性。

Conclusion: SFT Lock方法通过锁/解锁机制有效解决了ERC404模型中的身份连续性问题，减少了链上操作开销，为6G频谱资产的区块链证券化提供了更高效的解决方案。

Abstract: As 6G networks evolve, spectrum assets require flexible, dynamic, and efficient utilization, motivating blockchain based spectrum securitization. Existing approaches based on ERC404 style hybrid token models rely on frequent minting and burning during asset transfers, which disrupt token identity continuity and increase on chain overhead. This paper proposes the Semi Fungible Token Lock (SFT Lock) method, a lock/unlock based mechanism that preserves NFT identity and historical traceability while enabling fractional ownership and transferability. By replacing mint/burn operations with deterministic state transitions, SFT Lock ensures consistent lifecycle representation of spectrum assets and significantly reduces on chain operations. Based on this mechanism, a modular smart contract architecture is designed to support spectrum authorization, securitization, and sharing, and a staking mechanism is introduced to enhance asset liquidity. Experimental results on a private Ethereum network demonstrate that, compared with ERC404 style hybrid token models, the proposed method achieves substantial gas savings while maintaining functional correctness and traceability.

</details>


### [4] [Enhancing guidance for missing data in diffusion-based sequential recommendation](https://arxiv.org/abs/2601.15673)
*Qilong Yan,Yifei Xing,Dugang Liu,Jingpu Duan,Jian Yin*

Main category: cs.IR

TL;DR: 提出CARD模型，通过反事实注意力机制增强用户兴趣转折点信号，抑制序列噪声，提升扩散模型在序列推荐中的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法从分类转向扩散生成范式，但用户序列中的缺失数据导致引导信号质量下降。现有方法去除局部相似项，但忽略了用户兴趣的"关键转折点"，这些转折点对准确预测后续用户意图至关重要。

Method: 提出CARD模型，包含：(1) 双面汤普森采样方法识别经历显著兴趣转移的序列；(2) 对这些序列使用反事实注意力机制量化每个项目的重要性。通过动态重新加权的交互向量为扩散模型提供高质量引导信号。

Result: 实验表明该方法在真实世界数据上表现良好，且计算成本不高。

Conclusion: CARD模型通过增强兴趣转折点信号和抑制噪声，有效提升了扩散模型在序列推荐中的生成质量。

Abstract: Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.

</details>


### [5] [CoNRec: Context-Discerning Negative Recommendation with LLMs](https://arxiv.org/abs/2601.15721)
*Xinda Chen,Jiawei Wu,Yishuang Liu,Jialin Zhu,Shuwen Xiao,Junjun Zheng,Xiangheng Kong,Yuning Jiang*

Main category: cs.IR

TL;DR: 提出了首个用于负反馈建模的大语言模型框架，通过语义ID表示、渐进式GRPO训练和新型奖励函数，解决负反馈数据稀疏性和传统目标与用户真实负偏好不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要将负反馈作为辅助信号来增强正推荐，很少直接建模负兴趣，且由于负反馈数据稀疏性，模型易受正反馈主导的上下文理解偏差影响。传统负项预测目标与用户真实负偏好存在根本性不匹配。

Method: 1) 使用语义ID表示替代基于文本的物品描述；2) 引入物品级对齐任务增强LLM对负反馈语义上下文的理解；3) 设计渐进式GRPO训练范式动态平衡正负行为上下文利用；4) 提出基于多日未来负反馈及其协同信号的新型奖励函数和评估指标。

Result: 论文提出了首个负反馈建模的LLM框架，通过上下文辨别模块、语义ID表示和渐进式训练，有效缓解了负反馈稀疏性问题，并解决了传统目标与真实负偏好的不匹配问题。

Conclusion: 该框架为负反馈建模提供了新的解决方案，通过增强LLM对负反馈语义上下文的理解和动态平衡正负行为上下文，能够更准确地捕捉用户真实负偏好，为推荐系统性能评估提供了更可靠的指标。

Abstract: Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.

</details>


### [6] [CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval](https://arxiv.org/abs/2601.15849)
*Tsung-Hsiang Chou,Chen-Jui Yu,Shui-Hsiang Hsu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: CGPT：通过LLM生成监督信号增强表格检索的训练框架，使用聚类构建语义多样的部分表格，生成合成查询进行对比学习，在多个基准测试中显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 通用嵌入模型在文本检索中表现良好，但在表格检索中效果不佳，因为高度结构化的内容导致语义压缩和查询-表格不匹配。现有LLM检索增强方法虽然通过生成合成查询缓解问题，但通常依赖启发式的部分表格选择，且很少利用这些合成查询作为监督信号来改进嵌入模型。

Method: CGPT训练框架：1）通过K-means聚类表格实例，跨集群采样构建语义多样的部分表格；2）使用LLM为这些部分表格生成合成查询；3）通过硬负样本对比微调，利用合成查询监督信号优化嵌入模型。

Result: 在四个公开基准测试（MimoTable、OTTQA、FetaQA、E2E-WTQ）上，CGPT持续优于检索基线（包括QGpT），平均R@1提升16.54%。在统一多领域语料设置中，CGPT展现出强大的跨领域泛化能力，即使使用较小的LLM生成合成查询也保持有效。

Conclusion: 语义引导的部分表格构建，结合LLM生成监督信号的对比训练，为大规模表格检索提供了有效且可扩展的范式。该方法显著提升了表格检索性能，具有良好的泛化能力。

Abstract: General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.

</details>


### [7] [STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion](https://arxiv.org/abs/2601.15860)
*Shui-Hsiang Hsu,Tsung-Hsiang Chou,Chen-Jui Yu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: STAR框架通过语义聚类和加权融合改进表格表示，在五个基准测试中比QGpT获得更高的召回率


<details>
  <summary>Details</summary>
Motivation: 表格检索面临非结构化文本与结构化表格之间的结构和语义差异挑战，现有方法如QGpT依赖粗糙的部分表格采样和简单融合策略，限制了语义多样性和查询-表格对齐效果

Method: STAR框架采用语义聚类和加权融合：1) 基于表头感知的K-means聚类分组语义相似行，选择代表性中心实例构建多样化部分表格；2) 生成聚类特定的合成查询全面覆盖表格语义空间；3) 使用加权融合策略整合表格和查询嵌入，实现细粒度语义对齐

Result: 在五个基准测试中，STAR在所有数据集上均比QGpT获得更高的Recall，证明了语义聚类和自适应加权融合对鲁棒表格表示的有效性

Conclusion: STAR通过语义聚类和加权融合有效改善了表格语义表示，能够从结构和文本源中捕获互补信息，提高表格表示的表达能力

Abstract: Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.

</details>


### [8] [MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging](https://arxiv.org/abs/2601.15930)
*Tianjun Wei,Enneng Yang,Yingpeng Du,Huizhong Guo,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 首次系统研究生成式推荐中的模型合并，提出MMGRid框架分析跨上下文（时间演化与领域多样性）的模型合并，发现参数冲突可通过基础模型替换缓解，增量训练导致的近期偏差可通过加权上下文合并平衡。


<details>
  <summary>Details</summary>
Motivation: 模型合并为整合多个专业模型提供了高效机制，但在推荐系统中尚未充分探索。生成式推荐模型规模快速增长且计算成本高昂，模型合并对于成本敏感部署场景特别有吸引力。现实世界中存在因用户行为时间演化和应用领域异构性导致的不同上下文专业模型，如何合并这些模型是基础但未充分探索的挑战。

Method: 提出MMGRid框架，构建结构化的上下文网格，组织在时间演化和领域多样性诱导的多样化上下文中训练的生成式推荐检查点。所有检查点源自共享基础LLM但在特定上下文数据上微调，形成现实且受控的模型空间，用于系统分析跨生成式推荐范式和合并算法的模型合并。

Result: 研究发现：1）从LLM训练生成式推荐模型会因token分布偏移和目标差异引入参数冲突，可通过基础模型替换解耦任务感知和上下文特定参数变化来缓解；2）跨上下文的增量训练诱导近期偏差，可通过加权上下文合并有效平衡；3）最优合并权重与上下文依赖的交互特征相关，为实际部署中的权重选择提供实用指导。

Conclusion: 这是首次系统研究生成式推荐中的模型合并，揭示了参数冲突和近期偏差等关键挑战，并提出相应解决方案。研究结果为实际部署中的模型合并提供了实用指导，特别是权重选择与上下文特征的相关性发现。

Abstract: Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.

</details>


### [9] [Unveiling and Simulating Short-Video Addiction Behaviors via Economic Addiction Theory](https://arxiv.org/abs/2601.15975)
*Chen Xu,Zhipeng Yi,Ruizi Wang,Wenjie Wang,Jun Xu,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文提出AddictSim框架，结合经济成瘾理论和推荐系统行为数据，分析短视频成瘾模式，并通过多样性算法缓解成瘾行为


<details>
  <summary>Details</summary>
Motivation: 短视频平台用户流量大但存在成瘾问题，传统问卷研究样本小且有偏差，平台行为数据为分析成瘾行为提供了新机会

Method: 结合经济成瘾理论和推荐系统隐式行为数据，提出AddictSim训练框架，采用均值适应策略和群体相对策略优化

Result: 短视频成瘾遵循传统成瘾行为的功能模式，AddictSim在两个大规模数据集上优于现有方法，多样性算法能有效缓解成瘾行为

Conclusion: 利用平台行为数据和成瘾理论可以更好地理解和建模短视频成瘾，AddictSim框架为分析个性化成瘾模式提供了有效工具

Abstract: Short-video applications have attracted substantial user traffic. However, these platforms also foster problematic usage patterns, commonly referred to as short-video addiction, which pose risks to both user health and the sustainable development of platforms. Prior studies on this issue have primarily relied on questionnaires or volunteer-based data collection, which are often limited by small sample sizes and population biases. In contrast, short-video platforms have large-scale behavioral data, offering a valuable foundation for analyzing addictive behaviors. To examine addiction-aware behavior patterns, we combine economic addiction theory with users' implicit behavior captured by recommendation systems. Our analysis shows that short-video addiction follows functional patterns similar to traditional forms of addictive behavior (e.g., substance abuse) and that its intensity is consistent with findings from previous social science studies. To develop a simulator that can learn and model these patterns, we introduce a novel training framework, AddictSim. To consider the personalized addiction patterns, AddictSim uses a mean-to-adapted strategy with group relative policy optimization training. Experiments on two large-scale datasets show that AddictSim consistently outperforms existing training strategies. Our simulation results show that integrating diversity-aware algorithms can mitigate addictive behaviors well.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [10] [Do people expect different behavior from large language models acting on their behalf? Evidence from norm elicitations in two canonical economic games](https://arxiv.org/abs/2601.15312)
*Paweł Niszczota,Elia Antoniou*

Main category: cs.GT

TL;DR: 研究通过经济游戏发现，人们对于LLM代理决策与人类决策有不同的社会规范期望，特别是在资源分配和公平性监督方面存在差异。


<details>
  <summary>Details</summary>
Motivation: 虽然将任务委托给大型语言模型可以节省时间，但有证据表明这种委托会产生社会成本。研究旨在探索当LLM代替人类做出决策时，人们的社会规范期望是否发生变化。

Method: 使用两种经典经济游戏（独裁者游戏和最后通牒游戏），通过Krupka-Weber规范诱导任务测量社会适当性评分的变化。在英国和美国进行了两项预先注册的激励实验，样本量N=2,658。

Result: 1. 当不需要接受时，机器提出的报价被认为比人类提出的更不适当（尽管众数响应没有变化）。2. 当需要接受时，人们拒绝机器报价比拒绝人类报价更适当。3. 收到机器的拒绝与收到人类的拒绝在社会适当性上没有差异。

Conclusion: 人们对机器决定如何分配资源应用不同的规范，但不反对机器执行规范。这些发现与机器报价现在被视为同时具有认知和情感成分的观点一致。

Abstract: While delegating tasks to large language models (LLMs) can save people time, there is growing evidence that offloading tasks to such models produces social costs. We use behavior in two canonical economic games to study whether people have different expectations when decisions are made by LLMs acting on their behalf instead of themselves. More specifically, we study the social appropriateness of a spectrum of possible behaviors: when LLMs divide resources on our behalf (Dictator Game and Ultimatum Game) and when they monitor the fairness of splits of resources (Ultimatum Game). We use the Krupka-Weber norm elicitation task to detect shifts in social appropriateness ratings. Results of two pre-registered and incentivized experimental studies using representative samples from the UK and US (N = 2,658) show three key findings. First, people find that offers from machines - when no acceptance is necessary - are judged to be less appropriate than when they come from humans, although there is no shift in the modal response. Second - when acceptance is necessary - it is more appropriate for a person to reject offers from machines than from humans. Third, receiving a rejection of an offer from a machine is no less socially appropriate than receiving the same rejection from a human. Overall, these results suggest that people apply different norms for machines deciding on how to split resources but are not opposed to machines enforcing the norms. The findings are consistent with offers made by machines now being viewed as having both a cognitive and emotional component.

</details>


### [11] [On the closest balanced game](https://arxiv.org/abs/2601.15318)
*Pedro García-Segador,Michel Grabisch,Dylan Laplace Mermoud,Pedro Miranda*

Main category: cs.GT

TL;DR: 该论文提出了一种寻找最接近平衡博弈的算法，并证明当玩家数量增加时，最接近平衡博弈的核心趋于单点解


<details>
  <summary>Details</summary>
Motivation: 对于核心为空的合作博弈，需要找到最接近的平衡博弈（即具有非空核心的博弈），以定义新的解概念

Method: 提出快速算法寻找最接近平衡博弈（欧几里得距离下的正交投影），避免优化问题的指数复杂度，可处理最多20个玩家

Result: 实验显示当玩家数量增加时，最接近博弈具有单点核心的概率趋于1；数学证明证明了这一实验结果

Conclusion: 基于投影博弈的核心定义了一个新的解概念（最小二乘核心），当玩家数量增加时，该解趋于点解的概率为1

Abstract: Cooperative games with nonempty core are called balanced, and the set of balanced games is a polyhedron. Given a game with empty core, we look for the closest balanced game, in the sense of the (weighted) Euclidean distance, i.e., the orthogonal projection of the game on the set of balanced games. Besides an analytical approach which becomes rapidly intractable, we propose a fast algorithm to find the closest balanced game, avoiding exponential complexity for the optimization problem, and being able to run up to 20 players. We show experimentally that the probability that the closest game has a core reduced to a singleton tends to 1 when the number of players grow. We provide a mathematical proof that the proportion of facets whose games have a non-singleton core tends to 0 when the number of players grow, by finding an expression of the aymptotic growth of the number of minimal balanced collections. This permits to prove mathematically the experimental result. Consequently, taking the core of the projected game defines a new solution concept, which we call least square core due to its analogy with the least core, and our result shows that the probability that this is a point solution tends to 1 when the number of players grow.

</details>


### [12] [Rules Create Unequal Rewards: Elite Tennis Players Allocate Resources Efficiently](https://arxiv.org/abs/2601.15327)
*Masatsugu Yoshizawa,Yuta Kawamoto,Daisuke Takeshita*

Main category: cs.GT

TL;DR: 顶级网球运动员在比赛中能更高效地分配精力，在比分落后时理性放弃当前局以节省体能，这种策略使他们更接近帕累托最优边界。


<details>
  <summary>Details</summary>
Motivation: 研究竞争环境中资源分配效率与专业表现的关系。许多竞争环境（教育、政治等）存在阈值效应，但难以直接观察努力分配。网球比赛提供了理想的自然实验环境，因为每局比赛重置且输局中的得分无效，不同比分下得分的价值差异显著。

Method: 利用职业网球比赛数据，分析不同比分下球员的得分概率。通过构建帕累托前沿（理论上的得分概率与局胜率最优权衡边界），评估球员的实际表现与最优策略的接近程度。特别关注发球局和接发球局中的策略差异。

Result: 顶级球员更接近帕累托前沿，能更高效地将得分转化为局胜利。最优策略显示：当比分大幅落后时（如0-2、0-3），应降低得分概率以节省体能。顶级球员尤其在接发球局中表现出这种模式，因为接发球得分更难，需要更大程度地调整努力程度。

Conclusion: 精英表现反映了对规则创造的价值结构的高效适应。知道何时放弃与知道何时竞争同样重要，这是专业能力的根本体现。研究揭示了在阈值驱动的竞争环境中，理性资源分配是专业表现的关键因素。

Abstract: In many competitive settings, from education to politics, rules do not reward effort evenly, and thresholds (e.g., grade cutoffs or electoral majorities) make some moments disproportionately important. Success thus depends on efficiently allocating limited resources. However, empirical demonstration has been difficult because effort allocation is rarely observable and feedback is often delayed, limiting our understanding of expertise. Professional tennis provides an ideal natural experiment. Because each game resets after a player wins four points and points in a lost game are wasted, the value of a point varies sharply across scores. Efficient allocation should therefore win games without wasting points, conserving resources for future games. Such allocation manifests in score-dependent point-winning probabilities, from which we derive each player's Pareto frontier-the theoretical limit of the trade-off between game-winning probability and the expected points per game. Here, we show that top players operate closer to this frontier, converting points to game wins more efficiently. Optimal strategies reduce the probability of winning points when the player is far behind (e.g.,0-2, 0-3). This behavior is psychologically difficult-letting go of the current game-but represents a rational energy conservation strategy. Top players exhibit this pattern especially in return games, where winning points is harder than in service games, requiring them to drastically vary their efforts, consistent with game-theoretic predictions. These findings suggest that elite performance reflects efficient adaptation to rule-created value structures; knowing when to give up may be as fundamental to expertise as knowing when to compete.

</details>


### [13] [Equal-Pay Contracts](https://arxiv.org/abs/2601.15478)
*Michal Feldman,Yoav Gal-Tzur,Tomasz Ponitka,Maya Schlesinger*

Main category: cs.GT

TL;DR: 研究多智能体合同设计，关注公平支付约束下的激励问题，提出近似算法和硬度结果，并量化公平性带来的效用损失。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多环境限制支付差异，而先前研究主要关注无约束的异质支付合同。因此需要研究平等支付合同（所有代理获得相同报酬）及其近似变体。

Method: 研究平等支付合同和近似平等支付合同，提供算法和硬度结果，涵盖奖励函数层次结构（包括次模、XOS等），在二元和组合行动模型下分析。定义并分析"平等价格"来衡量公平性带来的效用损失。

Result: 设计了多项式时间O(1)近似算法：(1)组合行动下的次模奖励；(2)二元行动下的XOS奖励。证明了这些保证是紧的：排除了组合行动下PTAS的存在（即使对于总替代奖励），以及组合行动下XOS奖励的任何O(1)近似。获得了平等价格的紧界Θ(log n/log log n)。

Conclusion: 平等支付合同在多智能体激励设计中具有实际意义，虽然会带来效用损失，但损失有界。研究不仅解决了平等支付合同的问题，也为无约束合同设计提供了新见解，解决了两个重要的开放问题。

Abstract: We study multi-agent contract design, where a principal incentivizes a team of agents to take costly actions that jointly determine the project success via a combinatorial reward function. While prior work largely focuses on unconstrained contracts that allow heterogeneous payments across agents, many real-world environments limit payment dispersion. Motivated by this, we study equal-pay contracts, where all agents receive identical payments. Our results also extend to nearly-equal-pay contracts where any two payments are identical up to a constant factor.
  We provide both algorithmic and hardness results across a broad hierarchy of reward functions, under both binary and combinatorial action models. While we focus on equal-pay contracts, our analysis also yields new insights into unconstrained contract design, and resolves two important open problems. On the positive side, we design polynomial-time O(1)-approximation algorithms for (i) submodular rewards under combinatorial actions, and (ii) XOS rewards under binary actions. These guarantees are tight: We rule out the existence of (i) a PTAS for combinatorial actions, even for gross substitutes rewards (unless P = NP), and (ii) any O(1)-approximation for XOS rewards with combinatorial actions. Crucially, our hardness results hold even for unconstrained contracts, thereby settling the corresponding open problems in this setting.
  Finally, we quantify the loss induced by fairness via the price of equality, defined as the worst-case ratio between the optimal principal's utility achievable by unconstrained contracts and that achievable by equal-pay contracts. We obtain a bound of $Θ(\log n/ \log \log n)$, where $n$ is the number of agents. This gap is tight in a strong sense: the upper bound applies even for XOS rewards with combinatorial actions, while the lower bound arises already for additive rewards with binary actions.

</details>


### [14] [How to Tamper with a Parliament: Strategic Campaigns in Apportionment Elections](https://arxiv.org/abs/2601.15855)
*Robert Bredereck,Piotr Faliszewski,Michał Furdyna,Andrzej Kaczmarczyk,Joanna Kaczmarek,Martin Lackner,Christian Laußmann,Jörg Rothe,Tessa Seeger*

Main category: cs.GT

TL;DR: 研究议会选举中通过有限选民改变投票来操纵选举结果的战略竞选攻击的计算复杂性，包括阈值和多选区系统的影响，并提出第二机会投票模式


<details>
  <summary>Details</summary>
Motivation: 议会选举中，政党通过席位分配方法获得席位，许多国家设有选举门槛防止小党进入议会，还有多选区系统。研究通过有限选民改变投票来操纵选举结果的战略竞选攻击的计算复杂性，对选举安全具有重要意义

Method: 研究战略竞选问题的计算复杂性（经典和参数化复杂度），在真实选举数据上进行广泛实验，分析最优竞选策略的效果，并与启发式策略比较，研究阈值和选区数量的影响，提出第二机会投票模式作为替代方案

Result: 建立了战略竞选问题的计算复杂性结果，通过实验验证了最优竞选策略的有效性，分析了阈值和选区数量对竞选效果的影响，提出了第二机会投票模式并建立了其计算复杂性结果

Conclusion: 战略竞选攻击在议会选举中具有重要的计算复杂性特征，选举门槛和选区结构显著影响操纵难度，第二机会投票模式为选举系统设计提供了新的选择，需要进一步研究选举系统的抗操纵性

Abstract: In parliamentary elections, parties compete for a limited, typically fixed number of seats. Most parliaments are assembled using apportionment methods that distribute the seats based on the parties' vote counts. Common apportionment methods include divisor sequence methods (like D'Hondt or Sainte-Laguë), the largest-remainder method, and first-past-the-post. In many countries, an electoral threshold is implemented to prevent very small parties from entering the parliament. Further, several countries have apportionment systems that incorporate multiple districts. We study how computationally hard it is to change the election outcome (i.e., to increase or limit the influence of a distinguished party) by convincing a limited number of voters to change their vote. We refer to these bribery-style attacks as \emph{strategic campaigns} and study the corresponding problems in terms of their computational (both classical and parameterized) complexity. We also run extensive experiments on real-world election data and study the effectiveness of optimal campaigns, in particular as opposed to using heuristic bribing strategies and with respect to the influence of the threshold and the influence of the number of districts. For apportionment elections with threshold, finally, we propose -- as an alternative to the standard top-choice mode -- the second-chance mode where voters of parties below the threshold receive a second chance to vote for another party, and we establish computational complexity results also in this setting.

</details>


### [15] [Minimum Envy Graphical House Allocation Beyond Identical Valuations](https://arxiv.org/abs/2601.15864)
*Tanmay Inamdar,Pallavi Jain,Pranjal Pandey*

Main category: cs.GT

TL;DR: 研究非相同估值下的最小嫉妒图房屋分配问题，探索参数化复杂性和设计算法


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注相同估值的情况，非相同估值下的图房屋分配问题尚未被探索，需要研究其计算复杂性和算法设计

Method: 通过分析社交图结构和估值函数的限制来研究参数化复杂性，为不同图类设计中等指数时间算法，并为最大度≤1的图设计多项式时间算法

Result: 识别了导致可解性的结构限制，设计了多种图类的算法，特别是为最大度≤1的图提供了多项式时间算法

Conclusion: 首次系统研究了非相同估值下的图房屋分配问题，为这一未探索领域提供了理论基础和算法框架

Abstract: House allocation is an extremely well-studied problem in the field of fair allocation, where the goal is to assign $n$ houses to $n$ agents while satisfying certain fairness criterion, e.g., envy-freeness. To model social interactions, the Graphical House Allocation framework introduces a social graph $G$, in which each vertex corresponds to an agent, and an edge $(u, v)$ corresponds to the potential of agent $u$ to envy the agent $v$, based on their allocations and valuations. In undirected social graphs, the potential for envy is in both the directions. In the Minimum Envy Graphical House Allocation (ME-GHA) problem, given a set of $n$ agents, $n$ houses, a social graph, and agent's valuation functions, the goal is to find an allocation that minimizes the total envy summed up over all the edges of $G$. Recent work, [Hosseini et al., AAMAS 2023, AAMAS 2024] studied ME-GHA in the regime of polynomial-time algorithms, and designed exact and approximation algorithms, for certain graph classes under identical agent valuations. We initiate the study of \gha with non-identical valuations, a setting that has so far remained unexplored. We investigate the multivariate (parameterized) complexity of \gha by identifying structural restrictions on the social graph and valuation functions that yield tractability. We also design moderately exponential-time algorithms for several graph classes, and a polynomial-time algorithm for {binary valuations that returns an allocation with envy at most one when the social graph has maximum degree at most one.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST是一个交互式工具，允许用户用自然语言查询时空数据库，通过三层架构将自然语言转换为物理查询计划。


<details>
  <summary>Details</summary>
Motivation: 移动计算设备和定位技术的发展导致时空数据爆炸式增长，但非专业用户难以使用专业查询语言（如SQL）来查询这些数据，需要自然语言查询的支持。

Method: 采用三层架构：1) 知识库和语料库进行知识准备；2) 自然语言理解进行实体链接；3) 生成物理查询计划。系统支持范围查询、最近邻查询和连接查询等时空查询。

Result: 在四个真实和合成数据集上验证了NL4ST能够提供有效的时空物理查询计划，系统已在线部署并提供演示视频。

Conclusion: NL4ST填补了非专业用户与数据库查询计划之间的鸿沟，使时空数据库的自然语言查询成为可能，降低了查询门槛。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [17] [Efficient Cloud-edge Collaborative Approaches to SPARQL Queries over Large RDF graphs](https://arxiv.org/abs/2601.15992)
*Shidan Ma,Peng Peng,Xu Zhou,M. Tamer Özsu,Lei Zou,Guo Chen*

Main category: cs.DB

TL;DR: 本文首次探索将边缘计算集成到RDF图存储和SPARQL查询处理中，通过将图数据存储和处理迁移到边缘环境来提升查询性能，解决了数据本地化和网络调度两大挑战。


<details>
  <summary>Details</summary>
Motivation: 随着RDF图使用增加，基于云的SPARQL查询解决方案在带宽有限或系统负载高的环境中存在性能瓶颈，需要探索新的架构来改善查询性能。

Method: 1) 引入模式诱导子图概念解决数据本地化挑战；2) 构建综合考虑数据分布、查询特征、网络通信和计算资源的系统模型；3) 将查询分配和计算资源分配联合建模为混合整数非线性规划问题，并使用改进的分支定界算法求解。

Result: 在真实数据集和云平台上的实验结果表明，所提方法在效率方面优于最先进的基线方法。

Conclusion: 通过集成边缘计算，将图数据存储和处理迁移到边缘环境，可以有效提升RDF图查询性能，为解决云环境下SPARQL查询的性能瓶颈提供了新思路。

Abstract: With the increasing use of RDF graphs, storing and querying such data using SPARQL remains a critical problem. Current mainstream solutions rely on cloud-based data management architectures, but often suffer from performance bottle- necks in environments with limited bandwidth or high system load. To address this issue, this paper explores for the first time the integration of edge computing to move graph data storage and processing to edge environments, thereby improving query performance. This approach requires offloading query processing to edge servers, which involves addressing two challenges: data localization and network scheduling. First, the data localization challenge lies in computing the subgraphs maintained on edge servers to quickly identify the servers that can handle specific queries. To address this challenge, we introduce a new concept of pattern-induced subgraphs. Second, the network scheduling challenge involves efficiently assigning queries to edge and cloud servers to optimize overall system performance. We tackle this by constructing a overall system model that jointly captures data distribution, query characteristics, network communication, and computational resources. Accordingly, we further propose a joint formulation of query assignment and computational resource allocation, modeling it as a Mixed Integer Nonlinear Programming (MINLP) problem and solve this problem using a modified branch-and-bound algorithm. Experimental results on real datasets under a real cloud platform demonstrate that our proposed method outperforms the state-of-the-art baseline methods in terms of efficiency. The codes are available on GitHub

</details>


### [18] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: EAIFD是一种用于增量函数依赖发现的新算法，通过维护差异集的部分超图并将问题转化为超图上的最小命中集枚举，避免了完全重新运行，显著提升了性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 函数依赖是关系数据库中的基本完整性约束，但在增量更新下发现函数依赖仍然具有挑战性。静态算法由于完全重新执行而效率低下，增量算法则面临严重的性能和内存瓶颈。

Method: EAIFD维护差异集的部分超图，将增量函数依赖发现问题重构为超图上的最小命中集枚举。引入两个关键创新：1) 多属性哈希表(MHT)用于有效函数依赖的高频键值映射，其内存消耗与数据集大小无关；2) 两步验证策略，利用MHT有效减少验证空间，然后选择性加载数据块进行批量验证，避免重复I/O操作。

Result: 在真实世界数据集上的实验结果表明，与现有算法相比，EAIFD在运行时间上实现了高达一个数量级的加速，同时将内存使用量减少了两个数量级以上。

Conclusion: EAIFD为增量函数依赖发现提供了一个高效且可扩展的解决方案，通过创新的数据结构和验证策略，显著克服了现有方法的性能和内存瓶颈。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [19] [Nested and outlier embeddings into trees](https://arxiv.org/abs/2601.15470)
*Shuchi Chawla,Kristin Sheridan*

Main category: cs.DS

TL;DR: 本文提出一种高效算法，用于将度量空间概率嵌入到HST（分层分离树）中，允许存在少量异常点，在保证预期失真度的同时显著减少异常点数量。


<details>
  <summary>Details</summary>
Motivation: 现有HST嵌入方法在处理度量空间时可能产生大量异常点，影响实际应用效果。本文旨在开发更高效的异常点嵌入方法，以改进买断批量（buy-at-bulk）和按需乘车（dial-a-ride）等问题的近似算法。

Method: 1. 将嵌套嵌入概念从确定性嵌入扩展到概率嵌入；2. 开发寻找HST良好嵌套组合的方法；3. 结合Munagala等人的近似算法，设计高效采样算法，在保证失真度的同时控制异常点数量。

Result: 提出高效算法，输入度量空间(X,d)和目标失真度c，可采样概率嵌入，异常点数量最多为O(k/ε·log²k)，失真度最多为(32+ε)c（ε>0）。这为特定实例提供了更好的近似解。

Conclusion: 通过扩展嵌套嵌入概念到概率设置，并开发有效的HST嵌套组合方法，实现了在控制失真度的同时显著减少异常点数量的高效算法，改进了相关组合优化问题的近似解质量。

Abstract: In this paper, we consider outlier embeddings into HSTs and ultrametrics. In particular, for $(X,d)$, let $k$ be the size of the smallest subset of $X$ such that all but that subset (i.e. the ``outlier set'') can be probabilistically embedded into the space of HSTs with expected distortion at most $c$. Our primary result is showing that there exists an efficient algorithm that takes in $(X,d)$ and a target distortion $c$ and samples from a probabilistic embedding with at most $O(\frac k ε\log^2k)$ outliers and distortion at most $(32+ε)c$, for any $ε>0$. This leads to better instance-specific approximations for certain instances of the buy-at-bulk and dial-a-ride problems, whose current best approximation algorithms go through HST embeddings.
  In order to facilitate our results, we largely focus on the concept of compositions of nested embeddings introduced by [Chawla and Sheridan 2024]. A nested embedding is a composition of two embeddings of a metric space $(X,d)$ -- a low distortion embedding of a subset $S$ of nodes, and a higher distortion embedding of the entire metric. The composition is a single embedding that preserves the low distortion over $S$ and does not increase distortion over the remaining points by much. In this paper, we expand this concept from the setting of deterministic embeddings to the setting of probabilistic embeddings. We show how to find good nested compositions of embeddings into HSTs, and combine this with an approximation algorithm of [Munagala et al. 2023] to obtain our results.

</details>


### [20] [Tight Bounds for Gaussian Mean Estimation under Personalized Differential Privacy](https://arxiv.org/abs/2601.15682)
*Wei Dong,Li Ge*

Main category: cs.DS

TL;DR: 本文研究了高斯分布均值估计在个性化差分隐私(PDP)下的问题，提出了在有限和无限PDP两种设置下的最优均值估计器，算法上界与理论下界匹配至对数因子。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅针对有界分布在有界PDP下的均值估计，而高斯分布具有无界支撑，元素贡献差异大，需要同时考虑隐私信息和数据值。在无限PDP下，隐私信息受保护，权重计算方式不明确，问题更具挑战性。

Method: 针对有界和无限PDP两种设置，首先推导问题的理论下界，然后设计PDP均值估计器，使算法上界与对应的理论下界匹配至对数因子。

Result: 提出了高斯分布均值估计在两种PDP设置下的最优估计器，算法性能达到理论下界（至对数因子），解决了无界支撑分布下个性化隐私保护的挑战。

Conclusion: 本文解决了高斯分布均值估计在个性化差分隐私下的关键问题，为无界支撑分布提供了理论保证和实用算法，填补了该领域的研究空白。

Abstract: We study mean estimation for Gaussian distributions under \textit{personalized differential privacy} (PDP), where each record has its own privacy budget. PDP is commonly considered in two variants: \textit{bounded} and \textit{unbounded} PDP. In bounded PDP, the privacy budgets are public and neighboring datasets differ by replacing one record. In unbounded PDP, neighboring datasets differ by adding or removing a record; consequently, an algorithm must additionally protect participation information, making both the dataset size and the privacy profile sensitive. Existing works have only studied mean estimation over bounded distributions under bounded PDP. Different from mean estimation for distributions with bounded range, where each element can be treated equally and we only need to consider the privacy diversity of elements, the challenge for Gaussian is that, elements can have very different contributions due to the unbounded support. we need to jointly consider the privacy information and the data values. Such a problem becomes even more challenging under unbounded PDP, where the privacy information is protected and the way to compute the weights becomes unclear. In this paper, we address these challenges by proposing optimal Gaussian mean estimators under both bounded and unbounded PDP, where in each setting we first derive lower bounds for both problems, following PDP mean estimators with the algorithmic upper bounds matching the corresponding lower bounds up to logarithmic factors.

</details>


### [21] [Improved Approximation Ratios for the Shortest Common Superstring Problem with Reverse Complements](https://arxiv.org/abs/2601.15814)
*Ryosuke Yamano,Tetsuo Shibuya*

Main category: cs.DS

TL;DR: 本文改进了SCS-RC问题的近似算法，将MGREEDY和TGREEDY算法的近似比分别从4和3提升到3.75和2.875。


<details>
  <summary>Details</summary>
Motivation: SCS-RC问题在生物信息学中自然出现，但之前的研究仅将标准SCS的算法扩展到该问题，保持了原有的4和3近似比，缺乏针对该问题的改进。

Method: 将MGREEDY和TGREEDY算法扩展到反向互补设置，并扩展经典证明方法来处理由反向互补引入的双向重叠。

Result: 证明了MGREEDY和TGREEDY在反向互补设置下分别达到3.75和2.875的近似比，这是SCS-RC问题的首次正式改进。

Conclusion: 本文提供了SCS-RC问题的第一个改进近似保证，其中2.875近似算法目前是该问题的最佳已知界限。

Abstract: The Shortest Common Superstring (SCS) problem asks for the shortest string that contains each of a given set of strings as a substring. Its reverse-complement variant, the Shortest Common Superstring problem with Reverse Complements (SCS-RC), naturally arises in bioinformatics applications, where for each input string, either the string itself or its reverse complement must appear as a substring of the superstring. The well-known MGREEDY algorithm for the standard SCS constructs a superstring by first computing an optimal cycle cover on the overlap graph and then concatenating the strings corresponding to the cycles, while its refined variant, TGREEDY, further improves the approximation ratio. Although the original 4- and 3-approximation bounds of these algorithms have been successively improved for the standard SCS, no such progress has been made for the reverse-complement setting. A previous study extended MGREEDY to SCS-RC with a 4-approximation guarantee and briefly suggested that extending TGREEDY to the reverse-complement setting could achieve a 3-approximation. In this work, we strengthen these results by proving that the extensions of MGREEDY and TGREEDY to the reverse-complement setting achieve 3.75- and 2.875-approximation ratios, respectively. Our analysis extends the classical proofs for the standard SCS to handle the bidirectional overlaps introduced by reverse complements. These results provide the first formal improvement of approximation guarantees for SCS-RC, with the 2.875-approximate algorithm currently representing the best known bound for this problem.

</details>


### [22] [Finding large sparse induced subgraphs in graphs of small (but not very small) tree-independence number](https://arxiv.org/abs/2601.15861)
*Daniel Lokshtanov,Michał Pilipczuk,Paweł Rzążewski*

Main category: cs.DS

TL;DR: 本文改进了树独立数有界图的优化问题算法，将运行时间从指数级改进为n^O(k)，使其在树独立数多对数级别的图类中具有拟多项式时间复杂度。


<details>
  <summary>Details</summary>
Motivation: Lima等人[ESA 2024]提出的算法虽然能在树独立数有界图中解决CMSO₂优化问题，但其时间复杂度随k增长过快，当树独立数超常数时算法不实用。需要改进算法以获得更好的时间复杂度。

Method: 提出了原始算法的精炼版本，通过改进算法设计，将运行时间优化为n^O(k)，其中n是顶点数，k是树独立数，隐藏因子取决于解的树宽界和CMSO₂性质。

Result: 算法在树独立数多对数级别的图类中具有拟多项式时间复杂度，在具有亚线性平衡团基分离器的几何交图类中具有亚指数时间复杂度。

Conclusion: 改进后的算法显著降低了时间复杂度，使得在树独立数多对数级别的图类中能够高效解决CMSO₂优化问题，扩展了算法的实用范围。

Abstract: The independence number of a tree decomposition is the size of a largest independent set contained in a single bag. The tree-independence number of a graph $G$ is the minimum independence number of a tree decomposition of $G$. As shown recently by Lima et al. [ESA~2024], a large family of optimization problems asking for a maximum-weight induced subgraph of bounded treewidth, satisfying a given \textsf{CMSO}$_2$ property, can be solved in polynomial time in graphs whose tree-independence number is bounded by some constant~$k$.
  However, the complexity of the algorithm of Lima et al. grows rapidly with $k$, making it useless if the tree-independence number is superconstant. In this paper we present a refined version of the algorithm. We show that the same family of problems can be solved in time~$n^{\mathcal{O}(k)}$, where $n$ is the number of vertices of the instance, $k$ is the tree-independence number, and the $\mathcal{O}(\cdot)$-notation hides factors depending on the treewidth bound of the solution and the considered \textsf{CMSO}$_2$ property.
  This running time is quasipolynomial for classes of graphs with polylogarithmic tree-independence number; several such classes were recently discovered. Furthermore, the running time is subexponential for many natural classes of geometric intersection graphs -- namely, ones that admit balanced clique-based separators of sublinear size.

</details>


### [23] [Dynamic Pattern Matching with Wildcards](https://arxiv.org/abs/2601.16182)
*Arshia Ataee Naeini,Amir-Parsa Mobed,Masoud Seddighin,Saeed Seddighin*

Main category: cs.DS

TL;DR: 本文研究了带通配符的动态模式匹配问题，提出了多种算法，包括O(nlog²n)预处理的通用算法，以及针对特殊情况的亚线性算法，并给出了基于SETH的条件下界。


<details>
  <summary>Details</summary>
Motivation: 动态模式匹配在实际应用中非常重要，特别是当模式包含通配符时。现有算法在处理动态更新时效率不高，需要设计更高效的算法来处理文本和模式同时更新的场景。

Method: 1. 通用算法：使用O(nlog²n)预处理时间，更新/查询时间为O(kn^{k/(k+1)} + k² log n)
2. 特殊情况1：当模式包含w个非通配符时，使用O(nw)预处理和O(w + log n)更新
3. 特殊情况2：当最多有两个非通配符时，结合FFT和块分解，实现确定性亚线性算法，预处理O(n^1.8)，更新O(n^0.8 log n)

Result: 1. 对于常数k，算法实现了真正的亚线性时间
2. 当k=o(log n)时，算法是亚线性的
3. 基于SETH的条件下界表明，对于k=Ω(log n)的情况，在亚二次预处理时间下实现真正的亚线性更新时间是不可能的
4. 针对特殊情况的算法在相应条件下实现了亚线性性能

Conclusion: 本文为带通配符的动态模式匹配问题提供了高效的算法解决方案，包括通用算法和针对特殊情况的优化算法，并通过条件性下界证明了某些情况下亚线性更新的不可能性，为这一重要问题建立了理论界限。

Abstract: We study the fully dynamic pattern matching problem where the pattern may contain up to kwildcard symbols, each matching any symbol of the alphabet. Both the text and the pattern are subject to updates (insert, delete, change). We design an algorithm with O(nlog^2 n) preprocessing and update/query time O(knk/k+1 + k2 log n). The bound is truly sublinear for a constant k, and sublinear when k= o(log n). We further complement our results with a conditional lower bound: assuming subquadratic preprocessing time, achieving truly sublinear update time for the case k = Ω(log n) would contradict the Strong Exponential Time Hypothesis (SETH). Finally, we develop sublinear algorithms for two special cases: - If the pattern contains w non-wildcard symbols, we give an algorithm with preprocessing time O(nw) and update time O(w + log n), which is truly sublinear whenever wis truly sublinear. - Using FFT technique combined with block decomposition, we design a deterministic truly sublinear algorithm with preprocessing time O(n^1.8) and update time O(n^0.8 log n) for the case that there are at most two non-wildcards.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [24] [Partially Polarized Polar Codes: A New Design for 6G Control Channels](https://arxiv.org/abs/2601.15404)
*Arman Fazeli,Mohammad M. Mansour,Ziyuan Zhu,Louay Jalloul*

Main category: cs.IT

TL;DR: PPP码是一种新型类极化码，通过选择性剪枝极化核来修改合成比特信道容量，确保解码早期有保证数量的非冻结比特可用，从而提升早期终止效果。


<details>
  <summary>Details</summary>
Motivation: 在5G下行控制信道的盲解码场景中，用户设备需要处理多个候选码字，其中许多不包含有效控制信息。传统极化码在早期终止方面效率有限，硬件限制也阻碍了直接扩展。

Method: 从传统极化码出发，通过选择性剪枝极化核来构造PPP码，修改合成比特信道容量以确保解码早期有保证数量的非冻结比特可用。同时提出了针对PPP码的冻结比特图设计策略。

Result: PPP码相比传统极化码在性能上有显著提升，特别是在较大块长度时。与现有方法（如聚合或分段）相比，PPP码实现了更高效率且无需额外硬件支持。

Conclusion: PPP码通过选择性极化核剪枝有效改善了早期终止能力，特别适用于下行控制信道的盲解码场景，在性能、效率和硬件需求方面都优于现有方法。

Abstract: We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes.

</details>


### [25] [Rank-metric codes over arbitrary fields: Bounds and constructions](https://arxiv.org/abs/2601.15464)
*Alessandro Neri,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 该论文综述了秩度量码的发展历程、数学基础、边界与构造方法，特别关注了从有限域到更一般代数结构（如代数闭域、实数域）的扩展，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 秩度量码在1978年由Delsarte提出，后由Gabidulin重新发现，已成为编码理论的核心主题。它们在网络编码中有重要应用，并与多个数学领域有深刻联系。本文旨在系统梳理秩度量码的理论发展，特别是将其从有限域扩展到更一般的代数结构，以揭示更广泛的数学性质。

Method: 采用综述研究方法，系统分析秩度量码的数学基础：1) 研究Singleton-like边界条件及其在不同代数结构下的紧致性；2) 探讨具有循环Galois扩张的域上最大秩距离(MRD)码的构造；3) 分析线性秩度量码与系统和规避子空间的关系；4) 回顾代数闭域和实数域上的相关结果（这些结果先前出现在拓扑和测度论中）。

Result: 1) 在有限域情况下，Singleton-like边界是紧致的，但在更一般的代数结构中可能不紧致；2) 建立了具有循环Galois扩张的域上MRD码的构造方法；3) 揭示了线性秩度量码与系统和规避子空间的深刻联系；4) 将拓扑和测度论中关于代数闭域和实数域的结果与秩度量码理论联系起来。

Conclusion: 秩度量码理论已从有限域扩展到更一般的代数结构，展现出丰富的数学内涵。未来研究方向包括：1) 关于MRD码存在性的猜想；2) 在各种域扩张上探索秩度量码的性质；3) 进一步连接编码理论与拓扑、测度论等其他数学分支。

Abstract: Rank-metric codes, defined as sets of matrices over a finite field with the rank distance, have gained significant attention due to their applications in network coding and connections to diverse mathematical areas. Initially studied by Delsarte in 1978 and later rediscovered by Gabidulin, these codes have become a central topic in coding theory. This paper surveys the development and mathematical foundations, in particular, regarding bounds and constructions of rank-metric codes, emphasizing their extension beyond finite fields to more general settings. We examine Singleton-like bounds on code parameters, demonstrating their sharpness in finite field cases and contrasting this with contexts where the bounds are not tight. Furthermore, we discuss constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions and the relationship between linear rank-metric codes with systems and evasive subspaces. The paper also reviews results for algebraically closed fields and real numbers, previously appearing in the context of topology and measure theory. We conclude by proposing future research directions, including conjectures on MRD code existence and the exploration of rank-metric codes over various field extensions.

</details>


### [26] [Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds](https://arxiv.org/abs/2601.15505)
*Tyler Kann,Matthieu R. Bloch,Shrinivas Kudekar,Ruediger Urbanke*

Main category: cs.IT

TL;DR: 本文提出了一种通过稳定子码作为信道变换来改进量子哈希界的方法，通过计算诱导逻辑泡利错误分布，利用解码器侧信息获得更高的可达速率。


<details>
  <summary>Details</summary>
Motivation: 量子哈希界对于无记忆泡利信道并非总是紧的，现有方法仅适用于某些非对称信道。需要一种更通用的方法来改进各种泡利信道的可达速率。

Method: 将任意稳定子码作为信道变换，构造完全辛表，计算物理泡利信道下逻辑泡利错误和校验子的联合分布，利用解码器侧信息获得可达速率。

Result: 通过对小型变换进行结构化搜索，发现在先前研究的具有偏斜独立错误的泡利信道族中，该方法能够改进基线哈希界。

Conclusion: 稳定子码作为信道变换的方法能够普遍改进泡利信道的可达速率，为量子通信提供了更优的编码方案。

Abstract: The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\![ n, k ]\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

</details>


### [27] [A Class of Subadditive Information Measures and their Applications](https://arxiv.org/abs/2601.15639)
*Hamidreza Abin,Mahdi Zinati,Amin Gohari,Mohammad Hossein Yassaee,Mohammad Mahdi Mojahedian*

Main category: cs.IT

TL;DR: 该论文提出了(G,f)-散度，通过将非递减函数G应用于f-散度D_f得到，并定义了相应的(G,f)-信息度量。核心主题是研究乘积分布和乘积信道上的次可加性，建立了简化原则，并应用于信道编码、假设检验和球面打包指数框架。


<details>
  <summary>Details</summary>
Motivation: 研究f-散度的推广形式，建立更一般的散度度量框架，探索其在信息论中的次可加性性质，为信道编码、假设检验等应用提供理论基础。

Method: 引入(G,f)-散度作为两参数散度族，定义相应的(G,f)-信息度量。建立简化原则，证明对于广泛的G类，只需在二元字母表上验证散度次可加性。针对特定G函数（x, log(1+x), -log(1-x)），推导保证次可加性的f的充分条件。

Result: 建立了(G,f)-散度的次可加性理论框架，推导了简化验证原则，获得了多个标准f-散度的次可加性条件。将结果应用于信道编码有限块长逆定理、二元假设检验界限，并扩展了Shannon-Gallager-Berlekamp球面打包指数框架。

Conclusion: 提出的(G,f)-散度框架统一了多种散度度量，建立的次可加性理论为信息论中的多个应用问题提供了新的分析工具和界限，扩展了经典结果的应用范围。

Abstract: We introduce a two-parameter family of discrepancy measures, termed \emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $
I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\in\{x,\log(1+x),-\log(1-x)\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.

</details>


### [28] [Generative AI-Empowered Semantic Twin Channel Model for ISAC](https://arxiv.org/abs/2601.15642)
*Yi Chen,Yatao Hu,Ming Li,Chong Han*

Main category: cs.IT

TL;DR: 该论文提出了一种面向语义的ISAC信道建模方法，通过生成式AI构建语义孪生信道模型，在保持环境语义的同时平衡准确性与复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前ISAC信道建模存在明显差距：统计模型过于粗糙，忽略了传感所需的关键多径特征；确定性模型计算效率低下，难以扩展到系统级评估。需要一种能统一环境语义与信道行为的抽象方法。

Method: 提出语义导向的信道建模原则，引入生成式AI赋能的语义孪生信道模型，通过多语义级别连接环境语义与可观测信道结构，生成具有物理合理性的信道实现。

Result: 案例研究表明，该模型在具有挑战性的多视角设置下保持语义一致性，为可控仿真、数据集生成和可复现的ISAC基准测试提供了实用路径。

Conclusion: 环境语义是ISAC信道建模的关键，语义孪生信道模型为未来ISAC设计和标准化提供了有效的建模框架，能够平衡准确性、复杂性和计算效率。

Abstract: Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization.

</details>


### [29] [Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems](https://arxiv.org/abs/2601.15723)
*Gunank Jakhar,Gowtham R. Kurri,Suryajith Chillara,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 本文在熵不等式与子模性关联的基础上，建立了Madiman-Tetali不等式的凸函数推广，提出了改进的Loomis-Whitney型投影不等式，并利用Shearer引理扩展了极值图论问题的结果。


<details>
  <summary>Details</summary>
Motivation: 熵不等式与子模性之间存在深刻联系，已有Madiman-Tetali和Sason等学者建立了统一框架。本文旨在在这些框架基础上进一步推进，建立更一般的凸函数不等式，改进经典投影不等式，并扩展极值图论问题的结果。

Method: 1. 建立Madiman-Tetali强、弱不等式的凸函数推广；2. 利用特殊情况的Madiman-Tetali不等式推导改进的Loomis-Whitney型投影不等式；3. 使用Shearer引理研究极值图论问题，扩展先前结果。

Result: 1. 获得了子模函数的凸函数推广不等式；2. 提出了包含切片结构信息的改进Loomis-Whitney投影不等式；3. 恢复了Sason和Boucheron等人的结果并进行了扩展。

Conclusion: 本文在熵不等式与子模性的统一框架下取得了三个重要进展：建立了凸函数推广不等式，改进了经典投影不等式，并扩展了极值图论问题的结果，为信息论和组合数学提供了新的工具。

Abstract: It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.

</details>


### [30] [Recursive Flow: A Generative Framework for MIMO Channel Estimation](https://arxiv.org/abs/2601.15767)
*Zehua Jiang,Fenghao Zhu,Chongwen Huang,Richeng Jin,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Mérouane Debbah*

Main category: cs.IT

TL;DR: RC-Flow是一种用于大规模MIMO信道估计的新型递归流匹配方法，通过闭环精炼框架和自适应调度策略，在噪声主导场景下实现高效准确的信道重建。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中的信道估计是一个基本挑战，估计精度直接影响频谱效率和链路可靠性。传统生成模型在噪声环境下性能受限，需要更鲁棒的解决方案。

Method: 提出RC-Flow方法，结合预训练的流匹配先验，通过串行重启机制和锚定轨迹校正建立闭环精炼框架。采用流一致性先验方向与数据保真度近端投影的协同，并引入自适应双调度策略来平衡收敛速度与重建精度。

Result: RC-Flow在低信噪比场景下相比基于分数的基线方法获得2.7 dB的性能增益，同时推理延迟降低两个数量级。在多种噪声水平下均达到最先进的性能。

Conclusion: RC-Flow通过递归流匹配和闭环精炼框架，为大规模MIMO信道估计提供了高效鲁棒的解决方案，特别适用于噪声主导场景，在性能与效率之间实现了良好平衡。

Abstract: Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.

</details>


### [31] [Practical applications of Set Shaping Theory to Non-Uniform Sequences](https://arxiv.org/abs/2601.15853)
*A. Schmidt,A. Vdberg,A. Petit*

Main category: cs.IT

TL;DR: SST通过构造双射映射将原始序列集映射到更大序列空间的结构化区域，实现信息内容减少。本文解决了非均匀序列应用中序列排序的指数复杂度问题，提出近似排序方法，保持SST结构要求的同时实现理论预测的整形增益。


<details>
  <summary>Details</summary>
Motivation: 集合整形理论(SST)超越了经典固定空间模型，但应用于非均匀序列时面临主要实验困难：需要根据信息内容对原始和变换集的序列进行排序，而精确排序具有指数复杂度，直接实现不切实际。

Method: 提出近似但信息丰富的排序方法，在保持SST结构要求的同时实现理论预测的整形增益。该方法避免了指数复杂度的精确排序，使SST能够实际应用于非均匀序列。

Result: 扩展了先前均匀分布序列的实验结果，证明SST的整形优势在非均匀序列中仍然存在。实现了理论预测的整形增益，克服了指数复杂度障碍。

Conclusion: 通过近似排序方法成功克服了SST应用于非均匀序列的主要障碍，证明了整形优势的普适性。为促进可重复性，相关软件已在GitHub上公开。

Abstract: Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work

</details>


### [32] [Blind Identification of Channel Codes: A Subspace-Coding Approach](https://arxiv.org/abs/2601.15903)
*Pramod Singh,Prasad Krishnan,Arti Yardi*

Main category: cs.IT

TL;DR: 提出了一种用于二进制对称信道（BSC）上信道码盲识别的新方法——最小去噪子空间差异解码器，该方法结合汉明度量和子空间度量解码原理，在随机线性码识别方面优于现有通用技术。


<details>
  <summary>Details</summary>
Motivation: 现有信道码盲识别方法大多依赖于码族的特殊结构，计算复杂度高，且缺乏严格的理论性能保证。需要一种更通用、高效且具有理论保证的码识别方法。

Method: 基于算子信道的子空间码框架，结合汉明度量和子空间度量解码原理，提出了最小去噪子空间差异解码器。该方法适用于二进制对称信道（BSC）。

Result: 为有界权重错误情况下的码识别提供了理论保证，并给出了在BSC上使用的错误概率界限。仿真显示，该方法在大多数信道条件下，即使接收向量数量有限，对随机线性码的识别性能也优于现有通用技术。

Conclusion: 提出的最小去噪子空间差异解码器为信道码盲识别提供了一种有效的新方法，具有理论保证和实际性能优势，特别适用于随机线性码的识别。

Abstract: The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors.

</details>


### [33] [A Remark on Downlink Massive Random Access](https://arxiv.org/abs/2601.15928)
*Yuchen Liao,Wenyi Zhang*

Main category: cs.IT

TL;DR: 该论文指出大规模随机接入的下行链路中，通过组合数学的覆盖阵列设计，可以构建确定性变长码，将开销降低至不超过1+log₂e比特。


<details>
  <summary>Details</summary>
Motivation: 在大规模随机接入的下行链路中，传统方法需要显式编码活跃用户身份，这会带来与总用户数对数相关的显著开销。虽然已有随机编码方法能降低开销，但需要寻找更优的确定性编码方案。

Method: 将大规模随机接入的码设计问题建模为组合数学中的覆盖阵列问题，利用覆盖阵列理论构建确定性变长码。

Result: 证明了存在确定性构造的变长码，其开销不超过1+log₂e比特，这比之前随机编码方法的上界更优。

Conclusion: 通过将大规模随机接入问题与覆盖阵列理论联系起来，提供了更优的确定性编码方案，显著降低了开销。

Abstract: In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.

</details>


### [34] [Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing](https://arxiv.org/abs/2601.16030)
*Jiancheng An,Chau Yuen,Marco Di Renzo,Mehdi Bennis,Merouane Debbah,Lajos Hanzo*

Main category: cs.IT

TL;DR: 该论文综述了堆叠智能超表面技术，这是一种结合神经网络、电磁计算和超表面的新兴技术，旨在通过直接处理电磁波实现高速、大规模并行、低功耗的信号处理。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的抽象特征提取能力、电磁计算的波传播特性以及超表面的电磁波调控能力，开发物理神经网络技术，实现电磁域的高效信号处理。

Method: 采用堆叠智能超表面技术，通过多层亚波长超原子结构直接处理电磁波，并讨论从两个不同视角设计的优化/训练策略来配置SIM实现所需功能。

Result: SIM技术在通信、感知和计算领域展现出多样化应用，实验证据表明其能在单一设备中支持多种功能，具有独特优势。

Conclusion: SIM技术为下一代无线网络提供了有前景的解决方案，但仍需解决关键技术挑战以充分发挥其潜力，文章指出了有前景的研究方向。

Abstract: Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential.

</details>


### [35] [RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance](https://arxiv.org/abs/2601.16033)
*Zhixin Chen,Yixuan Huang,Zhengze Ji,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: 本文提出了一种基于可重构智能表面(RIS)的协作式集成感知与通信(ISAC)网络，用于低空监视。采用主动RIS增强信号强度，将低空监视建模为压缩感知成像问题，并通过子空间追踪算法求解。


<details>
  <summary>Details</summary>
Motivation: 低空经济对多个行业发展至关重要，但传统监视方法存在部署成本高、信号强度低的局限性，需要开发先进的低空监视技术。

Method: 1) 提出RIS辅助的协作ISAC网络，利用RIS将ISAC信号反射到低空进行感知；2) 采用主动RIS放大信号增强强度；3) 将低空监视建模为基于压缩感知理论的成像问题，使用子空间追踪算法求解；4) 推导系统的克拉美-罗下界(CRLB)，分析系统参数对感知性能的影响。

Result: 数值结果表明，在相同功率约束下，主动RIS优于被动RIS，能够在高达300米的高度实现有效成像和目标检测。

Conclusion: 所提出的RIS辅助低空成像系统为低空监视提供了一种有效的解决方案，主动RIS在功率约束下具有性能优势，系统参数分析为ISAC系统配置提供了指导。

Abstract: The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters.

</details>


### [36] [Tri-Hybrid Beamforming Design for integrated Sensing and Communications](https://arxiv.org/abs/2601.16036)
*Tianyu Fang,Mengyuan Ma,Markku Juntti,Nhan Thanh Nguyen*

Main category: cs.IT

TL;DR: 研究基于三混合波束成形架构的集成感知与通信设计，通过优化通信信噪比和感知功率，实现能效提升和空间增益改进。


<details>
  <summary>Details</summary>
Motivation: 在超大规模天线阵列中使用低成本可编程超表面天线实现能效通信系统，同时提升通信和感知性能。

Method: 提出多目标优化问题，平衡通信信噪比和目标方向的感知功率，考虑总功耗和三混合波束成形架构的物理限制，开发高效迭代算法，每次迭代中变量以闭式更新。

Result: 三混合架构提高了空间增益和能量效率，但与传统混合波束成形架构相比，波束对准能力有所降低。

Conclusion: 三混合波束成形架构为集成感知与通信系统提供了有效的解决方案，在能效和空间增益方面具有优势，但需要权衡波束对准能力。

Abstract: Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.

</details>


### [37] [Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time](https://arxiv.org/abs/2601.16164)
*Emmanuel Abbe,Colin Sandon,Oscar Sprumont*

Main category: cs.IT

TL;DR: 本文提出了两种张量Reed-Muller码的构造，能够在低于信道容量的任何恒定码率下实现准线性时间解码，并提供了相应的错误概率和解码时间分析。


<details>
  <summary>Details</summary>
Motivation: Reed-Muller码是经典纠错码，但其解码复杂度较高。本文旨在构造具有准线性时间解码能力的张量Reed-Muller码，同时保持接近信道容量的码率，以解决实际应用中解码效率的问题。

Method: 定义张量Reed-Muller码为多个Reed-Muller码的张量积，生成矩阵是各分量码生成矩阵的张量积。提出了两种构造：1) t=3时，错误概率为n^{-ω(log n)}，解码时间O(n log log n)；2) t≥4时，错误概率为2^{-n^{1/2-1/(2(t-2))-o(1)}}，解码时间O(n log n)。核心工具是多项式时间算法，用于从(d_min(C))/(2max{d_min(C_i)})-1个对抗错误中解码任意张量码。

Result: 成功构造了在低于信道容量的任何恒定码率R下可准线性时间解码的张量Reed-Muller码。第一种构造(t=3)具有超多项式小的错误概率和O(n log log n)解码时间；第二种构造(t≥4)具有指数小的错误概率和O(n log n)解码时间。提出的解码算法不要求分量码本身具有多项式时间解码能力。

Conclusion: 张量Reed-Muller码能够实现接近信道容量的码率同时保持准线性时间解码，为高效纠错码设计提供了新途径。提出的解码算法具有普适性，适用于任意张量码，不依赖于分量码的解码复杂度。

Abstract: Define the codewords of the Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;r_2,m_2;\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\left\{x_{ij}\right\}_{i=1,\dots,t}^{j=1,\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\dots,x_{im_i}$. The generator matrix of $\mathsf{TRM}(r_1,m_1;\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\mathsf{RM}(r_1,m_1),\dots, \mathsf{RM}(r_t,m_t)$.
  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:
  1) Our first construction (with $t=3$) has error probability $n^{-ω(\log n)}$ and decoding time $O(n\log\log n)$.
  2) Our second construction, for any $t\geq 4$, has error probability $2^{-n^{\frac{1}{2}-\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\log n)$.
  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\otimes\dotsc\otimes C_t$ from $\frac{d_{\min}(C)}{2\max\{d_{\min}(C_1),\dotsc,d_{\min}(C_t) \}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\dotsc,C_t$ to themselves be decodable in polynomial time.

</details>


### [38] [Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach](https://arxiv.org/abs/2601.16171)
*Ali Khalesi,Ahmad Tanha,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 提出基于张量理论的分布式计算方案，用于多服务器多用户场景下的多项式函数计算，通过张量分解和分块技术降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 在N服务器、K用户的分布式计算环境中，用户请求对L个实值基子函数进行多变量多项式评估。现有方法在计算和通信成本方面存在不足，需要更高效的任务分配和数据通信技术

Method: 采用张量理论方法：1) 将请求的非线性可分解函数表示为张量$\bar{\mathcal{F}}$；2) 通过固定支撑SVD张量分解方法将$\bar{\mathcal{F}}$稀疏分解为张量$\bar{\mathcal{E}}$和矩阵$\mathbf{D}$；3) 利用多维度子张量分块技术；4) 分解结果直接定义任务分配、连接性和通信模式

Result: 设计了可行的计算方案，推导了计算和通信成本，相比现有技术有显著性能提升

Conclusion: 基于张量分解的方法为分布式计算中的非线性函数评估提供了有效的任务分配和通信优化方案，在计算和通信成本方面优于现有技术

Abstract: The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\bar{\mathcal{F}}$, whose sparse decomposition into a tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.

</details>
