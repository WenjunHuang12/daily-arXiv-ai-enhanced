<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Using nonassociative algebras to classify skew polycyclic codes up to isometry and equivalence](https://arxiv.org/abs/2508.10139)
*Susanne Pumpluen*

Main category: cs.IT

TL;DR: 提出了新的斜多环码的等价和等距定义，以减少已知分类数量并明确不同概念的重合条件。


<details>
  <summary>Details</summary>
Motivation: 改进现有斜多环码的等价和等距分类，避免重复代码并提升分类精确性。

Method: 利用斜多环码生成器与主左理想生成器的一一对应关系，通过代数同构保持汉明距离进行分类。

Result: 减少了等价和等距类的数量，明确了重合条件，并推广到非结合代数环境。

Conclusion: 新定义提升了斜多环码的分类效率，适用于更广泛的代数结构和度量空间。

Abstract: We propose new definitions of equivalence and isometry for skew polycyclic
codes that will lead to tighter classifications than existing ones. This helps
to reduce the number of previously known isometry and equivalence classes, and
state precisely when these different notions coincide. In the process, we
classify classes of skew $(f,\sigma,\delta)$-polycyclic codes with the same
performance parameters, to avoid duplicating already existing codes.
  We exploit that the generator of a skew polycyclic code is in one-one
correspondence with the generator of a principal left ideal in its ambient
algebra. Algebra isomorphisms that preserve the Hamming distance (called
isometries) map generators of principal left ideals to generators of principal
left ideals and preserve length, dimension and Hamming distance of the codes.
We allow the ambient algebras to be nonassociative, thus eliminating the need
on restrictions on the length of the codes. The isometries between the ambient
algebras can also be used to classify corresponding linear codes equipped with
the rank metric.

</details>


### [2] [Space-time Coded Differential Modulation for Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2508.10244)
*Jiawei Qiu,Harry Leib*

Main category: cs.IT

TL;DR: 论文提出了一种结合差分空时调制（DSTM）和差分反射调制（DRM）的方案，以绕过RIS系统中的CSI需求，同时提升误码率性能。


<details>
  <summary>Details</summary>
Motivation: RIS技术是6G系统的关键技术，但CSI获取困难，因此需要一种无需CSI的方案。

Method: 采用基于酉群码的DSTM方案，并与DRM结合，扩展了反射模式数量，提供了编码和解码复杂度的分析。

Result: 仿真结果表明，DRM-DSTM编码系统在准静态瑞利衰落信道中优于未编码的DRM。

Conclusion: DRM-DSTM方案有效解决了RIS系统中的CSI问题，并提升了性能。

Abstract: Reconfigurable Intelligent Surfaces (RIS) hold the promise of improving
significantly coverage, as well as spectral and energy efficiency in wireless
communication systems. Techniques based on RIS form a key technology for 6G
systems. An important issue in RIS technology is Channel State Information
(CSI), which is much more difficult to acquire in such systems. This work
introduces a Differential Space-Time Modulation (DSTM) scheme integrated with
Differential Reflecting Modulation (DRM) to bypass the requirement for CSI in
such systems, while providing error rate gains. The DSTM scheme is based on
unitary group codes. We first consider uncoded DRM for RIS to serve as a
reference point. Next we provide an overview of DSTM and outline the procedures
for its integration with DRM. Furthermore, we explore the extension of both the
original DRM and the coded DRM-DSTM scheme to a larger number of RIS reflecting
patterns $K$, and provide tables of codes for $K= 2, 3, 4$. Encoding and
decoding complexities are studied as well. Extensives simulation results over
quasi-static Rayleigh fading channels confirm the effectiveness of the DRM-DSTM
coded system, illustrating its advantages over uncoded DRM with proper system
parameters.

</details>


### [3] [The Conditional Regret-Capacity Theorem for Batch Universal Prediction](https://arxiv.org/abs/2508.10282)
*Marco Bondaschi,Michael Gastpar*

Main category: cs.IT

TL;DR: 本文推导了经典遗憾容量定理的条件版本，用于通用预测中寻找最小批次遗憾的下界，并推广到Rényi信息度量。


<details>
  <summary>Details</summary>
Motivation: 研究如何在有批次训练数据的情况下，为预测器提供更通用的遗憾下界。

Method: 推导条件版本的遗憾容量定理，并将其应用于二元无记忆源类。

Result: 揭示了条件Rényi散度与条件Sibson互信息之间的深层联系。

Conclusion: 条件版本的遗憾容量定理为通用预测提供了新的理论工具，并扩展了信息度量的应用范围。

Abstract: We derive a conditional version of the classical regret-capacity theorem.
This result can be used in universal prediction to find lower bounds on the
minimal batch regret, which is a recently introduced generalization of the
average regret, when batches of training data are available to the predictor.
As an example, we apply this result to the class of binary memoryless sources.
Finally, we generalize the theorem to R\'enyi information measures, revealing a
deep connection between the conditional R\'enyi divergence and the conditional
Sibson's mutual information.

</details>


### [4] [Energy-Efficient Index and Code Index Modulations for Spread CPM Signals in Internet of Things](https://arxiv.org/abs/2508.10290)
*Long Yuan,Wenkun Wen,Junlin Liu,Peiran Wu,Minghua Xia*

Main category: cs.IT

TL;DR: 论文提出两种新型调制方案（IM-CPM-SS和CIM-CPM-SS），结合CPM与SS技术，解决了物联网技术中的低功耗、高谱效、低成本和大连接需求。


<details>
  <summary>Details</summary>
Motivation: 物联网技术发展面临四大挑战：超低功耗、高谱效、低成本和大连接支持。

Method: 提出两种方案：IM-CPM-SS（利用索引调制选择扩频序列）和CIM-CPM-SS（引入码索引调制，分区映射输入比特）。

Result: 两种方案在BER、谱效、能效、计算复杂度和非线性放大器条件下均优于传统方法。

Conclusion: 新方案在保持恒包络连续相位优势的同时，提升了谱效和能效，对非线性失真具有强鲁棒性。

Abstract: The evolution of Internet of Things technologies is driven by four key
demands: ultra-low power consumption, high spectral efficiency, reduced
implementation cost, and support for massive connectivity. To address these
challenges, this paper proposes two novel modulation schemes that integrate
continuous phase modulation (CPM) with spread spectrum (SS) techniques. We
begin by establishing the quasi-orthogonality properties of CPM-SS sequences.
The first scheme, termed IM-CPM-SS, employs index modulation (IM) to select
spreading sequences from the CPM-SS set, thereby improving spectral efficiency
while maintaining the constant-envelope property. The second scheme, referred
to as CIM-CPM-SS, introduces code index modulation (CIM), which partitions the
input bits such that one subset is mapped to phase-shift keying symbols and the
other to CPM-SS sequence indices. Both schemes are applied to downlink
non-orthogonal multiple access (NOMA) systems. We analyze their performance in
terms of bit error rate (BER), spectral and energy efficiency, computational
complexity, and peak-to-average power ratio characteristics under nonlinear
amplifier conditions. Simulation results demonstrate that both schemes
outperform conventional approaches in BER while preserving the benefits of
constant-envelope, continuous-phase signaling. Furthermore, they achieve higher
spectral and energy efficiency and exhibit strong resilience to nonlinear
distortions in downlink NOMA scenarios.

</details>


### [5] [Integrated Communication and Remote Sensing in LEO Satellite Systems: Protocol, Architecture and Prototype](https://arxiv.org/abs/2508.10317)
*Yichao Xu,Xiaoming Chen,Ming Ying,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 提出一种基于正交延迟多普勒分复用（ODDM）波形的LEO卫星通信与SAR遥感集成系统，实现实时SAR成像与信息传输。


<details>
  <summary>Details</summary>
Motivation: 解决LEO卫星系统中高动态信道和有限处理能力对通信与遥感集成的挑战。

Method: 采用ODDM波形和集成收发器，设计兼容5G NR的传输协议和统一信号处理框架。

Result: 通过分析和仿真验证了系统性能，并开发了SDR原型验证毫米波频段的有效性。

Conclusion: 该系统为LEO卫星提供了高效的通信与遥感集成解决方案。

Abstract: In this paper, we explore the integration of communication and synthetic
aperture radar (SAR)-based remote sensing in low Earth orbit (LEO) satellite
systems to provide real-time SAR imaging and information transmission.
Considering the high-mobility characteristics of satellite channels and limited
processing capabilities of satellite payloads, we propose an integrated
communication and remote sensing architecture based on an orthogonal
delay-Doppler division multiplexing (ODDM) signal waveform. Both communication
and SAR imaging functionalities are achieved with an integrated transceiver
onboard the LEO satellite, utilizing the same waveform and radio frequency (RF)
front-end. Based on such an architecture, we propose a transmission protocol
compatible with the 5G NR standard using downlink pilots for joint channel
estimation and SAR imaging. Furthermore, we design a unified signal processing
framework for the integrated satellite receiver to simultaneously achieve
high-performance channel sensing, low-complexity channel equalization and
interference-free SAR imaging. Finally, the performance of the proposed
integrated system is demonstrated through comprehensive analysis and extensive
simulations in the sub-6 GHz band. Moreover, a software-defined radio (SDR)
prototype is presented to validate its effectiveness for real-time SAR imaging
and information transmission in satellite direct-connect user equipment (UE)
scenarios within the millimeter-wave (mmWave) band.

</details>


### [6] [Predictive Position Control for Movable Antenna Arrays in UAV Communications: A Spatio-Temporal Transformer-LSTM Framework](https://arxiv.org/abs/2508.10720)
*Kan Yu,Kaixuan Li,Xiaowu Liu,Qixun Zhang,Zhiyong Feng*

Main category: cs.IT

TL;DR: 论文提出了一种预测性MA-UAV协作控制框架，通过优化天线位置和预测未来位置，解决了城市低空通信中的实时链路优化和安全问题。


<details>
  <summary>Details</summary>
Motivation: 城市复杂环境中动态障碍和多径效应导致链路衰减和覆盖盲区，传统方法难以平衡能效、实时适应性和空间灵活性。

Method: 提出预测性MA-UAV协作控制框架：1) 通过保密率最大化优化天线位置；2) 使用Transformer增强的LSTM网络预测未来MA位置。

Result: 仿真显示预测精度显著提升（NMSE降低超49%），通信可靠性优于现有基准。

Conclusion: 该框架有效解决了MA部署中的速度不匹配问题，提升了城市低空通信的性能。

Abstract: In complex urban environments, dynamic obstacles and multipath effects lead
to significant link attenuation and pervasive coverage blind spots.
Conventional approaches based on large-scale fixed antenna arrays and UAV
trajectory optimization struggle to balance energy efficiency, real-time
adaptation, and spatial flexibility. The movable antenna (MA) technology has
emerged as a promising solution, offering enhanced spatial flexibility and
reduced energy consumption to overcome the bottlenecks of urban low-altitude
communications. However, MA deployment faces a critical velocity mismatch
between UAV mobility and mechanical repositioning latency, undermining
real-time link optimization and security assurance. To overcome this, we
propose a predictive MA-UAV collaborative control framework. First, optimal
antenna positions are derived via secrecy rate maximization. Second, a
Transformer-enhanced long short-term memory (LSTM) network predicts future MA
positions by capturing spatio-temporal correlations in antenna trajectories.
Extensive simulations demonstrate superior prediction accuracy (NMSE reduction
exceeds 49\%) and communication reliability versus current popular benchmarks.

</details>


### [7] [MapLibre Tile: A Next Generation Vector Tile Format](https://arxiv.org/abs/2508.10791)
*Markus Tremmel,Roland Zink*

Main category: cs.IT

TL;DR: 本文介绍了MapLibre Tile (MLT)格式，一种新型矢量瓦片规范，旨在解决Mapbox Vector Tile (MVT)的局限性。实验表明，MLT在压缩比和解码速度上显著优于MVT，并为下一代地图渲染器奠定了基础。


<details>
  <summary>Details</summary>
Motivation: MVT格式已使用近十年，无法完全适应新型地理空间数据源的需求，因此需要一种更高效的格式。

Method: 设计并实现了MLT格式，通过实验模拟用户会话，对比MLT与MVT在压缩比、解码速度和处理性能上的表现。

Result: MLT在压缩比上比MVT高3倍，某些大瓦片上甚至高6倍；解码速度快3倍，处理性能显著提升。

Conclusion: MLT是一种高效的新型矢量瓦片格式，为下一代地图渲染器提供了基础，有望完全利用GPU处理，突破摩尔定律的限制。

Abstract: The Mapbox Vector Tile (MVT) format is widely considered the leading open
standard for large-scale map visualization, as evidenced by its widespread
adoption by major technology companies such as AWS, Meta, and Microsoft for
their products and services. However, MVT was developed nearly a decade ago
and, consequently, does not fully align with the capabilities of new geospatial
data sources that are characterized by rapidly increasing data volumes due to
advancements in geospatial sensors and automated detection through artificial
intelligence. In this paper, we introduce the MapLibre Tile (MLT) format, a
novel vector tile specification designed from the ground up to address the
limitations of MVT. Our experiments, simulating user sessions on widely used
basemap datasets, demonstrate that MLT achieves up to three times better
compression ratios compared to MVT on encoded tilesets, with over six times
better on certain large tiles. Additionally, MLT offers decoding speeds that
are up to three times faster and significantly enhances processing performance.
MLT also introduces new functionalities and is specifically designed to lay the
foundation for the next generation of map renderers, which we expect to
entirely offload processing to the GPU, thereby overcoming the stagnation of
Moore`s law.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [8] [Ensembling Synchronisation-based and Face-Voice Association Paradigms for Robust Active Speaker Detection in Egocentric Recordings](https://arxiv.org/abs/2508.10580)
*Jason Clarke,Yoshihiko Gotoh,Stefan Goetze*

Main category: cs.MM

TL;DR: 论文提出了一种简单有效的集成方法，结合同步依赖和同步无关的模型输出，通过加权平均融合互补线索，提升了主动说话人检测在复杂场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称视角录制中因遮挡、运动模糊和音频干扰导致的视听同步检测困难，以及传统方法在复杂条件下的性能下降问题。

Method: 采用集成方法融合同步依赖和同步无关的模型输出，并优化了预处理流程以提升集成效果。

Result: 在Ego4D-AVD验证集上，集成方法分别达到70.2%和66.7%的平均精度（mAP）。

Conclusion: 集成方法有效结合了两种模型的互补优势，显著提升了复杂场景下的检测性能。

Abstract: Audiovisual active speaker detection (ASD) in egocentric recordings is
challenged by frequent occlusions, motion blur, and audio interference, which
undermine the discernability of temporal synchrony between lip movement and
speech. Traditional synchronisation-based systems perform well under clean
conditions but degrade sharply in first-person recordings. Conversely,
face-voice association (FVA)-based methods forgo synchronisation modelling in
favour of cross-modal biometric matching, exhibiting robustness to transient
visual corruption but suffering when overlapping speech or front-end
segmentation errors occur. In this paper, a simple yet effective ensemble
approach is proposed to fuse synchronisation-dependent and
synchronisation-agnostic model outputs via weighted averaging, thereby
harnessing complementary cues without introducing complex fusion architectures.
A refined preprocessing pipeline for the FVA-based component is also introduced
to optimise ensemble integration. Experiments on the Ego4D-AVD validation set
demonstrate that the ensemble attains 70.2% and 66.7% mean Average Precision
(mAP) with TalkNet and Light-ASD backbones, respectively. A qualitative
analysis stratified by face image quality and utterance masking prevalence
further substantiates the complementary strengths of each component.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 论文提出首个统一评估框架，用于评估个体倾向学习（ITL）方法是否真正捕捉到标注者行为模式，并引入两种新指标：DIC和BAE。


<details>
  <summary>Details</summary>
Motivation: 现有ITL方法缺乏评估框架，无法验证其是否真正捕捉标注者行为模式并提供有意义的解释。

Method: 提出统一评估框架，包含两种新指标：DIC（量化模型捕捉标注者倾向的能力）和BAE（评估模型解释与标注者行为的一致性）。

Result: 实验验证了所提评估框架的有效性。

Conclusion: 该框架填补了ITL方法评估的空白，为理解标注者行为提供了可靠工具。

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [10] [OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services](https://arxiv.org/abs/2508.09992)
*Daniel Groos*

Main category: cs.LG

TL;DR: OpenFPL是一个基于公开数据的开源预测方法，旨在为Fantasy Premier League玩家提供高精度的球员表现预测，其准确性媲美商业服务。


<details>
  <summary>Details</summary>
Motivation: 当前高精度预测仅由商业服务提供，且依赖专有数据，OpenFPL希望通过开源方法实现预测的民主化。

Method: OpenFPL采用基于位置的特异性集成模型，优化了2020-21至2023-24赛季的Fantasy Premier League和Understat数据。

Result: 在2024-25赛季的测试中，OpenFPL的准确性媲美商业服务，且在高回报球员（>2分）预测上表现更优。

Conclusion: OpenFPL为玩家提供了长期规划和短期决策的支持，展示了开源方法在预测领域的潜力。

Abstract: Fantasy Premier League engages the football community in selecting the
Premier League players who will perform best from gameweek to gameweek. Access
to accurate performance forecasts gives participants an edge over competitors
by guiding expectations about player outcomes and reducing uncertainty in squad
selection. However, high-accuracy forecasts are currently limited to commercial
services whose inner workings are undisclosed and that rely on proprietary
data. This paper aims to democratize access to highly accurate forecasts of
player performance by presenting OpenFPL, an open-source Fantasy Premier League
forecasting method developed exclusively from public data. Comprising
position-specific ensemble models optimized on Fantasy Premier League and
Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL
achieves accuracy comparable to a leading commercial service when tested
prospectively on data from the 2024-25 season. OpenFPL also surpasses the
commercial benchmark for high-return players ($>$ 2 points), which are most
influential for rank gains. These findings hold across one-, two-, and
three-gameweek forecast horizons, supporting long-term planning of transfers
and strategies while also informing final-day decisions.

</details>


### [11] [xRFM: Accurate, scalable, and interpretable feature learning models for tabular data](https://arxiv.org/abs/2508.10053)
*Daniel Beaglehole,David Holzmüller,Adityanarayanan Radhakrishnan,Mikhail Belkin*

Main category: cs.LG

TL;DR: xRFM是一种结合特征学习核机器与树结构的算法，用于表格数据预测任务，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据预测的现有方法（如GBDTs）进步有限，需要结合神经网络和特征学习的新方法提升性能。

Method: xRFM结合特征学习核机器与树结构，适应数据局部结构并支持大规模训练数据。

Result: 在100个回归数据集和200个分类数据集上，xRFM性能优于31种其他方法，包括TabPFNv2和GBDTs。

Conclusion: xRFM在表格数据预测中表现优异，且通过平均梯度外积提供可解释性。

Abstract: Inference from tabular data, collections of continuous and categorical
variables organized into matrices, is a foundation for modern technology and
science. Yet, in contrast to the explosive changes in the rest of AI, the best
practice for these predictive tasks has been relatively unchanged and is still
primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very
recently, there has been renewed interest in developing state-of-the-art
methods for tabular data based on recent developments in neural networks and
feature learning methods. In this work, we introduce xRFM, an algorithm that
combines feature learning kernel machines with a tree structure to both adapt
to the local structure of the data and scale to essentially unlimited amounts
of training data.
  We show that compared to $31$ other methods, including recently introduced
tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance
across $100$ regression datasets and is competitive to the best methods across
$200$ classification datasets outperforming GBDTs. Additionally, xRFM provides
interpretability natively through the Average Gradient Outer Product.

</details>


### [12] [A Personalized Exercise Assistant using Reinforcement Learning (PEARL): Results from a four-arm Randomized-controlled Trial](https://arxiv.org/abs/2508.10060)
*Amy Armento Lee,Narayan Hegde,Nina Deliu,Emily Rosenzweig,Arun Suggala,Sriram Lakshminarasimhan,Qian He,John Hernandez,Martin Seneviratne,Rahul Singh,Pradnesh Kalkar,Karthikeyan Shanmugam,Aravindan Raghuveer,Abhimanyu Singh,My Nguyen,James Taylor,Jatin Alla,Sofia S. Villar,Hulya Emir-Farinas*

Main category: cs.LG

TL;DR: PEARL研究首次大规模评估了基于强化学习算法的个性化PA干预，结果显示RL组在1和2个月时步数显著增加。


<details>
  <summary>Details</summary>
Motivation: 解决全球普遍缺乏身体活动的问题，探索mHealth干预（尤其是JITAIs）的潜力。

Method: 13,463名Fitbit用户随机分为四组（控制、随机、固定、RL），RL组通过算法个性化推送PA提示。

Result: RL组在1和2个月时步数显著高于其他组（如1个月时比对照组多296步）。

Conclusion: 行为科学支持的RL算法可有效个性化数字健康干预，促进身体活动。

Abstract: Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.

</details>


### [13] [Measuring Time Series Forecast Stability for Demand Planning](https://arxiv.org/abs/2508.10063)
*Steven Klee,Yuntian Xia*

Main category: cs.LG

TL;DR: 论文探讨了时间序列预测中模型稳定性与准确性的平衡，提出模型诱导随机性作为稳定性度量，并通过案例研究表明集成模型能提高稳定性而不显著降低准确性。


<details>
  <summary>Details</summary>
Motivation: 需求规划者更重视预测的稳定性和一致性，而非仅追求准确性提升。模型预测的剧烈波动会增加人工干预，降低信任。

Method: 通过案例研究，评估了多种先进预测模型（如Chronos、DeepAR等）在公开数据集上的稳定性和准确性。

Result: 集成模型在保持或提升准确性的同时显著提高了稳定性。

Conclusion: 建议进一步研究生产系统中预测模型的稳定性问题。

Abstract: Time series forecasting is a critical first step in generating demand plans
for supply chains. Experiments on time series models typically focus on
demonstrating improvements in forecast accuracy over existing/baseline
solutions, quantified according to some accuracy metric. There is no doubt that
forecast accuracy is important; however in production systems, demand planners
often value consistency and stability over incremental accuracy improvements.
Assuming that the inputs have not changed significantly, forecasts that vary
drastically from one planning cycle to the next require high amounts of human
intervention, which frustrates demand planners and can even cause them to lose
trust in ML forecasting models. We study model-induced stochasticity, which
quantifies the variance of a set of forecasts produced by a single model when
the set of inputs is fixed. Models with lower variance are more stable.
  Recently the forecasting community has seen significant advances in forecast
accuracy through the development of deep machine learning models for time
series forecasting. We perform a case study measuring the stability and
accuracy of state-of-the-art forecasting models (Chronos, DeepAR, PatchTST,
Temporal Fusion Transformer, TiDE, and the AutoGluon best quality ensemble) on
public data sets from the M5 competition and Favorita grocery sales. We show
that ensemble models improve stability without significantly deteriorating (or
even improving) forecast accuracy. While these results may not be surprising,
the main point of this paper is to propose the need for further study of
forecast stability for models that are being deployed in production systems.

</details>


### [14] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型的约束解码方法，首次解决了扩散LLMs在生成符合形式语言（如C++或JSON）输出时的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的生成结果不一定符合形式语言的语法约束，现有约束解码方法不适用于扩散模型，因此需要一种新方法。

Method: 将约束解码问题转化为更一般的加法填充问题，并通过判断目标语言与正则语言的交集是否为空来解决，提出高效算法。

Result: 实验表明，该方法在C++代码填充和JSON数据提取等应用中，几乎完美地保持了语法正确性，同时功能正确性也有所提升。

Conclusion: 该方法不仅高效，且在实际应用中表现出色，为扩散模型的约束解码提供了可行方案。

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [15] [Welfare-Centric Clustering](https://arxiv.org/abs/2508.10345)
*Claire Jie Zhang,Seyed A. Esmaeili,Jamie Morgenstern*

Main category: cs.LG

TL;DR: 本文提出了一种基于福利的公平聚类方法，通过距离和比例代表建模群体效用，并提出了两种优化目标和相应算法。


<details>
  <summary>Details</summary>
Motivation: 传统公平聚类方法可能产生不理想的结果，因此转向福利中心化方法以更好地建模群体效用。

Method: 基于距离和比例代表建模群体效用，提出Rawlsian和Utilitarian两种优化目标，并开发新算法。

Result: 在多个真实数据集上，新方法显著优于现有公平聚类基线。

Conclusion: 福利中心化聚类方法在理论和实践中均表现出优越性。

Abstract: Fair clustering has traditionally focused on ensuring equitable group
representation or equalizing group-specific clustering costs. However,
Dickerson et al. (2025) recently showed that these fairness notions may yield
undesirable or unintuitive clustering outcomes and advocated for a
welfare-centric clustering approach that models the utilities of the groups. In
this work, we model group utilities based on both distances and proportional
representation and formalize two optimization objectives based on
welfare-centric clustering: the Rawlsian (Egalitarian) objective and the
Utilitarian objective. We introduce novel algorithms for both objectives and
prove theoretical guarantees for them. Empirical evaluations on multiple
real-world datasets demonstrate that our methods significantly outperform
existing fair clustering baselines.

</details>


### [16] [Less is More: Learning Graph Tasks with Just LLMs](https://arxiv.org/abs/2508.10115)
*Sola Shirai,Kavitha Srinivas,Julian Dolby,Michael Katz,Horst Samulowitz,Shirin Sohrabi*

Main category: cs.LG

TL;DR: 研究发现，小型LLMs通过训练可以解决图任务，并能泛化到新任务和结构，无需专用图编码器。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能在不依赖专用图编码模型的情况下解决基础图任务，并评估其泛化能力。

Method: 训练LLMs使用链式思维解决方案，测试其在未见过的图结构和任务上的表现。

Result: 小型LLMs能通过学习解决图任务，并能泛化到新任务和结构。

Conclusion: LLMs具备解决图任务的潜力，无需依赖专用图编码器。

Abstract: For large language models (LLMs), reasoning over graphs could help solve many
problems. Prior work has tried to improve LLM graph reasoning by examining how
best to serialize graphs as text and by combining GNNs and LLMs. However, the
merits of such approaches remain unclear, so we empirically answer the
following research questions: (1) Can LLMs learn to solve fundamental graph
tasks without specialized graph encoding models?, (2) Can LLMs generalize
learned solutions to unseen graph structures or tasks?, and (3) What are the
merits of competing approaches to learn graph tasks? We show that even small
LLMs can learn to solve graph tasks by training them with instructive
chain-of-thought solutions, and this training generalizes, without specialized
graph encoders, to new tasks and graph structures.

</details>


### [17] [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Mengyang Zhao,Teng Fu,Bin Li,Xiangyang Xue*

Main category: cs.LG

TL;DR: CAD-RL是一种结合链式思维（CoT）和强化学习的多模态框架，用于从自然语言生成CAD建模代码，显著提高了推理质量、输出精度和代码可执行性。


<details>
  <summary>Details</summary>
Motivation: 当前CAD工作流程需要大量领域知识和手动建模，而直接翻译人类设计意图为可执行CAD代码仍具挑战性。

Method: 提出CAD-RL框架，结合CoT引导的冷启动和目标驱动的强化学习，采用三种任务特定奖励和三种优化策略。

Result: 实验表明，CAD-RL在推理质量、输出精度和代码可执行性上优于现有视觉语言模型。

Conclusion: CAD-RL为自动化CAD建模提供了有效解决方案，并发布了ExeCAD数据集支持训练和基准测试。

Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (LLMs) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under sparse and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.

</details>


### [18] [Confounding is a Pervasive Problem in Real World Recommender Systems](https://arxiv.org/abs/2508.10479)
*Alexander Merkov,David Rohde,Alexandre Gilotte,Benjamin Heymann*

Main category: cs.LG

TL;DR: 论文探讨了推荐系统中未观察到的混杂因素问题，指出常见实践可能引入混杂，影响性能，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 未观察到的混杂因素会导致因果效应估计偏差，影响观察性研究的有效性。推荐系统虽依赖完全观察数据，但某些实践可能忽略特征，引发类似问题。

Method: 通过模拟研究展示常见实践（如特征工程、A/B测试和模块化）如何引入混杂，并提供实际建议。

Result: 研究发现这些实践确实会引入混杂因素，损害推荐系统性能。

Conclusion: 论文提出了减少或避免混杂因素影响的实用建议，帮助优化推荐系统。

Abstract: Unobserved confounding arises when an unmeasured feature influences both the
treatment and the outcome, leading to biased causal effect estimates. This
issue undermines observational studies in fields like economics, medicine,
ecology or epidemiology. Recommender systems leveraging fully observed data
seem not to be vulnerable to this problem. However many standard practices in
recommender systems result in observed features being ignored, resulting in
effectively the same problem. This paper will show that numerous common
practices such as feature engineering, A/B testing and modularization can in
fact introduce confounding into recommendation systems and hamper their
performance. Several illustrations of the phenomena are provided, supported by
simulation studies with practical suggestions about how practitioners may
reduce or avoid the affects of confounding in real systems.

</details>


### [19] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: Nested-ReFT是一种新的强化微调框架，通过动态层跳过降低训练成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统ReFT框架在训练过程中因生成多个推理步骤而带来的高计算成本问题。

Method: 利用目标模型的部分层作为行为模型生成离策略补全，动态跳过层以减少推理成本。

Result: 在数学推理任务中提高了计算效率（tokens/sec），同时性能与基线ReFT相当。

Conclusion: Nested-ReFT通过优化计算效率，为LLM在复杂推理任务中的应用提供了可行方案。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [20] [Contrastive ECOC: Learning Output Codes for Adversarial Defense](https://arxiv.org/abs/2508.10491)
*Che-Yu Chou,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于对比学习的自动编码本学习方法，改进了传统的多类分类任务中的编码机制。


<details>
  <summary>Details</summary>
Motivation: 传统的ECOC方法依赖人工设计或随机生成的编码本，效率低且可能效果不佳。本文旨在通过自动化学习编码本，提高分类任务的鲁棒性。

Method: 提出了三种基于对比学习的模型，直接从数据中自适应学习编码本。

Result: 在四个数据集上，提出的模型在对抗攻击下表现出比基线更强的鲁棒性。

Conclusion: 自动化学习编码本的方法在多类分类任务中具有显著优势，尤其适用于对抗性环境。

Abstract: Although one-hot encoding is commonly used for multiclass classification, it
is not always the most effective encoding mechanism. Error Correcting Output
Codes (ECOC) address multiclass classification by mapping each class to a
unique codeword used as a label. Traditional ECOC methods rely on manually
designed or randomly generated codebooks, which are labor-intensive and may
yield suboptimal, dataset-agnostic results. This paper introduces three models
for automated codebook learning based on contrastive learning, allowing
codebooks to be learned directly and adaptively from data. Across four
datasets, our proposed models demonstrate superior robustness to adversarial
attacks compared to two baselines. The source is available at
https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.

</details>


### [21] [rETF-semiSL: Semi-Supervised Learning for Neural Collapse in Temporal Data](https://arxiv.org/abs/2508.10147)
*Yuhan Xie,William Cappelletti,Mahsa Shoaran,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出了一种新的半监督预训练策略，通过强制潜在表示满足神经崩溃现象，结合生成式预训练任务和时序增强策略，显著提升了时间序列分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督和半监督学习方法在预训练任务选择上依赖启发式方法，且其迁移性不足，因此需要一种更理论化的预训练策略。

Method: 使用旋转等角紧框架分类器和伪标签预训练深度编码器，结合生成式预训练任务和时序增强策略。

Result: 在三个多元时间序列分类数据集上，该方法显著优于现有预训练任务，适用于LSTM、Transformer和状态空间模型。

Conclusion: 预训练目标与理论驱动的嵌入几何对齐能显著提升性能。

Abstract: Deep neural networks for time series must capture complex temporal patterns,
to effectively represent dynamic data. Self- and semi-supervised learning
methods show promising results in pre-training large models, which -- when
finetuned for classification -- often outperform their counterparts trained
from scratch. Still, the choice of pretext training tasks is often heuristic
and their transferability to downstream classification is not granted, thus we
propose a novel semi-supervised pre-training strategy to enforce latent
representations that satisfy the Neural Collapse phenomenon observed in
optimally trained neural classifiers. We use a rotational equiangular tight
frame-classifier and pseudo-labeling to pre-train deep encoders with few
labeled samples. Furthermore, to effectively capture temporal dynamics while
enforcing embedding separability, we integrate generative pretext tasks with
our method, and we define a novel sequential augmentation strategy. We show
that our method significantly outperforms previous pretext tasks when applied
to LSTMs, transformers, and state-space models on three multivariate time
series classification datasets. These results highlight the benefit of aligning
pre-training objectives with theoretically grounded embedding geometry.

</details>


### [22] [Out-of-Distribution Detection using Counterfactual Distance](https://arxiv.org/abs/2508.10148)
*Maria Stoica,Francesco Leofante,Alessio Lomuscio*

Main category: cs.LG

TL;DR: 提出了一种基于反事实解释的后处理OOD检测方法，通过计算输入到决策边界的距离，并结合嵌入空间的优化策略提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 实现准确且可解释的OOD检测，以提升机器学习系统的安全性。

Method: 利用反事实解释计算输入到决策边界的距离，并在嵌入空间中优化计算以提高效率。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200上表现优异，AUROC和FPR95指标优于现有方法。

Conclusion: 该方法不仅检测性能强，还能通过反事实解释提供结果的可解释性。

Abstract: Accurate and explainable out-of-distribution (OOD) detection is required to
use machine learning systems safely. Previous work has shown that feature
distance to decision boundaries can be used to identify OOD data effectively.
In this paper, we build on this intuition and propose a post-hoc OOD detection
method that, given an input, calculates the distance to decision boundaries by
leveraging counterfactual explanations. Since computing explanations can be
expensive for large architectures, we also propose strategies to improve
scalability by computing counterfactuals directly in embedding space.
Crucially, as the method employs counterfactual explanations, we can seamlessly
use them to help interpret the results of our detector. We show that our method
is in line with the state of the art on CIFAR-10, achieving 93.50% AUROC and
25.80% FPR95. Our method outperforms these methods on CIFAR-100 with 97.05%
AUROC and 13.79% FPR95 and on ImageNet-200 with 92.55% AUROC and 33.55% FPR95
across four OOD datasets

</details>


### [23] [Characterizing Evolution in Expectation-Maximization Estimates for Overspecified Mixed Linear Regression](https://arxiv.org/abs/2508.10154)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本文研究了EM算法在过拟合两成分混合线性回归（2MLR）中的行为，分析了模型误设对收敛速度和统计精度的影响。


<details>
  <summary>Details</summary>
Motivation: 解决模型误设（即模型成分多于数据分布成分）对EM算法行为的影响，特别是在2MLR中的理论分析。

Method: 在理论和有限样本层面分析EM算法的收敛性，区分不平衡和平衡初始混合权重的影响。

Result: 不平衡初始权重下回归参数线性收敛（O(log(1/ϵ))），平衡初始权重下亚线性收敛（O(ϵ⁻²)）；有限样本下统计精度分别为O((d/n)¹/²)和O((d/n)¹/⁴)。

Conclusion: EM算法在过拟合2MLR中的收敛行为与初始权重和模型误设密切相关，为实际应用提供了理论指导。

Abstract: Mixture models have attracted significant attention due to practical
effectiveness and comprehensive theoretical foundations. A persisting challenge
is model misspecification, which occurs when the model to be fitted has more
mixture components than those in the data distribution. In this paper, we
develop a theoretical understanding of the Expectation-Maximization (EM)
algorithm's behavior in the context of targeted model misspecification for
overspecified two-component Mixed Linear Regression (2MLR) with unknown
$d$-dimensional regression parameters and mixing weights. In Theorem 5.1 at the
population level, with an unbalanced initial guess for mixing weights, we
establish linear convergence of regression parameters in $O(\log(1/\epsilon))$
steps. Conversely, with a balanced initial guess for mixing weights, we observe
sublinear convergence in $O(\epsilon^{-2})$ steps to achieve the
$\epsilon$-accuracy at Euclidean distance. In Theorem 6.1 at the finite-sample
level, for mixtures with sufficiently unbalanced fixed mixing weights, we
demonstrate a statistical accuracy of $O((d/n)^{1/2})$, whereas for those with
sufficiently balanced fixed mixing weights, the accuracy is $O((d/n)^{1/4})$
given $n$ data samples. Furthermore, we underscore the connection between our
population level and finite-sample level results: by setting the desired final
accuracy $\epsilon$ in Theorem 5.1 to match that in Theorem 6.1 at the
finite-sample level, namely letting $\epsilon = O((d/n)^{1/2})$ for
sufficiently unbalanced fixed mixing weights and $\epsilon = O((d/n)^{1/4})$
for sufficiently balanced fixed mixing weights, we intuitively derive iteration
complexity bounds $O(\log (1/\epsilon))=O(\log (n/d))$ and
$O(\epsilon^{-2})=O((n/d)^{1/2})$ at the finite-sample level for sufficiently
unbalanced and balanced initial mixing weights. We further extend our analysis
in overspecified setting to low SNR regime.

</details>


### [24] [Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1](https://arxiv.org/abs/2508.10173)
*Petr Spelda,Vit Stritecky*

Main category: cs.LG

TL;DR: 论文探讨了推理语言模型的评估，指出性能提升不仅来自算法改进或模型规模，还受基准测试作为学习课程的影响。


<details>
  <summary>Details</summary>
Motivation: 随着推理成为大型语言模型的下一个扩展维度，需要深入研究其在关键任务中的能力，以更好地理解其泛化能力。

Method: 通过基准驱动的AI选择方法，研究了DeepSeek-R1模型在顺序决策问题中的表现。

Result: 研究发现，某些基准测试可以作为训练课程而非未见的测试集，从而提升模型的泛化能力。

Conclusion: 通过基准测试引导AI发展，将评估转化为学习过程，强调了测试任务的新颖性对衡量推理模型泛化能力的重要性。

Abstract: Evaluation of reasoning language models gained importance after it was
observed that they can combine their existing capabilities into novel traces of
intermediate steps before task completion and that the traces can sometimes
help them to generalize better than past models. As reasoning becomes the next
scaling dimension of large language models, careful study of their capabilities
in critical tasks is needed. We show that better performance is not always
caused by test-time algorithmic improvements or model sizes but also by using
impactful benchmarks as curricula for learning. We call this benchmark-driven
selection of AI and show its effects on DeepSeek-R1 using our sequential
decision-making problem from Humanity's Last Exam. Steering development of AI
by impactful benchmarks trades evaluation for learning and makes novelty of
test tasks key for measuring generalization capabilities of reasoning models.
Consequently, some benchmarks could be seen as curricula for training rather
than unseen test sets.

</details>


### [25] [An Explainable AI based approach for Monitoring Animal Health](https://arxiv.org/abs/2508.10210)
*Rahul Janaa,Shubham Dixit,Mrityunjay Sharma,Ritesh Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种基于可解释机器学习的方法，通过3轴加速度传感器和物联网设备监测奶牛活动，帮助农民优化养殖管理。


<details>
  <summary>Details</summary>
Motivation: 由于难以追踪农场所有动物，监测奶牛健康和优化产量是奶农面临的主要挑战。

Method: 利用蓝牙物联网设备和4G网络收集数据，结合信号处理和滑动窗口技术提取特征，采用超参数优化的机器学习模型进行分类。

Result: k-最近邻分类器表现最佳，训练集AUC均值0.98，测试集0.99。SHAP框架用于解释特征重要性。

Conclusion: 该方法为可持续畜牧业提供了可解释且实用的机器学习模型。

Abstract: Monitoring cattle health and optimizing yield are key challenges faced by
dairy farmers due to difficulties in tracking all animals on the farm. This
work aims to showcase modern data-driven farming practices based on explainable
machine learning(ML) methods that explain the activity and behaviour of dairy
cattle (cows). Continuous data collection of 3-axis accelerometer sensors and
usage of robust ML methodologies and algorithms, provide farmers and
researchers with actionable information on cattle activity, allowing farmers to
make informed decisions and incorporate sustainable practices. This study
utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for
seamless data transmission, immediate analysis, inference generation, and
explains the models performance with explainability frameworks. Special
emphasis is put on the pre-processing of the accelerometers time series data,
including the extraction of statistical characteristics, signal processing
techniques, and lag-based features using the sliding window technique. Various
hyperparameter-optimized ML models are evaluated across varying window lengths
for activity classification. The k-nearest neighbour Classifier achieved the
best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the
training set and 0.99 on testing set). In order to ensure transparency,
Explainable AI based frameworks such as SHAP is used to interpret feature
importance that can be understood and used by practitioners. A detailed
comparison of the important features, along with the stability analysis of
selected features, supports development of explainable and practical ML models
for sustainable livestock management.

</details>


### [26] [AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade](https://arxiv.org/abs/2508.10219)
*Will Fein,Ryan J. Horwitz,John E. Brown III,Amit Misra,Felipe Oviedo,Kevin White,Juan M. Lavista Ferres,Samuel K. Wasser*

Main category: cs.LG

TL;DR: AI分析象牙上的手写标记，为打击野生动物犯罪提供低成本、可扩展的取证方法。


<details>
  <summary>Details</summary>
Motivation: 象牙贸易导致非洲象数量下降，但现有取证方法（如DNA分析）成本高且有时不可行，而手写标记虽易获取却未被充分利用。

Method: 利用AI驱动的流程，从6,085张象牙照片中提取17,000多个手写标记，并通过目标检测模型和AI工具识别184个重复标记，其中20个标记连接多个查获批次。

Result: 发现20个重复标记连接多个象牙查获批次，证明AI手写分析可作为补充取证手段。

Conclusion: AI手写分析填补了取证空白，展示了其在野生动物犯罪调查中的潜力。

Abstract: The transnational ivory trade continues to drive the decline of elephant
populations across Africa, and trafficking networks remain difficult to
disrupt. Tusks seized by law enforcement officials carry forensic information
on the traffickers responsible for their export, including DNA evidence and
handwritten markings made by traffickers. For 20 years, analyses of tusk DNA
have identified where elephants were poached and established connections among
shipments of ivory. While the links established using genetic evidence are
extremely conclusive, genetic data is expensive and sometimes impossible to
obtain. But though handwritten markings are easy to photograph, they are rarely
documented or analyzed. Here, we present an AI-driven pipeline for extracting
and analyzing handwritten markings on seized elephant tusks, offering a novel,
scalable, and low-cost source of forensic evidence. Having collected 6,085
photographs from eight large seizures of ivory over a 6-year period
(2014-2019), we used an object detection model to extract over 17,000
individual markings, which were then labeled and described using
state-of-the-art AI tools. We identified 184 recurring "signature markings"
that connect the tusks on which they appear. 20 signature markings were
observed in multiple seizures, establishing forensic links between these
seizures through traffickers involved in both shipments. This work complements
other investigative techniques by filling in gaps where other data sources are
unavailable. The study demonstrates the transformative potential of AI in
wildlife forensics and highlights practical steps for integrating handwriting
analysis into efforts to disrupt organized wildlife crime.

</details>


### [27] [Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine](https://arxiv.org/abs/2508.10228)
*Abdelmoula El Yazizi,Samee U. Khan,Yaroslav Koshka*

Main category: cs.LG

TL;DR: 研究比较了D-Wave量子退火器和Gibbs采样在RBM训练中的表现，发现两者在局部谷（LV）覆盖上差异显著，但重叠较多，潜在改进空间在于结合经典与量子方法。


<details>
  <summary>Details</summary>
Motivation: 评估D-Wave量子退火器在RBM采样中的表现，探索其是否优于传统Gibbs采样，并分析潜在改进方向。

Method: 使用局部谷（LV）中心化方法，比较D-Wave和Gibbs采样在RBM训练中的LV覆盖和能量分布。

Result: D-Wave采样覆盖更多LV，但与Gibbs采样重叠较多；后期训练阶段差异更显著，但未显著提升采样质量。

Conclusion: D-Wave采样未显著优于Gibbs采样，但结合经典与量子方法可能带来改进。

Abstract: A local-valley (LV) centered approach to assessing the quality of sampling
from Restricted Boltzmann Machines (RBMs) was applied to the latest generation
of the D-Wave quantum annealer. D-Wave and Gibbs samples from a classically
trained RBM were obtained at conditions relevant to the
contrastive-divergence-based RBM learning. The samples were compared for the
number of the LVs to which they belonged and the energy of the corresponding
local minima. No significant (desirable) increase in the number of the LVs has
been achieved by decreasing the D-Wave annealing time. At any training epoch,
the states sampled by the D-Wave belonged to a somewhat higher number of LVs
than in the Gibbs sampling. However, many of those LVs found by the two
techniques differed. For high-probability sampled states, the two techniques
were (unfavorably) less complementary and more overlapping. Nevertheless, many
potentially "important" local minima, i.e., those having intermediate, even if
not high, probability values, were found by only one of the two sampling
techniques while missed by the other. The two techniques overlapped less at
later than earlier training epochs, which is precisely the stage of the
training when modest improvements to the sampling quality could make meaningful
differences for the RBM trainability. The results of this work may explain the
failure of previous investigations to achieve substantial (or any) improvement
when using D-Wave-based sampling. However, the results reveal some potential
for improvement, e.g., using a combined classical-quantum approach.

</details>


### [28] [Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study](https://arxiv.org/abs/2508.10233)
*Li Sun,Shuheng Chen,Junyi Fan,Yong Si,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 该研究开发了一种可解释的机器学习模型，用于早期预测肝硬化重症患者的急性肾损伤（AKI）。


<details>
  <summary>Details</summary>
Motivation: 肝硬化患者中AKI发生率高且预后差，现有预测工具准确性不足且不符合ICU工作流程，因此需要开发更准确的早期预测方法。

Method: 研究回顾性分析了MIMIC-IV v2.2数据库，筛选了1240名肝硬化ICU患者，提取了48小时内的实验室和生理数据，通过预处理、特征选择和算法训练（包括LightGBM等6种方法）构建预测模型。

Result: LightGBM表现最佳（AUROC 0.808，准确率0.704），关键预测因子包括部分凝血活酶时间延长、低pH值等，与肝硬化AKI机制一致。

Conclusion: 该模型能准确早期预测肝硬化ICU患者的AKI风险，高阴性预测值支持低风险患者的安全管理，模型的可解释性有助于临床信任和针对性预防。

Abstract: Background: Cirrhosis is a progressive liver disease with high mortality and
frequent complications, notably acute kidney injury (AKI), which occurs in up
to 50% of hospitalized patients and worsens outcomes. AKI stems from complex
hemodynamic, inflammatory, and metabolic changes, making early detection
essential. Many predictive tools lack accuracy, interpretability, and alignment
with intensive care unit (ICU) workflows. This study developed an interpretable
machine learning model for early AKI prediction in critically ill patients with
cirrhosis.
  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,
identifying 1240 adult ICU patients with cirrhosis and excluding those with ICU
stays under 48 hours or missing key data. Laboratory and physiological
variables from the first 48 hours were extracted. The pipeline included
preprocessing, missingness filtering, LASSO feature selection, and SMOTE class
balancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,
naive Bayes, and neural networks-were trained and evaluated using AUROC,
accuracy, F1-score, sensitivity, specificity, and predictive values.
  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI
0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged
partial thromboplastin time, absence of outside-facility 20G placement, low pH,
and altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting
actionable targets.
  Conclusion: The LightGBM-based model enables accurate early AKI risk
stratification in ICU patients with cirrhosis using routine clinical variables.
Its high negative predictive value supports safe de-escalation for low-risk
patients, and interpretability fosters clinician trust and targeted prevention.
External validation and integration into electronic health record systems are
warranted.

</details>


### [29] [Can Transformers Break Encryption Schemes via In-Context Learning?](https://arxiv.org/abs/2508.10235)
*Jathin Korrapati,Patrick Mendoza,Aditya Tomar,Abein Abraham*

Main category: cs.LG

TL;DR: 论文提出了一种新的上下文学习（ICL）应用，用于学习密码学函数，如单字母替换和Vigenère密码，以评估Transformer模型的归纳偏差和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在密码学领域的上下文学习能力，特别是对隐藏双射映射的推理能力。

Method: 通过少量（密文，明文）对，训练模型推断密码替换规则，并解码新密文。

Result: 模型能够从上下文中学习并推断密码学函数的隐藏映射。

Conclusion: ICL在密码学函数学习中具有潜力，为Transformer的推理能力提供了新的评估场景。

Abstract: In-context learning (ICL) has emerged as a powerful capability of
transformer-based language models, enabling them to perform tasks by
conditioning on a small number of examples presented at inference time, without
any parameter updates. Prior work has shown that transformers can generalize
over simple function classes like linear functions, decision trees, even neural
networks, purely from context, focusing on numerical or symbolic reasoning over
underlying well-structured functions. Instead, we propose a novel application
of ICL into the domain of cryptographic function learning, specifically
focusing on ciphers such as mono-alphabetic substitution and Vigen\`ere
ciphers, two classes of private-key encryption schemes. These ciphers involve a
fixed but hidden bijective mapping between plain text and cipher text
characters. Given a small set of (cipher text, plain text) pairs, the goal is
for the model to infer the underlying substitution and decode a new cipher text
word. This setting poses a structured inference challenge, which is well-suited
for evaluating the inductive biases and generalization capabilities of
transformers under the ICL paradigm. Code is available at
https://github.com/adistomar/CS182-project.

</details>


### [30] [Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models](https://arxiv.org/abs/2508.10243)
*Taibiao Zhao,Mingxuan Sun,Hao Wang,Xiaobing Chen,Xiangwei Zhou*

Main category: cs.LG

TL;DR: 提出了一种无需重新训练的Transformer后门攻击方法HPMI，通过剪枝和注入恶意头实现，具有高隐蔽性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer后门攻击方法依赖重新训练或修改架构，资源消耗大且侵入性强。

Method: HPMI通过剪枝最不重要头并注入预训练恶意头，无需重新训练或修改架构。

Result: 实验显示HPMI攻击成功率达99.55%，且能绕过四种先进防御机制，对干净数据准确率影响极小。

Conclusion: HPMI是一种高效、隐蔽且鲁棒的后门攻击方法，优于依赖重新训练的现有方法。

Abstract: Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that transformers are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
transformers that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target transformer.
Technically, HPMI works by pruning the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.

</details>


### [31] [Convergence Analysis of Max-Min Exponential Neural Network Operators in Orlicz Space](https://arxiv.org/abs/2508.10248)
*Satyaranjan Pradhan,Madan Mohan Soren*

Main category: cs.LG

TL;DR: 提出了一种基于Max Min方法的指数神经网络算子，扩展为Kantorovich型算子，研究了点态和一致收敛性，并在Orlicz空间中分析了收敛行为。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的神经网络算子框架，以改进函数逼近的性能和效率。

Method: 使用Max Min方法和指数神经网络算子，结合Kantorovich型扩展，分析点态和一致收敛性，利用对数连续性模估计收敛速率。

Result: 在Orlicz空间中验证了算子的收敛行为，并通过图形展示了逼近误差。

Conclusion: Max Min Kantorovich型指数神经网络算子在函数逼近中表现出良好的收敛性和实用性。

Abstract: In this current work, we propose a Max Min approach for approximating
functions using exponential neural network operators. We extend this framework
to develop the Max Min Kantorovich-type exponential neural network operators
and investigate their approximation properties. We study both pointwise and
uniform convergence for univariate functions. To analyze the order of
convergence, we use the logarithmic modulus of continuity and estimate the
corresponding rate of convergence. Furthermore, we examine the convergence
behavior of the Max Min Kantorovich type exponential neural network operators
within the Orlicz space setting. We provide some graphical representations to
illustrate the approximation error of the function through suitable kernel and
sigmoidal activation functions.

</details>


### [32] [Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters](https://arxiv.org/abs/2508.10253)
*Guanzi Yao,Heyao Liu,Linyan Dai*

Main category: cs.LG

TL;DR: 本文提出了一种基于多智能体强化学习的自适应资源编排方法，用于解决云原生数据库系统中的高资源动态性和调度复杂性。


<details>
  <summary>Details</summary>
Motivation: 云原生数据库系统面临高资源动态性和调度复杂性的挑战，传统方法难以有效应对。

Method: 采用多智能体强化学习，引入异构角色建模机制和奖励塑造机制，结合局部观察与全局反馈。

Result: 实验表明，该方法在资源利用率、调度延迟、策略收敛速度等方面优于传统方法。

Conclusion: 该方法在复杂调度环境中表现出强泛化能力和实用性。

Abstract: This paper addresses the challenges of high resource dynamism and scheduling
complexity in cloud-native database systems. It proposes an adaptive resource
orchestration method based on multi-agent reinforcement learning. The method
introduces a heterogeneous role-based agent modeling mechanism. This allows
different resource entities, such as compute nodes, storage nodes, and
schedulers, to adopt distinct policy representations. These agents are better
able to reflect diverse functional responsibilities and local environmental
characteristics within the system. A reward-shaping mechanism is designed to
integrate local observations with global feedback. This helps mitigate policy
learning bias caused by incomplete state observations. By combining real-time
local performance signals with global system value estimation, the mechanism
improves coordination among agents and enhances policy convergence stability. A
unified multi-agent training framework is developed and evaluated on a
representative production scheduling dataset. Experimental results show that
the proposed method outperforms traditional approaches across multiple key
metrics. These include resource utilization, scheduling latency, policy
convergence speed, system stability, and fairness. The results demonstrate
strong generalization and practical utility. Across various experimental
scenarios, the method proves effective in handling orchestration tasks with
high concurrency, high-dimensional state spaces, and complex dependency
relationships. This confirms its advantages in real-world, large-scale
scheduling environments.

</details>


### [33] [Federated Anomaly Detection for Multi-Tenant Cloud Platforms with Personalized Modeling](https://arxiv.org/abs/2508.10255)
*Yuxi Wang,Heyao Liu,Nyutian Long,Guanzi Yao*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的异常检测方法，解决多租户云环境中的数据隐私泄露、资源行为异质性和集中式建模限制等问题。


<details>
  <summary>Details</summary>
Motivation: 多租户云环境中数据隐私保护和异质性资源行为是主要挑战，传统集中式建模存在局限性。

Method: 建立联邦训练框架，各租户用本地数据训练模型，通过参数聚合优化全局模型，引入个性化参数调整机制，并使用马氏距离计算异常分数。

Result: 实验表明，该方法在精度、召回率和F1分数上优于主流模型，且在复杂场景中表现稳定。

Conclusion: 该方法在多租户云环境中具有实际应用潜力，适用于智能资源监控和异常诊断。

Abstract: This paper proposes an anomaly detection method based on federated learning
to address key challenges in multi-tenant cloud environments, including data
privacy leakage, heterogeneous resource behavior, and the limitations of
centralized modeling. The method establishes a federated training framework
involving multiple tenants. Each tenant trains the model locally using private
resource usage data. Through parameter aggregation, a global model is
optimized, enabling cross-tenant collaborative anomaly detection while
preserving data privacy. To improve adaptability to diverse resource usage
patterns, a personalized parameter adjustment mechanism is introduced. This
allows the model to retain tenant-specific feature representations while
sharing global knowledge. In the model output stage, the Mahalanobis distance
is used to compute anomaly scores. This enhances both the accuracy and
stability of anomaly detection. The experiments use real telemetry data from a
cloud platform to construct a simulated multi-tenant environment. The study
evaluates the model's performance under varying participation rates and noise
injection levels. These comparisons demonstrate the proposed method's
robustness and detection accuracy. Experimental results show that the proposed
method outperforms existing mainstream models across key metrics such as
Precision, Recall, and F1-Score. It also maintains stable performance in
various complex scenarios. These findings highlight the method's practical
potential for intelligent resource monitoring and anomaly diagnosis in cloud
computing environments.

</details>


### [34] [Source Component Shift Adaptation via Offline Decomposition and Online Mixing Approach](https://arxiv.org/abs/2508.10257)
*Ryuta Matsuno*

Main category: cs.LG

TL;DR: 提出了一种基于离线分解和在线混合的方法，用于解决源组件偏移适应问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线学习方法难以有效利用重复偏移，而基于模型池的方法无法捕捉单个源组件，导致适应性能不佳。

Method: 通过EM算法离线分解源组件，再通过在线凸优化更新混合权重。

Result: 在多个真实回归数据集上，累积测试损失降低高达67.4%。

Conclusion: 该方法充分利用偏移特性，实现了优于现有方法的适应性能。

Abstract: This paper addresses source component shift adaptation, aiming to update
predictions adapting to source component shifts for incoming data streams based
on past training data. Existing online learning methods often fail to utilize
recurring shifts effectively, while model-pool-based methods struggle to
capture individual source components, leading to poor adaptation. In this
paper, we propose a source component shift adaptation method via an offline
decomposition and online mixing approach. We theoretically identify that the
problem can be divided into two subproblems: offline source component
decomposition and online mixing weight adaptation. Based on this, our method
first determines prediction models, each of which learns a source component
solely based on past training data offline through the EM algorithm. Then, it
updates the mixing weight of the prediction models for precise prediction
through online convex optimization. Thanks to our theoretical derivation, our
method fully leverages the characteristics of the shifts, achieving superior
adaptation performance over existing methods. Experiments conducted on various
real-world regression datasets demonstrate that our method outperforms
baselines, reducing the cumulative test loss by up to 67.4%.

</details>


### [35] [Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach](https://arxiv.org/abs/2508.10284)
*Ricardo Diaz-Rincon,Muxuan Liang,Adolfo Ramirez-Zamora,Benjamin Shickel*

Main category: cs.LG

TL;DR: 论文提出了一种基于共形预测的框架，用于预测帕金森病患者的药物需求，并提供可靠的预测区间和统计保证，以优化药物治疗方案。


<details>
  <summary>Details</summary>
Motivation: 帕金森病药物治疗管理因疾病进展和反应异质性而复杂，现有方法依赖试错决策，缺乏系统预测方法，且机器学习预测未考虑不确定性，影响临床信任和实用性。

Method: 开发了一种共形预测框架，处理住院数据中的零膨胀问题，分两阶段预测药物需求：先识别需调整药物的患者，再预测左旋多巴等效日剂量调整。

Result: 框架在佛罗里达大学健康系统的631例住院数据中验证，实现了边际覆盖并缩短预测区间长度，提供精确短期预测和宽泛长期预测。

Conclusion: 通过量化不确定性，该框架支持基于证据的左旋多巴剂量决策，优化症状控制并减少副作用，提高生活质量。

Abstract: Parkinson's Disease (PD) medication management presents unique challenges due
to heterogeneous disease progression and treatment response. Neurologists must
balance symptom control with optimal dopaminergic dosing based on functional
disability while minimizing side effects. This balance is crucial as inadequate
or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and
neuropsychiatric effects, significantly reducing quality of life. Current
approaches rely on trial-and-error decisions without systematic predictive
methods. Despite machine learning advances, clinical adoption remains limited
due to reliance on point predictions that do not account for prediction
uncertainty, undermining clinical trust and utility. Clinicians require not
only predictions of future medication needs but also reliable confidence
measures. Without quantified uncertainty, adjustments risk premature escalation
to maximum doses or prolonged inadequate symptom control. We developed a
conformal prediction framework anticipating medication needs up to two years in
advance with reliable prediction intervals and statistical guarantees. Our
approach addresses zero-inflation in PD inpatient data, where patients maintain
stable medication regimens between visits. Using electronic health records from
631 inpatient admissions at University of Florida Health (2011-2021), our
two-stage approach identifies patients likely to need medication changes, then
predicts required levodopa equivalent daily dose adjustments. Our framework
achieved marginal coverage while reducing prediction interval lengths compared
to traditional approaches, providing precise predictions for short-term
planning and wider ranges for long-term forecasting. By quantifying
uncertainty, our approach enables evidence-based decisions about levodopa
dosing, optimizing symptom control while minimizing side effects and improving
life quality.

</details>


### [36] [SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning](https://arxiv.org/abs/2508.10298)
*Weijian Mai,Jiamin Wu,Yu Zhu,Zhouheng Yao,Dongzhan Zhou,Andrew F. Luo,Qihao Zheng,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: SynBrain是一个生成框架，通过概率建模和语义约束，解决了视觉到神经映射中的生物变异性问题，提升了fMRI编码和解码性能。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法难以同时建模生物变异性与功能一致性，SynBrain旨在填补这一空白。

Method: SynBrain结合BrainVAE（概率学习）和语义到神经映射器，实现高保真fMRI合成。

Result: SynBrain在视觉到fMRI编码性能上超越现有方法，并支持少样本适应和高质量信号合成。

Conclusion: SynBrain不仅提升了性能，还揭示了跨试验和受试者的功能一致性，具有生物可解释性。

Abstract: Deciphering how visual stimuli are transformed into cortical responses is a
fundamental challenge in computational neuroscience. This visual-to-neural
mapping is inherently a one-to-many relationship, as identical visual inputs
reliably evoke variable hemodynamic responses across trials, contexts, and
subjects. However, existing deterministic methods struggle to simultaneously
model this biological variability while capturing the underlying functional
consistency that encodes stimulus information. To address these limitations, we
propose SynBrain, a generative framework that simulates the transformation from
visual semantics to neural responses in a probabilistic and biologically
interpretable manner. SynBrain introduces two key components: (i) BrainVAE
models neural representations as continuous probability distributions via
probabilistic learning while maintaining functional consistency through visual
semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic
transmission pathway, projecting visual semantics into the neural response
manifold to facilitate high-fidelity fMRI synthesis. Experimental results
demonstrate that SynBrain surpasses state-of-the-art methods in
subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain
adapts efficiently to new subjects with few-shot data and synthesizes
high-quality fMRI signals that are effective in improving data-limited
fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional
consistency across trials and subjects, with synthesized signals capturing
interpretable patterns shaped by biological neural variability. The code will
be made publicly available.

</details>


### [37] [Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning](https://arxiv.org/abs/2508.10299)
*Danni Peng,Yuan Wang,Kangning Cai,Peiyan Ning,Jiming Xu,Yong Liu,Rick Siow Mong Goh,Qingsong Wei,Huazhu Fu*

Main category: cs.LG

TL;DR: FedKEI是一种新的联邦学习框架，通过跨客户端和跨任务的知识转移，为新任务生成初始化参数，提升医疗领域中的任务适应性。


<details>
  <summary>Details</summary>
Motivation: 医疗环境中需要快速适应新任务或疾病，同时保护隐私，FedKEI旨在通过知识转移实现这一目标。

Method: FedKEI通过全局聚类和双层优化方案，学习跨集群和集群内的权重，个性化知识转移。

Result: 在皮肤病、胸部X光和视网膜OCT数据集上，FedKEI优于现有方法，能更好地适应新疾病。

Conclusion: FedKEI通过知识转移和个性化优化，显著提升了联邦学习在医疗任务中的适应能力。

Abstract: In healthcare, federated learning (FL) is a widely adopted framework that
enables privacy-preserving collaboration among medical institutions. With large
foundation models (FMs) demonstrating impressive capabilities, using FMs in FL
through cost-efficient adapter tuning has become a popular approach. Given the
rapidly evolving healthcare environment, it is crucial for individual clients
to quickly adapt to new tasks or diseases by tuning adapters while drawing upon
past experiences. In this work, we introduce Federated Knowledge-Enhanced
Initialization (FedKEI), a novel framework that leverages cross-client and
cross-task transfer from past knowledge to generate informed initializations
for learning new tasks with adapters. FedKEI begins with a global clustering
process at the server to generalize knowledge across tasks, followed by the
optimization of aggregation weights across clusters (inter-cluster weights) and
within each cluster (intra-cluster weights) to personalize knowledge transfer
for each new task. To facilitate more effective learning of the inter- and
intra-cluster weights, we adopt a bi-level optimization scheme that
collaboratively learns the global intra-cluster weights across clients and
optimizes the local inter-cluster weights toward each client's task objective.
Extensive experiments on three benchmark datasets of different modalities,
including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's
advantage in adapting to new diseases compared to state-of-the-art methods.

</details>


### [38] [A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.10315)
*Keke Gai,Dongjue Wang,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.LG

TL;DR: CLIP-Fed是一个联邦学习中的后门防御框架，利用视觉语言预训练模型的零样本学习能力，解决了异构数据分布下的防御问题，并通过原型对比损失和KL散度优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的后门防御方法依赖于同质数据分布或干净服务器数据集的假设，限制了实用性和有效性，异构数据分布下的防御仍具挑战性。

Method: CLIP-Fed结合预聚合和后聚合防御策略，利用多模态大语言模型和频率分析构建服务器数据集，通过原型对比损失和KL散度对齐全局模型与CLIP的知识。

Result: 在CIFAR-10和CIFAR-10-LT数据集上，CLIP-Fed平均降低ASR（2.03%和1.35%），同时提高MA（7.92%和0.48%）。

Conclusion: CLIP-Fed在异构数据分布下有效防御后门攻击，同时提升模型性能，优于现有方法。

Abstract: Existing backdoor defense methods in Federated Learning (FL) rely on the
assumption of homogeneous client data distributions or the availability of a
clean serve dataset, which limits the practicality and effectiveness. Defending
against backdoor attacks under heterogeneous client data distributions while
preserving model performance remains a significant challenge. In this paper, we
propose a FL backdoor defense framework named CLIP-Fed, which leverages the
zero-shot learning capabilities of vision-language pre-training models. By
integrating both pre-aggregation and post-aggregation defense strategies,
CLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.
To address privacy concerns and enhance the coverage of the dataset against
diverse triggers, we construct and augment the server dataset using the
multimodal large language model and frequency analysis without any client
samples. To address class prototype deviations caused by backdoor samples and
eliminate the correlation between trigger patterns and target labels, CLIP-Fed
aligns the knowledge of the global model and CLIP on the augmented dataset
using prototype contrastive loss and Kullback-Leibler divergence. Extensive
experiments on representative datasets validate the effectiveness of CLIP-Fed.
Compared to state-of-the-art methods, CLIP-Fed achieves an average reduction in
ASR, i.e., 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving
average MA by 7.92\% and 0.48\%, respectively.

</details>


### [39] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: 论文提出了一种多层次的IoMT入侵检测系统框架，能够检测零日攻击并区分已知和未知威胁，实验显示高准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: IoMT设备易受网络攻击，传统集中式IDS因延迟和隐私问题不适用，需要一种高效且适用于资源受限设备的解决方案。

Method: 采用多层次框架，第一层（近边缘）使用元学习或OCC进行粗粒度流量过滤，后续层（远边缘、云）识别攻击类型和新颖性。

Result: 在CICIoMT2024数据集上达到99.77%准确率和97.8% F1分数，第一层能高效检测零日攻击。

Conclusion: 该框架在IoMT环境中具有强适用性，能有效应对零日攻击和已知威胁。

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [40] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 本文研究了语义通信中的分布学习问题，揭示了学习源统计量的基本条件，并量化了估计误差对语义失真的影响。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信专注于单个意义的传输优化，但缺乏对未知先验条件下源统计量学习的理解。

Method: 通过理论分析，证明了学习性需要有效传输矩阵的满秩性，并量化了分布估计的收敛速度和语义失真。

Result: 实验验证了系统条件对学习速率和性能的关键影响，揭示了编码方案在即时性能与长期学习性之间的权衡。

Conclusion: 研究首次严格刻画了语义通信中的统计学习，为平衡即时性能与适应能力的系统设计提供了原则。

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [41] [eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing](https://arxiv.org/abs/2508.10370)
*Jiyong Kim,Jaeho Lee,Jiahao Lin,Alish Kanani,Miao Sun,Umit Y. Ogras,Jaehyun Park*

Main category: cs.LG

TL;DR: eMamba是一个专为边缘设备优化的硬件加速框架，用于部署Mamba模型，通过轻量级替代和近似操作提升效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在边缘设备上的部署缺乏优化的硬件加速框架，限制了其实际应用潜力。

Method: eMamba通过替换复杂归一化层、近似昂贵操作，并进行近似感知的神经架构搜索（NAS）来优化模型。

Result: 在多个数据集上，eMamba以更少的参数实现与现有技术相当的准确性，并在FPGA和ASIC上显著降低延迟、提高吞吐量，同时减少资源消耗。

Conclusion: eMamba为Mamba模型在边缘设备上的高效部署提供了可行的解决方案，兼具性能和资源效率。

Abstract: State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art transformer models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware acceleration frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware acceleration framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62$\times$ lower latency and
2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area,
9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than
baseline solutions while maintaining competitive accuracy.

</details>


### [42] [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
*Aditya Tomar,Coleman Hooper,Minjae Lee,Haocheng Xi,Rishabh Tiwari,Wonjun Kang,Luca Manolache,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: XQuant通过低比特量化和动态重构KV缓存，显著减少LLM推理的内存占用，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: LLM推理因内存占用和带宽需求大而效率低下，而计算能力增长快于内存能力，需新算法平衡计算与内存。

Method: XQuant量化并缓存层输入激活X，动态重构KV，减少内存操作；XQuant-CL利用跨层相似性进一步压缩。

Result: XQuant实现7.7倍内存节省，困惑度损失<0.1；XQuant-CL达10倍节省，困惑度损失仅0.01。

Conclusion: XQuant利用硬件计算优势消除内存瓶颈，超越现有KV缓存量化方法，接近FP16精度。

Abstract: Although LLM inference has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of LLM inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2$\times$
memory savings compared to KV caching. By applying XQuant, we achieve up to
$\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10$\times$ memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5$\times$ memory savings with only $0.1$
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art KV cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.

</details>


### [43] [SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks](https://arxiv.org/abs/2508.10428)
*Pengbo Shen,Yaqing Wang,Ni Mu,Yao Luan,Runpeng Xie,Senhao Yang,Lexiang Wang,Hao Hu,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 论文提出了SC2Arena和StarEvolve，用于全面评估LLMs在复杂决策（如《星际争霸II》）中的表现，解决了现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能全面捕捉《星际争霸II》的复杂性，如完整游戏背景、多样化动作空间和所有可玩种族。

Method: 提出SC2Arena基准支持所有可玩种族和低层动作空间，并优化文本观察；StarEvolve框架结合战略规划与战术执行，通过迭代自校正和高质量数据微调实现持续改进。

Result: 实验表明StarEvolve在战略规划中表现优异，SC2Arena为开发通用智能体提供了新见解。

Conclusion: SC2Arena和StarEvolve填补了现有基准的不足，为复杂决策任务提供了更全面的评估工具。

Abstract: Evaluating large language models (LLMs) in complex decision-making is
essential for advancing AI's ability for strategic planning and real-time
adaptation. However, existing benchmarks for tasks like StarCraft II fail to
capture the game's full complexity, such as its complete game context, diverse
action spaces, and all playable races. To address this gap, we present
SC2Arena, a benchmark that fully supports all playable races, low-level action
spaces, and optimizes text-based observations to tackle spatial reasoning
challenges. Complementing this, we introduce StarEvolve, a hierarchical
framework that integrates strategic planning with tactical execution, featuring
iterative self-correction and continuous improvement via fine-tuning on
high-quality gameplay data. Its key components include a
Planner-Executor-Verifier structure to break down gameplay, and a scoring
system for selecting high-quality training samples. Comprehensive analysis
using SC2Arena provides valuable insights into developing generalist agents
that were not possible with previous benchmarks. Experimental results also
demonstrate that our proposed StarEvolve achieves superior performance in
strategic planning. Our code, environment, and algorithms are publicly
available.

</details>


### [44] [Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models](https://arxiv.org/abs/2508.10435)
*Tianxiao Cao,Kyohei Atarashi,Hisashi Kashima*

Main category: cs.LG

TL;DR: SAM在过参数化模型中提升泛化能力，但其在张量化和尺度不变模型中的行为研究不足。本文通过尺度不变性分析SAM的范数动态，提出Norm Deviation概念，并基于此提出DAS方法，实验证明其性能优于SAM且计算开销更低。


<details>
  <summary>Details</summary>
Motivation: 探索SAM在更广泛的张量化和尺度不变模型中的行为，弥补现有研究的不足。

Method: 利用尺度不变性分析SAM的范数动态，引入Norm Deviation概念，提出DAS方法，通过数据自适应缩放核心范数。

Result: DAS在张量补全、噪声训练、模型压缩和参数高效微调中表现优于SAM，且计算开销更低。

Conclusion: DAS通过显式模仿SAM的隐式正则化行为，提供了一种高效且性能优越的优化方法。

Abstract: Sharpness-Aware Minimization (SAM) has been proven to be an effective
optimization technique for improving generalization in overparameterized
models. While prior works have explored the implicit regularization of SAM in
simple two-core scale-invariant settings, its behavior in more general
tensorized or scale-invariant models remains underexplored. In this work, we
leverage scale-invariance to analyze the norm dynamics of SAM in general
tensorized models. We introduce the notion of \emph{Norm Deviation} as a global
measure of core norm imbalance, and derive its evolution under SAM using
gradient flow analysis. We show that SAM's implicit control of Norm Deviation
is governed by the covariance between core norms and their gradient magnitudes.
Motivated by these findings, we propose a simple yet effective method,
\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this
regularization behavior by scaling core norms in a data-adaptive manner. Our
experiments across tensor completion, noisy training, model compression, and
parameter-efficient fine-tuning confirm that DAS achieves competitive or
improved performance over SAM, while offering reduced computational overhead.

</details>


### [45] [RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations](https://arxiv.org/abs/2508.10455)
*Asiful Arefeen,Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: RealAC框架生成真实且可操作的对抗解释，自动保持特征间依赖关系，并支持用户冻结特定属性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工约束或领域知识，难以捕捉复杂非线性关系，且缺乏用户定制化。

Method: RealAC通过对齐特征对的联合分布保持依赖关系，并允许用户冻结属性。

Result: 在合成和真实数据集上，RealAC在因果性、依赖保持和真实性指标上优于基线方法。

Conclusion: RealAC提供了一种因果感知且用户中心的对抗解释生成方案。

Abstract: Counterfactual explanations provide human-understandable reasoning for
AI-made decisions by describing minimal changes to input features that would
alter a model's prediction. To be truly useful in practice, such explanations
must be realistic and feasible -- they should respect both the underlying data
distribution and user-defined feasibility constraints. Existing approaches
often enforce inter-feature dependencies through rigid, hand-crafted
constraints or domain-specific knowledge, which limits their generalizability
and ability to capture complex, nonlinear relations inherent in data. Moreover,
they rarely accommodate user-specified preferences and suggest explanations
that are causally implausible or infeasible to act upon. We introduce RealAC, a
domain-agnostic framework for generating realistic and actionable
counterfactuals. RealAC automatically preserves complex inter-feature
dependencies without relying on explicit domain knowledge -- by aligning the
joint distributions of feature pairs between factual and counterfactual
instances. The framework also allows end-users to ``freeze'' attributes they
cannot or do not wish to change by suppressing change in frozen features during
optimization. Evaluations on three synthetic and two real datasets demonstrate
that RealAC balances realism with actionability. Our method outperforms
state-of-the-art baselines and Large Language Model-based counterfactual
generation techniques in causal edge score, dependency preservation score, and
IM1 realism metric and offers a solution for causality-aware and user-centric
counterfactual generation.

</details>


### [46] [X-Node: Self-Explanation is All We Need](https://arxiv.org/abs/2508.10461)
*Prajit Sengupta,Islem Rekik*

Main category: cs.LG

TL;DR: X-Node是一个自解释的图神经网络框架，通过生成每个节点的解释向量，结合轻量级推理模块和预训练语言模型，提供局部且可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: GNN在计算机视觉和医学图像分类中表现出色，但其决策过程不透明，限制了在高风险临床应用中的可信度。现有解释方法多为全局性，缺乏对单个节点决策的深入理解。

Method: X-Node为每个节点构建结构化上下文向量，包含可解释的局部拓扑特征，并通过轻量级推理模块生成解释向量，用于重建嵌入、生成自然语言解释，并反馈到消息传递中。

Result: 在MedMNIST和MorphoMNIST数据集上，X-Node在保持分类准确性的同时，生成了忠实且针对每个节点的解释。

Conclusion: X-Node通过自解释机制提升了GNN的可解释性，适用于需要高透明度的应用场景。

Abstract: Graph neural networks (GNNs) have achieved state-of-the-art results in
computer vision and medical image classification tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where interpretability is essential. Existing explainability
techniques for GNNs are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce X-Node, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as degree, centrality,
clustering, feature saliency, and label agreement within its local topology. A
lightweight Reasoner module maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a decoder to enforce faithfulness, (2) generating a natural
language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate X-Node on two graph
datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,
and GIN backbones. Our results show that X-Node maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/X-Node.

</details>


### [47] [GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation](https://arxiv.org/abs/2508.10471)
*Xinrui Li,Qilin Fan,Tianfu Wang,Kaiwen Wei,Ke Yu,Xu Zhang*

Main category: cs.LG

TL;DR: GraphFedMIG是一种联邦图学习框架，通过生成对抗网络解决数据异构和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习中数据异构和类别不平衡会损害模型性能，尤其是少数类节点难以学习有效嵌入。

Method: 采用分层生成对抗网络，客户端训练本地生成器，通过互信息引导机制优化生成器参数。

Result: 在四个真实数据集上实验表明，GraphFedMIG优于其他基线方法。

Conclusion: GraphFedMIG有效解决了联邦图学习中的类别不平衡问题，提升了模型性能。

Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively
train powerful graph neural networks without sharing their private,
decentralized graph data. Inherited from generic federated learning, FGL is
critically challenged by statistical heterogeneity, where non-IID data
distributions across clients can severely impair model performance. A
particularly destructive form of this is class imbalance, which causes the
global model to become biased towards majority classes and fail at identifying
rare but critical events. This issue is exacerbated in FGL, as nodes from a
minority class are often surrounded by biased neighborhood information,
hindering the learning of expressive embeddings. To grapple with this
challenge, we propose GraphFedMIG, a novel FGL framework that reframes the
problem as a federated generative data augmentation task. GraphFedMIG employs a
hierarchical generative adversarial network where each client trains a local
generator to synthesize high-fidelity feature representations. To provide
tailored supervision, clients are grouped into clusters, each sharing a
dedicated discriminator. Crucially, the framework designs a mutual
information-guided mechanism to steer the evolution of these client generators.
By calculating each client's unique informational value, this mechanism
corrects the local generator parameters, ensuring that subsequent rounds of
mutual information-guided generation are focused on producing high-value,
minority-class features. We conduct extensive experiments on four real-world
datasets, and the results demonstrate the superiority of the proposed
GraphFedMIG compared with other baselines.

</details>


### [48] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: EDAPT框架通过持续模型适应消除脑机接口（BCI）的校准需求，结合群体预训练和在线微调，显著提升解码精度。


<details>
  <summary>Details</summary>
Motivation: 脑机接口因神经信号漂移和用户间差异导致精度下降，频繁校准限制了实际应用。

Method: EDAPT先基于多用户数据训练基线解码器，再通过监督微调持续个性化模型。

Result: 在三个BCI任务的九组数据中，EDAPT比静态方法精度更高，且运行高效。

Conclusion: EDAPT为无校准BCI提供了实用路径，降低了部署障碍。

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


### [49] [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://arxiv.org/abs/2508.10480)
*Panagiotis D. Grontas,Antonio Terpin,Efe C. Balta,Raffaello D'Andrea,John Lygeros*

Main category: cs.LG

TL;DR: 论文提出了一种确保神经网络输出满足凸约束的输出层方法$\Pi$net，结合算子分裂和隐函数定理，实现了快速可靠的投影和反向传播。


<details>
  <summary>Details</summary>
Motivation: 解决传统优化求解器在参数化约束优化问题中效率低下的问题，提供一种高效且可靠的解决方案。

Method: 利用算子分裂实现快速投影（前向传播），结合隐函数定理进行反向传播，设计了一个可行且高效的优化代理模型。

Result: $\Pi$net在单问题和批量问题求解中均显著快于传统求解器，训练时间、解质量和超参数鲁棒性优于现有学习方法。

Conclusion: $\Pi$net在多车辆运动规划等非凸偏好问题中表现优异，并提供了GPU友好的JAX实现和调优启发式方法。

Abstract: We introduce an output layer for neural networks that ensures satisfaction of
convex constraints. Our approach, $\Pi$net, leverages operator splitting for
rapid and reliable projections in the forward pass, and the implicit function
theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design
optimization proxy for parametric constrained optimization problems and obtain
modest-accuracy solutions faster than traditional solvers when solving a single
problem, and significantly faster for a batch of problems. We surpass
state-of-the-art learning approaches in terms of training time, solution
quality, and robustness to hyperparameter tuning, while maintaining similar
inference times. Finally, we tackle multi-vehicle motion planning with
non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package
implemented in JAX with effective tuning heuristics.

</details>


### [50] [Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures](https://arxiv.org/abs/2508.10489)
*Jonas Ulmen,Ganesh Sundaram,Daniel Görges*

Main category: cs.LG

TL;DR: 论文提出了一种基于连续时间动态系统和神经ODE的新方法，用于从任意观测数据构建世界模型，并通过简单摆系统的图像数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着JEPA等联合嵌入预测架构的出现，其表现优于基于重构的方法，因此需要开发更高效的世界建模技术。

Method: 结合序列嵌入与神经ODE，利用损失函数强制嵌入收缩性和Lipschitz常数，构建有序的潜在状态空间。

Result: 方法成功生成了简单摆系统的结构化潜在状态空间模型，仅使用图像数据。

Conclusion: 该技术为开发更通用的控制算法和估计方法提供了新途径，尤其在机器人领域有广泛应用。

Abstract: With the advent of Joint Embedding Predictive Architectures (JEPAs), which
appear to be more capable than reconstruction-based methods, this paper
introduces a novel technique for creating world models using continuous-time
dynamic systems from arbitrary observation data. The proposed method integrates
sequence embeddings with neural ordinary differential equations (neural ODEs).
It employs loss functions that enforce contractive embeddings and Lipschitz
constants in state transitions to construct a well-organized latent state
space. The approach's effectiveness is demonstrated through the generation of
structured latent state-space models for a simple pendulum system using only
image data. This opens up a new technique for developing more general control
algorithms and estimation techniques with broad applications in robotics.

</details>


### [51] [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](https://arxiv.org/abs/2508.10490)
*Amir Mehrpanah,Matteo Gamba,Kevin Smith,Hossein Azizpour*

Main category: cs.LG

TL;DR: 论文提出了一种统一的光谱框架，用于分析和量化解释的平滑性、忠实性及其权衡，并验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: ReLU网络在视觉数据中广泛使用，但其梯度解释存在噪声和难以解释的问题，现有方法（如GradCAM）以牺牲忠实性为代价平滑解释。

Method: 引入光谱框架，系统分析平滑性和忠实性，量化并正则化ReLU网络对高频信息的贡献。

Result: 分析表明基于代理的平滑会扭曲解释，导致“解释差距”，并通过实验验证了理论发现。

Conclusion: 提供了一种原则性方法来识别平滑性与忠实性的权衡，为解释方法的设计提供了理论支持。

Abstract: ReLU networks, while prevalent for visual data, have sharp transitions,
sometimes relying on individual pixels for predictions, making vanilla
gradient-based explanations noisy and difficult to interpret. Existing methods,
such as GradCAM, smooth these explanations by producing surrogate models at the
cost of faithfulness. We introduce a unifying spectral framework to
systematically analyze and quantify smoothness, faithfulness, and their
trade-off in explanations. Using this framework, we quantify and regularize the
contribution of ReLU networks to high-frequency information, providing a
principled approach to identifying this trade-off. Our analysis characterizes
how surrogate-based smoothing distorts explanations, leading to an
``explanation gap'' that we formally define and measure for different post-hoc
methods. Finally, we validate our theoretical findings across different design
choices, datasets, and ablations.

</details>


### [52] [A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation](https://arxiv.org/abs/2508.10494)
*Jiulin Li,Ping Huang,Yexin Li,Shuo Chen,Juewen Hu,Ye Tian*

Main category: cs.LG

TL;DR: MAGUS是一个模块化框架，通过解耦的认知和决策阶段统一多模态理解与生成，支持灵活扩展和高性能。


<details>
  <summary>Details</summary>
Motivation: 现实多模态应用需要任意模态间的理解和生成能力，但现有方法缺乏灵活性和扩展性。

Method: MAGUS采用多代理协作和增长感知搜索机制，结合LLM推理和扩散模型生成。

Result: 在多个基准测试中表现优异，超越GPT-4o等强基线。

Conclusion: MAGUS为多模态任务提供了高效、灵活的解决方案。

Abstract: Real-world multimodal applications often require any-to-any capabilities,
enabling both understanding and generation across modalities including text,
image, audio, and video. However, integrating the strengths of autoregressive
language models (LLMs) for reasoning and diffusion models for high-fidelity
generation remains challenging. Existing approaches rely on rigid pipelines or
tightly coupled architectures, limiting flexibility and scalability. We propose
MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that
unifies multimodal understanding and generation via two decoupled phases:
Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration
within a shared textual workspace. In the Cognition phase, three
role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -
engage in collaborative dialogue to perform structured understanding and
planning. The Deliberation phase incorporates a Growth-Aware Search mechanism
that orchestrates LLM-based reasoning and diffusion-based generation in a
mutually reinforcing manner. MAGUS supports plug-and-play extensibility,
scalable any-to-any modality conversion, and semantic alignment - all without
the need for joint training. Experiments across multiple benchmarks, including
image, video, and audio generation, as well as cross-modal instruction
following, demonstrate that MAGUS outperforms strong baselines and
state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the
powerful closed-source model GPT-4o.

</details>


### [53] [Nonlocal Monte Carlo via Reinforcement Learning](https://arxiv.org/abs/2508.10520)
*Dmitrii Dobrynin,Masoud Mohseni,John Paul Strachan*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度强化学习的非平衡非局部蒙特卡洛算法（NMC），用于优化复杂的组合优化问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC算法在解决硬基准问题时效率低下，尤其是在计算相变附近时，难以摆脱局部最优解。

Method: 利用深度强化学习训练NMC的非局部转移策略，通过观察能量变化和局部最小能量景观作为奖励和状态。

Result: 在硬随机4-SAT基准测试中，该方法在剩余能量、求解时间和解多样性方面优于传统MCMC和非局部模拟退火。

Conclusion: 深度强化学习可有效优化NMC算法，为组合优化问题提供更高效的解决方案。

Abstract: Optimizing or sampling complex cost functions of combinatorial optimization
problems is a longstanding challenge across disciplines and applications. When
employing family of conventional algorithms based on Markov Chain Monte Carlo
(MCMC) such as simulated annealing or parallel tempering, one assumes
homogeneous (equilibrium) temperature profiles across input. This instance
independent approach was shown to be ineffective for the hardest benchmarks
near a computational phase transition when the so-called overlap-gap-property
holds. In these regimes conventional MCMC struggles to unfreeze rigid
variables, escape suboptimal basins of attraction, and sample high-quality and
diverse solutions. In order to mitigate these challenges, Nonequilibrium
Nonlocal Monte Carlo (NMC) algorithms were proposed that leverage inhomogeneous
temperature profiles thereby accelerating exploration of the configuration
space without compromising its exploitation. Here, we employ deep reinforcement
learning (RL) to train the nonlocal transition policies of NMC which were
previously designed phenomenologically. We demonstrate that the resulting
solver can be trained solely by observing energy changes of the configuration
space exploration as RL rewards and the local minimum energy landscape geometry
as RL states. We further show that the trained policies improve upon the
standard MCMC-based and nonlocal simulated annealing on hard uniform random and
scale-free random 4-SAT benchmarks in terms of residual energy,
time-to-solution, and diversity of solutions metrics.

</details>


### [54] [Projected Coupled Diffusion for Test-Time Constrained Joint Generation](https://arxiv.org/abs/2508.10531)
*Hao Luan,Yi Xian Goh,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 提出了一种名为PCD的新方法，用于在无需重新训练的情况下，通过联合多个预训练扩散模型生成相关样本并满足任务特定约束。


<details>
  <summary>Details</summary>
Motivation: 解决在无需重新训练的情况下，如何联合多个预训练扩散模型生成相关样本并满足约束的挑战。

Method: PCD引入耦合引导项和投影步骤，在生成过程中协调扩散模型并强制满足硬约束。

Result: 在图像对生成、物体操作和多机器人运动规划等应用中，PCD表现出更好的耦合效果和约束满足能力，且计算成本较低。

Conclusion: PCD是一种高效且有效的测试时框架，适用于联合生成任务。

Abstract: Modifications to test-time sampling have emerged as an important extension to
diffusion algorithms, with the goal of biasing the generative process to
achieve a given objective without having to retrain the entire diffusion model.
However, generating jointly correlated samples from multiple pre-trained
diffusion models while simultaneously enforcing task-specific constraints
without costly retraining has remained challenging. To this end, we propose
Projected Coupled Diffusion (PCD), a novel test-time framework for constrained
joint generation. PCD introduces a coupled guidance term into the generative
dynamics to encourage coordination between diffusion models and incorporates a
projection step at each diffusion step to enforce hard constraints.
Empirically, we demonstrate the effectiveness of PCD in application scenarios
of image-pair generation, object manipulation, and multi-robot motion planning.
Our results show improved coupling effects and guaranteed constraint
satisfaction without incurring excessive computational costs.

</details>


### [55] [Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation](https://arxiv.org/abs/2508.10541)
*Brian Shing-Hei Wong,Joshua Mincheol Kim,Sin-Hang Fung,Qing Xiong,Kelvin Fu-Kiu Ao,Junkang Wei,Ran Wang,Dan Michelle Wang,Jingying Zhou,Bo Feng,Alfred Sze-Lok Cheng,Kevin Y. Yip,Stephen Kwok-Wing Tsui,Qin Cao*

Main category: cs.LG

TL;DR: Applm利用xTrimoPGLM蛋白质语言模型，显著优于现有方法，能识别新过敏原、区分同源蛋白中的过敏原，并评估突变影响。


<details>
  <summary>Details</summary>
Motivation: 过敏原蛋白对公共健康构成挑战，需更准确识别方法。

Method: 基于1000亿参数的xTrimoPGLM模型开发Applm框架。

Result: Applm在多种任务中优于7种现有方法，尤其在识别新过敏原和区分同源蛋白方面。

Conclusion: xTrimoPGLM对Applm性能至关重要，开源框架和数据集将推动未来研究。

Abstract: Allergens, typically proteins capable of triggering adverse immune responses,
represent a significant public health challenge. To accurately identify
allergen proteins, we introduce Applm (Allergen Prediction with Protein
Language Models), a computational framework that leverages the 100-billion
parameter xTrimoPGLM protein language model. We show that Applm consistently
outperforms seven state-of-the-art methods in a diverse set of tasks that
closely resemble difficult real-world scenarios. These include identifying
novel allergens that lack similar examples in the training set, differentiating
between allergens and non-allergens among homologs with high sequence
similarity, and assessing functional consequences of mutations that create few
changes to the protein sequences. Our analysis confirms that xTrimoPGLM,
originally trained on one trillion tokens to capture general protein sequence
characteristics, is crucial for Applm's performance by detecting important
differences among protein sequences. In addition to providing Applm as
open-source software, we also provide our carefully curated benchmark datasets
to facilitate future research.

</details>


### [56] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 论文提出了一种针对软件工程任务的强化学习框架（SWE-oriented RL Framework），并引入Gated Reward Accumulation（G-RA）方法，通过累积即时奖励来优化长期目标，显著提高了任务完成率和修改率。


<details>
  <summary>Details</summary>
Motivation: 解决长时程强化学习中奖励稀疏性问题，避免现有方法因奖励偏差或任务分解导致的优化不稳定。

Method: 提出SWE-oriented RL Framework，支持多轮交互和Docker执行，并设计G-RA方法，仅在长期奖励达标时累积即时奖励。

Result: 实验显示G-RA显著提升任务完成率（47.6%→93.8%和22.0%→86.0%）和修改率（19.6%→23.8%和12.0%→42.0%），避免奖励偏差导致的策略退化。

Conclusion: G-RA为长时程强化学习提供了平衡奖励累积的实用解决方案，验证了其在软件工程任务中的有效性。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [57] [Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot](https://arxiv.org/abs/2508.10581)
*Jeroen Berrevoets,Julianna Piskorz,Robert Davis,Harry Amad,Jim Weatherall,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: CATE-B是一个基于大型语言模型的辅助系统，旨在简化观测数据中的治疗效果估计过程。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的治疗效果估计复杂且需要专业知识，限制了先进技术的应用。

Method: CATE-B结合因果发现、最小不确定性调整集和新颖的回归方法选择，通过LLM提供交互式指导。

Result: 系统降低了因果分析的难度，并提供了多领域基准任务以支持评估。

Conclusion: CATE-B为自动化治疗效果估计奠定了基础，推动了因果分析的普及。

Abstract: Estimating treatment effects (TE) from observational data is a critical yet
complex task in many fields, from healthcare and economics to public policy.
While recent advances in machine learning and causal inference have produced
powerful estimation techniques, their adoption remains limited due to the need
for deep expertise in causal assumptions, adjustment strategies, and model
selection. In this paper, we introduce CATE-B, an open-source co-pilot system
that uses large language models (LLMs) within an agentic framework to guide
users through the end-to-end process of treatment effect estimation. CATE-B
assists in (i) constructing a structural causal model via causal discovery and
LLM-based edge orientation, (ii) identifying robust adjustment sets through a
novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting
appropriate regression methods tailored to the causal structure and dataset
characteristics. To encourage reproducibility and evaluation, we release a
suite of benchmark tasks spanning diverse domains and causal complexities. By
combining causal inference with intelligent, interactive assistance, CATE-B
lowers the barrier to rigorous causal analysis and lays the foundation for a
new class of benchmarks in automated treatment effect estimation.

</details>


### [58] [GNN-based Unified Deep Learning](https://arxiv.org/abs/2508.10583)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: 提出了一种统一学习方法，通过将不同模型编码为图表示并在共享图学习空间中优化，提升医学影像模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中因分布偏移导致的模型泛化问题，支持不同架构模型的统一训练。

Method: 将各模型编码为图表示，通过统一图神经网络（uGNN）优化，实现参数共享和知识迁移。

Result: 在MorphoMNIST和MedMNIST基准测试中表现优异，对分布偏移数据具有强鲁棒性。

Conclusion: 统一学习方法有效提升了异构模型在医学影像中的泛化性能。

Abstract: Deep learning models often struggle to maintain generalizability in medical
imaging, particularly under domain-fracture scenarios where distribution shifts
arise from varying imaging techniques, acquisition protocols, patient
populations, demographics, and equipment. In practice, each hospital may need
to train distinct models - differing in learning task, width, and depth - to
match local data. For example, one hospital may use Euclidean architectures
such as MLPs and CNNs for tabular or grid-like image data, while another may
require non-Euclidean architectures such as graph neural networks (GNNs) for
irregular data like brain connectomes. How to train such heterogeneous models
coherently across datasets, while enhancing each model's generalizability,
remains an open problem. We propose unified learning, a new paradigm that
encodes each model into a graph representation, enabling unification in a
shared graph learning space. A GNN then guides optimization of these unified
models. By decoupling parameters of individual models and controlling them
through a unified GNN (uGNN), our method supports parameter sharing and
knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and
distributions, improving generalizability. Evaluations on MorphoMNIST and two
MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified
learning boosts performance when models are trained on unique distributions and
tested on mixed ones, demonstrating strong robustness to unseen data with large
distribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN

</details>


### [59] [FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection](https://arxiv.org/abs/2508.10594)
*Yunfeng Zhao,Yixin Liu,Shiyuan Li,Qingfeng Chen,Yu Zheng,Shirui Pan*

Main category: cs.LG

TL;DR: FreeGAD是一种无需训练的图异常检测方法，通过亲和门控残差编码器和锚节点统计偏差实现高效、可扩展的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图异常检测方法部署成本高且可扩展性差，研究发现训练阶段对性能影响有限。

Method: 使用亲和门控残差编码器生成异常感知表示，并利用锚节点作为伪正常和异常引导，通过统计偏差计算异常分数。

Result: 在多个基准数据集上，FreeGAD表现出优异的异常检测性能、效率和可扩展性。

Conclusion: FreeGAD无需训练即可实现高效、可扩展的异常检测，为实际应用提供了新思路。

Abstract: Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the
majority within a graph, playing a crucial role in applications such as social
networks and e-commerce. Despite the current advancements in deep
learning-based GAD, existing approaches often suffer from high deployment costs
and poor scalability due to their complex and resource-intensive training
processes. Surprisingly, our empirical findings suggest that the training phase
of deep GAD methods, commonly perceived as crucial, may actually contribute
less to anomaly detection performance than expected. Inspired by this, we
propose FreeGAD, a novel training-free yet effective GAD method. Specifically,
it leverages an affinity-gated residual encoder to generate anomaly-aware
representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal
and anomalous guides, followed by calculating anomaly scores through
anchor-guided statistical deviations. Extensive experiments demonstrate that
FreeGAD achieves superior anomaly detection performance, efficiency, and
scalability on multiple benchmark datasets from diverse domains, without any
training or iterative optimization.

</details>


### [60] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,Gökhan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 本文提出了一种基于生成对抗变换器（GATs）的新方法，用于解决能源系统模型中时间序列上采样的信息丢失和噪声问题，无需真实高分辨率数据即可训练。


<details>
  <summary>Details</summary>
Motivation: 传统上采样方法存在信息丢失或噪声增加的问题，而现有高级模型（如时间序列生成模型）因依赖高分辨率数据训练而面临应用悖论。

Method: 采用生成对抗变换器（GATs），无需真实高分辨率数据即可训练，生成统计特性相似的高分辨率时间序列。

Result: 与常规插值方法相比，RMSE降低9%，模型预测控制（MPC）场景的准确性提高13%。

Conclusion: GATs方法有效解决了上采样问题，显著提升了性能，适用于能源系统模型。

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>


### [61] [On Spectral Properties of Gradient-based Explanation Methods](https://arxiv.org/abs/2508.10595)
*Amir Mehrpanah,Erik Englesson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 论文通过概率和频谱视角分析深度网络解释方法，揭示梯度导致的频谱偏差，并提出标准化扰动尺度和SpectralLens聚合方法以解决不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 提高对深度网络预测结果的信任度，解决现有解释方法因形式化不足导致的可靠性问题。

Method: 采用概率和频谱视角分析解释方法，提出标准化扰动尺度和SpectralLens聚合方法。

Result: 揭示了梯度导致的频谱偏差，验证了标准化扰动尺度和SpectralLens的有效性。

Conclusion: 通过形式化分析改进了解释方法的可靠性，为设计更稳定的解释方法提供了理论支持。

Abstract: Understanding the behavior of deep networks is crucial to increase our
confidence in their results. Despite an extensive body of work for explaining
their predictions, researchers have faced reliability issues, which can be
attributed to insufficient formalism. In our research, we adopt novel
probabilistic and spectral perspectives to formally analyze explanation
methods. Our study reveals a pervasive spectral bias stemming from the use of
gradient, and sheds light on some common design choices that have been
discovered experimentally, in particular, the use of squared gradient and input
perturbation. We further characterize how the choice of perturbation
hyperparameters in explanation methods, such as SmoothGrad, can lead to
inconsistent explanations and introduce two remedies based on our proposed
formalism: (i) a mechanism to determine a standard perturbation scale, and (ii)
an aggregation method which we call SpectralLens. Finally, we substantiate our
theoretical results through quantitative evaluations.

</details>


### [62] [SPHENIC: Topology-Informed Multi-View Clustering for Spatial Transcriptomics](https://arxiv.org/abs/2508.10646)
*Chenkai Guo,Yikai Zhu,Jing Yangum,Renxiang Guan,Por Lip Yee,Guangdun Peng,Dayu Hu*

Main category: cs.LG

TL;DR: SPHENIC是一种新的空间转录组聚类方法，通过结合空间拓扑特征和优化空间邻域信息，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间转录组聚类中存在拓扑学习不足和空间邻域信息建模不充分的问题，导致聚类效果不佳。

Method: SPHENIC结合了不变拓扑特征和空间约束与分布优化模块（SCDOM），以稳定表示学习并优化空间嵌入。

Result: 在14个基准数据集上，SPHENIC比现有最佳方法性能提升3.31%-6.54%。

Conclusion: SPHENIC通过改进拓扑学习和空间嵌入优化，显著提升了空间转录组聚类的效果。

Abstract: By incorporating spatial location information, spatial-transcriptomics
clustering yields more comprehensive insights into cell subpopulation
identification. Despite recent progress, existing methods have at least two
limitations: (i) topological learning typically considers only representations
of individual cells or their interaction graphs; however, spatial
transcriptomic profiles are often noisy, making these approaches vulnerable to
low-quality topological signals, and (ii) insufficient modeling of spatial
neighborhood information leads to low-quality spatial embeddings. To address
these limitations, we propose SPHENIC, a novel Spatial Persistent Homology
Enhanced Neighborhood Integrative Clustering method. Specifically, SPHENIC
incorporates invariant topological features into the clustering network to
achieve stable representation learning. Additionally, to construct high-quality
spatial embeddings that reflect the true cellular distribution, we design the
Spatial Constraint and Distribution Optimization Module (SCDOM). This module
increases the similarity between a cell's embedding and those of its spatial
neighbors, decreases similarity with non-neighboring cells, and thereby
produces clustering-friendly spatial embeddings. Extensive experiments on 14
benchmark spatial transcriptomic slices demonstrate that SPHENIC achieves
superior performance on the spatial clustering task, outperforming existing
state-of-the-art methods by 3.31%-6.54% over the best alternative.

</details>


### [63] [REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations](https://arxiv.org/abs/2508.10701)
*Tianlong Yu,Lihong Liu,Ziyi Zhou,Fudu Xing,Kailong Wang,Yang Yang*

Main category: cs.LG

TL;DR: REFN是一种基于强化学习的框架，利用大型语言模型（LLM）自动生成网络过滤器，以应对1天或N天漏洞威胁，解决了现有防御措施的不足。


<details>
  <summary>Details</summary>
Motivation: 现有防御措施（如主机补丁和网络过滤）在可扩展性、兼容性和部署过程中存在问题，无法有效应对大规模漏洞威胁。

Method: REFN通过强化学习（RL）驱动在线网络奖励训练LLM，结合Agentic RAG知识蒸馏、RL From VNF管道和在线代理验证，解决LLM在漏洞修复中的局限性。

Result: 在22类漏洞测试中，REFN表现出高效（补丁平均时间3.65小时）、高准确率（比替代方案高21.1%）和强扩展性（支持10K设备）。

Conclusion: REFN为利用LLM快速预防大规模漏洞威胁提供了初步解决方案。

Abstract: The exploitation of 1 day or n day vulnerabilities poses severe threats to
networked devices due to massive deployment scales and delayed patching
(average Mean Time To Patch exceeds 60 days). Existing defenses, including host
based patching and network based filtering, are inadequate due to limited
scalability across diverse devices, compatibility issues especially with
embedded or legacy systems, and error prone deployment process (manual patch
validation). To address these issues, we introduce REFN (Reinforcement Learning
From Network), a novel framework that trains Large Language Models (LLMs) to
autonomously generate network filters to prevent 1 day or n day exploitations.
REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven
by online network rewards instead of traditional Human Feedback (RLHF). REFN
guarantees compatibility via unified deployment on edge security gateways
(Amazon Eero). REFN provides robustness via online validation using real
network traffic. Crucially, REFN addresses three core challenges in training
LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability
fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging
current LLMs language to network gaps through an RL From VNF Pipeline that
translates language context (vulnerability description) into network
enforcement, 3) addressing the LLM hallucination and non determinism via the
Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22
families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1
percent higher accuracy than alternatives), efficiency (Mean Time To Patch of
3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an
initial step toward training LLMs to rapidly prevent massive scale 1 day or n
day exploitations.

</details>


### [64] [Oops!... They Stole it Again: Attacks on Split Learning](https://arxiv.org/abs/2508.10598)
*Tanveer Khan,Antonis Michalas*

Main category: cs.LG

TL;DR: 本文系统回顾了分割学习（SL）中的多种攻击方式，并分析了现有防御方法的有效性及局限性，为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 分割学习通过客户端保留数据提升了隐私性，但其分布式特性引入了新的安全挑战，需全面探索潜在攻击。

Method: 分类攻击方式（攻击者角色、隐私风险类型、数据泄露时机、漏洞位置），并分析现有防御方法（密码学、数据修改、分布式技术、混合方案）。

Result: 揭示了安全漏洞，指出了现有防御方法的有效性和局限性。

Conclusion: 通过识别开放挑战和未来方向，为改进分割学习的隐私问题及进一步研究提供了指导。

Abstract: Split Learning (SL) is a collaborative learning approach that improves
privacy by keeping data on the client-side while sharing only the intermediate
output with a server. However, the distributed nature of SL introduces new
security challenges, necessitating a comprehensive exploration of potential
attacks. This paper systematically reviews various attacks on SL, classifying
them based on factors such as the attacker's role, the type of privacy risks,
when data leaks occur, and where vulnerabilities exist. We also analyze
existing defense methods, including cryptographic methods, data modification
approaches, distributed techniques, and hybrid solutions. Our findings reveal
security gaps, highlighting the effectiveness and limitations of existing
defenses. By identifying open challenges and future directions, this work
provides valuable information to improve SL privacy issues and guide further
research.

</details>


### [65] [Electromagnetic Simulations of Antennas on GPUs for Machine Learning Applications](https://arxiv.org/abs/2508.10713)
*Murat Temiz,Vemund Bakken*

Main category: cs.LG

TL;DR: 该研究提出了一种基于GPU的天线仿真框架，用于机器学习的应用，并与商业软件结果进行了比较。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法需要大量数据训练，但电磁仿真计算复杂且耗时，因此利用GPU加速生成数据集。

Method: 使用开源电磁仿真软件gprMax结合GPU，生成大量天线仿真数据，并比较不同机器学习模型的性能。

Result: 入门级GPU性能优于高端CPU，高端游戏GPU性能是高端CPU的18倍；开源软件在足够精细的分辨率下与商业软件结果相似。

Conclusion: GPU显著提升了电磁仿真的效率，开源软件在特定条件下可替代商业软件。

Abstract: This study proposes an antenna simulation framework powered by graphics
processing units (GPUs) based on an open-source electromagnetic (EM) simulation
software (gprMax) for machine learning applications of antenna design and
optimization. Furthermore, it compares the simulation results with those
obtained through commercial EM software. The proposed software framework for
machine learning and surrogate model applications will produce antenna data
sets consisting of a large number of antenna simulation results using GPUs.
Although machine learning methods can attain the optimum solutions for many
problems, they are known to be data-hungry and require a great deal of samples
for the training stage of the algorithms. However, producing a sufficient
number of training samples in EM applications within a limited time is
challenging due to the high computational complexity of EM simulations.
Therefore, GPUs are utilized in this study to simulate a large number of
antennas with predefined or random antenna shape parameters to produce data
sets. Moreover, this study also compares various machine learning and deep
learning models in terms of antenna parameter estimation performance. This
study demonstrates that an entry-level GPU substantially outperforms a high-end
CPU in terms of computational performance, while a high-end gaming GPU can
achieve around 18 times more computational performance compared to a high-end
CPU. Moreover, it is shown that the open-source EM simulation software can
deliver similar results to those obtained via commercial software in the
simulation of microstrip antennas when the spatial resolution of the
simulations is sufficiently fine.

</details>


### [66] [Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.10608)
*Davide Guidobene,Lorenzo Benedetti,Diego Arapovic*

Main category: cs.LG

TL;DR: 本文提出了一种通过方差缩减技术提高多目标强化学习（MORL）样本效率的方法，解决了现有策略梯度方法（PGMs）样本效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）需要同时优化多个可能冲突的目标，但现有策略梯度方法（PGMs）样本效率低，限制了其在大规模状态-动作空间中的应用。

Method: 采用方差缩减技术，减少策略梯度的样本复杂度，同时保持通用假设。

Result: 提出的方法显著提高了样本效率，同时保留了策略梯度方法的可扩展性优势。

Conclusion: 方差缩减技术是提高MORL样本效率的有效方法，适用于复杂决策场景。

Abstract: Multi-Objective Reinforcement Learning (MORL) is a generalization of
traditional Reinforcement Learning (RL) that aims to optimize multiple, often
conflicting objectives simultaneously rather than focusing on a single reward.
This approach is crucial in complex decision-making scenarios where agents must
balance trade-offs between various goals, such as maximizing performance while
minimizing costs. We consider the problem of MORL where the objectives are
combined using a non-linear scalarization function. Just like in standard RL,
policy gradient methods (PGMs) are amongst the most effective for handling
large and continuous state-action spaces in MORL. However, existing PGMs for
MORL suffer from high sample inefficiency, requiring large amounts of data to
be effective. Previous attempts to solve this problem rely on overly strict
assumptions, losing PGMs' benefits in scalability to large state-action spaces.
In this work, we address the issue of sample efficiency by implementing
variance-reduction techniques to reduce the sample complexity of policy
gradients while maintaining general assumptions.

</details>


### [67] [APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares](https://arxiv.org/abs/2508.10732)
*Kejia Fan,Jianheng Tang,Zhirui Yang,Feijiang Han,Jiaxu Li,Run He,Yajiang Huang,Anfeng Liu,Houbing Herbert Song,Yunhuai Liu,Huiping Zhuang*

Main category: cs.LG

TL;DR: 论文提出了一种基于双流最小二乘的解析个性化联邦学习（APFL）方法，解决了非独立同分布数据对个性化联邦学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法对非IID数据敏感，影响集体泛化和个性化效果。

Method: 使用基础模型作为冻结主干提取特征，开发双流解析模型实现全局泛化和局部个性化。

Result: APFL在多个数据集上表现优于现有方法，准确率提升1.10%-15.45%。

Conclusion: APFL通过双流设计实现异质性不变性，显著提升个性化联邦学习效果。

Abstract: Personalized Federated Learning (PFL) has presented a significant challenge
to deliver personalized models to individual clients through collaborative
training. Existing PFL methods are often vulnerable to non-IID data, which
severely hinders collective generalization and then compromises the subsequent
personalization efforts. In this paper, to address this non-IID issue in PFL,
we propose an Analytic Personalized Federated Learning (APFL) approach via
dual-stream least squares. In our APFL, we use a foundation model as a frozen
backbone for feature extraction. Subsequent to the feature extractor, we
develop dual-stream analytic models to achieve both collective generalization
and individual personalization. Specifically, our APFL incorporates a shared
primary stream for global generalization across all clients, and a dedicated
refinement stream for local personalization of each individual client. The
analytical solutions of our APFL enable its ideal property of heterogeneity
invariance, theoretically meaning that each personalized model remains
identical regardless of how heterogeneous the data are distributed across all
other clients. Empirical results across various datasets also validate the
superiority of our APFL over state-of-the-art baselines, with advantages of at
least 1.10%-15.45% in accuracy.

</details>


### [68] [Beyond Random Sampling: Instance Quality-Based Data Partitioning via Item Response Theory](https://arxiv.org/abs/2508.10628)
*Lucas Cardoso,Vitor Santos,José Ribeiro Filho,Ricardo Prudêncio,Regiane Kawasaki,Ronnie Alves*

Main category: cs.LG

TL;DR: 该研究提出使用项目反应理论（IRT）参数来指导机器学习模型验证阶段的数据集划分，结果显示IRT能揭示实例的异质性并优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的数据划分方法忽略了每个实例的内在质量，因此需要更稳健的验证方法。

Method: 利用IRT参数对数据集进行划分，并评估其对多个机器学习模型性能的影响。

Result: IRT揭示了数据集的异质性，并创建了平衡的分区，显著改善了模型性能。猜测参数是关键因素，高猜测实例会显著降低模型性能。

Conclusion: IRT为机器学习模型验证提供了更有效的分区策略，有助于理解模型的偏差-方差权衡。

Abstract: Robust validation of Machine Learning (ML) models is essential, but
traditional data partitioning approaches often ignore the intrinsic quality of
each instance. This study proposes the use of Item Response Theory (IRT)
parameters to characterize and guide the partitioning of datasets in the model
validation stage. The impact of IRT-informed partitioning strategies on the
performance of several ML models in four tabular datasets was evaluated. The
results obtained demonstrate that IRT reveals an inherent heterogeneity of the
instances and highlights the existence of informative subgroups of instances
within the same dataset. Based on IRT, balanced partitions were created that
consistently help to better understand the tradeoff between bias and variance
of the models. In addition, the guessing parameter proved to be a determining
factor: training with high-guessing instances can significantly impair model
performance and resulted in cases with accuracy below 50%, while other
partitions reached more than 70% in the same dataset.

</details>


### [69] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: 论文探讨了在强化学习中使用Pass@k作为奖励以提升探索能力，并提出了Pass@k Training方法及其优势。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR中探索与利用的平衡问题，避免策略陷入局部最优。

Method: 采用Pass@k作为奖励训练策略模型，并推导其优势的解析解。

Result: Pass@k Training提升了探索能力，探索与利用可相互促进。

Conclusion: Pass@k Training为RLVR提供了一种高效方法，优势函数设计是未来研究方向。

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [70] [Energy-Based Models for Predicting Mutational Effects on Proteins](https://arxiv.org/abs/2508.10629)
*Patrick Soga,Zhenyu Lei,Yinhan He,Camille Bilodeau,Jundong Li*

Main category: cs.LG

TL;DR: 提出了一种新的ΔΔG预测方法，通过能量模型和序列模型分解ΔΔG，避免了传统方法中蛋白质构象分布估计的难题。


<details>
  <summary>Details</summary>
Motivation: ΔΔG预测在蛋白质工程和药物发现中至关重要，但传统方法因蛋白质构象分布估计困难而受限。

Method: 将ΔΔG分解为序列模型估计的序列成分和能量模型估计的结构成分，并假设结合态与非结合态平衡以简化计算。

Result: 在ΔΔG预测和SARS-CoV-2抗体优化中优于现有深度学习方法。

Conclusion: 新方法通过物理归纳偏置和统计力学基础，显著提升了ΔΔG预测的准确性。

Abstract: Predicting changes in binding free energy ($\Delta\Delta G$) is a vital task
in protein engineering and protein-protein interaction (PPI) engineering for
drug discovery. Previous works have observed a high correlation between
$\Delta\Delta G$ and entropy, using probabilities of biologically important
objects such as side chain angles and residue identities to estimate
$\Delta\Delta G$. However, estimating the full conformational distribution of a
protein complex is generally considered intractable. In this work, we propose a
new approach to $\Delta\Delta G$ prediction that avoids this issue by instead
leveraging energy-based models for estimating the probability of a complex's
conformation. Specifically, we novelly decompose $\Delta\Delta G$ into a
sequence-based component estimated by an inverse folding model and a
structure-based component estimated by an energy model. This decomposition is
made tractable by assuming equilibrium between the bound and unbound states,
allowing us to simplify the estimation of degeneracies associated with each
state. Unlike previous deep learning-based methods, our method incorporates an
energy-based physical inductive bias by connecting the often-used sequence
log-odds ratio-based approach to $\Delta\Delta G$ prediction with a new
$\Delta\Delta E$ term grounded in statistical mechanics. We demonstrate
superiority over existing state-of-the-art structure and sequence-based deep
learning methods in $\Delta\Delta G$ prediction and antibody optimization
against SARS-CoV-2.

</details>


### [71] [Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets](https://arxiv.org/abs/2508.10758)
*Nicolas Lapautre,Maria Marchenko,Carlos Miguel Patiño,Xin Zhou*

Main category: cs.LG

TL;DR: 该论文通过结合Erwin架构与Native Sparse Attention（NSA）机制，解决了Transformer模型在大规模物理系统数据集上注意力机制二次复杂度的问题，提升了效率和感受野。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在大规模物理系统数据上注意力机制二次复杂度的问题，以提升效率和性能。

Method: 结合Erwin架构与NSA机制，将其适配于非序列数据，并在三个物理科学数据集（宇宙学模拟、分子动力学和气压建模）上评估。

Result: 性能达到或超过原始Erwin模型，并验证了Erwin论文的实验结果。

Conclusion: Erwin NSA模型有效解决了注意力机制的复杂度问题，适用于大规模物理系统。

Abstract: Unlocking the potential of transformers on datasets of large physical systems
depends on overcoming the quadratic scaling of the attention mechanism. This
work explores combining the Erwin architecture with the Native Sparse Attention
(NSA) mechanism to improve the efficiency and receptive field of transformer
models for large-scale physical systems, addressing the challenge of quadratic
attention complexity. We adapt the NSA mechanism for non-sequential data,
implement the Erwin NSA model, and evaluate it on three datasets from the
physical sciences -- cosmology simulations, molecular dynamics, and air
pressure modeling -- achieving performance that matches or exceeds that of the
original Erwin model. Additionally, we reproduce the experimental results from
the Erwin paper to validate their implementation.

</details>


### [72] [Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection](https://arxiv.org/abs/2508.10644)
*Yihua Wang,Qi Jia,Cong Xu,Feiyu Chen,Yuhan Liu,Haotian Zhang,Liang Jin,Lu Liu,Zhichun Wang*

Main category: cs.LG

TL;DR: 论文提出了一种新的多模态讽刺检测方法MCIB，通过消除数据集中的捷径信号并改进模态融合策略，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据集中的捷径信号，导致模型在真实场景中泛化能力不足，且当前模态融合策略在多模态讽刺检测中存在缺陷。

Method: 构建了MUStARD++$^{R}$数据集以移除捷径信号，并提出了多模态条件信息瓶颈（MCIB）模型以实现高效的模态融合。

Result: 实验表明，MCIB在不依赖捷径学习的情况下取得了最佳性能。

Conclusion: MCIB通过改进模态融合和避免捷径学习，为复杂情感识别提供了更有效的解决方案。

Abstract: Multimodal sarcasm detection is a complex task that requires distinguishing
subtle complementary signals across modalities while filtering out irrelevant
information. Many advanced methods rely on learning shortcuts from datasets
rather than extracting intended sarcasm-related features. However, our
experiments show that shortcut learning impairs the model's generalization in
real-world scenarios. Furthermore, we reveal the weaknesses of current modality
fusion strategies for multimodal sarcasm detection through systematic
experiments, highlighting the necessity of focusing on effective modality
fusion for complex emotion recognition. To address these challenges, we
construct MUStARD++$^{R}$ by removing shortcut signals from MUStARD++. Then, a
Multimodal Conditional Information Bottleneck (MCIB) model is introduced to
enable efficient multimodal fusion for sarcasm detection. Experimental results
show that the MCIB achieves the best performance without relying on shortcut
learning.

</details>


### [73] [Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection](https://arxiv.org/abs/2508.10785)
*Shouju Wang,Yuchen Song,Sheng'en Li,Dongmian Zou*

Main category: cs.LG

TL;DR: 论文提出DECAF-GAD框架，解决基于自动编码器的图异常检测中的公平性问题，通过结构因果模型和公平性损失函数实现。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）在异常检测中可能放大数据偏见，导致不公平结果，而现有公平性研究多集中于节点分类任务，自动编码器架构的公平性研究不足。

Method: 提出DECAF-GAD框架，使用结构因果模型（SCM）解耦敏感属性，设计专用自动编码器架构和公平性损失函数。

Result: 实验表明，DECAF-GAD在异常检测性能和公平性指标上均优于基线方法。

Conclusion: DECAF-GAD有效解决了自动编码器架构的公平性问题，同时保持高性能。

Abstract: Graph anomaly detection (GAD) has become an increasingly important task
across various domains. With the rapid development of graph neural networks
(GNNs), GAD methods have achieved significant performance improvements.
However, fairness considerations in GAD remain largely underexplored. Indeed,
GNN-based GAD models can inherit and amplify biases present in training data,
potentially leading to unfair outcomes. While existing efforts have focused on
developing fair GNNs, most approaches target node classification tasks, where
models often rely on simple layer architectures rather than autoencoder-based
structures, which are the most widely used architecturs for anomaly detection.
To address fairness in autoencoder-based GAD models, we propose
\textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial
\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving
GAD performance. Specifically, we introduce a structural causal model (SCM) to
disentangle sensitive attributes from learned representations. Based on this
causal framework, we formulate a specialized autoencoder architecture along
with a fairness-guided loss function. Through extensive experiments on both
synthetic and real-world datasets, we demonstrate that DECAF-GAD not only
achieves competitive anomaly detection performance but also significantly
enhances fairness metrics compared to baseline GAD methods. Our code is
available at https://github.com/Tlhey/decaf_code.

</details>


### [74] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 论文提出了一种利用生成式AI（GenAI）预测土地覆盖变化的新方法，通过将LULC预测视为数据合成问题，并基于历史和辅助数据训练扩散模型，验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖变化对地球系统过程有重要影响，但目前LULC预测能力滞后于其他领域。

Method: 将LULC预测建模为数据合成问题，使用生成式AI（扩散模型）进行训练和预测。

Result: 实验表明，在分辨率≥0.7×0.7km²时，扩散模型的MAE低于无变化基线，验证了其有效性。

Conclusion: 生成式模型能捕捉历史数据中的时空模式，未来研究将进一步整合地球物理属性和驱动变量。

Abstract: Land cover, both present and future, has a significant effect on several
important Earth system processes. For example, impervious surfaces heat up and
speed up surface water runoff and reduce groundwater infiltration, with
concomitant effects on regional hydrology and flood risk. While regional Earth
System models have increasing skill at forecasting hydrologic and atmospheric
processes at high resolution in future climate scenarios, our ability to
forecast land-use and land-cover change (LULC), a critical input to risk and
consequences assessment for these scenarios, has lagged behind. In this paper,
we propose a new paradigm exploiting Generative AI (GenAI) for land cover
change forecasting by framing LULC forecasting as a data synthesis problem
conditioned on historical and auxiliary data-sources. We discuss desirable
properties of generative models that fundament our research premise, and
demonstrate the feasibility of our methodology through experiments on
imperviousness forecasting using historical data covering the entire
conterminous United States. Specifically, we train a diffusion model for
decadal forecasting of imperviousness and compare its performance to a baseline
that assumes no change at all. Evaluation across 12 metropolitan areas for a
year held-out during training indicate that for average resolutions $\geq
0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding
corroborates that such a generative model can capture spatiotemporal patterns
from historical data that are significant for projecting future change.
Finally, we discuss future research to incorporate auxiliary information on
physical properties about the Earth, as well as supporting simulation of
different scenarios by means of driver variables.

</details>


### [75] [Graph Learning via Logic-Based Weisfeiler-Leman Variants and Tabularization](https://arxiv.org/abs/2508.10651)
*Reijo Jaakkola,Tomi Janhunen,Antti Kuusisto,Magdalena Ortiz,Matias Selin,Mantas Šimkus*

Main category: cs.LG

TL;DR: 提出一种基于Weisfeiler-Leman算法变体的图分类新方法，通过表格化图数据并应用表格数据处理方法，实验表明其性能与现有最优方法相当且更高效。


<details>
  <summary>Details</summary>
Motivation: 探索图分类的新方法，结合Weisfeiler-Leman算法的变体，以提高效率和准确性。

Method: 通过修改Weisfeiler-Leman算法的逻辑框架生成变体，表格化图数据后应用表格数据处理方法。

Result: 在12个基准数据集上测试，性能与图神经网络和图核方法相当，且更高效。

Conclusion: 该方法在保持高准确性的同时提升了效率，并支持从图数据中提取可解释的逻辑公式。

Abstract: We present a novel approach for graph classification based on tabularizing
graph data via variants of the Weisfeiler-Leman algorithm and then applying
methods for tabular data. We investigate a comprehensive class of
Weisfeiler-Leman variants obtained by modifying the underlying logical
framework and establish a precise theoretical characterization of their
expressive power. We then test two selected variants on twelve benchmark
datasets that span a range of different domains. The experiments demonstrate
that our approach matches the accuracy of state-of-the-art graph neural
networks and graph kernels while being more time or memory efficient, depending
on the dataset. We also briefly discuss directly extracting interpretable modal
logic formulas from graph datasets.

</details>


### [76] [MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control](https://arxiv.org/abs/2508.10684)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Guan-Horng Liu,Yongxin Chen,Molei Tao*

Main category: cs.LG

TL;DR: 论文提出了一种名为MDNS的新框架，用于训练离散神经采样器，通过路径对齐和随机最优控制理论，解决了高维多模态分布采样问题。


<details>
  <summary>Details</summary>
Motivation: 在统计物理、机器学习和组合优化等领域，从离散状态空间中生成样本是一个重要但具有挑战性的任务，尤其是当状态空间基数大且分布多模态时。

Method: 提出了MDNS框架，通过路径对齐和连续时间马尔可夫链的随机最优控制理论，设计学习目标来训练神经采样器。

Result: 实验表明，MDNS在高维问题中能准确采样目标分布，并显著优于其他基于学习的方法。

Conclusion: MDNS框架在高效性和可扩展性方面表现出色，为离散神经采样提供了新的解决方案。

Abstract: We study the problem of learning a neural sampler to generate samples from
discrete state spaces where the target probability mass function
$\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an
important task in fields such as statistical physics, machine learning,
combinatorial optimization, etc. To better address this challenging task when
the state space has a large cardinality and the distribution is multi-modal, we
propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural
$\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete
neural samplers by aligning two path measures through a family of learning
objectives, theoretically grounded in the stochastic optimal control of the
continuous-time Markov chains. We validate the efficiency and scalability of
MDNS through extensive experiments on various distributions with distinct
statistical properties, where MDNS learns to accurately sample from the target
distributions despite the extremely high problem dimensions and outperforms
other learning-based baselines by a large margin. A comprehensive study of
ablations and extensions is also provided to demonstrate the efficacy and
potential of the proposed framework.

</details>


### [77] [IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data](https://arxiv.org/abs/2508.10775)
*Dong Xu,Zhangfan Yang,Jenna Xinyi Yao,Shuangbao Song,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: IBEX提出了一种基于信息瓶颈理论的粗到细流程，解决了蛋白质-配体复合物数据稀缺问题，显著提升了药物设计的性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-配体复合物数据稀缺导致现有方法难以学习可迁移的几何先验，容易过拟合训练集偏差。

Method: 采用PAC-Bayesian信息瓶颈理论量化样本信息密度，结合Scaffold Hopping任务提升模型能力，保留TargetDiff架构并通过L-BFGS优化细化构象。

Result: 在零样本对接成功率、Vina得分、QED等方面显著提升，并减少了外推误差。

Conclusion: IBEX通过信息瓶颈理论和优化策略，有效解决了数据稀缺问题，提升了药物设计的性能。

Abstract: Three-dimensional generative models increasingly drive structure-based drug
discovery, yet it remains constrained by the scarce publicly available
protein-ligand complexes. Under such data scarcity, almost all existing
pipelines struggle to learn transferable geometric priors and consequently
overfit to training-set biases. As such, we present IBEX, an
Information-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic
shortage of protein-ligand complex data in structure-based drug design.
Specifically, we use PAC-Bayesian information-bottleneck theory to quantify the
information density of each sample. This analysis reveals how different masking
strategies affect generalization and indicates that, compared with conventional
de novo generation, the constrained Scaffold Hopping task endows the model with
greater effective capacity and improved transfer performance. IBEX retains the
original TargetDiff architecture and hyperparameters for training to generate
molecules compatible with the binding pocket; it then applies an L-BFGS
optimization step to finely refine each conformation by optimizing five
physics-based terms and adjusting six translational and rotational degrees of
freedom in under one second. With only these modifications, IBEX raises the
zero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to
64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal
mol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus
3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves
state-of-the-art validity and diversity, and markedly reduces extrapolation
error.

</details>


### [78] [Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee](https://arxiv.org/abs/2508.10804)
*Yu-Heng Hung,Ping-Chun Hsieh,Kai Wang*

Main category: cs.LG

TL;DR: 论文提出了一种针对非平稳多臂老虎机问题的算法RMAB，结合滑动窗口强化学习和UCB机制，首次为非平稳RMAB问题提供了理论框架。


<details>
  <summary>Details</summary>
Motivation: 现实应用中（如医疗和推荐系统），传统RMAB算法的假设（平稳MDP）常因非平稳动态而失效，需要新方法应对。

Method: 提出RMAB算法，结合滑动窗口RL和UCB机制，学习动态变化的状态转移。

Result: 算法实现了$\widetilde{\mathcal{O}}(N^2 B^{\frac{1}{4}} T^{\frac{3}{4}})$的遗憾上界。

Conclusion: RMAB为非平稳RMAB问题提供了首个理论框架，具有实际应用潜力。

Abstract: Online restless multi-armed bandits (RMABs) typically assume that each arm
follows a stationary Markov Decision Process (MDP) with fixed state transitions
and rewards. However, in real-world applications like healthcare and
recommendation systems, these assumptions often break due to non-stationary
dynamics, posing significant challenges for traditional RMAB algorithms. In
this work, we specifically consider $N$-armd RMAB with non-stationary
transition constrained by bounded variation budgets $B$. Our proposed \rmab\;
algorithm integrates sliding window reinforcement learning (RL) with an upper
confidence bound (UCB) mechanism to simultaneously learn transition dynamics
and their variations. We further establish that \rmab\; achieves
$\widetilde{\mathcal{O}}(N^2 B^{\frac{1}{4}} T^{\frac{3}{4}})$ regret bound by
leveraging a relaxed definition of regret, providing a foundational theoretical
framework for non-stationary RMAB problems for the first time.

</details>


### [79] [Comparison of Data Reduction Criteria for Online Gaussian Processes](https://arxiv.org/abs/2508.10815)
*Thore Wietzke,Knut Graichen*

Main category: cs.LG

TL;DR: 本文比较了多种在线高斯过程（GP）的数据点缩减标准，分析了其计算复杂度和缩减行为，并提出了进一步过滤冗余数据点的接受标准。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在回归和系统识别中广泛应用，但其计算复杂度限制了其在小数据集上的适用性，尤其是在流式数据场景下。在线高斯过程通过定义最大数据点预算和移除冗余数据点来解决这一问题。

Method: 本文统一比较了多种缩减标准，分析了它们的计算复杂度和缩减行为，并在基准函数和真实数据集（包括动态系统识别任务）上进行了评估。

Result: 研究结果为在线GP算法选择合适标准提供了实用指南。

Conclusion: 本文为在线高斯过程算法的数据点缩减标准选择提供了实践指导。

Abstract: Gaussian Processes (GPs) are widely used for regression and system
identification due to their flexibility and ability to quantify uncertainty.
However, their computational complexity limits their applicability to small
datasets. Moreover in a streaming scenario, more and more datapoints accumulate
which is intractable even for Sparse GPs. Online GPs aim to alleviate this
problem by e.g. defining a maximum budget of datapoints and removing redundant
datapoints. This work provides a unified comparison of several reduction
criteria, analyzing both their computational complexity and reduction behavior.
The criteria are evaluated on benchmark functions and real-world datasets,
including dynamic system identification tasks. Additionally, acceptance
criteria are proposed to further filter out redundant datapoints. This work
yields practical guidelines for choosing a suitable criterion for an online GP
algorithm.

</details>


### [80] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 本文综述了记忆增强Transformer的统一框架，结合神经科学原理与工程进展，分析了功能目标、记忆表示和整合机制，并探讨了核心记忆操作及挑战。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长范围上下文保留、持续学习和知识整合方面的局限性。

Method: 提出一个统一框架，结合神经科学原理（如动态多时间尺度记忆、选择性注意和巩固）与工程进展，通过三个分类维度（功能目标、记忆表示、整合机制）组织进展。

Result: 揭示了从静态缓存向自适应、测试时学习系统的转变，并提出了分层缓冲和惊喜门控更新等解决方案。

Conclusion: 为认知启发、终身学习的Transformer架构提供了路线图。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


### [81] [SoK: Data Minimization in Machine Learning](https://arxiv.org/abs/2508.10836)
*Robin Staab,Nikola Jovanović,Kimberly Mai,Prakhar Ganesh,Martin Vechev,Ferdinando Fioretto,Matthew Jagielski*

Main category: cs.LG

TL;DR: 论文提出了一个数据最小化（DMML）的综合框架，旨在统一不同研究领域对数据最小化的理解，帮助实践者和研究者更有效地应用这一原则。


<details>
  <summary>Details</summary>
Motivation: 数据最小化是GDPR和CPRA等数据保护法规的核心原则，但在机器学习（ML）应用中常被忽视，导致实践者难以理解和实施。

Method: 引入了一个包含统一数据管道、对手和最小化点的DMML框架，并系统性地回顾了相关文献。

Result: 提出了首个结构化概述，帮助实践者和研究者统一理解并应用数据最小化策略。

Conclusion: 该框架促进了数据最小化在AI/ML中的统一理解和广泛应用。

Abstract: Data minimization (DM) describes the principle of collecting only the data
strictly necessary for a given task. It is a foundational principle across
major data protection regulations like GDPR and CPRA. Violations of this
principle have substantial real-world consequences, with regulatory actions
resulting in fines reaching hundreds of millions of dollars. Notably, the
relevance of data minimization is particularly pronounced in machine learning
(ML) applications, which typically rely on large datasets, resulting in an
emerging research area known as Data Minimization in Machine Learning (DMML).
At the same time, existing work on other ML privacy and security topics often
addresses concerns relevant to DMML without explicitly acknowledging the
connection. This disconnect leads to confusion among practitioners,
complicating their efforts to implement DM principles and interpret the
terminology, metrics, and evaluation criteria used across different research
communities. To address this gap, our work introduces a comprehensive framework
for DMML, including a unified data pipeline, adversaries, and points of
minimization. This framework allows us to systematically review the literature
on data minimization and \emph{DM-adjacent} methodologies, for the first time
presenting a structured overview designed to help practitioners and researchers
effectively apply DM principles. Our work facilitates a unified DM-centric
understanding and broader adoption of data minimization strategies in AI/ML.

</details>


### [82] [Efficiently Verifiable Proofs of Data Attribution](https://arxiv.org/abs/2508.10866)
*Ari Karchmer,Seth Neel,Martin Pawelczyk*

Main category: cs.LG

TL;DR: 论文提出了一种交互式验证方法，解决数据属性估计中的信任问题，确保资源受限方可以验证计算密集型方提供的数据属性是否可靠。


<details>
  <summary>Details</summary>
Motivation: 当前数据属性估计方法计算成本高，导致资源受限方难以信任计算密集型方提供的结果，尤其是在重要下游应用中。

Method: 提出了一种交互式验证协议，由计算能力强的Prover与资源受限的Verifier交互，确保数据属性的准确性和效率。

Result: 协议提供了形式化的完整性、可靠性和效率保证，Verifier的工作量仅与误差参数相关，与数据集大小无关。

Conclusion: 该方法为数据属性验证提供了高效且可靠的解决方案，适用于广泛的属性任务。

Abstract: Data attribution methods aim to answer useful counterfactual questions like
"what would a ML model's prediction be if it were trained on a different
dataset?" However, estimation of data attribution models through techniques
like empirical influence or "datamodeling" remains very computationally
expensive. This causes a critical trust issue: if only a few computationally
rich parties can obtain data attributions, how can resource-constrained parties
trust that the provided attributions are indeed "good," especially when they
are used for important downstream applications (e.g., data pricing)? In this
paper, we address this trust issue by proposing an interactive verification
paradigm for data attribution. An untrusted and computationally powerful Prover
learns data attributions, and then engages in an interactive proof with a
resource-constrained Verifier. Our main result is a protocol that provides
formal completeness, soundness, and efficiency guarantees in the sense of
Probably-Approximately-Correct (PAC) verification. Specifically, if both Prover
and Verifier follow the protocol, the Verifier accepts data attributions that
are {\epsilon}-close to the optimal data attributions (in terms of the Mean
Squared Error) with probability 1-{\delta}. Conversely, if the Prover
arbitrarily deviates from the protocol, even with infinite compute, then this
is detected (or it still yields data attributions to the Verifier) except with
probability {\delta}. Importantly, our protocol ensures the Verifier's
workload, measured by the number of independent model retrainings it must
perform, scales only as O(1/{\epsilon}); i.e., independently of the dataset
size. At a technical level, our results apply to efficiently verifying any
linear function over the boolean hypercube computed by the Prover, making them
broadly applicable to various attribution tasks.

</details>


### [83] [A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design](https://arxiv.org/abs/2508.10899)
*Haydn Thomas Jones,Natalie Maus,Josh Magnus Ludan,Maggie Ziyu Huan,Jiaming Liang,Marcelo Der Torossian Torres,Jiatao Liang,Zachary Ives,Yoseph Barash,Cesar de la Fuente-Nunez,Jacob R. Gardner,Mark Yatskar*

Main category: cs.LG

TL;DR: 论文介绍了一个名为\ourdataset的数据集，用于解决AI驱动药物设计中缺乏实验先验知识的问题，通过LLM管道提取文献中的先验信息，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: AI驱动的药物设计缺乏实验先验知识，可能导致设计出的分子违反隐含约束（如致突变性）。\ourdataset旨在填补这一空白。

Method: 使用LLM管道从文献中提取先验信息，构建包含3230万对自然语言事实和实体表示的数据集，并训练LLM、CLIP和LLava模型。

Result: 在TDC任务中，使用\ourdataset预训练的模型（1500万参数）性能优于20亿参数的TxGemma，并接近90亿参数模型。优化后的分子更安全且有效。

Conclusion: \ourdataset为药物设计提供了有效的先验知识，显著提升了模型性能，并公开了数据集以促进进一步研究。

Abstract: AI-driven discovery can greatly reduce design time and enhance new
therapeutics' effectiveness. Models using simulators explore broad design
spaces but risk violating implicit constraints due to a lack of experimental
priors. For example, in a new analysis we performed on a diverse set of models
on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules
proposed had high probability of being mutagenic. In this work, we introduce
\ourdataset, a dataset of priors for design problems extracted from literature
describing compounds used in lab settings. It is constructed with LLM pipelines
for discovering therapeutic entities in relevant paragraphs and summarizing
information in concise fair-use facts. \ourdataset~ consists of 32.3 million
pairs of natural language facts, and appropriate entity representations (i.e.
SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,
CLIP, and LLava architectures to reason jointly about text and design targets
and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is
highly effective for creating models with strong priors: in supervised
prediction problems that use our data as pretraining, our best models with 15M
learnable parameters outperform larger 2B TxGemma on both regression and
classification TDC tasks, and perform comparably to 9B models on average.
Models built with \ourdataset~can be used as constraints while optimizing for
novel molecules in GuacaMol, resulting in proposals that are safer and nearly
as effective. We release our dataset at
\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},
and will provide expanded versions as available literature grows.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [84] [Output-Sparse Matrix Multiplication Using Compressed Sensing](https://arxiv.org/abs/2508.10250)
*Huck Bennett,Karthik Gajulapalli,Alexander Golovnev,Evelyn Warton*

Main category: cs.DS

TL;DR: 论文提出了两种用于输出稀疏矩阵乘法（OSMM）的算法，并在完全稀疏设置下优化了这些算法。算法适用于任意环，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决输出稀疏矩阵乘法（OSMM）问题，特别是在输入矩阵和输出矩阵均为稀疏的情况下，提高计算效率。

Method: 1. 确定性算法：通过两轮压缩感知技术实现，运行时间为$n^{\omega(\delta/2, 1, 1)}$。2. 随机算法：结合压缩感知和矩阵乘法验证技术，运行时间为$n^{\omega(\delta - 1, 1, 1)}$。

Result: 确定性算法显著优于现有方法，随机算法与最新研究性能相当但更简单。

Conclusion: 提出的算法在性能和简洁性上均有优势，且随机算法的运行时间被证明是最优的。

Abstract: We give two algorithms for output-sparse matrix multiplication (OSMM), the
problem of multiplying two $n \times n$ matrices $A, B$ when their product $AB$
is promised to have at most $O(n^{\delta})$ many non-zero entries for a given
value $\delta \in [0, 2]$. We then show how to speed up these algorithms in the
fully sparse setting, where the input matrices $A, B$ are themselves sparse.
All of our algorithms work over arbitrary rings.
  Our first, deterministic algorithm for OSMM works via a two-pass reduction to
compressed sensing. It runs in roughly $n^{\omega(\delta/2, 1, 1)}$ time, where
$\omega(\cdot, \cdot, \cdot)$ is the rectangular matrix multiplication
exponent. This substantially improves on prior deterministic algorithms for
output-sparse matrix multiplication.
  Our second, randomized algorithm for OSMM works via a reduction to compressed
sensing and a variant of matrix multiplication verification, and runs in
roughly $n^{\omega(\delta - 1, 1, 1)}$ time. This algorithm and its extension
to the fully sparse setting have running times that match those of the
(randomized) algorithms for OSMM and FSMM, respectively, in recent work of
Abboud, Bringmann, Fischer, and K\"{u}nnemann (SODA, 2024). Our algorithm uses
different techniques and is arguably simpler.
  Finally, we observe that the running time of our randomized algorithm and the
algorithm of Abboud et al. are optimal via a simple reduction from rectangular
matrix multiplication.

</details>


### [85] [Lower Bounds on Tree Covers](https://arxiv.org/abs/2508.10376)
*Yu Chen,Zihan Tan,Hangyu Xu*

Main category: cs.DS

TL;DR: 论文改进了树覆盖问题的下界，从之前的Ω(log_k n)提升到Ω(n^(1/2^{k-1}))，填补了研究空白。


<details>
  <summary>Details</summary>
Motivation: 树覆盖在图算法中至关重要，但现有研究在常数k≥2时，上下界存在较大差距，需进一步缩小。

Method: 通过分析结构简单的网格状图，并利用组合不动点定理，提出新的证明方法。

Result: 成功将下界从Ω(log_k n)提升到Ω(n^(1/2^{k-1}))。

Conclusion: 新方法不仅解决了树覆盖问题，还可能适用于其他树状数据结构的分析。

Abstract: Given an $n$-point metric space $(X,d_X)$, a tree cover $\mathcal{T}$ is a
set of $|\mathcal{T}|=k$ trees on $X$ such that every pair of vertices in $X$
has a low-distortion path in one of the trees in $\mathcal{T}$. Tree covers
have been playing a crucial role in graph algorithms for decades, and the
research focus is the construction of tree covers with small size $k$ and
distortion.
  When $k=1$, the best distortion is known to be $\Theta(n)$. For a constant
$k\ge 2$, the best distortion upper bound is $\tilde O(n^{\frac 1 k})$ and the
strongest lower bound is $\Omega(\log_k n)$, leaving a gap to be closed. In
this paper, we improve the lower bound to $\Omega(n^{\frac{1}{2^{k-1}}})$.
  Our proof is a novel analysis on a structurally simple grid-like graph, which
utilizes some combinatorial fixed-point theorems. We believe that they will
prove useful for analyzing other tree-like data structures as well.

</details>


### [86] [On Fixed-Parameter Tractability of Weighted 0-1 Timed Matching Problem on Temporal Graphs](https://arxiv.org/abs/2508.10562)
*Rinku Kumar,Bodhisatwa Mazumdar,Subhrangsu Mandal*

Main category: cs.DS

TL;DR: 本文研究了时态图中最大0-1时间匹配问题的固定参数可解性，证明了即使在静态图树宽有限时问题仍为NP完全，并提出了一种基于顶点度和树宽的FPT算法。


<details>
  <summary>Details</summary>
Motivation: 时态图用于建模随时间变化的系统关系，研究其最大0-1时间匹配问题有助于理解动态系统中的匹配复杂性。

Method: 通过分析时态图的静态图树宽和顶点度，证明了问题的NP完全性和W[1]-难性，并设计了一种FPT算法。

Result: 问题在静态图树宽有限时仍为NP完全，且参数化为解大小时为W[1]-难；但基于顶点度和树宽的参数化是FPT可解的。

Conclusion: 时态图的0-1时间匹配问题在特定参数下可高效求解，为动态系统匹配问题提供了新的理论工具。

Abstract: Temporal graphs are introduced to model systems where the relationships among
the entities of the system evolve over time. In this paper, we consider the
temporal graphs where the edge set changes with time and all the changes are
known a priori. The underlying graph of a temporal graph is a static graph
consisting of all the vertices and edges that exist for at least one timestep
in the temporal graph. The concept of 0-1 timed matching in temporal graphs was
introduced by Mandal and Gupta [DAM2022] as an extension of the matching
problem in static graphs. A 0-1 timed matching of a temporal graph is a
non-overlapping subset of the edge set of that temporal graph. The problem of
finding the maximum 0-1 timed matching is proved to be NP-complete on multiple
classes of temporal graphs. We study the fixed-parameter tractability of the
maximum 0-1 timed matching problem. We prove that the problem remains to be
NP-complete even when the underlying static graph of the temporal graph has a
bounded treewidth. Furthermore, we establish that the problem is W[1]-hard when
parameterized by the solution size. Finally, we present a fixed-parameter
tractable (FPT) algorithm to address the problem when the problem is
parameterized by the maximum vertex degree and the treewidth of the underlying
graph of the temporal graph.

</details>


### [87] [Spirals and Beyond: Competitive Plane Search with Multi-Speed Agents](https://arxiv.org/abs/2508.10793)
*Konstantinos Georgiou,Caleb Jones,Matthew Madej*

Main category: cs.DS

TL;DR: 研究了多速度移动代理在平面上搜索隐藏点目标的最坏情况搜索时间最小化问题，提出了螺旋轨迹算法及其改进策略。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过多速度代理的协同搜索优化最坏情况下的搜索时间，尤其是针对不同速度代理的组合。

Method: 使用对数螺旋轨迹和角度偏移策略，设计了对称螺旋算法和混合螺旋-方向性策略。

Result: 给出了多速度代理的搜索成本上界，并证明在某些情况下非螺旋算法更优。

Conclusion: 螺旋轨迹在多速度代理中可能非最优，混合策略在慢速代理情况下表现更好。

Abstract: We consider the problem of minimizing the worst-case search time for a hidden
point target in the plane using multiple mobile agents of differing speeds, all
starting from a common origin. The search time is normalized by the target's
distance to the origin, following the standard convention in competitive
analysis. The goal is to minimize the maximum such normalized time over all
target locations, the search cost. As a base case, we extend the known result
for a single unit-speed agent, which achieves an optimal cost of about
$\mathcal{U}_1 = 17.28935$ via a logarithmic spiral, to $n$ unit-speed agents.
We give a symmetric spiral-based algorithm where each agent follows a
logarithmic spiral offset by equal angular phases. This yields a search cost
independent of which agent finds the target. We provide a closed-form upper
bound $\mathcal{U}_n$ for this setting, which we use in our general result. Our
main contribution is an upper bound on the worst-case normalized search time
for $n$ agents with arbitrary speeds. We give a framework that selects a subset
of agents and assigns spiral-type trajectories with speed-dependent angular
offsets, again making the search cost independent of which agent reaches the
target. A corollary shows that $n$ multi-speed agents (fastest speed 1) can
beat $k$ unit-speed agents (cost below $\mathcal{U}_k$) if the geometric mean
of their speeds exceeds $\mathcal{U}_n / \mathcal{U}_k$. This means slow agents
may be excluded if they lower the mean too much, motivating non-spiral
algorithms. We also give new upper bounds for point search in cones and conic
complements using a single unit-speed agent. These are then used to design
hybrid spiral-directional strategies, which outperform the spiral-based
algorithms when some agents are slow. This suggests that spiral-type
trajectories may not be optimal in the general multi-speed setting.

</details>


### [88] [Competitively Consistent Clustering](https://arxiv.org/abs/2508.10800)
*Niv Buchbinder,Roie Levin,Yue Yang*

Main category: cs.DS

TL;DR: 该论文研究了完全动态一致聚类问题，提出了在动态数据点变化时保持近似最优聚类解的方法，同时最小化中心点的调整次数。


<details>
  <summary>Details</summary>
Motivation: 动态数据点的频繁变化使得传统静态聚类方法难以适应，需要设计能够实时调整聚类中心并保持近似最优解的算法。

Method: 通过将问题转化为Positive Body Chasing框架，设计算法以维护O(β)-近似解，并限制总调整次数为O(log|F|logΔ)⋅OPT_rec^β。

Result: 算法在保持近似解的同时，调整次数接近最优离线算法的下限，且允许使用略多于k个中心点。

Conclusion: 论文通过理论分析和实验验证，证明了所提算法的有效性，并给出了接近最优的下界。

Abstract: In fully-dynamic consistent clustering, we are given a finite metric space
$(M,d)$, and a set $F\subseteq M$ of possible locations for opening centers.
Data points arrive and depart, and the goal is to maintain an approximately
optimal clustering solution at all times while minimizing the recourse, the
total number of additions/deletions of centers over time. Specifically, we
study fully dynamic versions of the classical $k$-center, facility location,
and $k$-median problems. We design algorithms that, given a parameter
$\beta\geq 1$, maintain an $O(\beta)$-approximate solution at all times, and
whose total recourse is bounded by $O(\log |F| \log \Delta) \cdot
\text{OPT}_\text{rec}^{\beta}$. Here $\text{OPT}_\text{rec}^{\beta}$ is the
minimal recourse of an offline algorithm that maintains a $\beta$-approximate
solution at all times, and $\Delta$ is the metric aspect ratio. Finally, while
we compare the performance of our algorithms to an optimal solution that
maintains $k$ centers, our algorithms are allowed to use slightly more than $k$
centers. We obtain our results via a reduction to the recently proposed
Positive Body Chasing framework of [Bhattacharya, Buchbinder, Levin, Saranurak,
FOCS 2023], which we show gives fractional solutions to our clustering problems
online. Our contribution is to round these fractional solutions while
preserving the approximation and recourse guarantees. We complement our
positive results with logarithmic lower bounds which show that our bounds are
nearly tight.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [89] [Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional Data](https://arxiv.org/abs/2508.10373)
*Yingfan Liu,Yandi Zhang,Jiadong Xie,Hui Li,Jeffrey Xu Yu,Jiangtao Cui*

Main category: cs.DB

TL;DR: 本文提出了一种新的隐私保护近似k最近邻（PP-ANNS）方法，通过单云服务器执行，减少了通信开销，并引入了一种新的加密方法和隐私保护索引，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在云计算和AI时代，数据所有者将向量外包到云端以提供k-ANNS服务，但现有方法无法同时满足隐私、效率、准确性和用户参与度的需求。

Method: 提出了一种基于单云服务器的PP-ANNS方案，采用新型距离比较加密方法，设计了隐私保护索引和基于过滤-精炼策略的搜索方法。

Result: 实验表明，该方法比现有方法快3个数量级，且不损失准确性。

Conclusion: 该方法在隐私保护、效率和准确性方面均优于现有解决方案。

Abstract: In the era of cloud computing and AI, data owners outsource ubiquitous
vectors to the cloud, which furnish approximate $k$-nearest neighbors
($k$-ANNS) services to users. To protect data privacy against the untrusted
server, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental
and urgent problem. However, existing PP-ANNS solutions fall short of meeting
the requirements of data privacy, efficiency, accuracy, and minimal user
involvement concurrently. To tackle this challenge, we introduce a novel
solution that primarily executes PP-ANNS on a single cloud server to avoid the
heavy communication overhead between the cloud and the user. To ensure data
privacy, we introduce a novel encryption method named distance comparison
encryption, facilitating secure, efficient, and exact distance comparisons. To
optimize the trade-off between data privacy and search performance, we design a
privacy-preserving index that combines the state-of-the-art $k$-ANNS method
with an approximate distance computation method. Then, we devise a search
method using a filter-and-refine strategy based on the index. Moreover, we
provide the security analysis of our solution and conduct extensive experiments
to demonstrate its superiority over existing solutions. Based on our
experimental results, our method accelerates PP-ANNS by up to 3 orders of
magnitude compared to state-of-the-art methods, while not compromising the
accuracy.

</details>


### [90] [Cross-Organizational Analysis of Parliamentary Processes: A Case Study](https://arxiv.org/abs/2508.10381)
*Paul-Julius Hillmann,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: 本文首次将流程挖掘应用于议会流程，通过分析德国三个州议会的立法流程，揭示了差异和最佳实践，为政治学与流程挖掘的跨学科研究开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管流程挖掘在商业中广泛应用，但跨组织流程比较的研究较少，因为企业不愿共享数据。本文利用德国州议会公开的数据，填补了这一空白。

Method: 通过流程挖掘技术分析三个德国州议会的立法流程，并与政治学家和联邦议会专家进行知识交流。

Result: 揭示了不同州议会流程的差异和最佳实践，为流程优化提供了依据。

Conclusion: 本文为政治学与流程挖掘的跨学科研究奠定了基础，并展示了公开数据在流程比较中的潜力。

Abstract: Process Mining has been widely adopted by businesses and has been shown to
help organizations analyze and optimize their processes. However, so far,
little attention has gone into the cross-organizational comparison of
processes, since many companies are hesitant to share their data. In this
paper, we explore the processes of German state parliaments that are often
legally required to share their data and run the same type of processes for
different geographical regions. This paper is the first attempt to apply
process mining to parliamentary processes and, therefore, contributes toward a
novel interdisciplinary research area that combines political science and
process mining. In our case study, we analyze legislative processes of three
German state parliaments and generate insights into their differences and best
practices. We provide a discussion of the relevance of our results that are
based on knowledge exchange with a political scientist and a domain expert from
the German federal parliament.

</details>


### [91] [Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching](https://arxiv.org/abs/2508.10460)
*Wei Tian,Jieming Shi,Man Lung Yiu*

Main category: cs.DB

TL;DR: 论文提出TRMMA和MMA方法，分别用于稀疏轨迹的恢复和地图匹配，显著提升了数据质量。


<details>
  <summary>Details</summary>
Motivation: 现实中的轨迹数据稀疏且与路网不对齐，影响应用性能，需要高质量数据。

Method: MMA通过分类任务将GPS点映射到候选路段；TRMMA利用MMA结果，通过双Transformer编码和有效解码推断缺失点。

Result: TRMMA和MMA在实验中表现最优，显著优于现有方法。

Conclusion: TRMMA和MMA能高效恢复高采样轨迹并准确匹配地图，提升数据质量。

Abstract: Real-world trajectories are often sparse with low-sampling rates (i.e., long
intervals between consecutive GPS points) and misaligned with road networks,
yet many applications demand high-quality data for optimal performance. To
improve data quality with sparse trajectories as input, we systematically study
two related research problems: trajectory recovery on road network, which aims
to infer missing points to recover high-sampling trajectories, and map
matching, which aims to map GPS points to road segments to determine underlying
routes. In this paper, we present efficient methods TRMMA and MMA for accurate
trajectory recovery and map matching, respectively, where MMA serves as the
first step of TRMMA. In MMA, we carefully formulate a classification task to
map a GPS point from sparse trajectories to a road segment over a small
candidate segment set, rather than the entire road network. We develop
techniques in MMA to generate effective embeddings that capture the patterns of
GPS data, directional information, and road segments, to accurately align
sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the
segments in the route returned by MMA to infer missing points with position
ratios on road segments, producing high-sampling trajectories efficiently by
avoiding evaluation of all road segments. Specifically, in TRMMA, we design a
dual-transformer encoding process to cohesively capture latent patterns in
trajectories and routes, and an effective decoding technique to sequentially
predict the position ratios and road segments of missing points. We conduct
extensive experiments to compare TRMMA and MMA with numerous existing methods
for trajectory recovery and map matching, respectively, on 4 large real-world
datasets. TRMMA and MMA consistently achieve the best result quality, often by
a significant margin.

</details>


### [92] [Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria](https://arxiv.org/abs/2508.10504)
*Zhliang Xiang,Meghyn Bienvenu,Gianluca Cima,Víctor Gutiérrez-Basulto,Yazmín Ibáñez-García*

Main category: cs.DB

TL;DR: ASPEN+扩展了ASPEN系统，支持局部合并和新优化标准，提升了集体实体解析的灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有系统ASPEN仅支持全局合并，而局部合并在某些场景下更合适，如不同实例的'J. Lee'可能指向不同实体。

Method: ASPEN+引入局部合并功能，并定义新的优化标准（如最小化规则冲突或最大化支持合并的规则数）。

Result: 实验表明，局部合并和新优化标准显著提高了准确性和运行效率。

Conclusion: ASPEN+通过支持局部合并和优化标准，为集体实体解析提供了更灵活和高效的解决方案。

Abstract: In this paper, we present ASPEN+, which extends an existing ASP-based system,
ASPEN,for collective entity resolution with two important functionalities:
support for local merges and new optimality criteria for preferred solutions.
Indeed, ASPEN only supports so-called global merges of entity-referring
constants (e.g. author ids), in which all occurrences of matched constants are
treated as equivalent and merged accordingly. However, it has been argued that
when resolving data values, local merges are often more appropriate, as e.g.
some instances of 'J. Lee' may refer to 'Joy Lee', while others should be
matched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+
offers new optimality criteria for selecting solutions, such as minimizing rule
violations or maximising the number of rules supporting a merge. Our main
contributions are thus (1) the formalisation and computational analysis of
various notions of optimal solution, and (2) an extensive experimental
evaluation on real-world datasets, demonstrating the effect of local merges and
the new optimality criteria on both accuracy and runtime.

</details>


### [93] [Emerging Skycube](https://arxiv.org/abs/2508.10516)
*Mickaël Martin Nevot*

Main category: cs.DB

TL;DR: 论文提出了一种结合多准则决策分析和趋势反转发现的方法，用于提取全局最优数据，并观察其演化。引入了Emerging Skycube概念，结合了Skycube和新兴数据立方体，并提出了一种减少计算时间和存储空间的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的DBMS集成解决方案无法计算Emerging Skycube，无法充分利用ROLAP分析工具。多准则决策分析需要多个属性，而新兴数据立方体仅有一个度量，因此需要扩展。

Method: 提出Emerging Skycube概念，结合Skycube和新兴数据立方体。通过两次Skycube计算后执行趋势反转，减少数据量。进一步提出两种减少方法：基于Skylines概念格的无损部分物化，以及封闭Emerging Skycube或L-Skycube以减少信息损失或实现无损压缩。

Result: Emerging Skycube的计算成本低于新兴数据立方体，且通过提出的减少方法可进一步节省计算时间和存储空间。

Conclusion: Emerging Skycube为多准则决策分析提供了一种高效的计算方法，通过优化减少了资源消耗，同时保持了数据完整性。

Abstract: Combining multi-criteria decision analysis and trend reversal discovery make
it possible to extract globally optimal, or non-dominated, data in relation to
several criteria, and then to observe their evolution according to a
decision-making property. Thus, we introduce Emerging Skycube, a concept
associating Skycube and emerging datacube. As far as we know, no
DBMS-integrated solution exists to compute an emerging Skycube, and hence
taking advantage of ROLAP analysis tools. An emerging datacube has only one
measure: we propose to use several to comply to multi-criteria decision
analysis constraints which requires multiple attributes. A datacube is
expensive to compute. An emerging datacube is about twice as expensive. On the
other hand, an emerging Skycube is cheaper as the trend reversal is computed
after two Skycube calculations, which considerably reduces the relation volume
in comparison with the initial one. It is possible to save even more computing
time and storage space. To this end, we propose two successive reductions.
First, a Skycube lossless partial materialisation using Skylines concepts
lattice, based on the agree concepts lattice and partitions lattice. Then,
either the closed emerging Skycube for an information-loss reduction, or the
closed emerging L-Skycube for a smaller but lossless reduction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [94] [Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment](https://arxiv.org/abs/2508.10116)
*Yipeng Zhang,Hongju Yu,Aritra Mandal,Canran Xu,Qunzhi Zhou,Zhe Wu*

Main category: cs.IR

TL;DR: OPAL框架通过多模态大语言模型（MLLM）从图像生成高质量、符合模式的商品描述，解决了电商中手动输入的不一致性和错误问题。


<details>
  <summary>Details</summary>
Motivation: 解决电商中手动或半手动输入商品信息导致的质量不一致、错误和效率低下的问题，尤其是针对C2C卖家。

Method: OPAL采用视觉指令调整和直接偏好优化微调MLLM，引入两种数据细化方法：MLLM辅助一致性增强和LLM辅助上下文理解。

Result: 在真实电商数据集上，OPAL在描述质量和模式完成率上均优于基线方法。

Conclusion: OPAL有效弥合视觉与文本模态间的差距，为电商平台提供可扩展的高质量内容生成方案。

Abstract: Item information, such as titles and attributes, is essential for effective
user engagement in e-commerce. However, manual or semi-manual entry of
structured item specifics often produces inconsistent quality, errors, and slow
turnaround, especially for Customer-to-Customer sellers. Generating accurate
descriptions directly from item images offers a promising alternative. Existing
retrieval-based solutions address some of these issues but often miss
fine-grained visual details and struggle with niche or specialized categories.
  We propose Optimized Preference-Based AI for Listings (OPAL), a framework for
generating schema-compliant, high-quality item descriptions from images using a
fine-tuned multimodal large language model (MLLM). OPAL addresses key
challenges in multimodal e-commerce applications, including bridging modality
gaps and capturing detailed contextual information. It introduces two data
refinement methods: MLLM-Assisted Conformity Enhancement, which ensures
alignment with structured schema requirements, and LLM-Assisted Contextual
Understanding, which improves the capture of nuanced and fine-grained
information from visual inputs.
  OPAL uses visual instruction tuning combined with direct preference
optimization to fine-tune the MLLM, reducing hallucinations and improving
robustness across different backbone architectures. We evaluate OPAL on
real-world e-commerce datasets, showing that it consistently outperforms
baseline methods in both description quality and schema completion rates. These
results demonstrate that OPAL effectively bridges the gap between visual and
textual modalities, delivering richer, more accurate, and more consistent item
descriptions. This work advances automated listing optimization and supports
scalable, high-quality content generation in e-commerce platforms.

</details>


### [95] [DS4RS: Community-Driven and Explainable Dataset Search Engine for Recommender System Research](https://arxiv.org/abs/2508.10238)
*Xinyang Shao,Tri Kurniawan Wijaya*

Main category: cs.IR

TL;DR: 提出一个社区驱动的可解释数据集搜索引擎，用于推荐系统研究，提升数据集发现和搜索透明度。


<details>
  <summary>Details</summary>
Motivation: 推荐系统研究中，找到适合特定任务或领域的数据集存在挑战，因为数据来源分散且元数据不一致。

Method: 开发一个支持语义搜索的社区驱动平台，涵盖数据集名称、描述和推荐领域等多属性，并提供搜索相关性解释。

Result: 平台公开可用（https://ds4rs.com），支持社区贡献标准化元数据，提升研究复现效率。

Conclusion: 该系统通过改进数据集发现和搜索解释性，促进了推荐系统研究的效率。

Abstract: Accessing suitable datasets is critical for research and development in
recommender systems. However, finding datasets that match specific
recommendation task or domains remains a challenge due to scattered sources and
inconsistent metadata. To address this gap, we propose a community-driven and
explainable dataset search engine tailored for recommender system research. Our
system supports semantic search across multiple dataset attributes, such as
dataset names, descriptions, and recommendation domain, and provides
explanations of search relevance to enhance transparency. The system encourages
community participation by allowing users to contribute standardized dataset
metadata in public repository. By improving dataset discoverability and search
interpretability, the system facilitates more efficient research reproduction.
The platform is publicly available at: https://ds4rs.com.

</details>


### [96] [Clicks Versus Conversion: Choosing a Recommender's Training Objective in E-Commerce](https://arxiv.org/abs/2508.10377)
*Michael Weiss,Robert Rosenbach,Christian Eggenberger*

Main category: cs.IR

TL;DR: 论文比较了在电商推荐系统中优化点击率（CTR）与优化转化率（如加购率ACR和订单提交率OSR）的效果，发现优化OSR能显著提升GMV，且不影响新品发现。


<details>
  <summary>Details</summary>
Motivation: 研究不同优化目标（CTR、ACR、OSR）对电商业务目标（如GMV）的影响，以验证哪种目标更有效。

Method: 通过在线A/B测试比较优化CTR与优化ACR/OSR的效果，并分析特征重要性。

Result: 优化OSR的GMV提升效果是优化CTR的五倍以上，同时不影响新品发现。

Conclusion: 优化直接与业务目标相关的指标（如OSR）比优化通用指标（如CTR）更有效。

Abstract: Ranking product recommendations to optimize for a high click-through rate
(CTR) or for high conversion, such as add-to-cart rate (ACR) and
Order-Submit-Rate (OSR, view-to-purchase conversion) are standard practices in
e-commerce. Optimizing for CTR appears like a straightforward choice: Training
data (i.e., click data) are simple to collect and often available in large
quantities. Additionally, CTR is used far beyond e-commerce, making it a
generalist, easily implemented option. ACR and OSR, on the other hand, are more
directly linked to a shop's business goals, such as the Gross Merchandise Value
(GMV). In this paper, we compare the effects of using either of these
objectives using an online A/B test. Among our key findings, we demonstrate
that in our shops, optimizing for OSR produces a GMV uplift more than five
times larger than when optimizing for CTR, without sacrificing new product
discovery. Our results also provide insights into the different feature
importances for each of the objectives.

</details>


### [97] [Proxy Model-Guided Reinforcement Learning for Client Selection in Federated Recommendation](https://arxiv.org/abs/2508.10401)
*Liang Qu,Jianxin Li,Wei Yuan,Penghui Ruan,Yuhui Shi,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文提出了一种名为ProxyRL-FRS的框架，通过代理模型和强化学习优化联邦推荐系统中的客户端选择，解决了现有随机选择策略导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐系统（FedRS）采用完全随机的客户端选择策略，忽视了用户数据的统计异质性，导致模型性能不佳。本文旨在解决这一问题。

Method: 提出ProxyRL-FRS框架，包括ProxyNCF（双分支代理模型）和SA强化学习代理，用于轻量级贡献评估和客户端选择。

Result: 在公开推荐数据集上的实验证明了ProxyRL-FRS的有效性。

Conclusion: ProxyRL-FRS通过代理模型和强化学习优化客户端选择，显著提升了联邦推荐系统的性能。

Abstract: Federated recommender systems have emerged as a promising privacy-preserving
paradigm, enabling personalized recommendation services without exposing users'
raw data. By keeping data local and relying on a central server to coordinate
training across distributed clients, FedRSs protect user privacy while
collaboratively learning global models. However, most existing FedRS frameworks
adopt fully random client selection strategy in each training round,
overlooking the statistical heterogeneity of user data arising from diverse
preferences and behavior patterns, thereby resulting in suboptimal model
performance. While some client selection strategies have been proposed in the
broader federated learning literature, these methods are typically designed for
generic tasks and fail to address the unique challenges of recommendation
scenarios, such as expensive contribution evaluation due to the large number of
clients, and sparse updates resulting from long-tail item distributions. To
bridge this gap, we propose ProxyRL-FRS, a proxy model-guided reinforcement
learning framework tailored for client selection in federated recommendation.
Specifically, we first introduce ProxyNCF, a dual-branch model deployed on each
client, which augments standard Neural Collaborative Filtering with an
additional proxy model branch that provides lightweight contribution
estimation, thus eliminating the need for expensive per-round local training
traditionally required to evaluate a client's contribution. Furthermore, we
design a staleness-aware SA reinforcement learning agent that selects clients
based on the proxy-estimated contribution, and is guided by a reward function
balancing recommendation accuracy and embedding staleness, thereby enriching
the update coverage of item embeddings. Experiments conducted on public
recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

</details>


### [98] [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)
*Gustavo Penha,Edoardo D'Amico,Marco De Nadai,Enrico Palumbo,Alexandre Tamborrino,Ali Vardasbi,Max Lefarov,Shawn Lin,Timothy Heath,Francesco Fabbri,Hugues Bouchard*

Main category: cs.IR

TL;DR: 论文探讨了如何构建适用于搜索和推荐任务的统一语义ID，通过比较不同策略，发现双编码器模型在统一语义ID空间中的表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统ID和语义ID在统一模型中的表现不一致，需要一种能同时优化搜索和推荐任务的语义ID构建方法。

Method: 比较了任务特定和跨任务的语义ID构建策略，使用双编码器模型微调后构建统一语义ID空间。

Result: 双编码器模型在统一语义ID空间中表现最佳，实现了搜索和推荐任务的双重优化。

Conclusion: 研究为统一生成推荐架构提供了有效方法，并启发了后续通用语义ID方案的研究。

Abstract: Generative models powered by Large Language Models (LLMs) are emerging as a
unified solution for powering both recommendation and search tasks. A key
design choice in these models is how to represent items, traditionally through
unique identifiers (IDs) and more recently with Semantic IDs composed of
discrete codes, obtained from embeddings. While task-specific embedding models
can improve performance for individual tasks, they may not generalize well in a
joint setting. In this paper, we explore how to construct Semantic IDs that
perform well both in search and recommendation when using a unified model. We
compare a range of strategies to construct Semantic IDs, looking into
task-specific and cross-tasks approaches, and also whether each task should
have its own semantic ID tokens in a joint search and recommendation generative
model. Our results show that using a bi-encoder model fine-tuned on both search
and recommendation tasks to obtain item embeddings, followed by the
construction of a unified Semantic ID space provides an effective trade-off,
enabling strong performance in both tasks. We hope these findings spark
follow-up work on generalisable, semantically grounded ID schemes and inform
the next wave of unified generative recommender architectures.

</details>


### [99] [Efficient Patent Searching Using Graph Transformers](https://arxiv.org/abs/2508.10496)
*Krzysztof Daniell,Igor Buzhinsky,Sebastian Björkqvist*

Main category: cs.IR

TL;DR: 提出了一种基于图Transformer的密集检索方法，用于专利搜索，显著提高了检索质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 专利搜索因文档数量庞大和需要复杂比较而具有挑战性，需要高效的搜索工具。

Method: 使用图表示发明的特征及其关系，通过专利审查员的引用作为相关性信号训练模型。

Result: 相比公开文本嵌入模型，检索质量和计算效率均有显著提升。

Conclusion: 该方法能模拟专业审查员的检索方式，有效提升专利搜索效果。

Abstract: Finding relevant prior art is crucial when deciding whether to file a new
patent application or invalidate an existing patent. However, searching for
prior art is challenging due to the large number of patent documents and the
need for nuanced comparisons to determine novelty. An accurate search engine is
therefore invaluable for speeding up the process. We present a Graph
Transformer-based dense retrieval method for patent searching where each
invention is represented by a graph describing its features and their
relationships. Our model processes these invention graphs and is trained using
prior art citations from patent office examiners as relevance signals. Using
graphs as input significantly improves the computational efficiency of
processing long documents, while leveraging examiner citations allows the model
to learn domain-specific similarities beyond simple text-based matching. The
result is a search engine that emulates how professional patent examiners
identify relevant documents. We compare our approach against publicly available
text embedding models and show substantial improvements in both prior art
retrieval quality and computational efficiency.

</details>


### [100] [DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System](https://arxiv.org/abs/2508.10584)
*Wencai Ye,Mingjie Sun,Shaoyun Shi,Peng Wang,Wenjin Wu,Peng Jiang*

Main category: cs.IR

TL;DR: 论文提出了一种名为DAS的单阶段双对齐语义ID方法，用于解决多模态推荐系统中语义ID与协作信号对齐的问题，避免了传统两阶段方法的信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语义ID生成方法因缺乏协作信号而导致与下游推荐目标不匹配，且两阶段框架设计存在信息丢失和灵活性不足的问题。

Method: DAS方法通过单阶段同时优化量化和对齐，采用多视图对比对齐和双学习两种创新方法，提升语义ID与协作信号的对齐效率。

Result: 实验证明DAS方法有效，已在快手App的广告场景中成功部署，每日服务超过4亿用户。

Conclusion: DAS方法在语义ID生成和对齐方面表现出色，解决了传统方法的局限性，具有实际应用价值。

Abstract: Semantic IDs are discrete identifiers generated by quantizing the Multi-modal
Large Language Models (MLLMs) embeddings, enabling efficient multi-modal
content integration in recommendation systems. However, their lack of
collaborative signals results in a misalignment with downstream discriminative
and generative recommendation objectives. Recent studies have introduced
various alignment mechanisms to address this problem, but their two-stage
framework design still leads to two main limitations: (1) inevitable
information loss during alignment, and (2) inflexibility in applying adaptive
alignment strategies, consequently constraining the mutual information
maximization during the alignment process. To address these limitations, we
propose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method
that simultaneously optimizes quantization and alignment, preserving semantic
integrity and alignment quality while avoiding the information loss typically
associated with two-stage methods. Meanwhile, DAS achieves more efficient
alignment between the semantic IDs and collaborative signals, with the
following two innovative and effective approaches: (1) Multi-view Constrative
Alignment: To maximize mutual information between semantic IDs and
collaborative signals, we first incorporate an ID-based CF debias module, and
then design three effective contrastive alignment methods: dual user-to-item
(u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence
item-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual
quantizations of users and ads, the constructed semantic IDs for users and ads
achieve stronger alignment. Finally, we conduct extensive offline experiments
and online A/B tests to evaluate DAS's effectiveness, which is now successfully
deployed across various advertising scenarios at Kuaishou App, serving over 400
million users daily.

</details>


### [101] [FuXi-β: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model](https://arxiv.org/abs/2508.10615)
*Yufei Ye,Wei Guo,Hao Wang,Hong Zhu,Yuyang Ye,Yong Liu,Huifeng Guo,Ruiming Tang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 论文提出了一种新框架FuXi-β，通过优化注意力机制和移除冗余计算，显著提升了生成推荐模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐模型（如HSTU和FuXi-α）存在效率瓶颈，如相对时间注意力偏置的索引操作和查询-键注意力图的计算冗余。

Method: 提出Functional Relative Attention Bias避免耗时操作，并设计Attention-Free Token Mixer模块移除查询-键注意力图。

Result: FuXi-β在多个数据集上优于现有模型，NDCG@10指标提升27%至47%，同时显著加速。

Conclusion: 新框架FuXi-β在保持扩展规律的同时，显著提升了生成推荐模型的效率和性能。

Abstract: Scaling laws for autoregressive generative recommenders reveal potential for
larger, more versatile systems but mean greater latency and training costs. To
accelerate training and inference, we investigated the recent generative
recommendation models HSTU and FuXi-$\alpha$, identifying two efficiency
bottlenecks: the indexing operations in relative temporal attention bias and
the computation of the query-key attention map. Additionally, we observed that
relative attention bias in self-attention mechanisms can also serve as
attention maps. Previous works like Synthesizer have shown that alternative
forms of attention maps can achieve similar performance, naturally raising the
question of whether some attention maps are redundant. Through empirical
experiments, we discovered that using the query-key attention map might degrade
the model's performance in recommendation tasks. To address these bottlenecks,
we propose a new framework applicable to Transformer-like recommendation
models. On one hand, we introduce Functional Relative Attention Bias, which
avoids the time-consuming operations of the original relative attention bias,
thereby accelerating the process. On the other hand, we remove the query-key
attention map from the original self-attention layer and design a new
Attention-Free Token Mixer module. Furthermore, by applying this framework to
FuXi-$\alpha$, we introduce a new model, FuXi-$\beta$. Experiments across
multiple datasets demonstrate that FuXi-$\beta$ outperforms previous
state-of-the-art models and achieves significant acceleration compared to
FuXi-$\alpha$, while also adhering to the scaling law. Notably, FuXi-$\beta$
shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale
industrial datasets compared to FuXi-$\alpha$. Our code is available in a
public repository: https://github.com/USTC-StarTeam/FuXi-beta

</details>


### [102] [Hypercomplex Prompt-aware Multimodal Recommendation](https://arxiv.org/abs/2508.10753)
*Zheyu Chen,Jinfeng Xu,Hewei Wang,Shuo Yang,Zitong Wan,Haibo Hu*

Main category: cs.IR

TL;DR: HPMRec提出了一种基于超复数嵌入的多模态推荐框架，解决了现有方法在表示多样性、非线性模态交互和动态优化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态表示学习中存在单表示能力有限、线性模态融合策略忽略非线性相关性以及静态优化方法无法动态缓解GCN过平滑问题。

Method: HPMRec利用超复数嵌入增强多模态特征表示多样性，采用超复数乘法建立非线性跨模态交互，并引入提示感知补偿机制缓解过平滑问题。

Result: 在四个公开数据集上的实验表明，HPMRec实现了最先进的推荐性能。

Conclusion: HPMRec通过超复数嵌入和提示感知机制，有效提升了多模态推荐系统的性能。

Abstract: Modern recommender systems face critical challenges in handling information
overload while addressing the inherent limitations of multimodal representation
learning. Existing methods suffer from three fundamental limitations: (1)
restricted ability to represent rich multimodal features through a single
representation, (2) existing linear modality fusion strategies ignore the deep
nonlinear correlations between modalities, and (3) static optimization methods
failing to dynamically mitigate the over-smoothing problem in graph
convolutional network (GCN). To overcome these limitations, we propose HPMRec,
a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which
utilizes hypercomplex embeddings in the form of multi-components to enhance the
representation diversity of multimodal features. HPMRec adopts the hypercomplex
multiplication to naturally establish nonlinear cross-modality interactions to
bridge semantic gaps, which is beneficial to explore the cross-modality
features. HPMRec also introduces the prompt-aware compensation mechanism to aid
the misalignment between components and modality-specific features loss, and
this mechanism fundamentally alleviates the over-smoothing problem. It further
designs self-supervised learning tasks that enhance representation diversity
and align different modalities. Extensive experiments on four public datasets
show that HPMRec achieves state-of-the-art recommendation performance.

</details>


### [103] [CrossDenoise: Denoising Implicit Feedback via a Lightweight Entity-Aware Synergistic Framework](https://arxiv.org/abs/2508.10851)
*Ze Liu,Xianquan Wang,Shuochen Liu,Jie Ma,Huibo Xu,Yupeng Han,Zhe Yang,Kai Zhang,Longfei Li,Jun Zhou*

Main category: cs.IR

TL;DR: CrossDenoise是一种轻量级框架，通过分解噪声估计为用户、项目和交互特定因素，显著提升推荐系统准确性。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈中的噪声（假阳性和假阴性）严重降低了推荐准确性，现有方法在实体感知建模、计算开销或超参数调优方面存在不足。

Method: 提出CrossDenoise框架，通过用户和项目的声誉因子（基于训练损失）与交互级权重（基于ECDF）结合，实现高效去噪。

Result: 在多个数据集和模型上，CrossDenoise显著优于现有方法，例如在Yelp上NDCG@50提升27.01%，且计算开销极低。

Conclusion: CrossDenoise提供了一种实用且可扩展的隐式反馈去噪解决方案，能有效区分干净与噪声样本，且对超参数设置鲁棒。

Abstract: Recommender systems heavily rely on implicit feedback, which is inherently
noisy due to false positives and negatives, severely degrading recommendation
accuracy. Existing denoising strategies often overlook entity-aware modeling,
suffer from high computational overhead, or demand excessive hyperparameter
tuning, limiting their real-world applicability. We propose CrossDenoise, a
novel and lightweight framework that addresses these challenges by
disentangling noise estimation into user-, item-, and interaction-specific
factors. Leveraging empirical observations that show significant heterogeneity
in user and item noise propensities, CrossDenoise computes entity reputation
factors (user/item reliability) via a rank-based linear mapping of average
training losses. These are fused with interaction-level weights derived from an
empirical cumulative distribution function (ECDF) of individual losses. This
design is model-agnostic, computationally efficient, and requires only two
intuitive hyperparameters. Extensive experiments on ML-1M, Yelp, and
Amazon-book datasets, across GMF, NeuMF, and CDAE backbones, demonstrate that
CrossDenoise consistently and significantly outperforms state-of-the-art
baselines. For instance, it achieves up to 27.01% NDCG@50 gain on Yelp with
NeuMF, while incurring negligible computational and memory overhead. Our
analysis confirms that CrossDenoise effectively separates clean from noisy
samples and remains robust under varied hyperparameter settings. It offers a
practical and scalable solution for denoising implicit feedback.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 本文综述了利用大语言模型（LLMs）自动化数学建模的最新进展，包括数据合成、模型微调、推理框架、基准数据集和性能评估，并构建了新的公平评估排行榜和在线资源门户。


<details>
  <summary>Details</summary>
Motivation: 优化建模在解决实际问题中具有广泛应用，但需要专业知识。LLMs的出现为自动化数学建模提供了新机会。

Method: 综述了技术栈的各个方面，包括数据合成、模型微调、推理框架等，并清理了基准数据集，构建了新的排行榜和在线门户。

Result: 发现基准数据集错误率高，清理后构建了公平评估排行榜和在线资源门户。

Conclusion: 当前方法存在局限性，未来研究需进一步探索。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [105] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体协作的图形设计评估系统（AgenticDRS），通过图匹配和提示扩展方法提升评估效果，并建立了DRS-BENCH基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 图形设计评估需要多维度分析，但现有方法缺乏系统性协作。

Method: 采用多智能体协作框架，结合图匹配和提示扩展技术，由元智能体协调评估。

Result: 实验表明AgenticDRS在评估图形设计和生成反馈方面优于现有方法。

Conclusion: 该系统为图形设计评估提供了新思路，有望推动这一实用但研究不足的领域发展。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [106] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊Nova AI挑战赛通过对抗性竞赛推动安全AI发展，大学团队开发了先进技术，如推理对齐和多轮越狱，提升了AI在软件开发中的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决AI在软件开发中的安全性问题，推动安全AI技术的进步。

Method: 通过对抗性竞赛（红队与安全AI助手对战）和多轮对话测试，结合高质量标注数据迭代改进。

Result: 团队开发了前沿技术，包括推理对齐、模型护栏、多轮越狱和高效探测大语言模型。

Conclusion: 挑战赛通过协作提升了AI安全性，为软件开发中的AI安全设定了新标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [107] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 论文研究了AI生成内容对人类行为和感知的影响，提出了MhAIM数据集和T-Lens系统，通过新指标量化用户对内容的判断和互动。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，其潜在风险（如误导信息）日益凸显。现有研究多关注内容真实性，而对其如何影响人类行为和感知的研究较少。

Method: 引入MhAIM数据集（含154,552条在线帖子，其中111,153条为AI生成），提出信任度、影响力和开放性三个新指标，并开发基于LLM的T-Lens系统（含HR-MCP协议）。

Result: 研究发现，当帖子包含图文且不一致时，人类更易识别AI内容。T-Lens系统能更好地预测人类反应，提升交互能力。

Conclusion: 研究为LLM提供了人类感知能力的实证工具，揭示了AI与人类认知的复杂关系，提出了缓解AI误导风险的可行策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [108] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体系统，通过关系提取检测新闻文章中的虚假信息，结合四种智能体，实现了高准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: 数字平台上虚假信息的广泛传播对信息完整性构成挑战，需要高效检测方法。

Method: 系统结合四种智能体：机器学习（逻辑回归）、维基百科知识检查（命名实体识别）、一致性检测（LLM提示工程）和网络数据抓取分析（关系三元组提取），通过MCP协议协调。

Result: 多智能体系统准确率达95.3%，F1分数0.964，显著优于传统方法。加权聚合方法优于算法阈值优化。

Conclusion: 模块化架构使系统易于扩展，同时保留决策过程细节，为虚假信息检测提供高效解决方案。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [109] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文系统回顾和比较了多个Agentic AI框架，分析了其架构、通信机制、内存管理等，并探讨了该领域的局限性与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，Agentic AI成为人工智能的新范式，但缺乏系统性的框架比较和未来研究方向。

Method: 通过系统回顾和比较分析多个Agentic AI框架（如CrewAI、LangGraph等），并深入分析通信协议（如CNP、A2A等）。

Result: 建立了一个Agentic AI系统的分类法，并提出了增强可扩展性、鲁棒性和互操作性的未来研究方向。

Conclusion: 本文为研究人员和从业者提供了关于自主AI系统的全面参考，并指明了未来研究的方向。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [110] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 本文研究了开源深度研究代理（ODR）与闭源系统在BrowseComp-Small基准上的表现，通过改进ODR提出ODR+，使其在基准测试中达到10%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理（DRAs）多为闭源系统，缺乏开源选择，本文旨在评估开源ODR的性能并提出改进。

Method: 通过改进ODR提出ODR+，并在BrowseComp-Small基准上测试其与闭源系统的表现。

Result: ODR+在测试集上达到10%的成功率，优于其他闭源和开源系统。

Conclusion: 改进后的ODR+在性能上取得显著提升，为开源DRAs提供了可行方案。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [111] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为LCPO的方法，通过控制输出长度来平衡大型推理模型（LRMs）的推理效率与质量，显著减少了输出长度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在复杂任务中表现优异，但冗长的输出增加了计算成本并可能导致过度思考，需平衡推理效率与质量。

Method: 分析生成路径分布并通过难度估计筛选轨迹，研究偏好优化方法的收敛行为，提出LCPO方法直接平衡NLL损失的隐式奖励。

Result: 实验表明，LCPO在多个基准测试中将平均输出长度减少50%以上，同时保持推理性能。

Conclusion: LCPO展示了在有限数据和训练下高效指导LRMs的潜力，为计算效率提供了新思路。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [112] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI是一个新型AutoML框架，通过动态探索解决方案空间、合并候选方案和集成RAG技术，解决了现有LLM-based AutoML系统的探索局限和执行瓶颈问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based AutoML系统存在探索策略单一和执行瓶颈问题，限制了其性能和迭代优化能力。

Method: KompeteAI采用动态解决方案空间探索，引入合并阶段整合候选方案，并集成RAG技术从Kaggle和arXiv获取策略。此外，通过预测评分模型和加速调试方法减少执行时间。

Result: KompeteAI在MLE-Bench基准测试中平均优于现有方法3%，并将管道评估速度提升6.9倍。

Conclusion: KompeteAI通过创新方法显著提升了AutoML系统的性能和效率，同时提出了新的基准测试Kompete-bench。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [113] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 该论文提出了一种基于事件熵势的概念，用于增强AI中的不确定性量化、决策和可解释性，并探讨了其在多个领域的应用。


<details>
  <summary>Details</summary>
Motivation: 通过引入事件熵势，统一和强化智能系统中的不确定性建模，结合物理学和信息理论的原则。

Method: 将物理学中的熵势概念调整为AI框架，提出事件中心度量，形式化定义并强调条件期望以处理反事实场景。

Result: 展示了熵势在策略评估、内在奖励设计、可解释AI和异常检测中的应用潜力。

Conclusion: 熵势框架为AI中的不确定性管理提供了理论基础、可解释性和多功能性，连接了热力学、信息理论和机器学习。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [114] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: STEP是一种基于预训练语言模型的对话推荐系统，通过课程引导的上下文-知识融合和轻量级任务特定提示调整，解决了现有系统在捕捉用户偏好深度语义和对话上下文方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统难以有效整合外部知识图谱信息，导致推荐结果与用户期望不符。

Method: STEP采用三阶段课程逐步对齐对话上下文与知识图谱实体，并通过双提示方案（对话前缀和推荐前缀）将融合表示注入冻结的语言模型。

Result: 实验结果表明，STEP在两个公共数据集上的推荐精度和对话质量优于主流方法。

Conclusion: STEP通过课程引导和双提示方案，显著提升了对话推荐系统的性能。

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [115] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 本文认为基于大语言模型（LLMs）的AIGC工具（如ChatGPT）的所谓“理解能力”和“推理能力”只是概念模糊者的错觉，LLMs本质上无法具备真正的理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 作者旨在澄清LLMs的工作原理本质限制，指出其无法实现真正正确的推理能力。

Method: 通过分析LLMs的工作原理和本质限制，论证其无法具备真正的理解与推理能力。

Result: LLMs的“理解”和“推理”只是表面现象，无法达到人类水平的真正能力。

Conclusion: LLMs因其工作原理的局限性，永远无法具备真正的理解与推理能力。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [116] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则的可验证逐步奖励机制（VSRM），通过奖励有效步骤和惩罚无效步骤，解决了大型推理模型（LRMs）的过度思考问题，显著提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中表现出色，但存在过度思考问题，导致计算资源浪费和效率降低。现有方法需要预设预算或选择模式，缺乏灵活性和可靠性。

Method: 提出VSRM机制，根据推理轨迹中中间状态的表现分配奖励，结合PPO和Reinforce++算法进行实验验证。

Result: 在AIME24和AIME25等数学推理基准测试中，VSRM显著减少了输出长度，同时保持了推理性能，有效抑制了无效步骤。

Conclusion: VSRM通过奖励有效步骤和惩罚无效步骤，从根本上缓解了过度思考问题，实现了效率与准确性的平衡。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [117] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: 该论文介绍了Dianping-Trust-Safety团队在META CRAG-MM挑战中的解决方案，通过结合视觉大语言模型、课程学习强化学习和外部知识检索，在多模态多轮问答任务中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决多模态多轮问答的复杂需求，特别是在结构化数据检索、多源信息合成和上下文理解方面的挑战。

Method: 1. 任务1：基于视觉大语言模型，通过GPT-4.1知识蒸馏和课程学习强化学习优化；2. 任务2和3：结合外部知识检索（如网页搜索API）处理复杂查询和多轮对话。

Result: 任务1以52.38%的优势排名第一，任务3排名第三，验证了方法的有效性。

Conclusion: 结合课程学习强化学习和外部知识检索的方法在多模态多轮问答任务中表现优异，具有实际应用潜力。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [118] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: HATRPO-W和HATRPO-G是两种改进HATRPO的方法，通过动态分配KL阈值提升多智能体强化学习的性能，分别基于KKT条件和贪婪算法，实验显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统HATRPO中固定KL阈值在异构智能体环境中可能导致更新缓慢和局部最优，需要更灵活的阈值分配方法。

Method: 提出HATRPO-W（基于KKT条件优化全局KL约束）和HATRPO-G（基于贪婪算法按改进-散度比分配阈值）。

Result: 实验表明两种方法均显著提升性能（超过22.5%），HATRPO-W学习更稳定。

Conclusion: 动态KL阈值分配在多智能体强化学习中更有效，HATRPO-W和HATRPO-G为异构环境提供了实用解决方案。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [119] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）在信息稀疏环境中的想象力推理能力，提出了基于“Turtle Soup”游戏的框架，包括新基准、代理和评估协议。实验揭示了LLMs的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法捕捉动态、探索性的想象力推理过程，因此需要新的研究框架。

Method: 基于“Turtle Soup”游戏设计了一个双语交互式基准（TurtleSoup-Bench）和新代理（Mosaic-Agent），并开发了多维评估协议。

Result: 实验显示LLMs在想象力推理中存在明显能力限制和性能差距。

Conclusion: 研究为LLMs的想象力推理提供了新见解，并为未来探索性代理行为研究奠定了基础。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [120] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG提出了一种结合知识聚合与检索策略的框架，解决了现有知识图谱RAG方法中语义孤岛和检索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱RAG方法存在语义孤岛和检索效率低的问题，限制了其有效性。

Method: LeanRAG采用语义聚合算法构建实体集群和显式关系，并通过结构引导的检索策略从细粒度实体到语义路径进行检索。

Result: 在四个QA基准测试中，LeanRAG显著优于现有方法，响应质量提升且检索冗余减少46%。

Conclusion: LeanRAG通过优化知识聚合和检索策略，显著提升了RAG的性能和效率。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [121] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef提出了一种结合医学本体层次结构和EHR共现模式的框架，用于提高药物推荐的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决EHR数据中罕见医学实体和不完整记录导致的模型泛化能力不足问题。

Method: 结合医学本体层次结构和EHR共现模式，使用双曲空间嵌入本体实体，并引入稀疏正则化优化共现图。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优异，且在模拟未见代码场景下保持高准确率。

Conclusion: HiRef通过结合本体和EHR数据，显著提升了药物推荐模型的鲁棒性和泛化能力。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [122] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: MM-Food-100K是一个公开的多模态食品数据集，包含10万样本，源自120万高质量食品图像库。数据集通过社区贡献和AI质量检查构建，支持溯源。实验表明，基于该数据集微调的大模型在营养预测任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 构建一个可溯源、高质量的多模态食品数据集，以支持食品智能研究。

Method: 采用Codatta贡献模型，结合社区众包和AI辅助质量检查，构建数据集。通过微调大模型验证其效用。

Result: 微调后的模型在营养预测任务上表现优于基线，验证了数据集的实用性。

Conclusion: MM-Food-100K是一个高质量、可溯源的食品数据集，为食品智能研究提供了有力支持。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [123] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0是一个统一系统，通过结构化数学知识体系、模型中心数据空间建模和强化学习训练范式，提升多模态大语言模型（MLLMs）的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据集构建和方法优化，忽视了知识驱动的设计和模型中心数据空间建模，导致MLLMs在复杂数学推理上表现不足。

Method: We-Math 2.0整合了四部分：1）五层次数学知识系统；2）MathBook-Standard和Pro数据集；3）两阶段强化学习框架；4）全面评估基准MathBookEval。

Result: 实验表明，MathBook-RL在四个基准测试中表现优异，并在MathBookEval上取得强结果，显示出良好的数学推理泛化能力。

Conclusion: We-Math 2.0通过综合知识系统和强化学习，显著提升了MLLMs的数学推理能力，为未来研究提供了新方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [124] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 论文提出FIRESPARQL框架，通过微调LLM和结合RAG技术，解决SKG问答中SPARQL查询生成的结构和语义错误，实验表明微调方法性能最佳。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的SPARQL查询生成在学术知识图谱（SKG）问答中的结构和语义错误问题。

Method: 提出FIRESPARQL框架，结合微调LLM、RAG技术和SPARQL查询修正层，评估多种配置（零样本、单样本、微调等）。

Result: 微调方法表现最佳，ROUGE-L达0.90，RelaxedEM达0.85。

Conclusion: FIRESPARQL框架显著提升了SKG问答中SPARQL查询生成的准确性和结果质量。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [125] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 论文提出了一种基于大语言模型（LLM）的空间查询系统SEQ-GPT，用于支持自然语言交互的多地点联合搜索任务。


<details>
  <summary>Details</summary>
Motivation: 现有地图服务在复杂任务（如多地点联合搜索）中用户体验受限，需要更灵活的自然语言交互方式。

Method: 引入SEQ-GPT系统，利用LLM的自然语言能力实现动态交互查询，并通过对话合成和多模型协作适配结构化空间数据。

Result: SEQ-GPT展示了通过自然语言交互实现多地点联合搜索的可行性，并提供了端到端的应用场景验证。

Conclusion: SEQ-GPT为空间搜索任务提供了更灵活的自然语言交互方案，扩展了LLM在空间服务中的应用潜力。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [126] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出DxDirector-7B，一种能主导全流程临床诊断的AI模型，显著提升诊断准确性和效率，减少医生工作量。


<details>
  <summary>Details</summary>
Motivation: 当前AI在临床诊断中仅作为辅助工具，无法主导全流程诊断，限制了其减轻医生负担和提升效率的潜力。

Method: 提出DxDirector-7B，一种具备深度思考能力的LLM，能主导诊断流程，并建立责任框架。

Result: 在罕见、复杂及真实案例中，DxDirector-7B显著优于现有医学LLM和通用LLM，减少医生工作量。

Conclusion: DxDirector-7B标志着AI从辅助工具转变为诊断主导者，为高效精准诊断提供新方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [127] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS提出了一种多模态框架，解决了现有工具增强代理系统在医疗影像推理中的黑盒推理、多模态整合不足和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有系统在医疗影像推理中存在黑盒推理、多模态整合不足和计算效率低的问题，限制了其实际应用。

Method: PASS通过概率采样多工具图的工作流，动态选择工具并生成可解释的概率注释路径，同时结合三阶段训练优化性能与成本。

Result: 实验表明，PASS在多个指标上显著优于基线模型，同时平衡了计算成本。

Conclusion: PASS为可解释、自适应和多模态的医疗代理系统提供了新范式。

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [128] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 论文探讨了语言模型（LM）对齐中静态和动态偏好数据的有效性差异，提出了对齐阶段假设，并开发了算法识别阶段边界。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型对齐中静态与动态偏好数据的有效性差异，以优化对齐过程。

Method: 提出对齐阶段假设，将过程分为偏好注入和偏好微调阶段，并通过实验验证。

Result: 实验表明动态数据在不同模型上效果差异显著（如Llama-3提升3倍，Zephyr降低0.4倍）。

Conclusion: 对齐阶段假设具有普适性，边界测量算法有效，为LM对齐提供了新视角。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [129] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 论文提出ComMCS方法，通过结合当前和后续步骤的蒙特卡洛估计器，降低方差，提升大语言模型在数学推理中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂领域（如数学）的推理能力仍有不足，现有方法因训练标注的估计误差（高方差）而受限。

Method: 提出ComMCS方法，线性组合当前和后续步骤的蒙特卡洛估计器，构建无偏估计器，降低方差且不增加计算成本。

Result: 在MATH-500和GSM8K基准测试中，ComMCS比回归优化方法高2.8分，比基线高2.2分。

Conclusion: ComMCS通过降低方差有效提升推理性能，且无需额外计算开销。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [130] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS通过正交子空间分配和混合子空间组合策略，有效减少多属性控制中的干扰，提升模型行为调控的精确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多属性联合调控时存在干扰和权衡问题，MSRS旨在解决这一挑战。

Method: MSRS为每个属性分配正交子空间，结合属性特定和共享子空间，并采用动态权重函数和令牌级调控机制。

Result: 实验表明MSRS显著减少属性冲突，优于现有方法，并能泛化到多种下游任务。

Conclusion: MSRS为多属性调控提供了一种高效且通用的解决方案。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [131] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM是一个基于大语言模型（LLM）的本体对齐框架，通过生成文本定义丰富本体概念的语义表示，结合嵌入模型和精确匹配工具提升对齐精度，在生物医学领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域异构知识源间的语义互操作性问题，提升本体对齐的精度和效率。

Method: 结合LLM生成文本定义、嵌入模型检索候选对齐，并利用精确匹配工具优化结果。

Result: 在OAEI Bio-ML测试中表现优异，超越传统方法和近期LLM方法。

Conclusion: GenOM框架通过语义增强和少样本提示，展现出强大的鲁棒性和适应性。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [132] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 提出了一种稀疏、目标感知的GNN表示方法，解决了传统密集图表示在大规模网格环境中的组合爆炸和信息稀疏问题，显著提升了策略泛化能力和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在符号规划领域中使用密集图表示状态，导致大规模问题中信息稀疏和内存需求激增，学习变得不可行。

Method: 采用稀疏、目标感知的GNN表示，选择性编码局部相关关系并显式整合目标相关的空间特征。

Result: 实验表明，该方法能有效扩展到更大规模的网格环境，显著提升策略泛化能力和成功率。

Conclusion: 为处理现实中的大规模广义规划任务提供了实用基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [133] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 论文通过引入临床试验自然语言推理基准，发现大型语言模型（LLMs）在知识获取和推理能力上存在显著差距，尽管知识准确率高，但推理能力较弱。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否通过扩展数据和参数就能获得结构化、可泛化的内部表示。

Method: 设计了包含四种推理类型的临床试验自然语言推理基准，并引入GKMRV探针以区分知识获取和推理失败。评估了六种当代LLMs在直接和思维链提示下的表现。

Result: LLMs在GKMRV任务上表现优异（准确率0.918），但在主推理任务上表现差（准确率0.25），且输出推理高度一致（一致性0.87），表明其依赖启发式方法。

Conclusion: 当前LLMs缺乏结构化、可组合的内部表示，导致在高风险领域中知识无法可靠应用。GKMRV框架为评估LLMs可靠性提供了有效工具。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [134] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 论文探讨了可解释AI（XAI）在视觉障碍用户中的可访问性问题，通过文献综述和概念验证提出了一种包容性设计方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键领域决策中的应用增加，可解释性成为提升理解的关键，但视觉障碍用户的可访问性研究不足。

Method: 采用文献综述（79项研究）和四部分概念验证（分类、角色定义、原型设计、评估）分析XAI的可访问性。

Result: 初步发现表明，简化解释比详细解释更易理解，多模态呈现有助于提升公平性。

Conclusion: XAI设计需考虑视觉障碍用户，简化解释和多模态呈现是提升可访问性的关键。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [135] [FPT-Approximability of Stable Matching Problems](https://arxiv.org/abs/2508.10129)
*Jiehua Chen,Sanjukta Roy,Sofia Simola*

Main category: cs.GT

TL;DR: 研究了稳定匹配相关的三个优化问题的参数化近似性，包括最小化阻塞对数的稳定婚姻和室友问题，以及最大化稳定匹配数的问题。前两个问题难以近似，第三个问题提出了FPT近似方案。


<details>
  <summary>Details</summary>
Motivation: 探索稳定匹配问题的参数化近似性，解决其计算复杂性。

Method: 分析三个问题的NP-hard和W[1]-hard性质，提出FPT近似方案。

Result: 前两个问题难以近似，第三个问题针对特定参数有FPT近似方案。

Conclusion: 稳定匹配问题的参数化近似性具有挑战性，但部分问题可通过特定参数解决。

Abstract: We study parameterized approximability of three optimization problems related
to stable matching: (1) Min-BP-SMI: Given a stable marriage instance and a
number k, find a size-at-least-k matching that minimizes the number $\beta$ of
blocking pairs; (2) Min-BP-SRI: Given a stable roommates instance, find a
matching that minimizes the number $\beta$ of blocking pairs; (3) Max-SMTI:
Given a stable marriage instance with preferences containing ties, find a
maximum-size stable matching.
  The first two problems are known to be NP-hard to approximate to any constant
factor and W[1]-hard with respect to $\beta$, making the existence of an EPTAS
or FPT-algorithms unlikely. We show that they are W[1]-hard with respect to
$\beta$ to approximate to any function of $\beta$. This means that unless
FPT=W[1], there is no FPT-approximation scheme for the parameter $\beta$. The
last problem (Max-SMTI) is known to be NP-hard to approximate to factor-29/33
and W[1]-hard with respect to the number of ties. We complement this and
present an FPT-approximation scheme for the parameter "number of agents with
ties".

</details>


### [136] [Contested Route Planning](https://arxiv.org/abs/2508.10189)
*Jakub Černý,Garud Iyengar,Christian Kroer*

Main category: cs.GT

TL;DR: 论文提出了一种基于博弈论的路由规划方法，用于对抗性环境中的物流路径优化，通过引入随机性提高路径的不可预测性，并利用双预言机框架实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 在对抗性环境中，传统的确定性路由规划容易被预测和破坏，因此需要一种更灵活且难以被对手预判的路由方法。

Method: 将路由规划建模为两人零和博弈，利用双预言机框架计算多样化的路径方案，并结合专用路由算法作为预言机。

Result: 方法在真实场景中验证有效，能够扩展到实际问题规模，并显著优于基线方法。

Conclusion: 通过博弈论建模和双预言机框架，该方法实现了对抗性环境下的高效、多样化路由规划。

Abstract: We consider the problem of routing for logistics purposes, in a contested
environment where an adversary attempts to disrupt the vehicle along the chosen
route. We construct a game-theoretic model that captures the problem of optimal
routing in such an environment. While basic robust deterministic routing plans
are already challenging to devise, they tend to be predictable, which can limit
their effectiveness. By introducing calculated randomness via modeling the
route planning process as a two-player zero-sum game, we compute immediately
deployable plans that are diversified and harder to anticipate. Although
solving the game exactly is intractable in theory, our use of the double-oracle
framework enables us to achieve computation times on the order of seconds,
making the approach operationally viable. In particular, the framework is
modular enough to accommodate specialized routing algorithms as oracles. We
evaluate our method on real-world scenarios, showing that it scales effectively
to realistic problem sizes and significantly benefits from explicitly modeling
the adversary's capabilities, as demonstrated through ablation studies and
comparisons with baseline approaches.

</details>


### [137] [Spatial Branch-and-Bound for Computing Multiplayer Nash Equilibrium](https://arxiv.org/abs/2508.10204)
*Jakub Černý,Shuvomoy Das Gupta,Christian Kroer*

Main category: cs.GT

TL;DR: 论文提出了一种将纳什均衡计算问题转化为多项式互补问题的方法，并开发了一种基于此的完整且可靠的空间分支定界算法。


<details>
  <summary>Details</summary>
Motivation: 现实多人游戏均衡的计算在多个领域至关重要，但现有方法因计算复杂性和收敛性问题而受限。

Method: 将纳什均衡问题转化为多项式互补问题，并设计空间分支定界算法。

Result: 算法在实证评估中显著优于现有完整方法。

Conclusion: 该方法为解决纳什均衡计算问题提供了高效且可靠的解决方案。

Abstract: Equilibria of realistic multiplayer games constitute a key solution concept
both in practical applications, such as online advertising auctions and
electricity markets, and in analytical frameworks used to study strategic
voting in elections or assess policy impacts in integrated assessment models.
However, efficiently computing these equilibria requires games to have a
carefully designed structure and satisfy numerous restrictions; otherwise, the
computational complexity becomes prohibitive. In particular, finding even
approximate Nash equilibria in general-sum normal-form games with two or more
players is known to be PPAD-complete. Current state-of-the-art algorithms for
computing Nash equilibria in multiplayer normal-form games either suffer from
poor scalability due to their reliance on non-convex optimization solvers, or
lack guarantees of convergence to a true equilibrium. In this paper, we propose
a formulation of the Nash equilibrium computation problem as a polynomial
complementarity problem and develop a complete and sound spatial
branch-and-bound algorithm based on this formulation. We provide a qualitative
analysis arguing why one should expect our approach to perform well, and show
the relationship between approximate solutions to our formulation and that of
computing an approximate Nash equilibrium. Empirical evaluations demonstrate
that our algorithm substantially outperforms existing complete methods.

</details>
