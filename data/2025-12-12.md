<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 4]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Efficient Defective Clique Enumeration and Search with Worst-Case Optimal Search Space](https://arxiv.org/abs/2512.10354)
*Jihoon Jang,Yehyun Nam,Kunsoo Park,Hyunjoon Kim*

Main category: cs.DS

TL;DR: 提出了一种新的"团优先"分支定界框架，用于枚举最大k-缺陷团和搜索最大k-缺陷团，通过新的枢轴技术实现了最优搜索空间，并在大规模真实数据集上比现有算法快4个数量级。


<details>
  <summary>Details</summary>
Motivation: k-缺陷团作为传统团的松弛定义，在链接预测、社区检测和社交网络分析等实际应用中很重要。现有算法存在小部分解的组合爆炸和次优搜索空间等限制。

Method: 提出"团优先"分支定界框架：先生成团，再添加缺失边。引入新的枢轴技术实现搜索空间大小为O(3^(n/3)·n^k)。利用缺陷团的直径二特性进一步减少搜索空间。为最大k-缺陷团搜索提出基于分支定界的高效框架。

Result: 证明了当k为常数时，最大k-缺陷团的最坏情况数量为Ω(3^(n/3)·n^k)，表明算法搜索空间是最坏情况最优的。在超过100万条边的真实基准数据集上，两种算法（枚举和搜索）的处理时间比现有最先进算法快达4个数量级。

Conclusion: 提出的"团优先"分支定界框架和枢轴技术有效解决了现有算法的局限性，实现了最优搜索空间和显著性能提升，为k-缺陷团的高效计算提供了新方法。

Abstract: A $k$-defective clique is a relaxation of the traditional clique definition, allowing up to $k$ missing edges. This relaxation is crucial in various real-world applications such as link prediction, community detection, and social network analysis. Although the problems of enumerating maximal $k$-defective cliques and searching a maximum $k$-defective clique have been extensively studied, existing algorithms suffer from limitations such as the combinatorial explosion of small partial solutions and sub-optimal search spaces. To address these limitations, we propose a novel clique-first branch-and-bound framework that first generates cliques and then adds missing edges. Furthermore, we introduce a new pivoting technique that achieves a search space size of $\mathcal{O}(3^{\frac{n}{3}} \cdot n^k)$, where $n$ is the number of vertices in the input graph. We prove that the worst-case number of maximal $k$-defective cliques is $Ω(3^{\frac{n}{3}} \cdot n^k)$ when $k$ is a constant, establishing that our algorithm's search space is worst-case optimal. Leveraging the diameter-two property of defective cliques, we further reduce the search space size to $\mathcal{O}(n \cdot 3^{\fracδ{3}} \cdot (δΔ)^k)$, where $δ$ is the degeneracy and $Δ$ is the maximum degree of the input graph. We also propose an efficient framework for maximum $k$-defective clique search based on our branch-and-bound, together with practical techniques to reduce the search space. Experiments on real-world benchmark datasets with more than 1 million edges demonstrate that each of our proposed algorithms for maximal $k$-defective clique enumeration and maximum $k$-defective clique search outperforms the respective state-of-the-art algorithms by up to four orders of magnitude in terms of processing time.

</details>


### [2] [Universal Hirschberg for Width Bounded Dynamic Programs](https://arxiv.org/abs/2512.10132)
*Logan Nye*

Main category: cs.DS

TL;DR: 该论文将Hirschberg算法的空间优化思想推广到有向无环图上的动态规划，实现了O(ωlogT + (logT)^O(1))空间复杂度的确定性回溯，适用于多种DP问题。


<details>
  <summary>Details</summary>
Motivation: Hirschberg算法虽然能将最长公共子序列问题的空间复杂度从O(N²)降至O(N)，但这种优化是否仅限于网格DP？论文旨在探索这种空间优化思想能否推广到更广泛的具有局部依赖关系的DP DAGs上。

Method: 将DP建模为在有向无环图上的确定性时间演化，利用拓扑排序和前沿宽度ω的概念。通过构建高度压缩的递归树，用前向重计算替代后向DP，暴露小的"中间前沿"来确保所有最优路径都必须经过。

Result: 证明了在标准离线随机访问模型中，任何此类DP都可在O(ωlogT + (logT)^O(1))空间内进行确定性回溯。该框架适用于非对称和带状序列比对、一维递推以及有界路径宽度图上的DP。

Conclusion: 空间高效的回溯是宽度有界DP DAGs的结构特性，而非网格算法的特殊性质。论文还讨论了Ω(ω)空间下界在单次前向模型中的必要性，支持了这种结构特性的观点。

Abstract: Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $ω$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(ω\log T + (\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small "middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $Ω(ω)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms.

</details>


### [3] [Approximate Counting in Local Lemma Regimes](https://arxiv.org/abs/2512.10134)
*Ryan L. Mann,Gabriel Waite*

Main category: cs.DS

TL;DR: 基于聚类展开方法，为局部引理体系中的事件交集概率和子空间交集维度问题建立了高效近似计数算法。


<details>
  <summary>Details</summary>
Motivation: 研究局部引理体系中自然问题的近似计数算法，特别是事件交集概率和子空间交集维度问题，这些在CNF公式和量子可满足性公式中都有应用。

Method: 采用聚类展开方法，针对对易投影算子提供完全多项式时间近似方案，针对一般投影算子提供两种算法：全局容斥稳定性条件下的FPTAS和谱间隙假设下的高效仿射近似。

Result: 获得了事件交集概率和子空间交集维度的有效近似算法，并作为推论得到了CNF公式满足赋值数量和量子可满足性公式满足子空间维度的有效近似算法。

Conclusion: 聚类展开方法为局部引理体系中的计数问题提供了有效的近似算法框架，扩展了传统计数问题的量子对应问题求解能力。

Abstract: We establish efficient approximate counting algorithms for several natural problems in local lemma regimes. In particular, we consider the probability of intersection of events and the dimension of intersection of subspaces. Our approach is based on the cluster expansion method. We obtain fully polynomial-time approximation schemes for both the probability of intersection and the dimension of intersection for commuting projectors. For general projectors, we provide two algorithms: a fully polynomial-time approximation scheme under a global inclusion-exclusion stability condition, and an efficient affine approximation under a spectral gap assumption. As corollaries of our results, we obtain efficient algorithms for approximating the number of satisfying assignments of conjunctive normal form formulae and the dimension of satisfying subspaces of quantum satisfiability formulae.

</details>


### [4] [Semi-Robust Communication Complexity of Maximum Matching](https://arxiv.org/abs/2512.10532)
*Gabriel Cipriani Huete,Adithya Diddapur,Pavel Dvořák,Christian Konrad*

Main category: cs.DS

TL;DR: 研究半鲁棒设置下的最大匹配单向通信复杂度，提出简单协议：Alice仅发送其边上的字典序第一最大匹配给Bob，证明该协议能达到3/4近似比且分析是紧的。


<details>
  <summary>Details</summary>
Motivation: 研究最大匹配问题在半鲁棒设置下的通信复杂度，其中最大匹配的边随机分配给Alice和Bob，而图中其他边则对抗性地分配。探索简单协议能否在这种困难设置下获得良好近似比。

Method: 提出简单协议：Alice计算其边上的字典序第一最大匹配，并将该匹配发送给Bob。Bob使用这些信息构建最终匹配。分析该协议在半鲁棒设置下的性能。

Result: 证明该简单协议在半鲁棒设置下能达到3/4近似比（期望值），且该分析是紧的。在完全鲁棒设置下也能达到3/4近似比，但存在实例显示协议只能达到0.832近似比，无法超越当前最优的5/6近似比。

Conclusion: 虽然简单协议在半鲁棒设置下表现优异（3/4近似比），但在完全鲁棒设置下无法超越现有最优结果（5/6近似比）。半鲁棒设置至少与完全鲁棒设置一样困难。

Abstract: We study the one-way two-party communication complexity of Maximum Matching in the semi-robust setting where the edges of a maximum matching are randomly partitioned between Alice and Bob, but all remaining edges of the input graph are adversarially partitioned between the two parties.
  We show that the simple protocol where Alice solely communicates a lexicographically-first maximum matching of their edges to Bob is surprisingly powerful: We prove that it yields a $3/4$-approximation in expectation and that our analysis is tight.
  The semi-robust setting is at least as hard as the fully robust setting. In this setting, all edges of the input graph are randomly partitioned between Alice and Bob, and the state-of-the-art result is a fairly involved $5/6$-approximation protocol that is based on the computation of edge-degree constrained subgraphs [Azarmehr, Behnezhad, ICALP'23]. Our protocol also immediately yields a $3/4$-approximation in the fully robust setting. One may wonder whether an improved analysis of our protocol in the fully robust setting is possible: While we cannot rule this out, we give an instance where our protocol only achieves a $0.832 < 5/6 = 0.83$-approximation. Hence, while our simple protocol performs surprisingly well, it cannot be used to improve over the state-of-the-art in the fully robust setting.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [5] [Fourier Sparsity of Delta Functions and Matching Vector PIRs](https://arxiv.org/abs/2512.09941)
*Fatemeh Ghasemi,Swastik Kopparty*

Main category: cs.IT

TL;DR: 研究布尔函数傅里叶分析中delta函数的傅里叶稀疏性问题，该问题与基于匹配向量的私有信息检索方案相关，证明了delta函数傅里叶稀疏性的非平凡上下界，表明仅通过寻找更好的S解码多项式无法显著改进匹配向量PIR方案。


<details>
  <summary>Details</summary>
Motivation: 研究布尔函数傅里叶分析中一个基本自然问题：delta函数（在零点为1，在其他布尔点为零）的最小傅里叶稀疏性。这个问题不仅本身具有内在的理论意义，而且与匹配向量私有信息检索方案中的S解码多项式密切相关，寻找稀疏度更低的S解码多项式可以改进现有的PIR方案。

Method: 通过初等且简洁的证明方法，建立了delta函数傅里叶稀疏性的非平凡上界和下界。研究关注于定义在Z_m^r上的delta函数，分析其在傅里叶基下的稀疏性特性。

Result: 证明了delta函数傅里叶稀疏性的上下界，这些结果表明仅通过寻找更好的S解码多项式无法显著改进匹配向量PIR方案。特别地，基于已知匹配向量族的PIR方案无法通过S解码多项式实现常数服务器数量下的多对数通信复杂度。

Conclusion: 该研究揭示了通过改进S解码多项式来优化匹配向量PIR方案的内在局限性，但许多有趣的问题仍然开放，需要进一步探索。

Abstract: In this paper we study a basic and natural question about Fourier analysis of Boolean functions, which has applications to the study of Matching Vector based Private Information Retrieval (PIR) schemes. For integers m and r, define a delta function on {0,1}^r to be a function f: Z_m^r -> C with f(0) = 1 and f(x) = 0 for all nonzero Boolean x. The basic question we study is how small the Fourier sparsity of a delta function can be; namely how sparse such an f can be in the Fourier basis?
  In addition to being intrinsically interesting and natural, such questions arise naturally when studying "S-decoding polynomials" for the known matching vector families. Finding S-decoding polynomials of reduced sparsity, which corresponds to finding delta functions with low Fourier sparsity, would improve the current best PIR schemes.
  We show nontrivial upper and lower bounds on the Fourier sparsity of delta functions. Our proofs are elementary and clean. These results imply limitations on improving Matching Vector PIR schemes simply by finding better S-decoding polynomials. In particular, there are no S-decoding polynomials that can make Matching Vector PIRs based on the known matching vector families achieve polylogarithmic communication with a constant number of servers. Many interesting questions remain open.

</details>


### [6] [Improving the decoding performance of CA-polar codes](https://arxiv.org/abs/2512.10223)
*Jiewei Feng,Peihong Yuan,Ken R. Duffy,Muriel Médard*

Main category: cs.IT

TL;DR: 使用代码无关解码器将CA-SCL从不完整解码器转换为完整解码器，通过CRC校验失败时应用代码无关解码器来识别满足CRC的码字，在5G NR的CA-Polar码上获得最高0.2 dB的块错误率增益


<details>
  <summary>Details</summary>
Motivation: CA-SCL解码器是不完整的，当无法通过CRC校验时，需要一种方法将其转换为完整解码器以提高解码性能

Method: 当CA-SCL无法识别通过CRC校验的码字时，应用代码无关解码器来识别满足CRC的码字；对于系统CA-polar码，该方法能获得更大增益；利用块级软输出技术控制未检测错误率

Result: 在5G NR标准的CA-Polar码上获得最高0.2 dB的块错误率增益；对于系统CA-polar码，增益可达0.2~1 dB；能够控制未检测错误率

Conclusion: 代码无关解码器能有效将CA-SCL从不完整解码器转换为完整解码器，显著提高解码性能，特别是在系统CA-polar码中效果更佳

Abstract: We investigate the use of modern code-agnostic decoders to convert CA-SCL from an incomplete decoder to a complete one. When CA-SCL fails to identify a codeword that passes the CRC check, we apply a code-agnostic decoder that identifies a codeword that satisfies the CRC. We establish that this approach gives gains of up to 0.2 dB in block error rate for CA-Polar codes from the 5G New Radio standard. If, instead, the message had been encoded in a systematic CA-polar code, the gain improves to 0.2 ~ 1dB. Leveraging recent developments in blockwise soft output, we additionally establish that it is possible to control the undetected error rate even when using the CRC for error correction.

</details>


### [7] [OGC Geotech Interoperability Experiment Engineering Report](https://arxiv.org/abs/2512.10678)
*Mickaël Beaufils,Katharina Schleidt,Hylke van Der Schaaf,Daniel Ponti,Neil Chadwick,Derrick Dasenbrock*

Main category: cs.IT

TL;DR: OGC Geotech互操作性实验报告，旨在开发连接现有编码规范的通用地质工程数据概念模型，实现OGC与buildingSMART国际标准间的集成。


<details>
  <summary>Details</summary>
Motivation: 地质工程数据在不同标准和规范中存在编码差异，需要建立通用概念模型来实现跨标准的数据互操作性和集成。

Method: 通过OGC Geotech互操作性实验，开发连接现有编码规范的通用概念模型，基于项目wiki和文档进行工程报告编写。

Result: 完成了地质工程数据通用概念模型的开发，形成了可直接从项目wiki导入的工程报告，支持OGC与buildingSMART标准的集成。

Conclusion: 成功建立了地质工程数据的通用概念模型，为实现跨标准数据互操作性提供了基础框架，相关成果已在OGC平台发布。

Abstract: This Engineering Report (ER) describes the outcomes of the Open Geospatial Consortium (OGC) Geotech Interoperability Experiment (IE). The objective of this IE was to develop a common conceptual model for describing geotechnical engineering data that bridges existing specifications for encoding those data and which could be integrated across OGC and buildingSMART International Standards, This ER is directly imported from the project wiki found here: https://github.com/opengeospatial/Geotech/wiki. It is also available in html from here: https://docs.ogc.org/per/24-008.html Note that the wiki may be updated after the project end.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [8] [STARS: Semantic Tokens with Augmented Representations for Recommendation at Scale](https://arxiv.org/abs/2512.10149)
*Han Chen,Steven Zhu,Yingrui Li*

Main category: cs.IR

TL;DR: STARS是一个为大规模、低延迟电商场景设计的Transformer序列推荐框架，通过双记忆用户嵌入、语义物品标记、上下文感知评分和延迟敏感的两阶段检索管道，在保持毫秒级响应时间的同时显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现实电商推荐系统需要在严格的毫秒级延迟约束下提供相关推荐，同时面临冷启动产品、快速变化的用户意图以及季节性、节假日、促销等动态上下文挑战。现有系统难以同时满足这些要求。

Method: STARS采用Transformer架构，包含四个关键创新：1) 双记忆用户嵌入分离长期偏好和短期会话意图；2) 语义物品标记融合预训练文本嵌入、可学习增量和大模型生成的属性标签；3) 上下文感知评分学习日历和事件偏移；4) 延迟敏感的两阶段检索管道，离线生成嵌入，在线进行带过滤的最大内积搜索。

Result: 离线评估中，STARS相比现有LambdaMART系统提升Hit@5超过75%。大规模A/B测试（600万次访问）显示统计显著提升：总订单+0.8%，首页加购+2.0%，用户访问次数+0.5%。

Conclusion: 结合语义增强、多意图建模和部署导向设计，可以在不牺牲服务效率的情况下，在现实环境中实现最先进的推荐质量，证明了STARS框架在电商推荐中的有效性。

Abstract: Real-world ecommerce recommender systems must deliver relevant items under strict tens-of-milliseconds latency constraints despite challenges such as cold-start products, rapidly shifting user intent, and dynamic context including seasonality, holidays, and promotions. We introduce STARS, a transformer-based sequential recommendation framework built for large-scale, low-latency ecommerce settings. STARS combines several innovations: dual-memory user embeddings that separate long-term preferences from short-term session intent; semantic item tokens that fuse pretrained text embeddings, learnable deltas, and LLM-derived attribute tags, strengthening content-based matching, long-tail coverage, and cold-start performance; context-aware scoring with learned calendar and event offsets; and a latency-conscious two-stage retrieval pipeline that performs offline embedding generation and online maximum inner-product search with filtering, enabling tens-of-milliseconds response times. In offline evaluations on production-scale data, STARS improves Hit@5 by more than 75 percent relative to our existing LambdaMART system. A large-scale A/B test on 6 million visits shows statistically significant lifts, including Total Orders +0.8%, Add-to-Cart on Home +2.0%, and Visits per User +0.5%. These results demonstrate that combining semantic enrichment, multi-intent modeling, and deployment-oriented design can yield state-of-the-art recommendation quality in real-world environments without sacrificing serving efficiency.

</details>


### [9] [The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation](https://arxiv.org/abs/2512.10388)
*Ziwei Liu,Yejing Wang,Qidong Liu,Zijian Zhang,Chong Chen,Wei Huang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出H2Rec框架，通过双分支架构和双重对齐策略，协调语义ID和哈希ID，解决推荐系统中头部和尾部物品的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐系统使用哈希ID存在长尾问题，而引入语义ID的方法又面临协作压倒现象，导致头部和尾部物品性能权衡困境。

Method: 提出H2Rec框架：1) 双分支建模架构，分别捕捉语义ID的多粒度语义和哈希ID的独特协作身份；2) 双重对齐策略，桥接两种表示，促进知识转移。

Result: 在三个真实世界数据集上的实验表明，H2Rec能有效平衡头部和尾部物品的推荐质量，并超越现有基线方法。

Conclusion: H2Rec通过协调语义ID和哈希ID，解决了推荐系统中头部和尾部物品的性能权衡问题，为长尾推荐提供了有效解决方案。

Abstract: Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \textbf{\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\footnote{https://github.com/ziwliu8/H2Rec}.

</details>


### [10] [Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition](https://arxiv.org/abs/2512.10688)
*Lingfeng Liu,Yixin Song,Dazhong Shen,Bing Yin,Hao Li,Yanyong Zhang,Chao Wang*

Main category: cs.IR

TL;DR: 本文揭示了协同过滤中流行度偏差的本质是贝叶斯成对排序优化的内在几何伪影，并提出方向分解与校正框架进行几何修正


<details>
  <summary>Details</summary>
Motivation: 流行度偏差严重削弱了协同过滤模型的个性化能力，导致过度推荐热门项目而忽视用户对小众内容的真实偏好。现有方法将其视为外部混杂因素，但本文发现这是BPR优化的内在几何问题

Method: 提出方向分解与校正框架，通过非对称方向更新修正嵌入几何：引导正向交互沿个性化偏好方向，同时引导负向交互远离全局流行度方向，从几何源头解耦偏好与流行度

Result: 在多个基于BPR的架构上进行广泛实验，DDC显著优于最先进的去偏方法，将训练损失降低到强基线方法的5%以下，同时实现更优的推荐质量和公平性

Conclusion: 流行度偏差是BPR优化的内在几何伪影而非外部混杂因素，DDC框架通过几何修正有效解决了这一问题，为协同过滤中的去偏提供了新的理论基础和实践方法

Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [11] [Does Timeboost Reduce MEV-Related Spam? Theory and Evidence from Layer-2 Transactions](https://arxiv.org/abs/2512.10094)
*Brian Zhu*

Main category: cs.GT

TL;DR: Timeboost机制通过拍卖时间戳优势减少Layer-2区块链中的MEV相关垃圾交易，将用户支付从交易回滚成本转移到拍卖出价，增加排序器和DAO收入。


<details>
  <summary>Details</summary>
Motivation: Layer-2区块链中MEV机会常导致垃圾交易泛滥：大量相同交易几乎同时提交，多数会回滚，浪费区块空间。需要机制来减少这种低效行为。

Method: 1) 建立博弈论模型，用户选择提交交易副本数量；2) 扩展模型包含Timeboost拍卖和后续交易提交行为；3) 收集多个Layer-2网络的mempool数据，通过窄时间窗口内的相同交易测量垃圾交易；4) 以其他L2为基准，对Arbitrum采用Timeboost进行事件研究。

Result: Timeboost在均衡状态下减少了垃圾交易并增加了排序器/DAO收入，将用户支付从回滚成本转移到拍卖出价。实证显示Arbitrum采用后MEV相关垃圾交易减少、收入增加，与模型预测一致。

Conclusion: Timeboost机制有效解决了Layer-2区块链中的MEV垃圾交易问题，通过拍卖时间戳优势改变了用户行为，提高了网络效率并增加了收入，为类似问题提供了可行的解决方案。

Abstract: Maximal extractable value opportunities often induce spam in Layer-2 blockchains: many identical transactions are submitted near simultaneously, most of which revert, wasting blockspace. We study Timeboost, a mechanism on Arbitrum that auctions a timestamp advantage, crucial under first-come first-served sequencing rules. We develop a game-theoretic model in which users choose the number of transaction copies to submit, and extend upon the baseline setting by modeling the Timeboost auction and subsequent transaction submission behavior. We show that Timeboost reduces spam and increases sequencer/DAO revenue in equilibrium relative to the baseline, transferring user payments from revert costs to auction bids. Empirically, we assemble mempool data from multiple Layer-2 networks, measuring spam via identical transactions submitted in narrow time intervals, and conduct an event study around Timeboost adoption on Arbitrum using other L2s as contemporaneous benchmarks. We find a decline in MEV-related spam and an increase in revenue on Arbitrum post-adoption, consistent with model predictions.

</details>


### [12] [Computing Evolutionarily Stable Strategies in Imperfect-Information Games](https://arxiv.org/abs/2512.10279)
*Sam Ganzfried*

Main category: cs.GT

TL;DR: 提出一种计算对称完美回忆扩展式不完全信息博弈中演化稳定策略的算法，支持两人和多人游戏，在非退化游戏中能计算所有ESS，在退化游戏中计算子集，具有随时性特点。


<details>
  <summary>Details</summary>
Motivation: 演化稳定策略（ESS）在进化博弈论中很重要，但在扩展式不完全信息博弈中计算ESS很困难。现有方法主要针对标准式博弈，需要开发专门针对扩展式不完全信息博弈的ESS计算算法。

Method: 开发了计算对称完美回忆扩展式不完全信息博弈中ESS的算法。主要针对两人游戏，但可扩展到多人游戏。算法具有随时性，可提前停止以找到一个或多个ESS。在非退化游戏中能计算所有ESS，在退化游戏中计算子集。

Result: 算法在非退化游戏中能计算所有ESS，在退化游戏中计算子集。通过在不完全信息癌症信号博弈和随机游戏上的实验验证了算法的可扩展性。算法是可靠的，能有效处理扩展式不完全信息博弈中的ESS计算问题。

Conclusion: 成功开发了计算对称完美回忆扩展式不完全信息博弈中演化稳定策略的算法，填补了该领域的方法空白，为进化博弈论在复杂博弈场景中的应用提供了有效工具。

Abstract: We present an algorithm for computing evolutionarily stable strategies (ESSs) in symmetric perfect-recall extensive-form games of imperfect information. Our main algorithm is for two-player games, and we describe how it can be extended to multiplayer games. The algorithm is sound and computes all ESSs in nondegenerate games and a subset of them in degenerate games which contain an infinite continuum of symmetric Nash equilibria. The algorithm is anytime and can be stopped early to find one or more ESSs. We experiment on an imperfect-information cancer signaling game as well as random games to demonstrate scalability.

</details>


### [13] [Certifying Concavity and Monotonicity in Games via Sum-of-Squares Hierarchies](https://arxiv.org/abs/2512.10292)
*Vincent Leon,Iosif Sakos,Ryann Sim,Antonios Varvitsiotis*

Main category: cs.GT

TL;DR: 该论文研究多人博弈中凹性和单调性的可验证性问题，证明验证这些性质是NP难的，但提出了基于平方和规划的层次结构来有效验证和近似这些性质。


<details>
  <summary>Details</summary>
Motivation: 凹性和单调性在多人博弈中至关重要，它们保证了纳什均衡的存在性、收敛性和唯一性。然而，验证一个博弈是否具有这些性质在计算上非常困难，这限制了这些理论保证在实际中的应用。

Method: 1. 证明验证凹性和单调性是NP难的，即使对于具有多项式效用函数和基本半代数策略集的博弈；2. 开发两个基于平方和（SOS）规划的层次结构来验证凹性和单调性；3. 引入SOS-凹/单调博弈作为全局近似；4. 提出多项式时间算法计算给定博弈的最接近SOS-凹/单调博弈。

Result: 1. 验证凹性和单调性是NP难的；2. 提出的SOS层次结构能在多项式时间内求解每个层级；3. 几乎所有凹/单调博弈都能在有限层级被验证；4. 可以多项式时间内计算最接近的SOS-凹/单调博弈；5. 方法成功应用于不完全回忆扩展式博弈的经典例子。

Conclusion: 该研究为博弈论中的凹性和单调性验证提供了计算框架，通过平方和规划层次结构克服了NP难问题，使得这些重要性质在实际博弈分析中变得可验证和可近似。

Abstract: Concavity and its refinements underpin tractability in multiplayer games, where players independently choose actions to maximize their own payoffs which depend on other players' actions. In concave games, where players' strategy sets are compact and convex, and their payoffs are concave in their own actions, strong guarantees follow: Nash equilibria always exist and decentralized algorithms converge to equilibria. If the game is furthermore monotone, an even stronger guarantee holds: Nash equilibria are unique under strictness assumptions. Unfortunately, we show that certifying concavity or monotonicity is NP-hard, already for games where utilities are multivariate polynomials and compact, convex basic semialgebraic strategy sets -- an expressive class that captures extensive-form games with imperfect recall. On the positive side, we develop two hierarchies of sum-of-squares programs that certify concavity and monotonicity of a given game, and each level of the hierarchies can be solved in polynomial time. We show that almost all concave/monotone games are certified at some finite level of the hierarchies. Subsequently, we introduce SOS-concave/monotone games, which globally approximate concave/monotone games, and show that for any given game we can compute the closest SOS-concave/monotone game in polynomial time. Finally, we apply our techniques to canonical examples of imperfect recall extensive-form games.

</details>


### [14] [The $k$-flip Ising game](https://arxiv.org/abs/2512.10389)
*Kovalenko Aleksandr,Andrey Leonidov*

Main category: cs.GT

TL;DR: 研究完全图上N个玩家的部分并行动态噪声二元选择（Ising）博弈，其中k个玩家在每个时间步可以改变策略，称为k-flip Ising博弈。分析了博弈的转移矩阵、策略分布的前两阶矩，以及从亚稳态和不稳态到稳定态的首次击中时间分布。


<details>
  <summary>Details</summary>
Motivation: 研究多人博弈系统中策略演化的动力学特性，特别是当部分玩家可以同时改变策略时，系统如何从亚稳态或不稳态过渡到稳定态，以及这种过渡时间如何依赖于可同时改变策略的玩家数量k。

Method: 采用k-flip Ising博弈模型，在完全图上考虑N个玩家，每个时间步有k个玩家可以改变策略。通过解析计算博弈的转移矩阵，分析策略分布φ=N⁺/N的前两阶矩，并研究从不同初始状态到稳定态的首次击中时间分布的前两阶矩。

Result: 发现了亚稳态衰减过程中，首次击中时间分布的前两阶矩对k存在非平凡依赖关系，在某些特定的k*处出现最小值。这归因于k依赖的扩散力和恢复力之间的竞争。

Conclusion: 在k-flip Ising博弈中，可同时改变策略的玩家数量k对系统从亚稳态到稳定态的过渡动力学有重要影响，存在最优的k*使得过渡时间最短，这反映了系统中扩散机制和恢复机制之间的平衡。

Abstract: A partially parallel dynamical noisy binary choice (Ising) game in discrete time of $N$ players on complete graphs with $k$ players having a possibility of changing their strategies at each time moment called $k$-flip Ising game is considered. Analytical calculation of the transition matrix of game as well as the first two moments of the distribution of $\varphi=N^+/N$, where $N^+$ is a number of players adhering to one of the two strategies, is presented. First two moments of the first hitting time distribution for sample trajectories corresponding to transition from a metastable and unstable states to a stable one are considered. A nontrivial dependence of these moments on $k$ for the decay of a metastable state is discussed. A presence of the minima at certain $k^*$ is attributed to a competition between $k$-dependent diffusion and restoring forces.

</details>


### [15] [LLM-Auction: Generative Auction towards LLM-Native Advertising](https://arxiv.org/abs/2512.10551)
*Chujie Zhao,Qun Hu,Shiping Song,Dagui Chen,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.GT

TL;DR: 提出LLM-Auction：首个基于学习的生成式拍卖机制，将拍卖与LLM生成整合，用于LLM原生广告，通过IRPO算法优化分配效率


<details>
  <summary>Details</summary>
Motivation: LLM原生广告将广告自然融入LLM生成响应，但现有机制将拍卖与生成解耦，要么忽略外部性，要么需要多次LLM推理，不适用于工业场景

Method: 提出LLM-Auction机制，将分配优化建模为LLM输出与机制目标之间的偏好对齐问题，引入迭代奖励偏好优化(IRPO)算法，交替优化奖励模型和LLM

Result: LLM-Auction在分配效率上显著优于现有基线，同时实现期望的机制属性；识别了分配单调性和连续性，证明简单首价支付规则具有良好的激励特性

Conclusion: LLM-Auction是首个学习型生成式拍卖机制，能有效整合拍卖与LLM生成，为LLM原生广告提供实用解决方案，通过IRPO算法实现高效分配且无需额外推理成本

Abstract: The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.

</details>


### [16] [Dynamics of multidimensional Simple Clock Auctions](https://arxiv.org/abs/2512.10614)
*Jad Zeroual,Marianne Akian,Aurélien Bechler,Matthieu Chardy,Stéphane Gaubert*

Main category: cs.GT

TL;DR: 研究简单时钟拍卖中拥有完美信息玩家对简单竞标者的最优策略，当对手估值满足普通替代条件时，最优策略是固定拍品出价；建立连续时间模型并证明解的存在唯一性，价值函数分段线性；应用于2017年澳大利亚多频段频谱拍卖简化版分析。


<details>
  <summary>Details</summary>
Motivation: 研究频谱拍卖中常用的简单时钟拍卖机制，特别关注当一名参与者拥有完美信息而其他参与者采用简单竞标策略时的最优策略问题。这种信息不对称情况在实际拍卖中常见，需要理论分析来指导策略制定。

Method: 建立连续时间版本的简单时钟拍卖模型，价格遵循分段常数动力学的微分包含。使用Filippov意义下的解概念，证明解的存在唯一性，确保连续时间模型与离散时间拍卖在价格增量趋近于零时的极限一致。分析价值函数的性质。

Result: 当对手估值满足普通替代条件时，拥有完美信息玩家的最优策略是固定拍品出价。连续时间模型存在Filippov意义下的唯一解，价值函数分段线性（可能不连续）。这些理论结果在2017年澳大利亚多频段频谱拍卖简化版中得到验证和应用。

Conclusion: 简单时钟拍卖中信息优势玩家的最优策略具有固定性特征，连续时间模型为分析此类拍卖提供了有效框架，分段线性的价值函数性质简化了策略分析，实际拍卖案例验证了理论结果的应用价值。

Abstract: Simple Clock Auctions (SCA) are a mechanism commonly used in spectrum auctions to sell lots of frequency bandwidths. We study such an auction with one player having access to perfect information against straightforward bidders. When the opponents' valuations satisfy the ordinary substitutes condition, we show that it is optimal to bid on a fixed lot overtime. In this setting, we consider a continuous-time version of the SCA auction in which the prices follow a differential inclusion with a piecewise-constant dynamics. We show that there exists a unique solution in the sense of Filippov. This guarantees that the continuous-time model coincides with the limit of the discrete-time auction when price increments tend to zero. Moreover, we show that the value function of this limit auction is piecewise linear (though possibly discontinuous). Finally, we illustrate these results by analyzing a simplified version of the multiband Australian spectrum auction of 2017.

</details>


### [17] [Designing Truthful Mechanisms for Asymptotic Fair Division](https://arxiv.org/abs/2512.10892)
*Jugal Garg,Vishnu V. Narayan,Yuang Eric Shen*

Main category: cs.GT

TL;DR: 该论文研究了在渐近设置下公平分配物品的问题，提出了一个随机化机制，该机制在期望上是真实的，能在多项式时间内高效实现，并以高概率输出无嫉妒分配。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在物品价值分布良好时，当m=Ω(n log n)时，无嫉妒分配以高概率存在。但这些结果依赖于非策略证明机制（如最大福利分配或轮询算法），限制了在策略性代理设置中的应用。本研究旨在扩展理论到更广泛、更现实的联合价值分布类别，并解决Manurangsi和Suksompong在2017年提出的开放性问题。

Method: 提出了一种新的随机化机制，该机制在期望上是真实的（truthful in expectation），能在多项式时间内高效实现，并以高概率输出无嫉妒分配。该方法扩展到更广泛的联合价值分布，允许代理之间的相关性、原子性和对物品具有最高价值的概率不相等。进一步将机制扩展到渐近加权公平分配以及多代理类型和多物品类型设置。

Result: 证明了在更广泛的联合价值分布下，当m=Ω(n log n)时，无嫉妒分配以高概率存在。更重要的是，提出了一个随机化机制，该机制在期望上是真实的，能在多项式时间内高效实现，并以高概率输出无嫉妒分配，解决了先前提出的开放性问题。在渐近加权公平分配和多类型设置中也证明了新的结果。

Conclusion: 该研究扩展了渐近公平分配理论到更现实的联合价值分布，并首次提供了在期望上真实的、高效可实现的机制，以高概率产生无嫉妒分配，为策略性代理环境中的公平分配问题提供了实用解决方案。

Abstract: We study the problem of fairly allocating a set of $m$ goods among $n$ agents in the asymptotic setting, where each item's value for each agent is drawn from an underlying joint distribution. Prior works have shown that if this distribution is well-behaved, then an envy-free allocation exists with high probability when $m=Ω(n\log{n})$ [Dickerson et al., 2014]. Under the stronger assumption that item values are independently and identically distributed (i.i.d.) across agents, this requirement improves to $m=Ω(n\log{n}/\log{\log{n}})$, which is tight [Manurangsi and Suksompong, 2021]. However, these results rely on non-strategyproof mechanisms, such as maximum-welfare allocation or the round-robin algorithm, limiting their applicability in settings with strategic agents.
  In this work, we extend the theory to a broader, more realistic class of joint value distributions, allowing for correlations among agents, atomicity, and unequal probabilities of having the highest value for an item. We show that envy-free allocations continue to exist with a high probability when $m=Ω(n\log{n})$. More importantly, we give a new randomized mechanism that is truthful in expectation, efficiently implementable in polynomial time, and outputs envy-free allocations with high probability, answering an open question posed by [Manurangsi and Suksompong, 2017]. We further extend our mechanism to settings with asymptotic weighted fair division and multiple agent types and good types, proving new results in each case.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [18] [PANDAExpress: a Simpler and Faster PANDA Algorithm](https://arxiv.org/abs/2512.10217)
*Mahmoud Abo Khamis,Hung Q. Ngo,Dan Suciu*

Main category: cs.DB

TL;DR: PANDAExpress改进了PANDA算法，通过新的概率不等式和动态超平面分割方案，消除了polylog(N)因子，在保持通用性的同时达到了与专用算法相当的运行时间。


<details>
  <summary>Details</summary>
Motivation: PANDA算法虽然功能强大，能够处理各种连接查询和析取数据记录规则，但其运行时间中的polylog(N)因子使其在实际应用中不实用，无法达到专用算法的性能水平。

Method: 提出两个创新：1) 证明新的概率不等式来上界DDRs在任意度约束下的输出大小；2) 基于该证明开发PANDAExpress算法，采用动态构造的任意超平面分割方案替代PANDA的轴平行超平面，根据数据偏斜统计进行自适应划分。

Result: PANDAExpress消除了PANDA中的polylog(N)因子，在保持算法通用性和强大功能的同时，达到了与复杂专用算法相当的运行时间。

Conclusion: PANDAExpress解决了PANDA的主要弱点，通过创新的概率不等式和动态超平面分割方案，实现了既通用又高效的查询处理算法，在理论和实践上都取得了重要进展。

Abstract: PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.
  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\tilde O(\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power.

</details>


### [19] [Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint](https://arxiv.org/abs/2512.10621)
*Siwoo Song,Wonseok Shin,Kunsoo Park,Giuseppe F. Italiano,Zhengyi Yang,Wenjie Zhang*

Main category: cs.DB

TL;DR: 提出一种新的超图模式匹配算法，通过引入交集约束、候选超边空间和匹配-过滤框架，显著提升匹配效率


<details>
  <summary>Details</summary>
Motivation: 超图能够建模涉及多个顶点的复杂关系，超图模式匹配是寻找查询超图在数据超图中所有同构嵌入的基本问题。现有算法在处理复杂超图匹配时效率不足，需要更高效的解决方案。

Method: 1. 引入交集约束作为有效嵌入的必要充分条件，加速验证过程；2. 设计候选超边空间数据结构，存储查询超边与数据超边之间的潜在映射；3. 提出匹配-过滤框架，在回溯过程中交替进行匹配和过滤操作，仅保留兼容候选。

Result: 在真实世界数据集上的实验结果表明，该算法在查询处理时间方面显著优于最先进算法，性能提升可达数量级。

Conclusion: 提出的超图模式匹配算法通过创新的约束条件、数据结构和框架设计，实现了高效的超图匹配，为解决复杂关系建模中的模式匹配问题提供了有效方案。

Abstract: A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time.

</details>
