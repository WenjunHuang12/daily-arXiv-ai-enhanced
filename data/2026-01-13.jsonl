{"id": "2601.06678", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.06678", "abs": "https://arxiv.org/abs/2601.06678", "authors": ["Isabelle Mohr", "Joao Gandarela", "John Dujany", "Andre Freitas"], "title": "Reflective Reasoning for SQL Generation", "comment": null, "summary": "Robust text-to-SQL over complex, real-world databases remains brittle even with modern LLMs: iterative refinement often introduces syntactic and semantic drift, corrections tend to be non-transferable across queries, and naive use of large context windows scales poorly. We propose a controlled text-to-SQL framework built around reflective refinement. Instead of repeatedly rewriting the current SQL instance, the system decomposes generation into typed stages and applies feedback as persistent updates to the stage-level generation mechanism. A Reflection-Refinement Loop localizes violations to the responsible stage maximize preservation of previously validated constraints and support monotonic improvement over a query set. The method operates without gold SQL by combining interpreter-based checks with LLM-based semantic coverage verification as epistemic judges. Experiments on Spider and BIRD demonstrate consistent gains over strong prompting baselines, robust convergence within a small refinement budget, and improved execution accuracy across both frontier and open-weight model families."}
{"id": "2601.06705", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.06705", "abs": "https://arxiv.org/abs/2601.06705", "authors": ["Daan de Graaf", "Robert Brijder", "Soham Chakraborty", "George Fletcher", "Bram van de Wall", "Nikolay Yakovets"], "title": "Algorithm Support for Graph Databases, Done Right", "comment": "for GraphAlg compiler source code, see https://github.com/wildarch/graphalg", "summary": "Graph database query languages cannot express algorithms like PageRank, forcing costly data wrangling, while existing solutions such as algorithm libraries, vertex-centric APIs, and recursive CTEs lack the necessary combination of expressiveness, performance, and usability. We present GraphAlg: a domain-specific language for graph algorithms that compiles to relational algebra, enabling seamless integration with query processing pipelines. Built on linear algebra foundations, GraphAlg provides intuitive matrix operations that are amenable to aggressive optimization including sparsity analysis, loop-invariant code motion, and in-place aggregation. Our implementation in AvantGraph demonstrates significant code complexity reduction compared to SQL/Python and Pregel while achieving excellent performance on LDBC Graphalytics benchmarks. GraphAlg establishes that graph databases can serve as unified platforms for both queries and analytics."}
{"id": "2601.06727", "categories": ["cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.06727", "abs": "https://arxiv.org/abs/2601.06727", "authors": ["Chandan Suri", "Gursifath Bhasin"], "title": "Vextra: A Unified Middleware Abstraction for Heterogeneous Vector Database Systems", "comment": "11 pages, 8 figures", "summary": "The rapid integration of vector search into AI applications, particularly for Retrieval Augmented Generation (RAG), has catalyzed the emergence of a diverse ecosystem of specialized vector databases. While this innovation offers a rich choice of features and performance characteristics, it has simultaneously introduced a significant challenge: severe API fragmentation. Developers face a landscape of disparate, proprietary, and often volatile API contracts, which hinders application portability, increases maintenance overhead, and leads to vendor lock-in. This paper introduces Vextra, a novel middleware abstraction layer designed to address this fragmentation. Vextra presents a unified, high-level API for core database operations, including data upsertion, similarity search, and metadata filtering. It employs a pluggable adapter architecture to translate these unified API calls into the native protocols of various backend databases. We argue that such an abstraction layer is a critical step towards maturing the vector database ecosystem, fostering interoperability, and enabling higher-level query optimization, while imposing minimal performance overhead."}
{"id": "2601.06764", "categories": ["cs.DB", "cs.CC"], "pdf": "https://arxiv.org/pdf/2601.06764", "abs": "https://arxiv.org/abs/2601.06764", "authors": ["Jesse Comer", "Val Tannen"], "title": "The Complexity of Finding Missing Answer Repairs", "comment": "Accepted for publication at ICDT 2026", "summary": "We investigate the problem of identifying database repairs for missing tuples in query answers. We show that when the query is part of the input - the combined complexity setting - determining whether or not a repair exists is polynomial-time is equivalent to the satisfiability problem for classes of queries admitting a weak form of projection and selection. We then identify the sub-classes of unions of conjunctive queries with negated atoms, defined by the relational algebra operations permitted to appear in the query, for which the minimal repair problem can be solved in polynomial time. In contrast, we show that the problem is NP-hard, as well as set cover-hard to approximate via strict reductions, whenever both projection and join are permitted in the input query. Additionally, we show that finding the size of a minimal repair for unions of conjunctive queries (with negated atoms permitted) is OptP[log(n)]-complete, while computing a minimal repair is possible with O($n^2$) queries to an NP oracle. With recursion permitted, the combined complexity of all of these variants increases significantly, with an EXP lower bound. However, from the data complexity perspective, we show that minimal repairs can be identified in polynomial time for all queries expressible as semi-positive datalog programs."}
{"id": "2601.07279", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2601.07279", "abs": "https://arxiv.org/abs/2601.07279", "authors": ["Hodaya Barr", "Eden Hartman", "Yonatan Aumann", "Sarit Kraus"], "title": "Coalition Tactics: Bribery and Control in Parliamentary Elections", "comment": null, "summary": "Strategic manipulation of elections is typically studied in the context of promoting individual candidates.\n  In parliamentary elections, however, the focus shifts: voters may care more about the overall governing coalition than the individual parties' seat counts.\n  This paper studies this new problem: manipulating parliamentary elections with the goal of promoting the collective seat count of a coalition of parties.\n  We focus on proportional representation elections, and consider two variants of the problem; one in which the sole goal is to maximize the total number of seats held by the desired coalition, and the other with a dual objective of both promoting the coalition and promoting the relative power of some favorite party within the coalition.\n  We examine two types of strategic manipulations:\n  \\emph{bribery}, which allows modifying voters' preferences, and \\emph{control}, which allows\n  changing the sets of voters and parties.\n  We consider multiple bribery types, presenting polynomial-time algorithms for some, while proving NP-hardness for others.\n  For control, we provide polynomial-time algorithms for control by adding and deleting voters. In contrast, control by adding and deleting parties, we show, is either impossible (i.e., the problem is immune to control) or computationally hard, in particular, W[1]-hard when parameterized by the number of parties that can be added or deleted."}
{"id": "2601.06629", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06629", "abs": "https://arxiv.org/abs/2601.06629", "authors": ["Luis Alberto Croquevielle", "Roman Sokolovskii", "Thomas Heinis"], "title": "Lower Bounds for the Algorithmic Complexity of Learned Indexes", "comment": null, "summary": "Learned index structures aim to accelerate queries by training machine learning models to approximate the rank function associated with a database attribute. While effective in practice, their theoretical limitations are not fully understood. We present a general framework for proving lower bounds on query time for learned indexes, expressed in terms of their space overhead and parameterized by the model class used for approximation. Our formulation captures a broad family of learned indexes, including most existing designs, as piecewise model-based predictors.\n  We solve the problem of lower bounding query time in two steps: first, we use probabilistic tools to control the effect of sampling when the database attribute is drawn from a probability distribution. Then, we analyze the approximation-theoretic problem of how to optimally represent a cumulative distribution function with approximators from a given model class. Within this framework, we derive lower bounds under a range of modeling and distributional assumptions, paying particular attention to the case of piecewise linear and piecewise constant model classes, which are common in practical implementations.\n  Our analysis shows how tools from approximation theory, such as quantization and Kolmogorov widths, can be leveraged to formalize the space-time tradeoffs inherent to learned index structures. The resulting bounds illuminate core limitations of these methods."}
{"id": "2601.06389", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.06389", "abs": "https://arxiv.org/abs/2601.06389", "authors": ["Ramnath Kumar", "Prateek Jain", "Cho-Jui Hsieh"], "title": "Towards Building efficient Routed systems for Retrieval", "comment": null, "summary": "Late-interaction retrieval models like ColBERT achieve superior accuracy by enabling token-level interactions, but their computational cost hinders scalability and integration with Approximate Nearest Neighbor Search (ANNS). We introduce FastLane, a novel retrieval framework that dynamically routes queries to their most informative representations, eliminating redundant token comparisons. FastLane employs a learnable routing mechanism optimized alongside the embedding model, leveraging self-attention and differentiable selection to maximize efficiency. Our approach reduces computational complexity by up to 30x while maintaining competitive retrieval performance. By bridging late-interaction models with ANNS, FastLane enables scalable, low-latency retrieval, making it feasible for large-scale applications such as search engines, recommendation systems, and question-answering platforms. This work opens pathways for multi-lingual, multi-modal, and long-context retrieval, pushing the frontier of efficient and adaptive information retrieval."}
{"id": "2601.06059", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06059", "abs": "https://arxiv.org/abs/2601.06059", "authors": ["Bingyan Xie", "Yongpeng Wu", "Wenjun Zhang", "Derrick Wing Kwan Ng", "Merouane Debbah"], "title": "Context Video Semantic Transmission with Variable Length and Rate Coding over MIMO Channels", "comment": null, "summary": "The evolution of semantic communications has profoundly impacted wireless video transmission, whose applications dominate driver of modern bandwidth consumption. However, most existing schemes are predominantly optimized for simple additive white Gaussian noise or Rayleigh fading channels, neglecting the ubiquitous multiple-input multiple-output (MIMO) environments that critically hinder practical deployment. To bridge this gap, we propose the context video semantic transmission (CVST) framework under MIMO channels. Building upon an efficient contextual video transmission backbone, CVST effectively learns a context-channel correlation map to explicitly formulate the relationships between feature groups and MIMO subchannels. Leveraging these channel-aware features, we design a multi-reference entropy coding mechanism, enabling channel state-aware variable length coding. Furthermore, CVST incorporates a checkerboard-based feature modulation strategy to achieve multiple rate points within a single trained model, thereby enhancing deployment flexibility. These innovations constitute our multi-reference variable length and rate coding (MR-VLRC) scheme. By integrating contextual transmission with MR-VLRC, CVST demonstrates substantial performance gains over various standardized separated coding methods and recent wireless video semantic communication approaches. The code is available at https://github.com/xie233333/CVST."}
{"id": "2601.06940", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06940", "abs": "https://arxiv.org/abs/2601.06940", "authors": ["Hengyu Liu", "Tianyi Li", "Haoyu Wang", "Kristian Torp", "Tiancheng Zhang", "Yushuai Li", "Christian S. Jensen"], "title": "VISTA: Knowledge-Driven Interpretable Vessel Trajectory Imputation via Large Language Models", "comment": "22 pages, 13 figures, 3 algorithms, 5 tables. Code available at https://github.com/hyLiu1994/VISTA", "summary": "The Automatic Identification System provides critical information for maritime navigation and safety, yet its trajectories are often incomplete due to signal loss or deliberate tampering. Existing imputation methods emphasize trajectory recovery, paying limited attention to interpretability and failing to provide underlying knowledge that benefits downstream tasks such as anomaly detection and route planning. We propose knowledge-driven interpretable vessel trajectory imputation (VISTA), the first trajectory imputation framework that offers interpretability while simultaneously providing underlying knowledge to support downstream analysis. Specifically, we first define underlying knowledge as a combination of Structured Data-derived Knowledge (SDK) distilled from AIS data and Implicit LLM Knowledge acquired from large-scale Internet corpora. Second, to manage and leverage the SDK effectively at scale, we develop a data-knowledge-data loop that employs a Structured Data-derived Knowledge Graph for SDK extraction and knowledge-driven trajectory imputation. Third, to efficiently process large-scale AIS data, we introduce a workflow management layer that coordinates the end-to-end pipeline, enabling parallel knowledge extraction and trajectory imputation with anomaly handling and redundancy elimination. Experiments on two large AIS datasets show that VISTA is capable of state-of-the-art imputation accuracy and computational efficiency, improving over state-of-the-art baselines by 5%-94% and reducing time cost by 51%-93%, while producing interpretable knowledge cues that benefit downstream tasks. The source code and implementation details of VISTA are publicly available."}
{"id": "2601.07510", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.07510", "abs": "https://arxiv.org/abs/2601.07510", "authors": ["Xiang Li", "Jianwei Huang", "Kai Yang", "Chenyou Fan"], "title": "Machine Learning Model Trading with Verification under Information Asymmetry", "comment": "Accepted in IEEE TRANSACTIONS ON NETWORKING 2025", "summary": "Machine learning (ML) model trading, known for its role in protecting data privacy, faces a major challenge: information asymmetry. This issue can lead to model deception, a problem that current literature has not fully solved, where the seller misrepresents model performance to earn more. We propose a game-theoretic approach, adding a verification step in the ML model market that lets buyers check model quality before buying. However, this method can be expensive and offers imperfect information, making it harder for buyers to decide. Our analysis reveals that a seller might probabilistically conduct model deception considering the chance of model verification. This deception probability decreases with the verification accuracy and increases with the verification cost. To maximize seller payoff, we further design optimal pricing schemes accounting for heterogeneous buyers' strategic behaviors. Interestingly, we find that reducing information asymmetry benefits both the seller and buyer. Meanwhile, protecting buyer order information doesn't improve the payoff for the buyer or the seller. These findings highlight the importance of reducing information asymmetry in ML model trading and open new directions for future research."}
{"id": "2601.06723", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.06723", "abs": "https://arxiv.org/abs/2601.06723", "authors": ["Lisa Hellerstein", "Benedikt M. Plank", "Kevin Schewior"], "title": "Approximating Matroid Basis Testing for Partition Matroids using Budget-In-Expectation", "comment": "Full version of SODA 2026 paper", "summary": "We consider the following Stochastic Boolean Function Evaluation problem, which is closely related to several problems from the literature. A matroid $\\mathcal{M}$ (in compact representation) on ground set $E$ is given, and each element $i\\in E$ is active independently with known probability $p_i\\in(0,1)$. The elements can be queried, upon which it is revealed whether the respective element is active or not. The goal is to find an adaptive querying strategy for determining whether there is a basis of $\\mathcal{M}$ in which all elements are active, with the objective of minimizing the expected number of queries.\n  When $\\mathcal{M}$ is a uniform matroid, this is the problem of evaluating a $k$-of-$n$ function, first studied in the 1970s. This problem is well-understood, and has an optimal adaptive strategy that can be computed in polynomial time.\n  Taking $\\mathcal{M}$ to instead be a partition matroid, we show that previous approaches fail to give a constant-factor approximation. Our main result is a polynomial-time constant-factor approximation algorithm producing a randomized strategy for this partition matroid problem. We obtain this result by combining a new technique with several well-established techniques. Our algorithm adaptively interleaves solutions to several instances of a novel type of stochastic querying problem, with a constraint on the $\\textit{expected}$ cost. We believe that this type of problem is of independent interest, will spark follow-up work, and has the potential for additional applications."}
{"id": "2601.06458", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06458", "abs": "https://arxiv.org/abs/2601.06458", "authors": ["Sayak Chakrabarty", "Souradip Pal"], "title": "PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation", "comment": "9 pages, 2 figures", "summary": "Large Language Models (LLMs) have recently shown strong potential for usage in sequential recommendation tasks through text-only models, which combine advanced prompt design, contrastive alignment, and fine-tuning on downstream domain-specific data. While effective, these approaches overlook the rich visual information present in many real-world recommendation scenarios, particularly in e-commerce. This paper proposes PixRec - a vision-language framework that incorporates both textual attributes and product images into the recommendation pipeline. Our architecture leverages a vision-language model backbone capable of jointly processing image-text sequences, maintaining a dual-tower structure and mixed training objective while aligning multi-modal feature projections for both item-item and user-item interactions. Using the Amazon Reviews dataset augmented with product images, our experiments demonstrate $3\\times$ and 40% improvements in top-rank and top-10 rank accuracy over text-only recommenders respectively, indicating that visual features can help distinguish items with similar textual descriptions. Our work outlines future directions for scaling multi-modal recommenders training, enhancing visual-text feature fusion, and evaluating inference-time performance. This work takes a step toward building software systems utilizing visual information in sequential recommendation for real-world applications like e-commerce."}
{"id": "2601.06075", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06075", "abs": "https://arxiv.org/abs/2601.06075", "authors": ["Ali Hossary", "Laura Crosara", "Stefano Tomasin"], "title": "Jamming Detection in Cell-Free MIMO with Dynamic Graphs", "comment": null, "summary": "Jamming attacks pose a critical threat to wireless networks, particularly in cell-free massive MIMO systems, where distributed access points and user equipment (UE) create complex, time-varying topologies. This paper proposes a novel jamming detection framework leveraging dynamic graphs and graph convolutional neural networks (GCN) to address this challenge. By modeling the network as a dynamic graph, we capture evolving communication links and detect jamming attacks as anomalies in the graph evolution. A GCN-Transformer-based model, trained with supervised learning, learns graph embeddings to identify malicious interference. Performance evaluation in simulated scenarios with moving UEs, varying jamming conditions and channel fadings, demonstrates the method's effectiveness, which is assessed through accuracy and F1 score metrics, achieving promising results for effective jamming detection."}
{"id": "2601.07048", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07048", "abs": "https://arxiv.org/abs/2601.07048", "authors": ["Hunter McCoy", "Zikun Wang", "Prashant Pandey"], "title": "Jasper: ANNS Quantized for Speed, Built for Change on GPU", "comment": null, "summary": "Approximate nearest neighbor search (ANNS) is a core problem in machine learning and information retrieval applications. GPUs offer a promising path to high-performance ANNS: they provide massive parallelism for distance computations, are readily available, and can co-locate with downstream applications.\n  Despite these advantages, current GPU-accelerated ANNS systems face three key limitations. First, real-world applications operate on evolving datasets that require fast batch updates, yet most GPU indices must be rebuilt from scratch when new data arrives. Second, high-dimensional vectors strain memory bandwidth, but current GPU systems lack efficient quantization techniques that reduce data movement without introducing costly random memory accesses. Third, the data-dependent memory accesses inherent to greedy search make overlapping compute and memory difficult, leading to reduced performance.\n  We present Jasper, a GPU-native ANNS system with both high query throughput and updatability. Jasper builds on the Vamana graph index and overcomes existing bottlenecks via three contributions: (1) a CUDA batch-parallel construction algorithm that enables lock-free streaming insertions, (2) a GPU-efficient implementation of RaBitQ quantization that reduces memory footprint up to 8x without the random access penalties, and (3) an optimized greedy search kernel that increases compute utilization, resulting in better latency hiding and higher throughput.\n  Our evaluation across five datasets shows that Jasper achieves up to 1.93x higher query throughput than CAGRA and achieves up to 80% peak utilization as measured by the roofline model. Jasper's construction scales efficiently and constructs indices an average of 2.4x faster than CAGRA while providing updatability that CAGRA lacks. Compared to BANG, the previous fastest GPU Vamana implementation, Jasper delivers 19-131x faster queries."}
{"id": "2601.07712", "categories": ["cs.GT", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.07712", "abs": "https://arxiv.org/abs/2601.07712", "authors": ["Liyang Feng", "Hanlin Sun", "Yu Marco Nie", "Jun Xie", "Jiayang Li"], "title": "Enforcing Priority in Schedule-based User Equilibrium Transit Assignment", "comment": null, "summary": "Denied boarding in congested transit systems induces queuing delays and departure-time shifts that can reshape passenger flows. Correctly modeling these responses in transit assignment hinges on the enforcement of two priority rules: continuance priority for onboard passengers and first-come-first-served (FCFS) boarding among waiting passengers. Existing schedule-based models typically enforce these rules through explicit dynamic loading and group-level expected costs, yet discrete vehicle runs can induce nontrivial within-group cost differences that undermine behavioral consistency. We revisit the implicit-priority framework of Nguyen et al. (2001), which, by encoding boarding priority through the notion of available capacity, characterizes route and departure choices based on realized personal (rather than group-averaged) travel experiences. However, the framework lacks an explicit mathematical formulation and exact computational methods for finding equilibria. Here, we derive an equivalent nonlinear complementarity problem (NCP) formulation and establish equilibrium existence under mild conditions. We also show that multiple equilibria may exist, including behaviorally questionable ones. To rule out these artifacts, we propose a refined arc-level NCP formulation that not only corresponds to a tighter, behaviorally consistent equilibrium concept but also is more computationally tractable. We reformulate the NCP as a continuously differentiable mathematical program with equilibrium constraints (MPEC) and propose two solution algorithms. Numerical studies on benchmark instances and a Hong Kong case study demonstrate that the model reproduces continuance priority and FCFS queuing and captures departure-time shifts driven by the competition for boarding priority."}
{"id": "2601.06737", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.06737", "abs": "https://arxiv.org/abs/2601.06737", "authors": ["Anay Sinhal", "Arpana Sinhal", "Amit Sinhal", "Amit Hirawat"], "title": "Algorithmic Reductions: Network Flow and NP-Completeness in Real-World Scheduling Problems", "comment": null, "summary": "This paper presents two real-world scheduling problems and their algorithmic solutions through polynomial-time reductions. First, we address the Hospital Patient-to-Bed Assignment problem, demonstrating its reduction to Maximum Bipartite Matching and solution via Network Flow algorithms. Second, we tackle the University Course Scheduling problem, proving its NP-Completeness through reduction from Graph Coloring and providing greedy approximation algorithms. Both problems are implemented in Python, with experimental results validating theoretical complexity analyses. Our Network Flow solution achieves O(n2.51) empirical complexity, while the greedy coloring algorithms demonstrate O(n2) behavior with approximation ratios consistently below the theoretical delta + 1 bound."}
{"id": "2601.06551", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06551", "abs": "https://arxiv.org/abs/2601.06551", "authors": ["Sergii Voloshyn"], "title": "L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a \"retrieve-always\" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements."}
{"id": "2601.06077", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.06077", "abs": "https://arxiv.org/abs/2601.06077", "authors": ["Aolin Xu"], "title": "One if by Land, Two if by Sea, Three if by Four Seas, and More to Come -- Values of Perception, Prediction, Communication, and Common Sense in Decision Making", "comment": null, "summary": "This work aims to rigorously define the values of perception, prediction, communication, and common sense in decision making. The defined quantities are decision-theoretic, but have information-theoretic analogues, e.g., they share some simple but key mathematical properties with Shannon entropy and mutual information, and can reduce to these quantities in particular settings. One interesting observation is that, the value of perception without prediction can be negative, while the value of perception together with prediction and the value of prediction alone are always nonnegative. The defined quantities suggest answers to practical questions arising in the design of autonomous decision-making systems. Example questions include: Do we need to observe and predict the behavior of a particular agent? How important is it? What is the best order to observe and predict the agents? The defined quantities may also provide insights to cognitive science and neural science, toward the understanding of how natural decision makers make use of information gained from different sources and operations."}
{"id": "2601.07183", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07183", "abs": "https://arxiv.org/abs/2601.07183", "authors": ["Zehai Yang", "Shimin Chen"], "title": "RAIRS: Optimizing Redundant Assignment and List Layout for IVF-Based ANN Search", "comment": null, "summary": "IVF is one of the most widely used ANNS (Approximate Nearest Neighbors Search) methods in vector databases. The idea of redundant assignment is to assign a data vector to more than one IVF lists for reducing the chance of missing true neighbors in IVF search. However, the naive strategy, which selects the second IVF list based on the distance between a data vector and the list centroids, performs poorly. Previous work focuses only on the inner product distance, while there is no optimized list selection study for the most popular Euclidean space. Moreover, the IVF search may access the same vector in more than one lists, resulting in redundant distance computation and decreasing query throughput. In this paper, we present RAIRS to address the above two challenges. For the challenge of the list selection, we propose an optimized AIR metric for the Euclidean space. AIR takes not only distances but also directions into consideration in order to support queries that are closer to the data vector but father away from the first chosen list's centroid. For the challenge of redundant distance computation, we propose SEIL, an optimized list layout that exploits shared cells to reduce repeated distance computations for IVF search. Our experimental results using representative real-world data sets show that RAIRS out-performs existing redundant assignment solutions and achieves up to 1.33x improvement over the best-performing IVF method, IVF-PQ Fast Scan with refinement."}
{"id": "2601.07763", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.07763", "abs": "https://arxiv.org/abs/2601.07763", "authors": ["Tatiana Belova", "Yuriy Dementiev", "Artur Ignatiev", "Danil Sagunov"], "title": "Structural Approach to Guiding a Present-Biased Agent", "comment": "Accepted at AAAI 2026", "summary": "Time-inconsistent behavior, such as procrastination or abandonment of long-term goals, arises when agents evaluate immediate outcomes disproportionately higher than future ones. This leads to globally suboptimal behavior, where plans are frequently revised or abandoned entirely. In the influential model of Kleinberg and Oren (2014) such behavior is modeled by a present-biased agent navigating a task graph toward a goal, making locally optimal decisions at each step based on discounted future costs. As a result, the agent may repeatedly deviate from initial plans. Recent work by Belova et al. (2024) introduced a two-agent extension of this model, where a fully-aware principal attempts to guide the present-biased agent through a specific set of critical tasks without causing abandonment. This captures a rich class of principal-agent dynamics in behavioral settings.\n  In this paper, we provide a comprehensive algorithmic characterization of this problem. We analyze its computational complexity through the framework of parameterized algorithms, focusing on graph parameters that naturally emerge in this setting, such as treewidth, vertex cover, and feedback vertex set. Our main result is a fixed-parameter tractable algorithm when parameterized by the treewidth of the task graph and the number of distinct (v,t)-path costs. Our algorithm encaptures several input settings, such as bounded edge costs and restricted task graph structure. We demonstrate that our main result yields efficient algorithms for a number of such configurations.\n  We complement this with tight hardness results, that highlight the extreme difficulty of the problem even on simplest graphs with bounded number of nodes and constant parameter values, and motivate our choice of parameters. We delineate tractable and intractable regions of the problem landscape, which include answers to open questions of Belova et al. (2024)."}
{"id": "2601.06828", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.06828", "abs": "https://arxiv.org/abs/2601.06828", "authors": ["Swarnalipa Datta", "Arijit Ghosh", "Chandrima Kayal", "Manaswi Paraashar", "Manmatha Roy"], "title": "Spectral Shadows: When Communication Complexity Meets Linear Invariance Testing", "comment": "17 pages", "summary": "In this short note, we initiate the study of the Linear Isomorphism Testing Problem in the setting of communication complexity, a natural linear algebraic generalization of the classical Equality problem. Given Boolean functions $f, g : \\mathbb{F}_2^n \\to \\{-1, +1\\}$, Alice and Bob are tasked with determining whether $f$ and $g$ are equivalent up to a nonsingular linear transformation of the input variables, or far from being so. This problem has been extensively investigated in several models of computation, including standard algorithmic and property testing frameworks, owing to its fundamental connections with combinatorial circuit design, complexity theory, and cryptography. However, despite its broad relevance, it has remained unexplored in the context of communication complexity, a gap we address in this work.\n  Our main results demonstrate that the approximate spectral norm of the input functions plays a central role in governing the communication complexity of this problem. We design a simple deterministic protocol whose communication cost is polynomial in the approximate spectral norm, and complement it with nearly matching lower bounds (up to a quadratic gap). In the randomised setting with private coins, we present an even more efficient protocol, though equally simple, that achieves a quadratically improved dependence on the approximate spectral norm compared to the deterministic case, and we prove that such a dependence is essentially unavoidable.\n  These results identify the approximate spectral norm as a key complexity measure for testing linear invariance in the communication complexity framework. As a core technical ingredient, we establish new junta theorems for Boolean functions with small approximate spectral norm, which may be of independent interest in Fourier analysis and learning theory."}
{"id": "2601.06613", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.06613", "abs": "https://arxiv.org/abs/2601.06613", "authors": ["Ariana Metović", "Nicolai Maisch", "Samed Ajdinović", "Armin Lechler", "Andreas Wortmann", "Oliver Riedel"], "title": "Industrial Semantics-Aware Digital Twins: A Hybrid Graph Matching Approach for Asset Administration Shells", "comment": null, "summary": "Although the Asset Administration Shell (AAS) standard provides a structured and machine-readable representation of industrial assets, their semantic comparability remains a major challenge, particularly when different vocabularies and modeling practices are used. Engineering would benefit from retrieving existing AAS models that are similar to the target in order to reuse submodels, parameters, and metadata. In practice, however, heterogeneous vocabularies and divergent modeling conventions hinder automated, content-level comparison across AAS. This paper proposes a hybrid graph matching approach to enable semantics-aware comparison of Digital Twin representations. The method combines rule-based pre-filtering using SPARQL with embedding-based similarity calculation leveraging RDF2vec to capture both structural and semantic relationships between AAS models. This contribution provides a foundation for enhanced discovery, reuse, and automated configuration in Digital Twin networks."}
{"id": "2601.06095", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06095", "abs": "https://arxiv.org/abs/2601.06095", "authors": ["Andrii Grekhov", "Volodymyr Kharchenko", "Vasyl Kondratiuk"], "title": "Deep Q-Network Based Resilient Drone Communication:Neutralizing First-Order Markov Jammers", "comment": "13 pages, 6 figures", "summary": "Deep Reinforcement Learning based solution for jamming communications using Frequency Hopping Spread Spectrum technology in a 16 channel radio environment is presented. Deep Q Network based transmitter continuously selects the next frequency hopping channel while facing first order reactive jamming, which uses observed transition statistics to predict and interrupt transmissions. Through self training, the proposed agent learns a uniform random frequency hopping policy that effectively neutralizes the predictive advantage of the jamming. In the presence of Rayleigh fading and additive noise, the impact of forward error correction Bose Chaudhuri Hocquenghem type codes is systematically evaluated, demonstrating that even moderate redundancy significantly reduces packet loss. Extensive visualization of the learning dynamics, channel utilization distribution, epsilon greedy decay, cumulative reward, BER and SNR evolution, and detailed packet loss tables confirms convergence to a near optimal jamming strategy. The results provide a practical framework for autonomous resilient communications in modern electronic warfare scenarios."}
{"id": "2601.07775", "categories": ["cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.07775", "abs": "https://arxiv.org/abs/2601.07775", "authors": ["Sarvin Bahmani", "Rasmus Ibsen-Jensen", "Soumyajit Paul", "Sven Schewe", "Friedrich Slivovsky", "Qiyi Tang", "Dominik Wojtczak", "Shufang Zhu"], "title": "The Complexity of Games with Randomised Control", "comment": "28 pages including appendices, accepted to FoSSaCS 2026", "summary": "We study the complexity of solving two-player infinite duration games played on a fixed finite graph, where the control of a node is not predetermined but rather assigned randomly. In classic random-turn games, control of each node is assigned randomly every time the node is visited during a play. In this work, we study two natural variants of this where control of each node is assigned only once: (i) control is assigned randomly during a play when a node is visited for the first time and does not change for the rest of the play and (ii) control is assigned a priori before the game starts for every node by independent coin tosses and then the game is played. We investigate the complexity of computing the winning probability with three kinds of objectives-reachability, parity, and energy. We show that the qualitative questions on all variants and all objectives are NL-complete. For the quantitative questions, we show that deciding whether the maximiser can win with probability at least a given threshold for every objective is PSPACE-complete under the first mechanism, and that computing the exact winning probability for every objective is sharp-P-complete under the second. To complement our hardness results for the second mechanism, we propose randomised approximation schemes that efficiently estimate the winning probability for all three objectives, assuming a bounded number of parity colours and unary-encoded weights for energy objectives, and we empirically demonstrate their fast convergence."}
{"id": "2601.06947", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.06947", "abs": "https://arxiv.org/abs/2601.06947", "authors": ["Mateus de Oliveira Oliveira", "Wim Van den Broeck"], "title": "Optimal Extended Formulations from Optimal Dynamic Programming Algorithms", "comment": null, "summary": "Vertex Subset Problems (VSPs) are a class of combinatorial optimization problems on graphs where the goal is to find a subset of vertices satisfying a predefined condition. Two prominent approaches for solving VSPs are dynamic programming over tree-like structures, such as tree decompositions or clique decompositions, and linear programming. In this work, we establish a sharp connection between both approaches by showing that if a vertex-subset problem $Π$ admits a solution-preserving dynamic programming algorithm that produces tables of size at most $α(k,n)$ when processing a tree decomposition of width at most $k$ of an $n$-vertex graph $G$, then the polytope $P_Π(G)$ defined as the convex-hull of solutions of $Π$ in $G$ has extension complexity at most $O(α(k,n)\\cdot n)$. Additionally, this upper bound is optimal under the exponential time hypothesis (ETH).\n  On the one hand, our results imply that ETH-optimal solution-preserving dynamic programming algorithms for combinatorial problems yield optimal-size parameterized extended formulations for the solution polytopes associated with instances of these problems. On the other hand, unconditional lower bounds obtained in the realm of the theory of extended formulations yield unconditional lower bounds on the table complexity of solution-preserving dynamic programming algorithms."}
{"id": "2601.06798", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.06798", "abs": "https://arxiv.org/abs/2601.06798", "authors": ["Zhiyang Zhang", "Junda She", "Kuo Cai", "Bo Chen", "Shiyao Wang", "Xinchen Luo", "Qiang Luo", "Ruiming Tang", "Han Li", "Kun Gai", "Guorui Zhou"], "title": "Unleashing the Native Recommendation Potential: LLM-Based Generative Recommendation via Structured Term Identifiers", "comment": null, "summary": "Leveraging the vast open-world knowledge and understanding capabilities of Large Language Models (LLMs) to develop general-purpose, semantically-aware recommender systems has emerged as a pivotal research direction in generative recommendation. However, existing methods face bottlenecks in constructing item identifiers. Text-based methods introduce LLMs' vast output space, leading to hallucination, while methods based on Semantic IDs (SIDs) encounter a semantic gap between SIDs and LLMs' native vocabulary, requiring costly vocabulary expansion and alignment training. To address this, this paper introduces Term IDs (TIDs), defined as a set of semantically rich and standardized textual keywords, to serve as robust item identifiers. We propose GRLM, a novel framework centered on TIDs, employs Context-aware Term Generation to convert item's metadata into standardized TIDs and utilizes Integrative Instruction Fine-tuning to collaboratively optimize term internalization and sequential recommendation. Additionally, Elastic Identifier Grounding is designed for robust item mapping. Extensive experiments on real-world datasets demonstrate that GRLM significantly outperforms baselines across multiple scenarios, pointing a promising direction for generalizable and high-performance generative recommendation systems."}
{"id": "2601.06110", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06110", "abs": "https://arxiv.org/abs/2601.06110", "authors": ["Zewei Guo", "Ranran Sun", "Yulong Shen", "Xiaohong Jiang"], "title": "Optimal Beamforming for Uplink Covert Communication in MIMO GEO Satellite-Terrestrial Systems", "comment": null, "summary": "This paper investigates the uplink covert communication in a multiple-input multiple-output (MIMO) satellite-terrestrial system consisting of an Earth station transmitter Alice, a geosynchronous Earth orbit (GEO) satellite receiver Bob, and multiple GEO satellite wardens around Bob, where each node in the system is equipped with an array of directional antennas. Based on beamforming and the default antenna orientation setting, we first propose a scheme for covert Alice-Bob uplink transmission. Under the perfect channel estimation scenario, we provide theoretical modeling for the system performance in terms of detection error probability (DEP), transmission outage probability (TOP) and covert rate (CR), and then explore the optimal beamforming (OB) design as well as the joint optimal beamforming and antenna orientation (JO-BA) design for CR maximization. We then extend our study to the imperfect channel estimation scenario, and conduct related performance modeling and OB/JO-BA designs for CR maximization. We also apply the techniques of semidefinite relaxation, alternating optimization, Rodrigues' rotation formula and 1-D search algorithm to develop efficient algorithms to solve the above optimization problems. Finally, extensive numerical results are presented to verify our theoretical results and to illustrate the efficiency of beamforming and antenna orientation design for supporting the uplink covert communication in MIMO GEO satellite-terrestrial systems."}
{"id": "2601.07482", "categories": ["cs.DS", "cs.DM", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07482", "abs": "https://arxiv.org/abs/2601.07482", "authors": ["Helia Karisani", "Mohammadreza Daneshvaramoli", "Hedyeh Beyhaghi", "Mohammad Hajiesmaili", "Cameron Musco"], "title": "The Secretary Problem with Predictions and a Chosen Order", "comment": "Accepted to the International Conference on Innovations in Theoretical Computer Science (ITCS 2026)", "summary": "We study a learning-augmented variant of the secretary problem, recently introduced by Fujii and Yoshida (2023), in which the decision-maker has access to machine-learned predictions of candidate values. The central challenge is to balance consistency and robustness: when predictions are accurate, the algorithm should select a near-optimal secretary, while under inaccurate predictions it should still guarantee a bounded competitive ratio.\n  We consider both the classical Random Order Secretary Problem (ROSP), where candidates arrive in a uniformly random order, and a more natural learning-augmented model in which the decision-maker may choose the arrival order based on predicted values. We call this model the Chosen Order Secretary Problem (COSP), capturing scenarios such as interview schedules set in advance.\n  We propose a new randomized algorithm applicable to both ROSP and COSP. Our method switches from fully trusting predictions to a threshold-based rule once a large prediction deviation is detected. Let $ε\\in [0,1]$ denote the maximum multiplicative prediction error. For ROSP, our algorithm achieves a competitive ratio of $\\max\\{0.221, (1-ε)/(1+ε)\\}$, improving upon the prior bound of $\\max\\{0.215, (1-ε)/(1+ε)\\}$. For COSP, we achieve $\\max\\{0.262, (1-ε)/(1+ε)\\}$, surpassing the $0.25$ worst-case bound for prior approaches and moving closer to the classical secretary benchmark of $1/e \\approx 0.368$. These results highlight the benefit of combining predictions with arrival-order control in online decision-making."}
{"id": "2601.07482", "categories": ["cs.DS", "cs.DM", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07482", "abs": "https://arxiv.org/abs/2601.07482", "authors": ["Helia Karisani", "Mohammadreza Daneshvaramoli", "Hedyeh Beyhaghi", "Mohammad Hajiesmaili", "Cameron Musco"], "title": "The Secretary Problem with Predictions and a Chosen Order", "comment": "Accepted to the International Conference on Innovations in Theoretical Computer Science (ITCS 2026)", "summary": "We study a learning-augmented variant of the secretary problem, recently introduced by Fujii and Yoshida (2023), in which the decision-maker has access to machine-learned predictions of candidate values. The central challenge is to balance consistency and robustness: when predictions are accurate, the algorithm should select a near-optimal secretary, while under inaccurate predictions it should still guarantee a bounded competitive ratio.\n  We consider both the classical Random Order Secretary Problem (ROSP), where candidates arrive in a uniformly random order, and a more natural learning-augmented model in which the decision-maker may choose the arrival order based on predicted values. We call this model the Chosen Order Secretary Problem (COSP), capturing scenarios such as interview schedules set in advance.\n  We propose a new randomized algorithm applicable to both ROSP and COSP. Our method switches from fully trusting predictions to a threshold-based rule once a large prediction deviation is detected. Let $ε\\in [0,1]$ denote the maximum multiplicative prediction error. For ROSP, our algorithm achieves a competitive ratio of $\\max\\{0.221, (1-ε)/(1+ε)\\}$, improving upon the prior bound of $\\max\\{0.215, (1-ε)/(1+ε)\\}$. For COSP, we achieve $\\max\\{0.262, (1-ε)/(1+ε)\\}$, surpassing the $0.25$ worst-case bound for prior approaches and moving closer to the classical secretary benchmark of $1/e \\approx 0.368$. These results highlight the benefit of combining predictions with arrival-order control in online decision-making."}
{"id": "2601.06873", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06873", "abs": "https://arxiv.org/abs/2601.06873", "authors": ["Mustafa Abdool", "Soumyadip Banerjee", "Moutupsi Paul", "Do-kyum Kim", "Xioawei Liu", "Bin Xu", "Tracy Yu", "Hui Gao", "Karen Ouyang", "Huiji Gao", "Liwei He", "Stephanie Moyerman", "Sanjeev Katariya"], "title": "Applying Embedding-Based Retrieval to Airbnb Search", "comment": "14 pages, 9 figures", "summary": "The goal of Airbnb search is to match guests with the ideal accommodation that fits their travel needs. This is a challenging problem, as popular search locations can have around a hundred thousand available homes, and guests themselves have a wide variety of preferences. Furthermore, the launch of new product features, such as \\textit{flexible date search,} significantly increased the number of eligible homes per search query. As such, there is a need for a sophisticated retrieval system which can provide high-quality candidates with low latency in a way that integrates with the overall ranking stack.\n  This paper details our journey to build an efficient and high-quality retrieval system for Airbnb search. We describe the key unique challenges we encountered when implementing an Embedding-Based Retrieval (EBR) system for a two sided marketplace like Airbnb -- such as the dynamic nature of the inventory, a lengthy user funnel with multiple stages, and a variety of product surfaces. We cover unique insights when modeling the retrieval problem, how to build robust evaluation systems, and design choices for online serving. The EBR system was launched to production and powers several use-cases such as regular search, flexible date and promotional emails for marketing campaigns. The system demonstrated statistically-significant improvements in key metrics, such as booking conversion, via A/B testing."}
{"id": "2601.06120", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06120", "abs": "https://arxiv.org/abs/2601.06120", "authors": ["Tilo Strutz", "Roman Rischke"], "title": "Range-Coder with fast Adaptation and Table-Based Decoding", "comment": null, "summary": "The transmission or storage of signals typically involves data compression. The final processing step in compression systems is generally an entropy coding stage, which converts symbols into a bit stream based on their probability distribution. A distinct class of entropy coding methods operates not by mapping input symbols to discrete codewords but by operating on intervals or ranges. This approach enables a more accurate approximation of the source entropy, particularly for sources with highly skewed or varying symbol distributions. Representative techniques in this category include traditional arithmetic coding, range coding, and methods based on asymmetric numeral systems (ANS). The complexity of these methods depends mainly on three processing steps: the core routines of encoding and decoding doing the calculations, the interval-based determination of the correct symbol at decoder, and the efforts of keeping updated with respect to the varying symbol distribution.\n  The interval-based symbol determination at decoder typically demands for a searching procedure. In previous literature, it could be shown that the search can be replaced by a table-based approach with only O(1)-complexity but having the side-effect that the adaptation of the symbols statistic becomes infeasible because of the high time-consumption of adapting the table.\n  We propose an adaptation process using a ring-buffer technique enabling the adaptive table-based decoding procedure as well as the replacement of a division by a bit-shift operation at encoder and decoder core routines. This accelerates the coding process significantly. In static (non-adaptive) mode, the coding time can be reduced by about 40 percent. In adaptive mode, the proposed technique is faster than alternative approaches for alphabets from about 12 to 64 different symbol when comparing the overall encoder+decoder time."}
{"id": "2601.07566", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.07566", "abs": "https://arxiv.org/abs/2601.07566", "authors": ["Noam Benson-Tilsen"], "title": "Dynamic $(Δ+ 1)$ Vertex Coloring", "comment": "16 pages, 5 figures", "summary": "Several recent results from dynamic and sublinear graph coloring are surveyed. This problem is widely studied and has motivating applications like network topology control, constraint satisfaction, and real-time resource scheduling. Graph coloring algorithms are called colorers. In §1 are defined graph coloring, the dynamic model, and the notion of performance of graph algorithms in the dynamic model. In particular $(Δ+ 1)$-coloring, sublinear performance, and oblivious and adaptive adversaries are noted and motivated. In §2 the pair of approximately optimal dynamic vertex colorers given in arXiv:1708.09080 are summarized as a warmup for the $(Δ+ 1)$-colorers. In §3 the state of the art in dynamic $(Δ+ 1)$-coloring is presented. This section comprises a pair of papers (arXiv:1711.04355 and arXiv:1910.02063) that improve dynamic $(Δ+ 1)$-coloring from the naive algorithm with $O(Δ)$ expected amortized update time to $O(\\log Δ)$, then to $O(1)$ with high probability. In §4 the results in arXiv:2411.04418, which gives a sublinear algorithm for $(Δ+ 1)$-coloring that generalizes oblivious adversaries to adaptive adversaries, are presented."}
{"id": "2601.06992", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06992", "abs": "https://arxiv.org/abs/2601.06992", "authors": ["Yixi Zhou", "Fan Zhang", "Yu Chen", "Haipeng Zhang", "Preslav Nakov", "Zhuohan Xie"], "title": "FinCARDS: Card-Based Analyst Reranking for Financial Document Question Answering", "comment": "15 pages, including figures and tables", "summary": "Financial question answering (QA) over long corporate filings requires evidence to satisfy strict constraints on entities, financial metrics, fiscal periods, and numeric values. However, existing LLM-based rerankers primarily optimize semantic relevance, leading to unstable rankings and opaque decisions on long documents. We propose FinCards, a structured reranking framework that reframes financial evidence selection as constraint satisfaction under a finance-aware schema. FinCards represents filing chunks and questions using aligned schema fields (entities, metrics, periods, and numeric spans), enabling deterministic field-level matching. Evidence is selected via a multi-stage tournament reranking with stability-aware aggregation, producing auditable decision traces. Across two corporate filing QA benchmarks, FinCards substantially improves early-rank retrieval over both lexical and LLM-based reranking baselines, while reducing ranking variance, without requiring model fine-tuning or unpredictable inference budgets. Our code is available at https://github.com/XanderZhou2022/FINCARDS."}
{"id": "2601.06125", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06125", "abs": "https://arxiv.org/abs/2601.06125", "authors": ["Shengcai Zhou", "Luping Xiang", "Yi Wang", "Kun Yang", "Kai Kit Wong", "Chan-Byoung Chae"], "title": "Extended Target Adaptive Beamforming for ISAC:A Perspective of Predictive Error Ellipse", "comment": null, "summary": "Utilizing communication signals to extract motion parameters has emerged as a key direction in Vehicle-to- Everything (V2X) networks. Accurately modeling the relationship between communication signals and sensing performance is critical for the advancement of such systems. Unlike prior work that relies primarily on qualitative analysis, this paper derives the Cramér-Rao Bound (CRB) for radar parameter estimation in the context of Orthogonal Frequency Division Multiplexing (OFDM) waveforms and Uniform Planar Array (UPA) configurations. Recognizing that vehicles may act as extended targets, we propose two New Radio (NR)-V2X-compatible beamforming schemes tailored to different phases of the communication process. During the initial beam establishment phase, we develop a beamforming approach based on the union of predictive error ellipses, which enhances scatterer localization through temporally assisted beam training. In the beam adjustment phase, we introduce an adaptive narrowest-beam strategy that leverages the positions of scatterers and the communication receiver (CR), enabling effective tracking with reduced complexity. The beam design problem is addressed using the minimum enclosing ellipse algorithm and tailored antenna control methods. Simulation results validate the proposed approach, showing up to a 32.4% improvement in achievable rate with a 32*32 transmit antenna array and a 5.2% gain with an 8*8 array, compared to conventional beam sweeping under identical SNR conditions."}
{"id": "2601.07125", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07125", "abs": "https://arxiv.org/abs/2601.07125", "authors": ["Sungguk Cha", "DongWook Kim", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon", "Sangyeob Lee"], "title": "ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System", "comment": "5 pages", "summary": "Multi-vector embedding models have emerged as a powerful paradigm for document retrieval, preserving fine-grained visual and textual details through token-level representations. However, this expressiveness comes at a staggering cost: storing embeddings for every token inflates index sizes by over $1000\\times$ compared to single-vector approaches, severely limiting scalability. We introduce \\textbf{ReinPool}, a reinforcement learning framework that learns to dynamically filter and pool multi-vector embeddings into compact, retrieval-optimized representations. By training with an inverse retrieval objective and NDCG-based rewards, ReinPool identifies and retains only the most discriminative vectors without requiring manual importance annotations. On the Vidore V2 benchmark across three vision-language embedding models, ReinPool compresses multi-vector representations by $746$--$1249\\times$ into single vectors while recovering 76--81\\% of full multi-vector retrieval performance. Compared to static mean pooling baselines, ReinPool achieves 22--33\\% absolute NDCG@3 improvement, demonstrating that learned selection significantly outperforms heuristic aggregation."}
{"id": "2601.06156", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06156", "abs": "https://arxiv.org/abs/2601.06156", "authors": ["Ziyu Huang", "Yong Zeng", "Shen Fu", "Xiaoli Xu", "Hongyang Du"], "title": "Channel Knowledge Map Construction via Guided Flow Matching", "comment": null, "summary": "The efficient construction of accurate channel knowledge maps (CKMs) is crucial for unleashing the full potential of environment-aware wireless networks, yet it remains a difficult ill-posed problem due to the sparsity of available location-specific channel knowledge data. Although diffusion-based methods such as denoising diffusion probabilistic models (DDPMs) have been exploited for CKM construction, they rely on iterative stochastic sampling, rendering them too slow for real-time wireless applications. To bridge the gap between high fidelity and efficient CKM construction, this letter introduces a novel framework based on linear transport guided flow matching (LT-GFM). Deviating from the noise-removal paradigm of diffusion models, our approach models the CKM generation process as a deterministic ordinary differential equation (ODE) that follows linear optimal transport paths, thereby drastically reducing the number of required inference steps. We propose a unified architecture that is applicable to not only the conventional channel gain map (CGM) construction, but also the more challenging spatial correlation map (SCM) construction. To achieve physics-informed CKM constructions, we integrate environmental semantics (e.g., building masks) for edge recovery and enforce Hermitian symmetry for property of the SCM. Simulation results verify that LT-GFM achieves superior distributional fidelity with significantly lower Fréchet Inception Distance (FID) and accelerates inference speed by a factor of 25 compared to DDPMs."}
{"id": "2601.07294", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07294", "abs": "https://arxiv.org/abs/2601.07294", "authors": ["Wenhao Lai", "Weike Pan", "Zhong Ming"], "title": "Towards Multi-Behavior Multi-Task Recommendation via Behavior-informed Graph Embedding Learning", "comment": null, "summary": "Multi-behavior recommendation (MBR) aims to improve the performance w.r.t. the target behavior (i.e., purchase) by leveraging auxiliary behaviors (e.g., click, favourite). However, in real-world scenarios, a recommendation method often needs to process different types of behaviors and generate personalized lists for each task (i.e., each behavior type). Such a new recommendation problem is referred to as multi-behavior multi-task recommendation (MMR). So far, the most powerful MBR methods usually model multi-behavior interactions using a cascading graph paradigm. Although significant progress has been made in optimizing the performance of the target behavior, it often neglects the performance of auxiliary behaviors. To compensate for the deficiencies of the cascading paradigm, we propose a novel solution for MMR, i.e., behavior-informed graph embedding learning (BiGEL). Specifically, we first obtain a set of behavior-aware embeddings by using a cascading graph paradigm. Subsequently, we introduce three key modules to improve the performance of the model. The cascading gated feedback (CGF) module enables a feedback-driven optimization process by integrating feedback from the target behavior to refine the auxiliary behaviors preferences. The global context enhancement (GCE) module integrates the global context to maintain the user's overall preferences, preventing the loss of key preferences due to individual behavior graph modeling. Finally, the contrastive preference alignment (CPA) module addresses the potential changes in user preferences during the cascading process by aligning the preferences of the target behaviors with the global preferences through contrastive learning. Extensive experiments on two real-world datasets demonstrate the effectiveness of our BiGEL compared with ten very competitive methods."}
{"id": "2601.06211", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06211", "abs": "https://arxiv.org/abs/2601.06211", "authors": ["Sunwoo Kim", "Byonghyo Shim"], "title": "Large Multimodal Model-Aided Scheduling for 6G Autonomous Communications", "comment": "16 pages", "summary": "Recently, large language models (LLMs) have gained significant attention for their ability to generate fast and accurate answer to the given query. These models have evolved into large multimodal models (LMMs), which can interpret and analyze multimodal inputs such as images and text. With the exponential growth of AI functionalities in autonomous devices, the central unit (CU), a digital processing unit performing AI inference, needs to handle LMMs to effectively control these devices. To ensure seamless command delivery to devices, the CU must perform the scheduling, which involves resource block (RB) allocation for data transmission and modulation and coding scheme (MCS) index selection based on the channel conditions. This task is challenging in many practical environments in 6G, where even small user movement can cause abrupt channel changes. In this paper, we propose a novel LMM-based scheduling technique to address this challenge. Our key idea is to leverage LMM to predict future channel parameters (e.g., distance, angles, and path gain) by analyzing the visual sensing information as well as pilot signals. By exploiting LMMs to predict the presence of reliable path and geometric information of users from the visual sensing information, and then combining these with past channel states from pilot signals, we can accurately predict future channel parameters. Using these predictions, we can preemptively make channel-aware scheduling decisions. From the numerical evaluations, we show that the proposed technique achieves more than 30% throughput gain over the conventional scheduling techniques."}
{"id": "2601.07449", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07449", "abs": "https://arxiv.org/abs/2601.07449", "authors": ["Hao Jiang", "Zhi Yang", "Annan Wang", "Yichi Zhang", "Weisi Lin"], "title": "RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking", "comment": null, "summary": "Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback from the deluge of user-generated content. While large language models have improved semantic assessment, existing ranking paradigms face a persistent trade-off in long-context settings. Pointwise scoring is efficient but often fails to account for list-level interactions, leading to miscalibrated top-$k$ rankings. Listwise approaches can leverage global context, yet they are computationally expensive and become unstable as candidate lists grow. To address this, we propose Residual Listwise Preference Optimization (RLPO), which formulates ranking as listwise representation-level residual correction over a strong pointwise LLM scorer. RLPO first produces calibrated pointwise scores and item representations, then applies a lightweight encoder over the representations to predict listwise score residuals, avoiding full token-level listwise processing. We also introduce a large-scale benchmark for long-context review ranking with human verification. Experiments show RLPO improves NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases."}
{"id": "2601.06430", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.06430", "abs": "https://arxiv.org/abs/2601.06430", "authors": ["Ruotong Zhao", "Shaokang Hu", "Deepak Mishra", "Derrick Wing Kwan Ng"], "title": "Robust and Secure Blockage-Aware Pinching Antenna-assisted Wireless Communication", "comment": "This work has been submitted to IEEE TMC", "summary": "In this work, we investigate a blockage-aware pinching antenna (PA) system designed for secure and robust wireless communication. The considered system comprises a base station equipped with multiple waveguides, each hosting multiple PAs, and serves multiple single-antenna legitimate users in the presence of multi-antenna eavesdroppers under imperfect channel state information (CSI). To safeguard confidential transmissions, artificial noise (AN) is deliberately injected to degrade the eavesdropping channels. Recognizing that conventional linear CSI-error bounds become overly conservative for spatially distributed PA architectures, we develop new geometry-aware uncertainty sets that jointly characterize eavesdroppers position and array-orientation errors. Building upon these sets, we formulate a robust joint optimization problem that determines per-waveguide beamforming and AN covariance, individual PA power-ratio allocation, and PA positions to maximize the system sum rate subject to secrecy constraints. The highly non-convex design problem is efficiently addressed via a low computational complexity iterative algorithm that capitalizes on block coordinate descent, penalty-based methods, majorization-minimization, the S-procedure, and Lipschitz-based surrogate functions. Simulation results demonstrate that sum rates for the proposed algorithm outperforms conventional fixed antenna systems by 4.7 dB, offering substantially improved rate and secrecy performance. In particular, (i) adaptive PA positioning preserves LoS to legitimate users while effectively exploiting waveguide geometry to disrupt eavesdropper channels, and (ii) neglecting blockage effects in the PA system significantly impacts the system design, leading to performance degradation and inadequate secrecy guarantees."}
{"id": "2601.07533", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.07533", "abs": "https://arxiv.org/abs/2601.07533", "authors": ["Julian Schelb", "Michael Wittweiler", "Marie Revellio", "Barbara Feichtinger", "Andreas Spitz"], "title": "Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature", "comment": null, "summary": "Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs."}
{"id": "2601.06447", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06447", "abs": "https://arxiv.org/abs/2601.06447", "authors": ["Boris Ryabko"], "title": "Error correction methods based on two-faced processes", "comment": null, "summary": "A new approach to the problem of error correction in communication channels is proposed, in which the input sequence is transformed in such a way that the interdependence of symbols is significantly increased. Then, after the sequence is transmitted over the channel, this property is used for error correction so that the remaining error rate is significantly reduced. The complexity of encoding and decoding is linear."}
{"id": "2601.07613", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07613", "abs": "https://arxiv.org/abs/2601.07613", "authors": ["Ke Shenqiang", "Wei Jianxiong", "Hua Qingsong"], "title": "GAP-Net: Calibrating User Intent via Gated Adaptive Progressive Learning for CTR Prediction", "comment": "9 pages, 3 figures", "summary": "Sequential user behavior modeling is pivotal for Click-Through Rate (CTR) prediction yet is hindered by three intrinsic bottlenecks: (1) the \"Attention Sink\" phenomenon, where standard Softmax compels the model to allocate probability mass to noisy behaviors; (2) the Static Query Assumption, which overlooks dynamic shifts in user intent driven by real-time contexts; and (3) Rigid View Aggregation, which fails to adaptively weight heterogeneous temporal signals according to the decision context. To bridge these gaps, we propose GAP-Net (Gated Adaptive Progressive Network), a unified framework establishing a \"Triple Gating\" architecture to progressively refine information from micro-level features to macro-level views. GAP-Net operates through three integrated mechanisms: (1) Adaptive Sparse-Gated Attention (ASGA) employs micro-level gating to enforce sparsity, effectively suppressing massive noise activations; (2) Gated Cascading Query Calibration (GCQC) dynamically aligns user intent by bridging real-time triggers and long-term memories via a meso-level cascading channel; and (3) Context-Gated Denoising Fusion (CGDF) performs macro-level modulation to orchestrate the aggregation of multi-view sequences. Extensive experiments on industrial datasets demonstrate that GAP-Net achieves substantial improvements over state-of-the-art baselines, exhibiting superior robustness against interaction noise and intent drift."}
{"id": "2601.06450", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06450", "abs": "https://arxiv.org/abs/2601.06450", "authors": ["Charul Rajput", "B. Sundar Rajan", "Ragnar Freij-Hollanti", "Camilla Hollanti"], "title": "Function-Correcting Partition codes", "comment": null, "summary": "We introduce function-correcting partition codes (FCPCs) that are a natural generalization of function-correcting codes (FCCs). A $t$-error function-correcting partition code is an $(\\mathcal{P},t)$-encoding defined directly on a partition $\\mathcal{P}$ of $\\mathbb{F}_q^k$. For a partition $\\mathcal{P}=\\{P_1,P_2,\\ldots,P_E\\}$ a systematic mapping $\\mathcal{C}_{\\mathcal{P}} : \\mathbb{F}_q^k \\rightarrow \\mathbb{F}_q^{k+r}$ is called a \\emph{$(\\mathcal{P},t)$-encoding} if for all $u\\in P_i$ and $v\\in P_j$ with $i\\neq j$, $d\\big(\\mathcal{C}_{\\mathcal{P}}(u), \\mathcal{C}_{\\mathcal{P}}(v)\\big)\\ge 2t+1.$ We show that any $t$-error correcting code for a function $f$, denoted by $(f,t)$-FCC is exactly an FCPC with respect to the domain partition induced by $f$, which makes these codes a natural generalization of FCCs. We use the join of domain partitions to construct a single code that protects multiple functions simultaneously. We define the notion of partition redundancy gain and partition rate gain to measure the bandwidth saved by using a single FCPC for multiple functions instead of constructing separate FCCs for each function. We specialize this to linear functions via coset partition of the intersection of their kernels. Then, we associate a partition graph to any given partition of $\\mathbb{F}_q^k$, and show that the existence of a suitable clique in this graph yields a set of representative information vectors that achieves the optimal redundancy. We showed the existence of a full-size clique in the partition graphs of weight partition and support partition. Finally, we introduce the notion of a block-preserving contraction for a partition, which helps reduce the problem of finding optimal redundancy for an FCPC. We observe that FCPCs naturally provide a form of partial privacy, in the sense that only the domain partition of the function needs to be revealed to the transmitter."}
{"id": "2601.07684", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07684", "abs": "https://arxiv.org/abs/2601.07684", "authors": ["Geoffrey Taghon"], "title": "AptaFind: A lightweight local interface for automated aptamer curation from scientific literature", "comment": "for associated source code, see https://github.com/usnistgov/aptafind", "summary": "Aptamer researchers face a literature landscape scattered across publications, supplements, and databases, with each search consuming hours that could be spent at the bench. AptaFind transforms this navigation problem through a three-tier intelligence architecture that recognizes research mining is a spectrum, not a binary success or failure. The system delivers direct sequence extraction when possible, curated research leads when extraction fails, and exhaustive literature discovery for additional confidence. By combining local language models for semantic understanding with deterministic algorithms for reliability, AptaFind operates without cloud dependencies or subscription barriers. Validation across 300 University of Texas Aptamer Database targets demonstrates 84 % with some literature found, 84 % with curated research leads, and 79 % with a direct sequence extraction, at a laptop-compute rate of over 900 targets an hour. The platform proves that even when direct sequence extraction fails, automation can still deliver the actionable intelligence researchers need by rapidly narrowing the search to high quality references."}
{"id": "2601.06492", "categories": ["cs.IT", "math.OC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.06492", "abs": "https://arxiv.org/abs/2601.06492", "authors": ["Chun-Neng Chu", "Wei-Fu Tseng", "Yen-Huan Li"], "title": "Algorithms for Computing the Petz-Augustin Capacity", "comment": null, "summary": "We propose the first algorithms with non-asymptotic convergence guarantees for computing the Petz-Augustin capacity, which generalizes the channel capacity and characterizes the optimal error exponent in classical-quantum channel coding. This capacity can be equivalently expressed as the maximization of two generalizations of mutual information: the Petz-Rényi information and the Petz-Augustin information. To maximize the Petz-Rényi information, we show that it corresponds to a convex Hölder-smooth optimization problem, and hence the universal fast gradient method of Nesterov (2015), along with its convergence guarantees, readily applies. Regarding the maximization of the Petz-Augustin information, we adopt a two-layered approach: we show that the objective function is smooth relative to the negative Shannon entropy and can be efficiently optimized by entropic mirror descent; each iteration of entropic mirror descent requires computing the Petz-Augustin information, for which we propose a novel fixed-point algorithm and establish its contractivity with respect to the Thompson metric. Notably, this two-layered approach can be viewed as a generalization of the mirror-descent interpretation of the Blahut-Arimoto algorithm due to He et al. (2024)."}
{"id": "2601.07183", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07183", "abs": "https://arxiv.org/abs/2601.07183", "authors": ["Zehai Yang", "Shimin Chen"], "title": "RAIRS: Optimizing Redundant Assignment and List Layout for IVF-Based ANN Search", "comment": null, "summary": "IVF is one of the most widely used ANNS (Approximate Nearest Neighbors Search) methods in vector databases. The idea of redundant assignment is to assign a data vector to more than one IVF lists for reducing the chance of missing true neighbors in IVF search. However, the naive strategy, which selects the second IVF list based on the distance between a data vector and the list centroids, performs poorly. Previous work focuses only on the inner product distance, while there is no optimized list selection study for the most popular Euclidean space. Moreover, the IVF search may access the same vector in more than one lists, resulting in redundant distance computation and decreasing query throughput. In this paper, we present RAIRS to address the above two challenges. For the challenge of the list selection, we propose an optimized AIR metric for the Euclidean space. AIR takes not only distances but also directions into consideration in order to support queries that are closer to the data vector but father away from the first chosen list's centroid. For the challenge of redundant distance computation, we propose SEIL, an optimized list layout that exploits shared cells to reduce repeated distance computations for IVF search. Our experimental results using representative real-world data sets show that RAIRS out-performs existing redundant assignment solutions and achieves up to 1.33x improvement over the best-performing IVF method, IVF-PQ Fast Scan with refinement."}
{"id": "2601.06493", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.06493", "abs": "https://arxiv.org/abs/2601.06493", "authors": ["Han Li", "Xiang Wang", "Fang-Wei Fu"], "title": "On the Number of Subsequences in the Nonbinary Deletion Channel", "comment": null, "summary": "In the deletion channel, an important problem is to determine the number of subsequences derived from a string $U$ of length $n$ when subjected to $t$ deletions. It is well-known that the number of subsequences in the setting exhibits a strong dependence on the number of runs in the string $U$, where a run is defined as a maximal substring of identical characters. In this paper we study the number of subsequences of a non-binary string in this scenario, and propose some improved bounds on the number of subsequences of $r$-run non-binary strings. Specifically, we characterize a family of $r$-run non-binary strings with the maximum number of subsequences under any $t$ deletions, and show that this number can be computed in polynomial time."}
{"id": "2601.06501", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06501", "abs": "https://arxiv.org/abs/2601.06501", "authors": ["Yuhan Yang", "Haoheng Yuan", "Chao Qi", "Fan Cheng", "Bin Dai"], "title": "Coding for Fading Channels with Imperfect CSI at the Transmitter and Quantized Feedback", "comment": "16 pages, 9 figures", "summary": "The classical Schalkwijk-Kailath (SK) scheme for the additive Gaussian noise channel with noiseless feedback is highly efficient since its coding complexity is extremely low and the decoding error doubly exponentially decays as the coding blocklength tends to infinity. However, how to extend the SK scheme to channel models with memory has yet to be solved. In this paper, we first investigate how to design SK-type scheme for the 2-path quasi-static fading channel with noiseless feedback. By viewing the signal of the second path as a relay and adopting an amplify-and-forward (AF) relay strategy, we show that the interference path signal can help to enhance the transmission rate. Besides this, for arbitrary multi-path fading channel with feedback, we also present an SK-type scheme for such a model, which\n  transforms the time domain channel into a frequency domain MIMO channel."}
{"id": "2601.06503", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06503", "abs": "https://arxiv.org/abs/2601.06503", "authors": ["Xiang Wang", "Weijun Fang", "Han Li", "Fang-Wei Fu"], "title": "Some New Results on Sequence Reconstruction Problem for Deletion Channels", "comment": null, "summary": "Levenshtein first introduced the sequence reconstruction problem in $2001$. In the realm of combinatorics, the sequence reconstruction problem is equivalent to determining the value of $N(n,d,t)$, which represents the maximum size of the intersection of two metric balls of radius $t$, given that the distance between their centers is at least $d$ and the sequence length is $n$. In this paper, We present a lower bound on $N(n,3,t)$ for $n\\geq 13$ and $t \\geq 4$. For $t=4$, we prove that this lower bound is tight. This settles an open question posed by Pham, Goyal, and Kiah, confirming that $N(n,3,4)=20n-166$ for all $n \\geq 13$."}
{"id": "2601.06527", "categories": ["cs.IT", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.06527", "abs": "https://arxiv.org/abs/2601.06527", "authors": ["Wataru Uemura", "Shogo Kawasaki"], "title": "Visible Light Communication using Led-Based AR Markers for Robot Localization", "comment": null, "summary": "A method of information transmission using visual markers has been widely studied. In this approach, information or identifiers (IDs) are encoded in the black-and-white pattern of each marker. By analyzing the geometric properties of the marker frame - such as its size, distortion, and coordinates - the relative position and orientation between the camera and the marker can be estimated. Furthermore, by associating the positional information of each marker with its corresponding ID, the position of the camera that takes the image picture can be calculated. In the field of mobile robotics, such markers are commonly utilized for robot localization. As mobile robots become more widely used in everyday environments, such visual markers are expected to be utilized across various contexts. In environments where robots collaborate with humans - such as in cell-based manufacturing systems in factories or in domestic settings with partner robots - it is desirable for such markers to be designed in a manner that appears natural and unobtrusive to humans. In this paper, we propose a method for implementing an ArUco marker in the form of illumination. In the proposed method, LEDs are arranged in accordance with the grid pattern of the marker, and the blinking frequency of each LED is determined based on the corresponding black or white cell. As a result, the illumination appears uniformly bright to the human eye, while the camera can capture variations in the blinking frequency. From these differences, the black-and-white pattern can be reconstructed, enabling the identification of the marker's tag information. We develop a prototype system, and conduct experiments which are conducted to evaluate its performance in terms of recognition accuracy under varying distances and viewing angles with respect to the ArUco marker."}
{"id": "2601.06558", "categories": ["cs.IT", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06558", "abs": "https://arxiv.org/abs/2601.06558", "authors": ["Jiao Xu", "Peng Li", "Bing Zheng"], "title": "Hard Thresholding Pursuit Algorithms for Least Absolute Deviations Problem", "comment": null, "summary": "Least absolute deviations (LAD) is a statistical optimality criterion widely utilized in scenarios where a minority of measurements are contaminated by outliers of arbitrary magnitudes. In this paper, we delve into the robustness of the variant of adaptive iterative hard thresholding to outliers, known as graded fast hard thresholding pursuit (GFHTP$_1$) algorithm. Unlike the majority of the state-of-the-art algorithms in this field, GFHTP$_1$ does not require prior information about the signal's sparsity. Moreover, its design is parameterless, which not only simplifies the implementation process but also removes the intricacies of parameter optimization. Numerical experiments reveal that the GFHTP$_1$ algorithm consistently outperforms competing algorithms in terms of both robustness and computational efficiency."}
{"id": "2601.06588", "categories": ["cs.IT", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.06588", "abs": "https://arxiv.org/abs/2601.06588", "authors": ["Zijiu Yang", "Qianqian Yang", "Shunpu Tang", "Tingting Yang", "Zhiguo Shi"], "title": "TCLNet: A Hybrid Transformer-CNN Framework Leveraging Language Models as Lossless Compressors for CSI Feedback", "comment": null, "summary": "In frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems, downlink channel state information (CSI) plays a crucial role in achieving high spectrum and energy efficiency. However, the CSI feedback overhead becomes a major bottleneck as the number of antennas increases. Although existing deep learning-based CSI compression methods have shown great potential, they still face limitations in capturing both local and global features of CSI, thereby limiting achievable compression efficiency. To address these issues, we propose TCLNet, a unified CSI compression framework that integrates a hybrid Transformer-CNN architecture for lossy compression with a hybrid language model (LM) and factorized model (FM) design for lossless compression. The lossy module jointly exploits local features and global context, while the lossless module adaptively switches between context-aware coding and parallel coding to optimize the rate-distortion-complexity (RDC) trade-off. Extensive experiments on both real-world and simulated datasets demonstrate that the proposed TCLNet outperforms existing approaches in terms of reconstruction accuracy and transmission efficiency, achieving up to a 5 dB performance gain across diverse scenarios. Moreover, we show that large language models (LLMs) can be leveraged as zero-shot CSI lossless compressors via carefully designed prompts."}
{"id": "2601.06609", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06609", "abs": "https://arxiv.org/abs/2601.06609", "authors": ["Anup Kushwaha", "Om Prakash"], "title": "Symplectic Hulls over a Non-Unital Ring", "comment": "24", "summary": "This paper presents the study of the symplectic hulls over a non-unital ring $ E= \\langle κ,τ\\mid 2 κ=2 τ=0,~ κ^2=κ,~ τ^2=τ,~ κτ=κ,~ τκ=τ\\rangle$. We first identify the residue and torsion codes of the left, right, and two-sided symplectic hulls, and characterize the generator matrix of the two-sided symplectic hull of a free $E$-linear code. Then, we explore the symplectic hull of the sum of two free $E$-linear codes. Subsequently, we provide two build-up techniques that extend a free $E$-linear code of smaller length and symplectic hull-rank to one of larger length and symplectic hull-rank. Further, for free $E$-linear codes, we discuss the permutation equivalence and investigate the symplectic hull-variation problem. An application of this study is given by classifying the free $E$-linear optimal codes for smaller lengths."}
{"id": "2601.06688", "categories": ["cs.IT", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.06688", "abs": "https://arxiv.org/abs/2601.06688", "authors": ["Terence Viaud", "Ioannis Kontoyiannis"], "title": "The Sample Complexity of Lossless Data Compression", "comment": null, "summary": "A new framework is introduced for examining and evaluating the fundamental limits of lossless data compression, that emphasizes genuinely non-asymptotic results. The {\\em sample complexity} of compressing a given source is defined as the smallest blocklength at which it is possible to compress that source at a specified rate and to within a specified excess-rate probability. This formulation parallels corresponding developments in statistics and computer science, and it facilitates the use of existing results on the sample complexity of various hypothesis testing problems. For arbitrary sources, the sample complexity of general variable-length compressors is shown to be tightly coupled with the sample complexity of prefix-free codes and fixed-length codes. For memoryless sources, it is shown that the sample complexity is characterized not by the source entropy, but by its Rényi entropy of order~$1/2$. Nonasymptotic bounds on the sample complexity are obtained, with explicit constants. Generalizations to Markov sources are established, showing that the sample complexity is determined by the source's Rényi entropy rate of order~$1/2$. Finally, bounds on the sample complexity of universal data compression are developed for arbitrary families of memoryless sources. There, the sample complexity is characterized by the minimum Rényi divergence of order~$1/2$ between elements of the family and the uniform distribution. The connection of this problem with identity testing and with the associated separation rates is explored and discussed."}
{"id": "2601.06732", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06732", "abs": "https://arxiv.org/abs/2601.06732", "authors": ["Hassan Touati", "Rodrigo C. de Lamare"], "title": "Study of Adaptive Reliability-Driven Conditional Innovation Decoding for LDPC Codes", "comment": "12 pages, 7 figures", "summary": "In this work, we present an adaptive reliability-driven conditional innovation (AR-CID) decoding algorithm for low-density parity check (LDPC) codes. The proposed AR-CID decoding algorithm consists of one stage of message quality checking and another stage of message passing refinement, which are incorporated into a residual belief propagation decoding strategy. An analysis of the AR-CID decoding algorithm is carried out along with a study of its computational complexity and latency characteristics. Simulation results for several examples of LDPC codes, including short and medium-length codes over an extended range of channel conditions, indicate that the proposed AR-CID decoding algorithm outperforms competing decoding techniques and has an extremely fast convergence, making it particularly suitable for low-delay applications."}
{"id": "2601.06836", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06836", "abs": "https://arxiv.org/abs/2601.06836", "authors": ["Zhou Li", "Xiang Zhang", "Kai Wan", "Hua Sun", "Mingyue Ji", "Giuseppe Caire"], "title": "Optimal Rate Region for Multi-server Secure Aggregation with User Collusion", "comment": "29 pages, 1 figures", "summary": "Secure aggregation is a fundamental primitive in privacy-preserving distributed learning systems, where an aggregator aims to compute the sum of users' inputs without revealing individual data. In this paper, we study a multi-server secure aggregation problem in a two-hop network consisting of multiple aggregation servers and multiple users per server, under the presence of user collusion. Each user communicates only with its associated server, while the servers exchange messages to jointly recover the global sum. We adopt an information-theoretic security framework, allowing up to $T$ users to collude with any server.\n  We characterize the complete optimal rate region in terms of user-to-server communication rate, server-to-server communication rate, individual key rate, and source key rate. Our main result shows that the minimum communication and individual key rates are all one symbol per input symbol, while the optimal source key rate is given by $\\min\\{U+V+T-2,\\, UV-1\\}$, where $U$ denotes the number of servers and $V$ the number of users per server. The achievability is established via a linear key construction that ensures correctness and security against colluding users, while the converse proof relies on tight entropy bounds derived from correctness and security constraints.\n  The results reveal a fundamental tradeoff between security and key efficiency and demonstrate that the multi-server architecture can significantly reduce the required key randomness compared to single-server secure aggregation. Our findings provide a complete information-theoretic characterization of secure aggregation in multi-server systems with user collusion."}
{"id": "2601.06906", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.06906", "abs": "https://arxiv.org/abs/2601.06906", "authors": ["Chong Huang", "Gaojie Chen", "Pei Xiao", "Zhu Han", "Rahim Tafazolli"], "title": "Large Artificial Intelligence Models for Future Wireless Communications", "comment": "8 Pages", "summary": "The anticipated integration of large artificial intelligence (AI) models with wireless communications is estimated to usher a transformative wave in the forthcoming information age. As wireless networks grow in complexity, the traditional methodologies employed for optimization and management face increasingly challenges. Large AI models have extensive parameter spaces and enhanced learning capabilities and can offer innovative solutions to these challenges. They are also capable of learning, adapting and optimizing in real-time. We introduce the potential and challenges of integrating large AI models into wireless communications, highlighting existing AIdriven applications and inherent challenges for future large AI models. In this paper, we propose the architecture of large AI models for future wireless communications, introduce their advantages in data analysis, resource allocation and real-time adaptation, discuss the potential challenges and corresponding solutions of energy, architecture design, privacy, security, ethical and regulatory. In addition, we explore the potential future directions of large AI models in wireless communications, laying the groundwork for forthcoming research in this area."}
{"id": "2601.06925", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.06925", "abs": "https://arxiv.org/abs/2601.06925", "authors": ["Hui Zhao", "Dirk Slock", "Petros Elia"], "title": "Caching Yields up to 5x Spectral Efficiency in Multi-Beam Satellite Communications", "comment": "11 pages, 6 figures", "summary": "This paper examines the integration of vector coded caching (VCC) into multi-beam satellite communications (SATCOM) systems and demonstrates that even limited receiver-side caching can substantially enhance spectral efficiency. By leveraging cached content to suppress interference, VCC enables the concurrent transmission of multiple precoded signal vectors that would otherwise require separate transmission resources. This leads to a multiplicative improvement in resource utilization in SATCOM. To characterize this performance, we model the satellite-to-ground channel using Rician-shadowed fading and after incorporating practical considerations such as matched-filter precoding, channel state information (CSI) acquisition overhead as well as CSI imperfections at the transmitter, we here derive closed-form expressions for the average sum rate and spectral efficiency gain of VCC in SATCOM. Our analysis, tightly validated through numerical simulations, reveals that VCC can yield spectral efficiency gains of 300% to 550% over traditional multi-user MISO SATCOM with the same resources. These gains -- which have nothing to do with multicasting, prefetching gains nor file popularity -- highlight VCC as a pure physical-layer solution for future high-throughput SATCOM systems, significantly narrowing the performance gap between satellite and wired networks."}
{"id": "2601.06969", "categories": ["cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06969", "abs": "https://arxiv.org/abs/2601.06969", "authors": ["Qinshan Zhang", "Bin Chen", "Yong Jiang", "Shu-Tao Xia"], "title": "Generalization Bounds for Transformer Channel Decoders", "comment": "18 pages, 3 figures", "summary": "Transformer channel decoders, such as the Error Correction Code Transformer (ECCT), have shown strong empirical performance in channel decoding, yet their generalization behavior remains theoretically unclear. This paper studies the generalization performance of ECCT from a learning-theoretic perspective. By establishing a connection between multiplicative noise estimation errors and bit-error-rate (BER), we derive an upper bound on the generalization gap via bit-wise Rademacher complexity. The resulting bound characterizes the dependence on code length, model parameters, and training set size, and applies to both single-layer and multi-layer ECCTs. We further show that parity-check-based masked attention induces sparsity that reduces the covering number, leading to a tighter generalization bound. To the best of our knowledge, this work provides the first theoretical generalization guarantees for this class of decoders."}
{"id": "2601.07034", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07034", "abs": "https://arxiv.org/abs/2601.07034", "authors": ["Ioannis Krikidis"], "title": "Quantum Optical Integrated Sensing and Communication with Homodyne BPSK Detection", "comment": "IEEE Wireless Communications Letters, 2026", "summary": "In this letter, we propose a quantum integrated sensing and communication scheme for a quantum optical link using binary phase-shift keying modulation and homodyne detection. The link operates over a phase-insensitive Gaussian channel with an unknown deterministic phase rotation, where the homodyne receiver jointly carries out symbol detection and phase estimation. We formulate a design problem that minimizes the bit-error rate subject to a Fisher information-based constraint on estimation accuracy. To solve it, we develop an iterative algorithm composed of an inner expectation-maximization loop for joint detection and estimation and an outer loop that adaptively retunes the local oscillator phase. Numerical results confirm the effectiveness of the proposed approach and demonstrate a fundamental trade-off between communication reliability and sensing accuracy."}
{"id": "2601.07053", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07053", "abs": "https://arxiv.org/abs/2601.07053", "authors": ["Chen Wang", "Eitan Yaakobi"], "title": "Random Access in DNA Storage: Algorithms, Constructions, and Bounds", "comment": null, "summary": "As DNA data storage moves closer to practical deployment, minimizing sequencing coverage depth is essential to reduce both operational costs and retrieval latency. This paper addresses the recently studied Random Access Problem, which evaluates the expected number of read samples required to recover a specific information strand from $n$ encoded strands. We propose a novel algorithm to compute the exact expected number of reads, achieving a computational complexity of $O(n)$ for fixed field size $q$ and information length $k$. Furthermore, we derive explicit formulas for the average and maximum expected number of reads, enabling an efficient search for optimal generator matrices under small parameters. Beyond theoretical analysis, we present new code constructions that improve the best-known upper bound from $0.8815k$ to $0.8811k$ for $k=3$, and achieve an upper bound of $0.8629k$ for $k=4$ for sufficiently large $q$. We also establish a tighter theoretical lower bound on the expected number of reads that improves upon state-of-the-art bounds. In particular, this bound establishes the optimality of the simple parity code for the case of $n=k+1$ across any alphabet $q$."}
{"id": "2601.07095", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07095", "abs": "https://arxiv.org/abs/2601.07095", "authors": ["Tadashi Wadayama", "Takumi Takahashi"], "title": "Score-Based VAMP with Fisher-Information-Based Onsager Correction", "comment": null, "summary": "We propose score-based VAMP (SC-VAMP), a variant of vector approximate message passing (VAMP) in which the Onsager correction is expressed and computed via conditional Fisher information, thereby enabling a Jacobian-free implementation. Using learned score functions, SC-VAMP constructs nonlinear MMSE estimators through Tweedie's formula and derives the corresponding Onsager terms from the score-norm statistics, avoiding the need for analytical derivatives of the prior or likelihood. When combined with random orthogonal/unitary mixing to mitigate non-ideal, structured or correlated sensing settings, the proposed framework extends VAMP to complex black-box inference problems where explicit modeling is intractable. Finally, by leveraging the entropic CLT, we provide an information-theoretic perspective on the Gaussian approximation underlying SE, offering insight into the decoupling principle beyond idealized i.i.d. settings, including nonlinear regimes."}
{"id": "2601.07147", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.07147", "abs": "https://arxiv.org/abs/2601.07147", "authors": ["Ji He"], "title": "PASS-Enabled Covert Communications With Distributed Cooperative Wardens", "comment": null, "summary": "This paper investigates PASS-enabled downlink covert communication in the presence of distributed surveillance, where multiple wardens perform signal detection and fuse their local binary decisions via majority-voting rule. We consider a dual-waveguide architecture that simultaneously delivers covert information and randomized jamming to hide the transmission footprint, incorporating three representative PASS power-radiation laws-general, proportional, and equal. To characterize the system-level detectability, we derive closed-form expressions for local false-alarm and miss-detection probabilities. By leveraging a probability-generating-function (PGF) and elementary-symmetric-polynomial (ESP) framework, combined with a breakpoint-based partition of the threshold domain, we obtain explicit closed-form characterizations of the system-level detection error probability (DEP) under non-i.i.d. majority-voting fusion. Building on this analytical framework, we formulate a robust optimization problem to maximize the average covert rate subject to covertness constraint. To solve the resulting nonconvex design, we develop an MM-BCD-SCA algorithm that produces tractable alternating updates for power/radiation variables and PA positions via convex surrogates and inner approximations of the DEP value function. Numerical results validate the theoretical analysis and demonstrate the impact of cooperative monitoring and PASS radiation laws on the covertness-rate tradeoff."}
{"id": "2601.07235", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07235", "abs": "https://arxiv.org/abs/2601.07235", "authors": ["Agnivo Gosai", "Shuvodeep De", "Karun Thankachan"], "title": "Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges", "comment": "31 Pages; 1 figure; 108 references; ongoing paper that would be submitted to suitable Wiley journal", "summary": "This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data."}
{"id": "2601.07240", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07240", "abs": "https://arxiv.org/abs/2601.07240", "authors": ["Mohammad Rowshan"], "title": "Bias-Aware BP Decoding of Quantum Codes via Directional Degeneracy", "comment": null, "summary": "We study directionally informed belief propagation (BP) decoding for quantum CSS codes, where anisotropic Tanner-graph structure and biased noise concentrate degeneracy along preferred directions. We formalize this by placing orientation weights on Tanner-graph edges, aggregating them into per-qubit directional weights, and defining a \\emph{directional degeneracy enumerator} that summarizes how degeneracy concentrates along those directions. A single bias parameter~$β$ maps these weights into site-dependent log-likelihood ratios (LLRs), yielding anisotropic priors that plug directly into standard BP$\\rightarrow$OSD decoders without changing the code construction. We derive bounds relating directional and Hamming distances, upper bound the number of degenerate error classes per syndrome as a function of distance, rate, and directional bias, and give a MacWilliams-type expression for the directional enumerator. Finite-length simulations under code-capacity noise show significant logical error-rate reductions -- often an order of magnitude at moderate physical error rates -- confirming that modest anisotropy is a simple and effective route to hardware-aware decoding gains."}
{"id": "2601.07246", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07246", "abs": "https://arxiv.org/abs/2601.07246", "authors": ["Jiayang Zou", "Luyao Fan", "Jiayang Gao", "Jia Wang"], "title": "Rate-distortion Theory on Non-compact Spaces: A Concentration-compactness Approach", "comment": null, "summary": "In this paper, we study rate-distortion theory for general sources with an emphasis on the existence of optimal reconstruction distributions. Classical existence results rely on compactness assumptions that are often violated in non-compact settings. By introducing the concentration-compactness principle into the analysis of the rate-distortion functional, we establish the existence of optimal reconstructions under mild coercivity conditions on the distortion function. Our results provide a unified and transparent existence theorem for rate-distortion problems on general non-compact spaces."}
{"id": "2601.07317", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07317", "abs": "https://arxiv.org/abs/2601.07317", "authors": ["Yuxuan Chen", "Qingqing Wu", "Guangji Chen", "Qiaoyan Peng", "Wen Chen"], "title": "Engineering Favorable Propagation: Near-Field IRS Deployment for Spatial Multiplexing", "comment": null, "summary": "In intelligent reflecting surface IRS assisted multiple input multiple output MIMO systems, a strong line of sight LoS link is required to compensate for the severe cascaded path loss. However, such a link renders the effective channel highly rank deficient and fundamentally limits spatial multiplexing. To overcome this limitation, this paper leverages the large aperture of sparse arrays to harness near field spherical wavefronts, and establishes a deterministic deployment criterion that strategically positions the IRS in the near field of a base station BS. This placement exploits the spherical wavefronts of the BS IRS link to engineer decorrelated channels, thereby fundamentally overcoming the rank deficiency issue in far field cascaded channels. Based on a physical channel model for the sparse BS array and the IRS, we characterize the rank properties and inter user correlation of the cascaded BS IRS user channel. We further derive a closed form favorable propagation metric that reveals how the sparse array geometry and the IRS position can be tuned to reduce inter user channel correlation. The resulting geometry driven deployment rule provides a simple guideline for creating a favorable propagation environment with enhanced effective degrees of freedom. The favorable channel statistics induced by our deployment criterion enable a low complexity maximum ratio transmission MRT precoding scheme. This serves as the foundation for an efficient algorithm that jointly optimizes the IRS phase shifts and power allocation based solely on long term statistical channel state information CSI. Simulation results validate the effectiveness of our deployment criterion and demonstrate that our optimization framework achieves significant performance gains over benchmark schemes."}
{"id": "2601.07322", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07322", "abs": "https://arxiv.org/abs/2601.07322", "authors": ["Jinnan Piao", "Dong Li", "Zhibo Li", "Ming Yang", "Xueting Yu", "Jincheng Dai"], "title": "Performance Bounds of Joint Detection with Kalman Filtering and Channel Decoding for Wireless Networked Control Systems", "comment": null, "summary": "The joint detection uses Kalman filtering (KF) to estimate the prior probability of control outputs to assist channel decoding. In this paper, we regard the joint detection as maximum a posteriori (MAP) decoding and derive the lower and upper bounds based on the pairwise error probability considering system interference, quantization interval, and weight distribution. We first derive the limiting bounds as the signal-to-noise ratio (SNR) goes to infinity and the system interference goes to zero. Then, we construct an infinite-state Markov chain to describe the consecutive packet losses of the control systems to derive the MAP bounds. Finally, the MAP bounds are approximated as the bounds of the transition probability from the state with no packet loss to the state with consecutive single packet loss. The simulation results show that the MAP performance of $\\left(64,16\\right)$ polar code and 16-bit CRC coincides with the limiting upper bound as the SNR increases and has $3.0$dB performance gain compared with the normal approximation of the finite block rate at block error rate $10^{-3}$."}
{"id": "2601.07340", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07340", "abs": "https://arxiv.org/abs/2601.07340", "authors": ["Zhou Li"], "title": "On the Extremal Source Key Rates for Secure Storage over Graphs", "comment": "13 pages, 7 figures", "summary": "This paper investigates secure storage codes over graphs, where multiple independent source symbols are encoded and stored at graph nodes subject to edge-wise correctness and security constraints. For each edge, a specified subset of source symbols must be recoverable from its two incident nodes, while no information about the remaining sources is revealed. To meet the security requirement, a shared source key may be employed. The ratio between the source symbol size and the source key size defines the source key rate, and the supremum of all achievable rates is referred to as the source key capacity.\n  We study extremal values of the source key capacity in secure storage systems and provide complete graph characterizations for several fundamental settings. For the case where each edge is associated with a single source symbol, we characterize all graphs whose source key capacity equals one. We then generalize this result to the case where each edge is associated with multiple source symbols and identify a broad class of graphs that achieve the corresponding extremal capacity under a mild structural condition. In addition, we characterize all graphs for which secure storage can be achieved without using any source key."}
{"id": "2601.07355", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07355", "abs": "https://arxiv.org/abs/2601.07355", "authors": ["Yichen Fu", "Tianming Wang", "Ke Wei"], "title": "Fast and Provable Nonconvex Robust Matrix Completion", "comment": null, "summary": "This paper studies the robust matrix completion problem and a computationally efficient non-convex method called ARMC has been proposed. This method is developed by introducing subspace projection to a singular value thresholding based method when updating the low rank part. Numerical experiments on synthetic and real data show that ARMC is superior to existing non-convex RMC methods. Through a refined analysis based on the leave-one-out technique, we have established the theoretical guarantee for ARMC subject to both sparse outliers and stochastic noise. The established bounds for the sample complexity and outlier sparsity are better than those established for a convex approach that also considers both outliers and stochastic noise."}
{"id": "2601.07388", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.07388", "abs": "https://arxiv.org/abs/2601.07388", "authors": ["Manuel Franco-Vivo"], "title": "Novel Decoding Algorithm for Noiseless Non-Adaptive Group Testing", "comment": null, "summary": "Group testing enables the identification of a small subset of defective items within a larger population by performing tests on pools of items rather than on each item individually. Over the years, it has not only attracted attention from the academic community, but has also demonstrated its potential in addressing real-world problems such as infectious disease screening, drug discovery and manufacturing quality control. With the emergence of the COVID-19 pandemic, interest in group testing has grown further, particularly in non-adaptive testing, due to its time efficiency compared to adaptive approaches. This highlights the importance of improving the performance currently achievable in such a scheme. This article focuses on advancing the field of noiseless non-adaptive group testing. The main objective of this work is to study and maximize the probability of successfully identifying the subset of defective items while performing as few tests as possible. To this end, we first note current well-known decoding algorithms, as well as established test design strategies for assigning items to pools. From this review, we identify key opportunities for improvement that inform the development of new decoding algorithms. Specifically, we propose a novel method, Weighted Sequential Combinatorial Orthogonal Matching Pursuit (W-SCOMP), to enhance the efficiency of existing detection procedures. Theoretical results demonstrate that W-SCOMP outperforms other algorithms in noiseless non-adaptive group testing. Furthermore, we develop a simulation framework to model the group testing process and conduct comparative evaluations between the proposed and existing algorithms. The empirical results are consistent with the theoretical findings. Overall, our work expands the range of available decoding algorithms and contributes to the broader understanding of noiseless non-adaptive group testing."}
{"id": "2601.07424", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07424", "abs": "https://arxiv.org/abs/2601.07424", "authors": ["Xu Gan", "Yuanwei Liu"], "title": "Center-Fed Pinching Antenna System (C-PASS) Aided Wireless Communications", "comment": null, "summary": "The novel architecture of the center-fed pinching antenna system (C-PASS) is investigated, where the waveguide-fed signal is divided into two propagation directions through controllable power splitting. By doing so, a doubled degree of freedom (DoF) is achieved compared to conventional PASS. Based on the new designed basic signal model of C-PASS, three practical operating protocols for C-PASS are proposed, namely power splitting (PS), direction switching (DS), and time switching (TS). Then, the sum-rate maximization problem for the joint optimization of transmit and pinching beamforming is formulated for each of the proposed protocols. 1) For PS, the highly coupled non-convex problem is first transformed into a tractable form via the weighted minimum mean square error reformulation and solved using the alternating optimization framework; 2) For DS, the above approach is subsequently extended to solve the mixed-integer constraints inherent for DS via the penalty-based algorithm; 3) For TS, the optimization problem can be decomposed into two subproblems and solved using the similar iterative techniques, while its optimal time allocation ratio is derived in closed form. Finally, numerical results reveal that TS is superior in the low-power regime, while PS and DS achieve significantly higher rates in the high-power regime due to the enhanced DoF."}
{"id": "2601.07472", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07472", "abs": "https://arxiv.org/abs/2601.07472", "authors": ["Sheng Su", "Yuhan Yang", "Chao Qi", "Xuan He", "Bin Dai", "Xiaohu Tang"], "title": "Secure Joint Source-Channel Coding for the AWGN Channel with Feedback: A Finite Blocklength Analysis", "comment": null, "summary": "In the literature, it has been shown that the secrecy capacity of the additive white Gaussian noise (AWGN) wiretap channel with noise-free feedback equals the capacity of the same model without secrecy constraint, and the classical Schalkwijk-Kailath (SK) scheme achieves the secrecy capacity. In this paper, we show that in finite blocklength regime, the SK scheme is not optimal, and propose a modified SK scheme which may perform better than the classical one. Besides this, this paper establishes a finite blocklength converse for the AWGN wiretap channel with feedback, which can also be viewed as a converse for the same model without secrecy constraint. To the best of the authors' knowledge, this is the first paper to address such a problem, and the results of this paper are further explained via numerical examples."}
{"id": "2601.07489", "categories": ["cs.IT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.07489", "abs": "https://arxiv.org/abs/2601.07489", "authors": ["Emiel Vanspranghels", "Zhuangzhuang Cui", "Sofie Pollin"], "title": "Frequency-Adaptive Multi-Band Architecture for Upper Mid-Band MIMO Systems", "comment": "5 pages, 5 figures, submitted to DySPAN 2026", "summary": "FR3 ($\\approx$7-24 GHz), also referred to as the upper mid-band, has recently emerged as promising spectrum for 6G; however, its propagation and MIMO characteristics vary significantly with frequency and environment, and spectrum availability may be intermittent due to incumbents. Using site-specific ray tracing (Sionna RT) in representative indoor and outdoor scenarios, we evaluate 7, 10, 14, 20, and 24 GHz under SISO and MIMO configurations. The results show that FR3 exhibits propagation characteristics intermediate between sub-6 GHz and mmWave bands while supporting meaningful spatial multiplexing, albeit with strong site dependence. Motivated by these findings, we propose a fully digital frequency-adaptive multi-band MIMO architecture that repurposes ADCs/DACs and baseband processing resources across FR3 subbands via switching, enabling dynamic trade-offs between bandwidth (spectrum gain) and antenna consolidation (MIMO gain) under availability and channel constraints. Simulation results demonstrate that exploiting additional spectrum is often optimal, while adaptive resource repurposing becomes beneficial when subbands are unavailable or when multiplexing gains are concentrated at specific frequencies."}
{"id": "2601.07515", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07515", "abs": "https://arxiv.org/abs/2601.07515", "authors": ["Yang Liu", "Bolin Wu", "Yuxin Han", "Kai Niu"], "title": "A Parity-Consistent Decomposition Method for the Weight Distribution of Pre-Transformed Polar Codes", "comment": null, "summary": "This paper introduces an efficient algorithm based on the Parity-Consistent Decomposition (PCD) method to determine the WD of pre-transformed polar codes. First, to address the bit dependencies introduced by the pre-transformation matrix, we propose an iterative algorithm to construct an \\emph{Expanded Information Set}. By expanding the information bits within this set into 0s and 1s, we eliminate the correlations among information bits, thereby enabling the recursive calculation of the Hamming weight distribution using the \\emph{PCD method}. Second, to further reduce computational complexity, we establish the theory of equivalence classes for pre-transformed polar codes. Codes within the same equivalence class share an identical weight distribution but correspond to different \\emph{Expanded Information Set} sizes. By selecting the pre-transformation matrix that minimizes the \\emph{Expanded Information Set} size within an equivalence class, we optimize the computation process. Numerical results demonstrate that the proposed method significantly reduces computational complexity compared to existing deterministic algorithms."}
{"id": "2601.07523", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07523", "abs": "https://arxiv.org/abs/2601.07523", "authors": ["Amirreza Zamani", "Sajad Daei", "Parastoo Sadeghi", "Mikael Skoglund"], "title": "Sparse Point-wise Privacy Leakage: Mechanism Design and Fundamental Limits", "comment": null, "summary": "We study an information-theoretic privacy mechanism design problem, where an agent observes useful data $Y$ that is arbitrarily correlated with sensitive data $X$, and design disclosed data $U$ generated from $Y$ (the agent has no direct access to $X$). We introduce \\emph{sparse point-wise privacy leakage}, a worst-case privacy criterion that enforces two simultaneous constraints for every disclosed symbol $u\\in\\mathcal{U}$: (i) $u$ may be correlated with at most $N$ realizations of $X$, and (ii) the total leakage toward those realizations is bounded. In the high-privacy regime, we use concepts from information geometry to obtain a local quadratic approximation of mutual information which measures utility between $U$ and $Y$. When the leakage matrix $P_{X|Y}$ is invertible, this approximation reduces the design problem to a sparse quadratic maximization, known as the Rayleigh-quotient problem, with an $\\ell_0$ constraint. We further show that, for the approximated problem, one can without loss of optimality restrict attention to a binary released variable $U$ with a uniform distribution. For small alphabet sizes, the exact sparsity-constrained optimum can be computed via combinatorial support enumeration, which quickly becomes intractable as the dimension grows. For general dimensions, the resulting sparse Rayleigh-quotient maximization is NP-hard and closely related to sparse principal component analysis (PCA). We propose a convex semidefinite programming (SDP) relaxation that is solvable in polynomial time and provides a tractable surrogate for the NP-hard design, together with a simple rounding procedure to recover a feasible leakage direction. We also identify a sparsity threshold beyond which the sparse optimum saturates at the unconstrained spectral value and the SDP relaxation becomes tight."}
{"id": "2601.07546", "categories": ["cs.IT", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.07546", "abs": "https://arxiv.org/abs/2601.07546", "authors": ["Shiv Pratap Singh Rathore", "Navin Kashyap"], "title": "Estimators for Substitution Rates in Genomes from Read Data", "comment": null, "summary": "We study the problem of estimating the mutation rate between two sequences from noisy sequencing reads. Existing alignment-free methods typically assume direct access to the full sequences. We extend these methods to the sequencing framework, where only noisy reads from the sequences are observed. We use a simple model in which both mutations and sequencing errors are substitutions. We propose multiple estimators, provide theoretical guarantees for one of them, and evaluate the others through simulations."}
{"id": "2601.07547", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07547", "abs": "https://arxiv.org/abs/2601.07547", "authors": ["Wentu Song", "Kui Cai", "Tony Q. S. Quek"], "title": "On the Sequence Reconstruction Problem for the Single-Deletion Two-Substitution Channel", "comment": null, "summary": "The Levenshtein sequence reconstruction problem studies the reconstruction of a transmitted sequence from multiple erroneous copies of it. A fundamental question in this field is to determine the minimum number of erroneous copies required to guarantee correct reconstruction of the original sequence. This problem is equivalent to determining the maximum possible intersection size of two error balls associated with the underlying channel. Existing research on the sequence reconstruction problem has largely focused on channels with a single type of error, such as insertions, deletions, or substitutions alone. However, relatively little is known for channels that involve a mixture of error types, for instance, channels allowing both deletions and substitutions. In this work, we study the sequence reconstruction problem for the single-deletion two-substitution channel, which allows one deletion and at most two substitutions applied to the transmitted sequence. Specifically, we prove that if two $q$-ary length-$n$ sequences have the Hamming distance $d\\geq 2$, where $q\\geq 2$ is any fixed integer, then the intersection size of their error balls under the single-deletion two-substitution channel is upper bounded by $(q^2-1)n^2-(3q^2+5q-5)n+O_q(1)$, where $O_q(1)$ is a constant independent from $n$ but dependent on $q$. Moreover, we show that this upper bound is tight up to an additive constant."}
{"id": "2601.07567", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07567", "abs": "https://arxiv.org/abs/2601.07567", "authors": ["Eimear Byrne", "Johan Vester Dinesen", "Ragnar Freij-Hollanti", "Camilla Hollanti"], "title": "A $q$-Polymatroid Framework for Information Leakage in Secure Linear Network Coding", "comment": null, "summary": "We study information leakage in secure linear network coding schemes based on nested rank-metric codes. We show that the amount of information leaked to an adversary that observes a subset of network links is characterized by the conditional rank function of a representable $q$-polymatroid associated with the underlying rank-metric code pair. Building on this connection, we introduce the notions of $q$-polymatroid ports and $q$-access structures and describe their structural properties. Moreover, we extend Massey's correspondence between minimal codewords and minimal access sets to the rank-metric setting and prove a $q$-analogue of the Brickell--Davenport theorem."}
{"id": "2601.07622", "categories": ["cs.IT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.07622", "abs": "https://arxiv.org/abs/2601.07622", "authors": ["Hao Wu", "Shengtian Yang", "Huiguo Gao", "Diao Wang", "Jun Chen", "Guanding Yu"], "title": "Clipped Affine Policy: Low-Complexity Near-Optimal Online Power Control for Energy Harvesting Communications over Fading Channels", "comment": "14 pages, 5 figures, v0.8", "summary": "This paper investigates online power control for point-to-point energy harvesting communications over wireless fading channels. A linear-policy-based approximation is derived for the relative-value function in the Bellman equation of the power control problem. This approximation leads to two fundamental power control policies: optimistic and robust clipped affine policies, both taking the form of a clipped affine function of the battery level and the reciprocal of channel signal-to-noise ratio coefficient. They are essentially battery-limited weighted directional waterfilling policies operating between adjacent time slots. By leveraging the relative-value approximation and derived policies, a domain-knowledge-enhanced reinforcement learning (RL) algorithm is proposed for online power control. The proposed approach is further extended to scenarios with energy and/or channel lookahead. Comprehensive simulation results demonstrate that the proposed methods achieve a good balance between computational complexity and optimality. In particular, the robust clipped affine policy (combined with RL, using at most five parameters) outperforms all existing approaches across various scenarios, with less than 2\\% performance loss relative to the optimal policy."}
{"id": "2601.07676", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07676", "abs": "https://arxiv.org/abs/2601.07676", "authors": ["Yuan Gao", "Weijun Fang", "Jingke Xu", "Jiejing Wen"], "title": "New $X$-Secure $T$-Private Information Retrieval Schemes via Rational Curves and Hermitian Curves", "comment": "18 pages, 1 figure", "summary": "$X$-secure and $T$-private information retrieval (XSTPIR) is a variant of private information retrieval where data security is guaranteed against collusion among up to $X$ servers and the user's retrieval privacy is guaranteed against collusion among up to $T$ servers. Recently, researchers have constructed XSTPIR schemes through the theory of algebraic geometry codes and algebraic curves, with the aim of obtaining XSTPIR schemes that have higher maximum PIR rates for fixed field size and $X,T$ (the number of servers $N$ is not restricted). The mainstream approach is to employ curves of higher genus that have more rational points, evolving from rational curves to elliptic curves to hyperelliptic curves and, most recently, to Hermitian curves.\n  In this paper, we propose a different perspective: with the shared goal of constructing XSTPIR schemes with higher maximum PIR rates, we move beyond the mainstream approach of seeking curves with higher genus and more rational points. Instead, we aim to achieve this goal by enhancing the utilization efficiency of rational points on curves that have already been considered in previous work. By introducing a family of bases for the polynomial space $\\text{span}_{\\mathbb{F}_q}\\{1,x,\\dots,x^{k-1}\\}$ as an alternative to the Lagrange interpolation basis, we develop two new families of XSTPIR schemes based on rational curves and Hermitian curves, respectively. Parameter comparisons demonstrate that our schemes achieve superior performance. Specifically, our Hermitian-curve-based XSTPIR scheme provides the largest known maximum PIR rates when the field size $q^2\\geq 14^2$ and $X+T\\geq 4q$. Moreover, for any field size $q^2\\geq 28^2$ and $X+T\\geq 4$, our two XSTPIR schemes collectively provide the largest known maximum PIR rates."}
{"id": "2601.07725", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07725", "abs": "https://arxiv.org/abs/2601.07725", "authors": ["Jessica Bariffi", "Drisana Bhatia", "Giuseppe Cotardo", "Violetta Weger"], "title": "Weak Composition Lattices and Ring-Linear Anticodes", "comment": null, "summary": "Lattices and partially ordered sets have played an increasingly important role in coding theory, providing combinatorial frameworks for studying structural and algebraic properties of error-correcting codes. Motivated by recent works connecting lattice theory, anticodes, and coding-theoretic invariants, we investigate ring-linear codes endowed with the Lee metric. We introduce and characterize optimal Lee-metric anticodes over the ring $\\mathbb{Z}/p^s\\mathbb{Z}$. We show that the family of such anticodes admits a natural partition into subtypes and forms a lattice under inclusion. We establish a bijection between this lattice and a lattice of weak compositions ordered by dominance. As an application, we use this correspondence to introduce new invariants for Lee-metric codes via an anticode approach."}
{"id": "2601.07797", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.07797", "abs": "https://arxiv.org/abs/2601.07797", "authors": ["Yiqi Chen", "Holger Boche", "Marc Geitz"], "title": "Lossy Source Coding with Broadcast Side Information", "comment": null, "summary": "This paper considers the source coding problem with broadcast side information. The side information is sent to two receivers through a noisy broadcast channel. We provide an outer bound of the rate--distortion--bandwidth (RDB) quadruples and achievable RDB quadruples when the helper uses a separation-based scheme. Some special cases with full characterization are also provided. We then compare the separation-based scheme with the uncoded scheme in the quadratic Gaussian case."}
