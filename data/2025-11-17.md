<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 10]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.DS](#cs.DS) [Total: 9]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [ResBench: A Comprehensive Framework for Evaluating Database Resilience](https://arxiv.org/abs/2511.11088)
*Puyun Hu,Wei Pan,Xun Jian,Zeqi Ma,Tianjie Li,Yang Shen,Chengzhi Han,Yudong Zhao,Zhanhuai Li*

Main category: cs.DB

TL;DR: 提出了ResBench基准测试框架，用于评估数据库在面临不利事件时的弹性能力，通过模拟不利事件并注入正常事务处理过程，从八个维度全面量化数据库弹性。


<details>
  <summary>Details</summary>
Motivation: 现有数据库基准测试主要关注理想运行环境下的性能，但现实场景中数据库会面临各种不利事件，需要从综合角度量化其应对能力。

Method: 设计ResBench框架，通过清晰的分层解耦实现测试过程的自动化、标准化和可视化，模拟不利事件并在正常事务处理过程中注入，使用模块收集多个指标进行评估。

Result: 从吞吐量、延迟、稳定性、抵抗性、恢复性、干扰周期、适应能力和指标偏差八个维度评估数据库弹性，所有结果通过用户友好的图形界面呈现。

Conclusion: ResBench能够有效评估数据库在不利环境下的弹性表现，为数据库系统在现实场景中的稳健性提供了标准化评估方法。

Abstract: Existing database benchmarks primarily focus on performance under ideal running environments. However, in real-world scenarios, databases probably face numerous adverse events. Quantifying the ability to cope with these events from a comprehensive perspective remains an open problem. We provide the definition of database resilience to describe its performance when facing adversity and propose ResBench, a benchmark for evaluating database resilience. This framework achieves automation, standardization, and visualization of the testing process through clear hierarchical decoupling. ResBench simulates adverse events and injects them during normal transaction processing, utilizing a module to gather multiple metrics for the evaluation model. We assess database resilience across eight dimensions: throughput, latency, stability, resistance, recovery, disturbance period, adaptation capability and metric deviation. All the results are presented to users via a user-friendly graphical interface. We demonstrate the execution process and result interpretation of ResBench using two types of adversity datasets.

</details>


### [2] [Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database](https://arxiv.org/abs/2511.11399)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DB

TL;DR: 提出了一种将知识补全阶段整合到图数据库-图机器学习应用中的创新架构，通过可扩展的传递关系揭示隐藏知识，显著影响数据集行为和指标


<details>
  <summary>Details</summary>
Motivation: 当前图数据库-图机器学习应用在分析数据时存在关键缺陷，特别是在知识图谱的知识补全方面，忽略了数据中实际存在的隐藏知识，可能导致错误的模型解释

Method: 引入可扩展的传递关系，通过衰减函数建模信息在网络中的传播，实现确定性知识流在多个节点间的流动

Result: 实验结果表明该方法从根本上重塑了数据集的拓扑结构和整体动态

Conclusion: 这种新的图数据库-图机器学习架构能够产生更好的模型，并释放基于图的数据分析的全部潜力

Abstract: Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.
  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.
  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [3] [LEMUR: Large scale End-to-end MUltimodal Recommendation](https://arxiv.org/abs/2511.10962)
*Xintian Han,Honggang Chen,Quan Lin,Jingyue Gao,Xiangyuan Ren,Lifei Zhu,Zhisheng Ye,Shikang Wu,XiongHang Xie,Xiaochu Gan,Bingzheng Wei,Peng Xu,Zhe Wang,Yuchao Zheng,Jingjian Lin,Di Wu,Junfeng Ge*

Main category: cs.IR

TL;DR: LEMUR是首个从原始数据端到端训练的大规模多模态推荐系统，通过联合优化多模态和推荐组件解决传统两阶段训练中的对齐问题，在抖音搜索中显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于ID的推荐系统面临冷启动和泛化挑战，现有工业多模态推荐采用两阶段训练，存在目标不对齐和无法动态适应新数据的问题。

Method: 提出LEMUR端到端训练框架，联合优化多模态和推荐组件；引入内存银行机制增量累积历史多模态表示，降低计算成本。

Result: 在抖音搜索部署一个月后，查询变化率衰减减少0.843%，QAUC提升0.81%；在抖音广告中多个离线指标均有显著提升。

Conclusion: 端到端多模态推荐在真实工业场景中具有优越性，验证了联合训练方法的有效性。

Abstract: Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.

</details>


### [4] [GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs](https://arxiv.org/abs/2511.11010)
*Kyle Deeds,Ying-Hsiang Huang,Claire Gong,Shreya Shaji,Alison Yan,Leslie Harka,Samuel J Klein,Shannon Zejiang Shen,Mark Phillips,Trevor Owens,Benjamin Charles Germain Lee*

Main category: cs.IR

TL;DR: GovScape是一个支持多模态搜索的公共系统，用于搜索2020年联邦政府爬取的1000万份PDF文档，提供元数据过滤、精确文本搜索、语义文本搜索和视觉搜索功能。


<details>
  <summary>Details</summary>
Motivation: 现有的网页档案虽然保存了大量政府PDF文档，但访问和发现能力有限，只能下载单个PDF或进行基本关键词搜索。

Method: 开发了包含元数据过滤、精确文本搜索、语义文本搜索和视觉搜索的多模态搜索系统，构建了嵌入管道、系统架构和开源代码库。

Result: 成功处理了10,015,993份联邦政府PDF文档（总计70,958,487页），总计算成本约1500美元，相当于每美元处理47,000页PDF。

Conclusion: GovScape展示了在1000万PDF规模上实现多模态搜索的可行性，并已开始向1亿+PDF规模扩展。

Abstract: Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as "redacted documents" or "pie charts." We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.

</details>


### [5] [Enhancing Group Recommendation using Soft Impute Singular Value Decomposition](https://arxiv.org/abs/2511.11172)
*Mubaraka Sani Ibrahim,Isah Charles Saidu,Lehel Csato*

Main category: cs.IR

TL;DR: 提出Group Soft-Impute SVD方法，利用软填充奇异值分解增强群组推荐，在稀疏高维数据上优于基线方法，特别在小用户群组中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 随着群组活动的普及，需要基于群组成员集体偏好为群组提供推荐。现有群组推荐系统因数据稀疏性和高维性而面临挑战。

Method: 使用软填充奇异值分解进行低秩矩阵补全，处理稀疏高维数据问题。

Result: 在Goodbooks、Movielens和合成数据集上，该方法在小用户群组中召回率优于基线方法，在所有群组规模下表现相当，且恢复的矩阵秩更低。

Conclusion: Group Soft-Impute SVD能有效处理高维数据，在群组推荐任务中表现优异。

Abstract: The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.

</details>


### [6] [Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation](https://arxiv.org/abs/2511.11255)
*Wencai Ye,Mingjie Sun,Shuhang Chen,Wenjin Wu,Peng Jiang*

Main category: cs.IR

TL;DR: Align³GR是一个将大语言模型转化为推荐系统的框架，通过统一token级、行为建模级和偏好级对齐来解决语义和行为不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在利用结构化世界知识和多步推理方面具有优势，但将其转化为实际推荐系统时存在语义和行为不匹配的根本挑战。

Method: 提出Align³GR框架，包含：双token化融合用户-物品语义和协同信号；双向语义对齐增强行为建模；结合自博弈和真实反馈的渐进式DPO策略进行动态偏好适应。

Result: 在公开数据集上，Recall@10提升17.8%，NDCG@10提升20.2%，在工业级大规模推荐平台的在线A/B测试和全规模部署中表现显著提升。

Conclusion: Align³GR通过多层次对齐方法有效解决了LLMs在推荐系统中的语义和行为不匹配问题，实现了显著的性能提升。

Abstract: Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.

</details>


### [7] [MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising](https://arxiv.org/abs/2511.11305)
*Chenghan Fu,Daoze Zhang,Yukang Lin,Zhanheng Nie,Xiang Zhang,Jianyu Liu,Yueran Liu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: MOON是一个用于电商多模态表示学习的可持续迭代实践框架，已在淘宝搜索广告系统全链路部署，在点击率预测任务上实现了20%的在线提升。


<details>
  <summary>Details</summary>
Motivation: 解决多模态表示学习目标与下游任务目标之间的不对齐问题，通过定义"交换率"来量化中间指标改进如何转化为下游收益。

Method: 采用"预训练、后训练、应用"三阶段训练范式，通过图像搜索召回作为关键中间指标来指导多模态模型优化，并在数据处理、训练策略、模型架构和下游应用四个维度持续迭代。

Result: 在淘宝搜索广告系统中全面部署，点击率预测任务实现了20%的在线提升，这是三年来该任务上最大的改进，已完成五次全规模迭代。

Conclusion: MOON框架成功解决了多模态表示学习与下游任务的对齐问题，通过系统化的迭代优化和规模化效应研究，为电商领域的多模态表示学习提供了有价值的实践经验。

Abstract: We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.

</details>


### [8] [SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation](https://arxiv.org/abs/2511.11370)
*Jiahao Wang,Bokang Fu,Yu Zhu,Yuli Liu*

Main category: cs.IR

TL;DR: 提出了基于集合的反思学习框架(SRLF)，通过"评估-验证-反思"闭环机制，利用LLM的上下文学习能力对整组物品进行整体判断，解决了传统点式方法在用户偏好理解和物品语义表示方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在推荐系统中主要关注单个物品的评分建模，这种点式方法导致用户偏好理解不准确和物品语义表示僵化的问题。

Method: SRLF框架采用集合层面的整体判断方法，综合分析物品间的复杂相互关系及其与用户偏好配置的集体一致性，通过"评估-验证-反思"闭环实现序列推荐。

Result: 大量实验验证了该方法的有效性，确认集合视角对于在序列推荐任务中实现最先进性能至关重要。

Conclusion: 集合层面的上下文理解方法能够捕捉用户行为中复杂的关系模式，使模型在序列推荐中表现显著更优。

Abstract: LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.
  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop "assess-validate-reflect" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [9] [AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization](https://arxiv.org/abs/2511.11106)
*Zhonghua Jiang,Kui Chen,Kunxi Li,Keting Yin,Yiyun Zhou,Zhaode Wang,Chengfei Lv,Shengyu Zhang*

Main category: cs.MM

TL;DR: AccKV是一个针对音频-视频大语言模型的高效推理优化框架，通过自适应聚焦和跨模态校准技术，在保持精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 音频-视频大语言模型在处理时域信息时会产生较大的键值缓存，传统优化策略基于任务选择性保留缓存，但研究发现高层注意力更偏向视频模态，且直接整合音频和视频的时空信息会导致信息混淆和性能下降。

Method: 提出层自适应聚焦技术，根据不同层特性选择关键模态，通过注意力重分配增强重要token识别；提出跨模态校准技术，先在模态内整合低效缓存，然后将低优先级模态与高优先级模态对齐，选择性淘汰低优先级缓存。

Result: 实验结果表明AccKV能显著提升AV-LLMs的计算效率，同时保持模型精度。

Conclusion: AccKV框架有效解决了AV-LLMs推理中的键值缓存优化问题，通过自适应聚焦和跨模态校准实现了高效的多模态推理。

Abstract: Recent advancements in Audio-Video Large Language Models (AV-LLMs) have enhanced their capabilities in tasks like audio-visual question answering and multimodal dialog systems. Video and audio introduce an extended temporal dimension, resulting in a larger key-value (KV) cache compared to static image embedding. A naive optimization strategy is to selectively focus on and retain KV caches of audio or video based on task. However, in the experiment, we observed that the attention of AV-LLMs to various modalities in the high layers is not strictly dependent on the task. In higher layers, the attention of AV-LLMs shifts more towards the video modality. In addition, we also found that directly integrating temporal KV of audio and spatial-temporal KV of video may lead to information confusion and significant performance degradation of AV-LLMs. If audio and video are processed indiscriminately, it may also lead to excessive compression or reservation of a certain modality, thereby disrupting the alignment between modalities. To address these challenges, we propose AccKV, an Adaptive-Focusing and Cross-Calibration KV cache optimization framework designed specifically for efficient AV-LLMs inference. Our method is based on layer adaptive focusing technology, selectively focusing on key modalities according to the characteristics of different layers, and enhances the recognition of heavy hitter tokens through attention redistribution. In addition, we propose a Cross-Calibration technique that first integrates inefficient KV caches within the audio and video modalities, and then aligns low-priority modalities with high-priority modalities to selectively evict KV cache of low-priority modalities. The experimental results show that AccKV can significantly improve the computational efficiency of AV-LLMs while maintaining accuracy.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [10] [Online Price Competition under Generalized Linear Demands](https://arxiv.org/abs/2511.10718)
*Daniele Bracale,Moulinath Banerjee,Cong Shi,Yuekai Sun*

Main category: cs.GT

TL;DR: 提出了PML-GLUCB策略，用于多卖家价格竞争环境，在广义需求函数下实现O(N²√TlogT)的遗憾界


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注线性需求模型，而实际市场中的需求函数往往更为复杂，需要开发适用于广义需求函数的多卖家竞争算法

Method: 结合惩罚最大似然估计和上置信界定价规则，提出PML-GLUCB策略，无需卖家间的协调探索阶段，适用于二元和实值需求观测

Result: 相对于动态基准策略，每个卖家实现了O(N²√TlogT)的遗憾界，基本匹配线性设置中的最优速率

Conclusion: 成功将椭圆势引理扩展到竞争性多智能体环境，为广义需求函数下的价格竞争提供了有效的分散化解决方案

Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $λ_i(\mathbf{p}) = μ_i(\langle \boldsymbolθ_{i,0}, \mathbf{p} \rangle)$, with known increasing link $μ_i$ and unknown parameter $\boldsymbolθ_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.

</details>


### [11] [Optimal Welfare in Noncooperative Network Formation under Attack](https://arxiv.org/abs/2511.10845)
*Natan Doubez,Pascal Lenzner,Marcus Wunderlich*

Main category: cs.GT

TL;DR: 本文改进了Goyal等人提出的网络攻击博弈模型，证明了自私代理构建的网络能够抵抗大规模攻击并保持渐近最优的社会福利，解决了开放性问题。


<details>
  <summary>Details</summary>
Motivation: 现代通信网络由自私代理分散控制，面临网络攻击威胁。现有模型对网络鲁棒性的分析不够精确，需要改进边界并解决开放性问题。

Method: 重新分析Goyal等人的博弈论模型，改进网络鲁棒性的紧边界分析，研究攻击者策略对网络破坏程度的影响。

Result: 证明自私代理构建的网络能够抵抗大规模攻击，保持渐近最优的社会福利，发现最小化社会福利的攻击者并非造成最大破坏。

Conclusion: 分散式网络在自私代理控制下仍具有强鲁棒性，攻击者策略选择影响实际破坏程度，为网络安全设计提供新见解。

Abstract: Communication networks are essential for our economy and our everyday lives. This makes them lucrative targets for attacks. Today, we see an ongoing battle between criminals that try to disrupt our key communication networks and security professionals that try to mitigate these attacks. However, today's networks, like the Internet or peer-to-peer networks among smart devices, are not controlled by a single authority, but instead consist of many independently administrated entities that are interconnected. Thus, both the decisions of how to interconnect and how to secure against potential attacks are taken in a decentralized way by selfish agents.
  This strategic setting, with agents that want to interconnect and potential attackers that want to disrupt the network, was captured via an influential game-theoretic model by Goyal, Jabbari, Kearns, Khanna, and Morgenstern (WINE 2016). We revisit this model and show improved tight bounds on the achieved robustness of networks created by selfish agents. As our main result, we show that such networks can resist attacks of a large class of potential attackers, i.e., these networks maintain asymptotically optimal welfare post attack. This improves several bounds and resolves an open problem. Along the way, we show the counter-intuitive result, that attackers that aim at minimizing the social welfare post attack do not actually inflict the greatest possible damage.

</details>


### [12] [Playing with Peaks: A Game-Theoretic Comparison of Electricity Pricing Mechanisms](https://arxiv.org/abs/2511.10854)
*Vade Shah,Jason R. Marden*

Main category: cs.GT

TL;DR: 比较两种电力需求管理定价机制：随时峰值定价(AP)和同时峰值定价(CP)，分析它们在完全信息和不完全信息下的均衡峰值需求表现。


<details>
  <summary>Details</summary>
Motivation: 随着电力消费增长，降低峰值需求对防止电网基础设施过载和停电至关重要。不同定价机制在实践中使用但相对性能不明确，需要系统比较。

Method: 将电力市场建模为策略博弈，在完全信息和不完全信息两种情境下，分别分析AP和CP机制下的均衡峰值需求特征。

Result: 完全信息下，CP机制的均衡峰值需求从不高于AP机制；但不完全信息下，CP的协调机制可能适得其反，导致比AP更大的均衡峰值。

Conclusion: CP机制耦合用户成本的潜在收益必须与协调失败风险权衡；渐进式需求成本结构可能在不牺牲协调收益的同时缓解这些风险，在确定性和随机环境中都表现良好。

Abstract: As electricity consumption grows, reducing peak demand--the maximum load on the grid--has become critical for preventing infrastructure strain and blackouts. Pricing mechanisms that incentivize consumers with flexible loads to shift consumption away from high-demand periods have emerged as effective tools, yet different mechanisms are used in practice with unclear relative performance. This work compares two widely implemented approaches: anytime peak pricing (AP), where consumers pay for their individual maximum consumption, and coincident peak pricing (CP), where consumers pay for their consumption during the system-wide peak period. To compare these mechanisms, we model the electricity market as a strategic game and characterize the peak demand in equilibrium under both AP and CP. Our main result demonstrates that with perfect information, equilibrium peak demand under CP never exceeds that under AP; on the other hand, with imperfect information, the coordination introduced by CP can backfire and induce larger equilibrium peaks than AP. These findings demonstrate that potential gains from coupling users' costs (as done in CP) must be weighed against these miscoordination risks. We conclude with preliminary results indicating that progressive demand cost structures--rather than per-unit charges--may mitigate these risks while preserving coordination benefits, achieving desirable performance in both deterministic and stochastic settings.

</details>


### [13] [Fair Incentives for Early Arrival in 0-1 Cooperative Games](https://arxiv.org/abs/2511.11023)
*Yaoxin Ge,Yao Zhang,Dengji Zhao*

Main category: cs.GT

TL;DR: 提出了一种新的公平性机制Egalitarian Value-Sharing (EVS)，用于在线合作游戏中解决早期到达激励问题，旨在最小化每个到达顺序中价值分配与Shapley值之间的距离。


<details>
  <summary>Details</summary>
Motivation: 现有在线合作游戏解决方案忽略了单个到达顺序中的公平性，重要玩家可能一无所获，这在现实中不公平。

Method: 在0-1价值游戏中，计算每个到达顺序中价值分配与Shapley值的距离，并设计EVS机制来最小化该距离。

Result: EVS机制能够在所有做出贡献的玩家中最大化平等主义福利。

Conclusion: 提出的EVS机制解决了在线合作游戏中单个到达顺序的公平性问题，确保了价值分配的公平性。

Abstract: Incentives for early arrival (I4EA) was recently proposed for studying online cooperative games. In an online cooperative game, players arrive in an unknown order, and the value increase after each player arrived should be distributed immediately among all the arrived players. Although there is only one arriving order in the game, we also hope that the value distribution is equal to their Shapley value in expectation. To achieve these goals, the early solutions ignored the fairness in each single arriving order. More specifically, an important player may receive nothing in a game, which seems unfair in reality. To combat this, we propose refined fairness in this paper and design new solutions in 0-1 value games. Specifically, we compute the distance of the distribution in each order to the Shapley value and aim to minimize it. We propose a new mechanism called Egalitarian Value-Sharing (EVS) to do so. We also show that the mechanism can maximize the egalitarian welfare among all the players who made contributions.

</details>


### [14] [ε-Optimally Solving Two-Player Zero-Sum POSGs](https://arxiv.org/abs/2511.11282)
*Erwan Christian Escudie,Matthia Sabatelli,Olivier Buffet,Jilles Steeve Dibangoye*

Main category: cs.GT

TL;DR: 提出了首个从零和部分可观测随机博弈到转移独立零和随机博弈的无损归约框架，使得能够系统性地应用动态规划方法求解部分可观测博弈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的零和部分可观测随机博弈缺乏与动态规划技术的理论连接，之前的尝试缺乏无损归约，限制了通用算法的开发。

Method: 通过无损归约将零和部分可观测随机博弈转化为转移独立零和随机博弈，然后应用基于动态规划的方法如点基值迭代算法。

Result: 在多个基准领域上，通过该归约应用的点基值迭代算法产生ε最优策略，性能一致匹配或优于现有最先进方法。

Conclusion: 该工作为零和随机博弈到部分可观测环境的算法和理论迁移开辟了系统性途径。

Abstract: We present a novel framework for ε-optimally solving two-player zero-sum partially observable stochastic games (zs-POSGs). These games pose a major challenge due to the absence of a principled connection with dynamic programming (DP) techniques developed for two-player zero-sum stochastic games (zs-SGs). Prior attempts at transferring solution methods have lacked a lossless reduction, defined here as a transformation that preserves value functions, equilibrium strategies, and optimality structure, thereby limiting generalisation to ad-hoc algorithms. This work introduces the first lossless reduction from zs-POSGs to transition-independent zs-SGs, enabling the principled application of a broad class of DP-based methods. We show empirically that point-based value iteration (PBVI) algorithms, applied via this reduction, produce ε-optimal strategies across a range of benchmark domains, consistently matching or outperforming existing state-of-the-art methods. Our results open a systematic pathway for algorithmic and theoretical transfer from SGs to partially observable settings.

</details>


### [15] [Computing Equilibrium Nominations in Presidential Elections](https://arxiv.org/abs/2511.11365)
*Piotr Faliszewski,Stanislaw Kazmierowski,Grzegorz Lisowski,Ildiko Schlotter,Paolo Turrini*

Main category: cs.GT

TL;DR: 研究政党在多数投票选举中的战略候选人提名问题，引入政党对齐单峰偏好概念，提供多项式时间算法来识别这种偏好并解决相关提名问题。


<details>
  <summary>Details</summary>
Motivation: 研究政党在多数投票选举中如何战略性地提名候选人来影响选举结果，特别是在选民偏好具有特定结构的情况下。

Method: 引入政党对齐单峰偏好概念，开发多项式时间算法来识别偏好结构，并分析政党提名策略对选举结果的影响。

Result: 提供了识别政党对齐单峰偏好的多项式时间算法，证明了在三党选举中纯策略纳什均衡的保证存在性。

Conclusion: 在政党对齐单峰偏好假设下，可以高效解决候选人的战略提名问题，为多党选举的博弈分析提供了理论基础。

Abstract: We study strategic candidate nomination by parties in elections decided by Plurality voting. Each party selects a nominee before the election, and the winner is chosen from the nominated candidates based on the voters' preferences. We introduce a new restriction on these preferences, which we call party-aligned single-peakedness: all voters agree on a common ordering of the parties along an ideological axis, but may differ in their perceptions of the positions of individual candidates within each party. The preferences of each voter are single-peaked with respect to their own axis over the candidates, which is consistent with the global ordering of the parties. We present a polynomial-time algorithm for recognizing whether a preference profile satisfies party-aligned single-peakedness. In this domain, we give polynomial-time algorithms for deciding whether a given party can become the winner under some (or all) nominations, and whether this can occur in some pure Nash equilibrium. We also prove a tight result about the guaranteed existence of pure strategy Nash equilibria for elections with up to three parties for single-peaked and party-aligned single-peaked preference profiles.

</details>


### [16] [Nucleolus, Happy Nucleolus, and Vehicle Routing](https://arxiv.org/abs/2511.11371)
*Daniel Ebert,Antonia Ellerbrock*

Main category: cs.GT

TL;DR: 本文研究了快乐核仁在合作博弈成本分配中的计算问题，揭示了其与经典核仁的复杂关系，提出了处理线性规划中子空间避免约束的高效方法，并在车辆路径博弈中开发了实用启发式算法。


<details>
  <summary>Details</summary>
Motivation: 研究快乐核仁的计算问题，探索其与经典核仁的关系，并开发高效的算法来解决实际应用中的成本分配问题。

Method: 使用MPS方案中的线性规划方法，结合动态规划技术处理子空间避免约束，并设计基于先前见解的启发式算法。

Result: 发现快乐核仁与核仁之间存在反直觉的关系，开发了处理子空间避免约束的高效近似方法，并在车辆路径博弈中验证了启发式算法的有效性。

Conclusion: 快乐核仁与核仁的关系比预期复杂，提出的算法能够高效计算快乐核仁，为实际应用提供了可行的解决方案。

Abstract: We study the recently introduced fair division concept of the happy nucleolus for cost allocation among players in a cooperative game, with special focus on its computation. The happy nucleolus applies the same fairness criterion as the well-established nucleolus but with reduced total value. Still, we show that the relation between the two concepts is quite involved, and intuitive properties do not hold - e.g., the entry of a player in the happy nucleolus can be larger than the entry of the same player in the nucleolus, even for monotone and subadditive games. This refutes conjectures of Meir, Rosenschein and Malizia (2011).
  Further, we study the separation problem of the linear programs appearing in the MPS scheme for computing the (happy) nucleolus. It includes linear subspace avoidance constraints, which can be handled efficiently for problems with a certain dynamic programming formulation due to Köhnemann and Toth (2020). We show how to get rid of these constraints for all monotone games if we allow for an arbitrarily small error of epsilon, thus conserving known approximation guarantees for the same problem without subspace avoidance.
  Finally, we focus on practical results at the example of vehicle routing games by designing an efficient heuristic based on our previous insights and past work, and demonstrate its power.

</details>


### [17] [Public Goods Games in Directed Networks with Constraints on Sharing](https://arxiv.org/abs/2511.11475)
*Argyrios Deligkas,Gregory Gutin,Mark Jones,Philip R. Neary,Anders Yeo*

Main category: cs.GT

TL;DR: 该论文研究公共物品博弈中玩家在容量约束下的购买和共享决策，分析纳什均衡的存在性、计算复杂性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 研究有向图中公共物品博弈的均衡特性，其中物品不可分割且共享受容量限制，玩家需同时决定是否购买和选择哪些邻居共享。

Method: 采用博弈论分析方法，研究纯策略和混合策略纳什均衡，考察共享容量k和网络结构对均衡特性的影响。

Result: 建立了关于共享容量和网络结构的尖锐复杂性二分法，揭示了均衡存在性、计算和效率的精确边界条件。

Conclusion: 该模型为理解容量约束下公共物品博弈的均衡行为提供了系统分析框架，揭示了网络结构和共享容量的关键作用。

Abstract: In a public goods game, every player chooses whether or not to buy a good that all neighboring players will have access to. We consider a setting in which the good is indivisible, neighboring players are out-neighbors in a directed graph, and there is a capacity constraint on their number, k, that can benefit from the good. This means that each player makes a two-pronged decision: decide whether or not to buy and, conditional on buying, choose which k out-neighbors to share access. We examine both pure and mixed Nash equilibria in the model from the perspective of existence, computation, and efficiency. We perform a comprehensive study for these three dimensions with respect to both sharing capacity (k) and the network structure (the underlying directed graph), and establish sharp complexity dichotomies for each.

</details>


### [18] [Deviation Dynamics in Cardinal Hedonic Games](https://arxiv.org/abs/2511.11531)
*Valentin Zech,Martin Bullinger*

Main category: cs.GT

TL;DR: 该论文研究了基数享乐博弈中稳定分区的计算问题，提出了基于No-instances的元定理来证明偏差动态收敛的难度，并分析了在可加可分离享乐博弈中寻找个体理性和契约个体稳定分区的动态过程。


<details>
  <summary>Details</summary>
Motivation: 享乐博弈中稳定分区计算具有挑战性，因为存在稳定结果不存在的博弈实例。这些No-instances常被用来证明计算复杂性结果，作者希望在基数享乐博弈的动态模型中对此进行严格分析。

Method: 提出了元定理，基于No-instances的存在性来证明偏差动态收敛的难度。研究了可加可分离享乐博弈、分数享乐博弈和修正分数享乐博弈，涵盖了基于单智能体偏差的所有合理稳定性概念。提出了寻找个体理性和契约个体稳定分区的动态方法。

Result: 证明了偏差动态可能或必要收敛的判定难度。在可加可分离享乐博弈中，从单例分区开始的契约个体稳定动态可能在线性次偏差后收敛，但在最坏情况下可能需要指数次偏差。

Conclusion: 该研究为享乐博弈中稳定分区的计算复杂性提供了理论基础，证明了基于No-instances的元定理的有效性，并为寻找特定稳定性概念的分区提供了动态方法。

Abstract: Computing stable partitions in hedonic games is a challenging task because there exist games in which stable outcomes do not exist. Even more, these No-instances can often be leveraged to prove computational hardness results. We make this impression rigorous in a dynamic model of cardinal hedonic games by providing meta theorems. These imply hardness of deciding about the possible or necessary convergence of deviation dynamics based on the mere existence of No-instances. Our results hold for additively separable, fractional, and modified fractional hedonic games (ASHGs, FHGs, and MFHGs). Moreover, they encompass essentially all reasonable stability notions based on single-agent deviations. In addition, we propose dynamics as a method to find individually rational and contractually individual stable (CIS) partitions in ASHGs. In particular, we find that CIS dynamics from the singleton partition possibly converge after a linear number of deviations but may require an exponential number of deviations in the worst case.

</details>


### [19] [Incremental Data-Driven Policy Synthesis via Game Abstractions](https://arxiv.org/abs/2511.11545)
*Irmak Sağlam,Mahdi Nazeri,Alessandro Abate,Sadegh Soudjani,Anne-Kathrin Schmuck*

Main category: cs.GT

TL;DR: 提出了一种基于数据驱动的增量式控制策略合成方法，用于满足未知随机动态系统的时序逻辑目标，通过在线学习和增量博弈求解来提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 针对未知随机动态系统的控制策略合成问题，传统方法需要大量数据且计算成本高，需要一种能够随着数据积累逐步改进策略的高效方法。

Method: 构建有限随机博弈图抽象系统动态，使用增量式算法在数据积累过程中逐步精化可达集估计，并通过基于排名的增量博弈求解算法更新获胜区域。

Result: 方法能够单调地扩展获胜区域，数值案例研究表明相比从头求解的方法显著节省计算成本。

Conclusion: 提出的增量式框架为未知随机系统的控制策略合成提供了一种高效的数据驱动解决方案，能够随着数据积累逐步改进控制策略。

Abstract: We address the synthesis of control policies for unknown discrete-time stochastic dynamical systems to satisfy temporal logic objectives. We present a data-driven, abstraction-based control framework that integrates online learning with novel incremental game-solving. Under appropriate continuity assumptions, our method abstracts the system dynamics into a finite stochastic (2.5-player) game graph derived from data. Given a requirement over time on this graph, we compute the winning region -- i.e., the set of initial states from which the objective is satisfiable -- in the resulting game, together with a corresponding control policy.
  Our main contribution is the construction of abstractions, winning regions and control policies incrementally, as data about the system dynamics accumulates. Concretely, our algorithm refines under- and over-approximations of reachable sets for each state-action pair as new data samples arrive. These refinements induce structural modifications in the game graph abstraction -- such as the addition or removal of nodes and edges -- which in turn modify the winning region. Crucially, we show that these updates are inherently monotonic: under-approximations can only grow, over-approximations can only shrink, and the winning region can only expand.
  We exploit this monotonicity by defining an objective-induced ranking function on the nodes of the abstract game that increases monotonically as new data samples are incorporated. These ranks underpin our novel incremental game-solving algorithm, which employs customized gadgets (DAG-like subgames) within a rank-lifting algorithm to efficiently update the winning region. Numerical case studies demonstrate significant computational savings compared to the baseline approach, which resolves the entire game from scratch whenever new data samples arrive.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [20] [Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time](https://arxiv.org/abs/2511.10777)
*Xiaxin Li,Arya Mazumdar*

Main category: cs.IT

TL;DR: 本文提出两种在一位压缩感知中实现亚线性运行时间的支持恢复方案：一种是通用精确支持恢复，另一种是通用近似支持恢复，均显著改善了现有方法的运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有的一位压缩感知支持恢复方法通常需要Ω(n)的运行时间，这在处理高维信号时效率较低。本文旨在开发具有亚线性运行时间o(n)的高效支持恢复方案。

Method: 提出了两种方案：(1) 通用精确支持恢复方案，使用O(k²log(n/k)log n)次测量和O(km)运行时间；(2) 通用近似支持恢复方案，使用O(kε⁻¹log(n/k)log n)次测量和O(ε⁻¹m)运行时间；(3) 亚线性机制下的概率精确支持恢复方案，使用O(k(log k/log log k)log n)次测量和O(m)运行时间。

Result: 所提方案在运行时间上实现了显著改进：通用方案从Ω(n)降低到o(n)，近似方案在保持当前最优测量次数的同时显著改善运行时间，概率方案改进了Yang等人2025年的结果。

Conclusion: 本文证明了在一位压缩感知中实现亚线性运行时间的支持恢复是可行的，为高效处理高维稀疏信号提供了新的理论框架和实用方案。

Abstract: The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\in \mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\text{sign}(Ax)$, where $A\in \mathbb{R}^{m\times n}$ is a sensing matrix and $|\text{supp}(x)|\leq k, k \ll n$. Under this setting, most preexisting works have a recovery runtime $Ω(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1.i): For the universal exact support recovery, a scheme of $m=O(k^2\log(n/k)\log n)$ measurements and runtime $D=O(km)$. (1.ii): For the universal $ε$-approximate support recovery, the same scheme with $m=O(kε^{-1}\log(n/k)\log n)$ and runtime $D=O(ε^{-1}m)$, improving the runtime significantly with an extra $O(\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\frac{\log k}{\log\log k}\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025.

</details>


### [21] [Joint Beamforming and Position Optimization for IRS-Aided SWIPT with Movable Antennas](https://arxiv.org/abs/2511.11148)
*Yanze Zhu,Qingqing Wu,Xinrong Guan,Ziyuan Zheng,Honghao Wang,Wen Chen,Yang Liu,Yuan Guo*

Main category: cs.IT

TL;DR: 该论文提出将智能反射面和可移动天线技术引入SWIPT系统，通过联合优化基站和IRS的波束成形以及MA位置，在保证能量收集接收器需求的同时最大化信息解码接收器的加权和速率。


<details>
  <summary>Details</summary>
Motivation: SWIPT技术在物联网网络中面临长距离传播导致的严重衰减问题，导致能量收集效率低下。需要新技术来增强信息传输和能量收集性能。

Method: 结合WMMSE、BCD、MM和PDD框架开发高效算法，联合优化基站和IRS的波束成形以及MA位置，并提出可行性分析方法验证EHR需求的可实现性。

Result: 仿真结果表明所提方案具有显著优势，在考虑的场景下优化IRS配置比MA配置能获得更高的性能增益。

Conclusion: IRS和MA技术的结合能有效提升SWIPT系统性能，其中IRS配置优化在特定场景下表现更优。

Abstract: Simultaneous wireless information and power transfer (SWIPT) has been envisioned as a promising technology to support ubiquitous connectivity and reliable sustainability in Internet-of-Things (IoT) networks, which, however, generally suffers from severe attenuation caused by long distance propagation, leading to inefficient wireless power transfer (WPT) for energy harvesting receivers (EHRs). This paper proposes to introduce emerging intelligent reflecting surface (IRS) and movable antenna (MA) technologies into SWIPT systems aiming at enhancing information transmission for information decoding receivers (IDRs) and improving receive power of EHRs. We consider to maximize the weighted sum-rate of IDRs via jointly optimizing the active and passive beamforming at the base station (BS) and IRS, respectively, as well as the positions of MAs, while guaranteeing the requirements of all EHRs. To tackle this challenging task due to the non-convexity of associated optimization, we develop an efficient algorithm combining weighted minimal mean square error (WMMSE), block coordinate descent (BCD), majorization-minimization (MM), and penalty duality decomposition (PDD) frameworks. Besides, we present a feasibility characterization method to examine the achievability of EHRs' requirements. Simulation results demonstrate the significant benefits of our proposed solutions. Particularly, the optimized IRS configuration may exhibit higher performance gain than MA counterpart under our considered scenario.

</details>


### [22] [Mutual Coupling in Continuous Aperture Arrays: Physical Modeling and Beamforming Design](https://arxiv.org/abs/2511.11225)
*Zhaolin Wang,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Yuanwei Liu*

Main category: cs.IT

TL;DR: 本文研究了连续孔径阵列中的互耦合现象，建立了包含极化和表面损耗的物理模型，揭示了极化导致各向异性耦合并使传统半波长间距规则失效，提出了两种波束成形优化方法，并将模型扩展到离散阵列。


<details>
  <summary>Details</summary>
Motivation: 研究连续孔径阵列中的互耦合现象，因为传统模型忽略了这种耦合效应，导致性能预测不准确，特别是在考虑极化和表面损耗时。

Method: 1) 建立包含极化和表面损耗的物理模型；2) 通过变分法推导最优波束成形结构；3) 提出核近似法和共轭梯度法解决耦合核求逆问题；4) 将模型扩展到离散阵列。

Result: 1) 耦合离散阵列性能正确收敛到连续孔径极限，而无耦合模型违反物理规律；2) 极化导致阵列增益各向异性行为；3) 耦合波束模式比无耦合波束模式具有更高的方向性。

Conclusion: 互耦合是阵列设计中不可忽视的物理现象，极化效应使耦合呈现各向异性，考虑耦合的阵列设计能获得更好的性能，特别是更高的方向性。

Abstract: The phenomenon of mutual coupling in continuous aperture arrays (CAPAs) is studied. First, a general physical model for the phenomenon that accounts for both polarization and surface dissipation losses is developed. Then, the unipolarized coupling kernel is characterized, revealing that polarization induces anisotropic coupling and invalidates the conventional half-wavelength spacing rule for coupling elimination. Next, the beamforming design problem for CAPAs with coupling is formulated as a functional optimization problem, leading to the derivation of optimal beamforming structures via the calculus of variations. To address the challenge of inverting the coupling kernel in the optimal structure, two methods are proposed: 1) the kernel approximation method, which yields a closed-form solution via wavenumber-domain transformation and GaussLegendre quadrature, and 2) the conjugate gradient method, which addresses an equivalent quadratic functional optimization problem iteratively. Furthermore, the optimal array gain and beampattern are analyzed at the large-aperture limit. Finally, the proposed continuous mutual coupling model is extended to spatially discrete arrays (SPDAs), and comprehensive numerical results are provided, demonstrating that: 1) coupled SPDA performance correctly converges to the CAPA limit, while uncoupled models are shown to violate physics, 2) polarization results in anisotropic array gain behavior, and 3) the coupled beampattern exhibits higher directivity than the uncoupled beampattern.

</details>


### [23] [SCL Decoding of Non-Binary Linear Block Codes](https://arxiv.org/abs/2511.11256)
*Jingyu Lin,Li Chen,Xiaoqian Ye*

Main category: cs.IT

TL;DR: 提出了一种针对特征为2的有限域上的非二进制线性分组码的连续消除列表(SCL)译码方法，通过建立非二进制码字与r个二进制极化码之间的映射关系，实现亚二次复杂度的译码。


<details>
  <summary>Details</summary>
Motivation: 非二进制线性分组码在纠正突发错误方面具有优势，但高效的软判决译码仍然具有挑战性。

Method: 通过建立非二进制码字与r个二进制极化码的一对r映射，执行SCL译码，并提出r步译码路径排序策略。

Result: 在扩展Reed-Solomon码和非二进制扩展BCH码上的仿真表明，SCL译码在更少的有限域算术运算下优于现有最优软判决译码，对于长度为16的eRS码，可以用中等列表大小接近最大似然译码性能。

Conclusion: 所提出的SCL译码方法为非二进制线性分组码提供了一种高效的软判决译码方案，在性能和复杂度之间取得了良好平衡。

Abstract: Non-binary linear block codes (NB-LBCs) are an important class of error-correcting codes that are especially competent in correcting burst errors. They have broad applications in modern communications and storage systems. However, efficient soft-decision decoding of these codes remains challenging. This paper proposes successive cancellation list (SCL) decoding for NB-LBCs that are defined over a finite field of characteristic two, i.e., F_{2^r}, where r is the extension degree. By establishing a one-to-r mapping between the binary composition of each non-binary codeword and r binary polar codewords, SCL decoding of the r polar codes can be performed with a complexity that is sub-quadratic in the codeword length. An r-step decoding path sorting strategy is further proposed to facilitate the decoding. Simulation results on extended Reed-Solomon (eRS) and non-binary extended BCH (NB-eBCH) codes show that SCL decoding can outperform their state-of-the-art soft-decision decoding with fewer finite field arithmetic operations. For length-16 eRS codes, their maximum-likelihood (ML) decoding performances can be approached with a moderate list size.

</details>


### [24] [Joint Optimization for Multi-User Transmissive RIS-MIMO Systems](https://arxiv.org/abs/2511.11495)
*Zhengwei Jiang,Yufeng Zhou,Xusheng Zhu,Wen Chen,Qingqing Wu,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 提出了一种用于透射式可重构智能表面(RIS)多用户MIMO系统的联合优化框架，通过交替优化算法解决RIS系数、接收波束成形和功率分配的联合优化问题，显著提升了系统和速率。


<details>
  <summary>Details</summary>
Motivation: 透射式RIS作为未来无线网络的变革性架构，能够替代传统昂贵基站实现低成本、高能效传输。但在多用户MIMO系统中，RIS系数向量、功率分配和接收波束成形的联合优化面临非凸目标、耦合变量和恒定模约束等挑战。

Method: 将和速率最大化问题重构为可处理等价形式，开发高效的交替优化算法，将问题分解为RIS系数、接收波束成形器和功率分配的子问题，分别使用凸近似和DC规划等先进技术求解。

Result: 仿真结果表明，所提方法收敛速度快，相比传统方案实现了显著的和速率增益，验证了方法的有效性。

Conclusion: 透射式RIS作为下一代无线系统的关键技术具有巨大潜力，所提出的优化框架为其实际应用提供了有效解决方案。

Abstract: Transmissive reconfigurable intelligent surfaces (RIS) represent a transformative architecture for future wireless networks, enabling a paradigm shift from traditional costly base stations to low-cost, energy-efficient transmitters. This paper explores a downlink multi-user MIMO system where a transmissive RIS, illuminated by a single feed antenna, forms the core of the transmitter. The joint optimization of the RIS coefficient vector, power allocation, and receive beamforming in such a system is critical for performance but poses significant challenges due to the non-convex objective, coupled variables, and constant modulus constraints. To address these challenges, we propose a novel optimization framework. Our approach involves reformulating the sum-rate maximization problem into a tractable equivalent form and developing an efficient alternating optimization (AO) algorithm. This algorithm decomposes the problem into subproblems for the RIS coefficients, receive beamformers, and power allocation, each solved using advanced techniques including convex approximation and difference-of-convex programming. Simulation results demonstrate that our proposed method converges rapidly and achieves substantial sum-rate gains over conventional schemes, validating the effectiveness of our approach and highlighting the potential of transmissive RIS as a key technology for next-generation wireless systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [25] [Discounted Cuts: A Stackelberg Approach to Network Disruption](https://arxiv.org/abs/2511.10804)
*Pål Grønås Drange,Fedor V. Fomin,Petr Golovach,Danil Sagunov*

Main category: cs.DS

TL;DR: 本文研究Stackelberg版本的Most Vital Links问题，提出折扣割模型，在一般图上多数变体是NP完全问题，但在有界亏格图上可多项式时间求解。


<details>
  <summary>Details</summary>
Motivation: 研究攻击者与防御者之间的Stackelberg博弈，攻击者移除网络中的边以最大化破坏流量，防御者重新路由剩余流量，旨在连接人工智能、算法博弈论和运筹学领域。

Method: 引入折扣割数学模型，排除k条最昂贵或最便宜的边来评估割的成本，开发统一算法框架分析各种折扣割问题。

Result: 证明在一般图上多数折扣割变体是NP完全的，但在有界亏格图上所有折扣割问题都可多项式时间求解。

Conclusion: 折扣割模型推广了Most Vital Links问题，揭示了新的算法和复杂性理论特性，为实际网络应用提供了理论基础。

Abstract: We study a Stackelberg variant of the classical Most Vital Links problem, modeled as a one-round adversarial game between an attacker and a defender. The attacker strategically removes up to $k$ edges from a flow network to maximally disrupt flow between a source $s$ and a sink $t$, after which the defender optimally reroutes the remaining flow. To capture this attacker--defender interaction, we introduce a new mathematical model of discounted cuts, in which the cost of a cut is evaluated by excluding its $k$ most expensive edges. This model generalizes the Most Vital Links problem and uncovers novel algorithmic and complexity-theoretic properties.
  We develop a unified algorithmic framework for analyzing various forms of discounted cut problems, including minimizing or maximizing the cost of a cut under discount mechanisms that exclude either the $k$ most expensive or the $k$ cheapest edges. While most variants are NP-complete on general graphs, our main result establishes polynomial-time solvability for all discounted cut problems in our framework when the input is restricted to bounded-genus graphs, a relevant class that includes many real-world networks such as transportation and infrastructure networks. With this work, we aim to open collaborative bridges between artificial intelligence, algorithmic game theory, and operations research.

</details>


### [26] [Beating Meet-in-the-Middle for Subset Balancing Problems](https://arxiv.org/abs/2511.10823)
*Tim Randolph,Karol Węgrzycki*

Main category: cs.DS

TL;DR: 本文提出了针对子集平衡问题的首个突破"中间相遇"障碍的最坏情况算法，运行时间为O(|C|^{(0.5-ε)n})，改进了先前仅在平均情况下突破该障碍的结果。


<details>
  <summary>Details</summary>
Motivation: 子集平衡问题推广了子集和、划分和等子集和问题，但现有算法在|C|>2时无法突破O(|C|^{n/2})的时间复杂度瓶颈。本文旨在为多种系数集C设计最坏情况下突破这一"中间相遇"障碍的算法。

Method: 将Howgrave-Graham和Joux的表示技术从平均情况扩展到最坏情况，提出了三种新技术：实现最坏情况输入的"混合"、为不含0的系数集创建灵活输入表示、以及从包含"伪解对"的向量集中快速恢复兼容解对。

Result: 对于C = {-d,...,d}和C = {-d,...,d}\{0}，提出了运行时间为O(|C|^{(0.5-ε)n})的算法，首次在最坏情况下突破中间相遇障碍。同时大幅改进了等子集和问题的最优算法。

Conclusion: 成功将表示技术从平均情况扩展到最坏情况，为多种系数集提供了突破性算法，但仍有两个自然情况(C = {-2,-1,1,2}和C = {-1,1})未能突破中间相遇障碍。

Abstract: We consider exact algorithms for Subset Balancing, a family of related problems that generalizes Subset Sum, Partition, and Equal Subset Sum. Specifically, given as input an integer vector $\vec{x} \in \mathbb{Z}^n$ and a constant-size coefficient set $C \subset \mathbb{Z}$, we seek a nonzero solution vector $\vec{c} \in C^n$ satisfying $\vec{c} \cdot \vec{x} = 0$.
  For $C = \{-d,\ldots,d\}$, $d > 1$ and $C = \{-d,\ldots,d\}\setminus\{0\}$, $d > 2$, we present algorithms that run in time $O(|C|^{(0.5 - ε)n})$ for a constant $ε> 0$ that depends only on $C$. These are the first algorithms that break the $O(|C|^{n/2})$-time ``Meet-in-the-Middle barrier'' for these coefficient sets in the worst case. This improves on the result of Chen, Jin, Randolph and Servedio (SODA 2022), who broke the Meet-in-the-Middle barrier on these coefficient sets in the average-case setting. We also improve the best exact algorithm for Equal Subset Sum (Subset Balancing with $C = \{-1,0,1\}$), due to Mucha, Nederlof, Pawlewicz, and Węgrzycki (ESA 2019), by an exponential margin. This positively answers an open question of Jin, Williams, and Zhang (ESA 2025). Our results leave two natural cases in which we cannot yet break the Meet-in-the-Middle barrier: $C = \{-2, -1, 1, 2\}$ and $C = \{-1, 1\}$ (Partition).
  Our results bring the representation technique of Howgrave-Graham and Joux (CRYPTO 2010) from average-case to worst-case inputs for many $C$. This requires a variety of new techniques: we present strategies for (1) achieving good ``mixing'' with worst-case inputs, (2) creating flexible input representations for coefficient sets without 0, and (3) quickly recovering compatible solution pairs from sets of vectors containing ``pseudosolution pairs''. These techniques may find application to other algorithmic problems on integer sums or be of independent interest.

</details>


### [27] [A number-theoretic conjecture implying faster algorithms for polynomial factorization and integer factorization](https://arxiv.org/abs/2511.10851)
*Chris Umans,Siki Wang*

Main category: cs.DS

TL;DR: 本文提出了一种新的多项式因式分解策略，旨在突破现有的3/2指数障碍，通过数论猜想将多项式因式分解与整数分解问题联系起来。


<details>
  <summary>Details</summary>
Motivation: 当前有限域上单变量多项式因式分解的最快算法时间复杂度为O(n^{3/2+o(1)} polylog q)，作者认为这个3/2指数代表了采用baby-steps-giant-steps策略算法的固有障碍，需要新的策略来突破。

Method: 提出了一种新的策略，基于一个数论猜想：存在大小为n^β的集合S和T，其中元素是至多exp(n^α)的正整数，使得每个整数i∈[n]都能整除某个s-t（s∈S，t∈T）。如果α+β≤1+o(1)且α,β<1/2，就能改进多项式因式分解的3/2指数。

Result: 证明了如果α=β=1/3（最优情况），就能实现单变量多项式因式分解的4/3指数算法。同时，这也会将确定性整数分解算法的最优指数从1/5降低到1/6。

Conclusion: 该研究通过引入新的数论策略，为突破多项式因式分解的3/2指数障碍提供了可能途径，并且揭示了多项式因式分解与整数分解之间的深刻联系。

Abstract: The fastest known algorithm for factoring a degree $n$ univariate polynomial over a finite field $\mathbb{F}_q$ runs in time $O(n^{3/2 + o(1)}\text{polylog } q)$, and there is a reason to believe that the $3/2$ exponent represents a ''barrier'' inherent in algorithms that employ a so-called baby-steps-giant-steps strategy. In this paper, we propose a new strategy with the potential to overcome the $3/2$ barrier. In doing so we are led to a number-theoretic conjecture, one form of which is that there are sets $S, T$ of cardinality $n^β$, consisting of positive integers of magnitude at most $\exp(n^α)$, such that every integer $i \in [n]$ divides $s-t$ for some $s \in S, t \in T$. Achieving $α+ β\le 1 + o(1)$ is trivial; we show that achieving $α, β< 1/2$ (together with an assumption that $S, T$ are structured) implies an improvement to the exponent 3/2 for univariate polynomial factorization. Achieving $α= β= 1/3$ is best-possible and would imply an exponent 4/3 algorithm for univariate polynomial factorization. Interestingly, a second consequence would be a reduction of the current-best exponent for deterministic (exponential) algorithms for factoring integers, from $1/5$ to $1/6$.

</details>


### [28] [Cycle Basis Algorithms for Reducing Maximum Edge Participation](https://arxiv.org/abs/2511.10961)
*Fan Wang,Sandy Irani*

Main category: cs.DS

TL;DR: 本文研究了构建具有低最大边参与度的图循环基问题，这在量子容错中至关重要，因为它直接影响量子门实现的资源开销。


<details>
  <summary>Details</summary>
Motivation: 最大边参与度在量子容错中起着关键作用，它直接影响用于实现几乎通用量子门集的晶格手术程序的开销。

Method: 基于Freedman和Hastings的递归算法，引入了负载感知启发式方法，自适应选择顶点和边以最小化循环基构建过程中的边参与度。

Result: 在随机正则图和源自小型量子码的图上改进了经验性能，分析表明启发式方法的最大负载增长为(log n)^2量级。

Conclusion: 仔细的循环基构造可以在容错量子系统设计中带来显著的实际效益，这个问题在理论上也与图的基数数密切相关。

Abstract: We study the problem of constructing cycle bases of graphs with low maximum edge participation, defined as the maximum number of basis cycles that share any single edge. This quantity, though less studied than total weight or length, plays a critical role in quantum fault tolerance because it directly impacts the overhead of lattice surgery procedures used to implement an almost universal quantum gate set. Building on a recursive algorithm of Freedman and Hastings, we introduce a family of load-aware heuristics that adaptively select vertices and edges to minimize edge participation throughout the cycle basis construction. Our approach improves empirical performance on random regular graphs and on graphs derived from small quantum codes. We further analyze a simplified balls-into-bins process to establish lower bounds on edge participation. While the model differs from the cycle basis algorithm on real graphs, it captures what can be proven for our heuristics without using complex graph theoretic properties related to the distribution of cycles in the graph. Our analysis suggests that the maximum load of our heuristics grows on the order of (log n)^2. Our results indicate that careful cycle basis construction can yield significant practical benefits in the design of fault-tolerant quantum systems. This question also carries theoretical interest, as it is essentially identical to the basis number of a graph, defined as the minimum possible maximum edge participation over all cycle bases.

</details>


### [29] [R-enum Revisited: Speedup and Extension for Context-Sensitive Repeats and Net Frequencies](https://arxiv.org/abs/2511.11057)
*Kotaro Kimura,Tomohiro I*

Main category: cs.DS

TL;DR: 本文改进了r-enum算法，将时间复杂度从O(n log log_w(n/r))提升到O(n)，并扩展了算法以计算其他上下文敏感重复模式，如近超极大重复和超极大重复。同时研究了见证NSMRs的净出现次数，并构建了支持查询的数据结构。


<details>
  <summary>Details</summary>
Motivation: Nishimoto和Tabei提出的r-enum算法在压缩工作空间中枚举特征子串，但其时间复杂度中的O(n log log_w(n/r))项仍有改进空间。本文旨在优化这一时间复杂度，并扩展算法功能以处理更多类型的重复模式。

Method: 基于运行长度编码的BWT(RLBWT)，改进r-enum算法的时间复杂度，扩展算法以计算上下文敏感重复模式，并构建支持净频率查询的数据结构。证明了净出现次数的总数量小于2r。

Result: 成功将r-enum算法的时间复杂度改进为O(n)，扩展了算法功能，构建了在O(r)空间内支持净频率查询的数据结构，并获得了净出现次数和最小唯一子串数量的新上界2r。

Conclusion: 本文显著改进了字符串特征子串枚举算法的效率，扩展了其应用范围，并为重复模式分析提供了新的理论界限和实用工具。

Abstract: Nishimoto and Tabei [CPM, 2021] proposed r-enum, an algorithm to enumerate various characteristic substrings, including maximal repeats, in a string $T$ of length $n$ in $O(r)$ words of compressed working space, where $r \le n$ is the number of runs in the Burrows-Wheeler transform (BWT) of $T$. Given the run-length encoded BWT (RLBWT) of $T$, r-enum runs in $O(n\log\log_{w}(n/r))$ time in addition to the time linear to the number of output strings, where $w=Θ(\log n)$ is the word size. In this paper, we improve the $O(n\log\log_{w}(n/r))$ term to $O(n)$. We also extend r-enum to compute other context-sensitive repeats such as near-supermaximal repeats (NSMRs) and supermaximal repeats, and the context diversity for every maximal repeat in the same complexities. Furthermore, we study the occurrences that witness NSMRs, which have recently attracted attention under the name of net occurrences: An occurrence of a repeat is called a net occurrence if it is not covered by another repeat, and the net frequency of a repeat is the number of its net occurrences. With this terminology, an NSMR is defined to be a repeat with a positive net frequency. Given the RLBWT of $T$, we show how to compute the set $S^{nsmr}$ of all NSMRs in $T$ together with their net frequency/occurrences in $O(n)$ time and $O(r)$ space. We also show that an $O(r)$-space data structure can be built from the RLBWT to support queries of computing the net frequency of any query pattern $P$ in $O(|P|)$ time. The data structure is built in $O(r)$ space and in $O(n)$ time with high probability or deterministic $O(n+|S^{nsmr}|\log\log\min(σ,|S^{nsmr}|))$ time, where $σ\le r$ is the alphabet size of $T$. To achieve this, we prove that the total number of net occurrences is less than $2r$. We also get a new upper bound $2r$ of the number of minimal unique substrings in $T$, which may be of independent interest.

</details>


### [30] [An Efficient Algorithm for Minimizing Ordered Norms in Fractional Load Balancing](https://arxiv.org/abs/2511.11237)
*Daniel Blankenburg,Antonia Ellerbrock,Thomas Kesselheim,Jens Vygen*

Main category: cs.DS

TL;DR: 提出了一种随机算法，用于最小化负载向量的有序范数，该算法通过资源定价机制和动态成本预算实现高效近似求解。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要针对ℓ∞范数，无法扩展到任意有序范数。需要开发新方法来解决更广泛的有序范数最小化问题。

Method: 使用基于跟随正则化领导者范式的资源定价机制，通过有序范数的平滑近似和动态成本预算来控制更新步长，结合非均匀采样和鞅论保证迭代进展。

Result: 算法以高概率在O((n+d)(ε⁻²+loglog d)log(n+d))次oracle调用内计算得到(1+ε)-近似解。

Conclusion: 该方法成功扩展到任意有序范数，所提出的平滑近似具有非平凡稳定性，动态预算机制和非均匀更新是关键技术创新。

Abstract: We study the problem of minimizing an ordered norm of a load vector (indexed by a set of $d$ resources), where a finite number $n$ of customers $c$ contribute to the load of each resource by choosing a solution $x_c$ in a convex set $X_c \subseteq \mathbb{R}^d_{\geq 0}$; so we minimize $||\sum_{c}x_c||$ for some fixed ordered norm $||\cdot||$. We devise a randomized algorithm that computes a $(1+\varepsilon)$-approximate solution to this problem and makes, with high probability, $\mathcal{O}((n+d) (\varepsilon^{-2}+\log\log d)\log (n+d))$ calls to oracles that minimize linear functions (with non-negative coefficients) over $X_c$. While this has been known for the $\ell_{\infty}$ norm via the multiplicative weights update method, existing proof techniques do not extend to arbitrary ordered norms. Our algorithm uses a resource price mechanism that is motivated by the follow-the-regularized-leader paradigm, and is expressed by smooth approximations of ordered norms. We need and show that these have non-trivial stability properties, which may be of independent interest. For each customer, we define dynamic cost budgets, which evolve throughout the algorithm, to determine the allowed step sizes. This leads to non-uniform updates and may even reject certain oracle solutions. Using non-uniform sampling together with a martingale argument, we can guarantee sufficient expected progress in each iteration, and thus bound the total number of oracle calls with high probability.

</details>


### [31] [Improved Differentially Private Algorithms for Rank Aggregation](https://arxiv.org/abs/2511.11319)
*Quentin Hillebrand,Pasin Manurangsi,Vorapong Suppakitpaisarn,Phanu Vajanopath*

Main category: cs.DS

TL;DR: 本文改进了Kemeny排序聚合问题的差分隐私PTAS算法，并首次研究了footrule排序聚合问题的差分隐私保护方法。


<details>
  <summary>Details</summary>
Motivation: Alabi等人的工作提出了Kemeny排序聚合问题的差分隐私PTAS和5-近似算法，但存在较大的加性误差。本文旨在改进这些算法的加性误差，并首次研究footrule排序聚合问题的差分隐私保护。

Method: 提出了改进的差分隐私PTAS算法，加性误差更小；针对footrule排序聚合问题给出了接近最优的算法；作为推论，获得了Kemeny排序聚合问题的2-近似算法。

Result: 在中心模型中获得了加性误差更小的DP PTAS；首次实现了footrule排序聚合问题的差分隐私保护；获得了与Alabi等人5-近似算法相同加性误差的2-近似算法。

Conclusion: 本文显著改进了Kemeny排序聚合问题的差分隐私算法性能，并开创性地解决了footrule排序聚合问题的差分隐私保护，为排序聚合问题的隐私保护提供了更优的解决方案。

Abstract: Rank aggregation is a task of combining the rankings of items from multiple users into a single ranking that best represents the users' rankings. Alabi et al. (AAAI'22) presents differentially-private (DP) polynomial-time approximation schemes (PTASes) and $5$-approximation algorithms with certain additive errors for the Kemeny rank aggregation problem in both central and local models. In this paper, we present improved DP PTASes with smaller additive error in the central model. Furthermore, we are first to study the footrule rank aggregation problem under DP. We give a near-optimal algorithm for this problem; as a corollary, this leads to 2-approximation algorithms with the same additive error as the $5$-approximation algorithms of Alabi et al. for the Kemeny rank aggregation problem in both central and local models.

</details>


### [32] [Learning and Testing Convex Functions](https://arxiv.org/abs/2511.11498)
*Renato Ferreira Pinto,Cassandra Marcussen,Elchanan Mossel,Shivam Nadimpalli*

Main category: cs.DS

TL;DR: 本文研究了高斯空间下实值凸函数的学习和测试问题，给出了在Lipschitz平滑性假设下的学习算法和测试器，并提供了样本复杂度的上下界。


<details>
  <summary>Details</summary>
Motivation: 虽然函数凸性在数学、统计学和计算机科学中被广泛研究，但其可学习性和可测试性主要在离散或受限设置中研究，且通常使用不适合实值函数的汉明距离。本文旨在研究高斯空间下高维实值凸函数的学习和测试问题。

Method: 假设函数具有Lipschitz平滑性，在标准高斯测度下进行学习。提出了不可知论的正确学习算法，以及容忍性（双侧）测试器和单侧测试器。

Result: 学习算法使用n^O(1/ε²)样本达到误差ε，并在CSQ模型中给出了n^poly(1/ε)样本的下界。测试器具有相同的样本复杂度，单侧测试器使用O(√n/ε)^n样本。

Conclusion: 在高斯空间下，Lipschitz凸函数可以在多项式样本复杂度下学习和测试，平滑性假设是必要的，即使在单维情况下也是如此。

Abstract: We consider the problems of \emph{learning} and \emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.
  In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:
  - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\varepsilon$ using $n^{O(1/\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\mathrm{poly}(1/\varepsilon)}$ samples in the \emph{correlational statistical query (CSQ)} model.
  - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\sqrt{n}/\varepsilon)^n$ samples.

</details>


### [33] [Faster MAX-CUT on Bounded Threshold Rank Graphs](https://arxiv.org/abs/2511.11499)
*Prashanti Anderson,Samuel B. Hopkins,Amit Rajaraman,David Steurer*

Main category: cs.DS

TL;DR: 提出了在有限阈值秩图上近似2CSP问题的新算法，运行时间在1/ε上是多项式，在标签扩展图的阈值秩上是指数级，在输入规模上是近线性的。


<details>
  <summary>Details</summary>
Motivation: 现有算法在阈值秩和1/ε上运行时间是指数级的，需要开发更高效的近似算法来处理有限阈值秩图上的2CSP问题。

Method: 结合子空间枚举和半定规划的简单而新颖的方法，利用标签扩展图与基础图阈值秩之间的比较不等式。

Result: 获得了首个在有限阈值秩图上MAX-CUT的(1+O(ε))近似算法，运行时间为poly(1/ε)，并将2CSP的运行时间从输入规模的多项式改进到近线性。

Conclusion: 该算法为有限阈值秩图上的2CSP问题提供了高效的近似解决方案，显著改进了现有算法的运行时间。

Abstract: We design new algorithms for approximating 2CSPs on graphs with bounded threshold rank, that is, whose normalized adjacency matrix has few eigenvalues larger than $\varepsilon$, smaller than $-\varepsilon$, or both. Unlike on worst-case graphs, 2CSPs on bounded threshold rank graphs can be $(1+O(\varepsilon))$-approximated efficiently. Prior approximation algorithms for this problem run in time exponential in the threshold rank and $1/\varepsilon$. Our algorithm has running time which is polynomial in $1/\varepsilon$ and exponential in the threshold rank of the label-extended graph, and near-linear in the input size. As a consequence, we obtain the first $(1+O(\varepsilon))$ approximation for MAX-CUT on bounded threshold rank graphs running in $\mathrm{poly}(1/\varepsilon)$ time. We also improve the state-of-the-art running time for 2CSPs on bounded threshold-rank graphs from polynomial in input size to near-linear via a new comparison inequality between the threshold rank of the label-extended graph and base graph. Our algorithm is a simple yet novel combination of subspace enumeration and semidefinite programming.

</details>
