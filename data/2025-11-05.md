<div id=toc></div>

# Table of Contents

- [cs.MM](#cs.MM) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.IT](#cs.IT) [Total: 15]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.DB](#cs.DB) [Total: 8]


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1] [An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM](https://arxiv.org/abs/2511.02234)
*Jiawei Liu,Enis Berk Çoban,Zarina Schevchenko,Hao Tang,Zhigang Zhu,Michael I Mandel,Johanna Devaney*

Main category: cs.MM

TL;DR: 研究了在音频多模态大语言模型中使用交错指令调优的效果，发现交错提示能提升音频语义推理性能，但会降低音频标注能力


<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型训练方法只是简单地将非文本信息与文本提示拼接，可能无法实现模态的深度融合，限制了模型利用核心语言模型推理能力的能力

Method: 在音频MLLM中使用交错指令调优，将音频令牌交错插入提示中，使用新创建的SHARD音频推理基准进行实验

Result: 即使零样本交错提示也能改善推理任务性能，少量交错训练提示的微调能进一步改善结果，但会牺牲MLLM的音频标注能力

Conclusion: 交错指令调优能有效提升音频多模态大语言模型的语义推理能力，但需要在推理性能和标注能力之间权衡

Abstract: Standard training for Multi-modal Large Language Models (MLLMs) involves
concatenating non-textual information, like vision or audio, with a text
prompt. This approach may not encourage deep integration of modalities,
limiting the model's ability to leverage the core language model's reasoning
capabilities. This work examined the impact of interleaved instruction tuning
in an audio MLLM, where audio tokens are interleaved within the prompt. Using
the Listen, Think, and Understand (LTU) model as a testbed, we conduct an
experiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our
newly created reasoning benchmark for audio-based semantic reasoning focusing
on synonym and hypernym recognition. Our findings show that while even
zero-shot interleaved prompting improves performance on our reasoning tasks, a
small amount of fine-tuning using interleaved training prompts improves the
results further, however, at the expense of the MLLM's audio labeling ability.

</details>


### [2] [Wireless Video Semantic Communication with Decoupled Diffusion Multi-frame Compensation](https://arxiv.org/abs/2511.02478)
*Bingyan Xie,Yongpeng Wu,Yuxuan Shi,Biqian Feng,Wenjun Zhang,Jihong Park,Tony Quek*

Main category: cs.MM

TL;DR: 提出了一种无线视频语义通信框架WVSC-D，在语义层面而非像素层面进行视频编码，通过解耦扩散多帧补偿机制提高带宽效率。


<details>
  <summary>Details</summary>
Motivation: 现有无线视频传输方案直接在像素层面进行视频编码，忽略了视频中包含的内在语义信息，导致通信效率不高。

Method: WVSC-D框架首先将原始视频帧编码为语义帧，然后基于这种紧凑表示进行视频编码。引入参考语义帧替代传统运动向量，在接收端通过两阶段条件扩散过程生成补偿语义帧。

Result: 实验结果表明，WVSC-D相比其他基于深度学习的方法（如DVSC）在PSNR指标上提升了约1.8 dB。

Conclusion: 该语义通信框架通过语义级视频编码和扩散补偿机制，在保证视频传输性能的同时显著提高了带宽效率。

Abstract: Existing wireless video transmission schemes directly conduct video coding in
pixel level, while neglecting the inner semantics contained in videos. In this
paper, we propose a wireless video semantic communication framework with
decoupled diffusion multi-frame compensation (DDMFC), abbreviated as WVSC-D,
which integrates the idea of semantic communication into wireless video
transmission scenarios. WVSC-D first encodes original video frames as semantic
frames and then conducts video coding based on such compact representations,
enabling the video coding in semantic level rather than pixel level. Moreover,
to further reduce the communication overhead, a reference semantic frame is
introduced to substitute motion vectors of each frame in common video coding
methods. At the receiver, DDMFC is proposed to generate compensated current
semantic frame by a two-stage conditional diffusion process. With both the
reference frame transmission and DDMFC frame compensation, the bandwidth
efficiency improves with satisfying video transmission performance.
Experimental results verify the performance gain of WVSC-D over other DL-based
methods e.g. DVSC about 1.8 dB in terms of PSNR.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [3] [Solving cold start in news recommendations: a RippleNet-based system for large scale media outlet](https://arxiv.org/abs/2511.02052)
*Karol Radziszewski,Michał Szpunar,Piotr Ociepka,Mateusz Buczyński*

Main category: cs.IR

TL;DR: 基于RippleNet的可扩展推荐系统实现，针对媒体领域，在波兰大型在线媒体平台Onet.pl上部署，通过集成基于内容的项目嵌入解决冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决媒体推荐系统中新发布内容的冷启动问题，为Onet.pl平台提供有效的推荐解决方案。

Method: 将基于内容的项目嵌入集成到RippleNet的知识传播机制中，使用Amazon SageMaker进行分布式训练和推理，Apache Airflow编排数据管道和模型重训练工作流。

Result: 构建了包含用户和项目特征的综合黄金数据集，以及独立的交互表，实现了对新项目的有效评分。

Conclusion: 该系统架构支持灵活扩展和新信号的集成，为媒体推荐系统提供了可扩展的解决方案。

Abstract: We present a scalable recommender system implementation based on RippleNet,
tailored for the media domain with a production deployment in Onet.pl, one of
Poland's largest online media platforms. Our solution addresses the cold-start
problem for newly published content by integrating content-based item
embeddings into the knowledge propagation mechanism of RippleNet, enabling
effective scoring of previously unseen items. The system architecture leverages
Amazon SageMaker for distributed training and inference, and Apache Airflow for
orchestrating data pipelines and model retraining workflows. To ensure
high-quality training data, we constructed a comprehensive golden dataset
consisting of user and item features and a separate interaction table, all
enabling flexible extensions and integration of new signals.

</details>


### [4] [Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion](https://arxiv.org/abs/2511.02113)
*Hai-Dang Kieu,Min Xu,Thanh Trung Huynh,Dung D. Le*

Main category: cs.IR

TL;DR: VLIF是一个基于视觉语言模型和信息论的融合框架，通过细粒度视觉描述生成和信息感知融合来提升多模态推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐方法依赖粗糙的视觉特征和不受控的融合，导致冗余或不对齐的表征，视觉编码器难以捕捉与物品相关的显著语义。

Method: 使用VLM生成细粒度的标题引导描述来转换产品图像，并基于部分信息分解(PID)设计信息感知融合模块来解耦冗余和协同信号。

Result: 在三个Amazon数据集上的实验表明，VLIF持续优于最新的多模态基线方法，并显著增强了视觉特征的贡献。

Conclusion: VLIF通过视觉语义对齐和信息理论指导的融合，有效提升了多模态推荐的表征质量和性能。

Abstract: Recent advances in multimodal recommendation (MMR) have shown that
incorporating rich content sources such as images and text can lead to
significant gains representation quality. However, existing methods often rely
on coarse visual features and uncontrolled fusion, leading to redundant or
misaligned representations. As a result, visual encoders often fail to capture
salient, item-relevant semantics, limiting their contribution in multimodal
fusion. From an information-theoretic perspective, effective fusion should
balance the unique, shared, and redundant information across modalities,
preserving complementary cues while avoiding correlation bias. This paper
presents VLIF, a vision-language and information-theoretic fusion framework
that enhances multimodal recommendation through two key components. (i) A
VLM-based visual enrichment module generates fine-grained, title-guided
descriptions to transform product images into semantically aligned
representations. (ii) An information-aware fusion module, inspired by Partial
Information Decomposition (PID), disentangles redundant and synergistic signals
across modalities for controlled integration. Experiments on three Amazon
datasets demonstrate that VLIF consistently outperforms recent multimodal
baselines and substantially strengthens the contribution of visual features.

</details>


### [5] [KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain Recommendation](https://arxiv.org/abs/2511.02181)
*Yuhan Wang,Qing Xie,Zhifeng Bao,Mengzi Tang,Lin Li,Yongjian Liu*

Main category: cs.IR

TL;DR: KGBridge是一个基于知识图谱的提示学习框架，用于非重叠用户场景下的跨域序列推荐，通过关系级语义建模和两阶段训练解决KG稀疏性、用户重叠依赖和知识解耦问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的跨域推荐方法在非重叠用户场景下面临三个主要挑战：对KG稀疏性和流行度偏差敏感、依赖重叠用户进行域对齐、缺乏可迁移知识和领域特定知识的显式解耦。

Method: 提出KGBridge框架，包含KG增强提示编码器（将关系级语义建模为软提示）和两阶段训练范式（跨域预训练+隐私保护微调），通过关系感知语义控制和对应驱动解耦来分离领域共享和特定语义。

Result: 在基准数据集上的广泛实验表明，KGBridge始终优于最先进的基线方法，并在不同KG稀疏度下保持鲁棒性，有效缓解了KG增强跨域推荐中的结构不平衡和语义纠缠问题。

Conclusion: KGBridge通过结构化动态先验和显式知识解耦，实现了在非重叠用户场景下稳定有效的跨域知识迁移，为KG增强的跨域推荐提供了新思路。

Abstract: Knowledge Graphs (KGs), as structured knowledge bases that organize
relational information across diverse domains, provide a unified semantic
foundation for cross-domain recommendation (CDR). By integrating symbolic
knowledge with user-item interactions, KGs enrich semantic representations,
support reasoning, and enhance model interpretability. Despite this potential,
existing KG-based methods still face major challenges in CDR, particularly
under non-overlapping user scenarios. These challenges arise from: (C1)
sensitivity to KG sparsity and popularity bias, (C2) dependence on overlapping
users for domain alignment and (C3) lack of explicit disentanglement between
transferable and domain-specific knowledge, which limit effective and stable
knowledge transfer. To this end, we propose KGBridge, a knowledge-guided prompt
learning framework for cross-domain sequential recommendation under
non-overlapping user scenarios. KGBridge comprises two core components: a
KG-enhanced Prompt Encoder, which models relation-level semantics as soft
prompts to provide structured and dynamic priors for user sequence modeling
(addressing C1), and a Two-stage Training Paradigm, which combines cross-domain
pretraining and privacy-preserving fine-tuning to enable knowledge transfer
without user overlap (addressing C2). By combining relation-aware semantic
control with correspondence-driven disentanglement, KGBridge explicitly
separates and balances domain-shared and domain-specific semantics, thereby
maintaining complementarity and stabilizing adaptation during fine-tuning
(addressing C3). Extensive experiments on benchmark datasets demonstrate that
KGBridge consistently outperforms state-of-the-art baselines and remains robust
under varying KG sparsity, highlighting its effectiveness in mitigating
structural imbalance and semantic entanglement in KG-enhanced cross-domain
recommendation.

</details>


### [6] [Average Precision at Cutoff k under Random Rankings: Expectation and Variance](https://arxiv.org/abs/2511.02571)
*Tetiana Manzhos,Tetiana Ianevych,Olga Melnyk*

Main category: cs.IR

TL;DR: 本文推导了AP@k的期望和方差，为MAP@k评估指标提供随机基准，帮助更可靠地解释推荐系统和信息检索平台的排名质量。


<details>
  <summary>Details</summary>
Motivation: 推荐系统和信息检索平台依赖排名算法向用户展示最相关项目，需要可靠的评估指标。MAP@k被广泛使用，但需要了解其随机基准来正确解释评估结果。

Method: 推导了AP@k的期望和方差，覆盖了离线和在线两种广泛使用的评估模型。期望建立随机基准，方差量化随机波动程度。

Result: 得到了AP@k的期望和方差表达式，为MAP@k提供了随机基准参考值，能够区分真实性能提升和随机波动。

Conclusion: AP@k的期望和方差为MAP@k评估提供了重要的基准参考，有助于更可靠地解释排名算法的实际性能表现。

Abstract: Recommender systems and information retrieval platforms rely on ranking
algorithms to present the most relevant items to users, thereby improving
engagement and satisfaction. Assessing the quality of these rankings requires
reliable evaluation metrics. Among them, Mean Average Precision at cutoff k
(MAP@k) is widely used, as it accounts for both the relevance of items and
their positions in the list.
  In this paper, the expectation and variance of Average Precision at k (AP@k)
are derived since they can be used as biselines for MAP@k. Here, we covered two
widely used evaluation models: offline and online. The expectation establishes
the baseline, indicating the level of MAP@k that can be achieved by pure
chance. The variance complements this baseline by quantifying the extent of
random fluctuations, enabling a more reliable interpretation of observed
scores.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [7] [Fibbinary-Based Compression and Quantization for Efficient Neural Radio Receivers](https://arxiv.org/abs/2511.01921)
*Roberta Fiandaca,Manil Dev Gomony*

Main category: cs.IT

TL;DR: 本文提出量化与压缩策略来降低神经接收器的复杂度，包括均匀/非均匀量化和新型无损压缩算法，在保持性能的同时显著减少计算成本和内存占用。


<details>
  <summary>Details</summary>
Motivation: 神经接收器性能优异但网络复杂度高、计算成本大，难以在硬件受限设备上部署，需要优化策略来解决这一问题。

Method: 引入均匀和非均匀量化（如斐波那契码字量化），提出细粒度增量网络量化策略补偿量化损失，并开发两种新型无损压缩算法压缩冗余的斐波那契量化参数。

Result: 量化技术节省45%乘法器功耗和44%面积，结合压缩使内存占用减少63.4%，同时性能仍优于传统接收器。

Conclusion: 所提出的量化与压缩策略能有效降低神经接收器的硬件需求，使其更适合在资源受限设备上部署。

Abstract: Neural receivers have shown outstanding performance compared to the
conventional ones but this comes with a high network complexity leading to a
heavy computational cost. This poses significant challenges in their deployment
on hardware-constrained devices. To address the issue, this paper explores two
optimization strategies: quantization and compression. We introduce both
uniform and non-uniform quantization such as the Fibonacci Code word
Quantization (FCQ). A novel fine-grained approach to the Incremental Network
Quantization (INQ) strategy is then proposed to compensate for the losses
introduced by the above mentioned quantization techniques. Additionally, we
introduce two novel lossless compression algorithms that effectively reduce the
memory size by compressing sequences of Fibonacci quantized parameters
characterized by a huge redundancy. The quantization technique provides a
saving of 45\% and 44\% in the multiplier's power and area, respectively, and
its combination with the compression determines a 63.4\% reduction in memory
footprint, while still providing higher performances than a conventional
receiver.

</details>


### [8] [Analysis of Beam Misalignment Effect in Inter-Satellite FSO Links](https://arxiv.org/abs/2511.02189)
*Minje Kim,Hongjae Nam,Beomsoo Ko,Hyeongjun Park,Hwanjin Kim,Dong-Hyun Jung,Junil Choi*

Main category: cs.IT

TL;DR: 本文研究了自由空间光通信中光束未对准对卫星间链路的影响，提出了联合抖动和未对准误差的累积分布函数闭式表达式，并开发了截断CDF公式和二分算法来高效计算中断概率。


<details>
  <summary>Details</summary>
Motivation: 自由空间光通信在卫星间链路中具有高数据速率、低功耗和低干扰等优势，但其性能对光束未对准高度敏感。虽然通常采用指向提前角补偿，但该方法的有效性依赖于精确的轨道知识和先进的对准硬件，这在实践中并不总是可行。

Method: 推导了联合抖动和未对准引起的指向误差下FSO信道的累积分布函数闭式表达式，引入了截断CDF公式和二分算法来高效计算中断概率，并基于轨道动力学量化位移。

Result: 数值结果表明，所提出的模型与蒙特卡洛模拟结果高度吻合，验证了模型的有效性。

Conclusion: 所提出的模型在实际中对于设计卫星间FSO系统非常有用，能够保证收敛且计算开销最小。

Abstract: Free-space optical (FSO) communication has emerged as a promising technology
for inter-satellite links (ISLs) due to its high data rate, low power
consumption, and reduced interference. However, the performance of
inter-satellite FSO systems is highly sensitive to beam misalignment. While
pointing-ahead angle (PAA) compensation is commonly employed, the effectiveness
of PAA compensation depends on precise orbital knowledge and advanced alignment
hardware, which are not always feasible in practice. To address this challenge,
this paper investigates the impact of beam misalignment on inter-satellite FSO
communication. We derive a closed-form expression for the cumulative
distribution function (CDF) of the FSO channel under the joint jitter and
misalignment-induced pointing error, and introduce a truncated CDF formulation
with a bisection algorithm to efficiently compute outage probabilities with
guaranteed convergence and minimal computational overhead. To make the analysis
more practical, we quantify displacement based on orbital dynamics. Numerical
results demonstrate that the proposed model closely matches Monte Carlo
simulations, making the proposed model highly useful to design inter-satellite
FSO systems in practice.

</details>


### [9] [Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning](https://arxiv.org/abs/2511.02216)
*Hyemin Yu,Hong-Chuan Yang*

Main category: cs.IT

TL;DR: 提出了一种基于双智能体强化学习的自适应传输算法DRL-CoLA，用于两跳中继通信系统，在严格延迟约束下实现近最优可靠性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线通信系统需要支持超可靠低延迟通信(URLLC)服务，特别是在两跳协作通信中满足严格的URLLC要求具有挑战性。

Method: 将每跳传输参数配置建模为马尔可夫决策过程，提出双智能体强化学习算法DRL-CoLA，分布式学习延迟感知传输策略，自适应配置参数包括numerology、mini-slot大小和调制编码方案。

Result: 仿真结果表明，所提算法在满足严格延迟要求的同时实现了近最优的可靠性。

Conclusion: DRL-CoLA算法能够有效解决两跳中继通信系统中的URLLC传输挑战，为下一代无线通信系统提供了可行的解决方案。

Abstract: Next-generation wireless communication systems must support ultra-reliable
low-latency communication (URLLC) service for mission-critical applications.
Meeting stringent URLLC requirements is challenging, especially for two-hop
cooperative communication. In this paper, we develop an adaptive transmission
design for a two-hop relaying communication system. Each hop transmission
adaptively configures its transmission parameters separately, including
numerology, mini-slot size, and modulation and coding scheme, for reliable
packet transmission within a strict latency constraint. We formulate the
hop-specific transceiver configuration as a Markov decision process (MDP) and
propose a dual-agent reinforcement learning-based cooperative latency-aware
transmission (DRL-CoLA) algorithm to learn latency-aware transmission policies
in a distributed manner. Simulation results verify that the proposed algorithm
achieves the near-optimal reliability while satisfying strict latency
requirements.

</details>


### [10] [Revisiting Wireless-Powered MEC: A Cooperative Energy Recycling Framework for Task-Energy Co-Design](https://arxiv.org/abs/2511.02284)
*Haohao Qin,Bowen Gu,Xianhua Yu,Hao Xie,Yongjun Xu,Qihao Li,Liejun Wang*

Main category: cs.IT

TL;DR: 提出了一种支持协同能量回收的MEC框架，通过优化计算通信协同设计，在能量、延迟和功率约束下最大化用户最小可计算数据量


<details>
  <summary>Details</summary>
Motivation: 协同能量回收为无线供电MEC网络提供了提升能量利用率的新途径，但其与计算通信协同设计的结合尚未充分探索

Method: 通过松弛、最大比合并和变量替换将难解问题重构为凸形式，利用拉格朗日对偶和交替优化推导闭式解

Result: 仿真结果表明，所提出的CER机制显著增加了总可计算数据量，同时在异构用户间保持了公平性能

Conclusion: 该框架成功集成了协同能量回收与MEC系统，提供了分析性见解，并验证了在提升系统性能和公平性方面的有效性

Abstract: Cooperative energy recycling (CER) offers a new way to boost energy
utilization in wireless-powered multi-access edge computing (MEC) networks, yet
its integration with computation-communication co-design remains underexplored.
This paper proposes a CER-enabled MEC framework that maximizes the minimum
computable data among users under energy causality, latency, and power
constraints. The intractable problem is reformulated into a convex form through
relaxation, maximum ratio combining, and variable substitution, and closed-form
solutions are derived via Lagrangian duality and alternating optimization,
offering analytical insights. Simulation results verify that the proposed CER
mechanism markedly increases total computable data while maintaining equitable
performance across heterogeneous users.

</details>


### [11] [Fairness-Aware Computation Offloading in Wireless-Powered MEC Systems with Cooperative Energy Recycling](https://arxiv.org/abs/2511.02287)
*Haohao Qin,Bowen Gu,Dong Li,Xianhua Yu,Liejun Wang,Yuanwei Liu,Sumei Sun*

Main category: cs.IT

TL;DR: 本文研究了无线供电移动边缘计算系统中的协作能量回收技术，通过联合优化本地计算和计算卸载，在满足能量、延迟和任务大小约束的同时，平衡总可计算数据和用户公平性。


<details>
  <summary>Details</summary>
Motivation: 传统架构仅依赖专用电源，而本文通过让无线传感器从对等传输中回收能量来提高系统性能，解决资源变量耦合和公平性正则化带来的非凸性问题。

Method: 采用变量替换技术将问题转化为凸结构，利用拉格朗日对偶和交替优化高效求解，并为三种代表性公平性机制推导闭式解。

Result: 数值结果验证了所提CER框架的有效性，在吞吐量和适应性方面相比基准方案有显著提升，可调alpha公平机制在不同场景下提供灵活的性能-公平性权衡控制。

Conclusion: 协作能量回收技术能够显著提升无线供电移动边缘计算系统的性能，可调公平性机制为系统设计提供了灵活性。

Abstract: In this paper, cooperative energy recycling (CER) is investigated in
wireless-powered mobile edge computing systems. Unlike conventional
architectures that rely solely on a dedicated power source, wireless sensors
are additionally enabled to recycle energy from peer transmissions. To evaluate
system performance, a joint computation optimization problem is formulated that
integrates local computing and computation offloading, under an alpha-fairness
objective that balances total computable data and user fairness while
satisfying energy, latency, and task size constraints. Due to the inherent
non-convexity introduced by coupled resource variables and fairness
regularization, a variable-substitution technique is employed to transform the
problem into a convex structure, which is then efficiently solved using
Lagrangian duality and alternating optimization. To characterize the
fairness-efficiency tradeoff, closed-form solutions are derived for three
representative regimes: zero fairness, common fairness, and max-min fairness,
each offering distinct system-level insights. Numerical results validate the
effectiveness of the proposed CER-enabled framework, demonstrating significant
gains in throughput and adaptability over benchmark schemes. The tunable alpha
fairness mechanism provides flexible control over performance-fairness
trade-offs across diverse scenarios.

</details>


### [12] [Downlink Channel Estimation for mmWave Systems with Impulsive Interference](https://arxiv.org/abs/2511.02291)
*Kwonyeol Park,Gyoseung Lee,Hyeongtaek Lee,Hwanjin Kim,Junil Choi*

Main category: cs.IT

TL;DR: 提出了一种基于变分推理的贝叶斯信道估计技术，用于解决毫米波MIMO系统中因硬件非理想性或外部干扰引起的脉冲干扰问题。


<details>
  <summary>Details</summary>
Motivation: 毫米波MIMO系统在信道估计过程中面临脉冲干扰的挑战，这种干扰具有突发性、不可预测性和高功率特性，严重影响信道估计精度。

Method: 基于变分推理的贝叶斯信道估计技术，利用毫米波信道在角域中的稀疏性和脉冲干扰的间歇性特性，采用均值场近似进行后验推断，并将变分推理集成到稀疏贝叶斯学习框架中。

Result: 仿真结果表明，所提出的技术在信道估计精度方面优于基线方法。

Conclusion: 该变分推理方法能有效处理毫米波MIMO系统中的脉冲干扰问题，提高信道估计性能。

Abstract: In this paper, we investigate a channel estimation problem in a downlink
millimeter-wave (mmWave) multiple-input multiple-output (MIMO) system, which
suffers from impulsive interference caused by hardware non-idealities or
external disruptions. Specifically, impulsive interference presents a
significant challenge to channel estimation due to its sporadic, unpredictable,
and high-power nature. To tackle this issue, we develop a Bayesian channel
estimation technique based on variational inference (VI) that leverages the
sparsity of the mmWave channel in the angular domain and the intermittent
nature of impulsive interference to minimize channel estimation errors. The
proposed technique employs mean-field approximation to approximate posterior
inference and integrates VI into the sparse Bayesian learning (SBL) framework.
Simulation results demonstrate that the proposed technique outperforms
baselines in terms of channel estimation accuracy.

</details>


### [13] [Two-Parameter Rényi Information Quantities with Applications to Privacy Amplification and Soft Covering](https://arxiv.org/abs/2511.02297)
*Shi-Bing Li,Ke Li,Lei Yu*

Main category: cs.IT

TL;DR: 本文提出了一个双参数Rényi条件熵和双参数Rényi互信息的统一框架，建立了它们的基本性质，并应用于隐私放大和软覆盖问题的强逆指数分析。


<details>
  <summary>Details</summary>
Motivation: Rényi条件熵和Rényi互信息在文献中缺乏普遍接受的定义，不同应用场景下提出了多种定义。本文旨在建立一个统一的框架来研究这些信息量。

Method: 通过参数变换，将双参数Rényi条件熵与Hayashi和Tan的定义联系起来，并作为量子Rényi条件熵的经典特例。同时引入新的双参数Rényi互信息来统一三种常用变体。

Result: 建立了双参数Rényi条件熵的参数单调性和变分表达式，证明了双参数Rényi互信息的非负性、可加性、数据处理不等式、参数单调性、变分表达式以及凸凹性。

Conclusion: 这些双参数Rényi信息量能够刻画在α∈(0,∞)阶Rényi散度下的隐私放大和软覆盖问题的强逆指数。

Abstract: There are no universally accepted definitions of R\'enyi conditional entropy
and R\'enyi mutual information, although motivated by different applications,
several definitions have been proposed in the literature. In this paper, we
consider a family of two-parameter R\'enyi conditional entropy and a family of
two-parameter R\'enyi mutual information. By performing a change of variables
for the parameters, the two-parameter R\'enyi conditional entropy we study
coincides precisely with the definition introduced by Hayashi and Tan [IEEE
Trans. Inf. Theory, 2016], and it also emerges naturally as the classical
specialization of the three-parameter quantum R\'enyi conditional entropy
recently put forward by Rubboli, Goodarzi, and Tomamichel [arXiv:2410.21976
(2024)]. We establish several fundamental properties of the two-parameter
R\'enyi conditional entropy, including monotonicity with respect to the
parameters and variational expression. The associated two-parameter R\'enyi
mutual information considered in this paper is new and it unifies three
commonly used variants of R\'enyi mutual information. For this quantity, we
prove several important properties, including the non-negativity, additivity,
data processing inequality, monotonicity with respect to the parameters,
variational expression, as well as convexity and concavity. Finally, we
demonstrate that these two-parameter R\'enyi information quantities can be used
to characterize the strong converse exponents in privacy amplification and soft
covering problems under R\'enyi divergence of order $\alpha \in (0, \infty)$.

</details>


### [14] [Anomaly Detection-Based UE-Centric Inter-Cell Interference Suppression](https://arxiv.org/abs/2511.02320)
*Kwonyeol Park,Hyuckjin Choi,Beomsoo Ko,Minje Kim,Gyoseung Lee,Daecheol Kwon,Hyunjae Park,Byungseung Kim,Min-Ho Shin,Junil Choi*

Main category: cs.IT

TL;DR: 提出了一种基于用户设备的干扰抑制方案，使用Z-refined深度支持向量数据描述技术进行异常检测，有效检测并抑制小区间干扰，在有限训练资源下优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着频谱复用增加，邻区干扰导致系统性能显著下降，需要开发有效的干扰抑制方案来提升整体系统性能。

Method: 采用基于单类分类的异常检测技术，提出Z-refined深度支持向量数据描述方法，检测小区间干扰并应用干扰白化来减轻干扰。

Result: 数值结果表明，在有限时频训练资源下，该方案在干扰检测性能上优于多种基线方法，且与理想辅助干扰抑制方案性能相当。商用5G调制解调器芯片组的实验验证了该方案在多种3GPP标准信道环境下的性能提升。

Conclusion: 所提出的用户设备中心干扰抑制方案能有效检测和减轻小区间干扰，在真实5G环境中表现出良好的性能改进。

Abstract: The increasing spectral reuse can cause significant performance degradation
due to interference from neighboring cells. In such scenarios, developing
effective interference suppression schemes is necessary to improve overall
system performance. To tackle this issue, we propose a novel user
equipment-centric interference suppression scheme, which effectively detects
inter-cell interference (ICI) and subsequently applies interference whitening
to mitigate ICI. The proposed scheme, named Z-refined deep support vector data
description, exploits a one-class classification-based anomaly detection
technique. Numerical results verify that the proposed scheme outperforms
various baselines in terms of interference detection performance with limited
time or frequency resources for training and is comparable to the performance
based on an ideal genie-aided interference suppression scheme. Furthermore, we
demonstrate through test equipment experiments using a commercial
fifth-generation modem chipset that the proposed scheme shows performance
improvements across various 3rd generation partnership project standard channel
environments, including tapped delay line-A, -B, and -C models.

</details>


### [15] [$\mathbb{F}_q\mathbb{F}_{q^2}$-additive cyclic codes and their Gray images](https://arxiv.org/abs/2511.02325)
*Ankit Yadav,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 研究有限域FqFq2上的加法循环码，构造满足Singleton界的码，通过Gray映射获得最优线性码，并从F3F9加法码得到最优三元LCD码


<details>
  <summary>Details</summary>
Motivation: 研究有限域FqFq2上的加法循环码的性质和构造方法，旨在获得满足Singleton界的最优码和线性互补对偶码

Method: 确定生成多项式和最小生成集，构造满足Singleton界的Fq2加法循环码，使用Gray映射，从F3F9加法码获得最优三元LCD码

Result: 成功构造了满足Singleton界的Fq2加法循环码，通过Gray映射获得了最优线性码，并得到了最优三元LCD码

Conclusion: 有限域FqFq2上的加法循环码可以构造出满足Singleton界的最优码，并能通过适当映射获得最优线性码和LCD码

Abstract: We investigate additive cyclic codes over the alphabet
$\mathbb{F}_{q}\mathbb{F}_{q^2}$, where $q$ is a prime power. First, its
generator polynomials and minimal spanning set are determined. Then, examples
of $\mathbb{F}_{q^2}$-additive cyclic codes that satisfy the well-known
Singleton bound are constructed. Using a Gray map, we produce certain optimal
linear codes over $\mathbb{F}_{3}$. Finally, we obtain a few optimal ternary
linear complementary dual (LCD) codes from
$\mathbb{F}_{3}\mathbb{F}_{9}$-additive codes.

</details>


### [16] [Generalized informational functionals and new monotone measures of statistical complexity](https://arxiv.org/abs/2511.02502)
*Razvan Gabriel Iagar,David Puertas-Centeno*

Main category: cs.IT

TL;DR: 本文引入了双参数变换族，扩展了上下变换，定义了新的信息泛函（下矩和累积上矩），建立了与经典信息度量（矩、Rényi熵、Shannon熵、Fisher信息）的新不等式，并定义了统计复杂度的新度量。


<details>
  <summary>Details</summary>
Motivation: 扩展上下变换理论，引入新的信息泛函来连接概率分布的矩和熵，探索信息度量之间的深层数学结构。

Method: 通过双参数变换族定义下矩和累积上矩，建立这些新泛函与经典信息度量的不等式关系，并分析最小化密度函数。

Result: 获得了新的尖锐不等式，给出了最优界和最小化密度（用广义三角函数表示），定义了统计复杂度新度量并证明了单调性。

Conclusion: 新变换揭示了信息泛函之间复杂的结构关系，为信息理论提供了新的数学工具和深刻见解。

Abstract: In this paper we introduce a biparametric family of transformations which can
be seen as an extension of the so-called up and down transformations. This new
class of transformations allows to us to introduce new informational
functionals, which we have called \textit{down-moments} and \textit{cumulative
upper-moments}. A remarkable fact is that the down-moments provide, in some
cases, an interpolation between the $p$-th moments and the power R\'enyi
entropies of a probability density. We establish new and sharp inequalities
relating these new functionals to the classical informational measures such as
moments, R\'enyi and Shannon entropies and Fisher information measures. We also
give the optimal bounds as well as the minimizing densities, which are in some
cases expressed in terms of the generalized trigonometric functions. We
furthermore define new classes of measures of statistical complexity obtained
as quotients of the new functionals, and establish monotonicity properties for
them through an algebraic conjugation of up and down transformations. All of
these properties highlight an intricate structure of functional inequalities.

</details>


### [17] [Improved AntiGriesmer Bounds for Linear Anticodes and Applications](https://arxiv.org/abs/2511.02519)
*Guanghui Zhang,Bocong Chen,Liren Lin,Hongwei Liu*

Main category: cs.IT

TL;DR: 本文改进了Chen和Xie先前建立的线性反码的antiGriesmer界，取消了码长限制并将对偶码最小距离条件从≥3放宽到≥2，证明了对任何[n,k]_q线性反码，当d(C^⊥)≥2时，n ≤ ∑_{i=0}^{k-1} ⌊δ/q^i⌋成立。


<details>
  <summary>Details</summary>
Motivation: 改进现有的线性反码antiGriesmer界，扩大其适用范围，为线性反码的研究提供更全面的理论框架。

Method: 通过数学证明方法，在更宽松的条件下推导出改进的antiGriesmer不等式，并推导出多个推论。

Result: 成功证明了更一般的antiGriesmer界，取消了码长n < q^{k-1}的限制，将对偶码最小距离要求从≥3降低到≥2。

Conclusion: 新界显著扩展了antiGriesmer界的适用性，为线性码的构造和分类提供了更强大的工具，统一并扩展了早期研究成果。

Abstract: This paper improves the antiGriesmer bound for linear anticodes previously
established by Chen and Xie (Journal of Algebra, 673 (2025) 304-320). While the
original bound required the code length to satisfy $n < q^{k-1}$ and the dual
code to have minimum distance at least 3, our main result removes the length
restriction and relaxes the dual distance condition to at least 2.
Specifically, we prove that for any $[n,k]_q$ linear anticode $\mathcal{C}$
over $\mathbb{F}_q$ with diameter $\delta$ and $d(\mathcal{C}^\perp) \geq 2$,
the inequality \[ n \leq \sum_{i=0}^{k-1} \left\lfloor \frac{\delta}{q^i}
\right\rfloor \] holds. This generalization significantly broadens the
applicability of the antiGriesmer bound. We derive several corollaries,
including lower bounds on the diameter $\delta$ in terms of $n$ and $k$, upper
bounds on the code length $n$, and constraints on the dimension $k$.
Applications to the construction and classification of linear codes with few
weights are also discussed, along with examples demonstrating that our new
bound can be sharper than previous ones. Our work unifies and extends earlier
findings, providing a more comprehensive framework for studying linear
anticodes and their properties.

</details>


### [18] [Performance Analysis of Single-Antenna Fluid Antenna Systems via Extreme Value Theory](https://arxiv.org/abs/2511.02572)
*Rui Xu,Yinghui Ye,Xiaoli Chu,Guangyue Lu,Kai-Kit Wong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出了一个基于极值分布的性能评估框架，用于分析完全相关瑞利衰落下的单天线流体天线系统，通过Gumbel分布和广义极值分布建模，推导了中断概率和遍历容量的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 在单天线流体天线系统中，由于缺乏闭式的FAS信道分布，在完全相关衰落条件下推导准确且易处理的性能表达式具有挑战性。

Method: 使用极值分布建模FAS信道，首先采用Gumbel分布近似，然后扩展到广义极值分布，通过最大似然准则确定参数，推导闭式的中断概率和遍历容量表达式。

Result: 仿真结果表明，基于GEV的框架比Gumbel模型具有更高的精度，两种EVD方法都为相关衰落条件下的FAS性能评估提供了计算高效且分析易处理的工具。

Conclusion: 提出的基于极值分布的框架为完全相关瑞利衰落下的流体天线系统性能评估提供了准确且易处理的解决方案。

Abstract: In single-antenna fluid antenna systems (FASs), the transceiver dynamically
selects the antenna port with the strongest instantaneous channel to enhance
link reliability. However, deriving accurate yet tractable performance
expressions under fully correlated fading remains challenging, primarily due to
the absence of a closed-form distribution for the FAS channel. To address this
gap, this paper develops a novel performance evaluation framework for FAS
operating under fully correlated Rayleigh fading, by modeling the FAS channel
through extreme value distributions (EVDs). We first justify the suitability of
EVD modeling and approximate the FAS channel through the Gumbel distribution,
with parameters expressed as functions of the number of ports and the antenna
aperture size via the maximum likelihood (ML) criterion. Closed-form
expressions for the outage probability (OP) and ergodic capacity (EC) are then
derived. While the Gumbel model provides an excellent fit, minor deviations
arise in the extreme-probability regions. To further improve accuracy, we
extend the framework using the generalized extreme value (GEV) distribution and
obtain closed-form OP and EC approximations based on ML-derived parameters.
Simulation results confirm that the proposed GEV-based framework achieves
superior accuracy over the Gumbel-based model, while both EVD-based approaches
offer computationally efficient and analytically tractable tools for evaluating
the performance of FAS under realistic correlated fading conditions.

</details>


### [19] [Redundancy Maximization as a Principle of Associative Memory Learning](https://arxiv.org/abs/2511.02584)
*Mark Blümel,Andreas C. Schneider,Valentin Neuhaus,David A. Ehrlich,Marcel Graetz,Michael Wibral,Abdullah Makkeh,Viola Priesemann*

Main category: cs.IT

TL;DR: 该论文通过部分信息分解(PID)分析Hopfield网络，发现冗余信息最大化能显著提升网络记忆容量，提出基于信息论目标的新联想记忆设计原则。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield网络的局部计算原理尚不完全清楚，需要从信息理论角度理解联想记忆的信息处理机制。

Method: 应用部分信息分解(PID)框架分析Hopfield网络中单个神经元的信息处理，将冗余信息最大化作为学习目标进行优化。

Result: 优化冗余信息后，网络记忆容量从0.14提升到1.59，比经典Hopfield网络提升超过10倍，甚至优于最新的先进实现。

Conclusion: 冗余最大化是联想记忆的新设计原则，为基于信息论目标的新型联想记忆模型开辟了道路。

Abstract: Associative memory, traditionally modeled by Hopfield networks, enables the
retrieval of previously stored patterns from partial or noisy cues. Yet, the
local computational principles which are required to enable this function
remain incompletely understood. To formally characterize the local information
processing in such systems, we employ a recent extension of information theory
- Partial Information Decomposition (PID). PID decomposes the contribution of
different inputs to an output into unique information from each input,
redundant information across inputs, and synergistic information that emerges
from combining different inputs. Applying this framework to individual neurons
in classical Hopfield networks we find that below the memory capacity, the
information in a neuron's activity is characterized by high redundancy between
the external pattern input and the internal recurrent input, while synergy and
unique information are close to zero until the memory capacity is surpassed and
performance drops steeply. Inspired by this observation, we use redundancy as
an information-theoretic learning goal, which is directly optimized for each
neuron, dramatically increasing the network's memory capacity to 1.59, a more
than tenfold improvement over the 0.14 capacity of classical Hopfield networks
and even outperforming recent state-of-the-art implementations of Hopfield
networks. Ultimately, this work establishes redundancy maximization as a new
design principle for associative memories and opens pathways for new
associative memory models based on information-theoretic goals.

</details>


### [20] [Optimal Source Coding of Markov Chains for Real-Time Remote Estimation](https://arxiv.org/abs/2511.02803)
*Ismail Cosandal,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究马尔可夫链在传输时间和状态转移时间尺度相同条件下的源编码问题，提出基于MDP的最优编码策略，相比Huffman基准策略能减少平均传输时长。


<details>
  <summary>Details</summary>
Motivation: 传统源编码假设传输时间远小于状态转移时间，但在某些场景中传输时间和马尔可夫链状态转移发生在同一时间尺度，需要新的编码策略来最小化平均传输时长。

Method: 将问题建模为马尔可夫决策过程(MDP)，通过增强符号及其传输时长来构建状态空间，设计最优源编码策略。

Result: 在随机生成的马尔可夫过程中，提出的最优策略相比Huffman基准策略能够降低平均传输时长，性能增益取决于马尔可夫过程参数。

Conclusion: 在传输时间和状态转移时间尺度相同的场景下，基于MDP的最优源编码策略能够有效减少平均传输时长，优于传统的Huffman编码方法。

Abstract: We revisit the source coding problem for a Markov chain under the assumption
that the transmission times and how fast the Markov chain transitions its state
happen at the same time-scale. Specifically, we assume that the transmission of
each bit takes a single time slot, and the Markov chain updates its state in
the same time slot. Thus, the length of the codeword assigned to a symbol
determines the number of non-transmitted symbols, as well as, the probability
of the realization of the next symbol to be transmitted. We aim to minimize the
average transmission duration over an infinite horizon by proposing an optimal
source coding policy based on the last transmitted symbol and its transmission
duration. To find the optimal policy, we formulate the problem with a Markov
decision process (MDP) by augmenting the symbols alongside the transmission
duration of the symbols. Finally, we analyze two Huffman-based benchmark
policies and compare their performances with the proposed optimal policy. We
observe that, in randomly generated processes, our proposed optimal policy
decreases the average transmission duration compared to benchmark policies. The
performance gain varies based on the parameters of the Markov process.

</details>


### [21] [A Construction of Infinite Families of Self-Orthogonal Quasi-Cyclic Codes Using Constituent Codes.pdf](https://arxiv.org/abs/2511.02813)
*Gustavo Terra Bastos,Angelynn Álvarez,Cameron Williams*

Main category: cs.IT

TL;DR: 本文提出了一种构造无限族准循环码的方法，这些码关于欧几里得和埃尔米特内积是自正交的，并利用CSS构造得到了具有良好参数的量子纠错码。


<details>
  <summary>Details</summary>
Motivation: 准循环码最近被用于构造量子纠错码，本文旨在构建无限族的自正交准循环码，以改进量子码的参数。

Method: 通过定义在域扩展上的构成码来计算准循环码的维度和最小距离下界，并利用CSS构造从自正交码得到量子码。

Result: 构造的准循环码满足平方根状的最小距离下界，并且可以从构造中得到自对偶准循环码，由此产生的量子纠错码具有良好的参数。

Conclusion: 提出的方法能够系统性地构造无限族自正交准循环码，这些码可用于生成具有良好参数的量子纠错码。

Abstract: Quasi-cyclic codes have been recently employed in the constructions of
quantum error-correcting codes. In this paper, we propose a construction of
infinite families of quasi-cyclic codes which are self-orthogonal with respect
to the Euclidean and Hermitian inner products. In particular, their dimension
and a lower bound for their minimum distance are computed using their
constituent codes defined over field extensions of $\mathbb{F}_q$. We also show
that the lower bound for the minimum distance satisfies the square-root-like
lower bound and also show how self-dual quasi-cyclic codes can arise from our
construction. Using the CSS construction, we show the existence of quantum
error-correcting codes with good parameters.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [22] [Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games](https://arxiv.org/abs/2511.02157)
*Asrin Efe Yorulmaz,Tamer Başar*

Main category: cs.GT

TL;DR: 本文改进了马尔可夫博弈中粗相关均衡的收敛速率，从O(log⁵T/T)提升到O(logT/T)，达到与相关均衡相同的最佳收敛速率，并在高维设置中实现指数级改进。


<details>
  <summary>Details</summary>
Motivation: 无遗憾学习动态在博弈论中至关重要，能够实现粗相关均衡和相关均衡的分散式收敛。现有方法在马尔可夫博弈中的收敛速率不够理想，需要改进。

Method: 基于自适应步长技术，通过阶段式方案调整学习率，使用乐观跟随正则化领导者算法，定制化用于基于价值迭代的学习。

Result: 实现了O(logT/T)的收敛速率，比之前的最佳速率O(log⁵T/T)有显著改进，同时将动作集大小的依赖从多项式改进为多对数。

Conclusion: 提出的自博弈算法在马尔可夫博弈中实现了已知最快的粗相关均衡收敛速率，在高维设置中带来指数级收益。

Abstract: No-regret learning dynamics play a central role in game theory, enabling
decentralized convergence to equilibrium for concepts such as Coarse Correlated
Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the
convergence rate to CCE in general-sum Markov games, reducing it from the
previously best-known rate of $\mathcal{O}(\log^5 T / T)$ to a sharper
$\mathcal{O}(\log T / T)$. This matches the best known convergence rate for CE
in terms of $T$, number of iterations, while also improving the dependence on
the action set size from polynomial to polylogarithmic-yielding exponential
gains in high-dimensional settings. Our approach builds on recent advances in
adaptive step-size techniques for no-regret algorithms in normal-form games,
and extends them to the Markovian setting via a stage-wise scheme that adjusts
learning rates based on real-time feedback. We frame policy updates as an
instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for
value-iteration-based learning. The resulting self-play algorithm achieves, to
our knowledge, the fastest known convergence rate to CCE in Markov games.

</details>


### [23] [Human-AI Collaboration with Misaligned Preferences](https://arxiv.org/abs/2511.02746)
*Jiaxin Song,Parnian Shahkar,Kate Donahue,Bhaskar Ray Chaudhury*

Main category: cs.GT

TL;DR: 本文研究算法作为助手协助人类决策的场景，发现有时与不完全对齐的算法合作反而比与完全对齐的算法合作带来更高的人类效用。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，算法常作为助手帮助人类从大量选项中筛选出较小子集供最终选择，但人类偏好存在噪声且算法与人类偏好可能不完全对齐。

Method: 通过建模和理论分析，研究算法与人类在偏好不完全对齐情况下的协作机制。

Result: 研究发现人类从不对齐算法（犯不同错误）获得的效用高于从对齐算法获得的效用。

Conclusion: 研究结果对算法工具设计者和政策制定者有重要启示，需要重新思考算法对齐的最优策略。

Abstract: In many real-life settings, algorithms play the role of assistants, while
humans ultimately make the final decision. Often, algorithms specifically act
as curators, narrowing down a wide range of options into a smaller subset that
the human picks between: consider content recommendation or chatbot responses
to questions with multiple valid answers. Crucially, humans may not know their
own preferences perfectly either, but instead may only have access to a noisy
sampling over preferences. Algorithms can assist humans by curating a smaller
subset of items, but must also face the challenge of misalignment: humans may
have different preferences from each other (and from the algorithm), and the
algorithm may not know the exact preferences of the human they are facing at
any point in time. In this paper, we model and theoretically study such a
setting. Specifically, we show instances where humans benefit by collaborating
with a misaligned algorithm. Surprisingly, we show that humans gain more
utility from a misaligned algorithm (which makes different mistakes) than from
an aligned algorithm. Next, we build on this result by studying what properties
of algorithms maximize human welfare when the goals could be either utilitarian
welfare or ensuring all humans benefit. We conclude by discussing implications
for designers of algorithmic tools and policymakers.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [24] [Disjoint Paths in Expanders in Deterministic Almost-Linear Time via Hypergraph Perfect Matching](https://arxiv.org/abs/2511.02214)
*Matija Bucić,Zhongtian He,Shang-En Huang,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: 提出了在扩展图中寻找短边不相交路径的高效确定性算法，改进了之前仅适用于常数传导率扩展图的较慢算法。


<details>
  <summary>Details</summary>
Motivation: 之前确定性多项式时间算法仅适用于常数传导率的扩展图且速度较慢，需要开发更高效、更通用的算法来处理扩展图中的边不相交路径问题。

Method: 使用超图完美匹配的几乎线性时间算法，基于Haxell(1995)的Hall型条件的推广，该框架之前只有大多项式时间算法。

Result: 在满足特定条件下，算法能确定性计算边不相交路径：(1) 路径长度最多18log(n)/φ，总时间mn^(1+o(1))min{k, φ^(-1)}；(2) 路径长度最多n^(o(1))/φ，总时间m^(1+o(1))。

Conclusion: 该工作显著改进了扩展图中边不相交路径问题的确定性算法效率，并为超图完美匹配提供了几乎线性时间算法，具有广泛的应用价值。

Abstract: We design efficient deterministic algorithms for finding short edge-disjoint
paths in expanders. Specifically, given an $n$-vertex $m$-edge expander $G$ of
conductance $\phi$ and minimum degree $\delta$, and a set of pairs
$\{(s_i,t_i)\}_i$ such that each vertex appears in at most $k$ pairs, our
algorithm deterministically computes a set of edge-disjoint paths from $s_i$ to
$t_i$, one for every $i$: (1) each of length at most $18 \log (n)/\phi$ and in
$mn^{1+o(1)}\min\{k, \phi^{-1}\}$ total time, assuming $\phi^3\delta\ge (35\log
n)^3 k$, or (2) each of length at most $n^{o(1)}/\phi$ and in total
$m^{1+o(1)}$ time, assuming $\phi^3 \delta \ge n^{o(1)} k$. Before our work,
deterministic polynomial-time algorithms were known only for expanders with
constant conductance and were significantly slower. To obtain our result, we
give an almost-linear time algorithm for \emph{hypergraph perfect matching}
under generalizations of Hall-type conditions (Haxell 1995), a powerful
framework with applications in various settings, which until now has only
admitted large polynomial-time algorithms (Annamalai 2018).

</details>


### [25] [Fast Approximation Algorithm for Non-Monotone DR-submodular Maximization under Size Constraint](https://arxiv.org/abs/2511.02254)
*Tan D. Tran,Canh V. Pham*

Main category: cs.DS

TL;DR: 提出了两种算法FastDrSub和FastDrSub++来解决带尺寸约束的非单调DR-子模最大化问题，实现了常数近似比和O(n log k)的低查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决非单调DR-子模最大化问题，该问题在收益最大化等应用中很重要，但现有方法在查询复杂度或近似比方面存在不足。

Method: 设计了两种算法：FastDrSub提供0.044近似比，FastDrSub++改进到1/4-ε近似比，两者都保持O(n log k)的查询复杂度。

Result: 实验结果表明，提出的算法在查询复杂度和解质量方面显著优于现有方法，特别是在收益最大化问题上表现优异。

Conclusion: 这是第一个实现常数近似比且具有O(n log k)低查询复杂度的算法，为DR-子模最大化问题提供了高效实用的解决方案。

Abstract: This work studies the non-monotone DR-submodular Maximization over a ground
set of $n$ subject to a size constraint $k$. We propose two approximation
algorithms for solving this problem named FastDrSub and FastDrSub++. FastDrSub
offers an approximation ratio of $0.044$ with query complexity of $O(n
\log(k))$. The second one, FastDrSub++, improves upon it with a ratio of
$1/4-\epsilon$ within query complexity of $(n \log k)$ for an input parameter
$\epsilon >0$. Therefore, our proposed algorithms are the first constant-ratio
approximation algorithms for the problem with the low complexity of $O(n
\log(k))$.
  Additionally, both algorithms are experimentally evaluated and compared
against existing state-of-the-art methods, demonstrating their effectiveness in
solving the Revenue Maximization problem with DR-submodular objective function.
The experimental results show that our proposed algorithms significantly
outperform existing approaches in terms of both query complexity and solution
quality.

</details>


### [26] [Learning CNF formulas from uniform random solutions in the local lemma regime](https://arxiv.org/abs/2511.02487)
*Weiming Feng,Xiongxin Yang,Yixiao Yu,Yiyao Zhang*

Main category: cs.DS

TL;DR: 该论文改进了Valiant算法，显著降低了从均匀随机解中学习k-CNF公式的样本复杂度，从O(n^k)降低到O(log n)或Õ(n^exp(-√k))，并建立了新的信息论下界。


<details>
  <summary>Details</summary>
Motivation: 研究从独立同分布的均匀随机解中学习k-CNF公式的问题，这等价于学习具有k-wise硬约束的布尔马尔可夫随机场，目标是显著降低先前O(n^k)的高样本复杂度。

Method: 重新审视Valiant算法，在Lovász局部引理类型条件下学习有界子句交集大小的k-CNF，以及在可满足性阈值附近的随机k-CNF。

Result: 实现了显著改进：在有界子句交集大小条件下仅需O(log n)样本，在随机k-CNF条件下仅需Õ(n^exp(-√k))样本，远优于先前的O(n^k)。

Conclusion: 该工作显著推进了从随机解中学习布尔约束系统的理论，建立了改进的样本复杂度上界和新的信息论下界。

Abstract: We study the problem of learning a $n$-variables $k$-CNF formula $\Phi$ from
its i.i.d. uniform random solutions, which is equivalent to learning a Boolean
Markov random field (MRF) with $k$-wise hard constraints. Revisiting Valiant's
algorithm (Commun. ACM'84), we show that it can exactly learn (1) $k$-CNFs with
bounded clause intersection size under Lov\'asz local lemma type conditions,
from $O(\log n)$ samples; and (2) random $k$-CNFs near the satisfiability
threshold, from $\widetilde{O}(n^{\exp(-\sqrt{k})})$ samples. These results
significantly improve the previous $O(n^k)$ sample complexity. We further
establish new information-theoretic lower bounds on sample complexity for both
exact and approximate learning from i.i.d. uniform random solutions.

</details>


### [27] [A Simple and Fast $(3+\varepsilon)$-approximation for Constrained Correlation Clustering](https://arxiv.org/abs/2511.02705)
*Nate Veldt*

Main category: cs.DS

TL;DR: 本文改进了约束相关聚类问题的近似算法，将近似比从16降至3+ε，同时保持O(n³)的时间复杂度，通过新的覆盖线性规划和Pivot算法实现。


<details>
  <summary>Details</summary>
Motivation: Fischer等人最近提出了一个16-近似的O(n³)算法，本文旨在解决他们提出的开放问题，即在保持相同时间复杂度的同时显著降低近似比。

Method: 使用新的覆盖线性规划方法，通过组合快速近似求解，并应用熟悉的Pivot算法到辅助图进行舍入。对于只有友好或敌对约束的实例，开发了更简单的算法。

Result: 成功设计出时间复杂度为O(n³)、近似比为(3+ε)的算法，显著改进了之前的16-近似结果。

Conclusion: 本文解决了约束相关聚类中的一个重要开放问题，在保持算法简单性的同时实现了近似比的显著改进，为只有特定类型约束的实例提供了更简单的解决方案。

Abstract: In Constrained Correlation Clustering, the goal is to cluster a complete
signed graph in a way that minimizes the number of negative edges inside
clusters plus the number of positive edges between clusters, while respecting
hard constraints on how to cluster certain friendly or hostile node pairs.
Fischer et al. [FKKT25a] recently developed a $\tilde{O}(n^3)$-time
16-approximation algorithm for this problem. We settle an open question posed
by these authors by designing an algorithm that is equally fast but brings the
approximation factor down to $(3+\varepsilon)$ for arbitrary constant
$\varepsilon > 0$. Although several new algorithmic steps are needed to obtain
our improved approximation, our approach maintains many advantages in terms of
simplicity. In particular, it relies mainly on rounding a (new) covering linear
program, which can be approximated quickly and combinatorially. Furthermore,
the rounding step amounts to applying the very familiar Pivot algorithm to an
auxiliary graph. Finally, we develop much simpler algorithms for instances that
involve only friendly or only hostile constraints.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [28] [An Experimental Comparison of Alternative Techniques for Event-Log Augmentation](https://arxiv.org/abs/2511.01896)
*Alessandro Padella,Francesco Vinci,Massimiliano de Leoni*

Main category: cs.DB

TL;DR: 本文评估了7种最先进的事件日志增强技术，并与基于随机转移系统的基线技术进行比较。结果显示，结合资源队列建模的随机转移系统技术能生成更高质量的合成事件日志。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘依赖于大型事件日志，但获取足够数据存在限制。事件日志增强通过生成额外的真实过程执行轨迹来解决这一问题，但缺乏对现有技术有效性的全面比较。

Method: 在8个事件日志上评估7种最先进的增强技术，并与基于随机转移系统的基线技术进行比较。从相似性、预测信息保留、信息损失/增强和计算时间四个维度进行分析。

Result: 结果显示，结合资源队列建模的随机转移系统技术在多个标准下表现最佳。事件日志增强技术相比传统数据增强技术具有显著优势，后者无法考虑过程约束。

Conclusion: 基于随机转移系统结合资源队列建模的事件日志增强技术能生成更高质量的合成事件日志，为过程挖掘提供有效的数据增强解决方案。

Abstract: Process mining analyzes and improves processes by examining transactional
data stored in event logs, which record sequences of events with timestamps.
However, the effectiveness of process mining, especially when combined with
machine or deep learning, depends on having large event logs. Event log
augmentation addresses this limitation by generating additional traces that
simulate realistic process executions while considering various perspectives
like time, control-flow, workflow, resources, and domain-specific attributes.
Although prior research has explored event-log augmentation techniques, there
has been no comprehensive comparison of their effectiveness. This paper reports
on an evaluation of seven state-of-the-art augmentation techniques across eight
event logs. The results are also compared with those obtained by a baseline
technique based on a stochastic transition system. The comparison has been
carried on analyzing four different aspects: similarity, preservation of
predictive information, information loss/enhancement, and computational times
required. Results show that, considering the different criteria, a technique
based on a stochastic transition system combined with resource queue modeling
would provide higher quality synthetic event logs. Event-log augmentation
techniques are also compared with traditional data-augmentation techniques,
showing that the former provide significant benefits, whereas the latter fail
to consider process constraints.

</details>


### [29] [Towards Defect Phase Diagrams: From Research Data Management to Automated Workflows](https://arxiv.org/abs/2511.01942)
*Khalil Rejiba,Sang-Hyeok Lee,Christina Gasper,Martina Freund,Sandra Korte-Kerzel,Ulrich Kerzel*

Main category: cs.DB

TL;DR: 该论文提出了一种综合研究数据管理基础设施，用于构建缺陷相图，通过整合异构实验和模拟数据来支持材料设计研究。


<details>
  <summary>Details</summary>
Motivation: 构建缺陷相图需要系统整合来自不同研究组和地点的异构实验与模拟数据，研究数据管理是实现这一目标的关键使能技术。

Method: 建立综合RDM基础设施，包括：联合电子实验室笔记本和实验室信息管理系统、易用的大对象数据存储、自动元数据提取、交互式溯源图、自动化报告和分析工作流，核心是openBIS系统及其配套应用。

Result: 该集成方法减少了数据捕获和整理的摩擦，实现了可追溯和可重用的数据集，加速了跨机构缺陷相图的构建。

Conclusion: 综合研究数据管理基础设施成功解决了异构数据源和格式的挑战，为材料设计研究提供了有效的数据管理解决方案。

Abstract: Defect phase diagrams provide a unified description of crystal defect states
for materials design and are central to the scientific objectives of the
Collaborative Research Centre (CRC) 1394. Their construction requires the
systematic integration of heterogeneous experimental and simulation data across
research groups and locations. In this setting, research data management (RDM)
is a key enabler of new scientific insight by linking distributed research
activities and making complex data reproducible and reusable.
  To address the challenge of heterogeneous data sources and formats, a
comprehensive RDM infrastructure has been established that links experiment,
data, and analysis in a seamless workflow. The system combines: (1) a joint
electronic laboratory notebook and laboratory information management system,
(2) easy-to-use large-object data storage, (3) automatic metadata extraction
from heterogeneous and proprietary file formats, (4) interactive provenance
graphs for data exploration and reuse, and (5) automated reporting and analysis
workflows. The two key technological elements are the openBIS electronic
laboratory notebook and laboratory information management system, and a newly
developed companion application that extends openBIS with large-scale data
handling, automated metadata capture, and federated access to distributed
research data.
  This integrated approach reduces friction in data capture and curation,
enabling traceable and reusable datasets that accelerate the construction of
defect phase diagrams across institutions.

</details>


### [30] [InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations](https://arxiv.org/abs/2511.02002)
*Xiangru Jian,Zhengyuan Dong,M. Tamer Özsu*

Main category: cs.DB

TL;DR: InteracSPARQL是一个交互式SPARQL查询生成和优化系统，通过自然语言解释和LLM技术帮助非专家用户理解并改进SPARQL查询。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询语言语法复杂，需要理解复杂数据结构，对非专家用户构成挑战。

Method: 结合LLM和基于规则的方法，从SPARQL抽象语法树生成结构化解释，再通过LLM进行语言优化，支持用户交互式反馈和LLM驱动的自优化。

Result: 在标准基准测试中，相比基线方法，在查询准确性、解释清晰度和用户满意度方面都有显著提升。

Conclusion: 结合基于规则方法和LLM驱动的优化能够创建更易访问和稳健的SPARQL接口。

Abstract: In recent years, querying semantic web data using SPARQL has remained
challenging, especially for non-expert users, due to the language's complex
syntax and the prerequisite of understanding intricate data structures. To
address these challenges, we propose InteracSPARQL, an interactive SPARQL query
generation and refinement system that leverages natural language explanations
(NLEs) to enhance user comprehension and facilitate iterative query refinement.
InteracSPARQL integrates LLMs with a rule-based approach to first produce
structured explanations directly from SPARQL abstract syntax trees (ASTs),
followed by LLM-based linguistic refinements. Users can interactively refine
queries through direct feedback or LLM-driven self-refinement, enabling the
correction of ambiguous or incorrect query components in real time. We evaluate
InteracSPARQL on standard benchmarks, demonstrating significant improvements in
query accuracy, explanation clarity, and overall user satisfaction compared to
baseline approaches. Our experiments further highlight the effectiveness of
combining rule-based methods with LLM-driven refinements to create more
accessible and robust SPARQL interfaces.

</details>


### [31] [Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements](https://arxiv.org/abs/2511.02062)
*Yuting Yang,Tiancheng Yuan,Jamal Hashim,Thiago Garrett,Jeffrey Qian,Ann Zhang,Yifan Wang,Weijia Song,Ken Birman*

Main category: cs.DB

TL;DR: Vortex是一个面向SLO（服务水平目标）的ML推理服务平台，相比现有平台（如TorchServe和Ray Serve）能显著降低延迟并提高稳定性，特别是在高负载和RDMA环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着ML推理和知识检索服务需求的增长，特别是AI代理和终端用户应用的集成，对服务延迟SLO的要求日益严格。现有ML服务平台通过批处理优化吞吐量，但面临不可预测的尾部延迟问题。

Method: Vortex采用SLO优先的方法，构建专门的流水线架构来优化延迟性能，支持RDMA网络以进一步提升性能。

Result: 在相同任务下，Vortex相比TorchServe和Ray Serve在各种工作负载下实现了显著更低且更稳定的延迟，通常能在两倍以上的请求速率下满足给定的SLO目标。当使用RDMA时，Vortex的优势更加明显。

Conclusion: Vortex证明了SLO优先方法在ML推理服务中的有效性，能够为AI应用提供更可靠、低延迟的服务保障。

Abstract: There is growing interest in deploying ML inference and knowledge retrieval
as services that could support both interactive queries by end users and more
demanding request flows that arise from AIs integrated into a end-user
applications and deployed as agents. Our central premise is that these latter
cases will bring service level latency objectives (SLOs). Existing ML serving
platforms use batching to optimize for high throughput, exposing them to
unpredictable tail latencies. Vortex enables an SLO-first approach. For
identical tasks, Vortex's pipelines achieve significantly lower and more stable
latencies than TorchServe and Ray Serve over a wide range of workloads, often
enabling a given SLO target at more than twice the request rate. When RDMA is
available, the Vortex advantage is even more significant.

</details>


### [32] [Accelerating Graph Similarity Search through Integer Linear Programming](https://arxiv.org/abs/2511.02611)
*Andrea D'Ascenzo,Julian Meffert,Petra Mutzel,Fabrizio Rossi*

Main category: cs.DB

TL;DR: 提出了一种基于整数线性规划下界的图相似性搜索算法，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 图编辑距离(GED)是衡量图相似性的重要指标，但其精确计算是NP难问题。图相似性搜索在给定阈值下判断两个图的编辑距离是否低于该阈值，采用过滤-验证框架。现有过滤方法效果有限，需要更有效的下界估计来提高过滤效率。

Method: 基于整数线性规划定义了一个下界，证明该下界优于现有的分支匹配下界且可高效计算。提出使用下界算法层次结构和新颖的整数规划公式来利用阈值参数。

Result: 在标准测试集上的广泛计算实验表明，该方法在大多数测试阈值下显著优于现有最先进算法。

Conclusion: 提出的基于整数线性规划下界的图相似性搜索算法有效提高了过滤效率，在多个阈值下表现优于现有方法。

Abstract: The Graph Edit Distance (GED) is an important metric for measuring the
similarity between two (labeled) graphs. It is defined as the minimum cost
required to convert one graph into another through a series of (elementary)
edit operations. Its effectiveness in assessing the similarity of large graphs
is limited by the complexity of its exact calculation, which is NP-hard
theoretically and computationally challenging in practice. The latter can be
mitigated by switching to the Graph Similarity Search under GED constraints,
which determines whether the edit distance between two graphs is below a given
threshold. A popular framework for solving Graph Similarity Search under GED
constraints in a graph database for a query graph is the
filter-and-verification framework. Filtering discards unpromising graphs, while
the verification step certifies the similarity between the filtered graphs and
the query graph. To improve the filtering step, we define a lower bound based
on an integer linear programming formulation. We prove that this lower bound
dominates the effective branch match-based lower bound and can also be computed
efficiently. Consequently, we propose a graph similarity search algorithm that
uses a hierarchy of lower bound algorithms and solves a novel integer
programming formulation that exploits the threshold parameter. An extensive
computational experience on a well-assessed test bed shows that our approach
significantly outperforms the state-of-the-art algorithm on most of the
examined thresholds.

</details>


### [33] [Numbering Combinations for Compact Representation of Many-to-Many Relationship Sets](https://arxiv.org/abs/2511.02096)
*Savo Tomovic*

Main category: cs.DB

TL;DR: 提出组合关系集的概念，用于实现实体间的多对多映射关系，通过组合数系统编码实体，避免物理存储单独的关系表，并引入Rank-Join操作来整合信息。


<details>
  <summary>Details</summary>
Motivation: 动机源于数据仓库模型中多值维度和桥接表的设计与实现挑战。

Method: 使用组合数系统对实体进行编码，将组合关系信息封装到单个列中，不物理存储单独的关系表，并引入Rank-Join操作。

Result: 实现了组合关系集的有效表示，新列成为候选键，能够高效处理多对多关系。

Conclusion: 组合关系集提供了一种高效处理多对多关系的方法，避免了传统关系表的存储开销，适用于数据仓库模型中的复杂关系处理。

Abstract: In this paper we propose an approach to implement specific relation-ship set
between two entities called combinatorial relationship set. For the
combinatorial relationship set B between entity sets G and I the mapping
cardinality is many-to-many. Additionally, entities from G can be uniquely
encoded with a pair of values (h, k) generated with the procedure for numbering
combinations of entities from I. The encoding procedure is based on
combinatorial number system that provides a representation of all possible k
-combinations of a set of n elements by a single number. In general
many-to-many relationship sets are represented by a relation or table, while
the combinatorial relationship is not physically stored as separate table.
However, all information is encapsulated into a single column added to G. The
new column is a candidate key in G. Additional operation named Rank-Join to
fundamental relational-algebra is presented to combine information from g and i
associated with a combinatorial relationship set. Motivation for combinatorial
relationship originates from challenges in designing and implementing
multivalued dimensions and bridge tables in data-warehouse models.

</details>


### [34] [Relational Deep Dive: Error-Aware Queries Over Unstructured Data](https://arxiv.org/abs/2511.02711)
*Daren Chao,Kaiwen Chen,Naiqing Guan,Nick Koudas*

Main category: cs.DB

TL;DR: ReDD框架通过动态发现查询特定模式、填充关系表并确保错误感知提取，显著减少数据提取错误，从高达30%降至1%以下，同时保持高模式完整性。


<details>
  <summary>Details</summary>
Motivation: 非结构化数据普遍存在，但分析查询需要结构化表示，现有方法如RAG缺乏模式意识且难以跨文档对齐，导致高错误率。

Method: ReDD采用两阶段流水线：(1)迭代模式发现(ISD)识别最小可连接模式，(2)表格数据填充(TDP)使用基于LLM隐藏状态的轻量级分类器提取和校正数据，并引入SCAPE统计校准错误检测方法。

Result: 在多样化数据集上的实验表明，ReDD将数据提取错误从高达30%减少到1%以下，同时保持100%召回率的高模式完整性。

Conclusion: ReDD的模块化设计使精度-成本权衡实现细粒度控制，为高风险分析查询提供了稳健解决方案。

Abstract: Unstructured data is pervasive, but analytical queries demand structured
representations, creating a significant extraction challenge. Existing methods
like RAG lack schema awareness and struggle with cross-document alignment,
leading to high error rates. We propose ReDD (Relational Deep Dive), a
framework that dynamically discovers query-specific schemas, populates
relational tables, and ensures error-aware extraction with provable guarantees.
ReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)
identifies minimal, joinable schemas tailored to each query, and (2) Tabular
Data Population (TDP) extracts and corrects data using lightweight classifiers
trained on LLM hidden states. A main contribution of ReDD is SCAPE, a
statistically calibrated method for error detection with coverage guarantees,
and SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy
and human correction costs. Experiments across diverse datasets demonstrate
ReDD's effectiveness, reducing data extraction errors from up to 30% to below
1% while maintaining high schema completeness (100% recall) and precision.
ReDD's modular design enables fine-grained control over accuracy-cost
trade-offs, making it a robust solution for high-stakes analytical queries over
unstructured corpora.

</details>


### [35] [EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union Search across Data Lakes](https://arxiv.org/abs/2511.02674)
*Tim Otto*

Main category: cs.DB

TL;DR: EasyTUS是一个基于大语言模型的表联合搜索框架，通过模块化三步流程实现高效可扩展的数据湖表搜索，在TUSBench基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据湖虽然便于维护异构数据，但增加了数据发现任务的复杂性，特别是表联合搜索(TUS)任务需要找到能与给定输入表进行联合的表。

Method: 采用三步模块化流程：表序列化（格式化和采样）、表表示（利用LLM生成嵌入向量）、向量搜索（使用近似最近邻索引进行语义匹配），并引入TUSBench标准化评估环境。

Result: 在TUSBench实验中，EasyTUS在平均精度上提升达34.3%，数据准备速度提升79.2倍，查询处理性能提升7.7倍，且在元数据缺失场景下仍保持强劲性能。

Conclusion: EasyTUS框架在表联合搜索任务中表现出色，具有高效性、可扩展性和鲁棒性，能够适应不同数据湖环境。

Abstract: Data lakes enable easy maintenance of heterogeneous data in its native form.
While this flexibility can accelerate data ingestion, it shifts the complexity
of data preparation and query processing to data discovery tasks. One such task
is Table Union Search (TUS), which identifies tables that can be unioned with a
given input table. In this work, we present EasyTUS, a comprehensive framework
that leverages Large Language Models (LLMs) to perform efficient and scalable
Table Union Search across data lakes. EasyTUS implements the search pipeline as
three modular steps: Table Serialization for consistent formatting and
sampling, Table Representation that utilizes LLMs to generate embeddings, and
Vector Search that leverages approximate nearest neighbor indexing for semantic
matching. To enable reproducible and systematic evaluation, in this paper, we
also introduce TUSBench, a novel standardized benchmarking environment within
the EasyTUS framework. TUSBench supports unified comparisons across approaches
and data lakes, promoting transparency and progress in the field. Our
experiments using TUSBench show that EasyTUS consistently outperforms most of
the state-of the-art approaches, achieving improvements in average of up to
34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,
and up to 7.7x faster query processing performance. Furthermore, EasyTUS
maintains strong performance even in metadata-absent settings, highlighting its
robustness and adaptability across data lakes.

</details>
