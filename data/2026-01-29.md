<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 5]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.IT](#cs.IT) [Total: 10]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [A Cache-Aware Hybrid Sieve Combining Segmentation and Bit-Packing for Fast Prime Generation](https://arxiv.org/abs/2601.19909)
*Kathi Lakshmi Mani Thirdhana*

Main category: cs.DS

TL;DR: 提出了一种缓存感知的混合筛法，通过分段、位压缩和缓存行对齐块处理来优化内存带宽和缓存局部性，相比经典筛法减少8倍内存使用和提升2.4倍运行速度。


<details>
  <summary>Details</summary>
Motivation: 经典埃拉托斯特尼筛法虽然理论简单高效，但在现代CPU上受限于内存访问效率。需要设计缓存感知的算法来优化内存带宽和缓存局部性，提升实际性能。

Method: 采用缓存感知的混合筛法：1) 仅存储奇数并使用每值1位的位压缩减少内存；2) 将筛分范围划分为缓存大小的块以最小化缓存未命中；3) 重用平方根限制内的素数跨块处理；4) 结合分段、位压缩和缓存行对齐块处理技术。

Result: 实验结果显示：1) 内存使用减少高达8倍；2) 运行时间相比经典筛法提升2.4倍，相比分段筛法提升1.7倍；3) 在10^9范围内的基准测试证实架构感知算法设计能带来显著的实际性能提升。

Conclusion: 架构感知的算法设计对素数生成任务具有重要实践价值。通过优化内存访问模式和缓存利用，缓存感知混合筛法在内存使用和运行时间上都实现了显著改进，证明了针对现代CPU架构优化算法的重要性。

Abstract: Prime generation is a fundamental task in cryptography, number theory, and randomized algorithms. While the classical Sieve of Eratosthenes is simple and efficient in theory, its practical performance on modern central processing units is often limited by memory access inefficiencies. This paper introduces a cache-aware hybrid sieve that integrates segmentation, bit-packing, and cache-line-aligned block processing to optimize memory bandwidth and level one and level two cache locality.
  The proposed approach reduces memory usage by storing only odd numbers and using one bit per value. The sieve range is divided into cache-sized blocks to minimize cache misses, while primes up to the square root of the limit are reused across blocks. Experimental results demonstrate up to an eight times reduction in memory usage and runtime improvements of up to two point four times compared to the classical sieve and one point seven times compared to the segmented sieve. Benchmarks up to ten to the power of nine illustrate that architecture-aware algorithm design can yield substantial practical performance gains.

</details>


### [2] [Node-Weighted Multicut in Planar Digraphs](https://arxiv.org/abs/2601.20038)
*Chandra Chekuri,Rhea Jain*

Main category: cs.DS

TL;DR: 本文扩展了平面有向图中Multicut问题的O(log²n)近似算法到节点加权版本，同时实现了确定性算法并简化了原分析。


<details>
  <summary>Details</summary>
Motivation: 平面有向图中Multicut问题已有O(log²n)近似算法，但该算法是随机的且仅适用于边加权情况。需要将其扩展到节点加权版本，同时实现确定性算法并简化分析。

Method: 扩展[KS22]的算法和分析到节点加权Multicut问题，利用节点加权问题作为工具来实现确定性算法，并简化原算法和分析的某些方面。

Result: 获得了节点加权Multicut问题的O(log²n)近似算法，建立了相应的流割间隙上界，并通过标准技术得到Nonuniform Sparsest Cut问题的近似算法（需额外对数因子损失）。

Conclusion: 成功将平面有向图中Multicut的近似算法扩展到节点加权版本，实现了确定性算法，并简化了原分析，同时为Nonuniform Sparsest Cut问题提供了新的近似结果。

Abstract: Kawarabayashi and Sidiropoulos [KS22] obtained an $O(\log^2 n)$-approximation algorithm for Multicut in planar digraphs via a natural LP relaxation, which also establishes a corresponding upper bound on the multicommodity flow-cut gap. Their result is in contrast to a lower bound of $\tildeΩ(n^{1/7})$ on the flow-cut gap for general digraphs due to Chuzhoy and Khanna [CK09]. We extend the algorithm and analysis in [KS22] to the node-weighted Multicut problem. Unlike in general digraphs, node-weighted problems cannot be reduced to edge-weighted problems in a black box fashion due to the planarity restriction. We use the node-weighted problem as a vehicle to accomplish two additional goals: (i) to obtain a deterministic algorithm (the algorithm in [KS22] is randomized), and (ii) to simplify and clarify some aspects of the algorithm and analysis from [KS22]. The Multicut result, via a standard technique, implies an approximation for the Nonuniform Sparsest Cut problem with an additional logarithmic factor loss.

</details>


### [3] [Hypergraph Samplers: Typical and Worst Case Behavior](https://arxiv.org/abs/2601.20039)
*Vedat Levi Alev,Uriya A. First*

Main category: cs.DS

TL;DR: 研究k-均匀超图在随机算法误差缩减中的应用，揭示典型行为与最坏情况行为之间的差距，为分散器和顶点扩展器提供新的下界。


<details>
  <summary>Details</summary>
Motivation: 探索使用超图进行随机算法误差缩减的效用和局限性，特别是研究通过采样超图边来生成k个不重复种子的一般范式。

Method: 提出基于超图的误差缩减框架：从超图中均匀随机采样一条边，使用边上的顶点作为种子重复运行算法k次。分析该方法在单边误差和双边误差算法中的性能。

Result: 1. 单边误差缩减：若使用随机超边能将误差从p降至p^k+ε，则超图边数必须满足|E|=Ω(nk^{-1}ε^{-1})，为分散器和顶点扩展器提供新下界。
2. 典型行为分析：在顶点度分布合理的情况下，对(1-o(1))比例的算法和输入，使用超图生成的伪随机种子与独立同分布种子的误差缩减效果几乎相同。

Conclusion: 超图在随机算法误差缩减中表现出典型行为与最坏情况行为的显著差距：虽然最坏情况下需要较多随机比特，但在绝大多数实际情况下，使用合理超图生成的伪随机种子几乎能达到与独立同分布种子相同的误差缩减效果。

Abstract: We study the utility and limitations of using $k$-uniform hypergraphs $H = ([n], E)$ ($n \ge \mathrm{poly}(k)$) in the context of error reduction for randomized algorithms for decision problems with one- or two-sided error. Our error reduction idea is sampling a uniformly random hyperedge of $H$, and repeating the algorithm $k$ times using the hyperedge vertices as seeds. This is a general paradigm, which captures every pseudorandom method generating $k$ seeds without repetition. We show two results which imply a gap between the typical and the worst-case behavior of using $H$ for error-reduction.
  First, in the context of one-sided error reduction, if using a random hyperedge of $H$ decreases the error probability from $p$ to $p^k + ε$, then $H$ cannot have too few edges, i.e., $|E| = Ω(n k^{-1} ε^{-1})$. Thus, the number of random bits needed for reducing the error from $p$ to $p^k + ε$ cannot be reduced below $\lg n+\lg(ε^{-1})-\lg k+O(1)$. This is also true for hypergraphs of average uniformity $k$. Our result implies new lower bounds for dispersers and vertex-expanders.
  Second, if the vertex degrees are reasonably distributed, we show that in a $(1-o(1))$-fraction of the cases, choosing $k$ pseudorandom seeds using $H$ will reduce the error probability to at most $o(1)$ above the error probability of using $k$ IID seeds, for both algorithms with one- or two-sided error. Thus, despite our lower bound, for a $(1-o(1))$-fraction of randomized algorithms (and inputs) for decision problems, the advantage of using IID samples over samples obtained from a uniformly random edge of a reasonable hypergraph is negligible. A similar statement holds true for randomized algorithms with two-sided error.

</details>


### [4] [Dynamic framework for edge-connectivity maintenance of simple graphs](https://arxiv.org/abs/2601.20137)
*Blazej Wrobel*

Main category: cs.DS

TL;DR: 提出动态维护k边连通图的框架，支持边添加和删除操作，保持图的最小边割≥k


<details>
  <summary>Details</summary>
Motivation: 现有动态图算法主要关注报告最小割值，而非主动维护图的连通性。需要一种能动态维持k边连通性的框架，支持结构更新操作。

Method: 1) 冗余消除：结合Nagamochi-Ibaraki稀疏证书和Link-Cut Trees识别并移除因新边插入而变得冗余的边；2) 连通性恢复：利用局部增强策略，在稀疏化图上运行Dinic算法，计算最小增强边集来修复因边删除而破坏的k边连通性。

Result: 冗余消除算法达到O(k log n)摊销时间，连通性恢复算法在O(k·n^{5/3})时间内找到最小增强边集，严格保证消除的边与插入的边不同，且恢复过程不包含已删除的边。

Conclusion: 提出了首个动态维护k边连通图的框架，解决了冗余消除和连通性恢复两个核心问题，为动态图连通性维护提供了理论基础和实用算法。

Abstract: We present a dynamic framework for maintaining $k$-edge-connectivity of undirected, simple graphs subject to structural updates, specifically single edge additions and removals. The required edge-connectivity $k$ is a chosen, constant parameter. Unlike standard dynamic graph problems, such as dynamic minimum-cut, which focus solely on reporting the value of the minimum cut, our approach actively modifies the graph $G$ to maintain the edge-connectivity invariant $λ(G) \ge k$. We address two fundamental maintenance tasks: redundancy elimination, which identifies and removes an existing edge rendered redundant for $k$-edge-connectivity by new edge insertion, and connectivity restoration, which computes and inserts a minimal set of augmenting edges to restore graph's $k$-edge-connectivity following an old edge deletion. To preclude trivial reversals, we strictly enforce that the eliminated edge is distinct from the inserted edge and that restoration excludes the already deleted edge. Our solution of the first problem integrates Nagamochi-Ibaraki sparse certificates [Nagamochi and Ibaraki 1992] with Link-Cut Trees [Sleator and Tarjan 1983] to remove redundant edges in $O(k \log n)$ amortized time. For restoration, we propose a localized augmentation strategy that exploits the residual graph structure to bridge the minimum cut. By executing Dinic's [Dinic 1970] algorithm on the sparsified input graph, we identify the minimal edge set required to reconnect the graph in $O(k \cdot n^{5/3})$ time.

</details>


### [5] [Fully Dynamic Algorithms for Graph Spanners via Low-Diameter Router Decomposition](https://arxiv.org/abs/2601.20718)
*Julia Chuzhoy,Merav Parter*

Main category: cs.DS

TL;DR: 本文研究了在自适应对手下的全动态图环境中维护稀疏子图（spanner）的问题。提出了一种确定性算法，能在多项式拉伸因子下，以接近线性的空间复杂度和亚线性的更新时间维护spanner。核心贡献是开发了低直径路由器分解技术。


<details>
  <summary>Details</summary>
Motivation: 在全动态图设置中维护spanner是一个长期研究但理解不足的问题。目前没有算法能同时实现亚对数拉伸因子、亚线性更新时间和强亚二次方spanner大小。本文旨在填补这一空白，提供更高效的动态spanner维护算法。

Method: 开发了一种新的技术工具——低直径路由器分解。设计了一种确定性算法，将全动态图分解为边不相交的簇，每个簇都是有限直径的路由器，能够通过短路径和低拥塞路由多商品需求。与先前工作不同，该分解是"proper"的，确保簇内顶点对之间的路由路径完全包含在簇内。

Result: 对于任意512≤k≤(log n)^{1/49}和1/k≤δ≤1/400，算法维护的spanner具有拉伸因子poly(k)·2^{O(1/δ^6)}，大小|E(H)|≤O(n^{1+O(1/k)})，最坏情况更新时间n^{O(δ)}，调整开销n^{O(1/k)}。该结果在拉伸因子、更新时间和spanner大小之间取得了新的平衡。

Conclusion: 本文提出了首个在全动态自适应对手设置中，同时实现多项式拉伸因子、接近线性空间复杂度和亚线性更新时间的确定性spanner维护算法。低直径路由器分解技术不仅解决了spanner维护问题，还可应用于容错spanner和低拥塞spanner等动态算法。

Abstract: A $t$-spanner of an undirected $n$-vertex graph $G$ is a sparse subgraph $H$ of $G$ that preserves all pairwise distances between its vertices to within multiplicative factor $t$, also called the \emph{stretch}. We investigate the problem of maintaining spanners in the fully dynamic setting with an adaptive adversary. Despite a long line of research, this problem is still poorly understood: no algorithm achieving a sublogarithmic stretch, a sublinear in $n$ update time, and a strongly subquadratic in $n$ spanner size is currently known.
  One of our main results is a deterministic algorithm, that, for any $512 \leq k \leq (\log n)^{1/49}$ and $1/k\leq δ\leq 1/400$, maintains a spanner $H$ of a fully dynamic graph with stretch $poly(k)\cdot 2^{O(1/δ^6)}$ and size $|E(H)|\leq O(n^{1+O(1/k)})$, with worst-case update time $n^{O(δ)}$ and recourse $n^{O(1/k)}$. Our algorithm relies on a new technical tool that we develop, called low-diameter router decomposition. We design a deterministic algorithm that maintains a decomposition of a fully dynamic graph into edge-disjoint clusters with bounded vertex overlap, where each cluster $C$ is a bounded-diameter router, meaning that any reasonable multicommodity demand over the vertices of $C$ can be routed along short paths and with low congestion. A similar graph decomposition notion was introduced by [Haeupler et al., STOC 2022] and strengthened by [Haeupler et al., FOCS 2024]. However, in contrast to these and other prior works, the decomposition that our algorithm maintains is proper, ensuring that the routing paths between the pairs of vertices of each cluster $C$ are contained inside $C$, rather than in the entire graph $G$. We show additional applications of our router decomposition, including dynamic algorithms for fault-tolerant spanners and low-congestion spanners.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [6] [LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation](https://arxiv.org/abs/2601.20083)
*Lee Xiong,Zhirong Chen,Rahul Mayuranath,Shangran Qiu,Arda Ozdemir,Lu Li,Yang Hu,Dave Li,Jingtao Ren,Howard Cheng,Fabian Souto Herrera,Ahmed Agiza,Baruch Epshtein,Anuj Aggarwal,Julia Ulziisaikhan,Chao Wang,Dinesh Ramasamy,Parshva Doshi,Sri Reddy,Arnold Overwijk*

Main category: cs.IR

TL;DR: LLaTTE是一个用于广告推荐的Transformer架构，通过两阶段设计实现大规模序列建模，在Meta部署后带来4.3%的转化率提升。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的序列建模存在与LLM类似的幂律缩放规律，但受限于严格的延迟约束，需要找到在工业环境中利用缩放定律的实用方法。

Method: 提出LLaTTE架构：1）利用语义特征作为缩放前提；2）采用两阶段设计，将大型长上下文模型的计算卸载到异步上游用户模型；3）通过多阶段框架实现可预测的性能传递。

Result: 在Meta部署为最大用户模型，在Facebook Feed和Reels上实现4.3%的转化率提升，且服务开销最小，验证了推荐系统中缩放定律的实用性。

Conclusion: 推荐系统遵循与LLM类似的缩放定律，语义特征是缩放的关键前提，两阶段架构为工业推荐系统利用缩放定律提供了实用蓝图。

Abstract: We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.

</details>


### [7] [IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation](https://arxiv.org/abs/2601.20084)
*Yash Saxena,Ankur Padia,Kalpa Gunaratna,Manas Gaur*

Main category: cs.IR

TL;DR: IMRNNNs是一个轻量级框架，通过动态双向调制增强稠密检索器的可解释性和检索效果，在推理时使用两个独立适配器分别基于查询调整文档嵌入和基于初始检索文档反馈优化查询嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有稠密检索器使用静态嵌入，掩盖了查询和文档之间的双向语义关系，而事后方法如重排序计算成本高、增加推理延迟，且仍无法揭示底层语义对齐。需要一种既能解释又能增强检索的方法。

Method: 提出IMRNNNs框架，包含两个独立适配器：一个基于当前查询条件化文档嵌入，另一个利用初始检索文档的语料级反馈优化查询嵌入。通过迭代调制过程动态调整表示，揭示查询与文档间的可解释语义依赖。

Result: 在七个基准数据集上，将IMRNNNs应用于标准稠密检索器，相比最先进基线平均提升：nDCG +6.35%，召回率 +7.14%，MRR +7.04%。同时显著增强了检索可解释性。

Conclusion: 引入可解释性驱动的调制既能解释又能增强RAG系统中的检索，IMRNNNs通过动态双向调制在保持轻量级的同时，显著提升检索效果和可解释性。

Abstract: Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.

</details>


### [8] [Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms](https://arxiv.org/abs/2601.20131)
*Deep Shah,Sanket Badhe,Nehal Kathrotia*

Main category: cs.IR

TL;DR: 论文提出了一个四层框架来优化嵌入检索系统的效率-效果权衡：表示层（损失函数和架构）、粒度层（文档分割策略）、编排层（超越单向量的检索方法）和鲁棒性层（解决领域泛化、词汇盲点和时间漂移问题）。


<details>
  <summary>Details</summary>
Motivation: 设计嵌入检索系统需要在效率和效果之间进行复杂的权衡决策，目前缺乏一个系统化的框架来指导这些设计选择。

Method: 提出一个垂直遍历系统设计栈的四层框架：1) 表示层：分析损失函数和架构（Bi-encoders vs Cross-encoders）如何定义语义相关性和几何投影；2) 粒度层：评估原子和分层分块策略如何缓解长文档的信息瓶颈；3) 编排层：讨论超越单向量范式的方法，包括分层检索、代理分解和多阶段重排序管道；4) 鲁棒性层：识别解决领域泛化失败、词汇盲点和时间漂移导致的检索质量退化问题的架构缓解措施。

Result: 通过系统化地分类这些限制和设计选择，为从业者提供了一个全面的框架，用于优化现代神经搜索系统的效率-效果边界。

Conclusion: 该四层框架为嵌入检索系统的设计提供了一个结构化的方法论，帮助从业者在复杂的效率-效果权衡空间中做出明智的设计决策。

Abstract: Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.

</details>


### [9] [MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation](https://arxiv.org/abs/2601.20199)
*Jing Yan,Yimeng Bai,Zongyu Liu,Yahui Liu,Junwei Wang,Jingze Huang,Haoda Li,Sihao Ding,Shaohui Ruan,Yang Zhang*

Main category: cs.IR

TL;DR: MERGE是一种新一代物品索引范式，通过自适应构建聚类、动态监控聚类占用率以及细到粗的层次合并，解决了传统向量量化方法在流式推荐系统中的分配不准确、聚类不平衡和分离不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于向量量化的物品索引方法在处理流式行业推荐系统中常见的高度偏斜和非平稳的物品分布时表现不佳，导致分配准确性差、聚类占用不平衡以及聚类分离不足，需要更有效的索引方法。

Method: MERGE采用自适应从零开始构建聚类、动态监控聚类占用率，并通过细到粗的合并形成层次化索引结构，以应对流式推荐系统中的挑战。

Result: 实验表明MERGE在分配准确性、聚类均匀性和聚类分离方面显著优于现有索引方法，在线A/B测试显示关键业务指标有实质性提升。

Conclusion: MERGE有潜力成为大规模推荐系统的基础性索引方法，能够有效处理流式推荐系统中的物品分布挑战。

Abstract: Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.

</details>


### [10] [Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation](https://arxiv.org/abs/2601.20215)
*Na Li,Jiaqi Yu,Minzhi Xie,Tiantian He,Xiaoxiao Xu,Zixiu Wang,Lantao Hu,Yongqi Liu,Han Li,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: EASQ框架通过问卷反馈实现推荐系统的端到端用户满意度对齐，解决了稀疏满意度信号被海量行为数据淹没的问题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖点击、观看时长等间接行为信号作为用户满意度代理，但这些信号存在噪声和偏差。问卷反馈虽然能提供高质量的直接满意度监督，但极其稀疏且容易被海量行为数据淹没，难以融入在线推荐模型。

Method: 提出EASQ框架：1) 通过多任务架构和轻量级LoRA模块为稀疏问卷信号构建独立参数路径；2) 多任务设计分离稀疏满意度监督与密集行为信号；3) LoRA模块以参数隔离方式预注入用户偏好；4) 采用基于DPO的在线学习优化目标，实时对齐主模型输出与稀疏满意度信号。

Result: 离线和在线A/B测试表明，EASQ在多个场景下持续提升用户满意度指标。已在生产级短视频推荐系统中成功部署，带来显著且稳定的业务收益。

Conclusion: EASQ框架有效解决了稀疏满意度信号与海量行为数据的融合问题，实现了推荐模型与真实用户满意度的端到端实时对齐，为推荐系统优化提供了新思路。

Abstract: Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.

</details>


### [11] [MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation](https://arxiv.org/abs/2601.20234)
*Qihang Yu,Kairui Fu,Zhaocheng Du,Yuxuan Si,Kaiyuan Li,Weihao Zhao,Zhicheng Zhang,Jieming Zhu,Quanyu Dai,Zhenhua Dong,Shengyu Zhang,Kun Kuang,Fei Wu*

Main category: cs.IR

TL;DR: MALLOC是一个用于推荐系统中长序列压缩的内存感知基准测试平台，旨在解决大规模推荐系统中内存开销过大的问题。


<details>
  <summary>Details</summary>
Motivation: 随着推荐模型规模的扩大，计算成本显著增加，特别是在处理用户行为的长序列依赖时。现有方法通过预存储用户历史行为的中间状态来减少计算成本，但忽视了由此带来的巨大内存开销，这在拥有数十亿用户、每个用户可能有数千次交互的真实推荐系统中成为关键挑战。

Method: 引入MALLOC基准测试平台，对适用于大规模序列推荐的内存管理技术进行全面调查和系统分类，并将这些技术集成到最先进的推荐器中，构建可复现、易访问的评估平台。

Result: 通过在准确性、效率和复杂性方面的广泛实验，证明了MALLOC在推进大规模推荐系统方面的全面可靠性。

Conclusion: MALLOC为解决大规模推荐系统中的内存开销问题提供了一个全面的基准测试平台，填补了现有研究中内存管理技术在推荐任务上评估不足的空白。

Abstract: The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.

</details>


### [12] [One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking](https://arxiv.org/abs/2601.20283)
*Tanmay Karmakar,Sourav Saha,Debapriyo Majumdar,Surjyanee Halder*

Main category: cs.IR

TL;DR: 论文提出一种针对神经排序模型的单词语义攻击方法，通过插入或替换单个语义对齐的"查询中心"词来提升目标文档排名，在TREC-DL数据集上达到91%成功率，平均修改不到2个token。


<details>
  <summary>Details</summary>
Motivation: 尽管神经排序模型(NRMs)检索效果强，但先前研究表明它们对对抗性扰动脆弱。本文重新审视鲁棒性问题，探索最小化的查询感知攻击方法，揭示实际风险并推动未来防御机制发展。

Method: 提出基于查询中心的单词语义攻击：1)启发式和梯度引导变体；2)白盒方法识别有影响力的插入点；3)在BERT和monoT5重排序器上测试；4)引入新的诊断指标分析攻击敏感性。

Result: 在TREC-DL 2019/2020数据集上，单词语义攻击成功率高达91%，平均修改少于2个token，在可比白盒设置下实现竞争性排名和分数提升。分析发现中等排名文档最脆弱，存在"Goldilocks zone"。

Conclusion: 研究表明神经排序模型对单词语义攻击高度脆弱，揭示了实际安全风险。需要开发更鲁棒的防御机制，同时新的诊断指标有助于深入理解攻击敏感性，为未来研究提供方向。

Abstract: Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.

</details>


### [13] [Less is More: Benchmarking LLM Based Recommendation Agents](https://arxiv.org/abs/2601.20316)
*Kargi Chauhan,Mahalakshmi Venkateswarlu*

Main category: cs.IR

TL;DR: LLM推荐系统中，增加用户购买历史长度（5-50项）并不会显著提升推荐质量，使用5-10项历史即可节省88%推理成本而不损失质量。


<details>
  <summary>Details</summary>
Motivation: 挑战当前LLM推荐系统中"更多上下文更好"的普遍假设，验证增加用户购买历史长度是否真的能提升推荐质量。

Method: 使用四种先进LLM（GPT-4o-mini、DeepSeek-V3、Qwen2.5-72B、Gemini 2.5 Flash）在REGEN数据集上进行系统基准测试，上下文长度从5到50项，采用50名用户的被试内设计。

Result: 增加上下文长度（5-50项）对推荐质量无显著改善，质量分数保持稳定（0.17-0.23）。使用5-10项历史而非50项历史可减少约88%推理成本而不损失质量。

Conclusion: 挑战了"更多上下文更好"的范式，为基于LLM的推荐系统提供了成本效益优化的实用指南，建议使用较短历史来大幅降低成本。

Abstract: Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.
  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.

</details>


### [14] [Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval](https://arxiv.org/abs/2601.20391)
*Zhuocheng Zhang,Kangheng Liang,Guanxuan Li,Paul Henderson,Richard Mccreadie,Zijun Long*

Main category: cs.IR

TL;DR: DAI-TIR使用扩散模型生成查询图像作为额外意图视图，但可能产生与原始查询冲突的幻觉视觉线索，降低检索性能。DMCL通过语义一致性和扩散感知对比学习，将幻觉线索映射到零空间，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散增强的交互式文本到图像检索(DAI-TIR)通过扩散模型生成查询图像作为用户意图的额外视图，但扩散生成可能引入与原始查询文本冲突的幻觉视觉线索，这些幻觉线索会显著降低检索性能。

Method: 提出扩散感知多视图对比学习(DMCL)框架，将DAI-TIR建模为查询意图和目标图像表示的联合优化。引入语义一致性和扩散感知对比目标，对齐文本和扩散生成的查询视图，同时抑制幻觉查询信号，使编码器充当语义过滤器。

Result: 在五个标准基准测试中，DMCL在多轮Hits@10指标上取得一致改进，最高提升7.37%，优于之前的微调和零样本基线。注意力可视化和几何嵌入空间分析证实了过滤行为。

Conclusion: DMCL是一个通用且鲁棒的DAI-TIR训练框架，通过将幻觉线索映射到零空间，有效过滤虚假信号，更好地表示用户意图，提高检索性能。

Abstract: Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.

</details>


### [15] [When Vision Meets Texts in Listwise Reranking](https://arxiv.org/abs/2601.20623)
*Hongyi Cai*

Main category: cs.IR

TL;DR: Rank-Nexus：一个轻量级多模态图像-文本文档重排序器，通过渐进式跨模态训练策略解决模态鸿沟问题，在文本和图像重排序任务上均表现出色，仅需20亿参数。


<details>
  <summary>Details</summary>
Motivation: 当前信息检索领域虽然认识到整合视觉和文本信息的潜力，但图像-文本文档重排序面临模态鸿沟和对齐数据集稀缺的挑战。现有方法通常依赖大型模型（70亿到320亿参数）和基于推理的知识蒸馏，带来不必要的计算开销，且主要关注文本模态。

Method: 提出Rank-Nexus多模态图像-文本文档重排序器，采用列表式定性重排序方法。引入渐进式跨模态训练策略：1）分别训练模态：利用丰富的文本重排序数据将知识蒸馏到文本分支；2）对于图像数据稀缺问题，从多模态大语言模型在图像检索基准上的标注构建蒸馏对；3）蒸馏联合图像-文本重排序数据集。

Result: Rank-Nexus在文本重排序基准（TREC、BEIR）和具有挑战性的图像重排序基准（INQUIRE、MMDocIR）上均取得优异性能，仅使用轻量级的20亿参数预训练视觉语言模型。

Conclusion: 该高效设计确保了在多样化多模态场景下的强大泛化能力，无需过多参数或推理开销，为多模态信息检索提供了实用且高效的解决方案。

Abstract: Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.

</details>


### [16] [Overview of the TREC 2025 Tip-of-the-Tongue track](https://arxiv.org/abs/2601.20671)
*Jaime Arguello,Fernando Diaz,Maik Fröebe,To Eun Kim,Bhaskar Mitra*

Main category: cs.IR

TL;DR: TREC 2025 ToT 赛道扩展至通用领域，整合了来自MS-ToT数据集、人工主题开发和LLM生成查询的多样化测试查询集，共有9个团队提交了32个运行结果。


<details>
  <summary>Details</summary>
Motivation: 舌尖现象检索中，用户无法可靠回忆项目标识符，查询通常冗长且包含复杂现象，这对现有检索系统构成挑战。TREC 2025 ToT赛道旨在解决这一难题。

Method: 将赛道扩展至通用领域，整合了三种不同来源的测试查询：MS-ToT数据集、人工开发的主题、以及基于LLM的合成查询生成。

Result: 共有9个团队（包括赛道协调者）提交了32个运行结果，展示了不同方法在ToT检索任务上的表现。

Conclusion: TREC 2025 ToT赛道通过整合多样化的查询来源，为舌尖现象检索研究提供了更全面的评估框架，吸引了多个研究团队的参与。

Abstract: Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.

</details>


### [17] [MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature](https://arxiv.org/abs/2601.20709)
*Huan He,Xueqing Peng,Yutong Xie,Qijia Liu,Chia-Hsuan Chang,Lingfei Qian,Brian Ondov,Qiaozhu Mei,Hua Xu*

Main category: cs.IR

TL;DR: MedViz是一个结合多AI代理与交互式可视化的系统，用于探索大规模生物医学文献，通过语义地图和智能代理功能支持查询、总结和假设生成。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究人员面临数百万篇跨领域文献的导航挑战，传统搜索引擎仅提供排名文本列表，缺乏全局探索和深度分析支持，现有生成式AI工具与文献搜索工作流集成不足。

Method: 开发MedViz系统，集成多个AI代理与交互式可视化，结合数百万文章的语义地图，提供查询、总结和假设生成的代理驱动功能，支持迭代式问题精炼和趋势识别。

Result: MedViz将生物医学文献搜索转变为动态探索过程，帮助研究人员精炼问题、识别趋势、发现隐藏联系，加速知识发现。

Conclusion: 通过桥接智能代理与交互式可视化，MedViz解决了生物医学文献探索的现有局限，为大规模文献分析提供了创新解决方案。

Abstract: Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [18] [Quick Change Detection in Discrete-Time in Presence of a Covert Adversary](https://arxiv.org/abs/2601.20022)
*Amir Reza Ramtin,Philippe Nain,Don Towsley*

Main category: cs.IT

TL;DR: 研究隐蔽最快变化检测问题，其中对手知道检测器的虚警约束参数γ，并选择依赖于γ的平稳后变化分布以尽可能长时间不被检测。分析显示在γ→∞时，平均检测延迟(ADD)呈Θ(γ)增长，而经典设置中仅为O(log γ)。


<details>
  <summary>Details</summary>
Motivation: 传统最快变化检测假设后变化分布固定，但现实中对手可能知道检测器的虚警约束参数γ，并据此调整行为以保持隐蔽。需要研究这种隐蔽对手场景下的检测性能极限。

Method: 基于CuSum程序的理论基础，分析当后变化分布随γ→∞收敛到前变化分布时的渐近行为。推导平均检测延迟(ADD)和平均虚警时间(AT2FA)的精确渐近表达式，识别隐蔽行为的关键缩放规律。

Result: 建立了ADD和AT2FA的精确渐近表达式，发现ADD呈Θ(γ)增长（经典情况下为O(log γ)）。对于高斯和指数模型，给出了ADD作为Kullback-Leibler散度和γ函数的渐近特征。

Conclusion: 在隐蔽对手场景下，检测性能显著恶化，ADD呈线性而非对数增长。这揭示了对手利用检测器参数知识时的根本限制，为设计对抗性稳健的检测器提供了理论基础。

Abstract: We study the problem of covert quickest change detection in a discrete-time setting, where a sequence of observations undergoes a distributional change at an unknown time. Unlike classical formulations, we consider a covert adversary who has knowledge of the detector's false alarm constraint parameter $γ$ and selects a stationary post-change distribution that depends on it, seeking to remain undetected for as long as possible. Building on the theoretical foundations of the CuSum procedure, we rigorously characterize the asymptotic behavior of the average detection delay (ADD) and the average time to false alarm (AT2FA) when the post-change distribution converges to the pre-change distribution as $γ\to \infty$. Our analysis establishes exact asymptotic expressions for these quantities, extending and refining classical results that no longer hold in this regime. We identify the critical scaling laws governing covert behavior and derive explicit conditions under which an adversary can maintain covertness, defined by ADD = $Θ(γ)$, whereas in the classical setting, ADD grows only as $\mathcal{O}(\log γ)$. In particular, for Gaussian and Exponential models under adversarial perturbations of their respective parameters, we asymptotically characterize ADD as a function of the Kullback--Leibler divergence between the pre- and post-change distributions and $γ$.

</details>


### [19] [On Efficient Polyphase Network Implementation Using Successive Vector Approximation](https://arxiv.org/abs/2601.20411)
*Luiz F. da S. Coelho,Didier Le Ruyet,Paulo S. R. Diniz*

Main category: cs.IT

TL;DR: 提出一种基于匹配追踪的贪心算法，将多相网络从浮点数直接转换为SOPOT表示，实现无乘法器的高能效FBMC系统


<details>
  <summary>Details</summary>
Motivation: 设计滤波器组多载波(FBMC)系统中高能效的多相网络实现，通过无乘法器硬件降低计算复杂度

Method: 使用基于匹配追踪(MP)的贪心算法，将数值表示直接从浮点数转换为有符号二次幂和(SOPOT)形式

Result: 与现有无乘法器硬件设计方法相比，该方法在相似计算复杂度下获得更优性能

Conclusion: 提出的SOPOT转换方法为FBMC系统提供了一种高效的无乘法器多相网络实现方案

Abstract: In this work, we explore an energy-efficient implementation of the polyphase network for a filter bank multicarrier (FBMC) system. The network is approximated using a greedy algorithm based on matching pursuits (MP) that converts the numerical representation directly from floating point to sum of signed powers of two (SOPOT), which is key for a multiplierless implementation. We compare this technique with other state-of-the-art methods for designing multiplierless hardware, and show that our technique achieves superior performance with similar computational complexity.

</details>


### [20] [Energy Efficient Downlink mMIMO Using Dynamic Antenna and Power Adaptation](https://arxiv.org/abs/2601.20586)
*Ravi Sharan B A G,Maliha Jada,Anders Karstensen,Daniela Laselva,Jyri Hämäläinen,Silvio Mandelli*

Main category: cs.IT

TL;DR: 提出一种动态联合天线和功率自适应方案，通过用户调度和资源分配提升网络节能效果，在保证用户吞吐量的同时最大化节能


<details>
  <summary>Details</summary>
Motivation: 6G通信系统需要满足高数据速率需求，同时网络节能对降低运营成本和实现可持续发展目标至关重要。需要一种方案在保证用户服务质量的同时提升能源效率。

Method: 提出动态联合天线和功率自适应方案：1) 使用多CSI-RS框架进行天线自适应；2) 采用POLITE（功率感知链路自适应）技术进行功率自适应；3) 根据用户瞬时流量和信道条件变化进行自适应调整，在保证用户吞吐量的同时最大化网络节能。

Result: 数值仿真表明，该方案在不同网络负载条件下都能在网络节能和用户感知吞吐量之间取得平衡。特别是在低负载和轻负载条件下，能显著改善小区内干扰，大幅提升整体网络节能，同时确保用户感知吞吐量不受影响。

Conclusion: 提出的动态联合天线和功率自适应方案能有效提升6G mMIMO系统的网络节能效果，在保证用户服务质量的同时实现能源效率优化，为未来通信系统的可持续发展提供可行方案。

Abstract: Massive multiple-input multiple-output (mMIMO) technology and its future evolutions are expected to address the high data rate demands of sixth generation (6G) communication systems. At the same time, network energy savings (NES) is essential in reducing the operational costs and meeting the sustainability goals of network operators. In this regard, we propose a dynamic scheme for joint antenna and power adaptation to improve NES from a user scheduling and resource allocation perspective. Antenna adaptation is performed using the multiple channel state information resource signal (CSI-RS) framework. Furthermore, the recently introduced transmit power-aware link adaptation scheme, referred to as POLITE for short, is used as the power adaptation technique. The proposed scheme adapts to variations in users' instantaneous traffic and channel conditions to opportunistically maximize NES while also inherently accounting for the user throughput. Numerical simulation results show that the proposed scheme consistently achieves a balance between NES and user perceived throughput (UPT) for different network load conditions. Especially in low and light load conditions, the proposed scheme significantly improves the intra-cell interference and boosts the overall NES, while ensuring that UPT is unaffected.

</details>


### [21] [Shortest LCD embeddings of binary, ternary and quaternary linear codes](https://arxiv.org/abs/2601.20600)
*Junmin An,Ji-Hoon Hong,Jon-Lark Kim,Haeun Lim*

Main category: cs.IT

TL;DR: 研究线性码嵌入到最优LCD码的方法，通过确定生成矩阵需要添加的列数来获得最短LCD嵌入，并发现了多个新的最优LCD码。


<details>
  <summary>Details</summary>
Motivation: 自正交码的研究已取得进展，而LCD码作为自正交码的对偶（具有平凡核），自然的问题是能否将线性码嵌入到最优LCD码中。

Method: 首先确定将线性码嵌入到LCD码所需添加的列数，然后刻画所有可能的最短LCD嵌入形式。从二进制和三元汉明码开始，应用该方法构造最优LCD码。

Result: 发现了多个新的最优LCD码：三元LCD码[23,4,14]、[23,5,12]、[24,6,12]、[25,5,14]和四元LCD码[21,10,8]，这些码的最小距离比已知码大1。

Conclusion: 最短LCD嵌入方法在寻找各种域上的最优LCD码方面是有效的，该方法为构造最优LCD码提供了系统途径。

Abstract: In the recent years, there has been active research on self-orthogonal embeddings of linear codes since they yielded some optimal self-orthogonal codes. LCD codes have a trivial hull so they are counterparts of self-orthogonal codes. So it is a natural question whether one can embed linear codes into optimal LCD codes. To answer it, we first determine the number of columns to be added to a generator matrix of a linear code in order to embed the given code into an LCD code. Then we characterize all possible forms of shortest LCD embeddings of a linear code. As examples, we start from binary and ternary Hamming codes of small lengths and obtain optimal LCD codes with minimum distance 4. Furthermore, we find new ternary LCD codes with parameters including $[23, 4, 14]$, $[23, 5, 12]$, $[24, 6, 12]$, and $[25, 5, 14]$ and a new quaternary LCD $[21, 10, 8]$ code, each of which has minimum distance one greater than those of known codes. This shows that our shortest LCD embedding method is useful in finding optimal LCD codes over various fields.

</details>


### [22] [Helper-Assisted Coding for Gaussian Wiretap Channels: Deep Learning Meets PhySec](https://arxiv.org/abs/2601.20678)
*Vidhi Rana,Remi A. Chou,Taejoon Kim*

Main category: cs.IT

TL;DR: 本文首次设计基于深度学习和密码学工具的短分组长度编码，用于高斯窃听信道中的协作传输，相比无协作方案显著降低信息泄露。


<details>
  <summary>Details</summary>
Motivation: 当窃听者信道噪声小于合法接收者时，传统单发射机无法实现正保密速率。已有方案引入辅助发射机（helper）进行协作，但现有研究多关注渐近分组长度和非构造性编码，缺乏实际可用的短分组长度实现方案。

Method: 提出分层编码设计：1）可靠性层：基于连续干扰消除方法的自编码器架构实现；2）安全层：使用通用哈希函数实现。还提出替代自编码器架构，允许解码器独立估计消息，无需接收端训练时连续消除干扰，显著减少训练时间。

Result: 所提编码方案在信息泄露方面相比无协作的现有编码有严格改进。方案也适用于带辅助发射机的多址窃听信道，其中两个发射机向合法接收者发送保密消息。

Conclusion: 首次通过深度学习和密码学工具设计了显式的短分组长度编码，证明了窃听信道中两个发射机协作的实际可行性和优势，为实际安全通信系统提供了实用方案。

Abstract: Consider the Gaussian wiretap channel, where a transmitter wishes to send a confidential message to a legitimate receiver in the presence of an eavesdropper. It is well known that if the eavesdropper experiences less channel noise than the legitimate receiver, then it is impossible for the transmitter to achieve positive secrecy rates. A known solution to this issue consists in involving a second transmitter, referred to as a helper, to help the first transmitter to achieve security. While such a solution has been studied for the asymptotic blocklength regime and via non-constructive coding schemes, in this paper, for the first time, we design explicit and short blocklength codes using deep learning and cryptographic tools to demonstrate the benefit and practicality of cooperation between two transmitters over the wiretap channel. Specifically, our proposed codes show strict improvement in terms of information leakage compared to existing codes that do not consider a helper. Our code design approach relies on a reliability layer, implemented with an autoencoder architecture based on the successive interference cancellation method, and a security layer implemented with universal hash functions. We also propose an alternative autoencoder architecture that significantly reduces training time by allowing the decoders to independently estimate messages without successively canceling interference by the receiver during training. Additionally, we show that our code design is also applicable to the multiple access wiretap channel with helpers, where two transmitters send confidential messages to the legitimate receiver.

</details>


### [23] [Reflected wireless signals under random spatial sampling](https://arxiv.org/abs/2601.20699)
*H. Paul Keeler*

Main category: cs.IT

TL;DR: 论文提出传播模型，显示随机位置发射器会产生功率直方图中的无界峰值，这些峰值出现在确定性传播模型的转折点处。该现象对估计随机传播效应（如衰落）有直接影响，特别是在涉及墙壁反射时。


<details>
  <summary>Details</summary>
Motivation: 研究智能表面应用中的信号传播特性，特别是理解墙壁反射引起的信号衰落和功率振荡现象。传统上这些现象在波导中研究较多，但在无线网络中相对较少探索。

Method: 提出传播模型，通过简洁的数学论证解释功率直方图中出现无界峰值的机制。将这一基本结果应用于物理模型：单个发射器位于两个平行无源墙壁之间，分析反射引起的信号衰落。

Result: 发现当发射器随机定位时，如果信号强度是距离的振荡或非单调函数，会产生功率直方图中的无界峰值。对于发射器位于两墙中间的特殊情况，给出了包含Lerch超越函数的紧凑闭式表达式。

Conclusion: 该研究揭示了随机传播效应估计中的重要现象，特别是在涉及墙壁反射的场景中。这些见解可以为城市中部署的智能表面的设计决策提供信息。

Abstract: We present a propagation model showing that a transmitter randomly positioned in space generates unbounded peaks in the histogram of the resulting power, provided the signal strength is an oscillating or non-monotonic function of distance. Specifically, these peaks are singularities in the empirical probability density that occur at turning point values of the deterministic propagation model. We explain the underlying mechanism of this phenomenon through a concise mathematical argument. This observation has direct implications for estimating random propagation effects such as fading, particularly when reflections off walls are involved.
  Motivated by understanding intelligent surfaces, we apply this fundamental result to a physical model consisting of a single transmitter between two parallel passive walls. We analyze signal fading due to reflections and observe power oscillations resulting from wall reflections -- a phenomenon long studied in waveguides but relatively unexplored in wireless networks. For the special case where the transmitter is placed halfway between the walls, we present a compact closed-form expression for the received signal involving the Lerch transcendent function. The insights from this work can inform design decisions for intelligent surfaces deployed in cities.

</details>


### [24] [Anytime-Valid Quantum Tomography via Confidence Sequences](https://arxiv.org/abs/2601.20761)
*Aldo Cumitini,Luca Barletta,Osvaldo Simeone*

Main category: cs.IT

TL;DR: 提出一种量子态层析方法，能在测量序列的任何时间点提供包含真实量子态的置信集合，保证用户定义的概率覆盖


<details>
  <summary>Details</summary>
Motivation: 现有量子态层析方法缺乏在测量过程中随时提供不确定性量化能力，需要开发能在任何时间点提供有效置信估计的方法

Method: 基于"随时有效置信序列"的统计方法，将现有QST技术与置信集合结合，保证在测量序列的任何时间点都能提供包含真实态的置信区间

Result: 数值结果证实了所提随时有效QST方法的理论覆盖性质，能够在测量过程中任何时间点提供有效的置信估计

Conclusion: 该方法为量子态层析提供了严格的实时不确定性量化框架，增强了现有QST技术的实用性

Abstract: In this letter, we address the problem of developing quantum state tomography (QST) methods that remain valid at any time during a sequence of measurements. Specifically, the aim is to provide a rigorous quantification of the uncertainty associated with the current state estimate as data are acquired incrementally. To this end, the proposed framework augments existing QST techniques by associating current point estimates of the state with confidence sets that are guaranteed to contain the true quantum state with a user-defined probability. The methodology is grounded in recent statistical advances in anytime-valid confidence sequences. Numerical results confirm the theoretical coverage properties of the proposed anytime-valid QST.

</details>


### [25] [Repeater-Assisted Massive MIMO Full-Duplex Communications](https://arxiv.org/abs/2601.20822)
*Mohammadali Mohammadi,Dhanushka Kudathanthirige,Himal A. Suraweera,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 论文提出了一种用于全双工大规模MIMO系统中继器权重优化的方法，通过最大化加权最小频谱效率之和来提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 在全双工大规模MIMO系统中，多个单天线中继器同时放大和转发信号，需要优化中继器权重以最大化上下行用户设备的频谱效率。

Method: 采用逐次凸逼近技术来解决非凸优化问题，优化每个活动中继器的权重以最大化加权最小频谱效率之和。

Result: 优化后的全双工设计相比半双工系统，频谱效率提升了4倍和2.5倍，性能显著优于无中继辅助和有中继辅助的基准系统。

Conclusion: 提出的中继器权重优化方法能有效提升全双工大规模MIMO系统的频谱效率，为无线网络性能优化提供了有效解决方案。

Abstract: We consider a wireless network comprising multiple singleantenna repeaters that amplify and instantaneously re-transmit received signals in a full-duplex (FD) communication setting. Specifically, we study a massive multiple-input multiple output base station that simultaneously serves multiple uplink (UL) and downlink (DL) user equipment (UE) over the same frequency band. The focus is on the problem of repeater weight optimization at each active repeater to maximize the sum of the weighted minimum spectral efficiencies (SEs) for both UL and DL UEs. The resulting non-convex optimization problem is tackled using a successive convex approximation technique. To demonstrate the effectiveness of the proposed approach, we evaluate its performance against benchmark systems with and without repeater assistance. The optimized FD design achieves SE improvements of up to 4-fold and 2.5-fold compared to its half-duplex counterpart.

</details>


### [26] [Construction and Decoding of Convolutional Codes with optimal Column Distances](https://arxiv.org/abs/2601.20825)
*Julia Lieb,Michael Schaller*

Main category: cs.IT

TL;DR: 本文提出了一种在任意有限域上构造具有最优列距离的卷积码的方法，并证明对于所考虑的参数，所构造的码是唯一达到最优列距离的。这些码的结构与一阶Reed-Muller分组码密切相关，作者利用这一特性开发了针对这些码的简化复杂度Viterbi算法。


<details>
  <summary>Details</summary>
Motivation: 构建最大距离分布(MDP)卷积码通常需要使用非常大的有限域，而具有最优列距离的卷积码可以在任意给定的有限域上最大化列距离。本文旨在解决在任意有限域上构造具有最优列距离的卷积码的问题。

Method: 提出了一种构造具有最优列距离的卷积码的方法。该方法构造的码与一阶Reed-Muller分组码有密切的结构关系，作者利用这种关系开发了针对这些码的简化复杂度Viterbi算法。

Result: 成功构造了在任意有限域上具有最优列距离的卷积码，并证明对于所考虑的参数，所构造的码是唯一达到最优列距离的。此外，基于码与一阶Reed-Muller分组码的结构关系，开发了降低复杂度的Viterbi算法。

Conclusion: 本文提供了一种在任意有限域上构造具有最优列距离卷积码的有效方法，这些码与一阶Reed-Muller分组码有密切联系，这种关系使得能够开发出简化复杂度的Viterbi算法，为实际应用提供了便利。

Abstract: The construction of Maximum Distance Profile (MDP) convolutional codes in general requires the use of very large finite fields. In contrast convolutional codes with optimal column distances maximize the column distances for a given arbitrary finite field. In this paper, we present a construction of such convolutional codes. In addition, we prove that for the considered parameters the codes that we constructed are the only ones achieving optimal column distances. The structure of the presented convolutional codes with optimal column distances is strongly related to first order Reed-Muller block codes and we leverage this fact to develop a reduced complexity version of the Viterbi algorithm for these codes.

</details>


### [27] [Low-Complexity Pilot-Aided Doppler Ambiguity Estimation for OTFS Parametric Channel Estimation](https://arxiv.org/abs/2601.20827)
*Bo-Yuan Chen,Hsuan-Jung Su*

Main category: cs.IT

TL;DR: 提出一种针对OTFS调制在高速移动场景下多普勒模糊问题的低复杂度检测与补偿框架，通过相位差分析检测整数模糊度，再结合MLE进行信道恢复，有效消除误差平台。


<details>
  <summary>Details</summary>
Motivation: 在5G非地面网络等高移动性场景中，低轨卫星的极端轨道速度导致物理多普勒频移超出基本网格范围，产生多普勒模糊问题，使传统MLE信道估计器失效，需要新的解决方案。

Method: 提出两阶段估计器：1) 利用导频符号间的相位差分析检测整数模糊度；2) 结合精化的最大似然估计进行信道恢复。研究了两种导频布置方案：带保护区的嵌入式导频和数据环绕导频，分析干扰抑制与频谱效率的权衡。

Result: 仿真结果表明，所提方案有效消除了由模糊度引起的误差平台，在比特误码率和归一化均方误差性能上达到与穷举搜索基准相当的水平，同时保持与标准MLE相似的计算复杂度。

Conclusion: 该论文提出的低复杂度多普勒模糊检测与补偿框架成功解决了OTFS调制在高速移动环境中的信道估计挑战，为5G非地面网络等高移动性场景提供了有效的解决方案。

Abstract: Orthogonal Time Frequency Space (OTFS) modulation offers robust performance in high-mobility scenarios by transforming time-varying channels into the delay-Doppler (DD) domain. However, in high-mobility environment such as emerging 5G Non-Terrestrial Networks (NTN), the extreme orbital velocities of Low Earth Orbit (LEO) satellites frequently cause the physical Doppler shifts to exceed the fundamental grid range. This Doppler ambiguity induces severe model mismatch and renders traditional MLE channel estimators ineffective. To address this challenge, this paper proposes a novel low-complexity pilot-aided Doppler ambiguity detection and compensation framework. We first mathematically derive the OTFS input-output relationship in the presence of aliasing, revealing that Doppler ambiguity manifests itself as a distinct phase rotation along the delay dimension. Leveraging this insight, we developed a two-stage estimator that utilizes pairwise phase differences between pilot symbols to identify the integer ambiguity, followed by a refined Maximum Likelihood Estimation (MLE) for channel recovery. We investigate two pilot arrangements, Embedded Pilot with Guard Zone (EP-GZ) and Data-Surrounded Pilot (DSP), to analyze the trade-off between interference suppression and spectral efficiency. Simulation results demonstrate that the proposed scheme effectively eliminates the error floor caused by ambiguity, achieving Bit Error Rate (BER) and Normalized Mean Square Error (NMSE) performance comparable to the exhaustive search benchmark while maintaining a computational complexity similar to standard MLE.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [28] [DBTuneSuite: An Extendible Experimental Suite to Test the Time Performance of Multi-layer Tuning Options on Database Management Systems](https://arxiv.org/abs/2601.20015)
*Amani Agrawal,Tianxin Wang,Dennis Shasha*

Main category: cs.DB

TL;DR: DBTuneSuite是一个针对四种广泛部署的免费数据库系统的实验套件，测试它们在各种查询/更新负载和调优选项下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 为数据库系统工程师、高级用户、故障排除人员和学生提供实用的性能测试工具和指导，帮助理解不同数据库系统在各种工作负载下的表现差异。

Method: 开发了一个包含数据生成脚本、安装和运行测试脚本的实验套件，可扩展到其他测试和系统，对四种广泛部署的免费数据库系统进行性能测试。

Result: 提供了不同系统适合哪些查询类型的建议，并给出了量化证据表明实践中广泛使用的调优选项在不同系统中表现差异很大。

Conclusion: DBTuneSuite是一个有价值的工具，可以帮助用户选择适合特定工作负载的数据库系统，并理解调优选项的系统间差异。

Abstract: DBTuneSuite is a suite of experiments on four widely deployed free database systems to test their performance under various query/upsert loads and under various tuning options. The suite provides: (i) scripts to generate data and to install and run tests, making it expandable to other tests and systems; (ii) suggestions of which systems work best for which query types; and (iii) quantitative evidence that tuning options widely used in practice can behave very differently across systems. This paper is most useful for database system engineers, advanced database users and troubleshooters, and students.

</details>


### [29] [Delta Fair Sharing: Performance Isolation for Multi-Tenant Storage Systems](https://arxiv.org/abs/2601.20030)
*Tyler Griggs,Soujanya Ponnapalli,Dev Bali,Wenjie Ma,James DeLoye,Audrey Cheng,Jaewan Hong,Natacha Crooks,Scott Shenker,Ion Stoica,Matei Zaharia*

Main category: cs.DB

TL;DR: Delta Fair Sharing算法家族为高抢占延迟的存储系统提供性能隔离，通过δ-公平性和δ-帕累托效率性，将客户端获取资源的延迟限制在δ时间单位内，并确保高利用率。


<details>
  <summary>Details</summary>
Motivation: 现代云存储系统需要为多租户提供性能隔离，但传统公平共享方法因资源（如写缓冲区和读缓存）的高抢占延迟而失效，导致客户端尾部延迟出现不可接受的尖峰。

Method: 提出Delta Fair Sharing算法家族，满足两个关键属性：δ-公平性（将客户端接收公平资源份额的延迟限制在δ时间单位内）和δ-帕累托效率性（将未使用资源分配给需求未满足的客户端）。在FAIRDB（RocksDB的扩展）中实现这些算法。

Result: FAIRDB评估显示，相比最先进的替代方案，FAIRDB能更好地将行为良好的客户端与高需求工作负载隔离开来。

Conclusion: Delta Fair Sharing算法通过端到端捕获资源获取延迟，将行为良好客户端的尾部延迟尖峰限制在δ时间单位内，同时确保高利用率，为高抢占延迟的存储系统提供了有效的性能隔离解决方案。

Abstract: Modern storage systems, often deployed to support multiple tenants in the cloud, must provide performance isolation. Unfortunately, traditional approaches such as fair sharing do not provide performance isolation for storage systems, because their resources (e.g., write buffers and read caches) exhibit high preemption delays. These delays lead to unacceptable spikes in client tail latencies, as clients may be forced to wait arbitrarily long to receive their fair share of resources.
  We introduce Delta Fair Sharing, a family of algorithms for sharing resources with high preemption delays. These algorithms satisfy two key properties: $δ$-fairness, which bounds a client's delay in receiving its fair share of resources to $δ$ time units, and $δ$-Pareto-efficiency, which allocates unused resources to clients with unmet demand. Together, these properties capture resource-acquisition delays end-to-end, bound well-behaved clients' tail-latency spikes to $δ$ time units, and ensure high utilization. We implement such algorithms in FAIRDB, an extension of RocksDB. Our evaluation shows that FAIRDB isolates well-behaved clients from high-demand workloads better than state-of-the-art alternatives.

</details>


### [30] [ConStruM: A Structure-Guided LLM Framework for Context-Aware Schema Matching](https://arxiv.org/abs/2601.20482)
*Houming Chen,Zhe Zhang,H. V. Jagadish*

Main category: cs.DB

TL;DR: ConStruM是一个结构引导的预算证据打包框架，用于提升LLM在模式匹配中的性能，通过构建轻量级可重用结构，在查询时组装小型上下文包强调最具区分性的证据。


<details>
  <summary>Details</summary>
Motivation: 在数据集成中，模式匹配是核心任务，列名和描述对此很有价值，LLM可以利用这些自然语言模式元数据。然而，许多数据集的正确匹配需要超出列本身的额外证据。由于向LLM提供捕获这些证据所需的整个模式元数据不切实际，核心挑战是选择和组织最有用的上下文信息。

Method: ConStruM构建轻量级可重用结构，在查询时组装小型上下文包强调最具区分性的证据。作为附加组件，给定上游匹配器产生的候选目标短列表，它用结构化、查询特定的证据增强匹配器的最终LLM提示。开发了用于预算多级上下文检索的上下文树和全局相似性超图，该超图显示高度相似列组（源和目标两侧），通过在线计算或离线预计算的组感知区分线索进行总结。

Result: 在真实数据集上的实验表明，ConStruM通过提供和组织正确的上下文证据改进了匹配性能。

Conclusion: ConStruM是一个有效的结构引导框架，能够通过预算证据打包策略提升LLM在模式匹配任务中的表现，解决了提供适当上下文证据的核心挑战。

Abstract: Column matching is a central task in reconciling schemas for data integration. Column names and descriptions are valuable for this task. LLMs can leverage such natural-language schema metadata. However, in many datasets, correct matching requires additional evidence beyond the column itself. Because it is impractical to provide an LLM with the entire schema metadata needed to capture this evidence, the core challenge becomes to select and organize the most useful contextual information.
  We present ConStruM, a structure-guided framework for budgeted evidence packing in schema matching. ConStruM constructs a lightweight, reusable structure in which, at query time, it assembles a small context pack emphasizing the most discriminative evidence. ConStruM is designed as an add-on: given a shortlist of candidate targets produced by an upstream matcher, it augments the matcher's final LLM prompt with structured, query-specific evidence so that the final selection is better grounded. For this purpose, we develop a context tree for budgeted multi-level context retrieval and a global similarity hypergraph that surfaces groups of highly similar columns (on both the source and target sides), summarized via group-aware differentiation cues computed online or precomputed offline. Experiments on real datasets show that ConStruM improves matching by providing and organizing the right contextual evidence.

</details>


### [31] [ALER: An Active Learning Hybrid System for Efficient Entity Resolution](https://arxiv.org/abs/2601.20664)
*Dimitrios Karapiperis,Leonidas Akritidis,Panayiotis Bozanis,Vassilios Verykios*

Main category: cs.DB

TL;DR: ALER：一种新颖的半监督实体解析流水线，通过冻结双编码器架构和轻量级分类器解决传统主动学习方法的计算瓶颈，在保持高精度的同时大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前监督深度学习实体解析模型需要大量标注数据，而主动学习方法虽然能缓解标注稀缺问题，但存在严重的可扩展性瓶颈：要么需要从头重新训练复杂模型，要么需要解决NP难的选择问题，计算成本过高。

Method: ALER采用半监督流水线设计：1) 使用冻结的双编码器架构生成静态嵌入，避免重复训练；2) 在嵌入上迭代训练轻量级分类器；3) 通过K-Means对代表性样本进行语义分块，解决大规模候选池的内存瓶颈；4) 提出结合"困惑"和"自信"对的混合查询策略，高效优化决策边界并纠正高置信度错误。

Result: 在大型DBLP数据集上的评估显示，ALER训练循环加速1.3倍，解析延迟降低3.8倍，同时保持高精度，显著优于现有基线方法。

Conclusion: ALER成功解决了实体解析中语义精度与计算可扩展性之间的权衡问题，为大规模实际应用提供了高效可行的解决方案。

Abstract: Entity Resolution (ER) is a critical task for data integration, yet state-of-the-art supervised deep learning models remain impractical for many real-world applications due to their need for massive, expensive-to-obtain labeled datasets. While Active Learning (AL) offers a potential solution to this "label scarcity" problem, existing approaches introduce severe scalability bottlenecks. Specifically, they achieve high accuracy but incur prohibitive computational costs by re-training complex models from scratch or solving NP-hard selection problems in every iteration. In this paper, we propose ALER, a novel, semi-supervised pipeline designed to bridge the gap between semantic accuracy and computational scalability. ALER eliminates the training bottleneck by using a frozen bi-encoder architecture to generate static embeddings once and then iteratively training a lightweight classifier on top. To address the memory bottleneck associated with large-scale candidate pools, we first select a representative sample of the data and then use K-Means to partition this sample into semantically coherent chunks, enabling an efficient AL loop. We further propose a hybrid query strategy that combines "confused" and "confident" pairs to efficiently refine the decision boundary while correcting high-confidence errors.Extensive evaluation demonstrates ALER's superior efficiency, particularly on the large-scale DBLP dataset: it accelerates the training loop by 1.3x while drastically reducing resolution latency by a factor of 3.8 compared to the fastest baseline.

</details>


### [32] [The Monotone Priority System: Foundations of Contract-Specific Sequencing](https://arxiv.org/abs/2601.20783)
*Naveen Durvasula*

Main category: cs.DB

TL;DR: 提出基于全局优先级的智能合约调用排序系统，通过优先级约束保证交易顺序，满足五个独立公理


<details>
  <summary>Details</summary>
Motivation: 现代区块链应用需要为交易交互指定排序约束，但需要在表达能力和区块生产的可处理性之间取得平衡

Method: 允许合约开发者为每个调用设置整数全局优先级，要求调用的优先级不高于其引用的任何调用，区块生产者按优先级从高到低排序交易

Result: 证明该系统是唯一满足五个独立公理的排序系统

Conclusion: 提出的基于优先级的排序系统在表达能力和可处理性之间取得了良好平衡，为智能合约调用排序提供了公理化的解决方案

Abstract: Modern blockchain applications benefit from the ability to specify sequencing constraints on the transactions that interact with them. This paper proposes a principled and axiomatically justified way of adding sequencing constraints on smart contract function calls that balances expressivity with the tractability of block production. Specifically, we propose a system in which contract developers are allowed to set an integer global priority for each of their calls, so long as that the call's chosen priority is no higher than the priority of any of its referenced calls. Block builders must then simply sequence transactions in priority order (from high to low priority), breaking ties however they would like. We show that this system is the unique system that satisfies five independent axioms.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [33] [SFQA: A Comprehensive Perceptual Quality Assessment Dataset for Singing Face Generation](https://arxiv.org/abs/2601.20385)
*Zhilin Gao,Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Guangtao Zhai*

Main category: cs.MM

TL;DR: 论文提出了首个歌唱人脸生成质量评估数据集SFQA，包含5,184个视频，基于12种生成方法和主观评估，为歌唱人脸生成领域提供质量评估基准。


<details>
  <summary>Details</summary>
Motivation: 歌唱作为仅次于说话的第二常见面部运动，在数字人和智能体中有广泛应用潜力，但现有歌唱人脸生成质量参差不齐，缺乏有效的质量评估方法和数据集。

Method: 使用12种代表性生成方法，基于100张照片/肖像和36个音乐片段（7种风格），生成5,184个歌唱人脸视频构建SFQA数据集，并进行主观质量评估。

Result: 主观评估显示不同生成方法质量差异显著，基于SFQA数据集对现有客观质量评估算法进行全面基准测试。

Conclusion: SFQA数据集填补了歌唱人脸生成质量评估领域的空白，为未来方法开发和评估提供了重要基准。

Abstract: The Talking Face Generation task has enormous potential for various applications in digital humans and agents, etc. Singing, as a common facial movement second only to talking, can be regarded as a universal language across ethnicities and cultures. However, it is often underestimated in the field due to lack of singing face datasets and the domain gap between singing and talking in rhythm and amplitude. More significantly, the quality of Singing Face Generation (SFG) often falls short and is uneven or limited by different applicable scenarios, which prompts us to propose timely and effective quality assessment methods to ensure user experience. To address existing gaps in this domain, this paper introduces a new SFG content quality assessment dataset SFQA, built using 12 representative generation methods. During the construction of the dataset, 100 photographs or portraits, as well as 36 music clips from 7 different styles, are utilized to generate 5,184 singing face videos that constitute the SFQA dataset. To further explore the quality of SFG methods, subjective quality assessment is conducted by evaluators, whose ratings reveal a significant variation in quality among different generation methods. Based on our proposed SFQA dataset, we comprehensively benchmark the current objective quality assessment algorithms.

</details>


### [34] [Block Erasure-Aware Semantic Multimedia Compression via JSCC Autoencoder](https://arxiv.org/abs/2601.20707)
*Homa Esfahanizadeh,Nargis Fayaz,Jinfeng Du,Harish Viswanathan*

Main category: cs.MM

TL;DR: 提出基于AI的多媒体语义传输框架，在带宽受限时变信道中实现可靠语义重建，无需重传，支持智能拥塞控制和不等差错保护


<details>
  <summary>Details</summary>
Motivation: 针对视频会议、机器人控制等时延敏感应用中，大容量内容分多包传输时可能因信道损伤丢包的问题，传统重传机制会导致不可接受的延迟

Method: 采用联合信源信道编码（JSCC）框架，通过可调设计参数平衡低信道质量下的鲁棒性和高信道质量下的保真度，兼容现有网络协议

Result: 实验表明在图像和视频领域相比最先进基线方法有显著鲁棒性提升，随着信道条件恶化能实现优雅的质量降级

Conclusion: 该AI框架为时延敏感应用提供可靠的语义传输解决方案，无需重传机制，支持智能拥塞控制和不等差错保护，具有实际部署价值

Abstract: We present an AI-based framework for semantic transmission of multimedia data over band-limited, time-varying channels. The method targets scenarios where large content is split into multiple packets, with an unknown number potentially dropped due to channel impairments. Using joint source-channel coding (JSCC), our approach achieves reliable semantic reconstruction with graceful quality degradation as channel conditions worsen, eliminating the need for retransmissions that cause unacceptable delays in latency-sensitive applications such as video conferencing and robotic control. The framework is compatible with existing network protocols and further enables intelligent congestion control and unequal error protection. A tunable design parameter allows balancing robustness at low channel quality against fidelity at high channel quality. Experiments demonstrate significant robustness improvement over state-of-the-art baselines in both image and video domains.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [35] [Defensive Rebalancing for Automated Market Makers](https://arxiv.org/abs/2601.19950)
*Sam Devorsetz,Maurice Herlihy*

Main category: cs.GT

TL;DR: 论文提出防御性再平衡机制，通过直接转移资产保护CFMM免受套利价值泄漏，证明存在将套利倾向配置转为套利自由配置的再平衡方法，且套利自由配置等价于帕累托效率配置。


<details>
  <summary>Details</summary>
Motivation: 保护恒定函数做市商免受套利导致的价值泄漏，为流动性提供者提供主动防御机制，解决传统CFMM在价格波动时面临的套利攻击问题。

Method: 提出防御性再平衡机制，允许CFMM池间直接资产转移绕过标准交易协议；证明对于任何套利倾向配置，存在转向套利自由配置的再平衡；将最优再平衡问题建模为凸优化问题；扩展至混合再平衡框架，结合直接转移和标准交易。

Result: 证明套利自由配置当且仅当是帕累托效率配置；对于对数凹交易函数（包括恒定乘积做市商），最优再平衡问题可转化为凸优化问题且有唯一可计算解；混合再平衡框架可从非参与CFMM和中心化交易所等价格预言机做市商获取套利利润。

Conclusion: 防御性再平衡为未来AMM协议提供了理论基础，使流动性提供者能够主动防御套利攻击，提高系统整体效率，并为协议设计提供了计算可行的解决方案。

Abstract: This paper introduces and analyzes \emph{defensive rebalancing}, a novel mechanism for protecting constant-function market makers (CFMMs) from value leakage due to arbitrage. A \emph{rebalancing} transfers assets directly from one CFMM's pool to another's, bypassing the CFMMs' standard trading protocols. In any \emph{arbitrage-prone} configuration, we prove there exists a rebalancing to an \textit{arbitrage-free} configuration that strictly increases some CFMMs' liquidities without reducing the liquidities of the others. Moreover, we prove that a configuration is arbitrage-free if and only if it is \emph{Pareto efficient} under rebalancing, meaning that any further direct asset transfers must decrease some CFMM's liquidity. We prove that for any log-concave trading function, including the ubiquitous constant product market maker, the search for an optimal, arbitrage-free rebalancing that maximizes global liquidity while ensuring no participant is worse off can be cast as a convex optimization problem with a unique, computationally tractable solution. We extend this framework to \emph{mixed rebalancing}, where a subset of participating CFMMs use a combination of direct transfers and standard trades to transition to an arbitrage-free configuration while harvesting arbitrage profits from non-participating CFMMs, and from price oracle market makers such as centralized exchanges. Our results provide a rigorous foundation for future AMM protocols that proactively defend liquidity providers against arbitrage.

</details>


### [36] [Guiding the Recommender: Information-Aware Auto-Bidding for Content Promotion](https://arxiv.org/abs/2601.20422)
*Yumou Liu,Zhenzhe Zheng,Jiang Rong,Yao Hu,Fan Wu,Guihai Chen*

Main category: cs.GT

TL;DR: 论文提出了一种新的内容推广框架，将短期价值获取与长期模型改进相结合，通过梯度覆盖度优化和两阶段自动出价算法，提升推荐系统的长期性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现当前内容平台付费推广存在反直觉缺陷：虽然能拯救中低质量内容，但会损害高质量内容，因为强制曝光给次优受众会污染参与信号并降低未来推荐质量。需要平衡短期价值获取与长期模型改进。

Method: 1) 引入可分解代理目标"梯度覆盖度"，建立其与Fisher信息和最优实验设计的理论联系；2) 设计基于拉格朗日对偶的两阶段自动出价算法，通过影子价格动态控制预算，使用边际效用优化单次曝光出价；3) 提出置信门控梯度启发式方法，以及用于黑盒模型的零阶变体，实时估计学习信号。

Result: 在合成和真实数据集上的离线实验显示：该框架优于基线方法，获得更好的最终AUC/LogLoss，紧密遵循预算目标，在梯度近似为零阶时仍保持有效。理论保证包括复合目标的单调子模性、在线拍卖中的次线性遗憾和预算可行性。

Conclusion: 战略性、信息感知的内容推广能够超越简单的曝光最大化策略，改善长期模型性能和有机结果，为内容平台提供更优的推广范式。

Abstract: Modern content platforms offer paid promotion to mitigate cold start by allocating exposure via auctions. Our empirical analysis reveals a counterintuitive flaw in this paradigm: while promotion rescues low-to-medium quality content, it can harm high-quality content by forcing exposure to suboptimal audiences, polluting engagement signals and downgrading future recommendation. We recast content promotion as a dual-objective optimization that balances short-term value acquisition with long-term model improvement. To make this tractable at bid time in content promotion, we introduce a decomposable surrogate objective, gradient coverage, and establish its formal connection to Fisher Information and optimal experimental design. We design a two-stage auto-bidding algorithm based on Lagrange duality that dynamically paces budget through a shadow price and optimizes impression-level bids using per-impression marginal utilities. To address missing labels at bid time, we propose a confidence-gated gradient heuristic, paired with a zeroth-order variant for black-box models that reliably estimates learning signals in real time. We provide theoretical guarantees, proving monotone submodularity of the composite objective, sublinear regret in online auction, and budget feasibility. Extensive offline experiments on synthetic and real-world datasets validate the framework: it outperforms baselines, achieves superior final AUC/LogLoss, adheres closely to budget targets, and remains effective when gradients are approximated zeroth-order. These results show that strategic, information-aware promotion can improve long-term model performance and organic outcomes beyond naive impression-maximization strategies.

</details>


### [37] [Inequality in Congestion Games with Learning Agents](https://arxiv.org/abs/2601.20578)
*Dimitris Michailidis,Sennay Ghebreab,Fernando P. Santos*

Main category: cs.GT

TL;DR: 交通网络扩张可能同时提高效率但加剧不平等，因为不同学习速度的通勤者适应新路线的能力不同，导致资源获取不均。


<details>
  <summary>Details</summary>
Motivation: 研究交通网络扩张如何影响社会不平等，关注网络结构本身和通勤者适应能力差异如何共同导致效率与公平的权衡。

Method: 将通勤者建模为强化学习智能体，设置不同学习率反映资源信息获取不平等；提出"学习代价"衡量学习过程中的效率损失；分析双源节点的Braess悖论式网络和阿姆斯特丹地铁系统抽象模型。

Result: 网络扩张可能同时提高效率和加剧不平等，特别是当快速学习者在新路线开放后能更快适应并获取优势时；学习过程中的效率损失显著。

Conclusion: 交通政策不仅要考虑均衡结果，还必须考虑通勤者适应过程的异质性，因为两者共同决定了效率与公平的平衡。

Abstract: Who benefits from expanding transport networks? While designed to improve mobility, such interventions can also create inequality. In this paper, we show that disparities arise not only from the structure of the network itself but also from differences in how commuters adapt to it. We model commuters as reinforcement learning agents who adapt their travel choices at different learning rates, reflecting unequal access to resources and information. To capture potential efficiency-fairness tradeoffs, we introduce the Price of Learning (PoL), a measure of inefficiency during learning. We analyze both a stylized network -- inspired in the well-known Braess's paradox, yet with two-source nodes -- and an abstraction of a real-world metro system (Amsterdam). Our simulations show that network expansions can simultaneously increase efficiency and amplify inequality, especially when faster learners disproportionately benefit from new routes before others adapt. These results highlight that transport policies must account not only for equilibrium outcomes but also for the heterogeneous ways commuters adapt, since both shape the balance between efficiency and fairness.

</details>


### [38] [Independence of Approximate Clones](https://arxiv.org/abs/2601.20779)
*Théo Delemazure*

Main category: cs.GT

TL;DR: 研究序数选举中的近似克隆概念，探讨现有克隆独立性公理在近似克隆情况下的适用性，并通过实证分析验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 传统克隆独立性公理要求完美克隆（所有选民将两个候选人相邻排序），但现实中完美克隆几乎不可能出现。因此需要研究近似克隆情况下的独立性，使理论更贴近实际选举场景。

Method: 1) 定义两种量化近似克隆程度的度量方法；2) 理论分析现有满足完美克隆独立性的投票规则（如IRV、Ranked Pairs、Schulze）在近似克隆情况下的表现；3) 使用三个真实数据集进行实证研究：苏格兰地方选举、小型陪审团审议、花样滑冰裁判评分。

Result: 1) 对于至少四个候选人的选举，所有已知满足完美克隆独立性的规则在一般情况下都不满足近似克隆独立性；2) 对于三个候选人的情况有更积极的结果；3) 实证发现近似克隆在某些情境中很常见，且两个候选人越接近完美克隆，移除他们改变选举结果的可能性越小，特别是对那些满足完美克隆独立性的规则。

Conclusion: 完美克隆独立性公理在现实选举中过于严格，近似克隆概念更实用。虽然现有投票规则在一般意义上不满足近似克隆独立性，但实证表明它们在实际中表现良好，特别是当候选人非常接近完美克隆时。这为投票规则设计提供了更现实的评估框架。

Abstract: In an ordinal election, two candidates are said to be perfect clones if every voter ranks them adjacently. The independence of clones axiom then states that removing one of the two clones should not change the election outcome. This axiom has been extensively studied in social choice theory, and several voting rules are known to satisfy it (such as IRV, Ranked Pairs and Schulze). However, perfect clones are unlikely to occur in practice, especially for political elections with many voters.
  In this work, we study different notions of approximate clones in ordinal elections. Informally, two candidates are approximate clones in a preference profile if they are close to being perfect clones. We discuss two measures to quantify this proximity, and we show under which conditions the voting rules that are known to be independent of clones are also independent of approximate clones. In particular, we show that for elections with at least four candidates, none of these rules are independent of approximate clones in the general case. However, we find a more positive result for the case of three candidates. Finally, we conduct an empirical study of approximate clones and independence of approximate clones based on three real-world datasets: votes in local Scottish elections, votes in mini-jury deliberations, and votes of judges in figure skating competitions. We find that approximate clones are common in some contexts, and that the closest two candidates are to being perfect clones, the less likely their removal is to change the election outcome, especially for voting rules that are independent of perfect clones.

</details>
