{"id": "2512.06334", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06334", "abs": "https://arxiv.org/abs/2512.06334", "authors": ["Van-Thinh Vo", "Minh-Khoi Nguyen", "Minh-Huy Tran", "Anh-Quan Nguyen-Tran", "Duy-Tan Nguyen", "Khanh-Loi Nguyen", "Anh-Minh Phan"], "title": "Enhanced Multimodal Video Retrieval System: Integrating Query Expansion and Cross-modal Temporal Event Retrieval", "comment": "11 pages, 6 figures, SOICT 2025", "summary": "Multimedia information retrieval from videos remains a challenging problem. While recent systems have advanced multimodal search through semantic, object, and OCR queries - and can retrieve temporally consecutive scenes - they often rely on a single query modality for an entire sequence, limiting robustness in complex temporal contexts. To overcome this, we propose a cross-modal temporal event retrieval framework that enables different query modalities to describe distinct scenes within a sequence. To determine decision thresholds for scene transition and slide change adaptively, we build Kernel Density Gaussian Mixture Thresholding (KDE-GMM) algorithm, ensuring optimal keyframe selection. These extracted keyframes act as compact, high-quality visual exemplars that retain each segment's semantic essence, improving retrieval precision and efficiency. Additionally, the system incorporates a large language model (LLM) to refine and expand user queries, enhancing overall retrieval performance. The proposed system's effectiveness and robustness were demonstrated through its strong results in the Ho Chi Minh AI Challenge 2025."}
{"id": "2512.06381", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06381", "abs": "https://arxiv.org/abs/2512.06381", "authors": ["Tao Wang", "Xun Luo", "Jinlong Guo", "Yuliang Yan", "Jian Wu", "Yuning Jiang", "Bo Zheng"], "title": "Beyond Existing Retrievals: Cross-Scenario Incremental Sample Learning Framework", "comment": null, "summary": "The parallelized multi-retrieval architecture has been widely adopted in large-scale recommender systems for its computational efficiency and comprehensive coverage of user interests. Many retrieval methods typically integrate additional cross-scenario samples to enhance the overall performance ceiling. However, those model designs neglect the fact that a part of the cross-scenario samples have already been retrieved by existing models within a system, leading to diminishing marginal utility in delivering incremental performance gains. In this paper, we propose a novel retrieval framework IncRec, specifically for cross-scenario incremental sample learning. The innovations of IncRec can be highlighted as two aspects. Firstly, we construct extreme cross-scenario incremental samples that are not retrieved by any existing model. And we design an incremental sample learning framework which focuses on capturing incremental representation to improve the overall retrieval performance. Secondly, we introduce a consistency-aware alignment module to further make the model prefer incremental samples with high exposure probability. Extensive offline and online A/B tests validate the superiority of our framework over state-of-the-art retrieval methods. In particular, we deploy IncRec in the Taobao homepage recommendation, achieving a 1% increase in online transaction count, demonstrating its practical applicability."}
{"id": "2512.06449", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06449", "abs": "https://arxiv.org/abs/2512.06449", "authors": ["Jaewon Ahn", "Woosung Jang", "Beakcheol Jang"], "title": "Enhancing Medical Cross-Modal Hashing Retrieval using Dropout-Voting Mixture-of-Experts Fusion", "comment": "5 pages, 1 figure, workshop paper (MMGenSR 2025)", "summary": "In recent years, cross-modal retrieval using images and text has become an active area of research, especially in the medical domain. The abundance of data in various modalities in this field has led to a growing importance of cross-modal retrieval for efficient image interpretation, data-driven diagnostic support, and medical education. In the context of the increasing integration of distributed medical data across healthcare facilities with the objective of enhancing interoperability, it is imperative to optimize the performance of retrieval systems in terms of the speed, memory efficiency, and accuracy of the retrieved data. This necessity arises in response to the substantial surge in data volume that characterizes contemporary medical practices. In this study, we propose a novel framework that incorporates dropout voting and mixture-of-experts (MoE) based contrastive fusion modules into a CLIP-based cross-modal hashing retrieval structure. We also propose the application of hybrid loss. So we now call our model MCMFH which is a medical cross-modal fusion hashing retrieval. Our method enables the simultaneous achievement of high accuracy and fast retrieval speed in low-memory environments. The model is demonstrated through experiments on radiological and non-radiological medical datasets."}
{"id": "2512.06590", "categories": ["cs.IR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06590", "abs": "https://arxiv.org/abs/2512.06590", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "comment": "8 Pages", "summary": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost."}
{"id": "2512.07209", "categories": ["cs.MM", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07209", "abs": "https://arxiv.org/abs/2512.07209", "authors": ["Masato Ishii", "Akio Hayakawa", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits", "comment": null, "summary": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity."}
{"id": "2512.06211", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06211", "abs": "https://arxiv.org/abs/2512.06211", "authors": ["Martin G. Herold", "Evangelos Kipouridis", "Joachim Spoerhase"], "title": "A Broader View on Clustering under Cluster-Aware Norm Objectives", "comment": null, "summary": "We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.\n  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\\widetilde{O}(\\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\\widetilde{O}(\\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.\n  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$."}
{"id": "2512.06180", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.06180", "abs": "https://arxiv.org/abs/2512.06180", "authors": ["Jérôme Renault", "Eilon Solan", "Nicolas Vieille"], "title": "Strategic Experimentation with Private Payoffs", "comment": null, "summary": "We study a strategic experimentation game with exponential bandits, in which experiment outcomes are private. The equilibrium amount of experimentation is always higher than in the benchmark case where experiment outcomes are publicly observed. In addition, for pure equilibria, the equilibrium amount of experimentation is at least socially optimal, and possibly higher. We provide a tight bound on the degree of over-experimentation. The analysis rests on a new form of encouragement effect, according to which a player may hide the absence of a success to encourage future experimentation by the other player, which incentivizes current experimentation."}
{"id": "2512.06636", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.06636", "abs": "https://arxiv.org/abs/2512.06636", "authors": ["Chao Zhang", "Renée J. Miller"], "title": "Distribution-Aware Exploration for Adaptive HNSW Search", "comment": "Accepted for publication in SIGMOD 2026", "summary": "Hierarchical Navigable Small World (HNSW) is widely adopted for approximate nearest neighbor search (ANNS) for its ability to deliver high recall with low latency on large-scale, high-dimensional embeddings. The exploration factor, commonly referred to as ef, is a key parameter in HNSW-based vector search that balances accuracy and efficiency. However, existing systems typically rely on manually and statically configured ef values that are uniformly applied across all queries. This results in a distribution-agnostic configuration that fails to account for the non-uniform and skewed nature of real-world embedding data and query workloads. As a consequence, HNSW-based systems suffer from two key practical issues: (i) the absence of recall guarantees, and (ii) inefficient ANNS performance due to over- or under-searching. In this paper, we propose Adaptive-ef (Ada-ef), a data-driven, update-friendly, query-adaptive approach that dynamically configures ef for each query at runtime to approximately meet a declarative target recall with minimal computation. The core of our approach is a theoretically grounded statistical model that captures the similarity distribution between each query and the database vectors. Based on this foundation, we design a query scoring mechanism that distinguishes between queries requiring only small ef and those that need larger ef to meet a target recall, and accordingly assigns an appropriate ef to each query. Experimental results on real-world embeddings produced by state-of-the-art Transformer models from OpenAI and Cohere show that, compared with state-of-the-art learning-based adaptive approaches, our method achieves the target recall while avoiding both over- and under-searching, reducing online query latency by up to 4x, offline computation time by 50x, and offline memory usage by 100x."}
{"id": "2512.06156", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06156", "abs": "https://arxiv.org/abs/2512.06156", "authors": ["Jiasi Zhou", "Chintha Tellambura"], "title": "Hybrid Beamfocusing Design for RSMA-Enabled Near-Field Wideband Communications", "comment": "13pages and 9 figures", "summary": "Future wireless networks will utilize extremely large-scale antenna arrays (ELAAs) over high-frequency bands, which, however, produce near-field spherical wavefronts and spatial wideband effects. To exploit and mitigate these, this paper proposes a rate-splitting multiple access (RSMA)-enabled transmit scheme for wideband near-field communications (NFC). Our solution leverages true-time-delay (TTD)-based hybrid beamfocusing architectures to mitigate spatial wideband effect and reduce radio frequency chain requirements. The objective is to maximize the minimum rate by jointly optimizing frequency-dependent analog beamfocusing, frequency-independent analog beamfocusing, digital beamfocusing, and common rate allocation. To solve this complicated non-convex problem, we develop a penalty-based iterative algorithm that partitions the variables into three blocks and then employs block coordinate descent (BCD) to optimize each block alternately. This algorithm is further extended to support the sub-connected TTD-based analog beamfocusing architectures. Comprehensive simulation results indicate that our transmit scheme: 1) effectively compensates for spatial wideband effect, addressing a critical challenge in wideband operation; 2) achieves performance comparable to full digital beamfocusing while maintaining lower hardware complexity; 3) achieves substantial performance gains over the other two benchmarks."}
{"id": "2512.06641", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06641", "abs": "https://arxiv.org/abs/2512.06641", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "An Index-based Approach for Efficient and Effective Web Content Extraction", "comment": null, "summary": "As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages."}
{"id": "2512.06383", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.06383", "abs": "https://arxiv.org/abs/2512.06383", "authors": ["Tesshu Hanaka", "Yuto Okada", "Yota Otachi", "Lena Volk"], "title": "Finding a Maximum Common (Induced) Subgraph: Structural Parameters Revisited", "comment": "16 pages, 1 figure, WALCOM 2026", "summary": "We study the parameterized complexity of the problems of finding a maximum common (induced) subgraph of two given graphs. Since these problems generalize several NP-complete problems, they are intractable even when parameterized by strongly restricted structural parameters. Our contribution in this paper is to sharply complement the hardness of the problems by showing fixed-parameter tractable cases: both induced and non-induced problems parameterized by max-leaf number and by neighborhood diversity, and the induced problem parameterized by twin cover number. These results almost completely determine the complexity of the problems with respect to well-studied structural parameters. Also, the result on the twin cover number presents a rather rare example where the induced and non-induced cases have different complexity."}
{"id": "2512.06585", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.06585", "abs": "https://arxiv.org/abs/2512.06585", "authors": ["Frederick V. Qiu", "S. Matthew Weinberg", "Qianfan Zhang"], "title": "The Communication Complexity of Combinatorial Auctions with Additional Succinct Bidders", "comment": "39 pages, 2 figures, to appear in SODA 2026", "summary": "We study the communication complexity of welfare maximization in combinatorial auctions with bidders from either a standard valuation class (which require exponential communication to explicitly state, such as subadditive or XOS), or arbitrary succinct valuations (which can be fully described in polynomial communication, such as single-minded). Although succinct valuations can be efficiently communicated, we show that additional succinct bidders have a nontrivial impact on communication complexity of classical combinatorial auctions. Specifically, let $n$ be the number of subadditive/XOS bidders. We show that for SA $\\cup$ SC (the union of subadditive and succinct valuations): (1) There is a polynomial communication $3$-approximation algorithm; (2) As $n \\to \\infty$, there is a matching $3$-hardness of approximation, which (a) is larger than the optimal approximation ratio of $2$ for SA, and (b) holds even for SA $\\cup$ SM (the union of subadditive and single-minded valuations); and (3) For all $n \\geq 3$, there is a constant separation between the optimal approximation ratios for SA $\\cup$ SM and SA (and therefore between SA $\\cup$ SC and SA as well). Similarly, we show that for XOS $\\cup$ SC: (1) There is a polynomial communication $2$-approximation algorithm; (2) As $n \\to \\infty$, there is a matching $2$-hardness of approximation, which (a) is larger than the optimal approximation ratio of $e/(e-1)$ for XOS, and (b) holds even for XOS $\\cup$ SM; and (3) For all $n \\geq 2$, there is a constant separation between the optimal approximation ratios for XOS $\\cup$ SM and XOS (and therefore between XOS $\\cup$ SC and XOS as well)."}
{"id": "2512.06743", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.06743", "abs": "https://arxiv.org/abs/2512.06743", "authors": ["Guanjie Zheng", "Ziyang Su", "Yiheng Wang", "Yuhang Luo", "Hongwei Zhang", "Xuanhe Zhou", "Linghe Kong", "Fan Wu", "Wen Ling"], "title": "OSM+: Billion-Level Open Street Map Data Processing System for City-wide Experiments", "comment": null, "summary": "Road network data can provide rich information about cities and thus become the base for various urban research. However, processing large volume world-wide road network data requires intensive computing resources and the processed results might be different to be unified for testing downstream tasks. Therefore, in this paper, we process the OpenStreetMap data via a distributed computing of 5,000 cores on cloud services and release a structured world-wide 1-billion-vertex road network graph dataset with high accessibility (opensource and downloadable to the whole world) and usability (open-box graph structure and easy spatial query interface). To demonstrate how this dataset can be utilized easily, we present three illustrative use cases, including traffic prediction, city boundary detection and traffic policy control, and conduct extensive experiments for these three tasks. (1) For the well-investigated traffic prediction tasks, we release a new benchmark with 31 cities (traffic data processed and combined with our released OSM+ road network dataset), to provide much larger spatial coverage and more comprehensive evaluation of compared algorithms than the previously frequently-used datasets. This new benchmark will push the algorithms on their scalability from hundreds of road network intersections to thousands of intersections. (2) While for the more advanced traffic policy control task which requires interaction with the road network, we release a new 6 city datasets with much larger scale than the previous datasets. This brings new challenge for thousand-scale multi-agent coordination. (3) Along with the OSM+ dataset, the release of data converters facilitates the integration of multimodal spatial-temporal data for geospatial foundation model training, thereby expediting the process of uncovering compelling scientific insights. PVLDB Reference Forma"}
{"id": "2512.06238", "categories": ["cs.IT", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.06238", "abs": "https://arxiv.org/abs/2512.06238", "authors": ["Yuping Zheng", "Andrew Lamperski"], "title": "Non-Asymptotic Error Bounds for Causally Conditioned Directed Information Rates of Gaussian Sequences", "comment": "8 pages; under review for IFAC World Congress 2026", "summary": "Directed information and its causally conditioned variations are often used to measure causal influences between random processes. In practice, these quantities must be measured from data. Non-asymptotic error bounds for these estimates are known for sequences over finite alphabets, but less is known for real-valued data. This paper examines the case in which the data are sequences of Gaussian vectors. We provide an explicit formula for causally conditioned directed information rate based on optimal prediction and define an estimator based on this formula. We show that our estimator gives an error of order $O\\left(N^{-1/2}\\log(N)\\right)$ with high probability, where $N$ is the total sample size."}
{"id": "2512.06700", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06700", "abs": "https://arxiv.org/abs/2512.06700", "authors": ["Jiangxia Cao", "Ruochen Yang", "Xiang Chen", "Changxin Lao", "Yueyang Liu", "Yusheng Huang", "Yuanhao Tian", "Xiangyu Wu", "Shuang Yang", "Zhaojie Liu", "Guorui Zhou"], "title": "Foresight Prediction Enhanced Live-Streaming Recommendation", "comment": "Accepted by WSDM 2026", "summary": "Live-streaming, as an emerging media enabling real-time interaction between authors and users, has attracted significant attention. Unlike the stable playback time of traditional TV live or the fixed content of short video, live-streaming, due to the dynamics of content and time, poses higher requirements for the recommendation algorithm of the platform - understanding the ever-changing content in real time and push it to users at the appropriate moment. Through analysis, we find that users have a better experience and express more positive behaviors during highlight moments of the live-streaming. Furthermore, since the model lacks access to future content during recommendation, yet user engagement depends on how well subsequent content aligns with their interests, an intuitive solution is to predict future live-streaming content. Therefore, we perform semantic quantization on live-streaming segments to obtain Semantic ids (Sid), encode the historical Sid sequence to capture the author's characteristics, and model Sid evolution trend to enable foresight prediction of future content. This foresight enhances the ranking model through refined features. Extensive offline and online experiments demonstrate the effectiveness of our method."}
{"id": "2512.06458", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06458", "abs": "https://arxiv.org/abs/2512.06458", "authors": ["Rishiraj Bhattacharyya", "Sourav Chakraborty", "Yash Pote", "Uddalok Sarkar", "Sayantan Sen"], "title": "Instance Dependent Testing of Samplers using Interval Conditioning", "comment": null, "summary": "Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.\n  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers."}
{"id": "2512.07013", "categories": ["cs.GT", "econ.TH", "math.OC", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.07013", "abs": "https://arxiv.org/abs/2512.07013", "authors": ["Stefano Nasini", "Rabia Nessah", "Bertrand Wigniolle"], "title": "Learning Paths to Multi-Sector Equilibrium: Belief Dynamics Under Uncertain Returns to Scale", "comment": null, "summary": "This paper explores the dynamics of learning in a multi-sector general equilibrium model where firms operate under incomplete information about their production returns to scale. Firms iteratively update their beliefs using maximum a-posteriori estimation, derived from observed production outcomes, to refine their knowledge of their returns to scale. The implications of these learning dynamics for market equilibrium and the conditions under which firms can effectively learn their true returns to scale are the key objects of this study. Our results shed light on how idiosyncratic shocks influence the learning process and demonstrate that input decisions encode all pertinent information for belief updates. Additionally, we show that a long-memory (path-dependent) learning which keeps track of all past estimations ends up having a worse performance than a short-memory (path-independent) approach."}
{"id": "2512.06852", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.06852", "abs": "https://arxiv.org/abs/2512.06852", "authors": ["Manideep Reddy Chinthareddy"], "title": "A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases", "comment": "7 pages, 2 figures", "summary": "Many managed key-value and NoSQL databases - such as Amazon DynamoDB, Azure Cosmos DB, and Google Cloud Firestore - enforce strict maximum item sizes (e.g., 400 KB in DynamoDB). This constraint imposes significant architectural challenges for applications requiring low-latency, multi-region access to objects that exceed these limits. The standard industry recommendation is to offload payloads to object storage (e.g., Amazon S3) while retaining a pointer in the database. While cost-efficient, this \"pointer pattern\" introduces network overhead and exposes applications to non-deterministic replication lag between the database and the object store, creating race conditions in active-active architectures.\n  This paper presents a \"chunked-object\" pattern that persists large logical entities as sets of ordered chunks within the database itself. We precisely define the pattern and provide a reference implementation using Amazon DynamoDB Global Tables. The design generalizes to any key-value store with per-item size limits and multi-region replication. We evaluate the approach using telemetry from a production system processing over 200,000 transactions per hour. Results demonstrate that the chunked-object pattern eliminates cross-system replication lag hazards and reduces p99 cross-region time-to-consistency for 1 MB payloads by keeping data and metadata within a single consistency domain."}
{"id": "2512.06312", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06312", "abs": "https://arxiv.org/abs/2512.06312", "authors": ["Lawrence Ong", "Badri N. Vellambi", "Parastoo Sadeghi", "Jörg Kliewer"], "title": "Performance Bounds on Pliable Index Coding Using Absent Receivers", "comment": "Author's final manuscript", "summary": "We characterise bounds on the optimal broadcast rate for a few classes of pliable-index-coding instances. Unlike the majority of currently solved instances, which belong to a special class where all receivers with a certain side-information cardinality are either present or absent, we consider more general instances without this constraint. We devise a novel algorithm that constructs a decoding chain by iteratively adding a message that can be decoded by a receiver whose side information is already in the chain. If the decoding chain cannot proceed due to the absence of a receiver with the required messages, we skip a message by adding it to the chain regardless. We prove that a lower bound on the optimal broadcast rate is a function of the number of skipped messages, across all possible decoding choices of the receivers and any realisation of the algorithm for each decoding choice. While this result is not computationally feasible in isolation, it serves as a basis for deriving explicit lower bounds on the broadcast rate for specific classes of pliable-index-coding instances. These lower bounds depend on the number of absent receivers or the pattern of their side-information sets. Specifically, we explicitly characterise the optimal broadcast rate for instances with up to and including four absent receivers with any side-information pattern, as well as instances where the side-information sets are nested in particular ways."}
{"id": "2512.06879", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06879", "abs": "https://arxiv.org/abs/2512.06879", "authors": ["Li Ju", "Jun Zhao", "Mingxu Chai", "Ziyu Shen", "Xiangyang Wang", "Yage Geng", "Chunchun Ma", "Hao Peng", "Guangbin Li", "Tao Li", "Chengyong Liao", "Fu Wang", "Xiaolong Wang", "Junshen Chen", "Rui Gong", "Shijia Liang", "Feiyan Li", "Ming Zhang", "Kexin Tan", "Jujie Ye", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Yuankai Ying", "Yang Shi", "Yue Zhang", "Qi Zhang"], "title": "WisPaper: Your AI Scholar Search Engine", "comment": "17 pages, 2 figures", "summary": "Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \\textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \\textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \\textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \\textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \\textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry."}
{"id": "2512.06611", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.06611", "abs": "https://arxiv.org/abs/2512.06611", "authors": ["Rishi Gujjar", "Kevin Hua", "Robert Kleinberg", "Frederick V. Qiu"], "title": "The $k$-Fold Matroid Secretary Problem", "comment": "11 pages, 1 figure, to appear in SOSA 2026", "summary": "In the matroid secretary problem, elements $N := [n]$ of a matroid $\\mathcal{M} \\subseteq 2^N$ arrive in random order. When an element arrives, its weight is revealed and a choice must be made to accept or reject the element, subject to the constraint that the accepted set $S \\in \\mathcal{M}$. Kleinberg'05 gives a $(1-O(1/\\sqrt{k}))$-competitive algorithm when $\\mathcal{M}$ is a $k$-uniform matroid. We generalize their result, giving a $(1-O(\\sqrt{\\log(n)/k}))$-competitive algorithm when $\\mathcal{M}$ is a $k$-fold matroid union."}
{"id": "2512.07308", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.07308", "abs": "https://arxiv.org/abs/2512.07308", "authors": ["Elisheva S Shamash", "Zhong Fan"], "title": "Zero Carbon V2X Tariffs for Non-Domestic Customers", "comment": "25 pages, 1 figure", "summary": "With the aim of meeting the worlds net-zero objectives, electricity trading through contractual agreements is becoming increasingly relevant in global and local energy markets. We develop contracts enabling efficient energy trading using Vehicle to Everything technology which can be applied to regulate energy markets and reduce costs and carbon emissions by using electric vehicles with bidirectional batteries to store energy during offpeak hours for export during peak hours. We introduce a contract based on the VCG mechanism which enables fleets of electric vehicles to export electricity to the grid efficiently throughout the day, where each electric vehicle has its energy consumption and exporting schedules and costs."}
{"id": "2512.06988", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06988", "abs": "https://arxiv.org/abs/2512.06988", "authors": ["Skylar Homan", "Anoop Krishnadas", "Kira Adaricheva"], "title": "Space efficient implementation of hypergraph dualization in the D-basis algorithm", "comment": "21 pages, 3 figures, 10 tables. Submitted to Discrete Applied Mathematics. Results were presented at the AMS 2025 Fall Western Sectional Meeting at the University of Denver", "summary": "We present a new implementation of the $D$-basis algorithm called the Small Space which considerably reduces the algorithm's memory usage for data analysis applications. The previous implementation delivers the complete set of implications that hold on the set of attributes of an input binary table. In the new version, the only output is the frequencies of attributes that appear in the antecedents of implications from the $D$-basis, with a fixed consequent attribute. Such frequencies, rather than the implications themselves, became the primary focus in analysis of datasets where the $D$-basis has been applied over the last decade. The $D$-basis employs a hypergraph dualization algorithm, and a dualization implementation known as Reverse Search allows for the gradual computation of frequencies without the need for storing all discovered implications. We demonstrate the effectiveness of the Small Space implementation by comparing the runtimes and maximum memory usage of this new version with the current implementation."}
{"id": "2512.06452", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06452", "abs": "https://arxiv.org/abs/2512.06452", "authors": ["Yuxuan Song", "Haiquan Lu", "Chiya Zhang", "Beixiong Zheng", "Yong Zeng"], "title": "Trajectory Optimization for Cellular-Connected UAV in Complex Environment with Partial CKM", "comment": null, "summary": "Cellular-connected unmanned aerial vehicles (UAVs) are expected to play an increasingly important role in future wireless networks. To facilitate the reliable navigation for cellular-connected UAVs, channel knowledge map (CKM) is considered a promising approach capable of tackling the non-negligible co-channel interference resulting from the high line-of-sight (LoS) probability of air-ground (AG) channels. Nevertheless, due to measurement constraints and the aging of information, CKM is usually incomplete and needs to be regularly updated to capture the dynamic nature of complex environments. In this paper, we propose a novel trajectory design strategy in which UAV navigation and CKM completion are incorporated into a common framework, enabling mutual benefits for both tasks. Specifically, a cellular-connected UAV deployed in an urban environment measures the radio information during its flight and completes the CKM with Kriging interpolation. Based on the method of grid discretization and spherical approximation, a mixed-integer multi-objective optimization problem is formulated. The problem falls into the category of combinatorial mathematics and is essentially equivalent to determining an optimum sequence of grid points to traverse. Through proper mathematical manipulation, the problem is reformulated as variants of two classic models in graph theory, namely the shortest-path problem (SPP) and the traveling salesman problem (TSP). Two navigation strategies based on the two different models are proposed and thoroughly compared based on numerical results to provide implementable methods for engineering practice and reveal the trade-offs between UAV navigation and CKM completion. Simulation results reveal that the proposed navigation strategies can quickly expand the Pareto boundary of the problem and approach the performance of fully-known CKM."}
{"id": "2512.06883", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06883", "abs": "https://arxiv.org/abs/2512.06883", "authors": ["Zhongtao Rao", "Peilin Zhou", "Dading Chong", "Zhiwei Chen", "Shoujin Wang", "Nan Tang"], "title": "Structural and Disentangled Adaptation of Large Vision Language Models for Multimodal Recommendation", "comment": null, "summary": "Multimodal recommendation enhances accuracy by leveraging visual and textual signals, and its success largely depends on learning high-quality cross-modal representations. Recent advances in Large Vision-Language Models (LVLMs) offer unified multimodal representation learning, making them a promising backbone. However, applying LVLMs to recommendation remains challenging due to (i) representation misalignment, where domain gaps between item data and general pre-training lead to unaligned embedding spaces, and (ii) gradient conflicts during fine-tuning, where shared adapters cause interference and a lack of discriminative power. To address this, we propose SDA, a lightweight framework for Structural and Disentangled Adaptation, which integrates two components: Cross-Modal Structural Alignment (CMSA) and Modality-Disentangled Adaptation. CMSA aligns embeddings using intra-modal structures as a soft teacher, while MoDA mitigates gradient conflicts via expertized, gated low-rank paths to disentangle gradient flows. Experiments on three public Amazon datasets show SDA integrates seamlessly with existing multimodal and sequential recommenders, yielding average gains of 6.15% in Hit@10 and 8.64% in NDCG@10. It also achieves up to 12.83% and 18.70% gains on long-tail items with minimal inference overhead. Our code and full experimental results are available at https://github.com/RaoZhongtao/SDA."}
{"id": "2512.06997", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.06997", "abs": "https://arxiv.org/abs/2512.06997", "authors": ["Yiding Feng", "Rad Niazadeh", "Amin Saberi"], "title": "Near-Optimal Bayesian Online Assortment of Reusable Resources", "comment": "Journal version: Operations Research, 2024; Preliminary conference version: ACM EC 2022", "summary": "Motivated by the applications of rental services in e-commerce, we consider revenue maximization in online assortment of reusable resources for a stream of arriving consumers with different types. We design competitive online algorithms with respect to the optimum online policy in the Bayesian setting, in which types are drawn independently from known heterogeneous distributions over time. In the regime where the minimum of initial inventories $c_0$ is large, our main result is a near-optimal $1-\\min\\left(\\frac{1}{2},\\sqrt{\\log(c_0)/c_0}\\right)$ competitive algorithm for the general case of reusable resources. Our algorithm relies on an expected LP benchmark for the problem, solves this LP, and simulates the solution through an independent randomized rounding. The main challenge is obtaining point-wise inventory feasibility in a computationally efficient fashion from these simulation-based algorithms. To this end, we use several technical ingredients to design $\\textit{discarding policies}$ -- one for each resource. These policies handle the trade-off between the inventory feasibility under reusability and the revenue loss of each of the resources. However, discarding a unit of a resource changes the future consumption of other resources. To handle this new challenge, we also introduce $\\textit{post-processing}$ assortment procedures that help with designing and analyzing our discarding policies as they run in parallel, which might be of independent interest. As a side result, by leveraging techniques from the literature on prophet inequality, we further show an improved near-optimal $1-1/\\sqrt{c_0+3}$ competitive algorithm for the special case of non-reusable resources. We finally evaluate the performance of our algorithms using the numerical simulations on the synthetic data."}
{"id": "2512.06478", "categories": ["cs.IT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.06478", "abs": "https://arxiv.org/abs/2512.06478", "authors": ["Madhu Sudan"], "title": "Algebra in Algorithmic Coding Theory", "comment": "20 pages", "summary": "We survey the notion and history of error-correcting codes and the algorithms needed to make them effective in information transmission. We then give some basic as well as more modern constructions of, and algorithms for, error-correcting codes that depend on relatively simple elements of applied algebra. While the role of algebra in the constructions of codes has been widely acknowledged in texts and other writings, the role in the design of algorithms is often less widely understood, and this survey hopes to reduce this difference to some extent."}
{"id": "2512.07000", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07000", "abs": "https://arxiv.org/abs/2512.07000", "authors": ["Abderaouf Bahi", "Ibtissem Gasmi"], "title": "Benchmarking Deep Neural Networks for Modern Recommendation Systems", "comment": null, "summary": "This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms."}
{"id": "2512.07120", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07120", "abs": "https://arxiv.org/abs/2512.07120", "authors": ["J. Allagan", "G. Morgan", "S. Langley", "R. Lopez-Bonilla", "V. Deriglazov"], "title": "Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications", "comment": "18 pages", "summary": "We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography."}
{"id": "2512.06606", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06606", "abs": "https://arxiv.org/abs/2512.06606", "authors": ["Haolun", "Ni", "Lev Tauz", "Ryan Gabrys", "Lara Dolecek"], "title": "Improved Interactive Protocol for Synchronizing From Deletions", "comment": "14 pages, 3 figures. Extended version of a paper presented at ISIT 2025. A journal version is in preparation", "summary": "Data synchronization is a fundamental problem with applications in diverse fields such as cloud storage, genomics, and distributed systems. This paper addresses the challenge of synchronizing two files, one of which is a subsequence of the other and related through a constant rate of deletions, using an improved communication protocol. Building upon prior work, we integrate advanced multi-deletion correction codes into an existing baseline protocol, which previously relied on single-deletion correction. Our proposed protocol reduces communication cost by leveraging more general partitioning techniques as well as multi-deletion error correction. We derive a generalized upper bound on the expected number of transmitted bits, applicable to a broad class of deletion correction codes. Experimental results demonstrate that our approach outperforms the baseline in communication cost. These findings establish the efficacy of the improved protocol in achieving low-redundancy synchronization in scenarios where deletion errors occur."}
{"id": "2512.07216", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07216", "abs": "https://arxiv.org/abs/2512.07216", "authors": ["Bin Wu", "Feifan Yang", "Zhangming Chan", "Yu-Ran Gu", "Jiawei Feng", "Chao Yi", "Xiang-Rong Sheng", "Han Zhu", "Jian Xu", "Mang Ye", "Bo Zheng"], "title": "MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling", "comment": null, "summary": "Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io."}
{"id": "2512.07577", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.07577", "abs": "https://arxiv.org/abs/2512.07577", "authors": ["Artur Czumaj", "Christian Sohler"], "title": "Property Testing of Computational Networks", "comment": null, "summary": "In this paper we initiate the study of \\emph{property testing of weighted computational networks viewed as computational devices}. Our goal is to design property testing algorithms that for a given computational network with oracle access to the weights of the network, accept (with probability at least $\\frac23$) any network that computes a certain function (or a function with a certain property) and reject (with probability at least $\\frac23$) any network that is \\emph{far} from computing the function (or any function with the given property). We parameterize the notion of being far and want to reject networks that are \\emph{$(ε,δ)$-far}, which means that one needs to change an $ε$-fraction of the description of the network to obtain a network that computes a function that differs in at most a $δ$-fraction of inputs from the desired function (or any function with a given property).\n  To exemplify our framework, we present a case study involving simple neural Boolean networks with ReLU activation function. As a highlight, we demonstrate that for such networks, any near constant function is testable in query complexity independent of the network's size. We also show that a similar result cannot be achieved in a natural generalization of the distribution-free model to our setting, and also in a related vanilla testing model."}
{"id": "2512.06998", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06998", "abs": "https://arxiv.org/abs/2512.06998", "authors": ["Zinat Behdad", "Ozan Alp Topal", "Cicek Cavdar"], "title": "Cell-free ISAC for Drone Detection Considering Coverage and Age of Sensing", "comment": "6 pages, 5 figures", "summary": "The growing presence of unauthorized drones poses significant threats to public safety, underscoring the need for aerial surveillance solutions. This work proposes a cell-free integrated sensing and communication (ISAC) framework enabling drone detection within the existing communication network infrastructure, while maintaining communication services. The system exploits the spatial diversity and coordination of distributed access points (APs) in a cell-free massive MIMO architecture to detect aerial passive targets. To evaluate sensing performance, we introduce two key metrics: age of sensing (AoS), capturing the freshness of sensing information, and sensing coverage. The proposed AoS metric includes not only the transmission delays as in the existing models, but also the processing for sensing and networking delay, which are critical in dynamic environments like drone detection. We introduce an ambiguity parameter quantifying the similarity between the target-to-receiver channels for two hotspots and develop a novel network configuration strategy, including hotspot grouping, AP clustering, and sensing pilot assignment, leveraging simultaneous multi-point sensing to minimize AoS. Our results show that the best trade-off between AoS and sensing coverage is achieved when the number of hotspots sharing the same time/frequency resource matches the number of sensing pilots, indicating ambiguity as the primary factor limiting the sensing performance."}
{"id": "2512.07384", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07384", "abs": "https://arxiv.org/abs/2512.07384", "authors": ["Daniele Malitesta", "Claudio Pomo", "Vito Walter Anelli", "Alberto Carlo Maria Mancino", "Alejandro Bellogín", "Tommaso Di Noia"], "title": "On the Impact of Graph Neural Networks in Recommender Systems: A Topological Perspective", "comment": null, "summary": "In recommender systems, user-item interactions can be modeled as a bipartite graph, where user and item nodes are connected by undirected edges. This graph-based view has motivated the rapid adoption of graph neural networks (GNNs), which often outperform collaborative filtering (CF) methods such as latent factor models, deep neural networks, and generative strategies. Yet, despite their empirical success, the reasons why GNNs offer systematic advantages over other CF approaches remain only partially understood. This monograph advances a topology-centered perspective on GNN-based recommendation. We argue that a comprehensive understanding of these models' performance should consider the structural properties of user-item graphs and their interaction with GNN architectural design. To support this view, we introduce a formal taxonomy that distills common modeling patterns across eleven representative GNN-based recommendation approaches and consolidates them into a unified conceptual pipeline. We further formalize thirteen classical and topological characteristics of recommendation datasets and reinterpret them through the lens of graph machine learning. Using these definitions, we analyze the considered GNN-based recommender architectures to assess how and to what extent they encode such properties. Building on this analysis, we derive an explanatory framework that links measurable dataset characteristics to model behavior and performance. Taken together, this monograph re-frames GNN-based recommendation through its topological underpinnings and outlines open theoretical, data-centric, and evaluation challenges for the next generation of topology-aware recommender systems."}
{"id": "2512.07618", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2512.07618", "abs": "https://arxiv.org/abs/2512.07618", "authors": ["Jiratchaphat Nanta", "Vorapong Suppakitpaisarn", "Piyashat Sripratak"], "title": "Approximation Algorithms for the $b$-Matching and List-Restricted Variants of MaxQAP", "comment": null, "summary": "We study approximation algorithms for two natural generalizations of the Maximum Quadratic Assignment Problem (MaxQAP). In the Maximum List-Restricted Quadratic Assignment Problem, each node in one partite set may only be matched to nodes from a prescribed list. For instances on $n$ nodes where every list has size at least $n - O(\\sqrt{n})$, we design a randomized $O(\\sqrt{n})$-approximation algorithm based on the linear-programming relaxation and randomized rounding framework of Makarychev, Manokaran, and Sviridenko. In the Maximum Quadratic $b$-Matching Assignment Problem, we seek a $b$-matching that maximizes the MaxQAP objective. We refine the standard MaxQAP relaxation and combine randomized rounding over $b$ independent iterations with a polynomial-time algorithm for maximum-weight $b$-matching problem to obtain an $O(\\sqrt{bn})$-approximation. When $b$ is constant and all lists have size $n - O(\\sqrt{n})$, our guarantees asymptotically match the best known approximation factor for MaxQAP, yielding the first approximation algorithms for these two variants."}
{"id": "2512.07243", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.07243", "abs": "https://arxiv.org/abs/2512.07243", "authors": ["Anamika Singh", "Abhay Kumar Singh"], "title": "Function-Correcting Codes for Insertion-Deletion Channel", "comment": null, "summary": "In coding theory, handling errors that occur when symbols are inserted or deleted from a transmitted message is a long-standing challenge. Optimising redundancy for insertion and deletion channels remains a key open problem with significant importance for applications in DNA data storage and document exchange. Recently, a coding framework known as function-correcting codes has been proposed to address the challenge of minimising redundancy while preserving specific functions of the message. This framework has gained attention due to its potential applications in machine learning systems and long-term archival data storage. Motivated by the problem of redundancy optimisation for insertion and deletion channels, we propose a new framework called function-correcting codes for insdel channels. In this paper, we introduce the notions of function-correcting insertion codes, function-correcting deletion codes, and function-correcting insdel codes, and we show that these three formulations are equivalent. We then define insdel distance matrices and irregular insdel-distance codes, and derive lower and upper bounds on the optimal redundancy achievable by function-correcting codes for insdel channels. In addition, we establish Gilbert-Varshamov and Plotkin-like bounds on the length of irregular insdel-distance codes. Using the relation between optimal redundancy and the length of such codes, we obtain a simplified lower bound on optimal redundancy. Finally, we derive bounds on the optimal redundancy of function-correcting insdel codes for several classes of functions, including locally bounded functions, VT syndrome functions, the number-of-runs function, and the maximum-run-length function."}
{"id": "2512.07424", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07424", "abs": "https://arxiv.org/abs/2512.07424", "authors": ["Jiangxia Cao", "Shuo Yang", "Zijun Wang", "Qinghai Tan"], "title": "OnePiece: The Great Route to Generative Recommendation -- A Case Study from Tencent Algorithm Competition", "comment": "Work in progress", "summary": "In past years, the OpenAI's Scaling-Laws shows the amazing intelligence with the next-token prediction paradigm in neural language modeling, which pointing out a free-lunch way to enhance the model performance by scaling the model parameters. In RecSys, the retrieval stage is also follows a 'next-token prediction' paradigm, to recall the hunderds of items from the global item set, thus the generative recommendation usually refers specifically to the retrieval stage (without Tree-based methods). This raises a philosophical question: without a ground-truth next item, does the generative recommendation also holds a potential scaling law? In retrospect, the generative recommendation has two different technique paradigms: (1) ANN-based framework, utilizing the compressed user embedding to retrieve nearest other items in embedding space, e.g, Kuaiformer. (2) Auto-regressive-based framework, employing the beam search to decode the item from whole space, e.g, OneRec. In this paper, we devise a unified encoder-decoder framework to validate their scaling-laws at same time. Our empirical finding is that both of their losses strictly adhere to power-law Scaling Laws ($R^2$>0.9) within our unified architecture."}
{"id": "2512.07256", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.07256", "abs": "https://arxiv.org/abs/2512.07256", "authors": ["Yang Li", "Shitao Li", "Gaojun Luo", "San Ling"], "title": "Improved bounds and optimal constructions of pure quantum locally recoverable codes", "comment": "17 pages, 3 figures, another related work is about to be released", "summary": "By incorporating the concept of locality into quantum information theory, quantum locally recoverable codes (qLRCs) have been proposed, motivated by their potential applications in large-scale quantum data storage and their relevance to quantum LDPC codes. Despite the progress in optimal quantum error-correcting codes (QECCs), optimal constructions of qLRCs remain largely unexplored, partly due to the fact that the existing bounds for qLRCs are not sufficiently tight. In this paper, we focus on pure qLRCs derived from the Hermitian construction. We provide several new bounds for pure qLRCs and demonstrate that they are tighter than previously known bounds. Moreover, we show that a variety of classical QECCs, including quantum Hamming codes, quantum GRM codes, and quantum Solomon-Stiffler codes, give rise to pure qLRCs with explicit parameters. Based on these constructions, we further identify many infinite families of optimal qLRCs with respect to different bounds, achieving code lengths much larger than those of known optimal qLRCs."}
{"id": "2512.07452", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07452", "abs": "https://arxiv.org/abs/2512.07452", "authors": ["Clarisse Bardiot", "Pierre-Carl Langlais", "Bernard Jacquemin", "Jacob Hart", "Antonios Lagarias", "Nicolas Foucault", "Aurélie Lemaître-Legargeant", "Jeanne Fras"], "title": "From Show Programmes to Data: Designing a Workflow to Make Performing Arts Ephemera Accessible Through Language Models", "comment": "19 pages, 8 figures, 5 tables, 17 references", "summary": "Many heritage institutions hold extensive collections of theatre programmes, which remain largely underused due to their complex layouts and lack of structured metadata. In this paper, we present a workflow for transforming such documents into structured data using a combination of multimodal large language models (LLMs), an ontology-based reasoning model, and a custom extension of the Linked Art framework. We show how vision-language models can accurately parse and transcribe born-digital and digitised programmes, achieving over 98% of correct extraction. To overcome the challenges of semantic annotation, we train a reasoning model (POntAvignon) using reinforcement learning with both formal and semantic rewards. This approach enables automated RDF triple generation and supports alignment with existing knowledge graphs. Through a case study based on the Festival d'Avignon corpus, we demonstrate the potential for large-scale, ontology-driven analysis of performing arts data. Our results open new possibilities for interoperable, explainable, and sustainable computational theatre historiography."}
{"id": "2512.07309", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07309", "abs": "https://arxiv.org/abs/2512.07309", "authors": ["Guosheng Wang", "Shen Wang", "Lei Yang"], "title": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals", "comment": null, "summary": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning."}
{"id": "2512.07650", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07650", "abs": "https://arxiv.org/abs/2512.07650", "authors": ["Fuyuan Lyu", "Zhentai Chen", "Jingyan Jiang", "Lingjie Li", "Xing Tang", "Xiuqiang He", "Xue Liu"], "title": "Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation", "comment": null, "summary": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available."}
{"id": "2512.07343", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.07343", "abs": "https://arxiv.org/abs/2512.07343", "authors": ["Leijo Jose", "Lavanya G.", "Anuradha Sharma"], "title": "Linear codes over $\\frac{\\mathbb{F}_q[u]}{\\langle u^2 \\rangle}$ with mixed-alphabet defining sets and their Gray images: Constructions of projective few-weight, distance-optimal and minimal codes", "comment": null, "summary": "Let $\\mathcal{R}=\\frac{\\mathbb{F}_q[u]}{\\langle u^2 \\rangle}\\times \\mathbb{F}_q$ be the mixed alphabet ring. In this paper, we construct four infinite families of linear codes over the ring $\\frac{\\mathbb{F}_q[u]}{\\langle u^2 \\rangle}$ whose defining sets are certain nonempty subsets of $\\mathcal{R}^m$ associated with three simplicial complexes of $\\mathbb{F}_q^m,$ each possessing a single maximal element. We explicitly determine the parameters and Lee weight distributions of these codes. We also study their Gray images and obtain three infinite families of few weight, near Griesmer, distance optimal and minimal codes over $\\mathbb{F}_q$ with new parameters. We also provide two constructions of infinite families of projective few weight codes over $\\mathbb{F}_q$ with new parameters, and observe that these codes are self orthogonal for $q=2$ or $3.$ Additionally, we obtain two infinite families of binary distance optimal projective codes and an infinite family of dimension optimal projective codes over $\\mathbb{F}_q$ with new parameters. Apart from this, we construct an infinite family of quaternary projective $3$-weight codes whose non zero Hamming weights sum to $\\frac{9}{4}$ times the code length, which give rise to strongly walk regular graphs. As an application of our newly constructed minimal codes over $\\mathbb{F}_q$, we examine the minimal access structures of Massey's secret sharing schemes based on their duals and determine the number of dictatorial participants in these schemes. Finally, we investigate the locality properties of our newly constructed projective codes."}
{"id": "2512.06988", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06988", "abs": "https://arxiv.org/abs/2512.06988", "authors": ["Skylar Homan", "Anoop Krishnadas", "Kira Adaricheva"], "title": "Space efficient implementation of hypergraph dualization in the D-basis algorithm", "comment": "21 pages, 3 figures, 10 tables. Submitted to Discrete Applied Mathematics. Results were presented at the AMS 2025 Fall Western Sectional Meeting at the University of Denver", "summary": "We present a new implementation of the $D$-basis algorithm called the Small Space which considerably reduces the algorithm's memory usage for data analysis applications. The previous implementation delivers the complete set of implications that hold on the set of attributes of an input binary table. In the new version, the only output is the frequencies of attributes that appear in the antecedents of implications from the $D$-basis, with a fixed consequent attribute. Such frequencies, rather than the implications themselves, became the primary focus in analysis of datasets where the $D$-basis has been applied over the last decade. The $D$-basis employs a hypergraph dualization algorithm, and a dualization implementation known as Reverse Search allows for the gradual computation of frequencies without the need for storing all discovered implications. We demonstrate the effectiveness of the Small Space implementation by comparing the runtimes and maximum memory usage of this new version with the current implementation."}
{"id": "2512.07354", "categories": ["cs.IT", "math.QA", "math.RA"], "pdf": "https://arxiv.org/pdf/2512.07354", "abs": "https://arxiv.org/abs/2512.07354", "authors": ["Miguel Sales-Cabrera", "Xaro Soler-Escrivà", "Víctor Sotomayor"], "title": "Dualities of dihedral and generalised quaternion codes and applications to quantum codes", "comment": null, "summary": "Let $\\mathbb{F}_q$ be a finite field of $q$ elements, for some prime power $q$, and let $G$ be a finite group. A (left) group code, or simply a $G$-code, is a (left) ideal of the group algebra $\\mathbb{F}_q[G]$. In this paper, we provide a complete algebraic description for the hermitian dual code of any $D_n$-code over $\\mathbb{F}_{q^2}$, where $D_n$ is a dihedral group of order $2n$ with $\\gcd(q,n)=1$, through a suitable Wedderburn-Artin's decomposition of the group algebra $\\mathbb{F}_{q^2}[D_n]$, and we determine all distinct hermitian self-orthogonal $D_n$-codes over $\\mathbb{F}_{q^2}$. We also present a thorough representation of the euclidean dual code of any $Q_n$-code over $\\mathbb{F}_q$, where $Q_n$ is a generalised quaternion group of order $4n$ with $\\gcd(q,4n)=1$, via the Wedderburn-Artin's decomposition of the group algebra $\\mathbb{F}_q[Q_n]$. In particular, since the semisimple group algebras $\\mathbb{F}_{q^2}[Q_n]$ and $\\mathbb{F}_{q^2}[D_{2n}]$ are isomorphic, then the hermitian dual code of any $Q_n$-code has also been fully described. As application of the hermitian dualities computed, we give a systematic construction, via the structure of the group algebra, to obtain quantum error-correcting codes, and in fact we rebuild some already known optimal quantum codes with this methodical approach."}
{"id": "2512.07405", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07405", "abs": "https://arxiv.org/abs/2512.07405", "authors": ["Amnon Balanov", "Tamir Bendory", "Dan Edidin"], "title": "Orbit recovery under the rigid motions group", "comment": null, "summary": "We study the orbit recovery problem under the rigid-motion group SE(n), where the objective is to reconstruct an unknown signal from multiple noisy observations subjected to unknown rotations and translations. This problem is fundamental in signal processing, computer vision, and structural biology.\n  Our main theoretical contribution is bounding the sample complexity of this problem. We show that if the d-th order moment under the rotation group SO(n) uniquely determines the signal orbit, then orbit recovery under SE(n) is achievable with $N\\gtrsim σ^{2d+4}$ samples as the noise variance $σ^2 \\to \\infty$. The key technical insight is that the d-th order SO(n) moments can be explicitly recovered from (d+2)-order SE(n) autocorrelations, enabling us to transfer known results from the rotation-only setting to the rigid-motion case. We further harness this result to derive a matching bound to the sample complexity of the multi-target detection model that serves as an abstract framework for electron-microscopy-based technologies in structural biology, such as single-particle cryo-electron microscopy (cryo-EM) and cryo-electron tomography (cryo-ET).\n  Beyond theory, we present a provable computational pipeline for rigid-motion orbit recovery in three dimensions. Starting from rigid-motion autocorrelations, we extract the SO(3) moments and demonstrate successful reconstruction of a 3-D macromolecular structure. Importantly, this algorithmic approach is valid at any noise level, suggesting that even very small macromolecules, long believed to be inaccessible using structural biology electron-microscopy-based technologies, may, in principle, be reconstructed given sufficient data."}
{"id": "2512.07662", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.07662", "abs": "https://arxiv.org/abs/2512.07662", "authors": ["Ozan Aygün", "Ezgi Ozyilkan", "Elza Erkip"], "title": "Neural Compress-and-Forward for the Primitive Diamond Relay Channel", "comment": "Accepted to 2025 59th Asilomar Conference on Signals, Systems, and Computers", "summary": "The diamond relay channel, where a source communicates with a destination via two parallel relays, is one of the canonical models for cooperative communications. We focus on the primitive variant, where each relay observes a noisy version of the source signal and forwards a compressed description over an orthogonal, noiseless, finite-rate link to the destination. Compress-and-forward (CF) is particularly effective in this setting, especially under oblivious relaying where relays lack access to the source codebook. While neural CF methods have been studied in single-relay channels, extending them to the two-relay case is non-trivial, as it requires fully distributed compression without any inter-relay coordination. We demonstrate that learning-based quantizers at the relays can harness input correlations by operating remote, yet in a collaborative fashion, enabling effective distributed compression in line with Berger-Tung-style coding. Each relay separately compresses its observation using a one-shot learned quantizer, and the destination jointly decodes the source message. Simulation results show that the proposed scheme, trained end-to-end with finite-order modulation, operates close to the known theoretical bounds. These results demonstrate that neural CF can scale to multi-relay systems while maintaining both performance and interpretability."}
{"id": "2512.07704", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.07704", "abs": "https://arxiv.org/abs/2512.07704", "authors": ["Tengfei Qi", "Yifei Yang", "Xiong Deng", "Zhinan Sun", "Ziqiang Gao", "Xihua Zou", "Wei Pan", "Lianshan Yan"], "title": "Enhancing Channel Estimation for OTFS systems using Sparse Bayesian Learning with Adaptive Threshold", "comment": null, "summary": "Orthogonal time frequency space (OTFS) modulation is a two-dimensional modulation scheme designed in the delay-Doppler (DD) domain, exhibiting superior performance over orthogonal frequency division multiplexing (OFDM) modulation in environments with high Doppler frequency shifts. We investigated the channel estimation in the DD domain of OTFS systems, modeling it as a sparse signal recovery problem. Subsequently, within the existing sparse Bayesian learning framework, we proposed an adaptive Bayesian threshold-based active denoising mechanism. Combined with inverse-free sparse Bayesian learning, this effectively addresses the pseudo-peak issue in low signal-to-noise ratio (SNR) scenarios while maintaining low complexity. The simulation results demonstrate that this algorithm outperforms existing channel estimation algorithms in terms of anti-noise performance and complexity."}
