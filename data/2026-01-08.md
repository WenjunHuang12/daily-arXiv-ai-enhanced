<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.DS](#cs.DS) [Total: 5]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [LCPs of Subspace Codes](https://arxiv.org/abs/2601.03489)
*Sanjit Bhowmick*

Main category: cs.IT

TL;DR: 本文引入了子空间码的线性互补对(LCP)概念，给出了LCP子空间码的表征，基于补函数提供了存在性条件，提出了多种构造方法，并应用于插入错误校正。


<details>
  <summary>Details</summary>
Motivation: 传统线性码的线性互补对(LCP)概念在编码理论中已有研究，但子空间码的LCP概念尚未被探索。子空间码在随机网络编码等领域有重要应用，研究其LCP结构有助于扩展编码理论框架并开发新的应用。

Method: 1. 首先给出子空间码形成LCP的表征条件；2. 基于子空间码上的补函数提出LCP存在的充分条件；3. 使用多种技术构造子空间码的LCP，包括代数方法和组合方法；4. 将LCP子空间码应用于插入错误校正问题。

Result: 建立了子空间码LCP的理论框架，获得了存在性条件和构造方法，证明了这些构造在插入错误校正中的有效性，扩展了传统线性码LCP理论到子空间码领域。

Conclusion: 成功将线性互补对概念推广到子空间码，建立了完整的理论体系，提供了实用的构造方法，并展示了在插入错误校正中的应用价值，为子空间码理论的发展开辟了新方向。

Abstract: A subspace code is a nonempty collection of subspaces of the vector space $\mathbb{F}_q^{n}$. A pair of linear codes is called a linear complementary pair (in short LCP) of codes if their intersection is trivial and the sum of their dimensions equals the dimension of the ambient space. Equivalently, the two codes form an LCP if the direct sum of these two codes is equal to the entire space. In this paper, we introduce the concept of LCPs of subspace codes. We first provide a characterization of subspace codes that form an LCP. Furthermore, we present a sufficient condition for the existence of an LCP of subspace codes based on a complement function on a subspace code. In addition, we give several constructions of LCPs for subspace codes using various techniques and provide an application to insertion error correction.

</details>


### [2] [Hermitian LCD $2$-Quasi Abelian Codes over Finite Chain Rings](https://arxiv.org/abs/2601.03492)
*Sanjit Bhowmick,Kuntal Deka*

Main category: cs.IT

TL;DR: 该论文研究了一类Hermitian LCD 2-准阿贝尔码，证明了它们在有限域和有限链环上都是渐近好的。


<details>
  <summary>Details</summary>
Motivation: 研究Hermitian LCD 2-准阿贝尔码的构造和性质，特别是当相对最小权值较小时的情况，以扩展编码理论中渐近好码的类别。

Method: 引入了一类Hermitian LCD 2-准阿贝尔码，对相对最小权值较小的这类码进行了全面枚举，并将分析扩展到有限链环上，刻画了该环境下的2-准阿贝尔码。

Result: 证明了这类码在有限域上是渐近好的，并且在有限链环上也存在渐近好的Hermitian LCD 2-准阿贝尔码。

Conclusion: Hermitian LCD 2-准阿贝尔码在有限域和有限链环上都是渐近好的，为编码理论提供了新的渐近好码构造方法。

Abstract: This paper introduces a class of Hermitian LCD $2$-quasi-abelian codes over finite fields and presents a comprehensive enumeration of these codes in which relative minimum weights are small. We show that such codes are asymptotically good over finite fields. Furthermore, we extend our analysis to finite chain rings by characterizing $2$-quasi-abelian codes in this setting and proving the existence of asymptotically good Hermitian LCD $2$-quasi-abelian codes over finite chain rings as well.

</details>


### [3] [Low-Complexity Planar Beyond-Diagonal RIS Architecture Design Using Graph Theory](https://arxiv.org/abs/2601.03831)
*Matteo Nerini,Zheyu Wu,Shanpu Shen,Bruno Clerckx*

Main category: cs.IT

TL;DR: 使用图论分析可在双层PCB上实现的平面连接RIS架构，识别具有最大自由度的最优配置


<details>
  <summary>Details</summary>
Motivation: 传统BD-RIS架构需要多层PCB设计，增加了制造难度。需要找到在双层PCB上可实现的RIS架构，同时保持足够的性能自由度。

Method: 使用图论方法分析BD-RIS架构，特别关注可在双层PCB上实现的平面连接RIS。通过图论工具识别具有最多自由度的平面连接架构。

Result: 确定了可在双层PCB上实现的平面连接RIS架构，并识别出其中具有最大自由度的最优配置，这些配置在实际约束下预期能获得最佳性能。

Conclusion: 图论方法有效解决了BD-RIS在双层PCB上的实现问题，找到了平衡电路复杂度和性能的最优平面连接架构，为实际应用提供了可行方案。

Abstract: Reconfigurable intelligent surfaces (RISs) enable programmable control of the wireless propagation environment and are key enablers for future networks. Beyond-diagonal RIS (BD-RIS) architectures enhance conventional RIS by interconnecting elements through tunable impedance components, offering greater flexibility with higher circuit complexity. However, excessive interconnections between BD-RIS elements require multi-layer printed circuit board (PCB) designs, increasing fabrication difficulty. In this letter, we use graph theory to characterize the BD-RIS architectures that can be realized on double-layer PCBs, denoted as planar-connected RISs. Among the possible planar-connected RISs, we identify the ones with the most degrees of freedom, expected to achieve the best performance under practical constraints.

</details>


### [4] [Unique Decoding of Hyperderivative Reed-Solomon Codes](https://arxiv.org/abs/2601.03982)
*Haojie Gu,Jun Zhang*

Main category: cs.IT

TL;DR: 提出了一种用于NRT度量下超导数Reed-Solomon码的Welch-Berlekamp唯一解码算法


<details>
  <summary>Details</summary>
Motivation: 纠错码用于解决噪声信道上的可靠信息传输问题，解码效率是编码理论和实践中的基本问题。超导数Reed-Solomon码在NRT度量下的解码问题需要高效算法。

Method: 提出了NRT度量下超导数Reed-Solomon码的Welch-Berlekamp算法，用于唯一解码。

Result: 开发了一种针对NRT HRS码的高效解码算法，能够从包含错误的接收字中恢复传输的码字。

Conclusion: 该算法为NRT度量下的超导数Reed-Solomon码提供了一种有效的解码解决方案，有助于提高编码系统的可靠性和效率。

Abstract: Error-correcting codes are combinatorial objects designed to cope with the problem of reliable transmission of information on a noisy channel. A fundamental problem in coding theory and practice is to efficiently decode the received word with errors to obtain the transmitted codeword. In this paper, we consider the decoding problem of Hyperderivative Reed-Solomon (HRS) codes with respect to the NRT metric. Specifically, we propose a Welch-Berlekamp algorithm for the unique decoding of NRT HRS codes.

</details>


### [5] [Flexible-Duplex Cell-Free Architecture for Secure Uplink Communications in Low-Altitude Wireless Networks](https://arxiv.org/abs/2601.04011)
*Wei Shi,Wei Xu,Yongming Huang,Jiacheng Yao,Wenhao Hu,Dongming Wang*

Main category: cs.IT

TL;DR: 提出灵活双工无蜂窝架构，通过动态切换接入点工作模式（接收无人机上行或发送人工噪声）来增强低空无线网络的上行传输安全性。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络中无人机上行传输易受窃听，因为其发射功率有限、天线资源受限且空-地传播条件高度暴露，需要新的安全增强方案。

Method: 提出灵活双工无蜂窝架构，接入点可动态切换为接收模式（收集无人机上行）或发送模式（生成协作人工噪声）。采用惩罚对偶分解算法联合优化接入点模式选择、接收合并和人工噪声协方差设计。

Result: 仿真结果显示，该架构相比固定角色无蜂窝系统获得显著保密率增益。联合优化方法达到最高保密性能，低复杂度方法以低一个数量级的计算复杂度实现超过90%的最优性能。

Conclusion: 灵活双工无蜂窝架构为低空无线网络提供了有效的上行安全通信解决方案，通过动态分配接入点角色增强抗窃听能力，具有实际应用价值。

Abstract: Low-altitude wireless networks (LAWNs) are expected to play a central role in future 6G infrastructures, yet uplink transmissions of uncrewed aerial vehicles (UAVs) remain vulnerable to eavesdropping due to their limited transmit power, constrained antenna resources, and highly exposed air-ground propagation conditions. To address this fundamental bottleneck, we propose a flexible-duplex cell-free (CF) architecture in which each distributed access point (AP) can dynamically operate either as a receive AP for UAV uplink collection or as a transmit AP that generates cooperative artificial noise (AN) for secrecy enhancement. Such AP-level duplex flexibility introduces an additional spatial degree of freedom that enables distributed and adaptive protection against wiretapping in LAWNs. Building upon this architecture, we formulate a max-min secrecy-rate problem that jointly optimizes AP mode selection, receive combining, and AN covariance design. This tightly coupled and nonconvex optimization is tackled by first deriving the optimal receive combiners in closed form, followed by developing a penalty dual decomposition (PDD) algorithm with guaranteed convergence to a stationary solution. To further reduce computational burden, we propose a low-complexity sequential scheme that determines AP modes via a heuristic metric and then updates the AN covariance matrices through closed-form iterations embedded in the PDD framework. Simulation results show that the proposed flexible-duplex architecture yields substantial secrecy-rate gains over CF systems with fixed AP roles. The joint optimization method attains the highest secrecy performance, while the low-complexity approach achieves over 90% of the optimal performance with an order-of-magnitude lower computational complexity, offering a practical solution for secure uplink communications in LAWNs.

</details>


### [6] [Serving Every Symbol: All-Symbol PIR and Batch Codes](https://arxiv.org/abs/2601.04041)
*Avital Boruchovsky,Anina Gruica,Jonathan Niemann,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 该论文研究t-全符号PIR码和t-全符号批处理码的最小码长、结构特性及参数权衡，统一了多种已知码族并解决相关猜想。


<details>
  <summary>Details</summary>
Motivation: 研究t-全符号PIR码和批处理码的统一框架，该框架整合并扩展了多种已知码族（如一步多数逻辑可译码、PIR码、批处理码），旨在确定最小码长、分析结构特性，并探索参数间的权衡关系。

Method: 通过理论分析确定特定k和t值下的最小码长，表征达到最优的码的结构特性，推导长度、维度、最小距离和t之间的权衡界限，并研究MDS码和单纯形码在该框架下的表现。

Result: 确定了某些小k和t值下的最小码长，表征了最优码的结构特性，推导了参数间的权衡界限，展示了经典码族如何融入该框架，并解决了YAAKOBI2020中关于单纯形码作为t-函数批处理码的最小t值的部分猜想。

Conclusion: 该研究建立了t-全符号PIR码和批处理码的统一理论框架，解决了最小码长和结构特性等基本问题，为理解多种码族之间的内在联系提供了新视角，并推进了相关猜想的研究。

Abstract: A $t$-all-symbol PIR code and a $t$-all-symbol batch code of dimension $k$ consist of $n$ servers storing linear combinations of $k$ linearly independent information symbols with the following recovery property: any symbol stored by a server can be recovered from $t$ pairwise disjoint subsets of servers. In the batch setting, we further require that any multiset of size $t$ of stored symbols can be recovered from $t$ disjoint subsets of servers. This framework unifies and extends several well-known code families, including one-step majority-logic decodable codes, (functional) PIR codes, and (functional) batch codes.
  In this paper, we determine the minimum code length for some small values of $k$ and $t$, characterize structural properties of codes attaining this optimum, and derive bounds that show the trade-offs between length, dimension, minimum distance, and $t$. In addition, we study MDS codes and the simplex code, demonstrating how these classical families fit within our framework, and establish new cases of an open conjecture from \cite{YAAKOBI2020} concerning the minimal $t$ for which the simplex code is a $t$-functional batch code.

</details>


### [7] [Expectation Propagation for Distributed Inference in Grant-Free Cell-Free Massive MIMO](https://arxiv.org/abs/2601.04166)
*Christian Forsch,Laura Cottatellucci*

Main category: cs.IT

TL;DR: 提出两种基于期望传播的分布式算法，用于免授权无小区大规模MIMO系统中的联合活动检测、信道估计与数据检测，有效缓解导频污染并提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 免授权无小区大规模MIMO系统是下一代物联网网络的关键技术，但大量连接设备导致无法使用正交导频序列，造成严重的导频污染问题，影响信道估计和数据检测性能。同时，可扩展的GF-CF-MaMIMO网络需要分布式信号处理。

Method: 提出两种基于期望传播的分布式算法：JACD-EP使用高斯近似处理信道变量；JACD-EP-BG将信道变量建模为伯努利-高斯随机变量。通过推导BG分布的指数族表示，在因子图上构建高效的消息传递算法，实现联合活动检测、信道估计和数据检测。

Result: 仿真结果表明，提出的分布式算法能有效缓解导频污染，其检测精度优于（理想辅助的）集中式线性检测器。该框架在接入点和用户设备数量方面都具有良好的可扩展性。

Conclusion: 提出的基于期望传播的分布式算法为GF-CF-MaMIMO系统提供了一种有效的联合检测解决方案，能够处理导频污染问题，并在分布式架构下实现高性能检测，适用于大规模物联网网络。

Abstract: Grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) systems are anticipated to be a key enabling technology for next-generation Internet-of-Things (IoT) networks, as they support massive connectivity without explicit scheduling. However, the large amount of connected devices prevents the use of orthogonal pilot sequences, resulting in severe pilot contamination (PC) that degrades channel estimation and data detection performance. Furthermore, scalable GF-CF-MaMIMO networks inherently rely on distributed signal processing. In this work, we consider the uplink of a GF-CF-MaMIMO system and propose two novel distributed algorithms for joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP). The first algorithm, denoted as JACD-EP, uses Gaussian approximations for the channel variables, whereas the second, referred to as JACD-EP-BG, models them as Bernoulli-Gaussian (BG) random variables. To integrate the BG distribution into the EP framework, we derive its exponential family representation and develop the two algorithms as efficient message passing over a factor graph constructed from the a posteriori probability (APP) distribution. The proposed framework is inherently scalable with respect to both the number of access points (APs) and user equipments (UEs). Simulation results show the efficient mitigation of PC by the proposed distributed algorithms and their superior detection accuracy compared to (genie-aided) centralized linear detectors.

</details>


### [8] [A discrete Benamou-Brenier formulation of Optimal Transport on graphs](https://arxiv.org/abs/2601.04193)
*Kieran Morris,Oliver Johnson*

Main category: cs.IT

TL;DR: 提出图上的离散输运方程，推导Benamou-Brenier公式的离散版本，分类图上Wasserstein-1距离的所有测地线


<details>
  <summary>Details</summary>
Motivation: 将连续空间中的最优输运理论推广到离散图结构，建立图上的Wasserstein距离理论框架

Method: 提出连接顶点和边分布的离散输运方程，推导Benamou-Brenier公式的离散类比

Result: 建立了图上Wasserstein-1距离的离散Benamou-Brenier公式，分类了图上所有W1测地线

Conclusion: 成功将连续最优输运理论扩展到离散图结构，为图上的分布比较提供了理论工具

Abstract: We propose a discrete transport equation on graphs which connects distributions on both vertices and edges. We then derive a discrete analogue of the Benamou-Brenier formulation for Wasserstein-$1$ distance on a graph and as a result classify all $W_1$ geodesics on graphs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Algorithm and Strategy Construction for Sure-Almost-Sure Stochastic Parity Games](https://arxiv.org/abs/2601.03381)
*Laurent Doyen,Shibashis Guha*

Main category: cs.GT

TL;DR: 本文提出了一种基于递归算法的新方法，用于解决结合确定性和几乎确定性奇偶条件的随机双人博弈问题，能够构造获胜策略并改进复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 在"超越最坏情况"的合成框架中，需要将必须确定满足的硬性要求与概率性满足的软性要求相结合。现有的解决方案需要枚举对手的所有无记忆策略，这在所有情况下都是指数级的，并且无法在存在获胜策略时构造出这样的策略。

Method: 提出了一种基于获胜区域特征的递归算法。该算法能够构造实现确定性和几乎确定性奇偶条件组合的获胜策略，并为特殊问题类别（通过固定两个奇偶条件之一的索引定义）推导出新的复杂度和内存界限。

Result: 算法提供了对问题更深入的理解，能够构造获胜策略，并为特殊类别的问题改进了复杂度和内存界限分析。

Conclusion: 该递归算法比现有的指数级枚举方法更优，为结合确定性和几乎确定性奇偶条件的随机博弈问题提供了更有效的解决方案和理论分析。

Abstract: We consider turn-based stochastic two-player games with a combination of a parity condition that must hold surely, that is in all possible outcomes, and of a parity condition that must hold almost-surely, that is with probability 1. The problem of deciding the existence of a winning strategy in such games is central in the framework of synthesis beyond worst-case where a hard requirement that must hold surely is combined with a softer requirement. Recent works showed that the problem is coNP-complete, and infinite-memory strategies are necessary in general, even in one-player games (i.e., Markov decision processes). However, memoryless strategies are sufficient for the opponent player. Despite these comprehensive results, the known algorithmic solution enumerates all memoryless strategies of the opponent, which is exponential in all cases, and does not construct a winning strategy when one exists.
  We present a recursive algorithm, based on a characterisation of the winning region, that gives a deeper insight into the problem. In particular, we show how to construct a winning strategy to achieve the combination of sure and almost-sure parity, and we derive new complexity and memory bounds for special classes of the problem, defined by fixing the index of either of the two parity conditions.

</details>


### [10] [EFX and PO Allocation Exists for Two Types of Goods](https://arxiv.org/abs/2601.03438)
*Vladimir Davidiuk,Yuriy Dementiev,Artur Ignatiev,Danil Sagunov*

Main category: cs.GT

TL;DR: 证明了对于两种类型物品且估值均为正的情况下，EFX+PO分配总是存在且可在拟线性时间内计算，改进了先前仅保证EFX存在性的结果。


<details>
  <summary>Details</summary>
Motivation: 研究不可分割物品的公平有效分配问题，关注EFX这一重要公平概念。虽然已有工作证明了最多三种不同估值或两种类型物品下EFX的存在性，但一般情况仍未解决，且现有结果未保证帕累托最优性。

Method: 提出了一种相当简单高效的算法，在两种类型物品且估值均为正的条件下，构造同时满足EFX和帕累托最优的分配。

Result: 证明了EFX+PO分配总是存在，并且可以在拟线性时间内计算得到，这比Gorantla等人的结果更强（他们只保证了EFX存在性）。

Conclusion: 在两种类型物品且估值均为正的设定下，EFX与帕累托最优可以同时实现，且存在高效算法，为不可分割物品的公平有效分配提供了重要进展。

Abstract: We study the problem of fairly and efficiently allocating indivisible goods among agents with additive valuations. We focus on envy-freeness up to any good (EFX) -- an important fairness notion in fair division of indivisible goods. A central open question in this field is whether EFX allocations always exist for any number of agents. While prior work has established EFX existence for settings with at most three distinct valuations (Prakash HV et al. 2025) and for two types of goods (Gorantla, Marwaha, and Velusamy 2023), the general case remains unresolved.
  In this paper, we extend the existent knowledge by proving that EFX allocations satisfying Pareto optimality (PO) always exist and can be computed in quasiliniear time when there are two types of goods, given that the valuations are positive. This result strengthens the existing work of (Gorantla, Marwaha, and Velusamy 2023), which only guarantees the existence of EFX allocations without ensuring Pareto optimality. Our findings demonstrate a fairly simple and efficient algorithm constructing an EFX+PO allocation.

</details>


### [11] [From No-Regret to Strategically Robust Learning in Repeated Auctions](https://arxiv.org/abs/2601.03853)
*Junyao Zhao*

Main category: cs.GT

TL;DR: 任何无遗憾学习算法在分位数表示下都具有策略鲁棒性，可将标准遗憾保证转化为拍卖的策略鲁棒性保证


<details>
  <summary>Details</summary>
Motivation: 先前研究证明敏捷在线梯度下降在重复一级价格拍卖中具有策略鲁棒性，本文旨在证明这种性质不仅限于特定算法或拍卖格式，而是更普遍的规律

Method: 使用分位数表示单调竞价策略，将任何无遗憾学习算法应用于梯度反馈，建立迈尔森拍卖理论与标准无遗憾学习理论之间的简单关系

Result: 任何无遗憾学习算法在满足分配单调性和自愿参与的条件下都具有策略鲁棒性，乘性权重更新算法同时达到最优遗憾保证和最佳策略鲁棒性保证

Conclusion: 展示了将标准遗憾保证转化为特定博弈策略鲁棒性保证的潜力，无需显式最小化任何形式的交换遗憾

Abstract: In Bayesian single-item auctions, a monotone bidding strategy--one that prescribes a higher bid for a higher value type--can be equivalently represented as a partition of the quantile space into consecutive intervals corresponding to increasing bids. Kumar et al. (2024) prove that agile online gradient descent (OGD), when used to update a monotone bidding strategy through its quantile representation, is strategically robust in repeated first-price auctions: when all bidders employ agile OGD in this way, the auctioneer's average revenue per round is at most the revenue of Myerson's optimal auction, regardless of how she adjusts the reserve price over time.
  In this work, we show that this strategic robustness guarantee is not unique to agile OGD or to the first-price auction: any no-regret learning algorithm, when fed gradient feedback with respect to the quantile representation, is strategically robust, even if the auction format changes every round, provided the format satisfies allocation monotonicity and voluntary participation. In particular, the multiplicative weights update (MWU) algorithm simultaneously achieves the optimal regret guarantee and the best-known strategic robustness guarantee. At a technical level, our results are established via a simple relation that bridges Myerson's auction theory and standard no-regret learning theory. This showcases the potential of translating standard regret guarantees into strategic robustness guarantees for specific games, without explicitly minimizing any form of swap regret.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [12] [Transforming Video Subjective Testing with Training, Engagement, and Real-Time Feedback](https://arxiv.org/abs/2601.04184)
*Kumar Rahul,Sriram Sethuraman,Andrew Segall,Yixu Chen*

Main category: cs.MM

TL;DR: 提出一个集成框架，通过自动化训练测验、实时注意力评分和高效成对比较来改进主观视频质量评估，显著提高数据质量和减少非单调情况。


<details>
  <summary>Details</summary>
Motivation: 传统主观视频质量评估协议在捕捉细微感知差异和确保可靠用户输入方面存在局限性，需要改进评估者训练、注意力监控和比较效率。

Method: 采用三阶段方法：1) 自动化训练测验，让参与者学习视频质量指标；2) 实时注意力评分机制，使用"黄金"视频对监控评估者注意力；3) 高效的链式成对比较程序，以JOD单位生成质量分数。

Result: 实验显示训练测验显著提高黄金单元准确率并降低平局率，实时反馈进一步改善数据质量并产生最单调的质量评分，能显著减少高质量区域非单调情况。

Conclusion: 该集成框架通过改进评估者训练、注意力监控和比较效率，能显著提升主观视频质量评估的数据质量，有助于训练更好的客观视频质量指标。

Abstract: Subjective video quality assessment is crucial for optimizing streaming and compression, yet traditional protocols face limitations in capturing nuanced perceptual differences and ensuring reliable user input. We propose an integrated framework that enhances rater training, enforces attention through real-time scoring, and streamlines pairwise comparisons to recover quality scores with fewer comparisons. Participants first undergo an automated training quiz to learn key video quality indicators (e.g., compression artifacts) and verify their readiness. During the test, a real-time attention scoring mechanism, using "golden" video pairs, monitors and reinforces rater focus by applying penalties for lapses. An efficient chain-based pairwise comparison procedure is then employed, yielding quality scores in Just-Objectionable-Differences (JOD) units. Experiments comparing three groups (no training, training without feedback, and training with feedback) with 80 participants demonstrate that training-quiz significantly improves data quality in terms of golden unit accuracy and reduces tie rate, while real-time feedback further improves data quality and yields the most monotonic quality ratings. The new training, quiz, testing with feedback, 3-phase approach can significantly reduce the non-monotonic cases on the high quality part of the R-Q curve where normal viewer typically prefer the slightly compressed less-grainy content and help train a better objective video quality metric.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [13] [The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent](https://arxiv.org/abs/2601.03618)
*Muhammad Imam Luthfi Balaka,Raul Castro Fernandez*

Main category: cs.DB

TL;DR: Pneuma-Seeker是一个基于语言模型的系统，通过迭代交互帮助用户表达和满足模糊、演化的信息需求，将需求转化为关系数据模型并逐步收敛为可用文档。


<details>
  <summary>Details</summary>
Motivation: 数据发现和准备是数据管理生命周期中的持续瓶颈，特别是当用户意图模糊、演化或难以操作化时。需要一种方法来帮助用户表达和满足信息需求。

Method: 系统采用三种架构思想：1)上下文专业化以减少LLM在子任务中的负担；2)指挥式规划器来组装动态执行计划；3)基于共享状态的收敛机制。整合了RAG、代理框架和结构化数据准备技术。

Result: 通过LLM用户模拟评估显示，系统能帮助发现潜在意图、引导探索并生成适合用途的文档。同时作为新兴文档层，捕捉机构知识并支持组织记忆。

Conclusion: Pneuma-Seeker通过语言引导的工作流，有效解决了数据发现和准备中的意图表达问题，为模糊信息需求的满足提供了系统化解决方案。

Abstract: Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [14] [Enhancing Retrieval-Augmented Generation with Two-Stage Retrieval: FlashRank Reranking and Query Expansion](https://arxiv.org/abs/2601.03258)
*Sherine George*

Main category: cs.IR

TL;DR: 提出两阶段检索管道，结合LLM驱动的查询扩展和FlashRank重排器，在token预算下动态选择最优证据子集，提高RAG系统的准确性、忠实性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG框架面临检索召回率与有限LLM上下文之间的平衡问题：检索太少会遗漏关键上下文，检索太多会淹没提示窗口、稀释相关性并增加成本。

Method: 提出两阶段检索管道：1) LLM驱动的查询扩展提高候选召回率；2) FlashRank快速边际效用重排器，将文档效用建模为相关性、新颖性、简洁性和交叉编码器证据的加权组合，在token预算下动态选择最优证据子集。

Result: 该方法形成通用解决方案，提高答案准确性、忠实性和计算效率。

Conclusion: 通过集成查询扩展和基于效用的重排，该两阶段管道有效解决了RAG中检索数量与上下文限制的平衡问题，实现了更好的性能与效率权衡。

Abstract: Retrieval-Augmented Generation (RAG) couples a retriever with a large language model (LLM) to ground generated responses in external evidence. While this framework enhances factuality and domain adaptability, it faces a key bottleneck: balancing retrieval recall with limited LLM context. Retrieving too few passages risks missing critical context, while retrieving too many overwhelms the prompt window, diluting relevance and increasing cost.
  We propose a two-stage retrieval pipeline that integrates LLM-driven query expansion to improve candidate recall and FlashRank, a fast marginal-utility reranker that dynamically selects an optimal subset of evidence under a token budget. FlashRank models document utility as a weighted combination of relevance, novelty, brevity, and cross-encoder evidence. Together, these modules form a generalizable solution that increases answer accuracy, faithfulness, and computational efficiency.

</details>


### [15] [LLMDiRec: LLM-Enhanced Intent Diffusion for Sequential Recommendation](https://arxiv.org/abs/2601.03259)
*Bo-Chian Chen,Manel Slokom*

Main category: cs.IR

TL;DR: LLMDiRec：一种结合大语言模型与意图感知扩散模型的新推荐方法，通过动态融合ID嵌入与语义表示，显著提升对复杂用户意图的捕捉能力，尤其在长尾物品推荐上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型（包括先进的扩散模型）主要依赖ID嵌入，缺乏语义基础，难以捕捉用户行为背后的丰富语义意图，尤其对新用户和长尾物品效果不佳。

Method: 提出LLMDiRec方法，将大语言模型集成到意图感知扩散模型中，通过动态融合机制结合ID嵌入的协同信号与LLM的语义表示，并使用多任务目标对齐两种视图。

Result: 在五个公开数据集上的实验表明，LLMDiRec优于现有最先进算法，在捕捉复杂用户意图和提升长尾物品推荐性能方面表现尤为突出。

Conclusion: 通过结合大语言模型的语义理解能力与扩散模型的协同过滤优势，LLMDiRec有效解决了现有推荐系统在语义意图捕捉方面的局限性，为推荐系统提供了新的解决方案。

Abstract: Existing sequential recommendation models, even advanced diffusion-based approaches, often struggle to capture the rich semantic intent underlying user behavior, especially for new users or long-tail items. This limitation stems from their reliance on ID-based embeddings, which lack semantic grounding. We introduce LLMDiRec, a new approach that addresses this gap by integrating Large Language Models (LLMs) into an intent-aware diffusion model. Our approach combines collaborative signals from ID embeddings with rich semantic representations from LLMs, using a dynamic fusion mechanism and a multi-task objective to align both views. We run extensive experiments on five public datasets. We run extensive experiments on five public datasets. We demonstrate that \modelname outperforms state-of-the-art algorithms, with particularly strong improvements in capturing complex user intents and enhancing recommendation performance for long-tail items.

</details>


### [16] [Roles of MLLMs in Visually Rich Document Retrieval for RAG: A Survey](https://arxiv.org/abs/2601.03262)
*Xiantao Zhang*

Main category: cs.IR

TL;DR: 该综述探讨了多模态大语言模型如何解决视觉丰富文档检索中的挑战，将其分为三种角色：模态统一描述器、多模态嵌入器和端到端表示器，并比较了它们在检索粒度、信息保真度等方面的差异。


<details>
  <summary>Details</summary>
Motivation: 视觉丰富文档（如包含复杂布局、图表、表格的文档）给检索增强生成带来了独特挑战，包括布局依赖的语义、脆弱的OCR识别以及跨多个视觉元素分散的证据。需要探索多模态大语言模型如何使VRD检索变得实用。

Method: 将文献组织为三种角色：1) 模态统一描述器 - 将视觉内容转换为文本描述；2) 多模态嵌入器 - 生成联合的视觉-文本嵌入；3) 端到端表示器 - 直接处理原始文档进行检索。从检索粒度、信息保真度、延迟与索引大小、与重排序和接地的兼容性等维度进行比较。

Result: 分析了三种角色的优缺点和适用场景，提供了实用指导：何时优先选择每种角色。模态统一描述器适合文本主导检索，多模态嵌入器平衡保真度和效率，端到端表示器提供最高保真度但计算成本高。

Conclusion: 多模态大语言模型为视觉丰富文档检索提供了有前景的解决方案，但需要在不同角色间权衡。未来研究方向包括自适应检索单元、模型大小缩减、评估方法开发等，以推动该领域发展。

Abstract: Visually rich documents (VRDs) challenge retrieval-augmented generation (RAG) with layout-dependent semantics, brittle OCR, and evidence spread across complex figures and structured tables. This survey examines how Multimodal Large Language Models (MLLMs) are being used to make VRD retrieval practical for RAG. We organize the literature into three roles: Modality-Unifying Captioners, Multimodal Embedders, and End-to-End Representers. We compare these roles along retrieval granularity, information fidelity, latency and index size, and compatibility with reranking and grounding. We also outline key trade-offs and offer some practical guidance on when to favor each role. Finally, we identify promising directions for future research, including adaptive retrieval units, model size reduction, and the development of evaluation methods.

</details>


### [17] [Efficient Sequential Recommendation for Long Term User Interest Via Personalization](https://arxiv.org/abs/2601.03479)
*Qiang Zhang,Hanchao Yu,Ivan Ji,Chen Yuan,Yi Zhang,Chihuang Liu,Xiaolong Wang,Christopher E. Lambert,Ren Chen,Chen Kovacs,Xinzhu Bei,Renqin Cai,Rui Li,Lizhu Zhang,Xiangjun Fan,Qunshu Zhang,Benyu Zhang*

Main category: cs.IR

TL;DR: 提出一种基于个性化技术的序列推荐新方法，通过将长用户交互历史压缩为可学习token来降低计算成本，同时保持推荐准确性


<details>
  <summary>Details</summary>
Motivation: 现有序列模型在推荐应用中存在计算效率问题，特别是基于Transformer的模型具有二次方复杂度，难以处理长用户交互历史

Method: 将长用户交互历史压缩为可学习token，与近期交互结合生成推荐，可应用于现有Transformer推荐模型如HSTU和HLLM

Result: 显著降低计算成本的同时保持高推荐准确性，在多个序列模型上验证了方法的通用性和有效性

Conclusion: 提出的个性化序列推荐方法有效解决了长序列推荐的计算效率问题，为实际应用提供了可行的解决方案

Abstract: Recent years have witnessed success of sequential modeling, generative recommender, and large language model for recommendation. Though the scaling law has been validated for sequential models, it showed inefficiency in computational capacity when considering real-world applications like recommendation, due to the non-linear(quadratic) increasing nature of the transformer model. To improve the efficiency of the sequential model, we introduced a novel approach to sequential recommendation that leverages personalization techniques to enhance efficiency and performance. Our method compresses long user interaction histories into learnable tokens, which are then combined with recent interactions to generate recommendations. This approach significantly reduces computational costs while maintaining high recommendation accuracy. Our method could be applied to existing transformer based recommendation models, e.g., HSTU and HLLM. Extensive experiments on multiple sequential models demonstrate its versatility and effectiveness. Source code is available at \href{https://github.com/facebookresearch/PerSRec}{https://github.com/facebookresearch/PerSRec}.

</details>


### [18] [STELLA: Self-Reflective Terminology-Aware Framework for Building an Aerospace Information Retrieval Benchmark](https://arxiv.org/abs/2601.03496)
*Bongmin Kim*

Main category: cs.IR

TL;DR: 提出STELLA框架构建航空航天领域信息检索基准，包含术语匹配查询和术语无关查询，用于评估嵌入模型的词汇和语义匹配能力。


<details>
  <summary>Details</summary>
Motivation: 航空航天领域严重依赖技术文档检索，但缺乏反映该领域术语和查询意图特点的公开IR基准数据集。

Method: 使用STELLA框架从NASA技术报告构建基准，包含文档布局检测、段落分块、术语词典构建、合成查询生成和跨语言扩展。生成术语一致查询(TCQ)和术语无关查询(TAQ)两种查询类型。

Result: 评估7个嵌入模型显示，大型解码器模型在语义理解方面最强，而BM25等词汇匹配方法在需要精确术语匹配的领域仍具竞争力。

Conclusion: STELLA基准为航空航天领域IR任务提供了可复现的性能评估基础，有助于改进嵌入模型。

Abstract: Tasks in the aerospace industry heavily rely on searching and reusing large volumes of technical documents, yet there is no public information retrieval (IR) benchmark that reflects the terminology- and query-intent characteristics of this domain. To address this gap, this paper proposes the STELLA (Self-Reflective TErminoLogy-Aware Framework for BuiLding an Aerospace Information Retrieval Benchmark) framework. Using this framework, we introduce the STELLA benchmark, an aerospace-specific IR evaluation set constructed from NASA Technical Reports Server (NTRS) documents via a systematic pipeline that comprises document layout detection, passage chunking, terminology dictionary construction, synthetic query generation, and cross-lingual extension. The framework generates two types of queries: the Terminology Concordant Query (TCQ), which includes the terminology verbatim to evaluate lexical matching, and the Terminology Agnostic Query (TAQ), which utilizes the terminology's description to assess semantic matching. This enables a disentangled evaluation of the lexical and semantic matching capabilities of embedding models. In addition, we combine Chain-of-Density (CoD) and the Self-Reflection method with query generation to improve quality and implement a hybrid cross-lingual extension that reflects real user querying practices. Evaluation of seven embedding models on the STELLA benchmark shows that large decoder-based embedding models exhibit the strongest semantic understanding, while lexical matching methods such as BM25 remain highly competitive in domains where exact lexical matching technical term is crucial. The STELLA benchmark provides a reproducible foundation for reliable performance evaluation and improvement of embedding models in aerospace-domain IR tasks. The STELLA benchmark can be found in https://huggingface.co/datasets/telepix/STELLA.

</details>


### [19] [Shielded RecRL: Explanation Generation for Recommender Systems without Ranking Degradation](https://arxiv.org/abs/2601.03608)
*Ansh Tiwari,Ayush Chauhan*

Main category: cs.IR

TL;DR: Shielded RecRL是一种强化学习方法，通过两塔架构为推荐系统生成个性化解释，保持原有排序性能不变，仅训练LLM的0.4%参数，在亚马逊图书数据集上CTR提升22.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于RLHF的推荐方法直接优化物品排序，可能会损害推荐系统的原始排名性能。需要一种方法能在生成个性化解释的同时，保持推荐系统的核心排序能力不受影响。

Method: 采用两塔架构：保持推荐排序模型不变，语言模型学习生成有用解释。设计复合奖励信号（解释长度、内容相关性、连贯性），使用PPO和KL散度约束，通过LoRA适配器仅微调大语言模型0.4%的参数。

Result: 在亚马逊图书数据集（约5万条幻想和浪漫题材交互）上，相对点击率提升22.5%（1.225倍于基线），同时推荐系统的物品排序行为几乎保持不变。消融研究证实梯度屏蔽策略和奖励设计有效平衡了解释质量和策略漂移。

Conclusion: Shielded RecRL通过丰富的个性化解释增强了推荐系统的用户面向方面，同时不降低核心推荐准确性，实现了推荐性能与解释质量的平衡。

Abstract: We introduce Shielded RecRL, a reinforcement learning approach to generate personalized explanations for recommender systems without sacrificing the system's original ranking performance. Unlike prior RLHF-based recommender methods that directly optimize item rankings, our two-tower architecture keeps the recommender's ranking model intact while a language model learns to produce helpful explanations. We design a composite reward signal combining explanation length, content relevance, and coherence, and apply proximal policy optimization (PPO) with a KL-divergence constraint to fine-tune a large language model with only 0.4% of its parameters trainable via LoRA adapters. In experiments on an Amazon Books dataset (approximately 50K interactions in the fantasy and romance genres), Shielded RecRL improved the relative click-through rate (CTR) by 22.5% (1.225x over baseline) while keeping the recommender's item-ranking behavior virtually unchanged. An extensive ablation study confirms that our gradient shielding strategy and reward design effectively balance explanation quality and policy drift. Our results demonstrate that Shielded RecRL enhances user-facing aspects of recommendations through rich, personalized explanations without degrading core recommendation accuracy.

</details>


### [20] [Perception-Aware Bias Detection for Query Suggestions](https://arxiv.org/abs/2601.03730)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 本文扩展了查询建议中的偏见检测方法，针对人物相关搜索，引入感知感知指标以克服查询建议稀疏性和上下文元数据缺乏的问题，从而更好地检测系统性主题偏见。


<details>
  <summary>Details</summary>
Motivation: 网络搜索偏见检测研究主要关注搜索结果本身，而查询建议中的偏见问题被忽视。查询建议具有稀疏性、缺乏上下文元数据，且用户感知时间短暂，需要自动化的偏见检测方法。

Method: 在Bonart等人的人物相关搜索查询建议偏见检测流程基础上，引入感知感知指标，以克服查询建议的稀疏性和上下文元数据缺乏问题，增强系统性主题偏见的检测能力。

Result: 增强后的流程能够更好地检测搜索引擎人物相关搜索查询建议中的系统性主题偏见。分析结果证实了这一假设，且由于采用感知感知偏见检测指标，流程发现的偏见可反映用户实际能察觉的偏见。

Conclusion: 通过引入感知感知指标，本文成功扩展了查询建议偏见检测流程，使其能够更有效地识别人物相关搜索查询建议中的系统性主题偏见，且检测结果与用户实际感知相符。

Abstract: Bias in web search has been in the spotlight of bias detection research for quite a while. At the same time, little attention has been paid to query suggestions in this regard. Awareness of the problem of biased query suggestions has been raised. Likewise, there is a rising need for automatic bias detection approaches. This paper adds on the bias detection pipeline for bias detection in query suggestions of person-related search developed by Bonart et al. \cite{Bonart_2019a}. The sparseness and lack of contextual metadata of query suggestions make them a difficult subject for bias detection. Furthermore, query suggestions are perceived very briefly and subliminally. To overcome these issues, perception-aware metrics are introduced. Consequently, the enhanced pipeline is able to better detect systematic topical bias in search engine query suggestions for person-related searches. The results of an analysis performed with the developed pipeline confirm this assumption. Due to the perception-aware bias detection metrics, findings produced by the pipeline can be assumed to reflect bias that users would discern.

</details>


### [21] [Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning](https://arxiv.org/abs/2601.03748)
*Dario Maio,Stefano Rizzi*

Main category: cs.IR

TL;DR: 提出维度事实模型（DFM）作为指导大规模RAG系统多维分区的概念框架，将OLAP风格的多维建模与RAG架构结合，实现可解释、可治理的检索策略。


<details>
  <summary>Details</summary>
Motivation: 当前大规模RAG系统虽然通过水平分片、近似最近邻搜索等技术解决可扩展性问题，但缺乏概念性的语料库分区依据，依赖自下而上的相似性驱动组织，多维信息使用方式随意且缺乏结构。

Method: 提出维度事实模型（DFM）作为概念框架，结合语义聚类（优化嵌入空间局部性）和多维分区（基于时间、组织上下文等概念维度指导检索位置），支持分层路由和受控回退策略。

Result: DFM框架为RAG语料库提供了原则性的多维分区设计方法，将检索过程从"黑盒"相似性匹配转变为可治理、确定性的工作流程，确保在元数据不完整时检索仍保持鲁棒性。

Conclusion: 该位置论文旨在弥合OLAP风格多维建模与现代RAG架构之间的差距，促进大规模、原则性、可解释且可治理的检索策略的进一步研究。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.

</details>


### [22] [Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation](https://arxiv.org/abs/2601.03903)
*Yuhan Yang,Jie Zou,Guojia An,Jiwei Wei,Yang Yang,Heng Tao Shen*

Main category: cs.IR

TL;DR: DiffSBR：基于扩散的潜在邻居生成模型，通过检索增强和自增强扩散模块生成高质量潜在邻居，缓解会话推荐中的数据稀疏问题，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有会话推荐方法通常依赖显式观察的会话数据，忽略了潜在邻居（未直接观察但在兴趣空间中可能相关的会话），未能充分利用邻居会话的潜力。数据稀疏问题限制了推荐性能。

Method: 提出DiffSBR模型，包含两个扩散模块：1）检索增强扩散模块：利用检索到的邻居作为引导信号，约束和重构潜在邻居分布，并通过训练策略让检索器从生成器的反馈中学习；2）自增强扩散模块：通过对比学习注入当前会话的多模态信号，显式指导潜在邻居生成。生成潜在邻居后，用于增强会话表示以改进推荐。

Result: 在四个公开数据集上的大量实验表明，DiffSBR能够生成有效的潜在邻居，并在推荐性能上优于最先进的基线方法。

Conclusion: DiffSBR通过扩散模型生成高质量潜在邻居，有效缓解了会话推荐中的数据稀疏问题，提升了推荐性能，证明了利用潜在邻居的潜力对于改进会话推荐的重要性。

Abstract: Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [23] [Optimizing Exact String Matching via Statistical Anchoring](https://arxiv.org/abs/2601.03271)
*Omar Garraoui*

Main category: cs.DS

TL;DR: 提出一种针对自然语言文本的Boyer-Moore-Horspool算法改进，通过预处理模式识别统计上最不频繁的"锚点"字符，在搜索时优先验证该高熵位置，快速排除不匹配窗口，减少不必要的比较。


<details>
  <summary>Details</summary>
Motivation: 经典的模式匹配算法如Boyer-Moore-Horspool在通用文本上表现良好，但在自然语言文本中，可以利用语言统计特性进一步优化性能。通过利用字符频率分布信息，可以设计更有效的匹配策略。

Method: 1. 预处理搜索模式，识别统计上最不频繁的字符作为"锚点"；2. 在搜索过程中，首先验证这个高熵位置；3. 如果锚点不匹配，快速跳过当前窗口，减少不必要的完整模式比较；4. 保持原有算法的移位启发式不变。

Result: 该方法在不增加算法复杂度的情况下提高了模式匹配效率，通过优先验证高熵位置实现了"快速失败"策略，减少了不必要的字符比较，从而提升了整体性能。

Conclusion: 将基本的语言统计信息融入经典模式匹配技术可以在不增加复杂度的情况下提升性能，证明了结合领域特定知识（如自然语言统计特性）对算法优化的价值。

Abstract: In this work, we propose an enhancement to the Boyer-Moore-Horspool algorithm tailored for natural language text. The approach involves preprocessing the search pattern to identify its statistically least frequent character, referred to as the "anchor." During the search, verification is first performed at this high-entropy position, allowing the algorithm to quickly discard non-matching windows. This fail-fast strategy reduces unnecessary comparisons, improving overall efficiency. Our implementation shows that incorporating basic linguistic statistics into classical pattern-matching techniques can boost performance without increasing complexity to the shift heuristics.

</details>


### [24] [Counting hypertriangles through hypergraph orientations](https://arxiv.org/abs/2601.03573)
*Daniel Paul-Pena,Vaishali Surianarayanan,Deeparnab Chakrabarty,C. Seshadhri*

Main category: cs.DS

TL;DR: 提出DITCH算法，用于高效计数超图中的超三角形，相比现有方法快10-100倍且内存效率更高


<details>
  <summary>Details</summary>
Motivation: 许多现实世界数据集自然建模为超图，需要高效的超图模式计数算法。超三角形作为三个两两相交的超边集合，具有比图三角形更丰富的结构和多种不同的相交模式

Method: 受基于方向和退化性的经典图算法启发，开发了将图算法概念推广到超图的理论框架，并基于此实现了DITCH算法

Result: DITCH算法在实验中比现有最先进方法快10-100倍，且内存效率更高

Conclusion: 通过将图算法的方向和退化性概念推广到超图，可以开发出高效的超三角形计数算法，DITCH算法在实际应用中表现出显著优势

Abstract: Counting the number of small patterns is a central task in network analysis. While this problem is well studied for graphs, many real-world datasets are naturally modeled as hypergraphs, motivating the need for efficient hypergraph motif counting algorithms. In particular, we study the problem of counting hypertriangles - collections of three pairwise-intersecting hyperedges. These hypergraph patterns have a rich structure with multiple distinct intersection patterns unlike graph triangles.
  Inspired by classical graph algorithms based on orientations and degeneracy, we develop a theoretical framework that generalizes these concepts to hypergraphs and yields provable algorithms for hypertriangle counting. We implement these ideas in DITCH (Degeneracy Inspired Triangle Counter for Hypergraphs) and show experimentally that it is 10-100x faster and more memory efficient than existing state-of-the-art methods.

</details>


### [25] [On $k$-connectivity oracles in $k$-connected graphs](https://arxiv.org/abs/2601.03643)
*Zeev Nutov*

Main category: cs.DS

TL;DR: 该论文证明即使图是k-连通的，k-连通性预言机也需要Ω(kn)比特空间，回答了Pettie等人的开放问题


<details>
  <summary>Details</summary>
Motivation: Pettie等人证明对于无向图，任何k-连通性预言机都需要Ω(kn)比特空间，但他们提出开放问题：如果图本身是k-连通的，是否仍然需要Ω(kn)比特空间？本文旨在回答这个问题

Method: 使用非常简单的证明方法，证明即使图是k-连通的，k-连通性预言机仍然需要Ω(kn)比特空间

Result: 证明了对于k-连通图，任何k-连通性预言机仍然需要Ω(kn)比特空间，回答了Pettie等人的开放问题

Conclusion: 即使图是k-连通的，k-连通性预言机的空间复杂度下界仍然是Ω(kn)比特，这个下界是紧的，不能通过图的连通性假设来降低

Abstract: A $k$-connectivity oracle for a graph $G=(V,E)$ is a data structure that given $s,t \in V$ determines whether there are at least $k+1$ internally disjoint $st$-paths in $G$. For undirected graphs, Pettie, Saranurak & Yin [STOC 2022, pp. 151-161] proved that any $k$-connectivity oracle requires $Ω(kn)$ bits of space. They asked whether $Ω(kn)$ bits are still necessary if $G$ is $k$-connected. We will show by a very simple proof that this is so even if $G$ is $k$-connected, answering this open question.

</details>


### [26] [Complexity of Perfect and Ideal Resilience Verification in Fast Re-Route Networks](https://arxiv.org/abs/2601.03934)
*Matthias Bentert,Esra Ceylan-Kettler,Valentin Hübner,Stefan Schmid,Jiří Srba*

Main category: cs.DS

TL;DR: 本文研究了网络快速重路由机制中完美弹性的计算复杂性，证明了验证完美弹性是coNP完全的，但在某些简化场景下存在线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 现代通信网络采用完全分布式的快速重路由机制来快速恢复链路故障。这些机制依赖于节点上预安装的静态重路由规则，这些规则仅基于本地故障信息。理想情况下，网络应具有完美弹性：只要源节点和目标节点在故障后仍然物理连接，重路由规则就能确保数据包始终成功路由到目的地。然而，实现完美弹性并不总是可能，且关于何时以及如何实现完美弹性的算法方面知之甚少。

Method: 研究分析了本地快速重路由机制的计算复杂性。主要采用理论计算机科学中的复杂性分析方法，证明了验证给定静态重路由规则是否确保完美弹性是coNP完全的。同时研究了理想弹性（文献中常用的较弱弹性概念）的复杂性。还探讨了问题的其他基本变体，包括重路由规则具有特定模式（文献中称为跳过）的情况。在简化场景下，开发了线性时间算法。

Result: 主要结果是负面的：验证给定静态重路由规则是否确保完美弹性是coNP完全的。理想弹性的验证同样是coNP完全的。即使在重路由规则具有特定模式（跳过）的场景下，coNP完全性仍然成立。正面结果是：在节点没有数据包到达端口信息（即所谓入端口）的场景下，提出了完美弹性的验证和合成问题的线性时间算法。

Conclusion: 本文揭示了网络快速重路由机制中完美弹性分析的计算复杂性本质上是困难的（coNP完全），这解释了为什么在实践中难以实现完美弹性。然而，在某些简化场景下（如无入端口信息），问题变得可处理，存在高效算法。这些结果为网络弹性研究提供了重要的理论基础和算法边界。

Abstract: To achieve fast recovery from link failures, most modern communication networks feature fully decentralized fast re-routing mechanisms. These re-routing mechanisms rely on pre-installed static re-routing rules at the nodes (the routers), which depend only on local failure information, namely on the failed links incident to the node. Ideally, a network is perfectly resilient: the re-routing rules ensure that packets are always successfully routed to their destinations as long as the source and the destination are still physically connected in the underlying network after the failures. Unfortunately, there are examples where achieving perfect resilience is not possible. Surprisingly, only very little is known about the algorithmic aspect of when and how perfect resilience can be achieved.
  We investigate the computational complexity of analyzing such local fast re-routing mechanisms. Our main result is a negative one: we show that even checking whether a given set of static re-routing rules ensures perfect resilience is coNP-complete. We also show coNP-completeness of the so-called ideal resilience, a weaker notion of resilience often considered in the literature. Additionally, we investigate other fundamental variations of the problem. In particular, we show that our coNP-completeness proof also applies to scenarios where the re-routing rules have specific patterns (known as skipping in the literature).
  On the positive side, for scenarios where nodes do not have information about the link from which a packet arrived (the so-called in-port), we present linear-time algorithms for both the verification and synthesis problem for perfect resilience.

</details>


### [27] [A Polynomial Kernel for Face Cover on Non-Embedded Planar Graphs](https://arxiv.org/abs/2601.04169)
*Thekla Hamm,Sukanya Pandey,Krisztina Szilágyi*

Main category: cs.DS

TL;DR: 本文提出了平面图（无固定嵌入）中Face Cover Number问题的第一个多项式核


<details>
  <summary>Details</summary>
Motivation: Face Cover Number问题在给定平面嵌入时已有多项式核，但对于无固定嵌入的平面图，尚未有已知的多项式核。本文旨在解决这一挑战

Method: 采用自底向上的方法，在SPR树上构建核，同时保持面覆盖的基本性质。克服了没有预定义面边界的挑战

Result: 成功为无固定嵌入的平面图Face Cover Number问题构建了第一个多项式核

Conclusion: 本文解决了平面图（无固定嵌入）中Face Cover Number问题的多项式核构建问题，为参数化算法提供了重要工具

Abstract: Given a planar graph, a subset of its vertices called terminals, and $k \in \mathbb{N}$, the Face Cover Number problem asks whether the terminals lie on the boundaries of at most $k$ faces of some embedding of the input graph. When a plane graph is given in the input, the problem is known to have a polynomial kernel~\cite{GarneroST17}. In this paper, we present the first polynomial kernel for Face Cover Number when the input is a planar graph (without a fixed embedding). Our approach overcomes the challenge of not having a predefined set of face boundaries by building a kernel bottom-up on an SPR-tree while preserving the essential properties of the face cover along the way.

</details>
