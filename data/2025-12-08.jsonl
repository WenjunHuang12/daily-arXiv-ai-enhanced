{"id": "2512.05247", "categories": ["cs.DS", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.05247", "abs": "https://arxiv.org/abs/2512.05247", "authors": ["Spencer Gibson", "Yun William Yu"], "title": "Incorporating indel channels into average-case analysis of seed-chain-extend", "comment": "25 pages (10 page main text + 2 page biblio + 13 page appendix); conference submission", "summary": "Given a sequence $s_1$ of $n$ letters drawn i.i.d. from an alphabet of size $σ$ and a mutated substring $s_2$ of length $m < n$, we often want to recover the mutation history that generated $s_2$ from $s_1$. Modern sequence aligners are widely used for this task, and many employ the seed-chain-extend heuristic with $k$-mer seeds. Previously, Shaw and Yu showed that optimal linear-gap cost chaining can produce a chain with $1 - O\\left(\\frac{1}{\\sqrt{m}}\\right)$ recoverability, the proportion of the mutation history that is recovered, in $O\\left(mn^{2.43θ} \\log n\\right)$ expected time, where $θ< 0.206$ is the mutation rate under a substitution-only channel and $s_1$ is assumed to be uniformly random. However, a gap remains between theory and practice, since real genomic data includes insertions and deletions (indels), and yet seed-chain-extend remains effective. In this paper, we generalize those prior results by introducing mathematical machinery to deal with the two new obstacles introduced by indel channels: the dependence of neighboring anchors and the presence of anchors that are only partially correct. We are thus able\n  to prove that the expected recoverability of an optimal chain is $\\ge 1 - O\\Bigl(\\frac{1}{\\sqrt{m}}\\Bigr)$ and the expected runtime is $O(mn^{3.15 \\cdot θ_T}\\log n)$, when the total mutation rate given by the sum of the substitution, insertion, and deletion mutation rates ($θ_T = θ_i + θ_d + θ_s$) is less than $0.159$."}
{"id": "2512.05300", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.05300", "abs": "https://arxiv.org/abs/2512.05300", "authors": ["Yonggang Jiang", "Yaowei Long", "Thatchaphol Saranurak", "Benyu Wang"], "title": "Crude Approximation of Directed Minimum Cut and Arborescence Packing in Almost Linear Time", "comment": null, "summary": "We give almost-linear-time algorithms for approximating rooted minimum cut and maximum arborescence packing in directed graphs, two problems that are dual to each other [Edm73]. More specifically, for an $n$-vertex, $m$-edge directed graph $G$ whose $s$-rooted minimum cut value is $k$, our first algorithm computes an $s$-rooted cut of size at most $O(k\\log^{5} n)$ in $m^{1+o(1)}$ time, and our second algorithm packs $k$ $s$-rooted arborescences with $n^{o(1)}$ congestion in $m^{1+o(1)}$ time, certifying that the $s$-rooted minimum cut is at least $k / n^{o(1)}$. Our first algorithm also works for weighted graphs.\n  Prior to our work, the fastest algorithms for computing the $s$-rooted minimum cut were exact but had super-linear running time: either $\\tilde{O}(mk)$ [Gab91] or $\\tilde{O}(m^{1+o(1)}\\min\\{\\sqrt{n},n/m^{1/3}\\})$ [CLN+22]. The fastest known algorithms for packing $s$-rooted arborescences had no congestion, but required $\\tilde{O}(m \\cdot \\mathrm{poly}(k))$ time [BHKP08]."}
{"id": "2512.05235", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.05235", "abs": "https://arxiv.org/abs/2512.05235", "authors": ["David Pennock", "Daniel Schoepflin", "Kangning Wang"], "title": "Strategyproof Tournament Rules for Teams with a Constant Degree of Selfishness", "comment": null, "summary": "We revisit the well-studied problem of designing fair and manipulation-resistant tournament rules. In this problem, we seek a mechanism that (probabilistically) identifies the winner of a tournament after observing round-robin play among $n$ teams in a league. Such a mechanism should satisfy the natural properties of monotonicity and Condorcet consistency. Moreover, from the league's perspective, the winner-determination tournament rule should be strategyproof, meaning that no team can do better by losing a game on purpose.\n  Past work considered settings in which each team is fully selfish, caring only about its own probability of winning, and settings in which each team is fully selfless, caring only about the total winning probability of itself and the team to which it deliberately loses. More recently, researchers considered a mixture of these two settings with a parameter $λ$. Intermediate selfishness $λ$ means that a team will not lose on purpose unless its pair gains at least $λs$ winning probability, where $s$ is the individual team's sacrifice from its own winning probability. All of the dozens of previously known tournament rules require $λ= Ω(n)$ to be strategyproof, and it has been an open problem to find such a rule with the smallest $λ$.\n  In this work, we make significant progress by designing a tournament rule that is strategyproof with $λ= 11$. Along the way, we propose a new notion of multiplicative pairwise non-manipulability that ensures that two teams cannot manipulate the outcome of a game to increase the sum of their winning probabilities by more than a multiplicative factor $δ$ and provide a rule which is multiplicatively pairwise non-manipulable for $δ= 3.5$."}
{"id": "2512.05203", "categories": ["cs.DB", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.05203", "abs": "https://arxiv.org/abs/2512.05203", "authors": ["Vinicius Stein Dani", "Xixi Lu", "Iris Beerepoot"], "title": "Integrating Wearable Data into Process Mining: Event, Case and Activity Enrichment", "comment": "Accepted manuscript (on August 22, 2025) to the 1st International Workshop on Personal and Human-Centric Process Mining (PHPM 2025), held in conjunction with the 7th International Conference on Process Mining (ICPM 2025)", "summary": "In this short paper, we explore the enrichment of event logs with data from wearable devices. We discuss three approaches: (1) treating wearable data as event attributes, linking them directly to individual events, (2) treating wearable data as case attributes, using aggregated day-level scores, and (3) introducing new events derived from wearable data, such as sleep episodes or physical activities. To illustrate these approaches, we use real-world data from one person, matching health data from a smartwatch with events extracted from a digital calendar application. Finally, we discuss the technical and conceptual challenges involved in integrating wearable data into process mining for personal productivity and well-being."}
{"id": "2512.05249", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.05249", "abs": "https://arxiv.org/abs/2512.05249", "authors": ["Ankit Gupta", "Onur Dizdar", "Yun Chen", "Fehmi Emre Kadan", "Ata Sattarzadeh", "Stephen Wang"], "title": "Low-Complexity OFDM Deep Neural Receivers", "comment": null, "summary": "Deep neural receivers (NeuralRxs) for Orthogonal Frequency Division Multiplexing (OFDM) signals are proposed for enhanced decoding performance compared to their signal-processing based counterparts. However, the existing architectures ignore the required number of epochs for training convergence and floating-point operations (FLOPs), which increase significantly with improving performance. To tackle these challenges, we propose a new residual network (ResNet) block design for OFDM NeuralRx. Specifically, we leverage small kernel sizes and dilation rates to lower the number of FLOPs (NFLOPs) and uniform channel sizes to reduce the memory access cost (MAC). The ResNet block is designed with novel channel split and shuffle blocks, element-wise additions are removed, with Gaussian error linear unit (GELU) activations. Extensive simulations show that our proposed NeuralRx reduces NFLOPs and improves training convergence while improving the decoding accuracy."}
{"id": "2512.05119", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.05119", "abs": "https://arxiv.org/abs/2512.05119", "authors": ["Rongyang Zhang", "Yuqing Huang", "Chengqiang Lu", "Qimeng Wang", "Yan Gao", "Yi Wu", "Yao Hu", "Yin Xu", "Wei Wang", "Hao Wang", "Enhong Chen"], "title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "comment": "26 pages, 6 figures, NeurIPS 2025 D&B Track poster", "summary": "In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench."}
{"id": "2512.05271", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05271", "abs": "https://arxiv.org/abs/2512.05271", "authors": ["Rafael Frongillo", "Mary Monroe", "Eric Neyman", "Bo Waggoner"], "title": "Robust forecast aggregation via additional queries", "comment": null, "summary": "We study the problem of robust forecast aggregation: combining expert forecasts with provable accuracy guarantees compared to the best possible aggregation of the underlying information. Prior work shows strong impossibility results, e.g. that even under natural assumptions, no aggregation of the experts' individual forecasts can outperform simply following a random expert (Neyman and Roughgarden, 2022).\n  In this paper, we introduce a more general framework that allows the principal to elicit richer information from experts through structured queries. Our framework ensures that experts will truthfully report their underlying beliefs, and also enables us to define notions of complexity over the difficulty of asking these queries. Under a general model of independent but overlapping expert signals, we show that optimal aggregation is achievable in the worst case with each complexity measure bounded above by the number of agents $n$. We further establish tight tradeoffs between accuracy and query complexity: aggregation error decreases linearly with the number of queries, and vanishes when the \"order of reasoning\" and number of agents relevant to a query is $ω(\\sqrt{n})$. These results demonstrate that modest extensions to the space of expert queries dramatically strengthen the power of robust forecast aggregation. We therefore expect that our new query framework will open up a fruitful line of research in this area."}
{"id": "2512.05399", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.05399", "abs": "https://arxiv.org/abs/2512.05399", "authors": ["Sepanta Zeighami", "Shreya Shankar", "Aditya Parameswaran"], "title": "Featurized-Decomposition Join: Low-Cost Semantic Joins with Guarantees", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used within data systems to process large datasets with text fields. A broad class of such tasks involves a semantic join-joining two tables based on a natural language predicate per pair of tuples, evaluated using an LLM. Semantic joins generalize tasks such as entity matching and record categorization, as well as more complex text understanding tasks. A naive implementation is expensive as it requires invoking an LLM for every pair of rows in the cross product. Existing approaches mitigate this cost by first applying embedding-based semantic similarity to filter candidate pairs, deferring to an LLM only when similarity scores are deemed inconclusive. However, these methods yield limited gains in practice, since semantic similarity may not reliably predict the join outcome. We propose Featurized-Decomposition Join (FDJ for short), a novel approach for performing semantic joins that significantly reduces cost while preserving quality. FDJ automatically extracts features and combines them into a logical expression in conjunctive normal form that we call a featurized decomposition to effectively prune out non-matching pairs. A featurized decomposition extracts key information from text records and performs inexpensive comparisons on the extracted features. We show how to use LLMs to automatically extract reliable features and compose them into logical expressions while providing statistical guarantees on the output result-an inherently challenging problem due to dependencies among features. Experiments on real-world datasets show up to 10 times reduction in cost compared with the state-of-the-art while providing the same quality guarantees."}
{"id": "2512.05267", "categories": ["cs.IT", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05267", "abs": "https://arxiv.org/abs/2512.05267", "authors": ["Osvaldo Simeone", "Yaniv Romano"], "title": "Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective", "comment": null, "summary": "In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity."}
{"id": "2512.05334", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.05334", "abs": "https://arxiv.org/abs/2512.05334", "authors": ["Samaneh Mohtadi", "Kevin Roitero", "Stefano Mizzaro", "Gianluca Demartini"], "title": "The Effect of Document Summarization on LLM-Based Relevance Judgments", "comment": null, "summary": "Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments."}
{"id": "2512.05304", "categories": ["cs.GT", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2512.05304", "abs": "https://arxiv.org/abs/2512.05304", "authors": ["Rémi Castera", "Patrick Loiseau", "Bary S. R. Pradelski"], "title": "Correlation of Rankings in Matching Markets", "comment": "Management Science (2025)", "summary": "We study the role of correlation in matching markets, where multiple decision-makers simultaneously face selection problems from the same pool of candidates. We propose a model in which a candidate's priority scores across different decision-makers exhibit varying levels of correlation dependent on the candidate's sociodemographic group. Such differential correlation can arise in school choice due to the varying prevalence of selection criteria, in college admissions due to test-optional policies, or due to algorithmic monoculture, that is, when decision-makers rely on the same algorithms and data sets to evaluate candidates. We show that higher correlation for one of the groups generally improves the outcome for all groups, leading to higher efficiency. However, students from a given group are more likely to remain unmatched as their own correlation level increases. This implies that it is advantageous to belong to a low-correlation group. Finally, we extend the tie-breaking literature to multiple priority classes and intermediate levels of correlation. Overall, our results point to differential correlation as a previously overlooked systemic source of group inequalities in school, university, and job admissions."}
{"id": "2512.05417", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.05417", "abs": "https://arxiv.org/abs/2512.05417", "authors": ["Jinghe Song", "Zongyu Zuo", "Xuelian Lin", "Yang Wang", "Shuai Ma"], "title": "PETGraphDB: A Property Evolution Temporal Graph Data Management System", "comment": null, "summary": "Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency."}
{"id": "2512.05316", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.05316", "abs": "https://arxiv.org/abs/2512.05316", "authors": ["El Mahdi Mouloua", "Essaid Mohamed"], "title": "Foundations of information theory for coding theory", "comment": null, "summary": "Information theory is introduced in this lecture note with a particular emphasis on its relevance to algebraic coding theory. The document develops the mathematical foundations for quantifying uncertainty and information transmission by building upon Shannon's pioneering formulation of information, entropy, and channel capacity. Examples, including the binary symmetric channel, illustrate key concepts such as entropy, conditional entropy, mutual information, and the noisy channel model. Furthermore, the note describes the principles of maximum likelihood decoding and Shannon's noisy channel coding theorem, which characterizes the theoretical limits of reliable communication over noisy channels. Students and researchers seeking a connection between probabilistic frameworks of information theory and structural and algebraic techniques used in modern coding theory will find this work helpful."}
{"id": "2512.05411", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05411", "abs": "https://arxiv.org/abs/2512.05411", "authors": ["Pranav Pushkar Mishra", "Kranti Prakash Yeole", "Ramyashree Keshavamurthy", "Mokshit Bharat Surana", "Fatemeh Sarayloo"], "title": "A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems", "comment": "7 pages, 3 figures, 3 tables", "summary": "In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness."}
{"id": "2512.05667", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05667", "abs": "https://arxiv.org/abs/2512.05667", "authors": ["Jilles Steeve Dibangoye", "Thibaut Le Marre", "Ocan Sankur", "François Schwarzentruber"], "title": "On Dynamic Programming Theory for Leader-Follower Stochastic Games", "comment": "31 pages, 5 figures", "summary": "Leader-follower general-sum stochastic games (LF-GSSGs) model sequential decision-making under asymmetric commitment, where a leader commits to a policy and a follower best responds, yielding a strong Stackelberg equilibrium (SSE) with leader-favourable tie-breaking. This paper introduces a dynamic programming (DP) framework that applies Bellman recursion over credible sets-state abstractions formally representing all rational follower best responses under partial leader commitments-to compute SSEs. We first prove that any LF-GSSG admits a lossless reduction to a Markov decision process (MDP) over credible sets. We further establish that synthesising an optimal memoryless deterministic leader policy is NP-hard, motivating the development of ε-optimal DP algorithms with provable guarantees on leader exploitability. Experiments on standard mixed-motive benchmarks-including security games, resource allocation, and adversarial planning-demonstrate empirical gains in leader value and runtime scalability over state-of-the-art methods."}
{"id": "2512.05453", "categories": ["cs.DB", "cs.AI", "cs.CY", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.05453", "abs": "https://arxiv.org/abs/2512.05453", "authors": ["Luc Moreau", "Alfred Rossi", "Sophie Stalla-Bourdillon"], "title": "Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments", "comment": "17 pages, 8 figures. Code and examples available at https://github.com/alfredr/parajudica", "summary": "Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment."}
{"id": "2512.05967", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05967", "abs": "https://arxiv.org/abs/2512.05967", "authors": ["Francesco Granata", "Francesco Poggi", "Misael Mongiovì"], "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms", "comment": null, "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools."}
{"id": "2512.05843", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.05843", "abs": "https://arxiv.org/abs/2512.05843", "authors": ["Ilia Shilov", "Mingjia He", "Heinrich H. Nax", "Emilio Frazzoli", "Gioele Zardini", "Saverio Bolognani"], "title": "Invariant Price of Anarchy: a Metric for Welfarist Traffic Control", "comment": null, "summary": "The Price of Anarchy (PoA) is a standard metric for quantifying inefficiency in socio-technical systems, widely used to guide policies like traffic tolling. Conventional PoA analysis relies on exact numerical costs. However, in many settings, costs represent agents' preferences and may be defined only up to possibly arbitrary scaling and shifting, representing informational and modeling ambiguities. We observe that while such transformations preserve equilibrium and optimal outcomes, they change the PoA value. To resolve this issue, we rely on results from Social Choice Theory and define the Invariant PoA. By connecting admissible transformations to degrees of comparability of agents' costs, we derive the specific social welfare functions which ensure that efficiency evaluations do not depend on arbitrary rescalings or translations of individual costs. Case studies on a toy example and the Zurich network demonstrate that identical tolling strategies can lead to substantially different efficiency estimates depending on the assumed comparability. Our framework thus demonstrates that explicit axiomatic foundations are necessary in order to define efficiency metrics and to appropriately guide policy in large-scale infrastructure design robustly and effectively."}
{"id": "2512.05525", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05525", "abs": "https://arxiv.org/abs/2512.05525", "authors": ["Nils Strassenburg", "Boris Glavic", "Tilmann Rabl"], "title": "Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement", "comment": null, "summary": "Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks."}
