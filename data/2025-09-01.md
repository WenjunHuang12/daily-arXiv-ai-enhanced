<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 2]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Faster Linear Algebra Algorithms with Structured Random Matrices](https://arxiv.org/abs/2508.21189)
*Chris Camaño,Ethan N. Epperly,Raphael A. Meyer,Joel A. Tropp*

Main category: cs.DS

TL;DR: 该论文提出了一个名为"无意识子空间注入(OSI)"的新框架，用于分析结构化随机矩阵在随机线性代数算法中的应用。该框架将算法分析分解为两部分：证明算法在OSI条件下有效，以及证明特定随机矩阵模型具有OSI性质。


<details>
  <summary>Details</summary>
Motivation: 尽管结构化随机矩阵在低秩近似和最小二乘回归等随机算法中被广泛使用，但其设计和分析的基本问题仍未完全解决。需要一个新的理论框架来统一分析各种结构化随机矩阵的性能。

Method: 提出OSI属性作为分析结构化降维的统一框架。OSI要求随机矩阵平均保持向量长度，并以高概率不湮灭低维子空间中的任何向量。然后分析标准随机算法在OSI假设下的性能，并验证多种随机矩阵模型（稀疏矩阵、随机三角变换、张量积结构）的OSI性质。

Result: 理论结果表明，使用结构化随机矩阵可以实现更快、接近最优的运行时间。实验证据显示结构化随机矩阵在合成问题和科学应用中表现出色。

Conclusion: OSI框架为结构化随机矩阵的分析提供了统一的理论基础，证明了多种结构化随机矩阵的有效性，并为实际实现提供了指导，推动了随机线性代数算法的发展。

Abstract: To achieve the greatest possible speed, practitioners regularly implement
randomized algorithms for low-rank approximation and least-squares regression
with structured dimension reduction maps. Despite significant research effort,
basic questions remain about the design and analysis of randomized linear
algebra algorithms that employ structured random matrices.
  This paper develops a new perspective on structured dimension reduction,
based on the oblivious subspace injection (OSI) property. The OSI property is a
relatively weak assumption on a random matrix that holds when the matrix
preserves the length of vectors on average and, with high probability, does not
annihilate any vector in a low-dimensional subspace. With the OSI abstraction,
the analysis of a randomized linear algebra algorithm factors into two parts:
(i) proving that the algorithm works when implemented with an OSI; and (ii)
proving that a given random matrix model has the OSI property.
  This paper develops both parts of the program. First, it analyzes standard
randomized algorithms for low-rank approximation and least-squares regression
under the OSI assumption. Second, it identifies many examples of OSIs,
including random sparse matrices, randomized trigonometric transforms, and
random matrices with tensor product structure. These theoretical results imply
faster, near-optimal runtimes for several fundamental linear algebra tasks. The
paper also provides guidance on implementation, along with empirical evidence
that structured random matrices offer exemplary performance for a range of
synthetic problems and contemporary scientific applications.

</details>


### [2] [$Δ$-Motif: Subgraph Isomorphism at Scale via Data-Centric](https://arxiv.org/abs/2508.21287)
*Yulun Wang,Esteban Ginez,Jamie Friel,Yuval Baum,Jin-Sung Kim,Alex Shih,Oded Green*

Main category: cs.DS

TL;DR: Δ-Motif是一种基于GPU加速的子图同构算法，通过将图转换为表格形式并使用数据库操作（连接、排序、合并、过滤）来替代传统的回溯算法，在GPU上实现高达595倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统子图同构算法（如VF2）存在顺序瓶颈，无法充分利用现代并行硬件。子图同构在生物网络分析、社交网络挖掘和量子电路优化等领域至关重要，需要高性能解决方案。

Method: 将数据和模式图转换为表格形式，将子图同构问题转化为数据库原语操作。将图分解为称为motif的小构建块，使用可扩展的关系操作系统地组合它们。利用NVIDIA RAPIDS和Pandas等成熟优化库实现大规模并行化。

Result: 在GPU上相比VF2等传统算法实现高达595倍的加速。成功应用于量子电路编译，解决了量子计算中的关键瓶颈，能够扩展到近中期设备。

Conclusion: 该方法通过熟悉的数据库抽象实现了高性能图处理，无需低级编程即可提供卓越的计算效率，使高性能图处理更加普及化。

Abstract: Subgraph isomorphism is a fundamental problem in graph analysis that seeks to
find all instances of a pattern graph within a larger data graph while
preserving structural relationships. This NP-complete problem is central to
domains such as biological network analysis, social network mining, and quantum
circuit optimization. Traditional approaches rely on backtracking algorithms
like VF2, which suffer from sequential bottlenecks that limit their ability to
exploit modern parallel hardware. In this work, we introduce $\Delta$-Motif, a
GPU-accelerated subgraph isomorphism algorithm that reformulates the task
through the lens of database operations. Our key insight is to represent both
data and pattern graphs in tabular form, turning subgraph isomorphism into
database primitives including joins, sorts, merges, and filters. $\Delta$-Motif
decomposes graphs into small building blocks called motifs and systematically
combines them using scalable relational operations. By leveraging mature,
optimized libraries from the NVIDIA RAPIDS ecosystem and Pandas framework, our
solution achieves massive parallelism while remaining portable across systems
supporting standard relational primitives. Benchmarks show that $\Delta$-Motif
outperforms established algorithms like VF2, achieving speedups of up to
$595\times$ on GPUs. We further demonstrate its impact by applying it to
quantum circuit compilation, addressing a critical bottleneck in quantum
computing and enabling scaling to near- and medium-term devices. Our approach
democratizes high-performance graph processing by exposing it through familiar
database abstractions, eliminating the need for low-level programming while
delivering exceptional computational efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 研究架构归纳偏置如何影响大语言模型在教学对话中的认知行为，通过符号脚手架和短期记忆机制提升苏格拉底式教学的适应性结构化推理能力


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型架构设计对其在教学对话中认知行为的影响，特别是如何通过架构设计来塑造模型的教学策略和推理能力

Method: 引入符号脚手架机制和短期记忆模式，通过五种系统变体的控制消融实验，使用专家设计的评分标准评估模型输出，并采用基于LLM的评估框架进行可扩展的系统比较

Result: 完整系统始终优于基线变体，移除记忆或符号结构会降低关键认知行为（抽象、自适应探测和概念连续性），架构脚手架能可靠地塑造LLMs中涌现的教学策略

Conclusion: 架构层面的脚手架能够可靠地塑造大语言模型中涌现的教学策略，支持处理层面的解释，表明架构设计对模型认知行为具有重要影响

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [4] [Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs](https://arxiv.org/abs/2508.21238)
*Tingxuan Xu,Jiarui Feng,Justin Melendez,Kaleigh Roberts,Donghong Cai,Mingfang Zhu,Donald Elbert,Yixin Chen,Randall J. Bateman*

Main category: cs.AI

TL;DR: 本文评估了GraphRAG在阿尔茨海默病领域的应用效果，通过构建专业知识库和专家问题集，比较了GraphRAG与标准GPT-4o的回答质量和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学研究中的应用受到幻觉、领域知识有限和缺乏可解释性等限制，GraphRAG通过整合领域特定上下文信息来提高可靠性，但在生物医学等专业领域的评估研究仍然有限。

Method: 收集50篇论文和70个专家问题构建阿尔茨海默病知识库，使用GPT-4o作为LLM，比较GraphRAG与标准GPT-4o的回答质量，并评估不同RAG系统的可追溯性。

Result: 研究评估了两种流行GraphRAG系统的性能，提供了质量对比和可追溯性分析结果。

Conclusion: 研究为研究人员提供了易于使用的界面和预构建的阿尔茨海默病数据库，用于测试标准RAG和GraphRAG的性能，推动了GraphRAG在专业领域的应用评估。

Abstract: In the past two years, large language model (LLM)-based chatbots, such as
ChatGPT, have revolutionized various domains by enabling diverse task
completion and question-answering capabilities. However, their application in
scientific research remains constrained by challenges such as hallucinations,
limited domain-specific knowledge, and lack of explainability or traceability
for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has
emerged as a promising approach to improving chatbot reliability by integrating
domain-specific contextual information before response generation, addressing
some limitations of standard LLMs. Despite its potential, there are only
limited studies that evaluate GraphRAG on specific domains that require
intensive knowledge, like Alzheimer's disease or other biomedical domains. In
this paper, we assess the quality and traceability of two popular GraphRAG
systems. We compile a database of 50 papers and 70 expert questions related to
Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as
the LLM for answering queries. We then compare the quality of responses
generated by GraphRAG with those from a standard GPT-4o model. Additionally, we
discuss and evaluate the traceability of several Retrieval-Augmented Generation
(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a
pre-built Alzheimer's disease database for researchers to test the performance
of both standard RAG and GraphRAG.

</details>


### [5] [MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems](https://arxiv.org/abs/2508.21307)
*Sri Ram Macharla,Sridhar Murthy J,Anjaneyulu Pasala*

Main category: cs.AI

TL;DR: MultiFluxAI是一个创新的AI平台，用于解决产品工程中管理和集成大量不同数据源的挑战，通过生成式AI、向量化和智能编排技术提供动态的上下文感知响应。


<details>
  <summary>Details</summary>
Motivation: 解决产品工程领域中管理和集成大量不同数据源的挑战，增强数字生态系统中用户参与度，处理当前和新的服务相关查询。

Method: 利用先进的AI技术，包括生成式AI、向量化和智能编排（agentic orchestration），提供动态和上下文感知的复杂用户查询响应。

Result: 开发了一个能够处理多样化数据源并提供智能响应的AI平台，增强了用户参与度和查询处理能力。

Conclusion: MultiFluxAI平台通过集成多种AI技术，有效解决了产品工程中的数据管理挑战，提升了用户体验和系统响应能力。

Abstract: MultiFluxAI is an innovative AI platform developed to address the challenges
of managing and integrating vast, disparate data sources in product engineering
across application domains. It addresses both current and new service related
queries that enhance user engagement in the digital ecosystem. This platform
leverages advanced AI techniques, such as Generative AI, vectorization, and
agentic orchestration to provide dynamic and context-aware responses to complex
user queries.

</details>


### [6] [Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation](https://arxiv.org/abs/2508.21320)
*Mohsen Nayebi Kerdabadi,Arya Hadizadeh Moghaddam,Dongjie Wang,Zijun Yao*

Main category: cs.AI

TL;DR: LINKO是一个基于大语言模型的集成本体学习框架，通过同时利用多个医学本体图，在异构本体系统内和跨系统进行双轴知识传播，以增强医学概念表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有的医学本体学习方法主要关注单一本体系统或多个孤立的本体系统，缺乏跨本体连接的统一学习结构，导致概念表示学习局限于本体内部关系。

Method: 1) 使用LLM通过工程化提示（包含概念描述和本体上下文）为ontology概念嵌入提供图检索增强初始化；2) 通过双轴知识传播联合学习多个本体图中的医学概念：本体内部垂直传播和本体间水平传播；3) 作为插件编码器与现有EHR预测模型兼容。

Result: 在两个公共数据集上的广泛实验验证了LINKO相对于最先进基线的优越性能，在数据有限和罕见疾病预测场景中表现出更强的鲁棒性。

Conclusion: LINKO框架通过整合多个异构本体系统和跨本体知识传播，显著提升了医学概念表示学习的效果，特别是在数据稀缺和罕见疾病预测方面表现出色。

Abstract: Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.

</details>


### [7] [Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2508.21365)
*Yi Liao,Yu Gu,Yuan Sui,Zining Zhu,Yifan Lu,Guohua Tang,Zhongqian Sun,Wei Yang*

Main category: cs.AI

TL;DR: TiG框架通过将强化学习决策重新构建为语言建模任务，让大语言模型在游戏环境中直接交互学习程序性知识，同时保持其推理和解释能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂推理任务表现出色但在简单交互任务中表现不佳的问题，弥合陈述性知识和程序性知识之间的差距。

Method: 将基于强化学习的决策重新表述为语言建模任务：LLMs生成语言指导的策略，通过在线强化学习基于环境反馈进行迭代优化。

Result: TiG成功弥合了陈述性和程序性知识之间的差距，以显著更低的数据和计算需求实现了与传统RL方法相竞争的性能，并提供逐步自然语言解释。

Conclusion: TiG框架有效结合了LLMs的世界知识和推理能力与RL的环境交互学习，在保持透明度和可解释性的同时提升了交互决策能力。

Abstract: Large language models (LLMs) excel at complex reasoning tasks such as
mathematics and coding, yet they frequently struggle with simple interactive
tasks that young children perform effortlessly. This discrepancy highlights a
critical gap between declarative knowledge (knowing about something) and
procedural knowledge (knowing how to do something). Although traditional
reinforcement learning (RL) agents can acquire procedural knowledge through
environmental interaction, they often operate as black boxes and require
substantial training data. In contrast, LLMs possess extensive world knowledge
and reasoning capabilities, but are unable to effectively convert this static
knowledge into dynamic decision-making in interactive settings. To address this
challenge, we propose Think in Games (TiG), a novel framework that empowers
LLMs to develop procedural understanding through direct interaction with game
environments, while retaining their inherent reasoning and explanatory
abilities. Specifically, TiG reformulates RL-based decision-making as a
language modeling task: LLMs generate language-guided policies, which are
refined iteratively through online reinforcement learning based on
environmental feedback. Our experimental results show that TiG successfully
bridges the gap between declarative and procedural knowledge, achieving
competitive performance with dramatically lower data and computational demands
compared to conventional RL methods. Moreover, TiG provides step-by-step
natural language explanations for its decisions, greatly improving transparency
and interpretability in complex interactive tasks.

</details>


### [8] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM是一个新的音频-语言模型基准测试，集成了10个评估维度，包括感知、推理、公平性等，测试了14个模型，发现Gemini 2.5 Pro在多数方面表现最佳但存在群体不公平问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频-语言模型评估缺乏标准化基准，大多只测试一两个能力，且不同评估使用不同的提示方法和推理参数，难以进行模型间公平比较。

Method: 开发AHELM基准，整合多个数据集（包括新的PARADE和CoRe-Bench合成数据集），标准化提示、推理参数和评估指标，测试14个开源和闭源ALM模型以及3个基线系统。

Result: Gemini 2.5 Pro在10个维度中的5个排名第一，但在ASR任务中表现出群体不公平性（p=0.01）。基线系统表现良好，其中一个仅具备语音转文本能力的系统总体排名第5。

Conclusion: AHELM提供了全面的ALM评估框架，揭示了现有模型的优势和不足，特别是公平性问题，基准将持续更新以促进音频-语言模型的发展。

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


### [9] [AI Compute Architecture and Evolution Trends](https://arxiv.org/abs/2508.21394)
*Bor-Sung Liang*

Main category: cs.AI

TL;DR: 本文提出了AI计算的七层架构模型，从物理层到应用层，分析了AI发展的技术挑战和机遇，并探讨了从单AI智能体到AI生态系统的演进路径及经济可持续性问题。


<details>
  <summary>Details</summary>
Motivation: AI发展已从学术研究转向实际应用，但在各层面面临诸多挑战。文章旨在通过结构化方法分析AI的机遇与挑战，特别是计算架构的演进和生态系统建设问题。

Method: 提出七层AI计算架构模型（物理层、链路层、神经网络层、上下文层、智能体层、编排层、应用层），分析每层的发展轨迹和关键技术，并借鉴互联网产业发展经验进行预测。

Result: 建立了完整的AI计算架构框架，揭示了Scale-Up和Scale-Out策略对计算架构的影响，探讨了LLMs的不同发展路径，分析了上下文记忆的作用，以及从单智能体到生态系统的演进趋势。

Conclusion: AI发展不仅需要解决技术挑战，还需构建经济上自持续的生态系统。七层架构模型为理解AI发展提供了系统框架，互联网产业的经验可为AI未来发展提供重要参考。

Abstract: The focus of AI development has shifted from academic research to practical
applications. However, AI development faces numerous challenges at various
levels. This article will attempt to analyze the opportunities and challenges
of AI from several different perspectives using a structured approach. This
article proposes a seven-layer model for AI compute architecture, including
Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,
Orchestrator Layer, and Application Layer, from bottom to top. It also explains
how AI computing has evolved into this 7-layer architecture through the
three-stage evolution on large-scale language models (LLMs). For each layer, we
describe the development trajectory and key technologies. In Layers 1 and 2 we
discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies
on computing architecture. In Layer 3 we explore two different development
paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs
and compares it to traditional processor memory. In Layers 5 to 7 we discuss
the trends of AI agents and explore the issues in evolution from a single AI
agent to an AI-based ecosystem, and their impact on the AI industry.
Furthermore, AI development involves not only technical challenges but also the
economic issues to build self-sustainable ecosystem. This article analyzes the
internet industry to provide predictions on the future trajectory of AI
development.

</details>


### [10] [CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN](https://arxiv.org/abs/2508.21411)
*Leonard Frank Neis,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: CARJAN是一个基于AJAN多智能体框架和CARLA驾驶模拟器的半自动化交通场景生成与仿真工具，提供可视化界面和智能决策功能


<details>
  <summary>Details</summary>
Motivation: 城市交通场景中不同类型交互智能体（行人、骑行者、自动驾驶车辆）的建模和虚拟仿真仍然是一个挑战，需要更用户友好的工具

Method: 基于AJAN多智能体工程框架和CARLA驾驶模拟器，提供可视化用户界面，采用SPARQL行为树进行智能体决策和交互

Result: 开发了CARJAN工具，实现了交通场景布局的建模、存储和维护，支持动态场景仿真中的智能体交互

Conclusion: CARJAN为CARLA中的交互式、智能体驱动的虚拟交通场景生成和仿真提供了首个集成方法

Abstract: User-friendly modeling and virtual simulation of urban traffic scenarios with
different types of interacting agents such as pedestrians, cyclists and
autonomous vehicles remains a challenge. We present CARJAN, a novel tool for
semi-automated generation and simulation of such scenarios based on the
multi-agent engineering framework AJAN and the driving simulator CARLA. CARJAN
provides a visual user interface for the modeling, storage and maintenance of
traffic scenario layouts, and leverages SPARQL Behavior Tree-based
decision-making and interactions for agents in dynamic scenario simulations in
CARLA. CARJAN provides a first integrated approach for interactive, intelligent
agent-based generation and simulation of virtual traffic scenarios in CARLA.

</details>


### [11] [A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions](https://arxiv.org/abs/2508.21441)
*Christoph Beierle,Alexander Hahn,Diana Howey,Gabriele Kern-Isberner,Kai Sauerwald*

Main category: cs.AI

TL;DR: 本文从认知视角研究知识管理中的遗忘操作，在Spohn排序函数的认知状态下提出了5种通用类型和7种具体遗忘操作，并通过逻辑编程和AGM理论的公理进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 传统遗忘操作主要基于经典逻辑，如变量消除和AGM收缩，但缺乏对认知状态下遗忘操作的深入研究。本文旨在从认知角度探索具有丰富语义结构的遗忘操作。

Method: 采用认知状态视角，提出5种通用类型的认知遗忘操作，并在Spohn排序函数中实例化7种具体操作。借鉴逻辑编程和AGM理论的公理建立评估框架。

Result: 系统评估了所有具体遗忘操作，提供了全面的比较分析，突出了不同遗忘算子之间的差异和共同点。

Conclusion: 研究为认知状态下的遗忘操作提供了系统的理论框架和评估方法，扩展了传统逻辑基础上的遗忘理论研究。

Abstract: Forgetting as a knowledge management operation deliberately ignores parts of
the knowledge and beliefs of an agent, for various reasons. Forgetting has many
facets, one may want to forget parts of the syntax, a proposition, or a
conditional. In the literature, two main operators suitable for performing
forgetting have been proposed and investigated in depth: First, variable
elimination is a syntactical method that blends out certain atomic variables to
focus on the rest of the language. It has been mainly used in the area of logic
programming and answer set programming. Second, contraction in AGM belief
revision theory effectively removes propositions from belief sets under logical
deduction. Both operations rely mainly on classical logics. In this article, we
take an epistemic perspective and study forgetting operations in epistemic
states with richer semantic structures, but with clear links to propositional
logic. This allows us to investigate what forgetting in the epistemic
background means, thereby lifting well-known and novel forgetting operations to
the epistemic level. We present five general types of epistemic forgetting and
instantiate them with seven concrete forgetting operations for Spohn's ranking
functions. We take inspiration from postulates of forgetting both from logic
programming and AGM theory to propose a rich landscape of axioms for evaluating
forgetting operations. Finally, we evaluate all concrete forgetting operations
according to all postulates, leading to a novel comprehensive overview
highlighting differences and commonalities among the forgetting operators.

</details>


### [12] [Learning Lifted Action Models From Traces of Incomplete Actions and States](https://arxiv.org/abs/2508.21449)
*Niklas Jansen,Jonas Gösgens,Hector Geffner*

Main category: cs.AI

TL;DR: 学习滑动拼图游戏的lifted STRIPS模型，从仅包含瓦片位置的状态和简单动作标签（上、下、左、右）的随机轨迹中学习，无需完整STRIPS状态和动作参数。


<details>
  <summary>Details</summary>
Motivation: 解决从更现实的环境中学习模型的问题，其中观察到的状态不是完整的STRIPS状态（缺少某些谓词如空白位置），动作也不包含所有需要的参数。

Method: 提出STRIPS+变体，允许动作参数在前提条件中隐式存在并支持有限存在量化。开发SYNTH算法，为每个动作构建分层的前提条件表达式序列来唯一标识对象并隐式地接地动作参数。

Result: 建立了SYNTH算法的正确性和完备性，并在从现有STRIPS领域衍生的STRIPS+模型生成的状态-动作轨迹上测试了其可扩展性。

Conclusion: 该方法能够从部分可观察的状态和参数不完整的动作中有效学习lifted STRIPS模型，为更现实的模型学习场景提供了解决方案。

Abstract: Consider the problem of learning a lifted STRIPS model of the sliding-tile
puzzle from random state-action traces where the states represent the location
of the tiles only, and the actions are the labels up, down, left, and right,
with no arguments. Two challenges are involved in this problem. First, the
states are not full STRIPS states, as some predicates are missing, like the
atoms representing the position of the ``blank''. Second, the actions are not
full STRIPS either, as they do not reveal all the objects involved in the
actions effects and preconditions. Previous approaches have addressed different
versions of this model learning problem, but most assume that actions in the
traces are full STRIPS actions or that the domain predicates are all
observable. The new setting considered in this work is more ``realistic'', as
the atoms observed convey the state of the world but not full STRIPS states,
and the actions reveal the arguments needed for selecting the action but not
the ones needed for modeling it in STRIPS. For formulating and addressing the
learning problem, we introduce a variant of STRIPS, which we call STRIPS+,
where certain STRIPS action arguments can be left implicit in preconditions
which can also involve a limited form of existential quantification. The
learning problem becomes the problem of learning STRIPS+ models from STRIPS+
state-action traces. For this, the proposed learning algorithm, called SYNTH,
constructs a stratified sequence (conjunction) of precondition expressions or
``queries'' for each action, that denote unique objects in the state and ground
the implicit action arguments in STRIPS+. The correctness and completeness of
SYNTH is established, and its scalability is tested on state-action traces
obtained from STRIPS+ models derived from existing STRIPS domains.

</details>


### [13] [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.21475)
*Xijia Tao,Yihua Teng,Xinxing Su,Xinyu Fu,Jihao Wu,Chaofan Tao,Ziru Liu,Haoli Bai,Rui Liu,Lingpeng Kong*

Main category: cs.AI

TL;DR: MMSearch-Plus是一个包含311个任务的基准测试，专门设计来评估多模态语言模型在需要深度视觉推理、来源验证和长时程工具使用的复杂网络浏览任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态浏览基准测试往往可以通过简单的固定工作流程解决，无法真正测试模型在细粒度视觉推理、来源验证和长时程工具使用方面的多模态能力。

Method: 采用空间-时间外推法构建任务，每个任务包含多个弱局部视觉信号，需要通过迭代的文本-图像搜索进行提取和传播，并在检索噪声下进行交叉验证。提供了一个模型无关的代理框架和浏览工具。

Result: 最强代理(o3)在没有搜索的情况下准确率为15.1%，在框架下进行搜索后达到36.0%。最强的开源模型(Qwen-2.5-VL-72B-Instruct)在没有搜索时为0.0%，经过20轮搜索后达到6.9%。

Conclusion: 该基准测试揭示了当前多模态语言模型在来源验证、基于部件的推理和长时程规划方面的失败，为未来模型开发提供了重要的评估标准。

Abstract: Large multimodal language models (MLLMs) are increasingly deployed as web
agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed
workflows that lean on high-recall image search and nearby text-masking the
genuinely multimodal challenges of fine-grained visual reasoning, provenance
verification, and long-horizon tool use. We introduce MMSearch-Plus, a
benchmark of 311 tasks that highly demand multimodal understanding while
preserving the difficulty profile of strong text-only browsing suites. Each
item is constructed to contain multiple weak, localized visual signals that
must be extracted, propagated through iterative text-image search, and
cross-validated under retrieval noise before answering. Our curation procedure,
Spatial-Temporal Extrapolation, seeds questions whose answers require
extrapolating from spatial cues (micro-text, part-level appearance, layouts,
signage) and temporal traces (broadcast overlays, seasonal context) to
out-of-image facts such as events, dates, and venues. We provide a
model-agnostic agent framework with browsing tools and evaluate a range of
closed and open MLLMs. The strongest agent (o3) attains 15.1% without search
and 36.0% accuracy with rollout under our framework, while a strong open-source
model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20
rounds of search. Beyond answer accuracy, we assess bounding-box production and
cropped-image search, and conduct an error analysis that surfaces failures in
source verification, part-based reasoning, and long-horizon planning.

</details>


### [14] [Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis](https://arxiv.org/abs/2508.21517)
*Sweta Kaman,Ankita Sharma,Romi Banerjee*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于Z数的模糊推理系统，用于计算智慧评分和信心度，充分考虑智慧的多维性和不确定性特征。


<details>
  <summary>Details</summary>
Motivation: 现有智慧测量依赖自我报告，缺乏对谦虚和不确定性的考虑，需要一种能够同时考虑多维度和信心度的计算框架来改善心理学测量和人道AI。

Method: 使用Z数模糊推理系统，将智慧表达为智慧分数（限制）和信心度分数（确定性）。通过100名参与者的语言响应，映射到智慧的5个组成部分，使用21条规则和高斯核密度估计进行分数结合。

Result: 证明概念研究显示，该系统产生的双属性智慧表征与现有量表显示中等但显著的相关性，但与无关特质几乎无关，支持了聚合效度和区别效度。

Conclusion: 该研究将智慧形式化为一种多维度、关注不确定性的构念，不仅推进了心理学测量，还为AI系统提供了可解释、信心敏感的推理能力，在严格计算与人类判断之间找到安全的中间地带。

Abstract: Background: Wisdom is a superordinate construct that embraces perspective
taking, reflectiveness, prosocial orientation, reflective empathetic action,
and intellectual humility. Unlike conventional models of reasoning that are
rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,
requiring both graded evaluation and self-reflective humility. Current measures
depend on self-reports and seldom reflect the humility and uncertainty inherent
in wise reasoning. A computational framework that takes into account both
multidimensionality and confidence has the potential to improve psychological
science and allow humane AI. Method: We present a fuzzy inference system with Z
numbers, each of the decisions being expressed in terms of a wisdom score
(restriction) and confidence score (certainty). As part of this study,
participants (N = 100) were exposed to culturally neutral pictorial moral
dilemma tasks to which they generated think-aloud linguistic responses, which
were mapped into five theoretically based components of wisdom. The scores of
each individual component were combined using a base of 21 rules, with
membership functions tuned via Gaussian kernel density estimation. Results: In
a proof of concept study, the system produced dual attribute wisdom
representations that correlated modestly but significantly with established
scales while showing negligible relations with unrelated traits, supporting
convergent and divergent validity. Contribution: The contribution is to
formalize wisdom as a multidimensional, uncertainty-conscious construct,
operationalized in the form of Z-numbers. In addition to progressing
measurement in psychology, it calculates how fuzzy Z numbers can provide AI
systems with interpretable, confidence-sensitive reasoning that affords a safe,
middle ground between rigorous computation and human-like judgment.

</details>


### [15] [Counterfactual Scenarios for Automated Planning](https://arxiv.org/abs/2508.21521)
*Nicola Gigante,Francesco Leofante,Andrea Micheli*

Main category: cs.AI

TL;DR: 该论文提出了一种基于反事实场景的新解释范式，通过对规划问题进行最小修改来产生满足特定性质的计划，以充体传统反事实解释的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的反事实解释只关注对现有计划的最小修改，而无法抓住问题本身的高层次特性。需要一种能够揭示问题本质属性的解释方法。

Method: 提出两种定性的反事实场景实例化方法，通过明确量化必须满足特定属性的计划。分析了允许不同类型变更时生成反事实场景的计算复杂度。

Result: 证明了生成反事实场景的计算成本通常仅与为规划问题计算计划的成本相当，显示了该方法的实际可行性。

Conclusion: 该研究为规划领域提供了一个实用的反事实解释框架，能够有效地揭示问题的根本特性，为该领域的算法开发奠定了基础。

Abstract: Counterfactual Explanations (CEs) are a powerful technique used to explain
Machine Learning models by showing how the input to a model should be minimally
changed for the model to produce a different output. Similar proposals have
been made in the context of Automated Planning, where CEs have been
characterised in terms of minimal modifications to an existing plan that would
result in the satisfaction of a different goal. While such explanations may
help diagnose faults and reason about the characteristics of a plan, they fail
to capture higher-level properties of the problem being solved. To address this
limitation, we propose a novel explanation paradigm that is based on
counterfactual scenarios. In particular, given a planning problem $P$ and an
\ltlf formula $\psi$ defining desired properties of a plan, counterfactual
scenarios identify minimal modifications to $P$ such that it admits plans that
comply with $\psi$. In this paper, we present two qualitative instantiations of
counterfactual scenarios based on an explicit quantification over plans that
must satisfy $\psi$. We then characterise the computational complexity of
generating such counterfactual scenarios when different types of changes are
allowed on $P$. We show that producing counterfactual scenarios is often only
as expensive as computing a plan for $P$, thus demonstrating the practical
viability of our proposal and ultimately providing a framework to construct
practical algorithms in this area.

</details>


### [16] [HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining](https://arxiv.org/abs/2508.21540)
*Eduardo Illueca-Fernandez,Kaile Chen,Fernando Seoane,Farhad Abtahi*

Main category: cs.AI

TL;DR: 这篇论文提出了HealthProcessAI框架，通过整合多个大语言模型，自动解释生成医疗流程挖掘报告，解决了技术复杂性和访问性问题。


<details>
  <summary>Details</summary>
Motivation: 医疗流程挖掘应用面临技术复杂、缺乏标准化方法和训练资源不足等挑战，需要提高可访问性和易用性。

Method: 开发HealthProcessAI框架，包装Python(PM4PY)和R(bupaR)库，集成多个LLM模型进行自动流程图解释和报告生成，使用肺热恐症数据验证。

Result: 框架在4个实验场景中成功处理肺热恐数据，Claude Sonnet-4和Gemini 2.5-Pro获得最高一致性分数(3.79/4.0和3.65/4.0)。

Conclusion: 该框架通过AI驱动的自动解释，将复杂的流程挖掘结果转换为可执行的医疗见解，为医疗领域提供了新的方法进展。

Abstract: Process mining has emerged as a powerful analytical technique for
understanding complex healthcare workflows. However, its application faces
significant barriers, including technical complexity, a lack of standardized
approaches, and limited access to practical training resources. We introduce
HealthProcessAI, a GenAI framework designed to simplify process mining
applications in healthcare and epidemiology by providing a comprehensive
wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address
unfamiliarity and improve accessibility, the framework integrates multiple
Large Language Models (LLMs) for automated process map interpretation and
report generation, helping translate technical analyses into outputs that
diverse users can readily understand. We validated the framework using sepsis
progression data as a proof-of-concept example and compared the outputs of five
state-of-the-art LLM models through the OpenRouter platform. To test its
functionality, the framework successfully processed sepsis data across four
proof-of-concept scenarios, demonstrating robust technical performance and its
capability to generate reports through automated LLM analysis. LLM evaluation
using five independent LLMs as automated evaluators revealed distinct model
strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency
scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By
integrating multiple Large Language Models (LLMs) for automated interpretation
and report generation, the framework addresses widespread unfamiliarity with
process mining outputs, making them more accessible to clinicians, data
scientists, and researchers. This structured analytics and AI-driven
interpretation combination represents a novel methodological advance in
translating complex process mining results into potentially actionable insights
for healthcare applications.

</details>


### [17] [Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances](https://arxiv.org/abs/2508.21564)
*Issa Hanou,Sebastijan Dumančić,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: \u63d0\u51fa\u4e00\u79cd\u53d1\u73b0\u8de8\u57df\u901a\u7528\u91ce\u6807\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u89e3\u51b3\u8fc7\u7684\u5b9e\u4f8b\u81ea\u52a8\u63a8\u5e7f\u5230\u65b0\u95ee\u9898\uff0c\u4f7f\u7528\u72ec\u7acb\u4e8e\u5177\u4f53\u5bf9\u8c61\u7684\u72b6\u6001\u51fd\u6570\u6765\u63cf\u8ff0\u4e2d\u95f4\u76ee\u6807\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u91ce\u6807\u63d0\u53d6\u7b97\u6cd5\u5728\u67d0\u4e9b\u89c4\u5212\u95ee\u9898\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u901a\u7528\u5230\u6574\u4e2a\u57df\u7684\u91ce\u6807\u53d1\u73b0\u65b9\u6cd5\u3002

Method: \u901a\u8fc7\u72b6\u6001\u51fd\u6570\u6784\u5efa\u6709\u5411\u901a\u7528\u91ce\u6807\u56fe\uff0c\u5305\u542b\u5faa\u73af\u53ef\u80fd\u6027\u4ee5\u5904\u7406\u91cd\u590d\u5b50\u8ba1\u5212\uff0c\u5e76\u5c06\u8be5\u56fe\u7528\u4e8e\u89e3\u51b3\u65b0\u95ee\u9898\u5b9e\u4f8b\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4ece\u5c11\u91cf\u5c0f\u578b\u5b9e\u4f8b\u5b66\u4e60\u7684\u901a\u7528\u91ce\u6807\u56fe\u5728\u540c\u57df\u7684\u66f4\u5927\u5b9e\u4f8b\u4e2d\u540c\u6837\u6709\u6548\uff0c\u5f53\u8bc6\u522b\u51fa\u8868\u793a\u91cd\u590d\u7684\u5faa\u73af\u65f6\uff0c\u542f\u53d\u51fd\u6570\u6027\u80fd\u6bd4\u57fa\u51c6\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002

Conclusion: \u901a\u7528\u91ce\u6807\u80fd\u591f\u6350\u63cf\u57df\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u89e3\u91ca\u4e14\u5bf9\u81ea\u52a8\u89c4\u5212\u5668\u6709\u7528\uff0c\u53ea\u9700\u4ece\u540c\u4e00\u57df\u7684\u5c11\u91cf\u8ba1\u5212\u4e2d\u53d1\u73b0\u3002

Abstract: We propose a new framework for discovering landmarks that automatically
generalize across a domain. These generalized landmarks are learned from a set
of solved instances and describe intermediate goals for planning problems where
traditional landmark extraction algorithms fall short. Our generalized
landmarks extend beyond the predicates of a domain by using state functions
that are independent of the objects of a specific problem and apply to all
similar objects, thus capturing repetition. Based on these functions, we
construct a directed generalized landmark graph that defines the landmark
progression, including loop possibilities for repetitive subplans. We show how
to use this graph in a heuristic to solve new problem instances of the same
domain. Our results show that the generalized landmark graphs learned from a
few small instances are also effective for larger instances in the same domain.
If a loop that indicates repetition is identified, we see a significant
improvement in heuristic performance over the baseline. Generalized landmarks
capture domain information that is interpretable and useful to an automated
planner. This information can be discovered from a small set of plans for the
same domain.

</details>


### [18] [Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics](https://arxiv.org/abs/2508.21595)
*Yang You,Alex Schutz,Zhikun Li,Bruno Lacerda,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: 本文提出了确定性分布式部分可观测马尔可夫决策过程（Det-Dec-POMDPs）模型和一种可扩展的解法IDPP，专门处理大规模多机器人导航等高级多代理规划问题。


<details>
  <summary>Details</summary>
Motivation: 当前Dec-POMDP解法在处理大规模确定性多代理规划问题时效率低下，需要专门优化的方法来解决这些问题。

Method: 提出Iterative Deterministic POMDP Planning (IDPP)方法，基于经典Joint Equilibrium Search for Policies框架，专门优化处理大规模Det-Dec-POMDPs。

Result: IDPP方法能够有效处理当前Dec-POMDP解法无法高效处理的大规模确定性多代理规划问题。

Conclusion: 该研究为确定性多代理规划问题提供了一种可扩展的解决方案，IDPP方法在处理大规模问题时显示出良好的性能。

Abstract: Many high-level multi-agent planning problems, including multi-robot
navigation and path planning, can be effectively modeled using deterministic
actions and observations.
  In this work, we focus on such domains and introduce the class of
Deterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of
Dec-POMDPs characterized by deterministic transitions and observations
conditioned on the state and joint actions.
  We then propose a practical solver called Iterative Deterministic POMDP
Planning (IDPP). This method builds on the classic Joint Equilibrium Search for
Policies framework and is specifically optimized to handle large-scale
Det-Dec-POMDPs that current Dec-POMDP solvers are unable to address
efficiently.

</details>


### [19] [Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study](https://arxiv.org/abs/2508.21622)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 结合传统网络优化模型与大型语言模型的集成框架，为供应链规划提供交互式、可解释和角色感知的决策支持系统


<details>
  <summary>Details</summary>
Motivation: 解决复杂运筹学输出与业务利益相关者理解之间的差距，通过自然语言摘要、情境可视化和定制化KPI来弥合这一鸿沟

Method: 采用混合整数规划构建多周期多物品的战术库存再分配优化模型，结合AI代理、RESTful API和动态用户界面的技术架构

Result: 案例研究表明系统能够预防缺货、降低成本并维持服务水平，显著改善规划结果

Conclusion: 未来将集成私有LLM、迁移学习、强化学习和贝叶斯神经网络，以增强可解释性、适应性和实时决策能力

Abstract: This paper presents an integrated framework that combines traditional network
optimization models with large language models (LLMs) to deliver interactive,
explainable, and role-aware decision support for supply chain planning. The
proposed system bridges the gap between complex operations research outputs and
business stakeholder understanding by generating natural language summaries,
contextual visualizations, and tailored key performance indicators (KPIs). The
core optimization model addresses tactical inventory redistribution across a
network of distribution centers for multi-period and multi-item, using a
mixed-integer formulation. The technical architecture incorporates AI agents,
RESTful APIs, and a dynamic user interface to support real-time interaction,
configuration updates, and simulation-based insights. A case study demonstrates
how the system improves planning outcomes by preventing stockouts, reducing
costs, and maintaining service levels. Future extensions include integrating
private LLMs, transfer learning, reinforcement learning, and Bayesian neural
networks to enhance explainability, adaptability, and real-time
decision-making.

</details>


### [20] [A-MHA*: Anytime Multi-Heuristic A*](https://arxiv.org/abs/2508.21637)
*Ramkumar Natarajan,Muhammad Suhail Saleem,William Xiao,Sandip Aine,Howie Choset,Maxim Likhachev*

Main category: cs.AI

TL;DR: 本文提出了A-MHA*算法，将多启发式A*（MHA*）扩展为随时算法，能够快速找到可行解并持续改进，直到时间耗尽。


<details>
  <summary>Details</summary>
Motivation: MHA*虽然利用多个不可采纳启发式函数来加速搜索，但它是单次算法，无法随时间改进解质量，需要仔细设置膨胀因子。

Method: 受ARA*算法启发，将ARA*的概念精确适配到MHA*框架中，开发了随时版本A-MHA*，保持原有的次优性和完备性保证。

Result: 在3D路径规划和滑块拼图领域测试表明，A-MHA*相比MHA*和其他随时算法具有更好的性能。

Conclusion: A-MHA*成功地将MHA*扩展为随时算法，能够在保持理论保证的同时持续改进解质量，为实际应用提供了更灵活的解决方案。

Abstract: Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.

</details>


### [21] [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](https://arxiv.org/abs/2508.21648)
*Farhad Abtahi,Mehdi Astaraki,Fernando Seoane*

Main category: cs.AI

TL;DR: MEDLEY框架通过保留多个AI模型的多样性输出而非追求共识，将模型偏见视为潜在优势，幻觉视为待验证假设，为医疗AI提供透明化诊断不确定性


<details>
  <summary>Details</summary>
Motivation: 传统观点认为AI偏见是需要消除的缺陷，但人类推理本身就包含教育、文化和经验塑造的偏见，这些偏见可能不可避免且有价值

Method: 开发MEDLEY概念框架，协调30多个大语言模型，保留共识和少数观点，将模型特定偏见记录为优势，幻觉视为临床医生验证的临时假设

Result: 概念验证演示器在合成病例中成功保留了共识和少数观点，使诊断不确定性和潜在偏见对临床监督透明化

Conclusion: MEDLEY通过将AI不完美重新定义为资源，为开发可信医疗AI系统开辟了新的监管、伦理和创新途径，在临床医生监督下增强医疗推理

Abstract: Bias in medical artificial intelligence is conventionally viewed as a defect
requiring elimination. However, human reasoning inherently incorporates biases
shaped by education, culture, and experience, suggesting their presence may be
inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble
Diagnostic system with Leveraged diversitY), a conceptual framework that
orchestrates multiple AI models while preserving their diverse outputs rather
than collapsing them into a consensus. Unlike traditional approaches that
suppress disagreement, MEDLEY documents model-specific biases as potential
strengths and treats hallucinations as provisional hypotheses for clinician
verification. A proof-of-concept demonstrator was developed using over 30 large
language models, creating a minimum viable product that preserved both
consensus and minority views in synthetic cases, making diagnostic uncertainty
and latent biases transparent for clinical oversight. While not yet a validated
clinical tool, the demonstration illustrates how structured diversity can
enhance medical reasoning under clinician supervision. By reframing AI
imperfection as a resource, MEDLEY offers a paradigm shift that opens new
regulatory, ethical, and innovation pathways for developing trustworthy medical
AI systems.

</details>


### [22] [PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation](https://arxiv.org/abs/2508.21720)
*Jiho Choi,Seojeong Park,Seongjong Song,Hyunjung Shim*

Main category: cs.AI

TL;DR: PosterForest是一个无需训练的科学海报生成框架，通过分层中间表示和多智能体协作，实现了文本和视觉元素的语义整合与结构优化


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽视科学文档的分层结构和文本视觉元素的语义整合，需要同时解决这两个挑战

Method: 引入Poster Tree分层中间表示，采用多智能体协作策略（内容摘要和布局规划智能体），通过迭代协调和相互反馈实现联合优化

Result: 在多个学术领域的实验中，该方法在定性和定量评估上均优于现有基线，生成的海报质量最接近专家设计，具有更好的信息保存、结构清晰度和用户偏好

Conclusion: PosterForest框架通过分层表示和多智能体协作，有效解决了科学海报生成中的结构和语义整合问题，取得了优异性能

Abstract: We present a novel training-free framework, \textit{PosterForest}, for
automated scientific poster generation. Unlike prior approaches, which largely
neglect the hierarchical structure of scientific documents and the semantic
integration of textual and visual elements, our method addresses both
challenges directly. We introduce the \textit{Poster Tree}, a hierarchical
intermediate representation that jointly encodes document structure and
visual-textual relationships at multiple levels. Our framework employs a
multi-agent collaboration strategy, where agents specializing in content
summarization and layout planning iteratively coordinate and provide mutual
feedback. This approach enables the joint optimization of logical consistency,
content fidelity, and visual coherence. Extensive experiments on multiple
academic domains show that our method outperforms existing baselines in both
qualitative and quantitative evaluations. The resulting posters achieve quality
closest to expert-designed ground truth and deliver superior information
preservation, structural clarity, and user preference.

</details>


### [23] [Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem](https://arxiv.org/abs/2508.21730)
*Fabrizio Fagiolo,Nicolo' Vescera*

Main category: cs.AI

TL;DR: 提出了一种用于旅行商问题的变分量子算法，通过紧凑排列编码减少量子比特需求，采用优化-冻结-重用策略，在训练实例上优化电路拓扑后冻结结构，在新实例上仅重新优化参数，大幅减少求解时间。


<details>
  <summary>Details</summary>
Motivation: 解决传统变分量子算法在旅行商问题中需要为每个新实例重新进行结构搜索的高计算成本问题，旨在开发一种可重用电路拓扑的方法来提高NISQ设备的实用性。

Method: 使用紧凑排列编码减少量子比特需求；采用优化-冻结-重用策略：先用模拟退火优化训练实例的电路拓扑，然后冻结该结构，在新实例上仅重新优化电路参数；避免测试时的昂贵结构搜索。

Result: 在4-7个城市的40个随机对称实例上：4城市达到100%最优路径采样概率，5城市90%，6城市80%，7城市降至约20%，显示方法的可扩展性限制。结果表明对中等规模问题具有鲁棒泛化能力。

Conclusion: 冻结Ansatz策略能显著减少求解时间而不降低解质量，展示了在NISQ硬件上的即时可实施性。讨论了可扩展性限制、参数'热启动'初始化的影响，以及扩展到车辆路径和作业车间调度等更复杂问题的前景。

Abstract: In this paper we present a variational algorithm for the Traveling Salesman
Problem (TSP) that combines (i) a compact encoding of permutations, which
reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:
where the circuit topology (``Ansatz'') is first optimized on a training
instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel
instances, limited to a rapid re-optimization of only the circuit parameters.
This pipeline eliminates costly structural research in testing, making the
procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$
cities, the resulting Ansatz achieves an average optimal trip sampling
probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for
6 city cases. With 7 cities the success rate drops markedly to an average of
$\sim 20\%$, revealing the onset of scalability limitations of the proposed
method.
  The results show robust generalization ability for moderate problem sizes and
indicate how freezing the Ansatz can dramatically reduce time-to-solution
without degrading solution quality. The paper also discusses scalability
limitations, the impact of ``warm-start'' initialization of parameters, and
prospects for extension to more complex problems, such as Vehicle Routing and
Job-Shop Scheduling.

</details>


### [24] [Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions](https://arxiv.org/abs/2508.21742)
*Timothée Loranchet,Charles K. Assaad*

Main category: cs.AI

TL;DR: 本文提出了在给定高层摘要因果图的情况下，保证微观层面时间变量间边可定向性的条件，为复杂时间系统中利用专家知识进行因果发现提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析中理解时间变量间的因果关系是一个核心挑战，特别是在完整因果结构未知的情况下。专家通常能够提供高层摘要因果图来捕捉主要因果关系，但如何利用这种高层知识来指导微观层面的因果发现缺乏理论保证。

Method: 提出了在给定摘要因果图和假设存在忠实且因果充分的分布条件下，保证微观层面时间变量间边可定向性的理论条件。这些条件即使在宏观层面存在循环或双向边的情况下也适用。

Result: 建立了在摘要因果图背景下微观边定向的理论保证，证明了即使在宏观层面存在复杂结构（如循环或双向边）时，仍能确定微观层面的因果方向。

Conclusion: 研究结果为利用摘要因果图来指导复杂时间系统中的因果发现提供了实用的理论指导，强调了将专家知识融入观测时间序列数据因果推理的重要性。

Abstract: Understanding causal relations between temporal variables is a central
challenge in time series analysis, particularly when the full causal structure
is unknown. Even when the full causal structure cannot be fully specified,
experts often succeed in providing a high-level abstraction of the causal
graph, known as a summary causal graph, which captures the main causal
relations between different time series while abstracting away micro-level
details. In this work, we present conditions that guarantee the orientability
of micro-level edges between temporal variables given the background knowledge
encoded in a summary causal graph and assuming having access to a faithful and
causally sufficient distribution with respect to the true unknown graph. Our
results provide theoretical guarantees for edge orientation at the micro-level,
even in the presence of cycles or bidirected edges at the macro-level. These
findings offer practical guidance for leveraging SCGs to inform causal
discovery in complex temporal systems and highlight the value of incorporating
expert knowledge to improve causal inference from observational time series
data.

</details>


### [25] [Tree-Guided Diffusion Planner](https://arxiv.org/abs/2508.21800)
*Hyeonseong Jeon,Cheolhong Min,Jaesik Park*

Main category: cs.AI

TL;DR: 提出了Tree-guided Diffusion Planner (TDP)，一种零样本测试时规划框架，通过结构化轨迹生成平衡探索与利用，在非凸目标、不可微约束和多奖励结构的现实场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 标准梯度引导在凸可微奖励场景中表现良好，但在现实世界的非凸目标、不可微约束和多奖励结构场景中效果显著下降。现有监督规划方法需要任务特定训练或价值估计器，限制了测试时灵活性和零样本泛化能力。

Method: 将测试时规划构建为树搜索问题，采用双层采样过程：1) 通过无训练粒子引导生成多样化父轨迹以促进广泛探索；2) 通过任务目标引导的快速条件去噪精炼子轨迹。仅使用预训练模型和测试时奖励信号。

Result: 在三个多样化任务（迷宫金币拾取、机械臂方块操作、AntMaze多目标探索）上评估，TDP在所有任务上始终优于最先进方法。

Conclusion: TDP通过探索多样化轨迹区域并利用扩展解空间中的梯度信息，有效解决了梯度引导的局限性，为测试时规划提供了灵活且强大的零样本解决方案。

Abstract: Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.

</details>


### [26] [Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture](https://arxiv.org/abs/2508.21803)
*Yeawon Lee,Xiaoyang Wang,Christopher C. Yang*

Main category: cs.AI

TL;DR: 提出了一种多智能体协作系统来模拟临床团队会诊，通过动态分配专家智能体进行分层迭代辩论，在识别充血性心力衰竭、急性肾损伤和败血症方面比单智能体基线表现更优。


<details>
  <summary>Details</summary>
Motivation: 临床叙述的准确解释对患者护理至关重要，但单一大语言模型方法缺乏高风险临床任务所需的鲁棒性。

Method: 引入协作多智能体系统，模拟临床会诊团队。Manager智能体协调动态分配的专家智能体团队，通过分层迭代辩论达成共识，仅分析SOAP笔记中的主观和客观部分。

Result: 在420份MIMIC-III笔记数据集上评估，动态多智能体配置在识别三种疾病方面表现持续改善。定性分析显示该结构能有效呈现和权衡冲突证据。

Conclusion: 通过模拟临床团队的推理过程，该系统为开发更准确、鲁棒和可解释的临床决策支持工具提供了有前景的路径。

Abstract: Accurate interpretation of clinical narratives is critical for patient care,
but the complexity of these notes makes automation challenging. While Large
Language Models (LLMs) show promise, single-model approaches can lack the
robustness required for high-stakes clinical tasks. We introduce a
collaborative multi-agent system (MAS) that models a clinical consultation team
to address this gap. The system is tasked with identifying clinical problems by
analyzing only the Subjective (S) and Objective (O) sections of SOAP notes,
simulating the diagnostic reasoning process of synthesizing raw data into an
assessment. A Manager agent orchestrates a dynamically assigned team of
specialist agents who engage in a hierarchical, iterative debate to reach a
consensus. We evaluated our MAS against a single-agent baseline on a curated
dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration
demonstrated consistently improved performance in identifying congestive heart
failure, acute kidney injury, and sepsis. Qualitative analysis of the agent
debates reveals that this structure effectively surfaces and weighs conflicting
evidence, though it can occasionally be susceptible to groupthink. By modeling
a clinical team's reasoning process, our system offers a promising path toward
more accurate, robust, and interpretable clinical decision support tools.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [27] [A Flexible Design for Beam Squint Effect Suppression in IRS-Aided THz Communications](https://arxiv.org/abs/2508.21295)
*Yanze Zhu,Qingqing Wu,Wen Chen,Yang Liu,Ruiqi Liu*

Main category: cs.IT

TL;DR: 在太赫兹宽带MISO系统中，通过在基站和智能反射面部署可移动组件来缓解双波束偏移效应，提出基于MM方法的优化算法来最大化最小接收功率


<details>
  <summary>Details</summary>
Motivation: 解决太赫兹宽带通信中基站和智能反射面耦合导致的双波束偏移效应问题，提升系统性能

Method: 采用主最小化(MM)方法，通过精心配置可移动天线阵列和IRS子阵列的位置来优化系统

Result: 数值结果表明所提算法有效，利用基站和IRS的可移动组件能够显著缓解宽带太赫兹通信中的双波束偏移效应

Conclusion: 在基站和智能反射面部署可移动组件是缓解太赫兹宽带通信中双波束偏移效应的有效解决方案

Abstract: In this paper, we study employing movable components on both base station
(BS) and intelligent reflecting surface (IRS) in a wideband terahertz (THz)
multiple-input-single-output (MISO) system, where the BS is equipped with a
movable antenna (MA) array and the IRS consists of movable subarrays. To
alleviate double beam squint effect caused by the coupling of beam squint at
the BS and IRS, we propose to maximize the minimal received power across a wide
THz spectrum by delicately configuring the positions of MAs and IRS subarrays,
which is highly challenging. By adopting majorization-minimization (MM)
methodology, we develop an algorithm to tackle the aforementioned optimization.
Numerical results demonstrate the effectiveness of our proposed algorithm and
the benefit of utilizing movable components on the BS and IRS to mitigate
double beam squint effect in wideband THz communications.

</details>


### [28] [On the Weight Distribution of Concatenated Code Ensemble Based on the Plotkin Construction](https://arxiv.org/abs/2508.21515)
*Xiao Ma*

Main category: cs.IT

TL;DR: 本文揭示了基于Plotkin构造的级联码集合的重量分布与其分量码重量分布之间的关系


<details>
  <summary>Details</summary>
Motivation: 研究级联码集合的重量分布特性，为Reed-Muller类等码的重量分布计算提供理论基础

Method: 通过分析Plotkin构造的级联码集合，建立分量码重量分布与整体码重量分布之间的数学关系

Result: 发现了级联码集合重量分布与分量码重量分布之间的明确关系

Conclusion: 该关系可用于计算包括Reed-Muller类码在内的多种码的集合重量分布

Abstract: In this note, we reveal a relation between the weight distribution of a
concatenated code ensemble based on the Plotkin construction and those of its
component codes. The relation may find applications in the calculation of the
ensemble weight distributions for many codes, including Reed-Muller (RM)-like
codes.

</details>


### [29] [Analysis of Semantic Communication for Logic-based Hypothesis Deduction](https://arxiv.org/abs/2508.21755)
*Ahmet Faruk Saz,Siheng Xiong,Faramarz Fekri*

Main category: cs.IT

TL;DR: 该论文提出了一种基于一阶逻辑推理的语义通信分析框架，通过后验分布近似和资源分配优化，设计了单轮和多轮通信策略，在预算约束下实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究在接收方持有关于世界状态的假设集、发送方只有不完整证据但无法获取真实状态的情况下，如何通过有限信息通信帮助接收方识别最符合真实状态的假设。

Method: 使用Stirling近似将目标转化为约束有限时域资源分配问题，应用KKT条件得到截断注水算法解，设计单轮和多轮消息选择策略，接收方推理建模为m元贝叶斯假设检验问题。

Result: 在最大后验概率准则下，通信策略在预算约束内达到最优性能，理论分析和实验验证显示相比随机选择和现有方法显著降低错误率。

Conclusion: 该研究为语义通信提供了一种有效的理论框架和实用策略，通过优化信息传输实现了在有限资源下的最优推理性能。

Abstract: This work presents an analysis of semantic communication in the context of
First-Order Logic (FOL)-based deduction. Specifically, the receiver holds a set
of hypotheses about the State of the World (SotW), while the transmitter has
incomplete evidence about the true SotW but lacks access to the ground truth.
The transmitter aims to communicate limited information to help the receiver
identify the hypothesis most consistent with true SotW. We formulate the
objective as approximating the posterior distribution at the transmitter to the
receiver. Using Stirling's approximation, this reduces to a constrained,
finite-horizon resource allocation problem. Applying the Karush-Kuhn-Tucker
conditions yields a truncated water-filling solution. Despite the problem's
non-convexity, symmetry and permutation invariance ensure global optimality.
Based on this, we design message selection strategies, both for single- and
multi-round communication, and model the receiver's inference as an m-ary
Bayesian hypothesis testing problem. Under the Maximum A Posteriori (MAP) rule,
our communication strategy achieves optimal performance within budget
constraints. We further analyze convergence rates and validate the theoretical
findings through experiments, demonstrating reduced error over random selection
and prior methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [30] [The Hidden Cost of Defaults in Recommender System Evaluation](https://arxiv.org/abs/2508.21180)
*Hannah Berlin,Robin Svahn,Alan Said*

Main category: cs.IR

TL;DR: 研究发现RecBole推荐框架的默认设置（特别是未记录的早停策略）会过早终止随机搜索和贝叶斯优化，限制搜索覆盖范围，导致性能变异性与不同搜索策略间的差异相当。


<details>
  <summary>Details</summary>
Motivation: 超参数优化对推荐系统性能至关重要，但通常被视为次要问题。本研究从模型基准测试转向审计广泛使用的RecBole框架的行为，揭示框架内部默认设置对超参数搜索的影响。

Method: 使用6个模型和2个数据集，比较不同搜索策略，量化性能变异性和搜索路径不稳定性，分析RecBole框架的隐藏逻辑对超参数调优的影响。

Result: 发现RecBole的未记录早停策略会过早终止搜索过程，限制搜索覆盖范围，隐藏的框架逻辑引入的变异性与不同搜索策略间的差异相当。

Conclusion: 框架应被视为实验设计的活跃组成部分，需要更透明、可重复性感知的工具。研究为开发者和研究人员提供了缓解隐藏配置行为和改善超参数调优透明度的可行建议。

Abstract: Hyperparameter optimization is critical for improving the performance of
recommender systems, yet its implementation is often treated as a neutral or
secondary concern. In this work, we shift focus from model benchmarking to
auditing the behavior of RecBole, a widely used recommendation framework. We
show that RecBole's internal defaults, particularly an undocumented
early-stopping policy, can prematurely terminate Random Search and Bayesian
Optimization. This limits search coverage in ways that are not visible to
users. Using six models and two datasets, we compare search strategies and
quantify both performance variance and search path instability. Our findings
reveal that hidden framework logic can introduce variability comparable to the
differences between search strategies. These results highlight the importance
of treating frameworks as active components of experimental design and call for
more transparent, reproducibility-aware tooling in recommender systems
research. We provide actionable recommendations for researchers and developers
to mitigate hidden configuration behaviors and improve the transparency of
hyperparameter tuning workflows.

</details>


### [31] [Breaking the Cold-Start Barrier: Reinforcement Learning with Double and Dueling DQNs](https://arxiv.org/abs/2508.21259)
*Minda Zhao*

Main category: cs.IR

TL;DR: 该论文提出了一种基于强化学习的方法，使用Double和Dueling DQN来解决推荐系统中的冷启动用户问题，通过稀疏反馈动态学习用户偏好，在保护隐私的同时提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中新用户由于交互历史有限而难以获得准确推荐的冷启动问题，同时避免使用敏感的 demographic 数据以保护用户隐私。

Method: 结合Double和Dueling Deep Q-Networks (DQN)与矩阵分解模型，通过强化学习从稀疏反馈中动态学习用户偏好。

Result: 在大型电商数据集上相比传统方法（如基于流行度和主动学习策略）表现更优，特别是Dueling DQN显著降低了冷启动用户的均方根误差(RMSE)。

Conclusion: 该方法为隐私受限环境下的冷启动用户推荐问题提供了有效的解决方案，证明了强化学习在推荐系统中的实用价值。

Abstract: Recommender systems struggle to provide accurate suggestions to new users
with limited interaction history, a challenge known as the cold-user problem.
This paper proposes a reinforcement learning approach using Double and Dueling
Deep Q-Networks (DQN) to dynamically learn user preferences from sparse
feedback, enhancing recommendation accuracy without relying on sensitive
demographic data. By integrating these advanced DQN variants with a matrix
factorization model, we achieve superior performance on a large e-commerce
dataset compared to traditional methods like popularity-based and active
learning strategies. Experimental results show that our method, particularly
Dueling DQN, reduces Root Mean Square Error (RMSE) for cold users, offering an
effective solution for privacy-constrained environments.

</details>


### [32] [Towards On-Device Personalization: Cloud-device Collaborative Data Augmentation for Efficient On-device Language Model](https://arxiv.org/abs/2508.21313)
*Zhaofeng Zhong,Wei Yuan,Liang Qu,Tong Chen,Hao Wang,Xiangyu Zhao,Hongzhi Yin*

Main category: cs.IR

TL;DR: CDCDA-PLM是一个在设备端部署个性化语言模型的框架，通过云端LLM增强用户数据，解决个性化不足和设备依赖云端的问题


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在两个主要问题：1) 响应缺乏个性化定制；2) 严重依赖云端基础设施导致网络依赖和响应延迟。需要同时解决个性化和设备端部署的问题

Method: 提出CDCDA-PLM框架，利用云端LLM的强大泛化能力增强用户有限的个人数据，通过参数高效微调(PEFT)模块在用户本地设备上微调个性化语言模型

Result: 在广泛使用的个性化基准测试的六个任务中，实验结果表明CDCDA-PLM的有效性

Conclusion: 该框架成功实现了在设备端部署个性化语言模型，消除了对网络稳定性的依赖，确保了高响应速度

Abstract: With the advancement of large language models (LLMs), significant progress
has been achieved in various Natural Language Processing (NLP) tasks. However,
existing LLMs still face two major challenges that hinder their broader
adoption: (1) their responses tend to be generic and lack personalization
tailored to individual users, and (2) they rely heavily on cloud infrastructure
due to intensive computational requirements, leading to stable network
dependency and response delay. Recent research has predominantly focused on
either developing cloud-based personalized LLMs or exploring the on-device
deployment of general-purpose LLMs. However, few studies have addressed both
limitations simultaneously by investigating personalized on-device language
models. To bridge this gap, we propose CDCDA-PLM, a framework for deploying
personalized on-device language models on user devices with support from a
powerful cloud-based LLM. Specifically, CDCDA-PLM leverages the server-side
LLM's strong generalization capabilities to augment users' limited personal
data, mitigating the issue of data scarcity. Using both real and synthetic
data, A personalized on-device language models (LMs) is fine-tuned via
parameter-efficient fine-tuning (PEFT) modules and deployed on users' local
devices, enabling them to process queries without depending on cloud-based
LLMs. This approach eliminates reliance on network stability and ensures high
response speeds. Experimental results across six tasks in a widely used
personalization benchmark demonstrate the effectiveness of CDCDA-PLM.

</details>


### [33] [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Falk Scholer,Christina Lioma*

Main category: cs.IR

TL;DR: 该研究分析了推荐系统中群体公平性和个体公平性之间的关系，发现高度群体公平的推荐可能对个体非常不公平，填补了两种公平性类型关系理解的空白。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的公平性通常分为群体公平和个体公平，但之前的研究使用不同的评估指标，导致无法正确比较两种公平性类型的关系，不清楚提高一种公平性如何影响另一种。

Method: 通过对可用于两种公平性类型的评估指标进行全面比较，在3个数据集上对8次运行进行实验分析。

Result: 实验表明，对群体高度公平的推荐可能对个体非常不公平。

Conclusion: 这一发现对旨在提高推荐系统公平性的实践者具有重要价值，揭示了群体公平和个体公平之间的潜在冲突关系。

Abstract: Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.

</details>


### [34] [Evaluating Recabilities of Foundation Models: A Multi-Domain, Multi-Dataset Benchmark](https://arxiv.org/abs/2508.21354)
*Qijiong Liu,Jieming Zhu,Yingxin Lai,Xiaoyu Dong,Lu Fan,Zhipeng Bian,Zhenhua Dong,Xiao-Ming Wu*

Main category: cs.IR

TL;DR: RecBench-MD是一个新的基准测试，用于从零资源、多数据集和多领域角度评估基础模型的推荐能力，通过19个模型在15个数据集上的测试发现领域内微调效果最佳，跨数据集迁移学习对新场景有效，多领域训练显著提升模型适应性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在推荐能力方面的综合评估对于推进推荐基础模型的发展至关重要，需要从多数据集和多领域的角度进行全面评估。

Method: 开发了RecBench-MD基准测试，对19个基础模型在15个数据集（涵盖10个不同领域）上进行广泛评估，包括电子商务、娱乐和社交媒体等领域。

Result: 研究发现领域内微调能达到最佳性能，跨数据集迁移学习能为新推荐场景提供有效支持，多领域训练显著增强了基础模型的适应性。

Conclusion: 该研究为推荐基础模型的评估提供了重要基准，所有代码和数据已公开以促进未来研究，多领域训练和迁移学习策略对提升推荐系统性能具有重要意义。

Abstract: Comprehensive evaluation of the recommendation capabilities of existing
foundation models across diverse datasets and domains is essential for
advancing the development of recommendation foundation models. In this study,
we introduce RecBench-MD, a novel and comprehensive benchmark designed to
assess the recommendation abilities of foundation models from a zero-resource,
multi-dataset, and multi-domain perspective. Through extensive evaluations of
19 foundation models across 15 datasets spanning 10 diverse domains --
including e-commerce, entertainment, and social media -- we identify key
characteristics of these models in recommendation tasks. Our findings suggest
that in-domain fine-tuning achieves optimal performance, while cross-dataset
transfer learning provides effective practical support for new recommendation
scenarios. Additionally, we observe that multi-domain training significantly
enhances the adaptability of foundation models. All code and data have been
publicly released to facilitate future research.

</details>


### [35] [Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction](https://arxiv.org/abs/2508.21460)
*Xiaoxi Cui,Weihai Lu,Yu Tong,Yiheng Li,Zhejun Zhao*

Main category: cs.IR

TL;DR: 本文提出了Diff-MSIN框架，通过多模态特征增强、协同关系捕获和特征动态自适应融合三个模块，解决了现有多模态CTR预测方法无法有效解耦模态共性和特性、忽略模态间协同效应的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法主要基于ID模态，无法全面建模用户的多模态偏好。直接应用现有多模态融合方法存在两个问题：(1)无法有效解耦不同模态的共性和特性；(2)未考虑模态间的协同效应和复杂交互。

Method: 提出Diff-MSIN框架，包含三个创新模块：多模态特征增强(MFE)模块、协同关系捕获(SRC)模块和特征动态自适应融合(FDAF)模块。MFE和SRC模块提取模态间的协同、共性和特殊信息，FDAF模块捕获用户偏好并减少融合噪声。还设计了知识解耦方法以增强特征区分度。

Result: 在Rec-Tmall和三个Amazon数据集上的实验表明，该方法相比基线至少有1.67%的显著提升。

Conclusion: Diff-MSIN框架能有效提升多模态推荐系统的性能，为解决多模态CTR预测中的模态融合问题提供了有效方案。

Abstract: In click-through rate prediction, click-through rate prediction is used to
model users' interests. However, most of the existing CTR prediction methods
are mainly based on the ID modality. As a result, they are unable to
comprehensively model users' multi-modal preferences. Therefore, it is
necessary to introduce multi-modal CTR prediction. Although it seems appealing
to directly apply the existing multi-modal fusion methods to click-through rate
prediction models, these methods (1) fail to effectively disentangle
commonalities and specificities across different modalities; (2) fail to
consider the synergistic effects between modalities and model the complex
interactions between modalities.
  To address the above issues, this paper proposes the Diffusion-based
Multi-modal Synergy Interest Network (Diff-MSIN) framework for click-through
prediction. This framework introduces three innovative modules: the Multi-modal
Feature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module,
and the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC
Module extract synergistic, common, and special information among different
modalities. They effectively enhances the representation of the modalities,
improving the overall quality of the fusion. To encourage distinctiveness among
different features, we design a Knowledge Decoupling method. Additionally, the
FDAF Module focuses on capturing user preferences and reducing fusion noise. To
validate the effectiveness of the Diff-MSIN framework, we conducted extensive
experiments using the Rec-Tmall and three Amazon datasets. The results
demonstrate that our approach yields a significant improvement of at least
1.67% compared to the baseline, highlighting its potential for enhancing
multi-modal recommendation systems. Our code is available at the following
link: https://github.com/Cxx-0/Diff-MSIN.

</details>


### [36] [Geospatial Question Answering on Historical Maps Using Spatio-Temporal Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2508.21491)
*Ziyi Liu,Sidi Wu,Lorenz Hurni*

Main category: cs.IR

TL;DR: 基于历史地图数据构建时空知识图并集成大语言模型，开发了能够高效回答地理问题的GeoQA系统


<details>
  <summary>Details</summary>
Motivation: 为了让不熟悉数据库查询语言的用户也能自然直观地获取历史地图中的知识，需要将提取的矩阵化特征结构化组织

Method: 构建时空知识图，定义本体语言，集成大语言模型，研究事实性和描述性两类GeoQA流程，添加历史地图图像和网络搜索结果作为额外上下文

Result: 系统能够以高传递率和高语义准确性生成答案，并开发了支持交互式查询和可视化的网页应用

Conclusion: 通过知识图与大语言模型的结合，成功实现了高效的地理问答系统，为历史地图数据的访问和利用提供了新方法

Abstract: Recent advances have enabled the extraction of vectorized features from
digital historical maps. To fully leverage this information, however, the
extracted features must be organized in a structured and meaningful way that
supports efficient access and use. One promising approach is question answering
(QA), which allows users -- especially those unfamiliar with database query
languages -- to retrieve knowledge in a natural and intuitive manner. In this
project, we developed a GeoQA system by integrating a spatio-temporal knowledge
graph (KG) constructed from historical map data with large language models
(LLMs). Specifically, we have defined the ontology to guide the construction of
the spatio-temporal KG and investigated workflows of two different types of
GeoQA: factual and descriptive. Additional data sources, such as historical map
images and internet search results, are incorporated into our framework to
provide extra context for descriptive GeoQA. Evaluation results demonstrate
that the system can generate answers with a high delivery rate and a high
semantic accuracy. To make the framework accessible, we further developed a web
application that supports interactive querying and visualization.

</details>


### [37] [NewsReX: A More Efficient Approach to News Recommendation with Keras 3 and JAX](https://arxiv.org/abs/2508.21572)
*Igor L. R. Azevedo,Toyotaro Suzumura,Yuichiro Yasui*

Main category: cs.IR

TL;DR: NewsReX是一个基于Keras 3和JAX的开源新闻推荐库，显著提升计算效率，支持自定义数据集训练，并提供训练参数分析指南。


<details>
  <summary>Details</summary>
Motivation: 新闻推荐研究中的结果复现和比较越来越困难，原因是代码库碎片化、配置差异大以及资源密集型模型。需要一个统一的工具来简化这个过程。

Method: 开发基于Keras 3和JAX的现代化实现，提供计算效率优化，支持自定义数据集训练脚本，并分析关键训练参数如负采样策略、训练轮数等。

Result: 实验显示NewsReX比现有实现更快，在包括8GB RTX 3060 Ti在内的不同硬件上都能获得加速效果，并使用日本经济新闻专有数据集验证了功能。

Conclusion: NewsReX使复杂实验的复现更快、更易访问，为未来研究提供了有价值的参考，减少了基线比较时的冗余计算，并指导最佳实践。

Abstract: Reproducing and comparing results in news recommendation research has become
increasingly difficult. This is due to a fragmented ecosystem of diverse
codebases, varied configurations, and mainly due to resource-intensive models.
We introduce NewsReX, an open-source library designed to streamline this
process. Our key contribution is a modern implementation built on Keras 3 and
JAX, which provides an increase in computational efficiency. Experiments show
that NewsReX is faster than current implementations. To support broader
research, we provide a straightforward guide and scripts for training models on
custom datasets. We validated this functionality using a proprietary Japanese
news dataset from Nikkei News, a leading Japanese media corporation renowned
for its comprehensive coverage of business, economic, and financial news.
NewsReX makes reproducing complex experiments faster and more accessible to a
wider range of hardware making sure the speed up it also achieved for less
powerful GPUs, like an 8GB RTX 3060 Ti. Beyond the library, this paper offers
an analysis of key training parameters often overlooked in the literature,
including the effect of different negative sampling strategies, the varying
number of epochs, the impact of random batching, and more. This supplementary
analysis serves as a valuable reference for future research, aiming to reduce
redundant computation when comparing baselines and guide best practices. Code
available at https://github.com/igor17400/NewsReX.

</details>


### [38] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank (Extended Abstract)](https://arxiv.org/abs/2508.21698)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文研究双塔模型在点击数据训练中性能下降的问题，分析了日志策略的混淆效应和模型可识别性问题，提出了样本加权技术来缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 工业环境中使用点击数据训练的双塔模型出现排名性能下降的现象，需要探究其原因并提供解决方案。

Method: 理论分析双塔模型的可识别性条件，研究日志策略对模型的影响，并提出样本加权技术来减轻偏差。

Result: 发现需要文档位置交换或特征分布重叠才能从点击中恢复模型参数；日志策略在模型完美捕获用户行为时无偏差，但在模型不完美时会放大偏差。

Conclusion: 提供了双塔模型在点击数据训练中的可识别性条件和偏差分析，提出的样本加权技术为研究者和实践者提供了实用的解决方案。

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


### [39] [T-Retrievability: A Topic-Focused Approach to Measure Fair Document Exposure in Information Retrieval](https://arxiv.org/abs/2508.21704)
*Xuejun Chang,Zaiqiao Meng,Debasis Ganguly*

Main category: cs.IR

TL;DR: 提出了一种基于主题的局部可检索性度量方法(T-Retrievability)，通过分组计算主题相关文档的可检索性分数来更准确地评估检索模型的曝光偏差。


<details>
  <summary>Details</summary>
Motivation: 传统可检索性分数在整个集合上的不均匀分布可能不能准确反映曝光偏差，而是反映了主题相关性的变化。需要一种更细粒度的测量方法来评估检索公平性。

Method: 首先将文档按主题分组，然后在每个主题组内计算可检索性分数，最后聚合这些局部值得到集合级统计量。

Result: 使用T-Retrievability度量揭示了各种神经排序模型的曝光特征新见解，表明局部度量能提供对曝光公平性更细致的理解。

Conclusion: 提出的局部可检索性度量为评估IR系统中文档可访问性提供了更可靠的方法，能更准确地反映检索模型的曝光偏差。

Abstract: Retrievability of a document is a collection-based statistic that measures
its expected (reciprocal) rank of being retrieved within a specific rank
cut-off. A collection with uniformly distributed retrievability scores across
documents is an indicator of fair document exposure. While retrievability
scores have been used to quantify the fairness of exposure for a collection, in
our work, we use the distribution of retrievability scores to measure the
exposure bias of retrieval models. We hypothesise that an uneven distribution
of retrievability scores across the entire collection may not accurately
reflect exposure bias but rather indicate variations in topical relevance. As a
solution, we propose a topic-focused localised retrievability measure, which we
call \textit{T-Retrievability} (topic-retrievability), which first computes
retrievability scores over multiple groups of topically-related documents, and
then aggregates these localised values to obtain the collection-level
statistics. Our analysis using this proposed T-Retrievability measure uncovers
new insights into the exposure characteristics of various neural ranking
models. The findings suggest that this localised measure provides a more
nuanced understanding of exposure fairness, offering a more reliable approach
for assessing document accessibility in IR systems.

</details>


### [40] [DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for Lifelong User Post-click Behaviors](https://arxiv.org/abs/2508.21801)
*Zhuoxing Wei,Qingchen Xie,Qi Liu*

Main category: cs.IR

TL;DR: 提出了DMGIN模型，通过多模态LLM对用户点击后行为序列进行分组处理，解决了长序列计算效率问题和多模态嵌入的架构不匹配问题，在CTR预测和广告收入方面取得显著提升


<details>
  <summary>Details</summary>
Motivation: 传统方法处理长用户行为序列时存在计算效率低下和多模态嵌入与大型推荐模型架构不匹配的问题，需要一种既能保持模型效果又能提高效率的解决方案

Method: 使用多模态LLM对重复项目进行分组，通过组内兴趣统计和transformer分析组内特征，使用组间transformer捕捉用户兴趣演化，避免直接引入多模态嵌入带来的计算负担

Result: 在工业和公共数据集上的实验验证了DMGIN的有效性和效率，A/B测试显示CTR提升4.7%，每英里收入提升2.3%

Conclusion: DMGIN通过创新的分组策略有效解决了长序列处理和多模态嵌入的挑战，在保持模型性能的同时显著提升了计算效率，为推荐系统提供了实用的解决方案

Abstract: Modeling user interest based on lifelong user behavior sequences is crucial
for enhancing Click-Through Rate (CTR) prediction. However, long post-click
behavior sequences themselves pose severe performance issues: the sheer volume
of data leads to high computational costs and inefficiencies in model training
and inference. Traditional methods address this by introducing two-stage
approaches, but this compromises model effectiveness due to incomplete
utilization of the full sequence context. More importantly, integrating
multimodal embeddings into existing large recommendation models (LRM) presents
significant challenges: These embeddings often exacerbate computational burdens
and mismatch with LRM architectures. To address these issues and enhance the
model's efficiency and accuracy, we introduce Deep Multimodal Group Interest
Network (DMGIN). Given the observation that user post-click behavior sequences
contain a large number of repeated items with varying behaviors and timestamps,
DMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete
lifelong post-click behavior sequences more effectively, with almost no
additional computational overhead, as opposed to directly introducing
multimodal embeddings. To mitigate the potential information loss from
grouping, we have implemented two key strategies. First, we analyze behaviors
within each group using both interest statistics and intra-group transformers
to capture group traits. Second, apply inter-group transformers to temporally
ordered groups to capture the evolution of user group interests. Our extensive
experiments on both industrial and public datasets confirm the effectiveness
and efficiency of DMGIN. The A/B test in our LBS advertising system shows that
DMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [41] [lifeXplore at the Lifelog Search Challenge 2020](https://arxiv.org/abs/2508.21397)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: 本文介绍了lifeXplore系统，这是一个用于Lifelog搜索挑战赛的视频探索和检索工具，结合了特征地图浏览、概念搜索、过滤和手绘草图功能，并加入了YOLO9000、OCR和均匀采样等改进。


<details>
  <summary>Details</summary>
Motivation: 为了在Lifelog搜索挑战赛中更有效地检索个人生活日志时刻，需要开发快速响应时间限制查询的系统。基于前两届比赛的经验，需要改进现有系统的功能。

Method: 开发了lifeXplore系统，结合特征地图浏览、概念搜索过滤和手绘草图功能。系统改进包括加入YOLO9000深度概念检测、光学字符识别(OCR)技术，以及用均匀采样替代传统的镜头分割方法。

Result: 创建了一个综合性的视频探索和检索工具，能够处理Lifelog搜索挑战赛中的各种查询需求，提高了检索效率和准确性。

Conclusion: lifeXplore系统通过多种技术的整合和改进，为Lifelog数据检索提供了更强大的工具，有望在未来的挑战赛中取得更好表现。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC) - an
interactive competition for retrieving lifelogging moments - is co-located at
the annual ACM International Conference on Multimedia Retrieval (ICMR) and has
drawn international attention. With the goal of making an ever growing public
lifelogging dataset searchable, several teams develop systems for quickly
solving time-limited queries during the challenge. Having participated in both
previous LSC iterations, i.e. LSC2018 and LSC2019, we present our lifeXplore
system - a video exploration and retrieval tool combining feature map browsing,
concept search and filtering as well as hand-drawn sketching. The system is
improved by including additional deep concept YOLO9000, optical character
recognition (OCR) as well as adding uniform sampling as an alternative to the
system's traditional underlying shot segmentation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces](https://arxiv.org/abs/2508.21466)
*Kota Fukuzawa,Atsushi Suzuki,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 提出了黎曼流形归一化最大似然（Rm-NML）方法，解决传统NML在黎曼流形上的扩展问题，保持坐标变换不变性，并在双曲空间等对称空间上实现计算简化。


<details>
  <summary>Details</summary>
Motivation: 随着图数据规模扩大，双曲空间等黎曼流形数据空间受到关注。传统NML主要在欧氏空间发展，且依赖坐标系选择，难以扩展到黎曼流形。

Method: 定义反映黎曼流形几何结构的新NML（Rm-NML），保持坐标变换不变性，扩展现有NML计算技术到黎曼流形，并在黎曼对称空间上推导计算简化方法。

Result: Rm-NML在坐标变换下不变，在欧氏空间自然参数化下与传统NML一致，并成功计算了双曲空间上正态分布的Rm-NML。

Conclusion: 提出的Rm-NML方法有效解决了黎曼流形上的模型选择问题，为双曲空间等非欧几何数据空间的统计建模提供了新工具。

Abstract: In recent years, with the large-scale expansion of graph data, there has been
an increased focus on Riemannian manifold data spaces other than Euclidean
space. In particular, the development of hyperbolic spaces has been remarkable,
and they have high expressive power for graph data with hierarchical
structures. Normalized Maximum Likelihood (NML) is employed in regret
minimization and model selection. However, existing formulations of NML have
been developed primarily in Euclidean spaces and are inherently dependent on
the choice of coordinate systems, making it non-trivial to extend NML to
Riemannian manifolds. In this study, we define a new NML that reflects the
geometric structure of Riemannian manifolds, called the Riemannian manifold NML
(Rm-NML). This Rm-NML is invariant under coordinate transformations and
coincides with the conventional NML under the natural parameterization in
Euclidean space. We extend existing computational techniques for NML to the
setting of Riemannian manifolds. Furthermore, we derive a method to simplify
the computation of Rm-NML on Riemannian symmetric spaces, which encompass data
spaces of growing interest such as hyperbolic spaces. To illustrate the
practical application of our proposed method, we explicitly computed the Rm-NML
for normal distributions on hyperbolic spaces.

</details>


### [43] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 这篇论文提出了一种混合字符串相似性、主题建模、层次聚类和规则基础的流水线方法，用于聚类交易对方名称，解决传统自然语言模型在短文本聚类中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言模型不适合聚类银行支付消息中的交易对方名称，这类文本缺乏句法结构但包含手工输入的噪音和变体。现有的模糊匹配工具不能满足调查人员追踪资金流向的需求。

Method: 设计了一种混合方法，结合字符串相似性、主题建模、层次聚类和规则基础的流水线，能够处理未知聚类数量的情况，并提供了基于精准率和召回率的评估指标。

Result: 在真实标签数据集上进行测试，表现显著超过了基准的规则基础方法，减少了手动审查的需求，并在制裁调查中提高了对实体变体的控制能力。

Conclusion: 该方法有效填补了交易对方聚类工具的空白，保持了规则系统的可解释性，同时提高了聚类性能，对于金融监管和反欺诈领域具有重要价值。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [44] [Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI](https://arxiv.org/abs/2508.21101)
*Dilruk Perera,Gousia Habib,Qianyi Xu,Daniel J. Tan,Kai He,Erik Cambria,Mengling Feng*

Main category: cs.LG

TL;DR: 本文综述了强化学习在医疗领域的应用，从传统预测模型转向主动决策干预，涵盖多源信息融合、不同架构部署，并分析了技术方法、应用场景及伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI主要进行结果预测，而强化学习能够主动制定干预决策并优化长期目标，这代表了医疗人工智能向主动智能的根本性转变，需要系统性地探索其潜力与风险。

Method: 通过信息融合视角分析医疗强化学习系统，整合生命体征、实验室数据、临床记录等多源信号，采用时间级和决策级融合机制，支持集中式、联邦式和边缘架构部署。

Result: 系统梳理了基于模型和无模型方法、离线和批量约束方法等技术路线，全面分析了在重症监护、慢性病、心理健康等领域的应用趋势、差距和转化瓶颈。

Conclusion: 强化学习不仅是技术工具集，更是向临床环境中主动智能的根本转变，需要关注伦理、部署和奖励设计挑战，确保安全、人类对齐的策略学习，实现从预测机器到主动临床智能的转型。

Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

</details>


### [45] [Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning](https://arxiv.org/abs/2508.21103)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: 本文提出了一个多粒度EEG情绪分类框架，在GAMEEMO数据集上使用LSTM-GRU模型实现了优异性能，在二分类、多分类和多标签任务中分别达到0.932 F1分数、94.5%和90.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有EEG情绪识别研究大多局限于二分类或特定被试分类，限制了在真实情感计算系统中的泛化能力和部署应用。

Method: 采用结构化预处理策略（时间窗分割、混合统计和频域特征提取、z-score标准化），构建多粒度情绪标签体系（二分类、多分类、多标签），评估多种机器学习模型和深度学习架构。

Result: LSTM-GRU模型表现最佳，在二分类任务中F1分数达0.932，在多分类和多标签分类中准确率分别为94.5%和90.6%。

Conclusion: 提出的多粒度EEG情绪分类框架在GAMEEMO数据集上验证了有效性，LSTM-GRU模型展现出卓越性能，为真实情感计算系统提供了可行的解决方案。

Abstract: Recent advancements in EEG-based emotion recognition have shown promising
outcomes using both deep learning and classical machine learning approaches;
however, most existing studies focus narrowly on binary valence prediction or
subject-specific classification, which limits generalizability and deployment
in real-world affective computing systems. To address this gap, this paper
presents a unified, multigranularity EEG emotion classification framework built
on the GAMEEMO dataset, which consists of 14-channel EEG recordings and
continuous self-reported emotion ratings (boring, horrible, calm, and funny)
from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline
employs a structured preprocessing strategy that comprises temporal window
segmentation, hybrid statistical and frequency-domain feature extraction, and
z-score normalization to convert raw EEG signals into robust, discriminative
input vectors. Emotion labels are derived and encoded across three
complementary axes: (i) binary valence classification based on the averaged
polarity of positive and negative emotion ratings, and (ii) Multi-class emotion
classification, where the presence of the most affective state is predicted.
(iii) Fine-grained multi-label representation via binning each emotion into 10
ordinal classes. We evaluate a broad spectrum of models, including Random
Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,
LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently
outperforms the others, achieving an F1-score of 0.932 in the binary valence
task and 94.5% and 90.6% in both multi-class and Multi-Label emotion
classification.

</details>


### [46] [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)
*Wenfeng Feng,Penghong Zhao,Guochao Jiang,Chuzhan Hao,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: PVPO是一种通过优势参考锚点和数据预采样增强的高效强化学习方法，解决了无评论家方法依赖组内比较导致的局部最优和高计算成本问题


<details>
  <summary>Details</summary>
Motivation: 现有的无评论家强化学习方法（特别是组策略方法）严重依赖策略内的多重采样和比较来估计优势值，这容易导致策略陷入局部最优并增加计算成本

Method: 使用参考模型提前进行rollout计算奖励分数作为参考锚点，通过数据预采样评估样本难度并选择高增益数据，有效纠正组内比较带来的累积偏差并减少对rollout次数的依赖

Result: 在9个数据集和2个领域上的实验表明，PVPO达到了最先进的性能，在多个任务上展现出强大的泛化能力，并在不同规模的模型上表现出可扩展的性能

Conclusion: PVPO通过优势参考锚点和数据预采样机制，有效解决了无评论家强化学习方法的局限性，实现了更高的训练效率和更好的性能表现

Abstract: Critic-free reinforcement learning methods, particularly group policies, have
attracted considerable attention for their efficiency in complex tasks.
However, these methods rely heavily on multiple sampling and comparisons within
the policy to estimate advantage, which may cause the policy to fall into local
optimum and increase computational cost. To address these issues, we propose
PVPO, an efficient reinforcement learning method enhanced by an advantage
reference anchor and data pre-sampling. Specifically, we use the reference
model to rollout in advance and employ the calculated reward score as a
reference anchor. Our approach effectively corrects the cumulative bias
introduced by intra-group comparisons and significantly reduces reliance on the
number of rollouts. Meanwhile, the reference model can assess sample difficulty
during data pre-sampling, enabling effective selection of high-gain data to
improve training efficiency. Experiments conducted on nine datasets across two
domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our
approach not only demonstrates robust generalization across multiple tasks, but
also exhibits scalable performance across models of varying scales.

</details>


### [47] [Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models](https://arxiv.org/abs/2508.21106)
*Tatyana Matveeva,Aleksandr Katrutsa,Evgeny Frolov*

Main category: cs.LG

TL;DR: AdaGram是一种高效的全矩阵自适应梯度优化器，通过快速对称分解和低秩近似来降低计算和内存开销，在保持性能的同时实现比对角自适应方法更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应梯度方法（如Adagrad）使用对角预处理矩阵，无法捕捉参数相关性。全矩阵自适应方法虽然能建模这些相关性，但计算和内存成本过高，限制了在大规模模型中的应用。

Method: 提出AdaGram优化器，采用快速对称分解计算预处理更新方向，并使用矩阵积分器方法保持预处理器的低秩结构，从而降低内存和计算开销。

Result: 在标准机器学习任务上的数值实验表明，AdaGram在使用秩5及更小秩近似时，收敛速度更快或与对角自适应优化器性能相当。

Conclusion: AdaGram为大规模模型中的自适应优化提供了一个可扩展的解决方案，能够在保持性能的同时实现更高效的优化。

Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in
large-scale optimization. However, their use of diagonal preconditioning
matrices limits the ability to capture parameter correlations. Full-matrix
adaptive methods, approximating the exact Hessian, can model these correlations
and may enable faster convergence. At the same time, their computational and
memory costs are often prohibitive for large-scale models. To address this
limitation, we propose AdaGram, an optimizer that enables efficient full-matrix
adaptive gradient updates. To reduce memory and computational overhead, we
utilize fast symmetric factorization for computing the preconditioned update
direction at each iteration. Additionally, we maintain the low-rank structure
of a preconditioner along the optimization trajectory using matrix integrator
methods. Numerical experiments on standard machine learning tasks show that
AdaGram converges faster or matches the performance of diagonal adaptive
optimizers when using rank five and smaller rank approximations. This
demonstrates AdaGram's potential as a scalable solution for adaptive
optimization in large models.

</details>


### [48] [An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity](https://arxiv.org/abs/2508.21109)
*Georgios Vamvouras,Konstantinos Braimakis,Christos Tzivanidis*

Main category: cs.LG

TL;DR: 提出基于双向LSTM和注意力机制的深度学习框架，用于48小时温度、太阳辐照度和相对湿度预测，支持智能HVAC系统的模型预测控制，在精度和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为智能HVAC系统的模型预测控制提供准确可靠的短期气象预测，通过多变量联合预测和可解释性分析，提升建筑能源效率控制能力。

Method: 使用堆叠双向LSTM网络结合注意力机制，联合预测三个气象变量，利用2019-2022年历史气象数据训练，编码周期性时间特征，2023年数据评估泛化能力。

Result: 平均绝对误差：温度1.3°C、辐照度31W/m²、湿度6.7%，优于最先进的数值天气预报和机器学习基准方法，集成梯度和注意力权重提供了特征贡献和时序模式的可解释性分析。

Conclusion: 该框架通过多变量预测、注意力机制深度学习和可解释性技术的结合，推进了数据驱动的天气预报，展示了在节能建筑控制中通过可靠短期气象预测的潜力。

Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of
temperature, solar irradiance, and relative humidity to support Model
Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked
Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing
temporal and cross-feature dependencies by jointly predicting all three
variables. Historical meteorological data (2019-2022) with encoded cyclical
time features were used for training, while 2023 data evaluated generalization.
The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),
31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming
state-of-the-art numerical weather prediction and machine learning benchmarks.
Integrated Gradients quantified feature contributions, and attention weights
revealed temporal patterns, enhancing interpretability. By combining
multivariate forecasting, attention-based DL, and explainability, this work
advances data-driven weather prediction. The demonstrated accuracy and
transparency highlight the framework's potential for energy-efficient building
control through reliable short-term meteorological forecasting.

</details>


### [49] [Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI](https://arxiv.org/abs/2508.21111)
*Evan J. Chou,Lisa S. Locke,Harvey M. Soldan*

Main category: cs.LG

TL;DR: 该研究开发了一个基于机器学习和强化学习的异常检测系统，用于NASA深空网络(DSN)设备的退化监测和异常识别，包含数据重建、异常检测、严重程度分类和AI解释功能


<details>
  <summary>Details</summary>
Motivation: DSN天线和发射器随时间退化可能导致数据流中断，威胁依赖该网络的航天器连接。需要开发方法来帮助工程师直接通过数据识别异常和设备退化

Method: 使用机器学习技术进行数据重建和预测分析，结合统计计算和阈值进行实时异常检测。集成强化学习子系统进行异常严重程度分类，以及大型语言模型为异常数据提供解释。构建完整的数据管道系统连接数据提取、解析和处理流程

Result: 实现了完整的DSN异常检测数据工作流，包括从天线数据训练模型到发射器数据管道的整合，所有功能通过智能AI系统进行复杂推理和预测

Conclusion: 该系统能够有效识别DSN设备的异常和退化，为维护和操作提供支持，可通过人工反馈持续改进，确保未来太空任务的地球连接可靠性

Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities
that generate a large volume of multivariate time-series data. These facilities
contain DSN antennas and transmitters that undergo degradation over long
periods of time, which may cause costly disruptions to the data flow and
threaten the earth-connection of dozens of spacecraft that rely on the Deep
Space Network for their lifeline. The purpose of this study was to experiment
with different methods that would be able to assist JPL engineers with directly
pinpointing anomalies and equipment degradation through collected data, and
continue conducting maintenance and operations of the DSN for future space
missions around our universe. As such, we have researched various machine
learning techniques that can fully reconstruct data through predictive
analysis, and determine anomalous data entries within real-time datasets
through statistical computations and thresholds. On top of the fully trained
and tested machine learning models, we have also integrated the use of a
reinforcement learning subsystem that classifies identified anomalies based on
severity level and a Large Language Model that labels an explanation for each
anomalous data entry, all of which can be improved and fine-tuned over time
through human feedback/input. Specifically, for the DSN transmitters, we have
also implemented a full data pipeline system that connects the data extraction,
parsing, and processing workflow all together as there was no coherent program
or script for performing these tasks before. Using this data pipeline system,
we were able to then also connect the models trained from DSN antenna data,
completing the data workflow for DSN anomaly detection. This was all wrapped
around and further connected by an agentic AI system, where complex reasoning
was utilized to determine the classifications and predictions of anomalous
data.

</details>


### [50] [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)
*Pranoy Panda,Raghav Magazine,Chaitanya Devaguptapu,Sho Takemori,Vishal Sharma*

Main category: cs.LG

TL;DR: 将LLM路由问题建模为上下文多臂老虎机问题，提出PILOT方法通过共享嵌入空间和在线学习实现自适应路由选择


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法需要完整的查询-LLM最优配对知识，但现实场景缺乏这种完整映射且查询不断变化，需要自适应决策方法

Method: 开发查询和LLM的共享嵌入空间，通过离线人类偏好数据初始化和在线老虎机反馈精炼。提出PILOT（基于LinUCB的扩展）方法，并引入多选择背包问题建模在线成本策略

Result: 建立了能够反映查询与LLM亲和度的共享嵌入空间，实现了无需对所有查询在所有LLM上进行推理的自适应路由

Conclusion: 将LLM路由问题重新定义为上下文老虎机问题，提供了更实用的自适应决策框架，能够有效处理现实世界中的查询变化和资源约束

Abstract: Large Language Models (LLMs) have revolutionized natural language processing,
but their varying capabilities and costs pose challenges in practical
applications. LLM routing addresses this by dynamically selecting the most
suitable LLM for each query/task. Previous approaches treat this as a
supervised learning problem, assuming complete knowledge of optimal query-LLM
pairings. However, real-world scenarios lack such comprehensive mappings and
face evolving user queries. We thus propose to study LLM routing as a
contextual bandit problem, enabling adaptive decision-making using bandit
feedback without requiring exhaustive inference across all LLMs for all queries
(in contrast to supervised routing). To address this problem, we develop a
shared embedding space for queries and LLMs, where query and LLM embeddings are
aligned to reflect their affinity. This space is initially learned from offline
human preference data and refined through online bandit feedback. We
instantiate this idea through Preference-prior Informed Linucb fOr adaptive
rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets
for model routing, we introduce an online cost policy modeled as a multi-choice
knapsack problem, ensuring resource-efficient routing.

</details>


### [51] [Privacy Auditing Synthetic Data Release through Local Likelihood Attacks](https://arxiv.org/abs/2508.21146)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 提出Gen-LRA攻击方法，利用生成模型对训练数据特定区域的过拟合特性，无需模型知识即可有效检测合成数据中的隐私泄露


<details>
  <summary>Details</summary>
Motivation: 现有合成数据隐私审计框架依赖启发式方法和不合理假设，检测能力有限，需要更有效的成员推理攻击方法来评估生成模型隐私风险

Method: 提出生成似然比攻击(Gen-LRA)，这是一种无需模型知识或访问的黑盒攻击方法，通过评估测试观测对代理模型估计合成数据局部似然比的影响来构建攻击

Result: 在涵盖多个数据集、模型架构和攻击参数的全面基准测试中，Gen-LRA在多个性能指标上始终优于其他生成模型MIA方法

Conclusion: Gen-LRA作为合成数据发布的隐私审计工具非常有效，突显了生成模型过拟合在现实应用中带来的重大隐私风险

Abstract: Auditing the privacy leakage of synthetic data is an important but unresolved
problem. Most existing privacy auditing frameworks for synthetic data rely on
heuristics and unreasonable assumptions to attack the failure modes of
generative models, exhibiting limited capability to describe and detect the
privacy exposure of training data through synthetic data release. In this
paper, we study designing Membership Inference Attacks (MIAs) that specifically
exploit the observation that tabular generative models tend to significantly
overfit to certain regions of the training distribution. Here, we propose
Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally
efficient No-Box MIA that, with no assumption of model knowledge or access,
formulates its attack by evaluating the influence a test observation has in a
surrogate model's estimation of a local likelihood ratio over the synthetic
data. Assessed over a comprehensive benchmark spanning diverse datasets, model
architectures, and attack parameters, we find that Gen-LRA consistently
dominates other MIAs for generative models across multiple performance metrics.
These results underscore Gen-LRA's effectiveness as a privacy auditing tool for
the release of synthetic data, highlighting the significant privacy risks posed
by generative model overfitting in real-world applications.

</details>


### [52] [Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks](https://arxiv.org/abs/2508.21172)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出基于时间残差连接的深度残差回声状态网络(DeepResESN)，通过层次化未训练残差循环层显著提升记忆能力和长期时序建模性能


<details>
  <summary>Details</summary>
Motivation: 传统回声状态网络(ESN)在长期信息处理方面存在困难，需要改进网络结构来增强长期记忆和时序建模能力

Method: 构建深度未训练RNN，采用时间残差连接，研究不同正交配置（随机生成和固定结构）对网络动态的影响，并进行数学稳定性分析

Result: 在多种时间序列任务上实验表明，DeepResESN相比传统浅层和深度储备池计算具有显著优势

Conclusion: 通过层次化残差连接结构有效提升了未训练循环神经网络的长期记忆能力和时序建模性能

Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent
Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular
for their fast and efficient learning. However, traditional ESNs often struggle
with long-term information processing. In this paper, we introduce a novel
class of deep untrained RNNs based on temporal residual connections, called
Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a
hierarchy of untrained residual recurrent layers significantly boosts memory
capacity and long-term temporal modeling. For the temporal residual
connections, we consider different orthogonal configurations, including
randomly generated and fixed-structure configurations, and we study their
effect on network dynamics. A thorough mathematical analysis outlines necessary
and sufficient conditions to ensure stable dynamics within DeepResESN. Our
experiments on a variety of time series tasks showcase the advantages of the
proposed approach over traditional shallow and deep RC.

</details>


### [53] [FUTURE: Flexible Unlearning for Tree Ensemble](https://arxiv.org/abs/2508.21181)
*Ziheng Chen,Jin Huang,Jiali Cheng,Yuchan Guo,Mengjie Wang,Lalitesh Morishetti,Kaushiki Nag,Hadi Amiri*

Main category: cs.LG

TL;DR: FUTURE是一种基于梯度优化的树集成模型遗忘算法，通过概率模型近似解决不可微问题，实现高效且有效的样本遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和"被遗忘权"日益重要，现有树集成遗忘方法存在局限性：要么针对特定模型，要么依赖离散树结构，难以泛化到复杂集成且在大规模数据集上效率低下。

Method: 将样本遗忘问题表述为基于梯度的优化任务，采用概率模型近似来适应树集成的不可微性，实现端到端的遗忘学习。

Result: 在真实数据集上的广泛实验表明，FUTURE能够产生显著且成功的遗忘性能。

Conclusion: FUTURE算法有效解决了树集成模型在隐私保护场景下的样本遗忘问题，提供了一种高效且通用的解决方案。

Abstract: Tree ensembles are widely recognized for their effectiveness in
classification tasks, achieving state-of-the-art performance across diverse
domains, including bioinformatics, finance, and medical diagnosis. With
increasing emphasis on data privacy and the \textit{right to be forgotten},
several unlearning algorithms have been proposed to enable tree ensembles to
forget sensitive information. However, existing methods are often tailored to a
particular model or rely on the discrete tree structure, making them difficult
to generalize to complex ensembles and inefficient for large-scale datasets. To
address these limitations, we propose FUTURE, a novel unlearning algorithm for
tree ensembles. Specifically, we formulate the problem of forgetting samples as
a gradient-based optimization task. In order to accommodate
non-differentiability of tree ensembles, we adopt the probabilistic model
approximations within the optimization framework. This enables end-to-end
unlearning in an effective and efficient manner. Extensive experiments on
real-world datasets show that FUTURE yields significant and successful
unlearning performance.

</details>


### [54] [What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems](https://arxiv.org/abs/2508.21547)
*Jens Leysen,Marco Favier,Bart Goethals*

Main category: cs.LG

TL;DR: 数据最小化原则在推荐系统中的实现研究，证明技术上可行但实践中面临挑战


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中个人数据处理过度的法律要求与系统性能需求之间的矛盾

Method: 提出新的问题形式，分析多种数据最小化技术，研究关键因素影响

Result: 证明在不严重影响性能的情况下可实现显著的推理数据减少

Conclusion: 数据最小化技术上可行但实践挑战仍存，其效果受技术设置和用户特征影响，难以定义通用的必要性标准

Abstract: Data minimization is a legal principle requiring personal data processing to
be limited to what is necessary for a specified purpose. Operationalizing this
principle for recommender systems, which rely on extensive personal data,
remains a significant challenge. This paper conducts a feasibility study on
minimizing implicit feedback inference data for such systems. We propose a
novel problem formulation, analyze various minimization techniques, and
investigate key factors influencing their effectiveness. We demonstrate that
substantial inference data reduction is technically feasible without
significant performance loss. However, its practicality is critically
determined by two factors: the technical setting (e.g., performance targets,
choice of model) and user characteristics (e.g., history size, preference
complexity). Thus, while we establish its technical feasibility, we conclude
that data minimization remains practically challenging and its dependence on
the technical and user context makes a universal standard for data `necessity'
difficult to implement.

</details>


### [55] [Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium](https://arxiv.org/abs/2508.21186)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 该论文将LLM的解码过程形式化为概率单纯形上的约束变分原理，证明下一个token分布沿平滑轨迹收敛到softmax均衡，揭示了温度、top-k和核采样等实践参数的确切数学含义。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型解码过程的描述往往停留在softmax归一化的表面层次，缺乏严格的数学形式化。作者希望从变分原理的角度为解码过程提供一个最小化、自包含的理论框架。

Method: 采用约束变分原理方法，将解码过程建模为概率单纯形上的离散乘法权重（熵镜像）更新，研究其连续时间极限下的复制子流，分析温度、top-k和核采样等参数对轨迹的影响。

Result: 证明了在固定上下文和温度下，下一个token分布沿平滑轨迹收敛到softmax均衡；温度精确地重新标定了轨迹时间；top-k和核采样将流限制在具有相同保证的面上。

Conclusion: 该工作为LLM解码过程提供了严格的数学形式化，揭示了常见解码策略的深层数学结构，为理解路径依赖的分数调整和幻觉类行为奠定了基础。

Abstract: Decoding in large language models is often described as scoring tokens and
normalizing with softmax. We give a minimal, self-contained account of this
step as a constrained variational principle on the probability simplex. The
discrete, normalization-respecting ascent is the classical
multiplicative-weights (entropic mirror) update; its continuous-time limit is
the replicator flow. From these ingredients we prove that, for a fixed context
and temperature, the next-token distribution follows a smooth trajectory inside
the simplex and converges to the softmax equilibrium. This formalizes the
common ``manifold traversal'' intuition at the output-distribution level. The
analysis yields precise, practice-facing consequences: temperature acts as an
exact rescaling of time along the same trajectory, while top-k and nucleus
sampling restrict the flow to a face with identical guarantees. We also outline
a controlled account of path-dependent score adjustments and their connection
to loop-like, hallucination-style behavior. We make no claims about training
dynamics or internal representations; those are deferred to future work.

</details>


### [56] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 研究发现，LLM强化学习中的反直觉现象（如单样本训练、不精确奖励信号、负样本训练等）仅在模型与任务高度对齐时有效，而在更具挑战性的场景中，标准RL方法仍然更稳健有效。


<details>
  <summary>Details</summary>
Motivation: 最近LLM强化学习中出现了一系列反直觉现象，但这些现象的有效条件尚不明确，需要系统研究其适用范围和局限性。

Method: 通过系统全面的实验验证，在不同模型架构和任务领域检验反直觉主张，重点关注模型-任务对齐程度（用pass@k准确率衡量）的影响。

Result: 研究表明，许多反直觉结果只在模型与任务高度对齐时出现，而在更具挑战性的情况下，这些技术无法驱动实质性学习，标准RL方法仍然有效。

Conclusion: 模型-任务对齐程度是区分RL观察结果的关键因素，标准RL训练在各种设置中保持稳健，而反直觉方法仅在特定对齐条件下有效。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [57] [Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay](https://arxiv.org/abs/2508.21240)
*Pujan Thapa,Alexander Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 基于自组织地图(SOM)和变分自动编码器(VAE)的新题持续学习框架，无需存储原始数据或任务标签，通过统计参数生成合成样本进行回放学习


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的恒存形势问题，避免存储大量原始数据样本和任务标签，提供内存效率更高的解决方案

Method: 采用SOM与VAE结合的方法：高维数据(CIFAR)在VAE潜在空间进行SOM，低维数据(MNIST)直接使用SOM。为每个SOM单元存储运行均值、方差咏协方差矩阵，从中生成合成样本用于回放学习

Result: 在标准类增量学习测试中，方法与最优内存基方法竞争性能相当，超过无内存方法。在CIFAR-10和CIFAR-100上分别提升10%和7%的单类增量性能改善

Conclusion: 该方法提供了一种可扩展、无需任务标签且内存效率高的持续学习解决方案，同时支持学习过程可视化和训练后作为生成模型使用

Abstract: This work introduces a novel generative continual learning framework based on
self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable
memory-efficient replay, eliminating the need to store raw data samples or task
labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100,
we design a scheme where the SOM operates over the latent space learned by a
VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and
FashionMNIST, the SOM operates in a standalone fashion. Our method stores a
running mean, variance, and covariance for each SOM unit, from which synthetic
samples are then generated during future learning iterations. For the VAE-based
method, generated samples are then fed through the decoder to then be used in
subsequent replay. Experimental results on standard class-incremental
benchmarks show that our approach performs competitively with state-of-the-art
memory-based methods and outperforms memory-free methods, notably improving
over best state-of-the-art single class incremental performance on CIFAR-10 and
CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further
facilitates easy visualization of the learning process and can also be utilized
as a generative model post-training. Results show our method's capability as a
scalable, task-label-free, and memory-efficient solution for continual
learning.

</details>


### [58] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出基于混合专家(MoE)的元学习框架，动态整合三种先进CFD代理模型的预测，在汽车空气动力学预测中显著降低误差


<details>
  <summary>Details</summary>
Motivation: 高保真CFD仿真计算成本高，现有ML代理模型架构多样但无单一最优方案，需要利用架构多样性提升预测性能

Method: 使用门控网络动态组合DoMINO、X-MeshGraphNet和FigConvNet三种异构模型的预测，加入熵正则化防止模型坍塌，在DrivAerML数据集上训练验证

Result: MoE模型在所有物理量预测中均优于集成平均和最准确的单个专家模型，L-2预测误差显著降低

Conclusion: MoE框架通过协同整合专业架构的互补优势，为创建更鲁棒准确的复合代理模型提供了有效策略

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [59] [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
*Farnoush Rezaei Jafari,Oliver Eberle,Ashkan Khakzar,Neel Nanda*

Main category: cs.LG

TL;DR: Relevance Patching (RelP)是一种新的机制解释性方法，通过使用层间相关性传播(LRP)系数替代梯度，在保持计算效率的同时，比标准属性修补更准确地近似激活修补。


<details>
  <summary>Details</summary>
Motivation: 现有的激活修补方法计算成本高，而属性修补虽然快速但存在噪声问题且在深度非线性网络中可靠性降低，需要一种既高效又准确的方法。

Method: 提出Relevance Patching方法，使用Layer-wise Relevance Propagation (LRP)的传播系数替代局部梯度，仅需两次前向传播和一次反向传播，保持计算效率。

Result: 在多个模型和任务上验证显示，RelP比标准属性修补更准确地近似激活修补，特别是在GPT-2 Large的IOI任务中，MLP输出的Pearson相关系数从0.006提升到0.956。

Conclusion: RelP在保持计算效率的同时显著提高了对激活修补的近似准确性，为机制解释性研究提供了更可靠的工具。

Abstract: Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.

</details>


### [60] [Owen Sampling Accelerates Contribution Estimation in Federated Learning](https://arxiv.org/abs/2508.21261)
*Hossein KhademSohi,Hadi Hemmati,Jiayu Zhou,Steve Drew*

Main category: cs.LG

TL;DR: FedOwen是一个高效的联邦学习框架，使用Owen采样来近似计算Shapley值，在相同评估预算下比现有方法误差更小，并通过自适应客户端选择策略实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中准确估计客户端贡献对于公平奖励和选择有用客户端至关重要，但精确计算Shapley值在客户端数量大时计算复杂度呈指数增长，不可行。

Method: 提出FedOwen框架，使用Owen采样近似Shapley值，采用自适应客户端选择策略平衡利用高价值客户端和探索未被充分采样的客户端。

Result: 在固定评估成本下，FedOwen在相同通信轮数内比最先进的基线方法在非IID基准测试上实现了高达23%的最终准确率提升。

Conclusion: FedOwen通过高效的Shapley值近似和智能客户端选择，显著提高了联邦学习的效率和性能，特别适用于大规模联邦学习场景。

Abstract: Federated Learning (FL) aggregates information from multiple clients to train
a shared global model without exposing raw data. Accurately estimating each
client's contribution is essential not just for fair rewards, but for selecting
the most useful clients so the global model converges faster. The Shapley value
is a principled choice, yet exact computation scales exponentially with the
number of clients, making it infeasible for large federations. We propose
FedOwen, an efficient framework that uses Owen sampling to approximate Shapley
values under the same total evaluation budget as existing methods while keeping
the approximation error small. In addition, FedOwen uses an adaptive client
selection strategy that balances exploiting high-value clients with exploring
under-sampled ones, reducing bias and uncovering rare but informative data.
Under a fixed valuation cost, FedOwen achieves up to 23 percent higher final
accuracy within the same number of communication rounds compared to
state-of-the-art baselines on non-IID benchmarks.

</details>


### [61] [Guess-and-Learn (G&L): Measuring the Cumulative Error Cost of Cold-Start Adaptation](https://arxiv.org/abs/2508.21270)
*Roland Arnold*

Main category: cs.LG

TL;DR: G&L v1.0提出评估机器学习模型冷启动适应性的新框架，通过测量模型在从未标注数据集中顺序学习时累积的错误数量，揭示传统终点精度指标无法捕捉的早期学习效率差异。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估过度强调最终精度，忽略了从零开始学习时的适应成本（累积错误）。需要量化模型在早期学习阶段的效率和可靠性。

Method: 设计Guess-and-Learn协议：模型逐步选择实例、预测标签、接收真实标签并更新参数（在线或批量模式）。定义四个实验轨道（从零/预训练 × 在线/批量），在MNIST和AG News数据集上测试多种模型。

Result: 发现小模型能以较少初始错误适应，预训练效果因领域而异。所有当前模型性能仍远低于理论oracle参考带，存在明显的适应性差距。

Conclusion: G&L框架补充了传统基准测试，为开发既在极限下准确又从第一个样本开始就可靠的模型提供了可复现的评估框架。

Abstract: Evaluation of machine learning models typically emphasizes final accuracy,
overlooking the cost of adaptation: the cumulative errors incurred while
learning from scratch. Guess-and- Learn (G&L) v1.0 addresses this gap by
measuring cold-start adaptability - the total mistakes a model makes while
sequentially labeling an unlabeled dataset. At each step, the learner selects
an instance, predicts its label, receives the ground truth, and updates
parameters under either online (per-sample) or batch (delayed) mode. The
resulting error trajectory exposes adaptation speed, selection quality, and
bias - dynamics invisible to endpoint metrics.
  G&L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) to
disentangle the effects of initialization and update frequency. We formalize
the protocol, relate it to classical mistake-bound theory, and estimate a
heuristic "oracle reference band" for MNIST as a plausibility reference.
Baseline experiments on MNIST and AG News, spanning classical methods
(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), and
pretrained transformers (ViT-B/16, BERT-base), reveal systematic differences in
early-phase efficiency: smaller models can adapt with fewer initial errors,
while pretraining benefits vary by domain. Across settings, current models
remain well above the oracle band, highlighting an adaptability gap.
  By quantifying the mistake cost of early learning, G&L complements
conventional benchmarks and provides a reproducible framework for developing
learners that are not only accurate in the limit but also reliable from the
first examples.

</details>


### [62] [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)
*Ashok Devireddy,Shunping Huang*

Main category: cs.LG

TL;DR: CALM框架：基于Apache Beam和TimesFm的实时异常检测系统，通过持续微调和LLM判断机制应对概念漂移问题


<details>
  <summary>Details</summary>
Motivation: 传统离线训练的异常检测模型在面对概念漂移时性能显著下降，需要开发能够实时适应数据变化的解决方案

Method: 基于Apache Beam分布式处理框架，使用TimesFm基础模型进行预测式异常检测，采用闭环持续微调机制和LLM-as-a-Judge组件进行语义判断

Result: 在TSB-UAD基准测试中，持续微调模型相比静态预训练基础模型在大多数数据集上提升了ROC AUC分数

Conclusion: CALM框架通过自适应和LLM引导的方法，在动态流式环境中有效维持了高性能异常检测能力

Abstract: The detection of anomalies in non-stationary time-series streams is a
critical but challenging task across numerous industrial and scientific
domains. Traditional models, trained offline, suffer significant performance
degradation when faced with concept drift, where the underlying statistical
properties of the data change over time. This paper introduces CALM
(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for
real-time anomaly detection designed to address this challenge. CALM is built
on the Apache Beam distributed processing framework and leverages the TimesFm
foundation model for forecasting-based anomaly detection. The framework's
novelty lies in two core contributions. First, it implements a closed-loop,
continuous fine-tuning mechanism that allows the anomaly detection model to
adapt to evolving data patterns in near real-time. Second, it introduces an
LLM-as-a-Judge component, a Large Language Model that provides semantic,
context-aware judgments on detected anomalies to curate a high-quality training
dataset, deciding whether an anomaly represents transient noise or a meaningful
pattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Our
results demonstrate that the continuously fine-tuned model improves the ROC AUC
score in most datasets compared to the static, pre-trained base model,
validating the efficacy of our adaptive, LLM-guided approach to maintaining
high-performance anomaly detection in dynamic streaming environments.

</details>


### [63] [Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning](https://arxiv.org/abs/2508.21278)
*Yibin Sun,Nick Lim,Guilherme Weigert Cassales,Heitor Murilo Gomes,Bernhard Pfahringer,Albert Bifet,Anany Dwivedi*

Main category: cs.LG

TL;DR: 本文探索使用数据流学习技术检测肌电信号中的域偏移，通过KPCA预处理和多种漂移检测方法评估，发现现有技术在实时检测方面存在局限性


<details>
  <summary>Details</summary>
Motivation: 肌电信号固有的非平稳性使得域偏移检测具有挑战性，需要开发有效的实时检测方法来维持稳定的肌电解码模型

Method: 使用KPCA（余弦核）预处理Ninapro数据库的DB6数据集，评估CUSUM、Page-Hinckley和ADWIN等多种漂移检测方法

Result: 当前技术在肌电信号实时域偏移检测方面性能有限，未能达到高精度检测效果

Conclusion: 流式学习方法在维持稳定肌电解码模型方面具有潜力，但需要进一步研究来提高在真实场景中的鲁棒性和准确性

Abstract: Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.

</details>


### [64] [MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems](https://arxiv.org/abs/2508.21296)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: MyGO是一个受生物睡眠-觉醒周期启发的终身学习框架，通过生成式记忆模型和离线知识蒸馏来避免灾难性遗忘，无需存储原始数据，在隐私和存储效率方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有持续学习方法在数据隐私、存储限制和任务差异导致的性能下降等问题，特别是避免存储原始样本的需求。

Method: 采用觉醒-睡眠双阶段框架：觉醒阶段快速学习新任务并训练紧凑生成模型；睡眠阶段使用所有生成模型生成伪数据，通过知识蒸馏将新旧知识整合到核心特征提取器中。

Result: 在计算机视觉（Split-MNIST）和自然语言处理（Split-AG News）基准测试中，MyGO显著减轻了灾难性遗忘，保持了跨任务的高平均准确率。

Conclusion: MyGO框架有效解决了持续学习中的关键挑战，证明了其在不同领域的通用性和实用性，为隐私保护型终身学习提供了可行方案。

Abstract: Continual or Lifelong Learning aims to develop models capable of acquiring
new knowledge from a sequence of tasks without catastrophically forgetting what
has been learned before. Existing approaches often rely on storing samples from
previous tasks (experience replay) or employing complex regularization terms to
protect learned weights. However, these methods face challenges related to data
privacy, storage limitations, and performance degradation when tasks are
dissimilar. To address these challenges, we introduce MyGO (Memory Yielding
Generative Offline-consolidation), a novel lifelong learning framework inspired
by the biological wake-sleep cycle. During the "wake" phase, the system rapidly
learns a new task and trains a compact generative model (Generative Memory,
G-mem) to capture its data distribution. During the "sleep" phase, the system
enters an offline state, using all learned G-mem models to generate pseudo-data
("dreams") and consolidate new and old knowledge into a core feature extractor
via knowledge distillation. This approach obviates the need to store any raw
data, retaining only compact generative models, which offers significant
advantages in privacy and storage efficiency. We evaluate MyGO on computer
vision (Split-MNIST) and natural language processing (Split-AG News)
benchmarks, comparing it against a sequential fine-tuning baseline. The results
demonstrate that MyGO significantly mitigates catastrophic forgetting and
maintains high average accuracy across tasks, proving the framework's
effectiveness and domain-generality.

</details>


### [65] [Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning](https://arxiv.org/abs/2508.21300)
*Yejin Kim,Eunwon Kim,Buru Chang,Junsuk Choe*

Main category: cs.LG

TL;DR: VILA是一个新的机器遗忘框架，通过更准确地识别与敏感信息相关的参数并显著降低计算成本，解决了FILA方法的局限性，在参数效率和训练速度上实现了显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成内容时可能无意中泄露敏感信息，传统的重新训练方法计算成本过高，而现有的FILA方法虽然参数高效但仍需要访问全部模型参数且Fisher信息估计不准确。

Method: 提出VILA框架，明确考虑FILA忽略的Fisher信息基本假设，提高遗忘集参数识别的准确性，同时无需访问整个模型即可进行参数识别，大幅降低计算成本。

Result: 相比FILA，VILA实现了高达100倍的参数效率和40倍的训练速度提升，在TOFU、WMDP和MUSE等基准测试中创造了新的最先进性能。

Conclusion: VILA通过更准确的参数识别和计算效率优化，为机器遗忘提供了一个高效且有效的解决方案，在保护隐私的同时显著降低了计算开销。

Abstract: LLMs have demonstrated remarkable performance across various tasks but face
challenges related to unintentionally generating outputs containing sensitive
information. A straightforward approach to address this issue is to retrain the
model after excluding the problematic data. However, this approach incurs
prohibitively high computational costs. To overcome this limitation, machine
unlearning has emerged as a promising solution that can effectively remove
sensitive information without the need to retrain the model from scratch.
Recently, FILA has been proposed as a parameter-efficient unlearning method by
integrating LoRA adapters. Specifically, it calculates the Fisher information
to identify parameters associated with the forget set and assigns them to LoRA
adapters for updates. Despite its innovative approach, FILA still requires
access to all model parameters and does not adequately account for fundamental
assumptions underlying Fisher information, leading to inaccuracies in
importance estimation. To address these limitations, we propose VILA, a novel
unlearning framework that explicitly considers the assumptions overlooked in
FILA, thereby enhancing the accuracy of parameter identification for the forget
set. Moreover, VILA significantly reduces computational costs by enabling
parameter identification without accessing the entire model. Our method
achieves up to 100x higher parameter efficiency and 40x faster training speed
compared to FILA, and sets new state-of-the-art performance on benchmarks
including TOFU, WMDP, and MUSE. Our code is available at
https://github.com/kyj93790/VILA.

</details>


### [66] [Convergence of regularized agent-state-based Q-learning in POMDPs](https://arxiv.org/abs/2508.21314)
*Amit Sinha,Matthieu Geist,Aditya Mahajan*

Main category: cs.LG

TL;DR: 提出了一个分析Q-learning算法收敛性的框架，特别针对使用代理状态（非信念状态）和策略正则化的算法，证明了在温和技术条件下收敛到正则化MDP的固定点。


<details>
  <summary>Details</summary>
Motivation: 现有的Q-learning算法在实践中广泛使用代理状态（如循环神经网络状态）和策略正则化来促进探索和稳定学习，但缺乏对这些算法收敛性的理论理解。

Method: 研究最简单的正则化代理状态Q-learning算法（RASQL），分析其在温和技术条件下的收敛性，并扩展到学习周期性策略的变体。

Result: 证明RASQL收敛到适当定义的正则化MDP的固定点，该固定点取决于行为策略诱导的平稳分布，数值实验验证了理论结果。

Conclusion: 为使用代理状态和正则化的Q-learning算法提供了理论收敛保证，填补了实践算法与理论分析之间的空白。

Abstract: In this paper, we present a framework to understand the convergence of
commonly used Q-learning reinforcement learning algorithms in practice. Two
salient features of such algorithms are: (i)~the Q-table is recursively updated
using an agent state (such as the state of a recurrent neural network) which is
not a belief state or an information state and (ii)~policy regularization is
often used to encourage exploration and stabilize the learning algorithm. We
investigate the simplest form of such Q-learning algorithms which we call
regularized agent-state-based Q-learning (RASQL) and show that it converges
under mild technical conditions to the fixed point of an appropriately defined
regularized MDP, which depends on the stationary distribution induced by the
behavioral policy. We also show that a similar analysis continues to work for a
variant of RASQL that learns periodic policies. We present numerical examples
to illustrate that the empirical convergence behavior matches with the proposed
theoretical limit.

</details>


### [67] [Distribution-Aware Feature Selection for SAEs](https://arxiv.org/abs/2508.21324)
*Narmeen Oozeer,Nirmalendu Prakash,Michael Lan,Alice Rigg,Amirali Abdullah*

Main category: cs.LG

TL;DR: Sampled-SAE是一种改进的稀疏自编码器方法，通过在批次级别进行特征选择来平衡全局一致性和细粒度重建，解决了传统TopK SAE的效率问题和BatchTopK的"激活彩票"问题。


<details>
  <summary>Details</summary>
Motivation: 传统TopK SAE对每个token使用固定的K个最活跃特征进行重建，效率低下且忽略了不同token的信息量差异。BatchTopK虽然提高了平均重建质量，但存在"激活彩票"问题，即罕见的高幅度特征会挤掉信息丰富但幅度较低的特征。

Method: Sampled-SAE通过评分批次激活矩阵的列（特征），形成大小为Kl的候选池，然后应用Top-K从受限的特征池中选择token。参数l控制选择范围，l=1时仅从K个全局重要特征中选择，l较大时接近标准BatchTopK。

Result: 在Pythia-160M模型上的实验表明，没有单一的l值能在所有指标上达到最优，最佳选择取决于共享结构、重建保真度和下游性能之间的权衡。

Conclusion: Sampled-SAE将BatchTopK重新构建为一个可调、分布感知的方法家族，提供了在全局一致性和细粒度重建之间的灵活权衡。

Abstract: Sparse autoencoders (SAEs) decompose neural activations into interpretable
features. A widely adopted variant, the TopK SAE, reconstructs each token from
its K most active latents. However, this approach is inefficient, as some
tokens carry more information than others. BatchTopK addresses this limitation
by selecting top activations across a batch of tokens. This improves average
reconstruction but risks an "activation lottery," where rare high-magnitude
features crowd out more informative but lower-magnitude ones. To address this
issue, we introduce Sampled-SAE: we score the columns (representing features)
of the batch activation matrix (via $L_2$ norm or entropy), forming a candidate
pool of size $Kl$, and then apply Top-$K$ to select tokens across the batch
from the restricted pool of features. Varying $l$ traces a spectrum between
batch-level and token-specific selection. At $l=1$, tokens draw only from $K$
globally influential features, while larger $l$ expands the pool toward
standard BatchTopK and more token-specific features across the batch. Small $l$
thus enforces global consistency; large $l$ favors fine-grained reconstruction.
On Pythia-160M, no single value optimizes $l$ across all metrics: the best
choice depends on the trade-off between shared structure, reconstruction
fidelity, and downstream performance. Sampled-SAE thus reframes BatchTopK as a
tunable, distribution-aware family.

</details>


### [68] [Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models](https://arxiv.org/abs/2508.21330)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: Stage-Diff是一个基于扩散模型的分阶段生成模型，专门用于生成长时间序列，通过分阶段生成和渐进分解来平衡长期依赖性和数据分布漂移。


<details>
  <summary>Details</summary>
Motivation: 长时间序列具有复杂的长期时间模式和分布漂移，同时包含复杂的特征序列间关系，现有生成模型在处理这些挑战时存在困难。

Method: 提出Stage-Diff模型：1）分阶段序列生成和阶段间信息传递来保持长期依赖并建模分布漂移；2）每阶段内进行渐进序列分解，在不同时间尺度进行通道独立建模；3）阶段间信息传递使用多通道融合建模。

Result: 在多个真实世界数据集上的广泛实验验证了Stage-Diff在长时间序列生成任务中的有效性。

Conclusion: Stage-Diff通过结合通道独立建模的鲁棒性和多通道建模的信息融合优势，有效平衡了长时间序列的序列内和序列间依赖关系。

Abstract: Generative models have been successfully used in the field of time series
generation. However, when dealing with long-term time series, which span over
extended periods and exhibit more complex long-term temporal patterns, the task
of generation becomes significantly more challenging. Long-term time series
exhibit long-range temporal dependencies, but their data distribution also
undergoes gradual changes over time. Finding a balance between these long-term
dependencies and the drift in data distribution is a key challenge. On the
other hand, long-term time series contain more complex interrelationships
between different feature sequences, making the task of effectively capturing
both intra-sequence and inter-sequence dependencies another important
challenge. To address these issues, we propose Stage-Diff, a staged generative
model for long-term time series based on diffusion models. First, through
stage-wise sequence generation and inter-stage information transfer, the model
preserves long-term sequence dependencies while enabling the modeling of data
distribution shifts. Second, within each stage, progressive sequence
decomposition is applied to perform channel-independent modeling at different
time scales, while inter-stage information transfer utilizes multi-channel
fusion modeling. This approach combines the robustness of channel-independent
modeling with the information fusion advantages of multi-channel modeling,
effectively balancing the intra-sequence and inter-sequence dependencies of
long-term time series. Extensive experiments on multiple real-world datasets
validate the effectiveness of Stage-Diff in long-term time series generation
tasks.

</details>


### [69] [DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks](https://arxiv.org/abs/2508.21340)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: 提出DLGAN模型，通过双层生成对抗网络解决时间序列合成中时间依赖性和特征捕获问题，将生成过程分解为特征提取和序列重建两个阶段


<details>
  <summary>Details</summary>
Motivation: 现有时间序列合成方法基于随机序列进行时间建模，难以保证生成时间序列的时间依赖性，且难以准确捕获原始时间序列的特征信息

Method: 采用双层GAN结构：第一阶段构建完整的时间序列自编码器进行监督学习，确保重建过程能恢复序列时间依赖性；第二阶段使用GAN生成与真实序列特征向量对齐的合成特征向量

Result: 在四个公共数据集上的大量实验表明，该模型在各种评估指标上均表现出优越性

Conclusion: DLGAN是一个简单但有效的生成模型，能够有效解决时间序列合成中的时间依赖性和特征捕获问题

Abstract: Time series synthesis is an effective approach to ensuring the secure
circulation of time series data. Existing time series synthesis methods
typically perform temporal modeling based on random sequences to generate
target sequences, which often struggle to ensure the temporal dependencies in
the generated time series. Additionally, directly modeling temporal features on
random sequences makes it challenging to accurately capture the feature
information of the original time series. To address the above issues, we
propose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer
\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named
\textbf{DLGAN}. The model decomposes the time series generation process into
two stages: sequence feature extraction and sequence reconstruction. First,
these two stages form a complete time series autoencoder, enabling supervised
learning on the original time series to ensure that the reconstruction process
can restore the temporal dependencies of the sequence. Second, a Generative
Adversarial Network (GAN) is used to generate synthetic feature vectors that
align with the real-time sequence feature vectors, ensuring that the generator
can capture the temporal features from real time series. Extensive experiments
on four public datasets demonstrate the superiority of this model across
various evaluation metrics.

</details>


### [70] [Adaptive Heavy-Tailed Stochastic Gradient Descent](https://arxiv.org/abs/2508.21353)
*Bodu Gong,Gustavo Enrique Batista,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: 提出了AHTSGD算法，通过根据损失景观锐度动态调整注入噪声的尾部特性，在训练早期使用重尾噪声增强探索，后期转为轻尾噪声，从而加速收敛到宽盆地并提升泛化性能


<details>
  <summary>Details</summary>
Motivation: 针对大规模神经网络优化中过度依赖训练损失导致泛化能力不足的问题，基于梯度噪声的重尾分布特性和Edge of Stability现象，旨在通过动态噪声调整来寻找更宽的盆地以提升模型稳定性

Method: AHTSGD算法在训练早期注入重尾噪声增强探索能力，随着锐度稳定逐渐过渡到轻尾噪声，根据损失景观锐度动态调整注入噪声的尾部特性

Result: 在MNIST、CIFAR-10等基准测试中 consistently优于SGD和其他基于噪声的方法，在SVHN等噪声数据集上表现尤为突出，加速了从较差初始化的早期训练，提升了在干净和噪声设置下的泛化能力

Conclusion: AHTSGD是首个基于Edge of Stability现象调整优化器注入噪声性质的算法，能够有效促进向宽盆地的加速收敛，提高模型鲁棒性和泛化性能

Abstract: In the era of large-scale neural network models, optimization algorithms
often struggle with generalization due to an overreliance on training loss. One
key insight widely accepted in the machine learning community is the idea that
wide basins (regions around a local minimum where the loss increases gradually)
promote better generalization by offering greater stability to small changes in
input data or model parameters. In contrast, sharp minima are typically more
sensitive and less stable. Motivated by two key empirical observations - the
inherent heavy-tailed distribution of gradient noise in stochastic gradient
descent and the Edge of Stability phenomenon during neural network training, in
which curvature grows before settling at a plateau, we introduce Adaptive Heavy
Tailed Stochastic Gradient Descent (AHTSGD). The algorithm injects
heavier-tailed noise into the optimizer during the early stages of training to
enhance exploration and gradually transitions to lighter-tailed noise as
sharpness stabilizes. By dynamically adapting to the sharpness of the loss
landscape throughout training, AHTSGD promotes accelerated convergence to wide
basins. AHTSGD is the first algorithm to adjust the nature of injected noise
into an optimizer based on the Edge of Stability phenomenon. AHTSGD
consistently outperforms SGD and other noise-based methods on benchmarks like
MNIST and CIFAR-10, with marked gains on noisy datasets such as SVHN. It
ultimately accelerates early training from poor initializations and improves
generalization across clean and noisy settings, remaining robust to learning
rate choices.

</details>


### [71] [Iterative Inference in a Chess-Playing Neural Network](https://arxiv.org/abs/2508.21380)
*Elias Sandmann,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 神经网络表示构建过程分析：国际象棋AI策略网络呈现非平滑的复杂计算轨迹，与语言模型的平滑收敛形成对比


<details>
  <summary>Details</summary>
Motivation: 研究神经网络表示构建过程是通过平滑渐进细化还是复杂计算过程，通过分析超人类国际象棋引擎的策略网络来探索这一问题

Method: 扩展logit lens方法分析Leela Chess Zero策略网络，考察各层的棋力单调性、解谜能力、策略分布轨迹、走法排名相关性等指标

Result: 发现棋力和解谜能力呈现强单调趋势，但策略分布经常遵循非平滑轨迹：早期发现但随后丢弃的正确解、走法排名与最终输出相关性差、直到网络后期仍存在高策略分歧

Conclusion: 国际象棋策略网络的表示构建过程与语言模型的平滑分布收敛不同，呈现出更复杂的非平滑计算特征

Abstract: Do neural networks build their representations through smooth, gradual
refinement, or via more complex computational processes? We investigate this by
extending the logit lens to analyze the policy network of Leela Chess Zero, a
superhuman chess engine. We find strong monotonic trends in playing strength
and puzzle-solving ability across layers, yet policy distributions frequently
follow non-smooth trajectories. Evidence for this includes correct puzzle
solutions that are discovered early but subsequently discarded, move rankings
that remain poorly correlated with final outputs, and high policy divergence
until late in the network. These findings contrast with the smooth
distributional convergence typically observed in language models.

</details>


### [72] [PMODE: Theoretically Grounded and Modular Mixture Modeling](https://arxiv.org/abs/2508.21396)
*Robert A. Vandermeulen*

Main category: cs.LG

TL;DR: PMODE是一个模块化的混合建模框架，通过数据分区和分别拟合估计器来构建混合模型，支持参数和非参数组件，并在高维密度估计中表现优异


<details>
  <summary>Details</summary>
Motivation: 为了解决传统混合模型在处理不同分布族组件时的局限性，以及将理论方法扩展到高维实际应用的需求

Method: 通过数据分区策略，为每个子集分别拟合密度估计器（可以是参数或非参数方法），构建混合模型框架

Result: 达到了该类估计器的近最优速率，在CIFAR-10异常检测任务中与深度基线方法竞争性表现，能够处理数千维的高维数据

Conclusion: PMODE提供了一个通用且模块化的混合建模解决方案，即使在组件来自不同分布族时仍保持有效性，MV-PMODE成功将理论方法扩展到实际高维应用

Abstract: We introduce PMODE (Partitioned Mixture Of Density Estimators), a general and
modular framework for mixture modeling with both parametric and nonparametric
components. PMODE builds mixtures by partitioning the data and fitting separate
estimators to each subset. It attains near-optimal rates for this estimator
class and remains valid even when the mixture components come from different
distribution families. As an application, we develop MV-PMODE, which scales a
previously theoretical approach to high-dimensional density estimation to
settings with thousands of dimensions. Despite its simplicity, it performs
competitively against deep baselines on CIFAR-10 anomaly detection.

</details>


### [73] [Benchmarking the State of Networks with a Low-Cost Method Based on Reservoir Computing](https://arxiv.org/abs/2508.21420)
*Felix Simon Reimers,Carl-Hendrik Peters,Stefano Nichele*

Main category: cs.LG

TL;DR: 使用挪威移动网络数据，通过储层计算框架将网络数据转化为模型，通过代理任务性能来监测通信和移动网络状态的非侵入式低成本方法


<details>
  <summary>Details</summary>
Motivation: 需要一种非侵入式、低成本的方法来监测通信和移动网络的状态，利用现成的匿名聚合数据

Method: 将移动网络数据视为加权网络，初始化为回声状态网络(ESN)，使用神经科学启发的代理任务训练模型，通过模型性能变化来反映网络状态

Result: 实验显示代理任务性能与网络状态相关，当网络受到扰动时性能明显下降，证明了该方法的可行性

Conclusion: 该方法可作为概念验证，未来可发展为近实时监控工具，用于识别移动通信网络和交通网络的潜在弱点

Abstract: Using data from mobile network utilization in Norway, we showcase the
possibility of monitoring the state of communication and mobility networks with
a non-invasive, low-cost method. This method transforms the network data into a
model within the framework of reservoir computing and then measures the model's
performance on proxy tasks. Experimentally, we show how the performance on
these proxies relates to the state of the network. A key advantage of this
approach is that it uses readily available data sets and leverages the
reservoir computing framework for an inexpensive and largely agnostic method.
Data from mobile network utilization is available in an anonymous, aggregated
form with multiple snapshots per day. This data can be treated like a weighted
network. Reservoir computing allows the use of weighted, but untrained networks
as a machine learning tool. The network, initialized as a so-called echo state
network (ESN), projects incoming signals into a higher dimensional space, on
which a single trained layer operates. This consumes less energy than deep
neural networks in which every weight of the network is trained. We use
neuroscience inspired tasks and trained our ESN model to solve them. We then
show how the performance depends on certain network configurations and also how
it visibly decreases when perturbing the network. While this work serves as
proof of concept, we believe it can be elevated to be used for near-real-time
monitoring as well as the identification of possible weak spots of both mobile
communication networks as well as transportation networks.

</details>


### [74] [Rethinking Layer-wise Model Merging through Chain of Merges](https://arxiv.org/abs/2508.21421)
*Pietro Buzzega,Riccardo Salami,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 提出Chain of Merges (CoM)方法，通过自回归方式更新激活统计量来解决模型合并中的层间依赖问题，有效缓解协变量偏移导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着任务特定模型变体的激增，如何在不重新训练的情况下将这些专门化模块合并为统一模型成为关键挑战。现有合并技术通常独立处理每层，未能考虑深度网络中固有的层间依赖关系，导致分布不匹配问题。

Method: 提出Chain of Merges (CoM)方法，这是一种层间合并过程，以自回归方式更新激活统计量，明确考虑跨层交互作用。通过一系列条件最优更新产生连贯的合并模型。

Result: 在标准基准测试上的实验表明，CoM实现了最先进的性能表现。

Conclusion: CoM方法通过有效处理层间依赖和协变量偏移问题，为预训练模型的合并提供了有效的解决方案，达到了state-of-the-art的性能水平。

Abstract: Fine-tuning pretrained models has become a standard pathway to achieve
state-of-the-art performance across a wide range of domains, leading to a
proliferation of task-specific model variants. As the number of such
specialized modules in-creases, merging them into a unified model without
retraining has become a critical challenge. Existing merging techniques often
rely on interference heuristics,importance weighting, or activation matching
while treating each layer independently, thereby failing to account for the
inter-layer dependencies inherent in deep networks. This simplification leads
to distributional mismatches, especially inactivation-based methods, when
changes in early layers are not properly reflected in downstream ones. We
identify these mismatches as a form of internal covariate shift, comparable to
the phenomenon encountered in the initial phases of neural networks training.
To address it, we propose Chain of Merges (CoM), a layer-wise merging procedure
that updates activation statistics in an auto-regressive fashion, explicitly
accounting for cross-layer interactions. CoM produces a coherent merged model
through a series of conditionally optimal updates, effectively mitigating
degradation caused by covariate shift. Experiments on standard bench-marks
demonstrate that CoM achieves state-of-the-art performance.

</details>


### [75] [Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing](https://arxiv.org/abs/2508.21438)
*Rajiv Kailasanathan,William R. Clements,Mohammad Reza Boskabadi,Shawn M. Gibford,Emmanouil Papadakis,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.LG

TL;DR: 基于GAN集成和混合量子/经典方法的无监督异常检测框架，在连续生物制造中实现了更高的异常检测率


<details>
  <summary>Details</summary>
Motivation: 连续生物制造过程复杂且非线性，小的偏差就可能影响产量和稳定性，需要先进的异常检测方法来确保高效运行

Method: 使用生成对抗网络(GAN)集成框架，并采用混合量子/经典GAN方法，包括使用模拟量子电路和实际光子量子处理器

Result: 混合量子/经典方法在检测突发料源变异导致的异常时，实现了更高的异常检测率

Conclusion: 混合量子/经典方法在复杂连续生物制造过程中具有解决实际问题的潜力

Abstract: The development of continuous biomanufacturing processes requires robust and
early anomaly detection, since even minor deviations can compromise yield and
stability, leading to disruptions in scheduling, reduced weekly production, and
diminished economic performance. These processes are inherently complex and
exhibit non-linear dynamics with intricate relationships between process
variables, thus making advanced methods for anomaly detection essential for
efficient operation. In this work, we present a novel framework for
unsupervised anomaly detection in continuous biomanufacturing based on an
ensemble of generative adversarial networks (GANs). We first establish a
benchmark dataset simulating both normal and anomalous operation regimes in a
continuous process for the production of a small molecule. We then demonstrate
the effectiveness of our GAN-based framework in detecting anomalies caused by
sudden feedstock variability. Finally, we evaluate the impact of using a hybrid
quantum/classical GAN approach with both a simulated quantum circuit and a real
photonic quantum processor on anomaly detection performance. We find that the
hybrid approach yields improved anomaly detection rates. Our work shows the
potential of hybrid quantum/classical approaches for solving real-world
problems in complex continuous biomanufacturing processes.

</details>


### [76] [Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning](https://arxiv.org/abs/2508.21443)
*Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 提出了一种结合标准期望累积奖励和时间平均增长率的新型强化学习算法，通过几何平均和滑动窗口估计器来优化个体轨迹的长期性能


<details>
  <summary>Details</summary>
Motivation: 传统RL算法优化期望累积奖励，但在实际部署中，个体轨迹的性能更重要。需要同时考虑整体平均和个体轨迹的长期表现

Method: 定义了时间平均增长率的Bellman算子，提出使用几何平均和N滑动窗口的改进几何平均作为时间平均增长率的估计器，并将其作为正则化器嵌入目标函数

Result: 在具有挑战性的仿真环境中，该算法优于传统的RL方法

Conclusion: 该算法能够同时从整体平均和时间平均中受益，为实际应用中个体轨迹的长期性能优化提供了有效解决方案

Abstract: Reinforcement learning (RL) algorithms typically optimize the expected
cumulative reward, i.e., the expected value of the sum of scalar rewards an
agent receives over the course of a trajectory. The expected value averages the
performance over an infinite number of trajectories. However, when deploying
the agent in the real world, this ensemble average may be uninformative for the
performance of individual trajectories. Thus, in many applications, optimizing
the long-term performance of individual trajectories might be more desirable.
In this work, we propose a novel RL algorithm that combines the standard
ensemble average with the time-average growth rate, a measure for the long-term
performance of individual trajectories. We first define the Bellman operator
for the time-average growth rate. We then show that, under multiplicative
reward dynamics, the geometric mean aligns with the time-average growth rate.
To address more general and unknown reward dynamics, we propose a modified
geometric mean with $N$-sliding window that captures the path-dependency as an
estimator for the time-average growth rate. This estimator is embedded as a
regularizer into the objective, forming a practical algorithm and enabling the
policy to benefit from ensemble average and time-average simultaneously. We
evaluate our algorithm in challenging simulations, where it outperforms
conventional RL methods.

</details>


### [77] [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](https://arxiv.org/abs/2508.21468)
*Seungyeon Choi,Hwanhee Kim,Chihyun Park,Dahyeon Lee,Seungyong Lee,Yoonju Kim,Hyoungjoon Park,Sein Kwon,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: CByG框架将贝叶斯流网络扩展为基于梯度的条件生成模型，有效整合多药理学属性指导，在结合亲和力、合成可行性和选择性等关键指标上显著优于基线模型


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的药物设计方法主要关注结合亲和力，但实际药物发现还需要合成可行性和选择性等关键属性，这些在以往评估中被忽视

Method: 提出CByG框架，将贝叶斯流网络扩展为基于梯度的条件生成模型，能够稳健地整合属性特异性指导

Result: 在结合亲和力、合成可行性和选择性等多个关键评估标准上显著优于基线模型

Conclusion: CByG框架在现实药物发现应用中表现出卓越的有效性和实用性，解决了传统评估方法的局限性

Abstract: Recent advances in Structure-based Drug Design (SBDD) have leveraged
generative models for 3D molecular generation, predominantly evaluating model
performance by binding affinity to target proteins. However, practical drug
discovery necessitates high binding affinity along with synthetic feasibility
and selectivity, critical properties that were largely neglected in previous
evaluations. To address this gap, we identify fundamental limitations of
conventional diffusion-based generative models in effectively guiding molecule
generation toward these diverse pharmacological properties. We propose CByG, a
novel framework extending Bayesian Flow Network into a gradient-based
conditional generative model that robustly integrates property-specific
guidance. Additionally, we introduce a comprehensive evaluation scheme
incorporating practical benchmarks for binding affinity, synthetic feasibility,
and selectivity, overcoming the limitations of conventional evaluation methods.
Extensive experiments demonstrate that our proposed CByG framework
significantly outperforms baseline models across multiple essential evaluation
criteria, highlighting its effectiveness and practicality for real-world drug
discovery applications.

</details>


### [78] [Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning](https://arxiv.org/abs/2508.21488)
*Pascal R. van der Vaart,Neil Yorke-Smith,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 本文发现深度Q学习中存在冷后验效应，进行了前验和可能性假设的统计检验，并提出了改进的前验分布方案以提升测度性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的不确定性量化可以提高探索效率和稳健性，但现有的负海方法对前验和可能性假设的准确性研究不够。

Method: 通过实验研究前验分布，使用统计检验验证常见的正态可能性假设，并提出改进的前验设计方案。

Result: 证实了冷后验效应的存在，发现正态可能性假设经常被违背，改进的前验能够提升负海方法的性能。

Conclusion: 未来负海强化学习研究应重点关注更合适的可能性和前验设计，本文提供的简单可实现方案能够有效提高算法性能。

Abstract: Uncertainty quantification in reinforcement learning can greatly improve
exploration and robustness. Approximate Bayesian approaches have recently been
popularized to quantify uncertainty in model-free algorithms. However, so far
the focus has been on improving the accuracy of the posterior approximation,
instead of studying the accuracy of the prior and likelihood assumptions
underlying the posterior. In this work, we demonstrate that there is a cold
posterior effect in Bayesian deep Q-learning, where contrary to theory,
performance increases when reducing the temperature of the posterior. To
identify and overcome likely causes, we challenge common assumptions made on
the likelihood and priors in Bayesian model-free algorithms. We empirically
study prior distributions and show through statistical tests that the common
Gaussian likelihood assumption is frequently violated. We argue that developing
more suitable likelihoods and priors should be a key focus in future Bayesian
reinforcement learning research and we offer simple, implementable solutions
for better priors in deep Q-learning that lead to more performant Bayesian
algorithms.

</details>


### [79] [Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration](https://arxiv.org/abs/2508.21495)
*Piotr Kubaty,Filip Szatkowski,Metod Jazbec,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 早期退出模型中的检验检测比检验更有效，检验方法对样本排名变化敏感且与效率提升强相关


<details>
  <summary>Details</summary>
Motivation: 当前早期退出模型主要依赖信心基础的退出策略，但检验测量可能导致误导性结果：检验良好的分类器仍可能浪费计算资源

Method: 提出使用失败预测作为早期退出模型性能的替代代理指标，因为失败预测能够考虑样本排名的变化

Result: 实验结果显示失检验的网络可能超过检验良好的网络，失败预测与效率提升呈现强相关关系

Conclusion: 失败预测是设计和评估早期退出模型的更依赖基础，因为它更能反映样本排名变化并与计算效率直接相关

Abstract: Early-exit models speed up inference by attaching internal classifiers to
intermediate layers of the model and allowing computation to stop once a
prediction satisfies an exit criterion. Most early-exit methods rely on
confidence-based exit strategies, which motivated some works to calibrate
intermediate classifiers to improve the performance of the entire model. In
this paper, we show that calibration measures can be misleading indicators of
the performance of multi-exit models: a well-calibrated classifier may still
waste computation, and common calibration methods do not preserve the sample
ranking within a classifier. We demonstrate empirical cases where miscalibrated
networks outperform calibrated ones. As an alternative, we propose to use
failure prediction as a more useful proxy for early-exit model performance.
Unlike calibration, failure prediction accounts for changes in the ranking of
samples and shows a strong correlation with efficiency improvements, making it
a more dependable basis for designing and evaluating early-exit models.

</details>


### [80] [Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control](https://arxiv.org/abs/2508.21505)
*Vishal Pandey,Debasmita Biswas*

Main category: cs.LG

TL;DR: 提出了SNN-DT（脉冲决策变换器），将脉冲神经网络与决策变换器结合，在保持性能的同时大幅降低能耗，适用于边缘计算设备。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的强化学习代理依赖密集矩阵运算，能耗高，不适合能源受限的边缘平台。脉冲神经网络具有超低功耗特性，但之前的工作未能将脉冲动力学与回报条件序列建模无缝结合。

Method: 在每个自注意力块中嵌入Leaky Integrate-and-Fire神经元，通过替代梯度进行端到端训练，结合生物启发的三因子可塑性、相移脉冲位置编码和轻量级树突路由模块。

Result: 在经典控制基准测试中匹配或超越标准决策变换器性能，每个决策发射少于10个脉冲，能耗降低超过4个数量级。

Conclusion: 通过将序列建模与神经形态效率相结合，SNN-DT为嵌入式可穿戴设备开辟了实时低功耗控制的途径。

Abstract: Reinforcement learning agents based on Transformer architectures have
achieved impressive performance on sequential decision-making tasks, but their
reliance on dense matrix operations makes them ill-suited for
energy-constrained, edge-oriented platforms. Spiking neural networks promise
ultra-low-power, event-driven inference, yet no prior work has seamlessly
merged spiking dynamics with return-conditioned sequence modeling. We present
the Spiking Decision Transformer (SNN-DT), which embeds Leaky
Integrate-and-Fire neurons into each self-attention block, trains end-to-end
via surrogate gradients, and incorporates biologically inspired three-factor
plasticity, phase-shifted spike-based positional encodings, and a lightweight
dendritic routing module. Our implementation matches or exceeds standard
Decision Transformer performance on classic control benchmarks (CartPole-v1,
MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes
per decision, an energy proxy suggesting over four orders-of-magnitude
reduction in per inference energy. By marrying sequence modeling with
neuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power
control on embedded and wearable devices.

</details>


### [81] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 评估LLM在贷款审批表格数据上的表现和公平性，发现序列化格式选择显著影响性能和公平性，ICL提升性能但对公平性影响不一


<details>
  <summary>Details</summary>
Motivation: 随着LLM在高风险决策任务中的应用增加，需要评估其在处理表格数据时的性能和公平性，特别是在贷款审批等金融领域

Method: 使用来自加纳、德国和美国的序列化贷款审批数据集，评估LLM的零样本学习和上下文学习能力，比较不同序列化格式的影响

Result: 序列化格式选择显著影响LLM性能和公平性，某些格式如GReat和LIFT能提高F1分数但加剧公平性差距；ICL相对零样本基线提升性能4.9-59.6%，但对公平性的影响因数据集而异

Conclusion: 需要有效的表格数据表示方法和关注公平性的模型来提高LLM在金融决策中的可靠性

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [82] [On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature](https://arxiv.org/abs/2508.21513)
*Geri Skenderi*

Main category: cs.LG

TL;DR: 本文通过图里奇曲率解释了GNN在解决困难SAT问题时的性能下降问题，发现随机k-SAT公式的二部图具有负曲率特性，且曲率随问题难度增加而减小，导致GNN出现过度压缩现象。


<details>
  <summary>Details</summary>
Motivation: GNN在解决布尔可满足性问题时，在困难实例上性能急剧下降，需要探究这是否反映了根本性的架构限制。

Method: 使用图里奇曲率分析SAT公式的二部图结构，证明随机k-SAT公式具有负曲率特性，并通过实验验证曲率与问题复杂度的关系。

Result: 发现SAT公式的二部图曲率随问题难度增加而减小，GNN因过度压缩现象而无法有效处理长程依赖关系，曲率是问题复杂度的强指标并能预测性能。

Conclusion: 图曲率分析揭示了GNN在SAT求解中的局限性，为现有求解器设计原则提供了理论解释，并指出了未来研究的 promising 方向。

Abstract: Graph Neural Networks (GNNs) have recently shown promise as solvers for
Boolean Satisfiability Problems (SATs) by operating on graph representations of
logical formulas. However, their performance degrades sharply on harder
instances, raising the question of whether this reflects fundamental
architectural limitations. In this work, we provide a geometric explanation
through the lens of graph Ricci Curvature (RC), which quantifies local
connectivity bottlenecks. We prove that bipartite graphs derived from random
k-SAT formulas are inherently negatively curved, and that this curvature
decreases with instance difficulty. Building on this, we show that GNN-based
SAT solvers are affected by oversquashing, a phenomenon where long-range
dependencies become impossible to compress into fixed-length representations.
We validate our claims empirically across different SAT benchmarks and confirm
that curvature is both a strong indicator of problem complexity and can be used
to predict performance. Finally, we connect our findings to design principles
of existing solvers and outline promising directions for future work.

</details>


### [83] [Comprehensive Signal Quality Evaluation of a Wearable Textile ECG Garment: A Sex-Balanced Study](https://arxiv.org/abs/2508.21554)
*Maximilian P. Oppelt,Tobias S. Zech,Sarah H. Lorenz,Laurenz Ottmann,Jan Steffan,Bjoern M. Eskofier,Nadine R. Lang-Richter,Norman Pfeiffer*

Main category: cs.LG

TL;DR: 开发了一种新型纺织服装心电图设备，通过创新电极布局减少噪声和运动伪影，在男女各15名参与者的测试中显示出与参考设备相当的信号质量，并强调了性别特异性设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统心电图设备存在噪声和运动伪影问题，且缺乏对性别差异的考虑。本研究旨在开发一种纺织服装式心电图设备，提高信号保真度，并确保设备适用于不同性别和解剖结构。

Method: 采用创新电极布局的纺织服装设计，对15名男性和15名女性参与者进行综合评估。评估包括：信号质量定量指标、心率变异性等生理参数分析、机器学习分类任务、心电图形态特征分析，以及电极投影角度影响研究，所有分析按性别分层。

Result: 纺织系统在节律和形态分析中与参考设备达到高度一致的信号质量，表现出稳健的分类性能，并能识别影响信号采集的关键性别特异性决定因素。

Conclusion: 纺织基心电图服装在生理监测和心理生理状态检测方面具有实际可行性，强调了在可穿戴健康技术中纳入性别特异性设计考虑的重要性，以确保公平可靠的心脏诊断。

Abstract: We introduce a novel wearable textile-garment featuring an innovative
electrode placement aimed at minimizing noise and motion artifacts, thereby
enhancing signal fidelity in Electrocardiography (ECG) recordings. We present a
comprehensive, sex-balanced evaluation involving 15 healthy males and 15
healthy female participants to ensure the device's suitability across
anatomical and physiological variations. The assessment framework encompasses
distinct evaluation approaches: quantitative signal quality indices to
objectively benchmark device performance; rhythm-based analyzes of
physiological parameters such as heart rate and heart rate variability; machine
learning classification tasks to assess application-relevant predictive
utility; morphological analysis of ECG features including amplitude and
interval parameters; and investigations of the effects of electrode projection
angle given by the textile / body shape, with all analyzes stratified by sex to
elucidate sex-specific influences. Evaluations were conducted across various
activity phases representing real-world conditions. The results demonstrate
that the textile system achieves signal quality highly concordant with
reference devices in both rhythm and morphological analyses, exhibits robust
classification performance, and enables identification of key sex-specific
determinants affecting signal acquisition. These findings underscore the
practical viability of textile-based ECG garments for physiological monitoring
as well as psychophysiological state detection. Moreover, we identify the
importance of incorporating sex-specific design considerations to ensure
equitable and reliable cardiac diagnostics in wearable health technologies.

</details>


### [84] [Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation](https://arxiv.org/abs/2508.21559)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: PINNs通过将物理定律集成到学习框架中，在智能电网建模中表现出色，相比传统数据驱动方法在泛化能力和物理一致性方面具有优势，特别是在动态电网操作中保持较低误差。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据驱动方法在智能电网建模中的数据稀缺和物理一致性不足的问题，探索物理信息神经网络作为替代模型的潜力。

Method: 使用基于物理的损失函数训练PINNs（强制执行功率平衡、操作约束和电网稳定性），并与XGBoost、随机森林和线性回归在插值、交叉验证和轨迹预测三个实验中比较性能。

Result: PINNs在泛化能力方面优于数据驱动模型，在动态电网操作中保持较低的MAE，可靠地捕捉状态转换，而传统模型表现不稳定。尽管在极端操作状态下略有退化，但PINNs始终强制执行物理可行性。

Conclusion: PINNs是智能电网替代建模的范式转变工具，将数据驱动的灵活性与第一性原理的严谨性相结合，对实时电网控制和可扩展数字孪生具有重要意义。

Abstract: Physics-Informed Neural Networks (PINNs) present a transformative approach
for smart grid modeling by integrating physical laws directly into learning
frameworks, addressing critical challenges of data scarcity and physical
consistency in conventional data-driven methods. This paper evaluates PINNs'
capabilities as surrogate models for smart grid dynamics, comparing their
performance against XGBoost, Random Forest, and Linear Regression across three
key experiments: interpolation, cross-validation, and episodic trajectory
prediction. By training PINNs exclusively through physics-based loss functions
(enforcing power balance, operational constraints, and grid stability) we
demonstrate their superior generalization, outperforming data-driven models in
error reduction. Notably, PINNs maintain comparatively lower MAE in dynamic
grid operations, reliably capturing state transitions in both random and
expert-driven control scenarios, while traditional models exhibit erratic
performance. Despite slight degradation in extreme operational regimes, PINNs
consistently enforce physical feasibility, proving vital for safety-critical
applications. Our results contribute to establishing PINNs as a
paradigm-shifting tool for smart grid surrogation, bridging data-driven
flexibility with first-principles rigor. This work advances real-time grid
control and scalable digital twins, emphasizing the necessity of physics-aware
architectures in mission-critical energy systems.

</details>


### [85] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab是一个基于洞察蒸馏的框架，通过规则总结、策略性示例和反思学习，帮助大语言模型更好地处理表格分类任务，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型在少样本表格分类中具有潜力，但由于结构化数据的变异性面临挑战，需要将数据蒸馏为可操作的洞察来提升分类效果。

Method: 提出InsightTab框架，采用分而治之、先易后难和反思学习原则，结合大语言模型和数据建模技术，进行规则总结、策略性示例选择和洞察反思。

Result: 在9个数据集上的广泛评估显示，InsightTab相比最先进方法取得了持续改进，消融研究验证了原则引导的蒸馏过程的有效性。

Conclusion: InsightTab通过洞察蒸馏有效利用标注数据并管理偏差，使大语言模型能够更好地将其通用知识与特定表格任务需求对齐。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


### [86] [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](https://arxiv.org/abs/2508.21570)
*Bo Li,Yingqi Feng,Ming Jin,Xin Zheng,Yufei Tang,Laurent Cherubin,Alan Wee-Chung Liew,Can Wang,Qinghua Lu,Jingwei Yao,Shirui Pan,Hong Zhang,Xingquan Zhu*

Main category: cs.LG

TL;DR: 提出了OASIS扩散对抗框架来解决海洋盐度测量数据稀疏、不规则和噪声问题


<details>
  <summary>Details</summary>
Motivation: 海洋盐度对环流、气候和海洋生态系统至关重要，但现有测量方法存在稀疏性、不规则性和噪声问题，传统线性方法受限于云层覆盖、传感器漂移和卫星重访率低等问题

Method: 开发了OceAn Salinity Imputation System (OASIS)，一种新颖的扩散对抗框架

Result: 该方法旨在克服传统机器学习方法在严重稀疏性下的失败问题，并提供整合物理协变量的原则性方法

Conclusion: OASIS框架为解决海洋盐度数据插补问题提供了创新的解决方案

Abstract: Ocean salinity plays a vital role in circulation, climate, and marine
ecosystems, yet its measurement is often sparse, irregular, and noisy,
especially in drifter-based datasets. Traditional approaches, such as remote
sensing and optimal interpolation, rely on linearity and stationarity, and are
limited by cloud cover, sensor drift, and low satellite revisit rates. While
machine learning models offer flexibility, they often fail under severe
sparsity and lack principled ways to incorporate physical covariates without
specialized sensors. In this paper, we introduce the OceAn Salinity Imputation
System (OASIS), a novel diffusion adversarial framework designed to address
these challenges.

</details>


### [87] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: 本文证明了过参数化两层物理信息神经网络(PINNs)中随机梯度下降/流的线性收敛性，扩展了之前仅分析梯度下降的工作


<details>
  <summary>Details</summary>
Motivation: 随机梯度下降类算法在实践中广泛用于训练PINNs，但其收敛性保证对于这类神经求解器至关重要，现有研究主要关注梯度下降而缺乏对随机优化方法的理论分析

Method: 针对一般激活函数类，分析过参数化两层PINNs中随机梯度下降/流的训练过程，关键在于确保训练过程中合适Gram矩阵的正定性

Result: 建立了随机梯度下降/流在高概率意义下的线性收敛性，证明了随机优化方法在训练PINNs时的收敛保证

Conclusion: 该分析揭示了优化过程的动态特性，为通过随机算法训练的神经网络提供了理论保证，扩展了PINNs收敛性分析的理论框架

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


### [88] [Physics-Informed Spectral Modeling for Hyperspectral Imaging](https://arxiv.org/abs/2508.21618)
*Zuzanna Gawrysiak,Krzysztof Krawiec*

Main category: cs.LG

TL;DR: PhISM是一种无监督的物理信息深度学习架构，能够显式解耦高光谱观测数据并用连续基函数建模，在分类和回归任务上优于现有方法，且具有可解释的潜在表示


<details>
  <summary>Details</summary>
Motivation: 解决高光谱数据分析中需要大量标注数据、模型缺乏可解释性以及无法有效解耦观测数据的问题

Method: 采用物理信息深度学习架构，通过无监督学习显式解耦高光谱观测，使用连续基函数进行建模

Result: 在多个分类和回归基准测试中优于先前方法，只需要有限的标注数据，并提供可解释的潜在表示

Conclusion: PhISM架构成功实现了高光谱数据的无监督解耦和建模，在性能和可解释性方面都有显著提升

Abstract: We present PhISM, a physics-informed deep learning architecture that learns
without supervision to explicitly disentangle hyperspectral observations and
model them with continuous basis functions. \mname outperforms prior methods on
several classification and regression benchmarks, requires limited labeled
data, and provides additional insights thanks to interpretable latent
representation.

</details>


### [89] [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](https://arxiv.org/abs/2508.21620)
*Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 这篇专著为非专家提供概率决策算法理论分析的入门指南，涵盖多臂老虎机、贝叶斯优化和树搜索算法，旨在降低理论分析的门槛。


<details>
  <summary>Details</summary>
Motivation: 决策理论算法在材料发现、药物研发等实际应用中表现出色，但现有理论分析对非专家难以理解，阻碍了算法的进一步发展和应用。

Method: 提供自包含的理论分析介绍，仅需基础概率统计知识和高斯过程的基本了解，涵盖多臂老虎机、贝叶斯优化和树搜索等常用算法。

Result: 专著使非专家能够理解决策算法的理论分析，为开发下一代算法提供有价值的见解。

Conclusion: 通过降低理论分析的门槛，该专著有助于推动决策理论算法在科学发现等成本敏感领域的更广泛应用。

Abstract: Decision theories offer principled methods for making choices under various
types of uncertainty. Algorithms that implement these theories have been
successfully applied to a wide range of real-world problems, including
materials and drug discovery. Indeed, they are desirable since they can
adaptively gather information to make better decisions in the future, resulting
in data-efficient workflows. In scientific discovery, where experiments are
costly, these algorithms can thus significantly reduce the cost of
experimentation. Theoretical analyses of these algorithms are crucial for
understanding their behavior and providing valuable insights for developing
next-generation algorithms. However, theoretical analyses in the literature are
often inaccessible to non-experts. This monograph aims to provide an
accessible, self-contained introduction to the theoretical analysis of commonly
used probabilistic decision-making algorithms, including bandit algorithms,
Bayesian optimization, and tree search algorithms. Only basic knowledge of
probability theory and statistics, along with some elementary knowledge about
Gaussian processes, is assumed.

</details>


### [90] [Predicting Social Media Engagement from Emotional and Temporal Features](https://arxiv.org/abs/2508.21650)
*Yunwoo Kim,Junhyuk Hwang*

Main category: cs.LG

TL;DR: 使用情感和时间特征预测社交媒体参与度的机器学习方法，模型对点赞预测效果极佳（R²=0.98），但对评论预测效果一般（R²=0.41）


<details>
  <summary>Details</summary>
Motivation: 研究如何利用情感和时间元数据来预测社交媒体上的参与度（评论和点赞），探索哪些因素驱动不同类型的用户互动

Method: 基于HistGradientBoostingRegressor的多目标回归模型，使用对数变换处理偏斜的目标变量，数据集包含600首歌曲的情感标注（效价、唤醒度等）

Result: 模型在点赞预测上表现优异（R²=0.98），但在评论预测上效果较差（R²=0.41），表明点赞主要由情感和曝光信号驱动，而评论受其他未包含因素影响

Conclusion: 情感和时间元数据结合现有观看次数能有效预测未来参与度，但点赞和评论的驱动因素存在显著差异，需要进一步研究评论的额外影响因素

Abstract: We present a machine learning approach for predicting social media engagement
(comments and likes) from emotional and temporal features. The dataset contains
600 songs with annotations for valence, arousal, and related sentiment metrics.
A multi target regression model based on HistGradientBoostingRegressor is
trained on log transformed engagement ratios to address skewed targets.
Performance is evaluated with both a custom order of magnitude accuracy and
standard regression metrics, including the coefficient of determination (R^2).
Results show that emotional and temporal metadata, together with existing view
counts, predict future engagement effectively. The model attains R^2 = 0.98 for
likes but only R^2 = 0.41 for comments. This gap indicates that likes are
largely driven by readily captured affective and exposure signals, whereas
comments depend on additional factors not represented in the current feature
set.

</details>


### [91] [Activation Subspaces for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21695)
*Barış Zöngür,Robin Hesse,Stefan Roth*

Main category: cs.LG

TL;DR: 提出ActSub方法，通过奇异值分解分类头权重矩阵，将激活分解为决定性分量和无关分量，分别处理远分布偏移和近分布偏移的OOD检测问题


<details>
  <summary>Details</summary>
Motivation: 确保深度学习模型在现实应用中的可靠性，需要有效区分训练分布内(ID)和分布外(OOD)样本，特别是在不同分布偏移程度下的检测需求

Method: 使用奇异值分解分类头权重矩阵，将模型激活分解为决定性分量（对分类输出贡献最大）和无关分量（贡献最小）。远OOD时利用无关分量，近OOD时利用决定性分量

Result: 在多个标准OOD基准测试中取得了最先进的结果

Conclusion: ActSub方法通过分解激活空间的不同分量，有效处理了不同分布偏移程度的OOD检测问题，显著提升了检测性能

Abstract: To ensure the reliability of deep models in real-world applications,
out-of-distribution (OOD) detection methods aim to distinguish samples close to
the training distribution (in-distribution, ID) from those farther away (OOD).
In this work, we propose a novel OOD detection method that utilizes singular
value decomposition of the weight matrix of the classification head to
decompose the model's activations into decisive and insignificant components,
which contribute maximally, respectively minimally, to the final classifier
output. We find that the subspace of insignificant components more effectively
distinguishes ID from OOD data than raw activations in regimes of large
distribution shifts (Far-OOD). This occurs because the classification objective
leaves the insignificant subspace largely unaffected, yielding features that
are ''untainted'' by the target classification task. Conversely, in regimes of
smaller distribution shifts (Near-OOD), we find that activation shaping methods
profit from only considering the decisive subspace, as the insignificant
component can cause interference in the activation space. By combining two
findings into a single approach, termed ActSub, we achieve state-of-the-art
results in various standard OOD benchmarks.

</details>


### [92] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SLAC开发了SNL框架和Auto-SNL工具，用于在FPGA上部署实时机器学习推理模型，解决了高能物理实验中TB/s级数据流的实时处理挑战，相比现有工具hls4ml在延迟和资源使用上表现更优。


<details>
  <summary>Details</summary>
Motivation: LCLS-II自由电子激光器产生高达1MHz的X射线脉冲，探测器数据吞吐量超过1TB/s，传统数据传输和存储成本过高，需要实时机器学习数据压缩方案，但常规实现延迟过高。

Method: 开发SLAC神经网络库(SNL)框架，支持在FPGA上动态更新模型权重而无需重新综合；推出Auto-SNL Python扩展，将Python神经网络模型自动转换为SNL兼容的高级综合代码。

Result: 在Xilinx ZCU102 FPGA上的基准测试显示，SNL在多数神经网络架构中达到竞争性或更优的延迟性能，某些情况下还能节省FPGA资源。

Conclusion: SNL展示了强大的适应性，为高能物理、医学成像、机器人等领域的实时机器学习应用开辟了新机遇。

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


### [93] [Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety](https://arxiv.org/abs/2508.21722)
*Siddharth Mangalik,Ojas Deshpande,Adithya V. Ganesan,Sean A. P. Clouston,H. Andrew Schwartz*

Main category: cs.LG

TL;DR: 本研究将纵向回归断点设计(LRDD)扩展到统计学习框架，用于预测COVID-19事件对美国各县焦虑水平的断点和斜率变化，相比传统静态社区表征方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 估计社区特定心理健康事件影响对公共卫生政策至关重要，传统预测方法提供的信息有限，需要更可能具有因果关系的效应估计方法。

Method: 将LRDD从传统预测扩展到统计学习框架，利用位置历史得分、动态协变量(其他运行评估)和外生变量(静态表征)来预测未来的断点和斜率变化。

Result: 预测COVID-19事件引起的焦虑断点具有挑战性，但随着模型复杂度的提高而更可行，最佳结果来自整合外生和动态协变量(r=+0.46断点，r=+0.65斜率)。

Conclusion: 断点预测为估计未来或假设事件对特定社区的异质性影响开辟了新可能性，相比传统静态社区表征方法有显著改进。

Abstract: Estimating community-specific mental health effects of local events is vital
for public health policy. While forecasting mental health scores alone offers
limited insights into the impact of events on community well-being,
quasi-experimental designs like the Longitudinal Regression Discontinuity
Design (LRDD) from econometrics help researchers derive more effects that are
more likely to be causal from observational data. LRDDs aim to extrapolate the
size of changes in an outcome (e.g. a discontinuity in running scores for
anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond
traditional forecasting into a statistical learning framework whereby future
discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear
trajectories) are estimated given a location's history of the score, dynamic
covariates (other running assessments), and exogenous variables (static
representations). Applying our framework to predict discontinuities in the
anxiety of US counties from COVID-19 events, we found the task was difficult
but more achievable as the sophistication of models was increased, with the
best results coming from integrating exogenous and dynamic covariates. Our
approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$
for slope) over traditional static community representations. Discontinuity
forecasting raises new possibilities for estimating the idiosyncratic effects
of potential future or hypothetical events on specific communities.

</details>


### [94] [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](https://arxiv.org/abs/2508.21793)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: MoE-Health是一个新颖的混合专家框架，用于医疗保健预测中的多模态融合，能够有效处理不同模态和缺失数据，在MIMIC-IV数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实医疗环境中多模态数据往往不完整或变化多样，现有方法需要完整模态数据或依赖人工选择策略，限制了在实际临床环境中的应用。

Method: 提出MoE-Health框架，使用专门的专家网络和动态门控机制，根据可用数据模态动态选择和组合相关专家，灵活适应不同的数据可用性场景。

Result: 在MIMIC-IV数据集上的三个关键临床预测任务（院内死亡率预测、长期住院和再入院预测）中，MoE-Health相比现有多模态融合方法取得了更优性能，并在不同模态可用性模式下保持鲁棒性。

Conclusion: 该框架有效整合多模态信息，在处理异构和不完整医疗数据方面提供改进的预测性能和鲁棒性，特别适合部署在数据可用性多样的医疗环境中。

Abstract: Healthcare systems generate diverse multimodal data, including Electronic
Health Records (EHR), clinical notes, and medical images. Effectively
leveraging this data for clinical prediction is challenging, particularly as
real-world samples often present with varied or incomplete modalities. Existing
approaches typically require complete modality data or rely on manual selection
strategies, limiting their applicability in real-world clinical settings where
data availability varies across patients and institutions. To address these
limitations, we propose MoE-Health, a novel Mixture of Experts framework
designed for robust multimodal fusion in healthcare prediction. MoE-Health
architecture is specifically developed to handle samples with differing
modalities and improve performance on critical clinical tasks. By leveraging
specialized expert networks and a dynamic gating mechanism, our approach
dynamically selects and combines relevant experts based on available data
modalities, enabling flexible adaptation to varying data availability
scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical
clinical prediction tasks: in-hospital mortality prediction, long length of
stay, and hospital readmission prediction. Experimental results demonstrate
that MoE-Health achieves superior performance compared to existing multimodal
fusion methods while maintaining robustness across different modality
availability patterns. The framework effectively integrates multimodal
information, offering improved predictive performance and robustness in
handling heterogeneous and incomplete healthcare data, making it particularly
suitable for deployment in diverse healthcare environments with heterogeneous
data availability.

</details>


### [95] [UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking](https://arxiv.org/abs/2508.21772)
*V. Bugra Yesilkaynak,Emine Dari,Alican Mertan,Gozde Unal*

Main category: cs.LG

TL;DR: UniMLR是一种新的多标签排序范式，通过利用正标签之间的排序信息来建模类别相关性，而不是将所有正标签视为同等重要，从而统一了排序和分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有的多标签排序框架只利用标签二分为正负集合的信息，没有充分利用正标签之间的排序信息，无法捕捉类别相关性的差异。

Method: 提出UniMLR方法，将隐式类别相关性建模为概率分布，利用正标签的排序信息。同时创建了8个合成数据集(Ranked MNISTs)来解决数据稀缺和标注偏差问题。

Result: 统计证明该方法能准确学习正标签的排序表示，与真实排序一致且与底层显著性值成比例。在真实和合成数据集上的实验验证了框架的有效性。

Conclusion: UniMLR通过利用正标签排序信息成功统一了多标签排序中的排序和分类任务，为解决类别相关性建模提供了新思路。

Abstract: Existing multi-label ranking (MLR) frameworks only exploit information
deduced from the bipartition of labels into positive and negative sets.
Therefore, they do not benefit from ranking among positive labels, which is the
novel MLR approach we introduce in this paper. We propose UniMLR, a new MLR
paradigm that models implicit class relevance/significance values as
probability distributions using the ranking among positive labels, rather than
treating them as equally important. This approach unifies ranking and
classification tasks associated with MLR. Additionally, we address the
challenges of scarcity and annotation bias in MLR datasets by introducing eight
synthetic datasets (Ranked MNISTs) generated with varying
significance-determining factors, providing an enriched and controllable
experimental environment. We statistically demonstrate that our method
accurately learns a representation of the positive rank order, which is
consistent with the ground truth and proportional to the underlying
significance values. Finally, we conduct comprehensive empirical experiments on
both real-world and synthetic datasets, demonstrating the value of our proposed
framework.

</details>


### [96] [Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling](https://arxiv.org/abs/2508.21785)
*Peng Yang,Zhengdong Huang,Zicheng Xie,Wentao Tian,Jingyu Liu,Lunhong Dong*

Main category: cs.LG

TL;DR: 提出一个处理心率预测中数据异质性（设备源异质性和用户异质性）的框架，通过随机特征丢弃和时间感知注意力模块，在新建的ParroTao数据集和FitRec数据集上性能提升17%和15%。


<details>
  <summary>Details</summary>
Motivation: 心率预测在现实部署中面临数据异质性挑战，包括设备源异质性（不同设备特征集不同）和用户异质性（不同用户生理模式差异）。现有方法要么丢弃设备特定信息，要么无法建模用户特定差异，限制了实际性能。

Method: 提出学习对两种异质性都不敏感的潜在表示框架：1）随机特征丢弃策略处理源异质性，使模型对各种特征集具有鲁棒性；2）时间感知注意力模块捕获长期生理特征，使用对比学习目标构建判别性表示空间。

Result: 在新建的ParroTao数据集和公开FitRec数据集上，模型分别显著优于现有基线17%和15%。学习到的表示展现出强判别能力，下游应用任务验证了模型的实用价值。

Conclusion: 该框架有效解决了心率预测中的数据异质性问题，通过创新的特征处理和表示学习方法，在真实世界数据上取得了显著性能提升，具有重要的实际应用价值。

Abstract: Heart rate prediction is vital for personalized health monitoring and
fitness, while it frequently faces a critical challenge when deploying in
real-world: data heterogeneity. We classify it in two key dimensions: source
heterogeneity from fragmented device markets with varying feature sets, and
user heterogeneity reflecting distinct physiological patterns across
individuals and activities. Existing methods either discard device-specific
information, or fail to model user-specific differences, limiting their
real-world performance. To address this, we propose a framework that learns
latent representations agnostic to both heterogeneity, enabling downstream
predictors to work consistently under heterogeneous data patterns.
Specifically, we introduce a random feature dropout strategy to handle source
heterogeneity, making the model robust to various feature sets. To manage user
heterogeneity, we employ a time-aware attention module to capture long-term
physiological traits and use a contrastive learning objective to build a
discriminative representation space. To reflect the heterogeneous nature of
real-world data, we created and publicly released a new benchmark dataset,
ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that
our model significantly outperforms existing baselines by 17% and 15%,
respectively. Furthermore, analysis of the learned representations demonstrates
their strong discriminative power, and one downstream application task confirm
the practical value of our model.

</details>


### [97] [QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.21810)
*Jessica Liang,Anirudh Bharadwaj*

Main category: cs.LG

TL;DR: QR-LoRA是一种参数高效的微调方法，通过QR分解提取预训练权重的正交基，仅训练标量系数来大幅减少可训练参数数量，在GLUE任务上性能优于或持平全微调和标准LoRA。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA变体使用SVD初始化更新矩阵，但SVD在大模型上计算昂贵且奇异向量难以解释，需要更高效和结构化的参数高效微调方法。

Method: 使用带列旋转的QR分解从预训练权重矩阵提取正交基，将LoRA更新表示为这些基向量的线性组合，仅训练标量系数。

Result: 在GLUE任务上，QR-LoRA仅用601个参数就能达到或超过全微调、标准LoRA和SVD-LoRA的性能，参数减少量相比全微调超过1000倍，比典型LoRA设置少77倍。

Conclusion: QR-LoRA通过正交基表示提供了结构化的适应方式，大幅减少了参数数量同时保持了性能，为大规模语言模型的高效微调提供了新思路。

Abstract: The growing scale of Large Language Models (LLMs) has necessitated the
development of parameter-efficient fine-tuning techniques. Low-Rank Adaptation
(LoRA) has emerged as a promising approach, reducing the number of trainable
parameters by applying low-rank updates to pretrained weights. While standard
LoRA learns both update factors directly, several recent variants first
initialize those matrices via an SVD of the pretrained weights -- an operation
that can be expensive on large models and yields singular vectors that are not
always easy to interpret. In this work, we extract an orthonormal basis from
the pretrained weight matrix using QR decomposition with column pivoting, and
then express the LoRA update as a linear combination of these basis vectors --
training only the scalar coefficients, which imposes clear structure on
adaptation and drastically reduces parameter count. Experiments across GLUE
tasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,
standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular
value decomposition) with as few as 601 parameters -- a reduction of over 1000x
compared to full fine-tuning and 77x fewer than typical LoRA setups.

</details>


### [98] [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](https://arxiv.org/abs/2508.21815)
*Tobias Hyrup,Emmanouil Panagiotou,Arjun Roy,Arthur Zimek,Eirini Ntoutsi,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FLIP是一个基于Transformer的变分自编码器，结合潜在扩散模型生成表格数据，在隐私保护（Rényi差分隐私）下同时解决公平性问题，适用于任务无关的设置。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR、HIPAA等隐私法规和AI责任框架的实施，真实数据的使用面临更多限制。合成数据生成成为解决方案，但需要同时处理隐私和公平性问题，特别是在医疗等敏感领域的表格数据中。

Method: 使用Transformer-based VAE加上潜在扩散模型生成异构表格数据。隐私保护：训练时采用Rényi差分隐私约束；公平性：输入空间使用RDP兼容的平衡采样，潜在空间通过Centered Kernel Alignment对齐不同保护组的神经元激活模式。

Result: 实证结果表明FLIP在差分隐私约束下，能够有效提升任务无关的公平性，并在多种下游任务中表现出色。

Conclusion: FLIP提供了一个在隐私保护前提下同时解决数据公平性问题的有效框架，特别适用于任务无关的表格数据生成场景，具有广泛的适用性。

Abstract: As privacy regulations such as the GDPR and HIPAA and responsibility
frameworks for artificial intelligence such as the AI Act gain traction, the
ethical and responsible use of real-world data faces increasing constraints.
Synthetic data generation has emerged as a promising solution to risk-aware
data sharing and model development, particularly for tabular datasets that are
foundational to sensitive domains such as healthcare. To address both privacy
and fairness concerns in this setting, we propose FLIP (Fair Latent
Intervention under Privacy guarantees), a transformer-based variational
autoencoder augmented with latent diffusion to generate heterogeneous tabular
data. Unlike the typical setup in fairness-aware data generation, we assume a
task-agnostic setup, not reliant on a fixed, defined downstream task, thus
offering broader applicability. To ensure privacy, FLIP employs R\'enyi
differential privacy (RDP) constraints during training and addresses fairness
in the input space with RDP-compatible balanced sampling that accounts for
group-specific noise levels across multiple sampling rates. In the latent
space, we promote fairness by aligning neuron activation patterns across
protected groups using Centered Kernel Alignment (CKA), a similarity measure
extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment
encourages statistical independence between latent representations and the
protected feature. Empirical results demonstrate that FLIP effectively provides
significant fairness improvements for task-agnostic fairness and across diverse
downstream tasks under differential privacy constraints.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [99] [Auctions Meet Bandits: An Empirical Analysis](https://arxiv.org/abs/2508.21162)
*Mohammad Rashid,Omid Rafieian,Soheil Ghili*

Main category: cs.GT

TL;DR: 本文研究了赞助搜索拍卖中冷启动问题的探索策略优化，通过实证分析发现拍卖环境下的多臂老虎机问题与传统设置存在显著差异，并提出基于关键词特性的定制化探索策略，在收益和效率方面都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 赞助搜索拍卖中，新广告主的质量评分设置面临冷启动问题，平台使用多臂老虎机算法来平衡探索和利用，但拍卖环境下的最优探索策略尚不明确。

Method: 利用亚洲领先移动应用商店的数据，分析Thompson Sampling算法在第二价格拍卖中的表现，量化拍卖-老虎机组合模型下的探索收益，并提出基于关键词特性的定制化探索策略。

Result: 实证研究表明拍卖环境下的老虎机问题与传统设置存在显著差异，定制化探索策略能够在收益和效率方面实现帕累托前沿，为平台带来显著收益提升。

Conclusion: 针对赞助搜索拍卖的冷启动问题，需要根据拍卖环境的特殊性设计定制化的探索策略，基于关键词特性调整探索水平可以同时优化平台收益和效率指标。

Abstract: Sponsored search positions are typically allocated through real-time
auctions, where the outcomes depend on advertisers' quality-adjusted bids - the
product of their bids and quality scores. Although quality scoring helps
promote ads with higher conversion outcomes, setting these scores for new
advertisers in any given market is challenging, leading to the cold-start
problem. To address this, platforms incorporate multi-armed bandit algorithms
in auctions to balance exploration and exploitation. However, little is known
about the optimal exploration strategies in such auction environments. We
utilize data from a leading Asian mobile app store that places sponsored ads
for keywords. The platform employs a Thompson Sampling algorithm within a
second-price auction to learn quality scores and allocate a single sponsored
position for each keyword. We empirically quantify the gains from optimizing
exploration under this combined auction-bandit model and show that this problem
differs substantially from the canonical bandit problem. Drawing on these
empirical insights, we propose a customized exploration strategy in which the
platform adjusts the exploration levels for each keyword according to its
characteristics. We derive the Pareto frontier for revenue and efficiency and
provide actionable policies, demonstrating substantial gains for the platform
on both metrics when using a tailored exploration approach.

</details>


### [100] [A Soft Inducement Framework for Incentive-Aided Steering of No-Regret Players](https://arxiv.org/abs/2508.21672)
*Asrin Efe Yorulmaz,Raj Kiriti Velicheti,Melih Bastopcu,Tamer Başar*

Main category: cs.GT

TL;DR: 本文研究中介增强的两人正规形式博弈中的引导问题，中介通过信息和激励设计引导玩家达到特定行动配置。研究发现单纯信息设计或次线性支付方案无法实现所有期望目标，提出了结合一次性信息设计和重复博弈的新方法，提高了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 研究如何在有中介的博弈中有效引导玩家行为，解决单纯信息设计或有限激励无法实现所有期望行动配置的问题。

Method: 采用中介增强的两人正规形式博弈框架，结合信息设计和激励支付，提出一次性信息设计阶段将重复博弈转化为Stackelberg博弈的方法。

Result: 建立了成功引导的条件特征，推导了实现目标所需的最小常数支付下界，理论证明新方法能以常数因子提高收敛速度，并通过实证结果支持。

Conclusion: 提出的结合一次性信息设计和重复博弈的方法有效解决了单纯信息设计的局限性，显著提高了玩家行动配置向目标点收敛的效率。

Abstract: In this work, we investigate a steering problem in a mediator-augmented
two-player normal-form game, where the mediator aims to guide players toward a
specific action profile through information and incentive design. We first
characterize the games for which successful steering is possible. Moreover, we
establish that steering players to any desired action profile is not always
achievable with information design alone, nor when accompanied with sublinear
payment schemes. Consequently, we derive a lower bound on the constant payments
required per round to achieve this goal. To address these limitations incurred
with information design, we introduce an augmented approach that involves a
one-shot information design phase before the start of the repeated game,
transforming the prior interaction into a Stackelberg game. Finally, we
theoretically demonstrate that this approach improves the convergence rate of
players' action profiles to the target point by a constant factor with high
probability, and support it with empirical results.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [101] [ORCA: ORchestrating Causal Agent](https://arxiv.org/abs/2508.21304)
*Joanie Hayoun Chung,Chaemyung Lim,Sumin Lee,Sungbin Lim*

Main category: cs.DB

TL;DR: ORCA是一个基于LLM的智能代理系统，可自动化关系数据库中的因果推断工作流，通过人机交互保持专家监督，在表理解、查询生成和因果效应估计方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模增长，因果推断的数据分析工作流变得复杂，非专家在关系数据库中执行工作流会导致重复性瓶颈，阻碍及时的业务洞察。

Method: ORCA是一个LLM代理系统，能够自动化RDBMS中的常规工作流，包括：解释自然语言查询、导航数据库表、生成SQL代码、数据预处理以及使用因果推断库配置建模过程。

Result: 在基准和合成电商数据集上的实证评估显示，ORCA在表理解、查询生成和因果效应估计方面具有竞争优势，在估计平均处理效应方面比GPT-4o mini提高了7倍以上。

Conclusion: ORCA系统能够通过人机交互实现自动化因果推断工作流，使领域专家能够以较少的技术专业知识进行稳健的数据驱动决策。

Abstract: Causal inference is essential for decision-making science while the
complexity of the data analysis workflow, ranging from data wrangling to causal
analysis, increases substantially as the scale of data grows in complicated
business environments. Especially, the execution of the workflow in relational
databases by non-experts can result in repetitive bottlenecks which impede
timely and responsible business insights. To address this challenge, we propose
ORCA (Orchestrating Causal Agent), an LLM agentic system that can automate
routine workflows in RDBMS while preserving expert oversight via human-AI
interactions. ORCA orchestrates the full data analysis pipeline: interpreting
natural language queries, navigating tables from DB servers, generating proper
SQL codes, preprocessing data, and configuring modeling processes using causal
inference libraries. Domain experts still can control the automation through
iterative interactions with ORCA, enabling robust data-driven decision making
with less technical expertise in statistical computing. Empirical evaluations
on benchmark and synthetic e-commerce datasets demonstrate competitive
performance of ORCA in table understanding, query generation, and cause-effect
estimation -- achieving over $7\times$ improvement in estimating average
treatment compared to GPT-4o mini.

</details>


### [102] [Hilbert Forest in the SISAP 2025 Indexing Challenge](https://arxiv.org/abs/2508.21682)
*Yasunobu Imamura,Takeshi Shinohara,Naoya Higuchi,Kouichi Hirata,Tetsuji Kuboyama*

Main category: cs.DB

TL;DR: 提出基于希尔伯特空间填充曲线的Hilbert forest索引方法，在SISAP 2025挑战赛中表现出色，在严格内存限制下实现了竞争性的近似最近邻搜索性能


<details>
  <summary>Details</summary>
Motivation: 解决高维数据在严格内存限制（16GB RAM，8 CPU核心）下的高效索引和近似最近邻搜索问题

Method: 使用快速希尔伯特排序算法对高维点沿希尔伯特空间填充曲线排序，构建多个Hilbert树来支持近似最近邻搜索

Result: 在Task 1（PUBMED23数据集近似搜索）中表现竞争性，在Task 2（GOOAQ数据集k近邻图构建）中达到最快构建时间且满足召回率要求

Conclusion: 希尔伯特顺序索引方法在严格内存限制下具有实际有效性

Abstract: We report our participation in the SISAP 2025 Indexing Challenge using a
novel indexing technique called the Hilbert forest. The method is based on the
fast Hilbert sort algorithm, which efficiently orders high-dimensional points
along a Hilbert space-filling curve, and constructs multiple Hilbert trees to
support approximate nearest neighbor search. We submitted implementations to
both Task 1 (approximate search on the PUBMED23 dataset) and Task 2 (k-nearest
neighbor graph construction on the GOOAQ dataset) under the official resource
constraints of 16 GB RAM and 8 CPU cores. The Hilbert forest demonstrated
competitive performance in Task 1 and achieved the fastest construction time in
Task 2 while satisfying the required recall levels. These results highlight the
practical effectiveness of Hilbert order-based indexing under strict memory
limitations.

</details>
