{"id": "2510.18062", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.18062", "abs": "https://arxiv.org/abs/2510.18062", "authors": ["Reshef Meir", "Ganesh Ghalme"], "title": "On Condorcet's Jury Theorem with Abstention", "comment": null, "summary": "The well-known Condorcet Jury Theorem states that, under majority rule, the\nbetter of two alternatives is chosen with probability approaching one as the\npopulation grows. We study an asymmetric setting where voters face varying\nparticipation costs and share a possibly heuristic belief about their\npivotality (ability to influence the outcome).\n  In a costly voting setup where voters abstain if their participation cost is\ngreater than their pivotality estimate, we identify a single property of the\nheuristic belief -- weakly vanishing pivotality -- that gives rise to multiple\nstable equilibria in which elections are nearly tied. In contrast, strongly\nvanishing pivotality (as in the standard Calculus of Voting model) yields a\nunique, trivial equilibrium where only zero-cost voters participate as the\npopulation grows. We then characterize when nontrivial equilibria satisfy a\nversion of the Jury Theorem: below a sharp threshold, the majority-preferred\ncandidate wins with probability approaching one; above it, both candidates\neither win with equal probability."}
{"id": "2510.18567", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.18567", "abs": "https://arxiv.org/abs/2510.18567", "authors": ["Yiding Feng", "Mengfan Ma", "Bo Peng", "Zongqi Wan"], "title": "Contextual Search in Principal-Agent Games: The Curse of Degeneracy", "comment": null, "summary": "In this work, we introduce and study contextual search in general\nprincipal-agent games, where a principal repeatedly interacts with agents by\noffering contracts based on contextual information and historical feedback,\nwithout knowing the agents' true costs or rewards. Our model generalizes\nclassical contextual pricing by accommodating richer agent action spaces. Over\n$T$ rounds with $d$-dimensional contexts, we establish an asymptotically tight\nexponential $T^{1 - \\Theta(1/d)}$ bound in terms of the pessimistic Stackelberg\nregret, benchmarked against the best utility for the principal that is\nconsistent with the observed feedback.\n  We also establish a lower bound of $\\Omega(T^{\\frac{1}{2}-\\frac{1}{2d}})$ on\nthe classic Stackelberg regret for principal-agent games, demonstrating a\nsurprising double-exponential hardness separation from the contextual pricing\nproblem (a.k.a, the principal-agent game with two actions), which is known to\nadmit a near-optimal $O(d\\log\\log T)$ regret bound [Kleinberg and Leighton,\n2003, Leme and Schneider, 2018, Liu et al., 2021]. In particular, this\ndouble-exponential hardness separation occurs even in the special case with\nthree actions and two-dimensional context. We identify that this significant\nincrease in learning difficulty arises from a structural phenomenon that we\ncall contextual action degeneracy, where adversarially chosen contexts can make\nsome actions strictly dominated (and hence unincentivizable), blocking the\nprincipal's ability to explore or learn about them, and fundamentally limiting\nlearning progress."}
{"id": "2510.18718", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.18718", "abs": "https://arxiv.org/abs/2510.18718", "authors": ["Qishen Han", "Biaoshuai Tao", "Lirong Xia", "Chengkai Zhang", "Houyu Zhou"], "title": "Likelihood of the Existence of Average Justified Representation", "comment": "Accepted in SODA'26", "summary": "We study the approval-based multi-winner election problem where $n$ voters\njointly decide a committee of $k$ winners from $m$ candidates. We focus on the\naxiom \\emph{average justified representation} (AJR) proposed by Fernandez,\nElkind, Lackner, Garcia, Arias-Fisteus, Basanta-Val, and Skowron (2017). AJR\npostulates that every group of voters with a common preference should be\nsufficiently represented in that their average satisfaction should be no less\nthan their Hare quota. Formally, for every group of\n$\\lceil\\ell\\cdot\\frac{n}{k}\\rceil$ voters with $\\ell$ common approved\ncandidates, the average number of approved winners for this group should be at\nleast $\\ell$. It is well-known that a winning committee satisfying AJR is not\nguaranteed to exist for all multi-winner election instances. In this paper, we\nstudy the likelihood of the existence of AJR under the Erd\\H{o}s--R\\'enyi\nmodel. We consider the Erd\\H{o}s--R\\'enyi model parameterized by $p\\in[0,1]$\nthat samples multi-winner election instances from the distribution where each\nvoter approves each candidate with probability $p$ (and the events that voters\napprove candidates are independent), and we provide a clean and complete\ncharacterization of the existence of AJR committees in the case where $m$ is a\nconstant and $n$ tends to infinity. We show that there are two phase transition\npoints $p_1$ and $p_2$ (with $p_1\\leq p_2$) for the parameter $p$ such that: 1)\nwhen $p<p_1$ or $p>p_2$, an AJR committee exists with probability $1-o(1)$, 2)\nwhen $p_1<p<p_2$, an AJR committee exists with probability $o(1)$, and 3) when\n$p=p_1$ or $p=p_2$, the probability that an AJR committee exists is bounded\naway from both $0$ and $1$."}
{"id": "2510.18224", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18224", "abs": "https://arxiv.org/abs/2510.18224", "authors": ["Jiangong Chen", "Mingyu Zhu", "Bin Li"], "title": "EVER: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation", "comment": null, "summary": "Mixed Reality (MR)-aided operation overlays digital objects on the physical\nworld to provide a more immersive and intuitive operation process. A primary\nchallenge is the precise and fast auto-verification of whether the user follows\nMR guidance by comparing frames before and after each operation. The\npre-operation frame includes virtual guiding objects, while the post-operation\nframe contains physical counterparts. Existing approaches fall short of\naccounting for the discrepancies between physical and virtual objects due to\nimperfect 3D modeling or lighting estimation. In this paper, we propose EVER:\nan edge-assisted auto-verification system for mobile MR-aided operations.\nUnlike traditional frame-based similarity comparisons, EVER leverages the\nsegmentation model and rendering pipeline adapted to the unique attributes of\nframes with physical pieces and those with their virtual counterparts; it\nadopts a threshold-based strategy using Intersection over Union (IoU) metrics\nfor accurate auto-verification. To ensure fast auto-verification and low energy\nconsumption, EVER offloads compute-intensive tasks to an edge server. Through\ncomprehensive evaluations of public datasets and custom datasets with practical\nimplementation, EVER achieves over 90% verification accuracy within 100\nmilliseconds (significantly faster than average human reaction time of\napproximately 273 milliseconds), while consuming only minimal additional\ncomputational resources and energy compared to a system without\nauto-verification."}
{"id": "2510.18029", "categories": ["cs.DB", "cs.AI", "H.2.3"], "pdf": "https://arxiv.org/pdf/2510.18029", "abs": "https://arxiv.org/abs/2510.18029", "authors": ["Aymane Hassini"], "title": "DynaQuery: A Self-Adapting Framework for Querying Structured and Multimodal Data", "comment": "15 pages, 2 figures, 10 tables. Source code and experimental\n  artifacts are available at: https://github.com/aymanehassini/DynaQuery . The\n  'DynaQuery-Eval-5K' benchmark, introduced in this work, is also publicly\n  available at:\n  https://www.kaggle.com/datasets/aymanehassini/dynaquery-eval-5k-benchmark", "summary": "The rise of Large Language Models (LLMs) has accelerated the long-standing\ngoal of enabling natural language querying over complex, hybrid databases. Yet,\nthis ambition exposes a dual challenge: reasoning jointly over structured,\nmulti-relational schemas and the semantic content of linked unstructured\nassets. To overcome this, we present DynaQuery - a unified, self-adapting\nframework that serves as a practical blueprint for next-generation \"Unbound\nDatabases.\" At the heart of DynaQuery lies the Schema Introspection and Linking\nEngine (SILE), a novel systems primitive that elevates schema linking to a\nfirst-class query planning phase. We conduct a rigorous, multi-benchmark\nempirical evaluation of this structure-aware architecture against the prevalent\nunstructured Retrieval-Augmented Generation (RAG) paradigm. Our results\ndemonstrate that the unstructured retrieval paradigm is architecturally\nsusceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION,\nleading to unreliable query generation. In contrast, our SILE-based design\nestablishes a substantially more robust foundation, nearly eliminating this\nfailure mode. Moreover, end-to-end validation on a complex, newly curated\nbenchmark uncovers a key generalization principle: the transition from pure\nschema-awareness to holistic semantics-awareness. Taken together, our findings\nprovide a validated architectural basis for developing natural language\ndatabase interfaces that are robust, adaptable, and predictably consistent."}
{"id": "2510.17888", "categories": ["cs.DS", "math.OC", "Primary 90C10, Secondary 90C27, 90B06, 90C57"], "pdf": "https://arxiv.org/pdf/2510.17888", "abs": "https://arxiv.org/abs/2510.17888", "authors": ["Qilong Yuan"], "title": "Assignment-Routing Optimization with Cutting-Plane Subtour Elimination: Solver and Benchmark Dataset", "comment": "8 pages, 4 figures", "summary": "We study a joint routing-assignment optimization problem in which a set of\nitems must be paired one-to-one with a set of placeholders while simultaneously\ndetermining a Hamiltonian cycle that visits every node exactly once. Both the\nassignment and routing decisions are optimized jointly to minimize the total\ntravel cost. In this work, we propose a method to solve this problem using an\nexact MIP formulation with Gurobi, including cutting-plane subtour elimination.\nWith analysis of the computational complexity and through extensive\nexperiments, we analyze the computational limitations of this approach as the\nproblem size grows and reveal the challenges associated with the need for more\nefficient algorithms for larger instances. The dataset, formulations, and\nexperimental results provided here can serve as benchmarks for future studies\nin this research area. GitHub repository:\nhttps://github.com/QL-YUAN/Joint-Assignment-Routing-Optimization"}
{"id": "2510.18104", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18104", "abs": "https://arxiv.org/abs/2510.18104", "authors": ["Joeran Beel", "Bela Gipp", "Tobias Vente", "Moritz Baumgart", "Philipp Meister"], "title": "From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs", "comment": null, "summary": "Recommender-systems research has accelerated model and evaluation advances,\nyet largely neglects automating the research process itself. We argue for a\nshift from narrow AutoRecSys tools -- focused on algorithm selection and\nhyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab\n(AutoRecLab) that integrates end-to-end automation: problem ideation,\nliterature analysis, experimental design and execution, result interpretation,\nmanuscript drafting, and provenance logging. Drawing on recent progress in\nautomated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems),\nwe outline an agenda for the RecSys community: (1) build open AutoRecLab\nprototypes that combine LLM-driven ideation and reporting with automated\nexperimentation; (2) establish benchmarks and competitions that evaluate agents\non producing reproducible RecSys findings with minimal human input; (3) create\nreview venues for transparently AI-generated submissions; (4) define standards\nfor attribution and reproducibility via detailed research logs and metadata;\nand (5) foster interdisciplinary dialogue on ethics, governance, privacy, and\nfairness in autonomous research. Advancing this agenda can increase research\nthroughput, surface non-obvious insights, and position RecSys to contribute to\nemerging Artificial Research Intelligence. We conclude with a call to organise\na community retreat to coordinate next steps and co-author guidance for the\nresponsible integration of automated research systems."}
{"id": "2510.17841", "categories": ["cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.17841", "abs": "https://arxiv.org/abs/2510.17841", "authors": ["Ishir Rao"], "title": "Information Capacity of EEG: Theoretical and Computational Limits of Recoverable Neural Information", "comment": "3 pages, 5 figures", "summary": "Electroencephalography (EEG) is widely used to study human brain dynamics,\nyet its quantitative information capacity remains unclear. Here, we combine\ninformation theory and synthetic forward modeling to estimate the mutual\ninformation between latent cortical sources and EEG recordings. Using\nGaussian-channel theory and empirical simulations, we find that scalp EEG\nconveys only tens of bits per sample about low-dimensional neural activity.\nInformation saturates with approximately 64-128 electrodes and scales\nlogarithmically with signal-to-noise ratio (SNR). Linear decoders capture\nnearly all variance that is linearly recoverable, but the mutual information\nthey recover remains far below the analytic channel capacity, indicating that\nmeasurement physics - not algorithmic complexity - is the dominant limitation.\nThese results outline the intrinsic ceiling on how much structure about brain\nstate or thought content can be inferred from EEG."}
{"id": "2510.18409", "categories": ["cs.MM", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.18409", "abs": "https://arxiv.org/abs/2510.18409", "authors": ["Yuheng Wu", "Thanh-Tung Nguyen", "Lucas Liebe", "Quang Tau", "Pablo Espinosa Campos", "Jinghan Cheng", "Dongman Lee"], "title": "How2Compress: Scalable and Efficient Edge Video Analytics via Adaptive Granular Video Compression", "comment": "MM 2025", "summary": "With the rapid proliferation of the Internet of Things, video analytics has\nbecome a cornerstone application in wireless multimedia sensor networks. To\nsupport such applications under bandwidth constraints, learning-based adaptive\nquantization for video compression have demonstrated strong potential in\nreducing bitrate while maintaining analytical accuracy. However, existing\nframeworks often fail to fully exploit the fine-grained quality control enabled\nby modern blockbased video codecs, leaving significant compression efficiency\nuntapped.\n  In this paper, we present How2Compress, a simple yet effective framework\ndesigned to enhance video compression efficiency through precise, fine-grained\nquality control at the macroblock level. How2Compress is a plug-and-play module\nand can be seamlessly integrated into any existing edge video analytics\npipelines. We implement How2Compress on the H.264 codec and evaluate its\nperformance across diverse real-world scenarios. Experimental results show that\nHow2Compress achieves up to $50.4\\%$ bitrate savings and outperforms baselines\nby up to $3.01\\times$ without compromising accuracy, demonstrating its\npractical effectiveness and efficiency. Code is available at\nhttps://github.com/wyhallenwu/how2compress and a reproducible docker image at\nhttps://hub.docker.com/r/wuyuheng/how2compress."}
{"id": "2510.18049", "categories": ["cs.DS", "68W27", "F.2.0"], "pdf": "https://arxiv.org/pdf/2510.18049", "abs": "https://arxiv.org/abs/2510.18049", "authors": ["Allan Borodin", "Christodoulos Karavasilis", "David Zhang"], "title": "Online Randomness Extraction: Simulating Barely Random Algorithms in the Random Order Arrival Model", "comment": null, "summary": "Interest in the random order model (ROM) leads us to initiate a study of\nutilizing random-order arrivals to extract random bits with the goal of\nde-randomizing algorithms. Besides producing simple algorithms, simulating\nrandom bits through random arrivals enhances our understanding of the\ncomparative strength of randomized online algorithms (with adversarial input\nsequence) and deterministic algorithms in the ROM. We consider three $1$-bit\nrandomness extraction processes. Our best extraction process returns a bit with\na worst-case bias of $2 - \\sqrt{2} \\approx 0.585$ and operates under the mild\nassumption that there exist at least two distinct items in the input. We\nmotivate the applicability of this process by using it to simulate a number of\nbarely random algorithms for weighted interval selection (single-length\narbitrary weights, as well as monotone, C-benevolent and D-benevolent weighted\ninstances), the proportional and general knapsack problems, binary string\nguessing, and unweighted job throughput scheduling.\n  It is well known that there are many applications where a deterministic ROM\nalgorithm significantly outperforms any randomized online algorithm (in terms\nof competitive ratios). The classic example is that of the secretary problem.\nWe ask the following fundamental question: Is there any application for which a\nrandomized algorithm outperforms any deterministic ROM algorithm? Motivated by\nthis question, we view our randomness extraction applications as a constructive\napproach towards understanding the relation between randomized online\nalgorithms and deterministic ROM algorithms."}
{"id": "2510.18239", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18239", "abs": "https://arxiv.org/abs/2510.18239", "authors": ["Yunjiang Jiang", "Ayush Agarwal", "Yang Liu", "Bi Xue"], "title": "LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling", "comment": "16 pages", "summary": "Scaling large recommendation systems requires advancing three major\nfrontiers: processing longer user histories, expanding candidate sets, and\nincreasing model capacity. While promising, transformers' computational cost\nscales quadratically with the user sequence length and linearly with the number\nof candidates. This trade-off makes it prohibitively expensive to expand\ncandidate sets or increase sequence length at inference, despite the\nsignificant performance improvements.\n  We introduce \\textbf{LIME}, a novel architecture that resolves this\ntrade-off. Through two key innovations, LIME fundamentally reduces\ncomputational complexity. First, low-rank ``link embeddings\" enable\npre-computation of attention weights by decoupling user and candidate\ninteractions, making the inference cost nearly independent of candidate set\nsize. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the\ncomplexity with respect to user sequence length from quadratic ($O(N^2)$) to\nlinear ($O(N)$).\n  Experiments on public and industrial datasets show LIME achieves near-parity\nwith state-of-the-art transformers but with a 10$\\times$ inference speedup on\nlarge candidate sets or long sequence lengths. When tested on a major\nrecommendation platform, LIME improved user engagement while maintaining\nminimal inference costs with respect to candidate set size and user history\nlength, establishing a new paradigm for efficient and expressive recommendation\nsystems."}
{"id": "2510.18440", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.18440", "abs": "https://arxiv.org/abs/2510.18440", "authors": ["Bach Hung Luu", "Samuel Harry Gardner", "Sinh Cong Lam", "Trong Minh Hoang"], "title": "Performance of Modified Fractional Frequency Reuse Algorithm in Random Ultra Dense Networks", "comment": null, "summary": "Mitigating intercell interference by employing fractional frequency reuse\nalgorithms is one of the important approaches to improving user performance in\n5G and Beyond 5G cellular network systems, which typically have a high density\nof Base Stations (BSs). While most frequency reuse algorithms are based on the\ndownlink Signal-to-Interference-plus-Noise Ratio (SINR) or the distance between\nthe user and its serving BS to classify Cell-Edge Users (CEUs) and Cell-Center\nUsers (CCUs), this paper discusses a modified algorithm that uses the power\nratio between the signal strengths from the serving BS and the second nearest\nBS for user classification. Specifically, if the power ratio is below a\npredefined threshold, the user is classified as a CEU and is served with higher\ntransmission power. Simulation results show that increasing transmission power\nis necessary to enhance CEU performance, but it also degrades the performance\nof typical users. The use of frequency reuse algorithms is particularly\nfeasible in environments with a high density of obstacles, where intercell\ninterference can be effectively suppressed."}
{"id": "2510.18459", "categories": ["cs.MM", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.18459", "abs": "https://arxiv.org/abs/2510.18459", "authors": ["Tong Liu", "Zhiwei Fan", "Guanyan Peng", "Haodan Zhang", "Yucheng Zhang", "Zhen Wang", "Pengjin Xie", "Liang Liu"], "title": "DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation", "comment": null, "summary": "Short video streaming has become a dominant paradigm in digital media,\ncharacterized by rapid swiping interactions and diverse media content. A key\ntechnical challenge is designing an effective preloading strategy that\ndynamically selects and prioritizes download tasks from an evolving playlist,\nbalancing Quality of Experience (QoE) and bandwidth efficiency under practical\ncommercial constraints. However, real world analysis reveals critical\nlimitations of existing approaches: (1) insufficient adaptation of download\ntask sizes to dynamic conditions, and (2) watch time prediction models that are\ndifficult to deploy reliably at scale. In this paper, we propose DeLoad, a\nnovel preloading framework that addresses these issues by introducing dynamic\ntask sizing and a practical, multi dimensional watch time estimation method.\nAdditionally, a Deep Reinforcement Learning (DRL) enhanced agent is trained to\noptimize the download range decisions adaptively. Extensive evaluations\nconducted on an offline testing platform, leveraging massive real world network\ndata, demonstrate that DeLoad achieves significant improvements in QoE metrics\n(34.4% to 87.4% gain). Furthermore, after deployment on a large scale\ncommercial short video platform, DeLoad has increased overall user watch time\nby 0.09% while simultaneously reducing rebuffering events and 3.76% bandwidth\nconsumption."}
{"id": "2510.18057", "categories": ["cs.DS", "cs.CG", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.18057", "abs": "https://arxiv.org/abs/2510.18057", "authors": ["Talya Eden", "Ludmila Glinskih", "Sofya Raskhodnikova"], "title": "Fast Agnostic Learners in the Plane", "comment": null, "summary": "We investigate the computational efficiency of agnostic learning for several\nfundamental geometric concept classes in the plane. While the sample complexity\nof agnostic learning is well understood, its time complexity has received much\nless attention. We study the class of triangles and, more generally, the class\nof convex polygons with $k$ vertices for small $k$, as well as the class of\nconvex sets in a square. We present a proper agnostic learner for the class of\ntriangles that has optimal sample complexity and runs in time $\\tilde\nO({\\epsilon^{-6}})$, improving on the algorithm of Dobkin and Gunopulos (COLT\n`95) that runs in time $\\tilde O({\\epsilon^{-10}})$. For 4-gons and 5-gons, we\nimprove the running time from $O({\\epsilon^{-12}})$, achieved by Fischer and\nKwek (eCOLT `96), to $\\tilde O({\\epsilon^{-8}})$ and $\\tilde\nO({\\epsilon^{-10}})$, respectively.\n  We also design a proper agnostic learner for convex sets under the uniform\ndistribution over a square with running time $\\tilde O({\\epsilon^{-5}})$,\nimproving on the previous $\\tilde O(\\epsilon^{-8})$ bound at the cost of\nslightly higher sample complexity. Notably, agnostic learning of convex sets in\n$[0,1]^2$ under general distributions is impossible because this concept class\nhas infinite VC-dimension. Our agnostic learners use data structures and\nalgorithms from computational geometry and their analysis relies on tools from\ngeometry and probabilistic combinatorics. Because our learners are proper, they\nyield tolerant property testers with matching running times. Our results raise\na fundamental question of whether a gap between the sample and time complexity\nis inherent for agnostic learning of these and other natural concept classes."}
{"id": "2510.18277", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.18277", "abs": "https://arxiv.org/abs/2510.18277", "authors": ["Nikolaos Belibasakis", "Anastasios Giannaros", "Ioanna Giannoukou", "Spyros Sioutas"], "title": "Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization and Query-Driven Insights", "comment": null, "summary": "The increasing number of data a booking platform such as Booking.com and\nAirBnB offers make it challenging for interested parties to browse through the\navailable accommodations and analyze reviews in an efficient way. Efforts have\nbeen made from the booking platform providers to utilize recommender systems in\nan effort to enable the user to filter the results by factors such as stars,\namenities, cost but most valuable insights can be provided by the unstructured\ntext-based reviews. Going through these reviews one-by-one requires a\nsubstantial amount of time to be devoted while a respectable percentage of the\nreviews won't provide to the user what they are actually looking for.\n  This research publication explores how Large Language Models (LLMs) can\nenhance short rental apartments recommendations by summarizing and mining key\ninsights from user reviews. The web application presented in this paper, named\n\"instaGuide\", automates the procedure of isolating the text-based user reviews\nfrom a property on the Booking.com platform, synthesizing the summary of the\nreviews, and enabling the user to query specific aspects of the property in an\neffort to gain feedback on their personal questions/criteria.\n  During the development of the instaGuide tool, numerous LLM models were\nevaluated based on accuracy, cost, and response quality. The results suggest\nthat the LLM-powered summarization reduces significantly the amount of time the\nusers need to devote on their search for the right short rental apartment,\nimproving the overall decision-making procedure."}
{"id": "2510.18736", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.18736", "abs": "https://arxiv.org/abs/2510.18736", "authors": ["Laurent Bienvenu", "Hugo Gimbert", "Subin Pulari"], "title": "A Markov-Chain Characterization of Finite-State Dimension and a Generalization of Agafonov's Theorem", "comment": null, "summary": "Finite-state dimension quantifies the asymptotic rate of information in an\ninfinite sequence as perceived by finite automata. For a fixed alphabet, the\ninfinite sequences that have maximal finite-state dimension are exactly those\nthat are Borel normal, i.e., in which all words of any given length appear with\nthe same frequency. A theorem of Schnorr and Stimm (1972) shows that a real\nnumber is Borel normal if and only if, for every finite-state irreducible\nMarkov chain with fair transitions, when the chain is simulated using the\nbinary expansion of the given number, the empirical distribution of states\nconverges to its stationary distribution. In this paper we extend this\ncorrespondence beyond normal numbers. We show that the finite-state dimension\nof a sequence can be characterized in terms of the conditional Kullback-Leibler\ndivergence between the limiting distributions arising from the simulation of\nMarkov chains using the given sequence and their stationary distributions. This\nprovides a new information-theoretic characterization of finite-state dimension\nwhich generalizes the Schnorr-Stimm result.\n  As an application, we prove a generalization of Agafonov's theorem for normal\nnumbers. Agafonov's theorem states that a sequence is normal if and only if\nevery subsequence selected by a finite automaton is also normal. We extend this\nto arbitrary sequences by establishing a tight quantitative relationship\nbetween the finite-state dimension of a sequence and the finite-state\ndimensions of its automatic subsequences."}
{"id": "2510.18606", "categories": ["cs.MM", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18606", "abs": "https://arxiv.org/abs/2510.18606", "authors": ["Chunyu Qiao", "Tong Liu", "Yucheng Zhang", "Zhiwei Fan", "Pengjin Xie", "Zhen Wang", "Liang Liu"], "title": "PIRA: Pan-CDN Intra-video Resource Adaptation for Short Video Streaming", "comment": null, "summary": "In large scale short video platforms, CDN resource selection plays a critical\nrole in maintaining Quality of Experience (QoE) while controlling escalating\ntraffic costs. To better understand this phenomenon, we conduct in the wild\nnetwork measurements during video playback in a production short video system.\nThe results reveal that CDNs delivering higher average QoE often come at\ngreater financial cost, yet their connection quality fluctuates even within a\nsingle video underscoring a fundamental and dynamic trade off between QoE and\ncost. However, the problem of sustaining high QoE under cost constraints\nremains insufficiently investigated in the context of CDN selection for short\nvideo streaming. To address this, we propose PIRA, a dynamic resource selection\nalgorithm that optimizes QoE and cost in real time during video playback. PIRA\nformally integrating QoE and cost by a mathematical model, and introduce a\nintra video control theoretic CDN resource selection approach which can balance\nQoE and cost under network dynamics. To reduce the computation overheads, PIRA\nemploys state space pruning and adaptive parameter adjustment to efficiently\nsolve the high dimensional optimization problem. In large scale production\nexperiments involving 450,000 users over two weeks, PIRA outperforms the\nproduction baseline, achieving a 2.1% reduction in start up delay, 15.2%\nshorter rebuffering time, and 10% lower average unit traffic cost,\ndemonstrating its effectiveness in balancing user experience and financial cost\nat scale."}
{"id": "2510.18066", "categories": ["cs.DS", "math.CO", "05C05, 05C12, 05C40", "G.2.2"], "pdf": "https://arxiv.org/pdf/2510.18066", "abs": "https://arxiv.org/abs/2510.18066", "authors": ["Alicia Muth", "E. Dov Neimand"], "title": "A Generalization of Distance Domination", "comment": "NA", "summary": "Expanding on the graph theoretic ideas of k-component order connectivity and\ndistance-l domination, we present a quadratic-complexity algorithm that finds a\ntree's minimum failure-set cardinality, i.e., the minimum cardinality any\nsubset of the tree's vertices must have so that all clusters of vertices\nfurther away than some l do not exceed a cardinality threshold. Applications of\nsolutions to the expanded problems include choosing service center locations so\nthat no large neighborhoods are excluded from service, while reducing the\nredundancy inherent in distance domination problems."}
{"id": "2510.18364", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18364", "abs": "https://arxiv.org/abs/2510.18364", "authors": ["Quim Motger", "Xavier Franch", "Vincenzo Gervasi", "Jordi Marco"], "title": "Evaluating LLM-Based Mobile App Recommendations: An Empirical Study", "comment": "Under review", "summary": "Large Language Models (LLMs) are increasingly used to recommend mobile\napplications through natural language prompts, offering a flexible alternative\nto keyword-based app store search. Yet, the reasoning behind these\nrecommendations remains opaque, raising questions about their consistency,\nexplainability, and alignment with traditional App Store Optimization (ASO)\nmetrics. In this paper, we present an empirical analysis of how widely-used\ngeneral purpose LLMs generate, justify, and rank mobile app recommendations.\nOur contributions are: (i) a taxonomy of 16 generalizable ranking criteria\nelicited from LLM outputs; (ii) a systematic evaluation framework to analyse\nrecommendation consistency and responsiveness to explicit ranking instructions;\nand (iii) a replication package to support reproducibility and future research\non AI-based recommendation systems. Our findings reveal that LLMs rely on a\nbroad yet fragmented set of ranking criteria, only partially aligned with\nstandard ASO metrics. While top-ranked apps tend to be consistent across runs,\nvariability increases with ranking depth and search specificity. LLMs exhibit\nvarying sensitivity to explicit ranking instructions - ranging from substantial\nadaptations to near-identical outputs - highlighting their complex reasoning\ndynamics in conversational app discovery. Our results aim to support end-users,\napp developers, and recommender-systems researchers in navigating the emerging\nlandscape of conversational app discovery."}
{"id": "2510.18496", "categories": ["cs.DS", "cs.IT", "cs.OS", "cs.PL", "math.IT", "E.1; F.3.2; H.0"], "pdf": "https://arxiv.org/pdf/2510.18496", "abs": "https://arxiv.org/abs/2510.18496", "authors": ["Anamitra Ghorui", "Uday P. Khedker"], "title": "LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations", "comment": "Author's original manuscript. 22 pages of main content. Submitted to\n  the Programming Journal (programming-journal.org)", "summary": "Analysis of entire programs as a single unit, or whole-program analysis,\ninvolves propagation of large amounts of information through the control flow\nof the program. This is especially true for pointer analysis, where, unless\nsignificant compromises are made in the precision of the analysis, there is a\ncombinatorial blowup of information. One of the key problems we observed in our\nown efforts is that a lot of duplicate data was being propagated, and many\nlow-level data structure operations were repeated a large number of times.\n  We present what we consider to be a novel and generic data structure,\nLatticeHashForest (LHF), to store and operate on such information in a manner\nthat eliminates a majority of redundant computations and duplicate data in\nscenarios similar to those encountered in compilers and program optimization.\nLHF differs from similar work in this vein, such as hash-consing, ZDDs, and\nBDDs, by not only providing a way to efficiently operate on large, aggregate\nstructures, but also modifying the elements of such structures in a manner that\nthey can be deduplicated immediately. LHF also provides a way to perform a\nnested construction of elements such that they can be deduplicated at multiple\nlevels, cutting down the need for additional, nested computations.\n  We provide a detailed structural description, along with an abstract model of\nthis data structure. An entire C++ implementation of LHF is provided as an\nartifact along with evaluations of LHF using examples and benchmark programs.\nWe also supply API documentation and a user manual for users to make\nindependent applications of LHF. Our main use case in the realm of pointer\nanalysis shows memory usage reduction to an almost negligible fraction, and\nspeedups beyond 4x for input sizes approaching 10 million when compared to\nother implementations."}
{"id": "2510.18129", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18129", "abs": "https://arxiv.org/abs/2510.18129", "authors": ["William Kuszmaul", "Jingxun Liang", "Renfei Zhou"], "title": "Fingerprint Filters Are Optimal", "comment": "23 pages, 2 figures, in FOCS 2025", "summary": "Dynamic filters are data structures supporting approximate membership queries\nto a dynamic set $S$ of $n$ keys, allowing a small false-positive error rate\n$\\varepsilon$, under insertions and deletions to the set $S$. Essentially all\nknown constructions for dynamic filters use a technique known as\nfingerprinting. This technique, which was first introduced by Carter et al. in\n1978, inherently requires $$\\log \\binom{n \\varepsilon^{-1}}{n} = n \\log\n\\varepsilon^{-1} + n \\log e - o(n)$$ bits of space when $\\varepsilon = o(1)$.\nWhether or not this bound is optimal for all dynamic filters (rather than just\nfor fingerprint filters) has remained for decades as one of the central open\nquestions in the area. We resolve this question by proving a sharp lower bound\nof $n \\log \\varepsilon^{-1} + n \\log e - o(n)$ bits for $\\varepsilon = o(1)$,\nregardless of operation time."}
{"id": "2510.18527", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.18527", "abs": "https://arxiv.org/abs/2510.18527", "authors": ["Hongru Song", "Yu-an Liu", "Ruqing Zhang", "Jiafeng Guo", "Maarten de Rijke", "Sen Li", "Wenjun Peng", "Fuyu Lv", "Xueqi Cheng"], "title": "LLMs as Sparse Retrievers:A Framework for First-Stage Product Search", "comment": "16 pages", "summary": "Product search is a crucial component of modern e-commerce platforms, with\nbillions of user queries every day. In product search systems, first-stage\nretrieval should achieve high recall while ensuring efficient online\ndeployment. Sparse retrieval is particularly attractive in this context due to\nits interpretability and storage efficiency. However, sparse retrieval methods\nsuffer from severe vocabulary mismatch issues, leading to suboptimal\nperformance in product search scenarios.With their potential for semantic\nanalysis, large language models (LLMs) offer a promising avenue for mitigating\nvocabulary mismatch issues and thereby improving retrieval quality. Directly\napplying LLMs to sparse retrieval in product search exposes two key\nchallenges:(1)Queries and product titles are typically short and highly\nsusceptible to LLM-induced hallucinations, such as generating irrelevant\nexpansion terms or underweighting critical literal terms like brand names and\nmodel numbers;(2)The large vocabulary space of LLMs leads to difficulty in\ninitializing training effectively, making it challenging to learn meaningful\nsparse representations in such ultra-high-dimensional spaces.To address these\nchallenges, we propose PROSPER, a framework for PROduct search leveraging LLMs\nas SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that\nalleviates hallucination in lexical expansion by reinforcing underweighted\nliteral terms through a residual compensation mechanism; and (2)A lexical\nfocusing window that facilitates effective training initialization via a\ncoarse-to-fine sparsification strategy.Extensive offline and online experiments\nshow that PROSPER significantly outperforms sparse baselines and achieves\nrecall performance comparable to advanced dense retrievers, while also\nachieving revenue increments online."}
{"id": "2510.18164", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.18164", "abs": "https://arxiv.org/abs/2510.18164", "authors": ["Harry Buhrman", "Sevag Gharibian", "Zeph Landau", "François Le Gall", "Norbert Schuch", "Suguru Tamaki"], "title": "A Simpler Exponential-Time Approximation Algorithm for MAX-k-SAT", "comment": "8 pages, accepted to SOSA 2026", "summary": "We present an extremely simple polynomial-space exponential-time\n$(1-\\varepsilon)$-approximation algorithm for MAX-k-SAT that is (slightly)\nfaster than the previous known polynomial-space $(1-\\varepsilon)$-approximation\nalgorithms by Hirsch (Discrete Applied Mathematics, 2003) and Escoffier,\nPaschos and Tourniaire (Theoretical Computer Science, 2014). Our algorithm\nrepeatedly samples an assignment uniformly at random until finding an\nassignment that satisfies a large enough fraction of clauses. Surprisingly, we\ncan show the efficiency of this simpler approach by proving that in any\ninstance of MAX-k-SAT (or more generally any instance of MAXCSP), an\nexponential number of assignments satisfy a fraction of clauses close to the\noptimal value."}
{"id": "2510.18177", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.18177", "abs": "https://arxiv.org/abs/2510.18177", "authors": ["Sepehr Assadi", "Janani Sundaresan", "Helia Yazdanyar"], "title": "Coloring Graphs with Few Colors in the Streaming Model", "comment": "89 pages, SODA 2026", "summary": "We study graph coloring problems in the streaming model, where the goal is to\nprocess an $n$-vertex graph whose edges arrive in a stream, using a limited\nspace that is smaller than the trivial $O(n^2)$ bound. While prior work has\nlargely focused on coloring graphs with a large number of colors, we explore\nthe opposite end of the spectrum: deciding whether the input graph can be\ncolored using only a few, say, a constant number of colors. We are interested\nin each of the adversarial, random order, or dynamic streams.\n  Our work lays the foundation for this new direction by establishing upper and\nlower bounds on space complexity of key variants of the problem. Some of our\nmain results include:\n  - Adversarial: for distinguishing between $q$- vs $2^{\\Omega(q)}$-colorable\ngraphs, lower bounds of $n^{2-o(1)}$ space for $q$ up to\n$(\\log{n})^{1/2-o(1)}$, and $n^{1+\\Omega(1/\\log\\log{n})}$ space for $q$ further\nup to $(\\log{n})^{1-o(1)}$.\n  - Random order: for distinguishing between $q$- vs $q^t$-colorable graphs for\n$q,t \\geq 2$, an upper bound of $\\tilde{O}(n^{1+1/t})$ space. Specifically,\ndistinguishing between $q$-colorable graphs vs ones that are not even\npoly$(q)$-colorable can be done in $n^{1+o(1)}$ space unlike in adversarial\nstreams. Although, distinguishing between $q$-colorable vs\n$\\Omega(q^2)$-colorable graphs requires $\\Omega(n^2)$ space even in random\norder streams for constant $q$.\n  - Dynamic: for distinguishing between $q$- vs $q \\cdot t$-colorable graphs\nfor any $q \\geq 3$ and $t \\geq 1$, nearly optimal upper and lower bounds of\n$\\tilde{\\Theta}(n^2/t^2)$ space.\n  We develop several new technical tools along the way: cluster packing graphs,\na generalization of Ruzsa-Szemer\\'edi graphs; a player elimination framework\nbased on cluster packing graphs; and new edge and vertex sampling lemmas\ntailored to graph coloring."}
{"id": "2510.18180", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18180", "abs": "https://arxiv.org/abs/2510.18180", "authors": ["Vincent Cohen-Addad", "David P. Woodruff", "Shenghao Xie", "Samson Zhou"], "title": "Nearly Space-Optimal Graph and Hypergraph Sparsification in Insertion-Only Data Streams", "comment": null, "summary": "We study the problem of graph and hypergraph sparsification in insertion-only\ndata streams. The input is a hypergraph $H=(V, E, w)$ with $n$ nodes, $m$\nhyperedges, and rank $r$, and the goal is to compute a hypergraph $\\widehat{H}$\nthat preserves the energy of each vector $x \\in \\mathbb{R}^n$ in $H$, up to a\nsmall multiplicative error. In this paper, we give a streaming algorithm that\nachieves a $(1+\\varepsilon)$-approximation, using $\\frac{rn}{\\varepsilon^2}\n\\log^2 n \\log r \\cdot\\text{poly}(\\log \\log m)$ bits of space, matching the\nsample complexity of the best known offline algorithm up to $\\text{poly}(\\log\n\\log m)$ factors. Our approach also provides a streaming algorithm for graph\nsparsification that achieves a $(1+\\varepsilon)$-approximation, using\n$\\frac{n}{\\varepsilon^2} \\log n \\cdot\\text{poly}(\\log\\log n)$ bits of space,\nimproving the current bound by $\\log n$ factors. Furthermore, we give a\nspace-efficient streaming algorithm for min-cut approximation. Along the way,\nwe present an online algorithm for $(1+\\varepsilon)$-hypergraph sparsification,\nwhich is optimal up to poly-logarithmic factors. As a result, we achieve\n$(1+\\varepsilon)$-hypergraph sparsification in the sliding window model, with\nspace optimal up to poly-logarithmic factors. Lastly, we give an adversarially\nrobust algorithm for hypergraph sparsification using $\\frac{n}{\\varepsilon^2}\n\\cdot\\text{poly}(r, \\log n, \\log r, \\log \\log m)$ bits of space."}
{"id": "2510.18237", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18237", "abs": "https://arxiv.org/abs/2510.18237", "authors": ["Yang Hu", "William Kuszmaul", "Jingxun Liang", "Huacheng Yu", "Junkai Zhang", "Renfei Zhou"], "title": "Static Retrieval Revisited: To Optimality and Beyond", "comment": "28 pages, in FOCS 2025", "summary": "In the static retrieval problem, a data structure must answer retrieval\nqueries mapping a set of $n$ keys in a universe $[U]$ to $v$-bit values.\nInformation-theoretically, retrieval data structures can use as little as $nv$\nbits of space. For small value sizes $v$, it is possible to achieve $O(1)$\nquery time while using space $nv + o(n)$ bits -- whether or not such a result\nis possible for larger values of $v$ (e.g., $v = \\Theta(\\log n)$) has remained\nopen.\n  In this paper, we obtain a tight lower bound (as well as matching upper\nbounds) for the static retrieval problem. In the case where values are large,\nwe show that there is actually a significant tension between time and space. It\nis not possible, for example, to get $O(1)$ query time using $nv + o(n)$ bits\nof space, when $v = \\Theta(\\log n)$ (and assuming the word RAM model with\n$O(\\log n)$-bit words).\n  At first glance, our lower bound would seem to render retrieval unusable in\nmany settings that aim to achieve very low redundancy. However, our second\nresult offers a way around this: We show that, whenever a retrieval data\nstructure $D_1$ is stored along with another data structure $D_2$ (whose size\nis similar to or larger than the size of $D_1$), it is possible to implement\nthe combined data structure $D_1 \\cup D_2$ so that queries to $D_1$ take $O(1)$\ntime, operations on $D_2$ take the same asymptotic time as if $D_2$ were stored\non its own, and the total space is $nv + \\mathrm{Space}(D_2) + n^{0.67}$ bits."}
{"id": "2510.18274", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18274", "abs": "https://arxiv.org/abs/2510.18274", "authors": ["Yonggang Jiang", "Danupon Nanongkai", "Pachara Sawettamalya"], "title": "Minimum $s$--$t$ Cuts with Fewer Cut Queries", "comment": null, "summary": "We study the problem of computing a minimum $s$--$t$ cut in an unweighted,\nundirected graph via \\emph{cut queries}. In this model, the input graph is\naccessed through an oracle that, given a subset of vertices $S \\subseteq V$,\nreturns the size of the cut $(S, V \\setminus S)$.\n  This line of work was initiated by Rubinstein, Schramm, and Weinberg (ITCS\n2018), who gave a randomized algorithm that computes a minimum $s$--$t$ cut\nusing $\\widetilde{O}(n^{5/3})$ queries, thereby showing that one can avoid\nspending $\\widetilde{\\Theta}(n^2)$ queries required to learn the entire graph.\nA recent result by Anand, Saranurak, and Wang (SODA 2025) also matched this\nupper bound via a deterministic algorithm based on blocking flows.\n  In this work, we present a new randomized algorithm that improves the\ncut-query complexity to $\\widetilde{O}(n^{8/5})$. At the heart of our approach\nis a query-efficient subroutine that incrementally reveals the graph\nedge-by-edge while increasing the maximum $s$--$t$ flow in the learned subgraph\nat a rate faster than classical augmenting-path methods. Notably, our algorithm\nis simple, purely combinatorial, and can be naturally interpreted as a\nrecursive greedy procedure.\n  As a further consequence, we obtain a \\emph{deterministic} and\n\\emph{combinatorial} two-party communication protocol for computing a minimum\n$s$--$t$ cut using $\\widetilde{O}(n^{11/7})$ bits of communication. This\nimproves upon the previous best bound of $\\widetilde{O}(n^{5/3})$, which was\nobtained via reductions from the aforementioned cut-query algorithms. In\nparallel, it has been observed that an $\\widetilde{O}(n^{3/2})$-bit randomized\nprotocol can be achieved via continuous optimization techniques; however, these\nmethods are fundamentally different from our combinatorial approach."}
{"id": "2510.18379", "categories": ["cs.DS", "cs.CR", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.18379", "abs": "https://arxiv.org/abs/2510.18379", "authors": ["Clément L. Canonne", "Abigail Gentle", "Vikrant Singhal"], "title": "Uniformity Testing under User-Level Local Privacy", "comment": null, "summary": "We initiate the study of distribution testing under \\emph{user-level} local\ndifferential privacy, where each of $n$ users contributes $m$ samples from the\nunknown underlying distribution. This setting, albeit very natural, is\nsignificantly more challenging that the usual locally private setting, as for\nthe same parameter $\\varepsilon$ the privacy guarantee must now apply to a full\nbatch of $m$ data points. While some recent work consider distribution\n\\emph{learning} in this user-level setting, nothing was known for even the most\nfundamental testing task, uniformity testing (and its generalization, identity\ntesting).\n  We address this gap, by providing (nearly) sample-optimal user-level LDP\nalgorithms for uniformity and identity testing. Motivated by practical\nconsiderations, our main focus is on the private-coin, symmetric setting, which\ndoes not require users to share a common random seed nor to have been assigned\na globally unique identifier."}
{"id": "2510.18393", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2510.18393", "abs": "https://arxiv.org/abs/2510.18393", "authors": ["Florian Hörsch", "Csaba Király", "Mirabel Mendoza-Cadena", "Gyula Pap", "Eszter Szabó", "Yutaro Yamaguchi"], "title": "Odd and Even Harder Problems on Cycle-Factors", "comment": "17 pages, 7 figures", "summary": "For a graph (undirected, directed, or mixed), a cycle-factor is a collection\nof vertex-disjoint cycles covering the entire vertex set. Cycle-factors subject\nto parity constraints arise naturally in the study of structural graph theory\nand algorithmic complexity. In this work, we study four variants of the problem\nof finding a cycle-factor subject to parity constraints: (1) all cycles are\nodd, (2) all cycles are even, (3) at least one cycle is odd, and (4) at least\none cycle is even. These variants are considered in the undirected, directed,\nand mixed settings. We show that all but the fourth problem are NP-complete in\nall settings, while the complexity of the fourth one remains open for the\ndirected and undirected cases. We also show that in mixed graphs, even deciding\nthe existence of any cycle factor is NP-complete."}
{"id": "2510.18496", "categories": ["cs.DS", "cs.IT", "cs.OS", "cs.PL", "math.IT", "E.1; F.3.2; H.0"], "pdf": "https://arxiv.org/pdf/2510.18496", "abs": "https://arxiv.org/abs/2510.18496", "authors": ["Anamitra Ghorui", "Uday P. Khedker"], "title": "LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations", "comment": "Author's original manuscript. 22 pages of main content. Submitted to\n  the Programming Journal (programming-journal.org)", "summary": "Analysis of entire programs as a single unit, or whole-program analysis,\ninvolves propagation of large amounts of information through the control flow\nof the program. This is especially true for pointer analysis, where, unless\nsignificant compromises are made in the precision of the analysis, there is a\ncombinatorial blowup of information. One of the key problems we observed in our\nown efforts is that a lot of duplicate data was being propagated, and many\nlow-level data structure operations were repeated a large number of times.\n  We present what we consider to be a novel and generic data structure,\nLatticeHashForest (LHF), to store and operate on such information in a manner\nthat eliminates a majority of redundant computations and duplicate data in\nscenarios similar to those encountered in compilers and program optimization.\nLHF differs from similar work in this vein, such as hash-consing, ZDDs, and\nBDDs, by not only providing a way to efficiently operate on large, aggregate\nstructures, but also modifying the elements of such structures in a manner that\nthey can be deduplicated immediately. LHF also provides a way to perform a\nnested construction of elements such that they can be deduplicated at multiple\nlevels, cutting down the need for additional, nested computations.\n  We provide a detailed structural description, along with an abstract model of\nthis data structure. An entire C++ implementation of LHF is provided as an\nartifact along with evaluations of LHF using examples and benchmark programs.\nWe also supply API documentation and a user manual for users to make\nindependent applications of LHF. Our main use case in the realm of pointer\nanalysis shows memory usage reduction to an almost negligible fraction, and\nspeedups beyond 4x for input sizes approaching 10 million when compared to\nother implementations."}
{"id": "2510.18722", "categories": ["cs.DS", "math.MG", "68R12, 30L05, 30L15, 46B85, 53C23, 05C82, 68R10", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.18722", "abs": "https://arxiv.org/abs/2510.18722", "authors": ["Alexandros Eskenazis", "Manor Mendel", "Assaf Naor"], "title": "An optimal algorithm for average distance in typical regular graphs", "comment": "To appear in the proceedings of the 37th ACM-SIAM Symposium on\n  Discrete Algorithms. The appendix contains the full version as it was\n  submitted to SODA because it contains complete proofs of all the new\n  statements that are covered herein. In a later posting we will remove that\n  appendix and post it as a standalone paper which includes further results and\n  applications to pure mathematics", "summary": "We design a deterministic algorithm that, given $n$ points in a\n\\emph{typical} constant degree regular~graph, queries $O(n)$ distances to\noutput a constant factor approximation to the average distance among those\npoints, thus answering a question posed in~\\cite{MN14}. Our algorithm uses the\nmethod of~\\cite{MN14} to construct a sequence of constant degree graphs that\nare expanders with respect to certain nonpositively curved metric spaces,\ntogether with a new rigidity theorem for metric transforms of nonpositively\ncurved metric spaces. The fact that our algorithm works for typical (uniformly\nrandom) constant degree regular graphs rather than for all constant degree\ngraphs is unavoidable, thanks to the following impossibility result that we\nobtain: For every fixed $k\\in \\N$, the approximation factor of any algorithm\nfor average distance that works for all constant degree graphs and queries\n$o(n^{1+1/k})$ distances must necessarily be at least $2(k+1)$. This matches\nthe upper bound attained by the algorithm that was designed for general finite\nmetric spaces in~\\cite{BGS}. Thus, any algorithm for average distance in\nconstant degree graphs whose approximation guarantee is less than $4$ must\nquery $\\Omega(n^2)$ distances, any such algorithm whose approximation guarantee\nis less than $6$ must query $\\Omega(n^{3/2})$ distances, any such algorithm\nwhose approximation guarantee less than $8$ must query $\\Omega(n^{4/3})$\ndistances, and so forth, and furthermore there exist algorithms achieving those\nparameters."}
