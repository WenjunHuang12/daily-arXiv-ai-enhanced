{"id": "2601.15758", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.15758", "abs": "https://arxiv.org/abs/2601.15758", "authors": ["Xieyang Wang", "Mengyi Liu", "Weijia Yi", "Jianqiu Xu", "Raymond Chi-Wing Wong"], "title": "NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases", "comment": null, "summary": "The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ."}
{"id": "2601.15992", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.15992", "abs": "https://arxiv.org/abs/2601.15992", "authors": ["Shidan Ma", "Peng Peng", "Xu Zhou", "M. Tamer Özsu", "Lei Zou", "Guo Chen"], "title": "Efficient Cloud-edge Collaborative Approaches to SPARQL Queries over Large RDF graphs", "comment": null, "summary": "With the increasing use of RDF graphs, storing and querying such data using SPARQL remains a critical problem. Current mainstream solutions rely on cloud-based data management architectures, but often suffer from performance bottle- necks in environments with limited bandwidth or high system load. To address this issue, this paper explores for the first time the integration of edge computing to move graph data storage and processing to edge environments, thereby improving query performance. This approach requires offloading query processing to edge servers, which involves addressing two challenges: data localization and network scheduling. First, the data localization challenge lies in computing the subgraphs maintained on edge servers to quickly identify the servers that can handle specific queries. To address this challenge, we introduce a new concept of pattern-induced subgraphs. Second, the network scheduling challenge involves efficiently assigning queries to edge and cloud servers to optimize overall system performance. We tackle this by constructing a overall system model that jointly captures data distribution, query characteristics, network communication, and computational resources. Accordingly, we further propose a joint formulation of query assignment and computational resource allocation, modeling it as a Mixed Integer Nonlinear Programming (MINLP) problem and solve this problem using a modified branch-and-bound algorithm. Experimental results on real datasets under a real cloud platform demonstrate that our proposed method outperforms the state-of-the-art baseline methods in terms of efficiency. The codes are available on GitHub"}
{"id": "2601.16025", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16025", "abs": "https://arxiv.org/abs/2601.16025", "authors": ["Yajuan Xu", "Xixian Han", "Xiaolong Wan"], "title": "EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery", "comment": null, "summary": "Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery."}
{"id": "2601.15312", "categories": ["cs.GT", "cs.AI", "cs.CL", "cs.CY", "cs.HC", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.15312", "abs": "https://arxiv.org/abs/2601.15312", "authors": ["Paweł Niszczota", "Elia Antoniou"], "title": "Do people expect different behavior from large language models acting on their behalf? Evidence from norm elicitations in two canonical economic games", "comment": null, "summary": "While delegating tasks to large language models (LLMs) can save people time, there is growing evidence that offloading tasks to such models produces social costs. We use behavior in two canonical economic games to study whether people have different expectations when decisions are made by LLMs acting on their behalf instead of themselves. More specifically, we study the social appropriateness of a spectrum of possible behaviors: when LLMs divide resources on our behalf (Dictator Game and Ultimatum Game) and when they monitor the fairness of splits of resources (Ultimatum Game). We use the Krupka-Weber norm elicitation task to detect shifts in social appropriateness ratings. Results of two pre-registered and incentivized experimental studies using representative samples from the UK and US (N = 2,658) show three key findings. First, people find that offers from machines - when no acceptance is necessary - are judged to be less appropriate than when they come from humans, although there is no shift in the modal response. Second - when acceptance is necessary - it is more appropriate for a person to reject offers from machines than from humans. Third, receiving a rejection of an offer from a machine is no less socially appropriate than receiving the same rejection from a human. Overall, these results suggest that people apply different norms for machines deciding on how to split resources but are not opposed to machines enforcing the norms. The findings are consistent with offers made by machines now being viewed as having both a cognitive and emotional component."}
{"id": "2601.15470", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.15470", "abs": "https://arxiv.org/abs/2601.15470", "authors": ["Shuchi Chawla", "Kristin Sheridan"], "title": "Nested and outlier embeddings into trees", "comment": null, "summary": "In this paper, we consider outlier embeddings into HSTs and ultrametrics. In particular, for $(X,d)$, let $k$ be the size of the smallest subset of $X$ such that all but that subset (i.e. the ``outlier set'') can be probabilistically embedded into the space of HSTs with expected distortion at most $c$. Our primary result is showing that there exists an efficient algorithm that takes in $(X,d)$ and a target distortion $c$ and samples from a probabilistic embedding with at most $O(\\frac k ε\\log^2k)$ outliers and distortion at most $(32+ε)c$, for any $ε>0$. This leads to better instance-specific approximations for certain instances of the buy-at-bulk and dial-a-ride problems, whose current best approximation algorithms go through HST embeddings.\n  In order to facilitate our results, we largely focus on the concept of compositions of nested embeddings introduced by [Chawla and Sheridan 2024]. A nested embedding is a composition of two embeddings of a metric space $(X,d)$ -- a low distortion embedding of a subset $S$ of nodes, and a higher distortion embedding of the entire metric. The composition is a single embedding that preserves the low distortion over $S$ and does not increase distortion over the remaining points by much. In this paper, we expand this concept from the setting of deterministic embeddings to the setting of probabilistic embeddings. We show how to find good nested compositions of embeddings into HSTs, and combine this with an approximation algorithm of [Munagala et al. 2023] to obtain our results."}
{"id": "2601.15484", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15484", "abs": "https://arxiv.org/abs/2601.15484", "authors": ["Philipp Eibl", "Erica Coppolillo", "Simone Mungari", "Luca Luceri"], "title": "Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics", "comment": null, "summary": "Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available."}
{"id": "2601.15404", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15404", "abs": "https://arxiv.org/abs/2601.15404", "authors": ["Arman Fazeli", "Mohammad M. Mansour", "Ziyuan Zhu", "Louay Jalloul"], "title": "Partially Polarized Polar Codes: A New Design for 6G Control Channels", "comment": "15 pages, 7 figures, accepted for publication in IEEE ICC 2026. Final camera-ready version forthcoming", "summary": "We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes."}
{"id": "2601.15318", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.15318", "abs": "https://arxiv.org/abs/2601.15318", "authors": ["Pedro García-Segador", "Michel Grabisch", "Dylan Laplace Mermoud", "Pedro Miranda"], "title": "On the closest balanced game", "comment": null, "summary": "Cooperative games with nonempty core are called balanced, and the set of balanced games is a polyhedron. Given a game with empty core, we look for the closest balanced game, in the sense of the (weighted) Euclidean distance, i.e., the orthogonal projection of the game on the set of balanced games. Besides an analytical approach which becomes rapidly intractable, we propose a fast algorithm to find the closest balanced game, avoiding exponential complexity for the optimization problem, and being able to run up to 20 players. We show experimentally that the probability that the closest game has a core reduced to a singleton tends to 1 when the number of players grow. We provide a mathematical proof that the proportion of facets whose games have a non-singleton core tends to 0 when the number of players grow, by finding an expression of the aymptotic growth of the number of minimal balanced collections. This permits to prove mathematically the experimental result. Consequently, taking the core of the projected game defines a new solution concept, which we call least square core due to its analogy with the least core, and our result shows that the probability that this is a point solution tends to 1 when the number of players grow."}
{"id": "2601.15682", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.15682", "abs": "https://arxiv.org/abs/2601.15682", "authors": ["Wei Dong", "Li Ge"], "title": "Tight Bounds for Gaussian Mean Estimation under Personalized Differential Privacy", "comment": null, "summary": "We study mean estimation for Gaussian distributions under \\textit{personalized differential privacy} (PDP), where each record has its own privacy budget. PDP is commonly considered in two variants: \\textit{bounded} and \\textit{unbounded} PDP. In bounded PDP, the privacy budgets are public and neighboring datasets differ by replacing one record. In unbounded PDP, neighboring datasets differ by adding or removing a record; consequently, an algorithm must additionally protect participation information, making both the dataset size and the privacy profile sensitive. Existing works have only studied mean estimation over bounded distributions under bounded PDP. Different from mean estimation for distributions with bounded range, where each element can be treated equally and we only need to consider the privacy diversity of elements, the challenge for Gaussian is that, elements can have very different contributions due to the unbounded support. we need to jointly consider the privacy information and the data values. Such a problem becomes even more challenging under unbounded PDP, where the privacy information is protected and the way to compute the weights becomes unclear. In this paper, we address these challenges by proposing optimal Gaussian mean estimators under both bounded and unbounded PDP, where in each setting we first derive lower bounds for both problems, following PDP mean estimators with the algorithmic upper bounds matching the corresponding lower bounds up to logarithmic factors."}
{"id": "2601.15518", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15518", "abs": "https://arxiv.org/abs/2601.15518", "authors": ["Wenxin Zhou", "Ritesh Mehta", "Anthony Miyaguchi"], "title": "DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking", "comment": "Paper submitted to TREC 2025 (34th Text REtrieval Conference)", "summary": "We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval."}
{"id": "2601.15464", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.15464", "abs": "https://arxiv.org/abs/2601.15464", "authors": ["Alessandro Neri", "Ferdinando Zullo"], "title": "Rank-metric codes over arbitrary fields: Bounds and constructions", "comment": null, "summary": "Rank-metric codes, defined as sets of matrices over a finite field with the rank distance, have gained significant attention due to their applications in network coding and connections to diverse mathematical areas. Initially studied by Delsarte in 1978 and later rediscovered by Gabidulin, these codes have become a central topic in coding theory. This paper surveys the development and mathematical foundations, in particular, regarding bounds and constructions of rank-metric codes, emphasizing their extension beyond finite fields to more general settings. We examine Singleton-like bounds on code parameters, demonstrating their sharpness in finite field cases and contrasting this with contexts where the bounds are not tight. Furthermore, we discuss constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions and the relationship between linear rank-metric codes with systems and evasive subspaces. The paper also reviews results for algebraically closed fields and real numbers, previously appearing in the context of topology and measure theory. We conclude by proposing future research directions, including conjectures on MRD code existence and the exploration of rank-metric codes over various field extensions."}
{"id": "2601.15327", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.15327", "abs": "https://arxiv.org/abs/2601.15327", "authors": ["Masatsugu Yoshizawa", "Yuta Kawamoto", "Daisuke Takeshita"], "title": "Rules Create Unequal Rewards: Elite Tennis Players Allocate Resources Efficiently", "comment": "12 pages, 3 figures, 1 table", "summary": "In many competitive settings, from education to politics, rules do not reward effort evenly, and thresholds (e.g., grade cutoffs or electoral majorities) make some moments disproportionately important. Success thus depends on efficiently allocating limited resources. However, empirical demonstration has been difficult because effort allocation is rarely observable and feedback is often delayed, limiting our understanding of expertise. Professional tennis provides an ideal natural experiment. Because each game resets after a player wins four points and points in a lost game are wasted, the value of a point varies sharply across scores. Efficient allocation should therefore win games without wasting points, conserving resources for future games. Such allocation manifests in score-dependent point-winning probabilities, from which we derive each player's Pareto frontier-the theoretical limit of the trade-off between game-winning probability and the expected points per game. Here, we show that top players operate closer to this frontier, converting points to game wins more efficiently. Optimal strategies reduce the probability of winning points when the player is far behind (e.g.,0-2, 0-3). This behavior is psychologically difficult-letting go of the current game-but represents a rational energy conservation strategy. Top players exhibit this pattern especially in return games, where winning points is harder than in service games, requiring them to drastically vary their efforts, consistent with game-theoretic predictions. These findings suggest that elite performance reflects efficient adaptation to rule-created value structures; knowing when to give up may be as fundamental to expertise as knowing when to compete."}
{"id": "2601.15814", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.15814", "abs": "https://arxiv.org/abs/2601.15814", "authors": ["Ryosuke Yamano", "Tetsuo Shibuya"], "title": "Improved Approximation Ratios for the Shortest Common Superstring Problem with Reverse Complements", "comment": "Accepted to CPM 2026", "summary": "The Shortest Common Superstring (SCS) problem asks for the shortest string that contains each of a given set of strings as a substring. Its reverse-complement variant, the Shortest Common Superstring problem with Reverse Complements (SCS-RC), naturally arises in bioinformatics applications, where for each input string, either the string itself or its reverse complement must appear as a substring of the superstring. The well-known MGREEDY algorithm for the standard SCS constructs a superstring by first computing an optimal cycle cover on the overlap graph and then concatenating the strings corresponding to the cycles, while its refined variant, TGREEDY, further improves the approximation ratio. Although the original 4- and 3-approximation bounds of these algorithms have been successively improved for the standard SCS, no such progress has been made for the reverse-complement setting. A previous study extended MGREEDY to SCS-RC with a 4-approximation guarantee and briefly suggested that extending TGREEDY to the reverse-complement setting could achieve a 3-approximation. In this work, we strengthen these results by proving that the extensions of MGREEDY and TGREEDY to the reverse-complement setting achieve 3.75- and 2.875-approximation ratios, respectively. Our analysis extends the classical proofs for the standard SCS to handle the bidirectional overlaps introduced by reverse complements. These results provide the first formal improvement of approximation guarantees for SCS-RC, with the 2.875-approximate algorithm currently representing the best known bound for this problem."}
{"id": "2601.15594", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15594", "abs": "https://arxiv.org/abs/2601.15594", "authors": ["Zhixian Zhou", "Bin Chen", "Zhe Peng", "Zhiming Liang", "Ruijun Wu", "Chen Sun", "Shuo Wang"], "title": "Blockchain-Based Spectrum Resource Securitization via Semi-Fungible Token-Lock", "comment": null, "summary": "As 6G networks evolve, spectrum assets require flexible, dynamic, and efficient utilization, motivating blockchain based spectrum securitization. Existing approaches based on ERC404 style hybrid token models rely on frequent minting and burning during asset transfers, which disrupt token identity continuity and increase on chain overhead. This paper proposes the Semi Fungible Token Lock (SFT Lock) method, a lock/unlock based mechanism that preserves NFT identity and historical traceability while enabling fractional ownership and transferability. By replacing mint/burn operations with deterministic state transitions, SFT Lock ensures consistent lifecycle representation of spectrum assets and significantly reduces on chain operations. Based on this mechanism, a modular smart contract architecture is designed to support spectrum authorization, securitization, and sharing, and a staking mechanism is introduced to enhance asset liquidity. Experimental results on a private Ethereum network demonstrate that, compared with ERC404 style hybrid token models, the proposed method achieves substantial gas savings while maintaining functional correctness and traceability."}
{"id": "2601.15505", "categories": ["cs.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.15505", "abs": "https://arxiv.org/abs/2601.15505", "authors": ["Tyler Kann", "Matthieu R. Bloch", "Shrinivas Kudekar", "Ruediger Urbanke"], "title": "Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds", "comment": null, "summary": "The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\\![ n, k ]\\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work."}
{"id": "2601.15478", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.15478", "abs": "https://arxiv.org/abs/2601.15478", "authors": ["Michal Feldman", "Yoav Gal-Tzur", "Tomasz Ponitka", "Maya Schlesinger"], "title": "Equal-Pay Contracts", "comment": null, "summary": "We study multi-agent contract design, where a principal incentivizes a team of agents to take costly actions that jointly determine the project success via a combinatorial reward function. While prior work largely focuses on unconstrained contracts that allow heterogeneous payments across agents, many real-world environments limit payment dispersion. Motivated by this, we study equal-pay contracts, where all agents receive identical payments. Our results also extend to nearly-equal-pay contracts where any two payments are identical up to a constant factor.\n  We provide both algorithmic and hardness results across a broad hierarchy of reward functions, under both binary and combinatorial action models. While we focus on equal-pay contracts, our analysis also yields new insights into unconstrained contract design, and resolves two important open problems. On the positive side, we design polynomial-time O(1)-approximation algorithms for (i) submodular rewards under combinatorial actions, and (ii) XOS rewards under binary actions. These guarantees are tight: We rule out the existence of (i) a PTAS for combinatorial actions, even for gross substitutes rewards (unless P = NP), and (ii) any O(1)-approximation for XOS rewards with combinatorial actions. Crucially, our hardness results hold even for unconstrained contracts, thereby settling the corresponding open problems in this setting.\n  Finally, we quantify the loss induced by fairness via the price of equality, defined as the worst-case ratio between the optimal principal's utility achievable by unconstrained contracts and that achievable by equal-pay contracts. We obtain a bound of $Θ(\\log n/ \\log \\log n)$, where $n$ is the number of agents. This gap is tight in a strong sense: the upper bound applies even for XOS rewards with combinatorial actions, while the lower bound arises already for additive rewards with binary actions."}
{"id": "2601.15861", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.15861", "abs": "https://arxiv.org/abs/2601.15861", "authors": ["Daniel Lokshtanov", "Michał Pilipczuk", "Paweł Rzążewski"], "title": "Finding large sparse induced subgraphs in graphs of small (but not very small) tree-independence number", "comment": null, "summary": "The independence number of a tree decomposition is the size of a largest independent set contained in a single bag. The tree-independence number of a graph $G$ is the minimum independence number of a tree decomposition of $G$. As shown recently by Lima et al. [ESA~2024], a large family of optimization problems asking for a maximum-weight induced subgraph of bounded treewidth, satisfying a given \\textsf{CMSO}$_2$ property, can be solved in polynomial time in graphs whose tree-independence number is bounded by some constant~$k$.\n  However, the complexity of the algorithm of Lima et al. grows rapidly with $k$, making it useless if the tree-independence number is superconstant. In this paper we present a refined version of the algorithm. We show that the same family of problems can be solved in time~$n^{\\mathcal{O}(k)}$, where $n$ is the number of vertices of the instance, $k$ is the tree-independence number, and the $\\mathcal{O}(\\cdot)$-notation hides factors depending on the treewidth bound of the solution and the considered \\textsf{CMSO}$_2$ property.\n  This running time is quasipolynomial for classes of graphs with polylogarithmic tree-independence number; several such classes were recently discovered. Furthermore, the running time is subexponential for many natural classes of geometric intersection graphs -- namely, ones that admit balanced clique-based separators of sublinear size."}
{"id": "2601.15673", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15673", "abs": "https://arxiv.org/abs/2601.15673", "authors": ["Qilong Yan", "Yifei Xing", "Dugang Liu", "Jingpu Duan", "Jian Yin"], "title": "Enhancing guidance for missing data in diffusion-based sequential recommendation", "comment": "ICASSP 2026 accecpted", "summary": "Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD."}
{"id": "2601.15639", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15639", "abs": "https://arxiv.org/abs/2601.15639", "authors": ["Hamidreza Abin", "Mahdi Zinati", "Amin Gohari", "Mohammad Hossein Yassaee", "Mohammad Mahdi Mojahedian"], "title": "A Class of Subadditive Information Measures and their Applications", "comment": null, "summary": "We introduce a two-parameter family of discrepancy measures, termed \\emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $\nI_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\\in\\{x,\\log(1+x),-\\log(1-x)\\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences."}
{"id": "2601.15855", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.15855", "abs": "https://arxiv.org/abs/2601.15855", "authors": ["Robert Bredereck", "Piotr Faliszewski", "Michał Furdyna", "Andrzej Kaczmarczyk", "Joanna Kaczmarek", "Martin Lackner", "Christian Laußmann", "Jörg Rothe", "Tessa Seeger"], "title": "How to Tamper with a Parliament: Strategic Campaigns in Apportionment Elections", "comment": null, "summary": "In parliamentary elections, parties compete for a limited, typically fixed number of seats. Most parliaments are assembled using apportionment methods that distribute the seats based on the parties' vote counts. Common apportionment methods include divisor sequence methods (like D'Hondt or Sainte-Laguë), the largest-remainder method, and first-past-the-post. In many countries, an electoral threshold is implemented to prevent very small parties from entering the parliament. Further, several countries have apportionment systems that incorporate multiple districts. We study how computationally hard it is to change the election outcome (i.e., to increase or limit the influence of a distinguished party) by convincing a limited number of voters to change their vote. We refer to these bribery-style attacks as \\emph{strategic campaigns} and study the corresponding problems in terms of their computational (both classical and parameterized) complexity. We also run extensive experiments on real-world election data and study the effectiveness of optimal campaigns, in particular as opposed to using heuristic bribing strategies and with respect to the influence of the threshold and the influence of the number of districts. For apportionment elections with threshold, finally, we propose -- as an alternative to the standard top-choice mode -- the second-chance mode where voters of parties below the threshold receive a second chance to vote for another party, and we establish computational complexity results also in this setting."}
{"id": "2601.16182", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.16182", "abs": "https://arxiv.org/abs/2601.16182", "authors": ["Arshia Ataee Naeini", "Amir-Parsa Mobed", "Masoud Seddighin", "Saeed Seddighin"], "title": "Dynamic Pattern Matching with Wildcards", "comment": null, "summary": "We study the fully dynamic pattern matching problem where the pattern may contain up to kwildcard symbols, each matching any symbol of the alphabet. Both the text and the pattern are subject to updates (insert, delete, change). We design an algorithm with O(nlog^2 n) preprocessing and update/query time O(knk/k+1 + k2 log n). The bound is truly sublinear for a constant k, and sublinear when k= o(log n). We further complement our results with a conditional lower bound: assuming subquadratic preprocessing time, achieving truly sublinear update time for the case k = Ω(log n) would contradict the Strong Exponential Time Hypothesis (SETH). Finally, we develop sublinear algorithms for two special cases: - If the pattern contains w non-wildcard symbols, we give an algorithm with preprocessing time O(nw) and update time O(w + log n), which is truly sublinear whenever wis truly sublinear. - Using FFT technique combined with block decomposition, we design a deterministic truly sublinear algorithm with preprocessing time O(n^1.8) and update time O(n^0.8 log n) for the case that there are at most two non-wildcards."}
{"id": "2601.15721", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15721", "abs": "https://arxiv.org/abs/2601.15721", "authors": ["Xinda Chen", "Jiawei Wu", "Yishuang Liu", "Jialin Zhu", "Shuwen Xiao", "Junjun Zheng", "Xiangheng Kong", "Yuning Jiang"], "title": "CoNRec: Context-Discerning Negative Recommendation with LLMs", "comment": null, "summary": "Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals."}
{"id": "2601.15642", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15642", "abs": "https://arxiv.org/abs/2601.15642", "authors": ["Yi Chen", "Yatao Hu", "Ming Li", "Chong Han"], "title": "Generative AI-Empowered Semantic Twin Channel Model for ISAC", "comment": "7 paegs", "summary": "Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization."}
{"id": "2601.15864", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.15864", "abs": "https://arxiv.org/abs/2601.15864", "authors": ["Tanmay Inamdar", "Pallavi Jain", "Pranjal Pandey"], "title": "Minimum Envy Graphical House Allocation Beyond Identical Valuations", "comment": "21 pages, submitted to IJCAI 2026", "summary": "House allocation is an extremely well-studied problem in the field of fair allocation, where the goal is to assign $n$ houses to $n$ agents while satisfying certain fairness criterion, e.g., envy-freeness. To model social interactions, the Graphical House Allocation framework introduces a social graph $G$, in which each vertex corresponds to an agent, and an edge $(u, v)$ corresponds to the potential of agent $u$ to envy the agent $v$, based on their allocations and valuations. In undirected social graphs, the potential for envy is in both the directions. In the Minimum Envy Graphical House Allocation (ME-GHA) problem, given a set of $n$ agents, $n$ houses, a social graph, and agent's valuation functions, the goal is to find an allocation that minimizes the total envy summed up over all the edges of $G$. Recent work, [Hosseini et al., AAMAS 2023, AAMAS 2024] studied ME-GHA in the regime of polynomial-time algorithms, and designed exact and approximation algorithms, for certain graph classes under identical agent valuations. We initiate the study of \\gha with non-identical valuations, a setting that has so far remained unexplored. We investigate the multivariate (parameterized) complexity of \\gha by identifying structural restrictions on the social graph and valuation functions that yield tractability. We also design moderately exponential-time algorithms for several graph classes, and a polynomial-time algorithm for {binary valuations that returns an allocation with envy at most one when the social graph has maximum degree at most one."}
{"id": "2601.15864", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.15864", "abs": "https://arxiv.org/abs/2601.15864", "authors": ["Tanmay Inamdar", "Pallavi Jain", "Pranjal Pandey"], "title": "Minimum Envy Graphical House Allocation Beyond Identical Valuations", "comment": "21 pages, submitted to IJCAI 2026", "summary": "House allocation is an extremely well-studied problem in the field of fair allocation, where the goal is to assign $n$ houses to $n$ agents while satisfying certain fairness criterion, e.g., envy-freeness. To model social interactions, the Graphical House Allocation framework introduces a social graph $G$, in which each vertex corresponds to an agent, and an edge $(u, v)$ corresponds to the potential of agent $u$ to envy the agent $v$, based on their allocations and valuations. In undirected social graphs, the potential for envy is in both the directions. In the Minimum Envy Graphical House Allocation (ME-GHA) problem, given a set of $n$ agents, $n$ houses, a social graph, and agent's valuation functions, the goal is to find an allocation that minimizes the total envy summed up over all the edges of $G$. Recent work, [Hosseini et al., AAMAS 2023, AAMAS 2024] studied ME-GHA in the regime of polynomial-time algorithms, and designed exact and approximation algorithms, for certain graph classes under identical agent valuations. We initiate the study of \\gha with non-identical valuations, a setting that has so far remained unexplored. We investigate the multivariate (parameterized) complexity of \\gha by identifying structural restrictions on the social graph and valuation functions that yield tractability. We also design moderately exponential-time algorithms for several graph classes, and a polynomial-time algorithm for {binary valuations that returns an allocation with envy at most one when the social graph has maximum degree at most one."}
{"id": "2601.15849", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15849", "abs": "https://arxiv.org/abs/2601.15849", "authors": ["Tsung-Hsiang Chou", "Chen-Jui Yu", "Shui-Hsiang Hsu", "Yao-Chung Fan"], "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval", "comment": "Accepted at The Web Conference 2026 (WWW 2026)", "summary": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT."}
{"id": "2601.15723", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.15723", "abs": "https://arxiv.org/abs/2601.15723", "authors": ["Gunank Jakhar", "Gowtham R. Kurri", "Suryajith Chillara", "Vinod M. Prabhakaran"], "title": "Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems", "comment": "10 pages, 1 figure", "summary": "It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works."}
{"id": "2601.15860", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15860", "abs": "https://arxiv.org/abs/2601.15860", "authors": ["Shui-Hsiang Hsu", "Tsung-Hsiang Chou", "Chen-Jui Yu", "Yao-Chung Fan"], "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion", "comment": "Accepted at The Web Conference 2026 (WWW 2026)", "summary": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR."}
{"id": "2601.15767", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15767", "abs": "https://arxiv.org/abs/2601.15767", "authors": ["Zehua Jiang", "Fenghao Zhu", "Chongwen Huang", "Richeng Jin", "Zhaohui Yang", "Xiaoming Chen", "Zhaoyang Zhang", "Mérouane Debbah"], "title": "Recursive Flow: A Generative Framework for MIMO Channel Estimation", "comment": null, "summary": "Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline."}
{"id": "2601.15930", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15930", "abs": "https://arxiv.org/abs/2601.15930", "authors": ["Tianjun Wei", "Enneng Yang", "Yingpeng Du", "Huizhong Guo", "Jie Zhang", "Zhu Sun"], "title": "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging", "comment": "https://github.com/Joinn99/MMGRid", "summary": "Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments."}
{"id": "2601.15853", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15853", "abs": "https://arxiv.org/abs/2601.15853", "authors": ["A. Schmidt", "A. Vdberg", "A. Petit"], "title": "Practical applications of Set Shaping Theory to Non-Uniform Sequences", "comment": null, "summary": "Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work"}
{"id": "2601.15975", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15975", "abs": "https://arxiv.org/abs/2601.15975", "authors": ["Chen Xu", "Zhipeng Yi", "Ruizi Wang", "Wenjie Wang", "Jun Xu", "Maarten de Rijke"], "title": "Unveiling and Simulating Short-Video Addiction Behaviors via Economic Addiction Theory", "comment": "Accepted in TheWebConf 2026", "summary": "Short-video applications have attracted substantial user traffic. However, these platforms also foster problematic usage patterns, commonly referred to as short-video addiction, which pose risks to both user health and the sustainable development of platforms. Prior studies on this issue have primarily relied on questionnaires or volunteer-based data collection, which are often limited by small sample sizes and population biases. In contrast, short-video platforms have large-scale behavioral data, offering a valuable foundation for analyzing addictive behaviors. To examine addiction-aware behavior patterns, we combine economic addiction theory with users' implicit behavior captured by recommendation systems. Our analysis shows that short-video addiction follows functional patterns similar to traditional forms of addictive behavior (e.g., substance abuse) and that its intensity is consistent with findings from previous social science studies. To develop a simulator that can learn and model these patterns, we introduce a novel training framework, AddictSim. To consider the personalized addiction patterns, AddictSim uses a mean-to-adapted strategy with group relative policy optimization training. Experiments on two large-scale datasets show that AddictSim consistently outperforms existing training strategies. Our simulation results show that integrating diversity-aware algorithms can mitigate addictive behaviors well."}
{"id": "2601.15903", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15903", "abs": "https://arxiv.org/abs/2601.15903", "authors": ["Pramod Singh", "Prasad Krishnan", "Arti Yardi"], "title": "Blind Identification of Channel Codes: A Subspace-Coding Approach", "comment": "14 pages, 5 figures", "summary": "The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors."}
{"id": "2601.15928", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.15928", "abs": "https://arxiv.org/abs/2601.15928", "authors": ["Yuchen Liao", "Wenyi Zhang"], "title": "A Remark on Downlink Massive Random Access", "comment": null, "summary": "In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits."}
{"id": "2601.16030", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16030", "abs": "https://arxiv.org/abs/2601.16030", "authors": ["Jiancheng An", "Chau Yuen", "Marco Di Renzo", "Mehdi Bennis", "Merouane Debbah", "Lajos Hanzo"], "title": "Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing", "comment": "22 pages, 19 figures, 6 tables", "summary": "Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential."}
{"id": "2601.16033", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16033", "abs": "https://arxiv.org/abs/2601.16033", "authors": ["Zhixin Chen", "Yixuan Huang", "Zhengze Ji", "Jie Yang", "Shi Jin"], "title": "RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance", "comment": "Accepted by China Communications", "summary": "The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters."}
{"id": "2601.16036", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16036", "abs": "https://arxiv.org/abs/2601.16036", "authors": ["Tianyu Fang", "Mengyuan Ma", "Markku Juntti", "Nhan Thanh Nguyen"], "title": "Tri-Hybrid Beamforming Design for integrated Sensing and Communications", "comment": "Accepted by ICASSP 2026", "summary": "Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures."}
{"id": "2601.16164", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16164", "abs": "https://arxiv.org/abs/2601.16164", "authors": ["Emmanuel Abbe", "Colin Sandon", "Oscar Sprumont"], "title": "Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time", "comment": null, "summary": "Define the codewords of the Tensor Reed-Muller code $\\mathsf{TRM}(r_1,m_1;r_2,m_2;\\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\\left\\{x_{ij}\\right\\}_{i=1,\\dots,t}^{j=1,\\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\\dots,x_{im_i}$. The generator matrix of $\\mathsf{TRM}(r_1,m_1;\\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\\mathsf{RM}(r_1,m_1),\\dots, \\mathsf{RM}(r_t,m_t)$.\n  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\\mathsf{TRM}(r_1,m_1;\\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:\n  1) Our first construction (with $t=3$) has error probability $n^{-ω(\\log n)}$ and decoding time $O(n\\log\\log n)$.\n  2) Our second construction, for any $t\\geq 4$, has error probability $2^{-n^{\\frac{1}{2}-\\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\\log n)$.\n  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\\otimes\\dotsc\\otimes C_t$ from $\\frac{d_{\\min}(C)}{2\\max\\{d_{\\min}(C_1),\\dotsc,d_{\\min}(C_t) \\}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\\dotsc,C_t$ to themselves be decodable in polynomial time."}
{"id": "2601.16171", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16171", "abs": "https://arxiv.org/abs/2601.16171", "authors": ["Ali Khalesi", "Ahmad Tanha", "Derya Malak", "Petros Elia"], "title": "Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach", "comment": null, "summary": "The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\\bar{\\mathcal{F}}$, whose sparse decomposition into a tensor $\\bar{\\mathcal{E}}$ and matrix $\\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art."}
