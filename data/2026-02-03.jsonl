{"id": "2602.00771", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00771", "abs": "https://arxiv.org/abs/2602.00771", "authors": ["Matteo Bollini", "Francesco Bacchiocchi", "Samuel Coutts", "Matteo Castiglioni", "Alberto Marchesi"], "title": "Learning in Bayesian Stackelberg Games With Unknown Follower's Types", "comment": null, "summary": "We study online learning in Bayesian Stackelberg games, where a leader repeatedly interacts with a follower whose unknown private type is independently drawn at each round from an unknown probability distribution. The goal is to design algorithms that minimize the leader's regret with respect to always playing an optimal commitment computed with knowledge of the game. We consider, for the first time to the best of our knowledge, the most realistic case in which the leader does not know anything about the follower's types, i.e., the possible follower payoffs. This raises considerable additional challenges compared to the commonly studied case in which the payoffs of follower types are known. First, we prove a strong negative result: no-regret is unattainable under action feedback, i.e., when the leader only observes the follower's best response at the end of each round. Thus, we focus on the easier type feedback model, where the follower's type is also revealed. In such a setting, we propose a no-regret algorithm that achieves a regret of $\\widetilde{O}(\\sqrt{T})$, when ignoring the dependence on other parameters."}
{"id": "2602.00859", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.00859", "abs": "https://arxiv.org/abs/2602.00859", "authors": ["Anurag Satpathy", "Arindam Khanda", "Chittaranjan Swain", "Sajal K. Das"], "title": "ReACT-TTC: Capacity-Aware Top Trading Cycles for Post-Choice Reassignment in Shared CPS", "comment": "Accepted in the 17th ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS), Saint Mao, France, May 11-14, 2026", "summary": "Cyber-physical systems (CPS) increasingly manage shared physical resources in the presence of human decision-making, where system-assigned actions must be executed by users or agents in the physical world. A fundamental challenge in such settings is user non-compliance: individuals may deviate from assigned resources due to personal preferences or local information, degrading system efficiency and requiring light-weight reassignment schemes. This paper proposes a post-deviation reassignment framework for shared-resource CPS that operates on top of any initial allocation algorithm and is invoked only when users diverge from prescribed assignments. We advance the Top-Trading-Cycle (TTC) mechanism to enable voluntary, preference-driven exchanges after deviation events, and extend it to handle many-to-one resource capacities and unassigned resource conditions that are not supported by the classical TTC. We formalize these structural cases, introduce capacity-aware cycle-detection rules, and prove termination along with the preservation of Pareto efficiency, individual rationality, and strategy-proofness. A Prospect-Theoretic (PT) preference model is further incorporated to capture realistic user satisfaction behavior. We demonstrate the applicability of this framework on an electric-vehicle (EV) charging case study using real-world data, where it increases user satisfaction and effective assignment quality under non-compliant behavior."}
{"id": "2602.01048", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.01048", "abs": "https://arxiv.org/abs/2602.01048", "authors": ["Yuhang Guo", "Houyu Zhou"], "title": "Minimizing Inequity in Facility Location Games", "comment": "Accepted in AAAI 2026", "summary": "This paper studies the problem of minimizing group-level inequity in facility location games on the real line, where agents belong to different groups and may act strategically. We explore a fairness-oriented objective that minimizes the maximum group effect introduced by Marsh and Schilling (1994). Each group's effect is defined as its total or maximum distance to the nearest facility, weighted by group-specific factors. We show that this formulation generalizes several prominent optimization objectives, including the classical utilitarian (social cost) and egalitarian (maximum cost) objectives, as well as two group-fair objectives, maximum total and average group cost. In order to minimize the maximum group effect, we first propose two novel mechanisms for the single-facility case, the BALANCED mechanism and the MAJOR-PHANTOM mechanism. Both are strategyproof and achieve tight approximation guarantees under distinct formulations of the maximum group effect objective. Our mechanisms not only close the existing gap in approximation bounds for group-fairness objectives identified by Zhou, Li, and Chan (2022), but also unify many classical truthful mechanisms within a broader fairness-aware framework. For the two-facility case, we revisit and extend the classical endpoint mechanism to our generalized setting and demonstrate that it provides tight bounds for two distinct maximum group effect objectives."}
{"id": "2602.01066", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2602.01066", "abs": "https://arxiv.org/abs/2602.01066", "authors": ["Shipra Agrawal", "Yiding Feng", "Wei Tang"], "title": "Simple and Robust Quality Disclosure: The Power of Quantile Partition", "comment": null, "summary": "Quality information on online platforms is often conveyed through simple, percentile-based badges and tiers that remain stable across different market environments. Motivated by this empirical evidence, we study robust quality disclosure in a market where a platform commits to a public disclosure policy mapping the seller's product quality into a signal, and the seller subsequently sets a downstream monopoly price. Buyers have heterogeneous private types and valuations that are linear in quality. We evaluate a disclosure policy via a minimax competitive ratio: its worst-case revenue relative to the Bayesian-optimal disclosure-and-pricing benchmark, uniformly over all prior quality distributions, type distributions, and admissible valuations.\n  Our main results provide a sharp theoretical justification for quantile-partition disclosure. For K-quantile partition policies, we fully characterize the robust optimum: the optimal worst-case ratio is pinned down by a one-dimensional fixed-point equation and the optimal thresholds follow a backward recursion. We also give an explicit formula for the robust ratio of any quantile partition as a simple \"max-over-bins\" expression, which explains why the robust-optimal partition allocates finer resolution to upper quantiles and yields tight guarantees such as 1 + 1/K for uniform percentile buckets. In contrast, we show a robustness limit for finite-signal monotone (quality-threshold) partitions, which cannot beat a factor-2 approximation. Technically, our analysis reduces the robust quality disclosure to a robust disclosure design program by establishing a tight functional characterization of all feasible indirect revenue functions."}
{"id": "2602.00563", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.00563", "abs": "https://arxiv.org/abs/2602.00563", "authors": ["Yuhui Lai", "Shixun Huang", "Sheng Wang"], "title": "Updatable Balanced Index for Stable Streaming Similarity Search over Large-Scale Fresh Vectors", "comment": "Accepted for publication in the 13th IEEE International Conference on Big Data (BigData 2025). To appear", "summary": "As artificial intelligence gains more and more popularity, vectors are one of the most widely used data structures for services such as information retrieval and recommendation. Approximate Nearest Neighbor Search (ANNS), which generally relies on indices optimized for fast search to organize large datasets, has played a core role in these popular services. As the frequency of data shift grows, it is crucial for indices to accommodate new data and support real-time updates. Existing researches adopting two different approaches hold the following drawbacks: 1) approaches using additional buffers to temporarily store new data are resource-intensive and inefficient due to the global rebuilding processes; 2) approaches upgrading the internal index structure suffer from performance degradation because of update congestion and imbalanced distribution in streaming workloads. In this paper, we propose UBIS, an Updatable Balanced Index for stable streaming similarity Search, to resolve conflicts by scheduling concurrent updates and maintain good index quality by reducing imbalanced update cases, when the update frequency grows. Experimental results in the real-world datasets demonstrate that UBIS achieves up to 77% higher search accuracy and 45% higher update throughput on average compared to the state-of-the-art indices in streaming workloads."}
{"id": "2602.00162", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.00162", "abs": "https://arxiv.org/abs/2602.00162", "authors": ["Bingwei Zhang", "Chee Yap"], "title": "End Cover for Initial Value Problem: Complete Validated Algorithms with Complexity Analysis", "comment": null, "summary": "We consider the first-order autonomous ordinary differential equation \\[ \\mathbf{x}' = \\mathbf{f}(\\mathbf{x}), \\] where $\\mathbf{f} : \\mathbb{R}^n \\to \\mathbb{R}^n$ is locally Lipschitz. For a box $B_0 \\subseteq \\mathbb{R}^n$ and $h > 0$, we denote by $\\mathrm{IVP}_{\\mathbf{f}}(B_0,h)$ the set of solutions $\\mathbf{x} : [0,h] \\to \\mathbb{R}^n$ satisfying \\[ \\mathbf{x}'(t) = \\mathbf{f}(\\mathbf{x}(t)), \\qquad \\mathbf{x}(0) \\in B_0 . \\]\n  We present a complete validated algorithm for the following \\emph{End Cover Problem}: given $(\\mathbf{f}, B_0, \\varepsilon, h)$, compute a finite set $\\mathcal{C}$ of boxes such that \\[ \\mathrm{End}_{\\mathbf{f}}(B_0,h) \\;\\subseteq\\; \\bigcup_{B \\in \\mathcal{C}} B \\;\\subseteq\\; \\mathrm{End}_{\\mathbf{f}}(B_0,h) \\oplus [-\\varepsilon,\\varepsilon]^n , \\] where \\[ \\mathrm{End}_{\\mathbf{f}}(B_0,h) = \\left\\{ \\mathbf{x}(h) : \\mathbf{x} \\in \\mathrm{IVP}_{\\mathbf{f}}(B_0,h) \\right\\}. \\]\n  Moreover, we provide a complexity analysis of our algorithm and introduce a novel technique for computing the end cover $\\mathcal{C}$ based on covering the boundary of $\\mathrm{End}_{\\mathbf{f}}(B_0,h)$. Finally, we present experimental results demonstrating the practicality of our approach."}
{"id": "2602.00209", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00209", "abs": "https://arxiv.org/abs/2602.00209", "authors": ["Qingcao Li", "Miao He", "Liang Yi", "Qing Wen", "Yitao Zhang", "Hongshuo Jin", "Peng Cheng", "Zhongjie Ba", "Li Lu", "Kui Ren"], "title": "Divide and Conquer: Multimodal Video Deepfake Detection via Cross-Modal Fusion and Localization", "comment": "The 3rd Place, IJCAI 2025 Workshop on Deepfake Detection, Localization, and Interpretability", "summary": "This paper presents a system for detecting fake audio-visual content (i.e., video deepfake), developed for Track 2 of the DDL Challenge. The proposed system employs a two-stage framework, comprising unimodal detection and multimodal score fusion. Specifically, it incorporates an audio deepfake detection module and an audio localization module to analyze and pinpoint manipulated segments in the audio stream. In parallel, an image-based deepfake detection and localization module is employed to process the visual modality. To effectively leverage complementary information across different modalities, we further propose a multimodal score fusion strategy that integrates the outputs from both audio and visual modules. Guided by a detailed analysis of the training and evaluation dataset, we explore and evaluate several score calculation and fusion strategies to improve system robustness. Overall, the final fusion-based system achieves an AUC of 0.87, an AP of 0.55, and an AR of 0.23 on the challenge test set, resulting in a final score of 0.5528."}
{"id": "2602.00002", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00002", "abs": "https://arxiv.org/abs/2602.00002", "authors": ["Yu Zheng", "Chen Gao", "Jianxin Chang", "Yanan Niu", "Yang Song", "Depeng Jin", "Meng Wang", "Yong Li"], "title": "Disentangled Interest Network for Out-of-Distribution CTR Prediction", "comment": "Accepted by ACM TOIS", "summary": "Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/."}
{"id": "2602.00140", "categories": ["cs.IT", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.00140", "abs": "https://arxiv.org/abs/2602.00140", "authors": ["Peerasait Prachaseree", "Emma Lejeune"], "title": "Information Propagation and Encoding in Solids: A Quantitative Approach Towards Mechanical Intelligence", "comment": "32 pages; 9 figures", "summary": "Engineered systems typically separate mechanical function from information processing, whereas biological systems can exploit physical structure as a medium for information processing and computation. Motivated by this contrast, recent work in mechanics has explored embedding information-processing capabilities directly into mechanical structures. However, quantitative frameworks for evaluating such capabilities remain limited. Here we address a foundational question: how does information propagate through a solid body? Using elastic bodies as a model system, we apply information-theoretic tools to treat an elastic domain as an information encoder and quantify how information transmits from applied loads to discrete sensor locations. We further connect these measures to familiar mechanical phenomena, including Saint-Venant's effect and principal stress lines. Moving toward design, we show how geometry and architected materials can tune transmission, enabling elastic domains to either transmit or block information. Overall, this work advances quantifiable metrics and benchmark tasks for mechanical intelligence, supporting comparable designs of mechanically embodied information processing."}
{"id": "2602.01568", "categories": ["cs.GT", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01568", "abs": "https://arxiv.org/abs/2602.01568", "authors": ["Hamzah Khan", "Dong Ho Lee", "Jingqi Li", "Tianyu Qiu", "Christian Ellis", "Jesse Milzman", "Wesley Suttle", "David Fridovich-Keil"], "title": "Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations", "comment": null, "summary": "Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures."}
{"id": "2602.01701", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01701", "abs": "https://arxiv.org/abs/2602.01701", "authors": ["Ruyu Li", "Tinghui Zhang", "Haodi Ma", "Daisy Zhe Wang", "Yifan Wang"], "title": "Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems", "comment": null, "summary": "With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.\n  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some \"all-in-one\" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.\n  This paper introduces Meta Engine, a novel \"query system on query systems\", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets."}
{"id": "2602.00644", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.00644", "abs": "https://arxiv.org/abs/2602.00644", "authors": ["Ajinkya Gaikwad", "Soumen Maity", "Leeja R"], "title": "Hardness and Tractability of T_{h+1}-Free Edge Deletion", "comment": "23 pages", "summary": "We study the parameterized complexity of the T(h+1)-Free Edge Deletion problem. Given a graph G and integers k and h, the task is to delete at most k edges so that every connected component of the resulting graph has size at most h. The problem is NP-complete for every fixed h at least 3, while it is solvable in polynomial time for h at most 2.\n  Recent work showed strong hardness barriers: the problem is W[1]-hard when parameterized by the solution size together with the size of a feedback edge set, ruling out fixed-parameter tractability for many classical structural parameters. We significantly strengthen these negative results by proving W[1]-hardness when parameterized by the vertex deletion distance to a disjoint union of paths, the vertex deletion distance to a disjoint union of stars, or the twin cover number. These results unify and extend known hardness results for treewidth, pathwidth, and feedback vertex set, and show that several restrictive parameters, including treedepth, cluster vertex deletion number, and modular width, do not yield fixed-parameter tractability when h is unbounded.\n  On the positive side, we identify parameterizations that restore tractability. We show that the problem is fixed-parameter tractable when parameterized by cluster vertex deletion together with h, and also when parameterized by neighborhood diversity together with h via an integer linear programming formulation. We further present a fixed-parameter tractable bicriteria approximation algorithm parameterized by k. Finally, we show that the problem admits fixed-parameter tractable algorithms on split graphs and interval graphs, and we establish hardness for a directed generalization even on directed acyclic graphs."}
{"id": "2602.00607", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.00607", "abs": "https://arxiv.org/abs/2602.00607", "authors": ["Yang-Hao Zhou", "Haitian Li", "Rexar Lin", "Heyan Huang", "Jinxing Zhou", "Changsen Yuan", "Tian Lan", "Ziqin Zhou", "Yudong Li", "Jiajun Xu", "Jingyun Liao", "Yi-Ming Cheng", "Xuefeng Chen", "Xian-Ling Mao", "Yousheng Feng"], "title": "MTAVG-Bench: A Comprehensive Benchmark for Evaluating Multi-Talker Dialogue-Centric Audio-Video Generation", "comment": null, "summary": "Recent advances in text-to-audio-video (T2AV) generation have enabled models to synthesize audio-visual videos with multi-participant dialogues. However, existing evaluation benchmarks remain largely designed for human-recorded videos or single-speaker settings. As a result, potential errors that occur in generated multi-talker dialogue videos, such as identity drift, unnatural turn transitions, and audio-visual misalignment, cannot be effectively captured and analyzed. To address this issue, we introduce MTAVG-Bench, a benchmark for evaluating audio-visual multi-speaker dialogue generation. MTAVG-Bench is built via a semi-automatic pipeline, where 1.8k videos are generated using multiple popular models with carefully designed prompts, yielding 2.4k manually annotated QA pairs. The benchmark evaluates multi-speaker dialogue generation at four levels: audio-visual signal fidelity, temporal attribute consistency, social interaction, and cinematic expression. We benchmark 12 proprietary and open-source omni-models on MTAVG-Bench, with Gemini 3 Pro achieving the strongest overall performance, while leading open-source models remain competitive in signal fidelity and consistency. Overall, MTAVG-Bench enables fine-grained failure analysis for rigorous model comparison and targeted video generation refinement."}
{"id": "2602.00003", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00003", "abs": "https://arxiv.org/abs/2602.00003", "authors": ["Ye Liu", "Xu Chen", "Wuji Chen", "Mang Li"], "title": "Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts", "comment": "4 pages, 2 figures", "summary": "In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems."}
{"id": "2602.00142", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00142", "abs": "https://arxiv.org/abs/2602.00142", "authors": ["Boya Li", "Xiaonan Liu", "Dongzhu Liu", "Dusit Niyato", "Zhu Han"], "title": "Semantic-Aware Command and Control Transmission for Multi-UAVs", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) have played an important role in the low-altitude economy and have been used in various applications. However, with the increasing number of UAVs and explosive wireless data, the existing bit-oriented communication network has approached the Shannon capacity, which cannot satisfy the quality of service (QoS) with ultra-reliable low-latency communication (URLLC) requirements for command and control (C\\&C) transmission in bit-oriented UAV communication networks. To address this issue, we propose a novel semantic-aware C\\&C transmission for multi-UAVs under limited wireless resources. Specifically, we leverage semantic similarity to measure the variation in C\\&C messages for each UAV over continuous transmission time intervals (TTIs) and capture the correlation of C\\&C messages among UAVs, enabling multicast transmission. Based on the semantic similarity and the importance of UAV commands, we design a trigger function to quantify the QoS of UAVs. Then, to maximize the long-term QoS and exploit multicast opportunities of C\\&C messages induced by semantic similarity, we develop a proximal policy optimization (PPO) algorithm to jointly determine the transmission mode (unicast/multicast/idle) and the allocation of limited resource blocks (RBs) between a base station (BS) and UAVs. Experimental results show that our proposed semantic-aware framework significantly increases transmission efficiency and improves effectiveness compared with bit-oriented UAV transmission."}
{"id": "2602.02254", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.02254", "abs": "https://arxiv.org/abs/2602.02254", "authors": ["Samuel McCauley", "Benjamin Moseley", "Helia Niaparast", "Shikha Singh"], "title": "Stable Matching with Predictions: Robustness and Efficiency under Pruned Preferences", "comment": null, "summary": "In this paper, we study the fundamental problem of finding a stable matching in two-sided matching markets. In the classic variant, it is assumed that both sides of the market submit a ranked list of all agents on the other side. However, in large matching markets such as the National Resident Matching Program (NRMP), it is infeasible for hospitals to interview or mutually rank each resident. In this paper, we study the stable matching problem with truncated preference lists. In particular, we assume that, based on historical datasets, each hospital has a predicted rank of its likely match and only ranks residents within a bounded interval around that prediction.\n  We use the algorithms-with-predictions framework and show that the classic deferred-acceptance (DA) algorithm used to compute stable matchings is robust to such truncation. We present two algorithms and theoretically and empirically evaluate their performance. Our results show that even with reasonably accurate predictions, it is possible to significantly cut down on both instance size (the length of preference lists) as well as the number of proposals made. These results explain the practical success of the DA algorithm and connect market design to the emerging theory of algorithms with predictions."}
{"id": "2602.01822", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01822", "abs": "https://arxiv.org/abs/2602.01822", "authors": ["Philip Stroemert", "Hendrik Borgelt", "David Linke", "Mark Doerr", "Bhavin Katabathuni", "Oliver Koepler", "Norbert Kockmann"], "title": "ChemDCAT-AP: Enabling Semantic Interoperability with a Contextual Extension of DCAT-AP", "comment": "The peer-reviewed and accepted paper will be published in the proceedings of the 19th International Conference on Metadata and Semantics Research (MTSR 2025), Thessaloniki, Greece, 15 - 19 December 2025", "summary": "Cross-domain data integration drives interdisciplinary data reuse and knowledge transfer across domains. However, each discipline maintains its own metadata schemas and domain ontologies, employing distinct conceptual models and application profiles, which complicates semantic interoperability. The W3C Data Catalog Vocabulary (DCAT) offers a widely adopted RDF vocabulary for describing datasets and their distributions, but its core model is intentionally lightweight. Numerous domain-specific application profiles have emerged to enrich DCAT's expressivity, the most well-known DCAT-AP for public data. To facilitate cross-domain interoperability for research data, we propose DCAT-AP PLUS, a DCAT Application Profile (P)roviding additional (L)inks to (U)se-case (S)pecific context (DCAT-AP+). This generic application profile enables a comprehensive representation of the provenance and context of research data generation. DACT-AP+ introduces an upper-level layer that can be specialized by individual domains without sacrificing compatibility. We demonstrate the application of DCAT-AP+ and a specific profile ChemDCAT-AP to showcase the potential of data integration of the neighboring disciplines chemistry and catalysis. We adopt LinkML, a YAML-based modeling framework, to support schema inheritance, generate domain-specific subschemas, and provide mechanisms for data type harmonization, validation, and format conversion, ensuring smooth integration of DCAT-AP+ and ChemDCAT-AP within existing data infrastructures."}
{"id": "2602.00690", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2602.00690", "abs": "https://arxiv.org/abs/2602.00690", "authors": ["Christian Rosenke", "Mark Scheibner"], "title": "Fanciful Figurines flip Free Flood-It -- Polynomial-Time Miniature Painting on Co-gem-free Graphs", "comment": null, "summary": "Inspired by the eponymous hobby, we introduce Miniature Painting as the computational problem to paint a given graph $G=(V,E)$ according to a prescribed template $t \\colon V \\rightarrow C$, which assigns colors $C$ to the vertices of $G$. In this setting, the goal is to realize the template using a shortest possible sequence of brush strokes, where each stroke overwrites a connected vertex subset with a color in $C$. We show that this problem is equivalent to a reversal of the well-studied Free Flood-It game, in which a colored graph is decolored into a single color using as few moves as possible. This equivalence allows known complexity results for Free Flood-It to be transferred directly to Miniature Painting, including NP-hardness under severe structural restrictions, such as when $G$ is a grid, a tree, or a split graph. Our main contribution is a polynomial-time algorithm for Miniature Painting on graphs that are free of induced co-gems, a graph class that strictly generalizes cographs. As a direct consequence, Free Flood-It is also polynomial-time solvable on co-gem-free graphs, independent of the initial coloring."}
{"id": "2602.00701", "categories": ["cs.MM", "cs.CV", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.00701", "abs": "https://arxiv.org/abs/2602.00701", "authors": ["Mohamed Saleh", "Zahra Ahmadi"], "title": "Cross-Modal Binary Attention: An Energy-Efficient Fusion Framework for Audio-Visual Learning", "comment": null, "summary": "Effective multimodal fusion requires mechanisms that can capture complex cross-modal dependencies while remaining computationally scalable for real-world deployment. Existing audio-visual fusion approaches face a fundamental trade-off: attention-based methods effectively model cross-modal relationships but incur quadratic computational complexity that prevents hierarchical, multi-scale architectures, while efficient fusion strategies rely on simplistic concatenation that fails to extract complementary cross-modal information. We introduce CMQKA, a novel cross-modal fusion mechanism that achieves linear O(N) complexity through efficient binary operations, enabling scalable hierarchical fusion previously infeasible with conventional attention. CMQKA employs bidirectional cross-modal Query-Key attention to extract complementary spatiotemporal features and uses learnable residual fusion to preserve modality-specific characteristics while enriching representations with cross-modal information. Building upon CMQKA, we present SNNergy, an energy-efficient multimodal fusion framework with a hierarchical architecture that processes inputs through progressively decreasing spatial resolutions and increasing semantic abstraction. This multi-scale fusion capability allows the framework to capture both local patterns and global context across modalities. Implemented with event-driven binary spike operations, SNNergy achieves remarkable energy efficiency while maintaining fusion effectiveness and establishing new state-of-the-art results on challenging audio-visual benchmarks, including CREMA-D, AVE, and UrbanSound8K-AV, significantly outperforming existing multimodal fusion baselines. Our framework advances multimodal fusion by introducing a scalable fusion mechanism that enables hierarchical cross-modal integration with practical energy efficiency for real-world audio-visual intelligence systems."}
{"id": "2602.00004", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00004", "abs": "https://arxiv.org/abs/2602.00004", "authors": ["Yue Yu", "Ting Bai", "HengZhi Lan", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Chuan Shi"], "title": "C$^2$-Cite: Contextual-Aware Citation Generation for Attributed Large Language Models", "comment": "WSDM26", "summary": "The attribution technique enhances the credibility of LLMs by adding citations to the generated sentences, enabling users to trace back to the original sources and verify the reliability of the output. However, existing instruction-tuned attributed LLMs often fail to properly interpret the contextual semantics of citation symbols (e.g., [i]) during text generation. This shortcoming arises from their insufficient awareness of the context information surrounding citation markers, which in turn leads to disjointed references and poor integration of retrieved knowledge into the generated content. To address this issue, we propose a novel \\textbf{C}ontextual-aware \\textbf{C}itation generation framework (\\textbf{C$^2$}-\\textbf{Cite}) that explicitly integrates the semantic relationships between citation markers and their referenced content. Specifically, a contextual citation alignment mechanism is adopted: it first encodes the retrieved document contexts into the symbol representation of citations, then aligns the marker numbers by decoding information from a citation router function. This mechanism enables the transformation of citation markers from generic placeholders into active knowledge pointers that link to the referenced source information. Experimental results on the ALCE benchmark across three datasets validate our framework C$^2$-Cite++: it outperforms the SOTA baseline by an average of 5.8\\% in citation quality and 17.4\\% in response correctness. The implementation is publicly available at https://github.com/BAI-LAB/c2cite"}
{"id": "2602.01134", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01134", "abs": "https://arxiv.org/abs/2602.01134", "authors": ["Qin Yuan", "Chunlei Li", "Xiangyong Zeng"], "title": "The structure and enumeration of periodic binary sequences with high nonlinear complexity", "comment": null, "summary": "Nonlinear complexity, as an important measure for assessing the randomness of sequences, is defined as the length of the shortest feedback shift registers that can generate a given sequence. In this paper, the structure of n-periodic binary sequences with nonlinear complexity larger than or equal to 3n/4 is characterized. Based on their structure, an exact enumeration formula for the number of such periodic sequences is determined."}
{"id": "2602.02487", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.02487", "abs": "https://arxiv.org/abs/2602.02487", "authors": ["Timothy Highley", "Tannah Duncan", "Ilia Volkov"], "title": "Carry-Over Lottery Allocation: Practical Incentive-Compatible Drafts", "comment": "28 pages, 4 figures", "summary": "The NBA Draft lottery is designed to promote competitive balance by awarding better draft positions to weaker teams, but it creates incentives to deliberately lose, a practice known as tanking. We propose a draft mechanism that is simultaneously practical, incentive-compatible, and advantages weaker teams. The \\textbf{Carry-Over Lottery Allocation (COLA) Draft Mechanism} represents a paradigm shift in evaluating team quality, replacing a single season's standings with playoff outcomes over multiple years. COLA uses a draft lottery where every non-playoff team receives the same number of lottery tickets, removing incentives to lose additional games after elimination. Lottery tickets that do not win a top draft pick carry over to future lotteries, while playoff success or winning a top pick diminishes a team's accumulated tickets. Over time, COLA rewards teams with poor long-term performance and less prior draft assistance. By retaining the lottery format, COLA preserves transparency and fan engagement.\n  Real-world implementation challenges are addressed to demonstrate feasibility, including transitioning from the current system, handling traded draft picks, and accommodating draft classes of varying strength. The most significant challenge occurs in years with exceptionally strong draft classes, where teams may prefer missing the playoffs in order to gain lottery access, violating a foundational assumption: that teams prefer playoff success to lottery participation. We provide a solution to this problem, employing a truth-elicitation mechanism to identify such years and expand lottery eligibility to include as many playoff teams as necessary to preserve anti-tanking incentives."}
{"id": "2602.01873", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01873", "abs": "https://arxiv.org/abs/2602.01873", "authors": ["Andrey Chursin", "Lefteris Kokoris-Kogias", "Alex Orlov", "Alberto Sonnino", "Igor Zablotchi"], "title": "Tidehunter: Large-Value Storage With Minimal Data Relocation", "comment": null, "summary": "Log-Structured Merge-Trees (LSM-trees) dominate persistent key-value storage but suffer from high write amplification from 10x to 30x under random workloads due to repeated compaction. This overhead becomes prohibitive for large values with uniformly distributed keys, a workload common in content-addressable storage, deduplication systems, and blockchain validators. We present Tidehunter, a storage engine that eliminates value compaction by treating the Write-Ahead Log (WAL) as permanent storage rather than a temporary recovery buffer. Values are never overwritten; and small, lazily-flushed index tables map keys to WAL positions. Tidehunter introduces (a) lock-free writes that saturate NVMe drives through atomic allocation and parallel copying, (b) an optimistic index structure that exploits uniform key distributions for single-roundtrip lookups, and (c) epoch-based pruning that reclaims space without blocking writes. On a 1 TB dataset with 1 KB values, Tidehunter achieves 830K writes per second, that is 8.4x higher than RocksDB and 2.9x higher than BlobDB, while improving point queries by 1.7x and existence checks by 15.6x. We validate real-world impact by integrating Tidehunter into Sui, a high-throughput blockchain, where it maintains stable throughput and latency under loads that cause RocksDB-backed validators to collapse. Tidehunter is production-ready and is being deployed in production within Sui."}
{"id": "2602.01104", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.01104", "abs": "https://arxiv.org/abs/2602.01104", "authors": ["Poojan Shah", "Shashwat Agrawal", "Ragesh Jaiswal"], "title": "Fast $k$-means Seeding Under The Manifold Hypothesis", "comment": null, "summary": "We study beyond worst case analysis for the $k$-means problem where the goal is to model typical instances of $k$-means arising in practice. Existing theoretical approaches provide guarantees under certain assumptions on the optimal solutions to $k$-means, making them difficult to validate in practice. We propose the manifold hypothesis, where data obtained in ambient dimension $D$ concentrates around a low dimensional manifold of intrinsic dimension $d$, as a reasonable assumption to model real world clustering instances. We identify key geometric properties of datasets which have theoretically predictable scaling laws depending on the quantization exponent $\\varepsilon = 2/d$ using techniques from optimum quantization theory. We show how to exploit these regularities to design a fast seeding method called $\\operatorname{Qkmeans}$ which provides $O(ρ^{-2} \\log k)$ approximate solutions to the $k$-means problem in time $O(nD) + \\widetilde{O}(\\varepsilon^{1+ρ}ρ^{-1}k^{1+γ})$; where the exponent $γ= \\varepsilon + ρ$ for an input parameter $ρ< 1$. This allows us to obtain new runtime - quality tradeoffs. We perform a large scale empirical study across various domains to validate our theoretical predictions and algorithm performance to bridge theory and practice for beyond worst case data clustering."}
{"id": "2602.01284", "categories": ["cs.MM", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01284", "abs": "https://arxiv.org/abs/2602.01284", "authors": ["Chen Chen", "Dion Hoe-Lian Goh"], "title": "Seeing, Hearing, and Knowing Together: Multimodal Strategies in Deepfake Videos Detection", "comment": null, "summary": "As deepfake videos become increasingly difficult for people to recognise, understanding the strategies humans use is key to designing effective media literacy interventions. We conducted a study with 195 participants between the ages of 21 and 40, who judged real and deepfake videos, rated their confidence, and reported the cues they relied on across visual, audio, and knowledge strategies. Participants were more accurate with real videos than with deepfakes and showed lower expected calibration error for real content. Through association rule mining, we identified cue combinations that shaped performance. Visual appearance, vocal, and intuition often co-occurred for successful identifications, which highlights the importance of multimodal approaches in human detection. Our findings show which cues help or hinder detection and suggest directions for designing media literacy tools that guide effective cue use. Building on these insights can help people improve their identification skills and become more resilient to deceptive digital media."}
{"id": "2602.00005", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00005", "abs": "https://arxiv.org/abs/2602.00005", "authors": ["Shuai Wang", "Harrisen Scells", "Bevan Koopman", "Guido Zuccon"], "title": "AutoBool: An Reinforcement-Learning trained LLM for Effective Automated Boolean Query Generation for Systematic Reviews", "comment": null, "summary": "We present AutoBool, a reinforcement learning (RL) framework that trains large language models (LLMs) to generate effective Boolean queries for medical systematic reviews. Boolean queries are the primary mechanism for literature retrieval in this domain and must achieve high recall while maintaining reasonable precision - a challenging balance that existing prompt-based LLM approaches often struggle to achieve. A major limitation in this space is the lack of high-quality ground-truth Boolean queries for each topic, which makes supervised fine-tuning impractical. AutoBool addresses this challenge by using RL to directly optimize query generation with retrieval measures, without requiring target queries. To support this effort, we create and release the largest dataset of its kind: 65588 topics in total for training and evaluating the task of automatic Boolean query formulation. Experiments on our new dataset and two established datasets (CLEF TAR and Seed Collection) show that AutoBool significantly outperforms zero shot/few shot prompting and matches or exceeds the effectiveness of much larger GPT-based models (e.g., GPT-4o, O3) using smaller backbones. It also approaches effectiveness of expert-authored queries while retrieving 10 to 16 times fewer documents. Ablation studies reveal the critical roles of model backbone, size, decoding temperature, and prompt design. Code and data are available at https://github.com/ielab/AutoBool."}
{"id": "2602.01151", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01151", "abs": "https://arxiv.org/abs/2602.01151", "authors": ["Yubo Sun", "Gennian Ge"], "title": "On the Palindromic/Reverse-Complement Duplication Correcting Codes", "comment": null, "summary": "Motivated by applications in in-vivo DNA storage, we study codes for correcting duplications. A reverse-complement duplication of length $k$ is the insertion of the reversed and complemented copy of a substring of length $k$ adjacent to its original position, while a palindromic duplication only inserts the reversed copy without complementation. We first construct an explicit code with a single redundant symbol capable of correcting an arbitrary number of reverse-complement duplications (respectively, palindromic duplications), provided that all duplications have length $k \\ge 3\\lceil \\log_q n \\rceil$ and are disjoint. Next, we derive a Gilbert-Varshamov bound for codes that can correct a reverse-complement duplication (respectively, palindromic duplication) of arbitrary length, showing that the optimal redundancy is upper bounded by $2\\log_q n + \\log_q\\log_q n + O(1)$. Finally, for $q \\ge 4$, we present two explicit constructions of codes that can correct $t$ length-one reverse-complement duplications. The first construction achieves a redundancy of $2t\\log_q n + O(\\log_q\\log_q n)$ with encoding complexity $O(n)$ and decoding complexity $O\\big(n(\\log_2 n)^4\\big)$. The second construction achieves an improved redundancy of $(2t-1)\\log_q n + O(\\log_q\\log_q n)$, but with encoding and decoding complexities of $O\\big(n \\cdot \\mathrm{poly}(\\log_2 n)\\big)$."}
{"id": "2602.01952", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01952", "abs": "https://arxiv.org/abs/2602.01952", "authors": ["Wenjia Jiang", "Yiwei Wang", "Boyan Han", "Joey Tianyi Zhou", "Chi Zhang"], "title": "SQLAgent: Learning to Explore Before Generating as a Data Engineer", "comment": null, "summary": "Large Language Models have recently shown impressive capabilities in reasoning and code generation, making them promising tools for natural language interfaces to relational databases. However, existing approaches often fail to generalize in complex, real-world settings due to the highly database-specific nature of SQL reasoning, which requires deep familiarity with unique schemas, ambiguous semantics, and intricate join paths. To address this challenge, we introduce a novel two-stage LLM-based framework that decouples knowledge acquisition from query generation. In the Exploration Stage, the system autonomously constructs a database-specific knowledge base by navigating the schema with a Monte Carlo Tree Search-inspired strategy, generating triplets of schema fragments, executable queries, and natural language descriptions as usage examples. In the Deployment Stage, a dual-agent system leverages the collected knowledge as in-context examples to iteratively retrieve relevant information and generate accurate SQL queries in response to user questions. This design enables the agent to proactively familiarize itself with unseen databases and handle complex, multi-step reasoning. Extensive experiments on large-scale benchmarks demonstrate that our approach significantly improves accuracy over strong baselines, highlighting its effectiveness and generalizability."}
{"id": "2602.01350", "categories": ["cs.DS", "cs.DM", "cs.SE", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.01350", "abs": "https://arxiv.org/abs/2602.01350", "authors": ["Arnav Khinvasara", "Alexander Pikovski"], "title": "Benchmarking of algorithms for set partitions", "comment": null, "summary": "Set partitions are arrangements of distinct objects into groups. The problem of listing all set partitions arises in a variety of settings, in particular in combinatorial optimization tasks. After a brief review, we give practical approximate formulas for determining the number of set partitions, both for small and large set sizes. Several algorithms for enumerating all set partitions are reviewed, and benchmarking tests were conducted. The algorithm of Djokic et al. is recommended for practical use."}
{"id": "2602.01833", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2602.01833", "abs": "https://arxiv.org/abs/2602.01833", "authors": ["Xiang Li", "Xiaoming Zhang", "Dezhuang Miao", "Xianfu Cheng", "Dawei Li", "Honggui Han", "Zhoujun Li"], "title": "Mixture of Disentangled Experts with Missing Modalities for Robust Multimodal Sentiment Analysis", "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) integrates multiple modalities to infer human sentiment, but real-world noise often leads to missing or corrupted data. However, existing feature-disentangled methods struggle to handle the internal variations of heterogeneous information under uncertain missingness, making it difficult to learn effective multimodal representations from degraded modalities. To address this issue, we propose DERL, a Disentangled Expert Representation Learning framework for robust MSA. Specifically, DERL employs hybrid experts to adaptively disentangle multimodal inputs into orthogonal private and shared representation spaces. A multi-level reconstruction strategy is further developed to provide collaborative supervision, enhancing both the expressiveness and robustness of the learned representations. Finally, the disentangled features act as modality experts with distinct roles to generate importance-aware fusion results. Extensive experiments on two MSA benchmarks demonstrate that DERL outperforms state-of-the-art methods under various missing-modality conditions. For instance, our method achieves improvements of 2.47% in Acc-2 and 2.25% in MAE on MOSI under intra-modal missingness."}
{"id": "2602.00006", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00006", "abs": "https://arxiv.org/abs/2602.00006", "authors": ["Arun Kavishwar", "William Lotter"], "title": "FDA AI Search: Making FDA-Authorized AI Devices Searchable", "comment": "Findings paper presented at the 5th Machine Learning for Health (ML4H) Symposium (2025)", "summary": "Over 1,200 AI-enabled medical devices have received marketing authorization from the U.S. FDA, yet identifying devices suited to specific clinical needs remains challenging because the FDA's databases contain only limited metadata and non-searchable summary PDFs. To address this gap, we developed FDA AI Search, a website that enables semantic querying of FDA-authorized AI-enabled devices. The backend includes an embedding-based retrieval system, where LLM-extracted features from authorization summaries are compared to user queries to find relevant matches. We present quantitative and qualitative evaluation that support the effectiveness of the retrieval algorithm compared to keyword-based methods. As FDA-authorized AI devices become increasingly prevalent and their use cases expand, we envision that the tool will assist healthcare providers in identifying devices aligned with their clinical needs and support developers in formulating novel AI applications."}
{"id": "2602.01154", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01154", "abs": "https://arxiv.org/abs/2602.01154", "authors": ["Xiaofeng Liu", "Jun Zhang", "Fang-Wei Fu"], "title": "A class of pseudorandom sequences From Function Fields", "comment": null, "summary": "Motivated by the constructions of pseudorandom sequences over the cyclic elliptic function fields by Hu \\textit{et al.} in \\text{[IEEE Trans. Inf. Theory, 53(7), 2007]} and the constructions of low-correlation, large linear span binary sequences from function fields by Xing \\textit{et al.} in \\text{[IEEE Trans. Inf. Theory, 49(6), 2003]}, we utilize the bound derived by Weil \\text{[Basic Number Theory, Grund. der Math. Wiss.,\n  Bd 144]} and Deligne \\text{[ Lecture Notes in Mathematics, vol. 569 (Springer, Berlin, 1977)]} for the exponential sums over the general algebraic function fields and study the periods, linear complexities, linear complexity profiles, distributions of $r-$patterns, period correlation and nonlinear complexities for a class of $p-$ary sequences that generalize the constructions in \\text{[IEEE Trans. Inf. Theory, 49(6), 2003]} and [IEEE Trans. Inf. Theory, 53(7), 2007]."}
{"id": "2602.02025", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02025", "abs": "https://arxiv.org/abs/2602.02025", "authors": ["Serafeim Papadias", "Kostas Patroumpas", "Dimitrios Skoutas"], "title": "Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data", "comment": "13 pages, 7 figures, 9 tables", "summary": "Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance."}
{"id": "2602.01462", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.01462", "abs": "https://arxiv.org/abs/2602.01462", "authors": ["Miles Simmons", "Ishan Bansal", "Joe Cheriyan"], "title": "A $5$-Approximation Analysis for the Cover Small Cuts Problem", "comment": null, "summary": "In the Cover Small Cuts problem, we are given a capacitated (undirected) graph $G=(V,E,u)$ and a threshold value $λ$, as well as a set of links $L$ with end-nodes in $V$ and a non-negative cost for each link $\\ell\\in L$; the goal is to find a minimum-cost set of links such that each non-trivial cut of capacity less than $λ$ is covered by a link. Bansal, Cheriyan, Grout, and Ibrahimpur (arXiv:2209.11209, Algorithmica 2024) showed that the WGMV primal-dual algorithm, due to Williamson, Goemans, Mihail, and Vazirani (Combinatorica, 1995), achieves approximation ratio $16$ for the Cover Small Cuts problem; their analysis uses the notion of a pliable family of sets that satisfies a combinatorial property. Later, Bansal (arXiv:2308.15714v2, IPCO 2025) and then Nutov (arXiv:2504.03910, MFCS 2025) proved that the same algorithm achieves approximation ratio $6$. We show that the same algorithm achieves approximation ratio $5$, by using a stronger notion, namely, a pliable family of sets that satisfies symmetry and structural submodularity."}
{"id": "2602.00008", "categories": ["cs.IR", "cs.CY", "cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.00008", "abs": "https://arxiv.org/abs/2602.00008", "authors": ["He Wang", "Ziyu Zhou", "Hanxiang Liu"], "title": "Front-Loaded or Balanced? The Mechanism through Which Review Order Affects Overall Ratings in Premium Service Settings", "comment": null, "summary": "In the increasingly prevalent landscape of high-quality service contexts, whether consumer evaluation interfaces adopt a rating-first or review-first sequence has become a critical factor shaping rating authenticity and feedback quality. While prior research has primarily examined review content and sentiment, systematic investigation into how evaluation order influences rating outcomes remains limited. Through exploratory analyses, we find that Letterboxd -- which employs a review-first, rating-after mechanism -- exhibits a more centralized rating distribution with fewer extreme scores, whereas Yelp -- which adopts a rating-first, review-after mechanism -- shows a pronounced bimodal distribution with more polarized ratings. Three controlled experiments further demonstrate that in high-quality service contexts, a rating-first (vs. review-first) interface significantly elevates consumers' overall ratings. Mechanism analyses indicate that cognitive effort and affective heuristics serve as dual pathways: a rating-first (vs. review-first) sequence reduces cognitive effort and heightens affective heuristics, thereby increasing rating scores. Moreover, service quality moderates this process. When service quality is low, the rating-first (vs. review-first) sequence instead leads to lower ratings. This research reveals the psychological mechanisms through which evaluation order affects consumer ratings via cognitive and affective pathways. It extends theoretical understanding of online rating formation and offers practical implications for optimizing platform interface design to enhance rating authenticity and credibility."}
{"id": "2602.01174", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01174", "abs": "https://arxiv.org/abs/2602.01174", "authors": ["Li Wan", "Wenyi Zhang"], "title": "Reducing ORBGRAND Latency via Partial Gaussian Elimination", "comment": null, "summary": "Guessing Random Additive Noise Decoding (GRAND) is a universal framework for decoding all block codes by testing candidate error patterns (EPs). Ordered Reliability Bits GRAND (ORBGRAND) facilitates parallel implementation of GRAND by exploiting log-likelihood ratio (LLR) rankings but still suffers from high tail latency under unfavorable channel conditions, limiting its use in real-time systems.\n  We propose an elimination-aided ORBGRAND scheme that reduces decoding latency by integrating the Rank of the Most Reliable Erroneous (RMRE) bit with a partial Gaussian-elimination (GE) filtering mechanism. The scheme groups and jointly verifies EPs that share the same RMRE, and once a valid EP is identified, the ORBGRAND search is resumed. By leveraging prior GE steps to filter out unnecessary guesses, this approach significantly reduces the number of EPs to be tested, thereby lowering both average and worst-case latency while maintaining error-correction performance.\n  Simulation results show that compared to the original ORBGRAND, the elimination-aided ORBGRAND filters out more than 50\\% of EPs and correspondingly reduce overall computational complexity, all with no loss in block error rate. This demonstrates that this approach is suitable for ultra-reliable low-latency communication scenarios."}
{"id": "2602.02057", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.02057", "abs": "https://arxiv.org/abs/2602.02057", "authors": ["Anıl Eren Göçer", "Ioanna Tsakalidou", "Hamish Nicholson", "Kyoungmin Kim", "Anastasia Ailamaki"], "title": "QVCache: A Query-Aware Vector Cache", "comment": null, "summary": "Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. However, scaling approximate nearest neighbor (ANN) search to high recall under strict latency SLOs remains fundamentally constrained by memory capacity and I/O bandwidth. Disk-based vector search systems suffer severe latency degradation at high accuracy, while fully in-memory solutions incur prohibitive memory costs at billion-scale. Despite the central role of caching in traditional databases, vector search lacks a general query-level caching layer capable of amortizing repeated query work.\n  We present QVCache, the first backend-agnostic, query-level caching system for ANN search with bounded memory footprint. QVCache exploits semantic query repetition by performing similarity-aware caching rather than exact-match lookup. It dynamically learns region-specific distance thresholds using an online learning algorithm, enabling recall-preserving cache hits while bounding lookup latency and memory usage independently of dataset size. QVCache operates as a drop-in layer for existing vector databases. It maintains a megabyte-scale memory footprint and achieves sub-millisecond cache-hit latency, reducing end-to-end query latency by up to 40-1000x when integrated with existing ANN systems. For workloads exhibiting temporal-semantic locality, QVCache substantially reduces latency while preserving recall comparable to the underlying ANN backend, establishing it as a missing but essential caching layer for scalable vector search."}
{"id": "2602.01755", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2602.01755", "abs": "https://arxiv.org/abs/2602.01755", "authors": ["Luis M. B. Varona"], "title": "A polynomial-time algorithm for recognizing high-bandwidth graphs", "comment": "15 pages, 4 tables", "summary": "An unweighted, undirected graph $G$ on $n$ nodes is said to have \\emph{bandwidth} at most $k$ if its nodes can be labelled from $0$ to $n - 1$ such that no two adjacent nodes have labels that differ by more than $k$. It is known that one can decide whether the bandwidth of $G$ is at most $k$ in $O(n^k)$ time and $O(n^k)$ space using dynamic programming techniques. For small $k$ close to $0$, this approach is effectively polynomial, but as $k$ scales with $n$, it becomes superexponential, requiring up to $O(n^{n - 1})$ time (where $n - 1$ is the maximum possible bandwidth). In this paper, we reformulate the problem in terms of bipartite matching for sufficiently large $k \\ge \\lfloor (n - 1)/2 \\rfloor$, allowing us to use Hall's marriage theorem to develop an algorithm that runs in $O(n^{n - k + 1})$ time and $O(n)$ auxiliary space (beyond storage of the input graph). This yields polynomial complexity for large $k$ close to $n - 1$, demonstrating that the bandwidth recognition problem is solvable in polynomial time whenever either $k$ or $n - k$ remains small."}
{"id": "2602.00010", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00010", "abs": "https://arxiv.org/abs/2602.00010", "authors": ["Mathieu Ciancone", "Clovis Varangot-Reille", "Marion Schaeffer"], "title": "ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking", "comment": null, "summary": "In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases."}
{"id": "2602.01229", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.01229", "abs": "https://arxiv.org/abs/2602.01229", "authors": ["Hamed Talebian", "Aamir Mahmood", "Mikael Gidlund"], "title": "L-Moment-Based LOS and NLOS Channel Characterization via Four-parameter Kappa Distribution for AoA BLE CTE Measurements", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Bluetooth Low Energy (BLE) CTE transmissions provide in-phase and quadrature (IQ) samples whose empirical statistics are strongly governed by the propagation regime. in particular, the distributions differ markedly between line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. In NLOS, multipath-induced distortions typically degrade Angle-of-Arrivial (AoA) estimation accuracy. Existing BLE direction finding datasets rarely provide tightly controlled, IQ-level paired LOS and NLOS measurements with rigorous statistical validation, and commonly used flat-fading models can be inadequate for cluttered indoor environments exhibiting heavy-tailed power distributions. To address these limitations, we conduct a paired-geometry BLE AoA measurement campaign using an off-the-shelf module, collecting 132000 labeled CTE packets under matched anchor-tag conditions. A robust preprocessing stage removes anomalous CTEs using combined univariate and multivariate criteria. Feature-wise hypothesis tests on IQ-derived power features confirm strong LOS and NLOS separability. All mean differences are statistically significant; additionally, 92 percent of feature-wise variance differences are significant. We further compute L-moment ratios (LMRs) and analyze them in the L-moment Ratio Diagram (LMRD), showing that NLOS subsets exhibit markedly heavier tails and stronger asymmetry than LOS. Kappa-family distributions fitted from LMRs provide substantially improved dual scored L--moment goodness-of-fit (GoF), Specifically, for NLOS, which is the smallest discrepancy in the LMRD and a near-zero standardized L-kurtosis deviation. As a practice, we apply a self-supervised clustering to L-moment statistics, achieving a more separable representation, compared to product moments."}
{"id": "2602.02254", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.02254", "abs": "https://arxiv.org/abs/2602.02254", "authors": ["Samuel McCauley", "Benjamin Moseley", "Helia Niaparast", "Shikha Singh"], "title": "Stable Matching with Predictions: Robustness and Efficiency under Pruned Preferences", "comment": null, "summary": "In this paper, we study the fundamental problem of finding a stable matching in two-sided matching markets. In the classic variant, it is assumed that both sides of the market submit a ranked list of all agents on the other side. However, in large matching markets such as the National Resident Matching Program (NRMP), it is infeasible for hospitals to interview or mutually rank each resident. In this paper, we study the stable matching problem with truncated preference lists. In particular, we assume that, based on historical datasets, each hospital has a predicted rank of its likely match and only ranks residents within a bounded interval around that prediction.\n  We use the algorithms-with-predictions framework and show that the classic deferred-acceptance (DA) algorithm used to compute stable matchings is robust to such truncation. We present two algorithms and theoretically and empirically evaluate their performance. Our results show that even with reasonably accurate predictions, it is possible to significantly cut down on both instance size (the length of preference lists) as well as the number of proposals made. These results explain the practical success of the DA algorithm and connect market design to the emerging theory of algorithms with predictions."}
{"id": "2602.00011", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00011", "abs": "https://arxiv.org/abs/2602.00011", "authors": ["Fatima Nasser", "Fouad Trad", "Ammar Mohanna", "Ghada El-Hajj Fuleihan", "Ali Chehab"], "title": "Chained Prompting for Better Systematic Review Search Strategies", "comment": "Accepted in the 3rd International Conference on Foundation and Large Language Models (FLLM2025)", "summary": "Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice."}
{"id": "2602.01383", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01383", "abs": "https://arxiv.org/abs/2602.01383", "authors": ["Atif Ahmad Khan", "Shakir Ali", "Elif Segah Oztas", "Abhishek Kesarwani"], "title": "MDS matrices from skew polynomials with automorphisms and derivations", "comment": null, "summary": "Maximum Distance Separable (MDS) matrices play a central role in coding theory and symmetric-key cryptography due to their optimal diffusion properties. In this paper, we present a construction of MDS matrices using skew polynomial rings \\( \\mathbb{F}_q[X;θ,δ] \\), where \\( θ\\) is an automorphism and \\( δ\\) is a \\( θ\\)-derivation on \\( \\mathbb{F}_q \\). We introduce the notion of \\( δ_θ \\)-circulant matrices and study their structural properties. Necessary and sufficient conditions are derived under which these matrices are involutory and satisfy the MDS property. The resulting $δ_θ$-circulant matrix can be viewed as a generalization of classical constructions obtained in the absence of $θ$-derivations. One of the main contribution of this work is the construction of quasi recursive MDS matrices. In the setting of the skew polynomial ring $\\mathbb{F}_q[X;θ]$, we construct quasi recursive MDS matrices associated with companion matrices.\n  These matrices are shown to be involutory, yielding a strict improvement over the quasi-involutory constructions previously reported in the literature. Several illustrative results and examples are also provided."}
{"id": "2602.00013", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00013", "abs": "https://arxiv.org/abs/2602.00013", "authors": ["Vipul Dinesh Pawar"], "title": "Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking", "comment": null, "summary": "In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time."}
{"id": "2602.01555", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01555", "abs": "https://arxiv.org/abs/2602.01555", "authors": ["Inki Kim", "Hyuntae Ahn", "Yongjune Kim", "Hee-Youl Kwak", "Dae-Young Yun", "Sang-Hyo Kim"], "title": "Design of Root Protograph LDPC Codes Simultaneously Achieving Full Diversity and High Coding Gain", "comment": "Preprint", "summary": "This paper presents a novel design framework for protograph-based LDPC codes that simultaneously achieves full diversity in block-fading channels (BFCs) and nearcapacity performance in additive white Gaussian noise channels (AWGNCs). By leveraging a Boolean approximation-based analysis--Diversity Evolution (DivE)--we derive structural constraints for generalized rootchecks that guarantee full diversity. Based on these constraints, we propose a protograph template tailored for two-block BFCs. Furthermore, we employ a genetic algorithm guided by density evolution to optimize the protograph edges within this template for superior AWGNC performance. The resulting codes effectively bridge the gap between diversityoriented and capacity-oriented designs, exhibiting robust performance across both channel environments."}
{"id": "2602.00052", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00052", "abs": "https://arxiv.org/abs/2602.00052", "authors": ["Ramtin Babaeipour", "François Charest", "Madison Wright"], "title": "AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows", "comment": null, "summary": "Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring."}
{"id": "2602.01582", "categories": ["cs.IT", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01582", "abs": "https://arxiv.org/abs/2602.01582", "authors": ["Haoyu Lei", "Mohammad Jalali", "Chin Wa Lau", "Farzan Farnia"], "title": "On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations", "comment": null, "summary": "Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, and what cost is paid to achieve them? In this work, we study this question through the lens of robustness to distributional shifts at the channel output. We evaluate both input-dependent adversarial perturbations (FGM and projected gradient methods under $\\ell_2$ constraints) and universal adversarial perturbations that apply a single norm-bounded shift to all received vectors. Our results show that recent AI decoders, including ECCT and CrossMPT, could suffer significant performance degradation under such perturbations, despite superior nominal performance under i.i.d. AWGN. Moreover, adversarial perturbations transfer relatively strongly between AI decoders but weakly to BP-based decoders, and universal perturbations are substantially more harmful than random perturbations of equal norm. These numerical findings suggest a potential robustness cost and higher sensitivity to channel distribution underlying recent AI decoding gains."}
{"id": "2602.00083", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00083", "abs": "https://arxiv.org/abs/2602.00083", "authors": ["Yuxin Yang", "Gangda Deng", "Ömer Faruk Akgül", "Nima Chitsazan", "Yash Govilkar", "Akasha Tigalappanavara", "Shi-Xiong Zhang", "Sambit Sahu", "Viktor Prasanna"], "title": "SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost."}
{"id": "2602.01602", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01602", "abs": "https://arxiv.org/abs/2602.01602", "authors": ["Sanghyeon Cho", "Taewoo Park", "Seong-Joon Park", "Dae-Young Yun", "Hee-Youl Kwak", "Sang-Hyo Kim", "Yongjune Kim"], "title": "Spectral-Aligned Pruning for Universal Error-Correcting Code Transformers", "comment": null, "summary": "Recently, the Foundation Error Correction Code Transformer (FECCT) has emerged as a promising universal channel decoder, achieving competitive decoding performance across diverse code families by relying on a single shared model backbone, optionally followed by code-specific retraining. Despite this flexibility, the high computational complexity and large parameter footprint of transformer-based decoders present substantial obstacles to practical deployment. To address these challenges, we investigate structured pruning for FECCT and propose Spectral-Aligned Pruning (SAP), a structure-aware framework that enables cross-code reuse of structured pruning masks across codes by leveraging the spectrum of the corresponding bipartite graph. After pruning, SAP performs per-code recovery via parameter-efficient low-rank adaptation (LoRA), enabling a shared pruned backbone while storing only small code-specific adapter parameters. Experiments across diverse codes show that SAP achieves decoding performance comparable to dedicated per-code pruning, while enabling substantial reductions in computational cost and model memory footprint through kernel-level structured pruning."}
{"id": "2602.00296", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00296", "abs": "https://arxiv.org/abs/2602.00296", "authors": ["Ziqi Wang", "Xi Zhu", "Shuhang Lin", "Haochen Xue", "Minghao Guo", "Yongfeng Zhang"], "title": "RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a core paradigm for grounding large language models with external knowledge. Despite extensive efforts exploring diverse retrieval strategies, existing studies predominantly focus on query-side complexity or isolated method improvements, lacking a systematic understanding of how RAG paradigms behave across different query-corpus contexts and effectiveness-efficiency trade-offs. In this work, we introduce RAGRouter-Bench, the first dataset and benchmark designed for adaptive RAG routing. RAGRouter-Bench revisits retrieval from a query-corpus compatibility perspective and standardizes five representative RAG paradigms for systematic evaluation across 7,727 queries and 21,460 documents spanning diverse domains. The benchmark incorporates three canonical query types together with fine-grained semantic and structural corpus metrics, as well as a unified evaluation for both generation quality and resource consumption. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that no single RAG paradigm is universally optimal, that paradigm applicability is strongly shaped by query-corpus interactions, and that increased advanced mechanism does not necessarily yield better effectiveness-efficiency trade-offs. These findings underscore the necessity of routing-aware evaluation and establish a foundation for adaptive, interpretable, and generalizable next-generation RAG systems."}
{"id": "2602.01653", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.01653", "abs": "https://arxiv.org/abs/2602.01653", "authors": ["Enyu Shi", "Yiyang Zhu", "Jiayi Zhang", "Ziheng Liu", "Jiakang Zheng", "Jiancheng An", "Derrick Wing Kwan Ng", "Bo Ai", "Chau Yuen"], "title": "Low-Complexity Multi-Agent Continual Learning for Stacked Intelligent Metasurface-Assisted Secure Communications", "comment": "Enyu Shi and Yiyang Zhu contributed equally to this work", "summary": "Stacked intelligent metasurfaces (SIMs), composed of multiple layers of reconfigurable transmissive metasurfaces, are gaining prominence as a transformative technology for future wireless communication security. This paper investigates the integration of SIM into multi-user multiple-input multiple-output (MIMO) systems to enhance physical layer security. A novel system architecture is proposed, wherein each base station (BS) antenna transmits a dedicated single-user stream, while a multi-layer SIM executes wave-based beamforming in the electromagnetic domain, thereby avoiding the need for complex baseband digital precoding and significantly reducing hardware overhead. To maximize the weighted sum secrecy rate (WSSR), we formulate a joint precoding optimization problem over BS power allocation and SIM phase shifts, which is high-dimensional and non-convex due to the complexity of the objective function and the coupling among optimization variables. To address this, we propose a manifold-enhanced heterogeneous multi-agent continual learning (MHACL) framework that incorporates gradient representation and dual-scale policy optimization to achieve robust performance in dynamic environments with high demands for secure communication. Furthermore, we develop SIM-MHACL (SIMHACL), a low-complexity learning template that embeds phase coordination into a product manifold structure, reducing the exponential search space to linear complexity while maintaining physical feasibility. Simulation results validate that the proposed framework achieves millisecond-level per-iteratio ntraining in SIM-assisted systems, significantly outperforming various baseline schemes, with SIMHACL achieving comparable WSSR to MHACL while reducing computation time by 30\\%."}
{"id": "2602.00495", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00495", "abs": "https://arxiv.org/abs/2602.00495", "authors": ["Yiteng Tu", "Weihang Su", "Shuguang Han", "Yiqun Liu", "Qingyao Ai"], "title": "Equity vs. Equality: Optimizing Ranking Fairness for Tailored Provider Needs", "comment": null, "summary": "Ranking plays a central role in connecting users and providers in Information Retrieval (IR) systems, making provider-side fairness an important challenge. While recent research has begun to address fairness in ranking, most existing approaches adopt an equality-based perspective, aiming to ensure that providers with similar content receive similar exposure. However, it overlooks the diverse needs of real-world providers, whose utility from ranking may depend not only on exposure but also on outcomes like sales or engagement. Consequently, exposure-based fairness may not accurately capture the true utility perceived by different providers with varying priorities. To this end, we introduce an equity-oriented fairness framework that explicitly models each provider's preferences over key outcomes such as exposure and sales, thus evaluating whether a ranking algorithm can fulfill these individualized goals while maintaining overall fairness across providers. Based on this framework, we develop EquityRank, a gradient-based algorithm that jointly optimizes user-side effectiveness and provider-side equity. Extensive offline and online simulations demonstrate that EquityRank offers improved trade-offs between effectiveness and fairness and adapts to heterogeneous provider needs."}
{"id": "2602.01657", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01657", "abs": "https://arxiv.org/abs/2602.01657", "authors": ["Yujun Ji", "Ling Liu", "Shanxiang Lyu", "Chao Chen", "Tao Dai", "Baoming Bai"], "title": "Decoding Golay Codes and their Related Lattices: A PAC Code Perspective", "comment": null, "summary": "In this work, we propose a decoding method of Golay codes from the perspective of Polarization Adjusted Convolutional (PAC) codes. By invoking Forney's cubing construction of Golay codes and their generators $G^*(8,7)/(8,4)$, we found different construction methods of Golay codes from PAC codes, which result in an efficient parallel list decoding algorithm with near-maximum likelihood performance. Compared with existing methods, our method can get rid of index permutation and codeword puncturing. Using the new decoding method, some related lattices, such as Leech lattice $Λ_{24}$ and its principal sublattice $H_{24}$, can be also decoded efficiently."}
{"id": "2602.00632", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00632", "abs": "https://arxiv.org/abs/2602.00632", "authors": ["Hongxun Ding", "Keqin Bao", "Jizhi Zhang", "Yi Fang", "Wenxin Xu", "Fuli Feng", "Xiangnan He"], "title": "Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation", "comment": null, "summary": "While Long Chain-of-Thought (Long CoT) reasoning has shown promise in Large Language Models (LLMs), its adoption for enhancing recommendation quality is growing rapidly. In this work, we critically examine this trend and argue that Long CoT is inherently ill-suited for the sequential recommendation domain. We attribute this misalignment to two primary factors: excessive inference latency and the lack of explicit cognitive reasoning patterns in user behavioral data. Driven by these observations, we propose pivoting away from the CoT structure to directly leverage its underlying mechanism: Reinforcement Learning (RL), to explore the item space. However, applying RL directly faces significant obstacles, notably low sample efficiency-where most actions fail to provide learning signals-and training instability. To overcome these limitations, we propose RISER, a novel Reinforced Item Space Exploration framework for Recommendation. RISER is designed to transform non-learnable trajectories into effective pairwise preference data for optimization. Furthermore, it incorporates specific strategies to ensure stability, including the prevention of redundant rollouts and the constraint of token-level update magnitudes. Extensive experiments on three real-world datasets show that RISER significantly outperforms competitive baselines, establishing a robust paradigm for RL-enhanced LLM recommendation. Our code will be available at https://anonymous.4open.science/r/RISER/."}
{"id": "2602.01802", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01802", "abs": "https://arxiv.org/abs/2602.01802", "authors": ["Ke Feng", "François Baccelli", "Catherine Rosenberg"], "title": "Performance Guarantees of Cellular Networks with Hardcore Regulation and Scheduling", "comment": "presented at IEEE Globecom 2025, Taipei", "summary": "Providing performance guarantees is one of the {critical} objectives of {recent and future} communication networks, toward which regulations, {i.e., constraints on key system parameters,} have played an indispensable role. This is the case for large wireless communication networks, where spatial regulations (e.g., constraints on intercell distance) have recently been shown, through a spatial network calculus, to be essential for establishing provable wireless link-level guarantees. In this work, we focus on performance guarantees for {the downlink of} cellular networks where we impose a hardcore (spatial) regulation on base station (BS) locations and evaluate {how BS scheduling (which controls which BSs can transmit at a given time) impacts performance}. Hardcore regulation is the simplest form of spatial regulation that enforces a minimal distance between any pair of transmitters in the network. Within this framework of spatial network calculus, we first provide an upper bound on the power of total interference for a spatially regulated cellular network, and then, identify the regimes where scheduling BSs yields {better} link-level rate guarantees compared to scenarios where base stations are always active. The hexagonal cellular network is analyzed as a special case. The results offer insights into what spatial regulations are needed, when to choose scheduling, and how to potentially reduce the network power consumption {to provide a certain target performance guarantee}."}
{"id": "2602.00682", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00682", "abs": "https://arxiv.org/abs/2602.00682", "authors": ["Yuecheng Li", "Hengwei Ju", "Zeyu Song", "Wei Yang", "Chi Lu", "Peng Jiang", "Kun Gai"], "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment", "comment": "Under Review", "summary": "Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec."}
{"id": "2602.01829", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.01829", "abs": "https://arxiv.org/abs/2602.01829", "authors": ["Shumin Yao", "Hui Du", "Lifeng Xie", "Yaping Sun", "Hao Chen", "Nan Ma", "Xiaodong Xu"], "title": "Zero-Shot Knowledge Base Resizing for Rate-Adaptive Digital Semantic Communication", "comment": null, "summary": "Digital semantic communication systems, which often leverage the Vector Quantized Variational Autoencoder (VQ-VAE) framework, are pivotal for future wireless networks. In a VQ-VAE-based semantic communication system, the transmission rate is directly governed by the size of a discrete codebook known as knowledge base (KB). However, the KB size is a fixed hyperparameter, meaning that adapting the rate requires training and storing a separate model for each desired size -- a practice that is too computationally and storage-prohibitive to achieve truly granular rate control. To address this, we introduce a principled, zero-shot KB resizing method that enables on-the-fly rate adaptation without any retraining. Our approach establishes a global importance ranking for all vectors within a single, large parent KB by uncovering its inherent semantic hierarchy. This is achieved via a three-step framework: 1) embedding KB vectors into hyperbolic space to reveal their hierarchical relationships; 2) constructing a master semantic tree using a minimum spanning tree algorithm; 3) enabling instant resizing by iteratively pruning the least important leaf nodes. Extensive simulations demonstrate that our method achieves reconstruction quality nearly identical to that of dedicated KBs trained from scratch, while demanding only a fraction of the computational budget. Moreover, our approach exhibits superior robustness at very low rates, where conventional KBs suffer from catastrophic failure. Our work resolves a fundamental limitation of VQ-VAE-based semantic communication systems, offering a practical and efficient path toward flexible and rate-adaptive semantic communication."}
{"id": "2602.00727", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00727", "abs": "https://arxiv.org/abs/2602.00727", "authors": ["Fangda Chen", "Yueyang Wang", "Chaoli Lou", "Min Gao", "Qingyu Xiong"], "title": "SWGCN: Synergy Weighted Graph Convolutional Network for Multi-Behavior Recommendation", "comment": "Accepted by Information Sciences", "summary": "Multi-behavior recommendation paradigms have emerged to capture diverse user activities, forecasting primary conversions (e.g., purchases) by leveraging secondary signals like browsing history. However, current graph-based methods often overlook cross-behavioral synergistic signals and fine-grained intensity of individual actions. Motivated by the need to overcome these shortcomings, we introduce Synergy Weighted Graph Convolutional Network (SWGCN). SWGCN introduces two novel components: a Target Preference Weigher, which adaptively assigns weights to user-item interactions within each behavior, and a Synergy Alignment Task, which guides its training by leveraging an Auxiliary Preference Valuator. This task prioritizes interactions from synergistic signals that more accurately reflect user preferences. The performance of our model is rigorously evaluated through comprehensive tests on three open-source datasets, specifically Taobao, IJCAI, and Beibei. On the Taobao dataset, SWGCN yields relative gains of 112.49% and 156.36% in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), respectively. It also yields consistent gains on IJCAI and Beibei, confirming its robustness and generalizability across various datasets. Our implementation is open-sourced and can be accessed via https://github.com/FangdChen/SWGCN."}
{"id": "2602.02131", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02131", "abs": "https://arxiv.org/abs/2602.02131", "authors": ["Qian Zhang", "Ju Liu", "Yao Ge", "Yufei Zhao", "Wali Ullah Khan", "Zheng Dong", "Yong Liang Guan", "Chau Yuen"], "title": "Two-Stage Coded-Sliding Beam Training and QoS-Constrained Sum-Rate Maximization for SIM-Assisted Wireless Communications", "comment": null, "summary": "Stacked intelligent metasurfaces (SIM) provide a cost-effective and scalable solution for large-scale antenna communications.However, efficient channel state information acquisition and phase shift optimization remain critical challenges. In this paper, we develop a unified framework of low-complexity algorithms for SIM-assisted communication systems to address these issues. Specifically, we propose a generalized two-step codebook construction (TSCC) method that leverages two-dimensional angular-domain decoupling to transform planar array beamformer design into two independent one-dimensional linear array beamformer design problems, efficiently solved via the Gerchberg-Saxton algorithm and our proposed majorization-minimization-based proximal distance (PDMM) algorithm. We further develop a two-stage coded-sliding beam training (TSCSBT) method for low-overhead and high-accuracy beam training, where error-correcting codes are embedded in the first-stage training to enhance robustness against noise, and sliding sampling is subsequently performed around the matched angular samples to improve angular resolution. The proposed framework is further extended to multi-path user channels. Finally, a variable decoupling-based block successive upper bound minimization (VD-BSUM) algorithm is proposed to directly solve the QoS-constrained sum-rate maximization problem through closed-form iterative updates with substantially reduced computational complexity. Simulation results demonstrate the effectiveness of the proposed methods in achieving precise beam pattern realization, improved beam training accuracy and angular resolution, and enhanced sum-rate performance."}
{"id": "2602.00730", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00730", "abs": "https://arxiv.org/abs/2602.00730", "authors": ["Zixuan Li"], "title": "Towards Trustworthy Multimodal Recommendation", "comment": "Preprint, 10 pages, 5 figures", "summary": "Recent advances in multimodal recommendation have demonstrated the effectiveness of incorporating visual and textual content into collaborative filtering. However, real-world deployments raise an increasingly important yet underexplored issue: trustworthiness. On modern e-commerce platforms, multimodal content can be misleading or unreliable (e.g., visually inconsistent product images or click-bait titles), injecting untrustworthy signals into multimodal representations and making existing recommenders brittle under modality corruption. In this work, we take a step towards trustworthy multimodal recommendation from both a method and an analysis perspective. First, we propose a plug-and-play modality-level rectification component that mitigates untrustworthy modality features by learning soft correspondences between items and multimodal features. Using lightweight projections and Sinkhorn-based soft matching, the rectification suppresses mismatched modality signals while preserving semantic consistency, and can be integrated into existing multimodal recommenders without architectural modifications. Second, we present two practical insights on interaction-level trustworthiness under noisy collaborative signals: (i) training-set pseudo interactions can help or hurt performance under noise depending on prior-signal alignment; and (ii) propagation-graph pseudo edges can also help or hurt robustness, as message passing may amplify misalignment. Extensive experiments on multiple datasets and backbones under varying corruption levels demonstrate improved robustness from modality rectification and validate the above interaction-level observations."}
{"id": "2602.02435", "categories": ["cs.IT", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02435", "abs": "https://arxiv.org/abs/2602.02435", "authors": ["Subhankar Banerjee", "Sennur Ulukus"], "title": "Preemptive Scheduling for Age of Job Minimization in Task-Specific Machine Networks", "comment": null, "summary": "We consider a time-slotted job-assignment system consisting of a central server, $N$ task-specific networks of machines, and multiple users. Each network specializes in executing a distinct type of task. Users stochastically generate jobs of various types and forward them to the central server, which routes each job to the appropriate network of machines. Due to resource constraints, the server cannot serve all users' jobs simultaneously, which motivates the design of scheduling policies with possible preemption. To evaluate scheduling performance, we introduce a novel timeliness metric, the age of job, inspired by the well-known metric, the age of information. We study the problem of minimizing the long-term weighted average age of job. We first propose a max-weight policy by minimizing the one-step Lyapunov drift and then derive the Whittle index (WI) policy when the job completion times of the networks of machines follow geometric distributions. For general job completion time distributions, we introduce a Whittle index with max-weight fallback (WIMWF) policy. We also investigate the Net-gain maximization (NGM) policy. Numerically, we show that the proposed WIMWF policy achieves the best performance in the general job completion time setting. We also observe a scaling trend: two different max-weight policies can outperform the NGM policy in small systems, whereas the NGM policy improves as we scale the system size and becomes asymptotically better than max-weight policies. For geometric service times, the WI policy yields the lowest age across all considered system sizes."}
{"id": "2602.00805", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00805", "abs": "https://arxiv.org/abs/2602.00805", "authors": ["Yunhan Li", "Mingjie Xie", "Zihan Gong", "Zeyang Shi", "Gengshen Wu", "Min Yang"], "title": "Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training", "comment": "4 pages, 3 figures, 3 tables", "summary": "Recent advances in embedding-based retrieval have enabled dense retrievers to serve as core infrastructure in many industrial systems, where a single retrieval backbone is often shared across multiple downstream applications. In such settings, retrieval quality directly constrains system performance and extensibility, while coupling model selection, deployment, and rollback decisions across applications.\n  In this paper, we present empirical findings and a system-level solution for optimizing retrieval components deployed as a shared backbone in production legal retrieval systems. We adopt a multi-stage optimization framework for dense retrievers and rerankers, and show that different retrieval components exhibit stage-dependent trade-offs. These observations motivate a component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint. The resulting backbone is validated through end-to-end evaluation and deployed as a shared retrieval service supporting multiple industrial applications."}
{"id": "2602.02469", "categories": ["cs.IT", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02469", "abs": "https://arxiv.org/abs/2602.02469", "authors": ["Ahmed M. Elshazly", "Ahmed Arafa"], "title": "Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation", "comment": "To appear in IEEE ICC 2026", "summary": "We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \\emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \\emph{AgeTop-\\(k\\)}, which first picks the largest-magnitude entries and then chooses the \\(k\\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \\(k\\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\\(k\\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \\(k\\) depends on the channel, with smaller \\(k\\) being better in noisy settings."}
{"id": "2602.01023", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01023", "abs": "https://arxiv.org/abs/2602.01023", "authors": ["Kai Yuan", "Anthony Zheng", "Jia Hu", "Divyanshu Sheth", "Hemanth Velaga", "Kylee Kim", "Matteo Guarrera", "Besim Avci", "Xuetao Yin", "Rajyashree Mukherjee", "Sean Suchter"], "title": "Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment", "comment": "11 pages, 4 figures", "summary": "Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\\% reduction in keystrokes and 3.46\\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry."}
{"id": "2602.02489", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02489", "abs": "https://arxiv.org/abs/2602.02489", "authors": ["Amir Masoud Jafarpisheh", "Ali Khalesi", "Petros Elia"], "title": "Secure Multi-User Linearly-Separable Distributed Computing", "comment": null, "summary": "The introduction of the new multi-user linearly-separable distributed computing framework, has recently revealed how a parallel treatment of users can yield large parallelization gains with relatively low computation and communication costs. These gains stem from a new approach that converts the computing problem into a sparse matrix factorization problem; a matrix $F$ that describes the users' requests, is decomposed as \\(F = DE\\), where a \\(γ\\)-sparse \\(E\\) defines the task allocation across $N$ servers, and a \\(δ\\)-sparse \\(D\\) defines the connectivity between \\(N\\) servers and \\(K\\) users as well as the decoding process. While this approach provides near-optimal performance, its linear nature has raised data secrecy concerns.\n  We here adopt an information-theoretic secrecy framework, seeking guarantees that each user can learn nothing more than its own requested function. In this context, our main result provides two necessary and sufficient secrecy criteria; (i) for each user \\(k\\) who observes $α_k$ server responses, the common randomness visible to that user must span a subspace of dimension exactly $α_k-1$,\n  and (ii) for each user, removing from \\(\\mathbf{D}\\) the columns corresponding to the servers it observes must leave a matrix of rank at least \\(K-1\\). With these conditions in place, we design a general scheme -- that applies to finite and non-finite fields alike -- which is based on appending to \\(\\mathbf{E}\\) a basis of \\(\\mathrm{Null}(\\mathbf{D})\\) and by carefully injecting shared randomness. In many cases, this entails no additional costs. The scheme, while maintaining performance, guarantees perfect information-theoretic secrecy in the case of finite fields, while in the real case, the conditions yield an explicit mutual-information bound that can be made arbitrarily small by increasing the variance of Gaussian common randomness."}
{"id": "2602.01865", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01865", "abs": "https://arxiv.org/abs/2602.01865", "authors": ["Shaopeng Chen", "Chuyue Xie", "Huimin Ren", "Shaozong Zhang", "Han Zhang", "Ruobing Cheng", "Zhiqiang Cao", "Zehao Ju", "Gao Yu", "Jie Ding", "Xiaodong Chen", "Xuewu Jiao", "Shuanglong Li", "Liu Lin"], "title": "GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm", "comment": null, "summary": "Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized."}
{"id": "2602.02024", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02024", "abs": "https://arxiv.org/abs/2602.02024", "authors": ["Clémence Réda", "Tomas Rigaux", "Hiba Bederina", "Koh Takeuchi", "Hisashi Kashima", "Jill-Jênn Vie"], "title": "Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation", "comment": null, "summary": "A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing."}
{"id": "2602.02338", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02338", "abs": "https://arxiv.org/abs/2602.02338", "authors": ["Yu Liang", "Zhongjin Zhang", "Yuxuan Zhu", "Kerui Zhang", "Zhiluohan Guo", "Wenhang Zhou", "Zonqi Yang", "Kangle Wu", "Yabo Ni", "Anxiang Zeng", "Cong Fu", "Jianxin Wang", "Jiazhi Xia"], "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs", "comment": null, "summary": "Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID."}
{"id": "2602.02444", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02444", "abs": "https://arxiv.org/abs/2602.02444", "authors": ["Tyler Skow", "Alexander Martin", "Benjamin Van Durme", "Rama Chellappa", "Reno Kriz"], "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval", "comment": null, "summary": "Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient."}
