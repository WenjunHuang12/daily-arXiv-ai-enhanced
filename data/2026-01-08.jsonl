{"id": "2601.03258", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.03258", "abs": "https://arxiv.org/abs/2601.03258", "authors": ["Sherine George"], "title": "Enhancing Retrieval-Augmented Generation with Two-Stage Retrieval: FlashRank Reranking and Query Expansion", "comment": "3 pages, 1 figure, 3 tables", "summary": "Retrieval-Augmented Generation (RAG) couples a retriever with a large language model (LLM) to ground generated responses in external evidence. While this framework enhances factuality and domain adaptability, it faces a key bottleneck: balancing retrieval recall with limited LLM context. Retrieving too few passages risks missing critical context, while retrieving too many overwhelms the prompt window, diluting relevance and increasing cost.\n  We propose a two-stage retrieval pipeline that integrates LLM-driven query expansion to improve candidate recall and FlashRank, a fast marginal-utility reranker that dynamically selects an optimal subset of evidence under a token budget. FlashRank models document utility as a weighted combination of relevance, novelty, brevity, and cross-encoder evidence. Together, these modules form a generalizable solution that increases answer accuracy, faithfulness, and computational efficiency."}
{"id": "2601.03259", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.03259", "abs": "https://arxiv.org/abs/2601.03259", "authors": ["Bo-Chian Chen", "Manel Slokom"], "title": "LLMDiRec: LLM-Enhanced Intent Diffusion for Sequential Recommendation", "comment": "Under review", "summary": "Existing sequential recommendation models, even advanced diffusion-based approaches, often struggle to capture the rich semantic intent underlying user behavior, especially for new users or long-tail items. This limitation stems from their reliance on ID-based embeddings, which lack semantic grounding. We introduce LLMDiRec, a new approach that addresses this gap by integrating Large Language Models (LLMs) into an intent-aware diffusion model. Our approach combines collaborative signals from ID embeddings with rich semantic representations from LLMs, using a dynamic fusion mechanism and a multi-task objective to align both views. We run extensive experiments on five public datasets. We run extensive experiments on five public datasets. We demonstrate that \\modelname outperforms state-of-the-art algorithms, with particularly strong improvements in capturing complex user intents and enhancing recommendation performance for long-tail items."}
{"id": "2601.03262", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03262", "abs": "https://arxiv.org/abs/2601.03262", "authors": ["Xiantao Zhang"], "title": "Roles of MLLMs in Visually Rich Document Retrieval for RAG: A Survey", "comment": "18 pages; accepted at AACL-IJCNLP 2025 (main conference)", "summary": "Visually rich documents (VRDs) challenge retrieval-augmented generation (RAG) with layout-dependent semantics, brittle OCR, and evidence spread across complex figures and structured tables. This survey examines how Multimodal Large Language Models (MLLMs) are being used to make VRD retrieval practical for RAG. We organize the literature into three roles: Modality-Unifying Captioners, Multimodal Embedders, and End-to-End Representers. We compare these roles along retrieval granularity, information fidelity, latency and index size, and compatibility with reranking and grounding. We also outline key trade-offs and offer some practical guidance on when to favor each role. Finally, we identify promising directions for future research, including adaptive retrieval units, model size reduction, and the development of evaluation methods."}
{"id": "2601.03479", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03479", "abs": "https://arxiv.org/abs/2601.03479", "authors": ["Qiang Zhang", "Hanchao Yu", "Ivan Ji", "Chen Yuan", "Yi Zhang", "Chihuang Liu", "Xiaolong Wang", "Christopher E. Lambert", "Ren Chen", "Chen Kovacs", "Xinzhu Bei", "Renqin Cai", "Rui Li", "Lizhu Zhang", "Xiangjun Fan", "Qunshu Zhang", "Benyu Zhang"], "title": "Efficient Sequential Recommendation for Long Term User Interest Via Personalization", "comment": "ICDM 2025", "summary": "Recent years have witnessed success of sequential modeling, generative recommender, and large language model for recommendation. Though the scaling law has been validated for sequential models, it showed inefficiency in computational capacity when considering real-world applications like recommendation, due to the non-linear(quadratic) increasing nature of the transformer model. To improve the efficiency of the sequential model, we introduced a novel approach to sequential recommendation that leverages personalization techniques to enhance efficiency and performance. Our method compresses long user interaction histories into learnable tokens, which are then combined with recent interactions to generate recommendations. This approach significantly reduces computational costs while maintaining high recommendation accuracy. Our method could be applied to existing transformer based recommendation models, e.g., HSTU and HLLM. Extensive experiments on multiple sequential models demonstrate its versatility and effectiveness. Source code is available at \\href{https://github.com/facebookresearch/PerSRec}{https://github.com/facebookresearch/PerSRec}."}
{"id": "2601.04184", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2601.04184", "abs": "https://arxiv.org/abs/2601.04184", "authors": ["Kumar Rahul", "Sriram Sethuraman", "Andrew Segall", "Yixu Chen"], "title": "Transforming Video Subjective Testing with Training, Engagement, and Real-Time Feedback", "comment": "Accepted at 5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model (WVAQ), at IEEE/CVF WACV 2026", "summary": "Subjective video quality assessment is crucial for optimizing streaming and compression, yet traditional protocols face limitations in capturing nuanced perceptual differences and ensuring reliable user input. We propose an integrated framework that enhances rater training, enforces attention through real-time scoring, and streamlines pairwise comparisons to recover quality scores with fewer comparisons. Participants first undergo an automated training quiz to learn key video quality indicators (e.g., compression artifacts) and verify their readiness. During the test, a real-time attention scoring mechanism, using \"golden\" video pairs, monitors and reinforces rater focus by applying penalties for lapses. An efficient chain-based pairwise comparison procedure is then employed, yielding quality scores in Just-Objectionable-Differences (JOD) units. Experiments comparing three groups (no training, training without feedback, and training with feedback) with 80 participants demonstrate that training-quiz significantly improves data quality in terms of golden unit accuracy and reduces tie rate, while real-time feedback further improves data quality and yields the most monotonic quality ratings. The new training, quiz, testing with feedback, 3-phase approach can significantly reduce the non-monotonic cases on the high quality part of the R-Q curve where normal viewer typically prefer the slightly compressed less-grainy content and help train a better objective video quality metric."}
{"id": "2601.03489", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.03489", "abs": "https://arxiv.org/abs/2601.03489", "authors": ["Sanjit Bhowmick"], "title": "LCPs of Subspace Codes", "comment": null, "summary": "A subspace code is a nonempty collection of subspaces of the vector space $\\mathbb{F}_q^{n}$. A pair of linear codes is called a linear complementary pair (in short LCP) of codes if their intersection is trivial and the sum of their dimensions equals the dimension of the ambient space. Equivalently, the two codes form an LCP if the direct sum of these two codes is equal to the entire space. In this paper, we introduce the concept of LCPs of subspace codes. We first provide a characterization of subspace codes that form an LCP. Furthermore, we present a sufficient condition for the existence of an LCP of subspace codes based on a complement function on a subspace code. In addition, we give several constructions of LCPs for subspace codes using various techniques and provide an application to insertion error correction."}
{"id": "2601.03381", "categories": ["cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.03381", "abs": "https://arxiv.org/abs/2601.03381", "authors": ["Laurent Doyen", "Shibashis Guha"], "title": "Algorithm and Strategy Construction for Sure-Almost-Sure Stochastic Parity Games", "comment": "Extended version of STACS 2026 paper", "summary": "We consider turn-based stochastic two-player games with a combination of a parity condition that must hold surely, that is in all possible outcomes, and of a parity condition that must hold almost-surely, that is with probability 1. The problem of deciding the existence of a winning strategy in such games is central in the framework of synthesis beyond worst-case where a hard requirement that must hold surely is combined with a softer requirement. Recent works showed that the problem is coNP-complete, and infinite-memory strategies are necessary in general, even in one-player games (i.e., Markov decision processes). However, memoryless strategies are sufficient for the opponent player. Despite these comprehensive results, the known algorithmic solution enumerates all memoryless strategies of the opponent, which is exponential in all cases, and does not construct a winning strategy when one exists.\n  We present a recursive algorithm, based on a characterisation of the winning region, that gives a deeper insight into the problem. In particular, we show how to construct a winning strategy to achieve the combination of sure and almost-sure parity, and we derive new complexity and memory bounds for special classes of the problem, defined by fixing the index of either of the two parity conditions."}
{"id": "2601.03618", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.03618", "abs": "https://arxiv.org/abs/2601.03618", "authors": ["Muhammad Imam Luthfi Balaka", "Raul Castro Fernandez"], "title": "The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent", "comment": "CIDR 2026 Paper", "summary": "Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory."}
{"id": "2601.03271", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03271", "abs": "https://arxiv.org/abs/2601.03271", "authors": ["Omar Garraoui"], "title": "Optimizing Exact String Matching via Statistical Anchoring", "comment": null, "summary": "In this work, we propose an enhancement to the Boyer-Moore-Horspool algorithm tailored for natural language text. The approach involves preprocessing the search pattern to identify its statistically least frequent character, referred to as the \"anchor.\" During the search, verification is first performed at this high-entropy position, allowing the algorithm to quickly discard non-matching windows. This fail-fast strategy reduces unnecessary comparisons, improving overall efficiency. Our implementation shows that incorporating basic linguistic statistics into classical pattern-matching techniques can boost performance without increasing complexity to the shift heuristics."}
{"id": "2601.03496", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03496", "abs": "https://arxiv.org/abs/2601.03496", "authors": ["Bongmin Kim"], "title": "STELLA: Self-Reflective Terminology-Aware Framework for Building an Aerospace Information Retrieval Benchmark", "comment": "25 pages, 2 figures", "summary": "Tasks in the aerospace industry heavily rely on searching and reusing large volumes of technical documents, yet there is no public information retrieval (IR) benchmark that reflects the terminology- and query-intent characteristics of this domain. To address this gap, this paper proposes the STELLA (Self-Reflective TErminoLogy-Aware Framework for BuiLding an Aerospace Information Retrieval Benchmark) framework. Using this framework, we introduce the STELLA benchmark, an aerospace-specific IR evaluation set constructed from NASA Technical Reports Server (NTRS) documents via a systematic pipeline that comprises document layout detection, passage chunking, terminology dictionary construction, synthetic query generation, and cross-lingual extension. The framework generates two types of queries: the Terminology Concordant Query (TCQ), which includes the terminology verbatim to evaluate lexical matching, and the Terminology Agnostic Query (TAQ), which utilizes the terminology's description to assess semantic matching. This enables a disentangled evaluation of the lexical and semantic matching capabilities of embedding models. In addition, we combine Chain-of-Density (CoD) and the Self-Reflection method with query generation to improve quality and implement a hybrid cross-lingual extension that reflects real user querying practices. Evaluation of seven embedding models on the STELLA benchmark shows that large decoder-based embedding models exhibit the strongest semantic understanding, while lexical matching methods such as BM25 remain highly competitive in domains where exact lexical matching technical term is crucial. The STELLA benchmark provides a reproducible foundation for reliable performance evaluation and improvement of embedding models in aerospace-domain IR tasks. The STELLA benchmark can be found in https://huggingface.co/datasets/telepix/STELLA."}
{"id": "2601.03492", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.03492", "abs": "https://arxiv.org/abs/2601.03492", "authors": ["Sanjit Bhowmick", "Kuntal Deka"], "title": "Hermitian LCD $2$-Quasi Abelian Codes over Finite Chain Rings", "comment": null, "summary": "This paper introduces a class of Hermitian LCD $2$-quasi-abelian codes over finite fields and presents a comprehensive enumeration of these codes in which relative minimum weights are small. We show that such codes are asymptotically good over finite fields. Furthermore, we extend our analysis to finite chain rings by characterizing $2$-quasi-abelian codes in this setting and proving the existence of asymptotically good Hermitian LCD $2$-quasi-abelian codes over finite chain rings as well."}
{"id": "2601.03438", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.03438", "abs": "https://arxiv.org/abs/2601.03438", "authors": ["Vladimir Davidiuk", "Yuriy Dementiev", "Artur Ignatiev", "Danil Sagunov"], "title": "EFX and PO Allocation Exists for Two Types of Goods", "comment": "Accepted at AAAI 2026", "summary": "We study the problem of fairly and efficiently allocating indivisible goods among agents with additive valuations. We focus on envy-freeness up to any good (EFX) -- an important fairness notion in fair division of indivisible goods. A central open question in this field is whether EFX allocations always exist for any number of agents. While prior work has established EFX existence for settings with at most three distinct valuations (Prakash HV et al. 2025) and for two types of goods (Gorantla, Marwaha, and Velusamy 2023), the general case remains unresolved.\n  In this paper, we extend the existent knowledge by proving that EFX allocations satisfying Pareto optimality (PO) always exist and can be computed in quasiliniear time when there are two types of goods, given that the valuations are positive. This result strengthens the existing work of (Gorantla, Marwaha, and Velusamy 2023), which only guarantees the existence of EFX allocations without ensuring Pareto optimality. Our findings demonstrate a fairly simple and efficient algorithm constructing an EFX+PO allocation."}
{"id": "2601.03573", "categories": ["cs.DS", "cs.DB", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03573", "abs": "https://arxiv.org/abs/2601.03573", "authors": ["Daniel Paul-Pena", "Vaishali Surianarayanan", "Deeparnab Chakrabarty", "C. Seshadhri"], "title": "Counting hypertriangles through hypergraph orientations", "comment": null, "summary": "Counting the number of small patterns is a central task in network analysis. While this problem is well studied for graphs, many real-world datasets are naturally modeled as hypergraphs, motivating the need for efficient hypergraph motif counting algorithms. In particular, we study the problem of counting hypertriangles - collections of three pairwise-intersecting hyperedges. These hypergraph patterns have a rich structure with multiple distinct intersection patterns unlike graph triangles.\n  Inspired by classical graph algorithms based on orientations and degeneracy, we develop a theoretical framework that generalizes these concepts to hypergraphs and yields provable algorithms for hypertriangle counting. We implement these ideas in DITCH (Degeneracy Inspired Triangle Counter for Hypergraphs) and show experimentally that it is 10-100x faster and more memory efficient than existing state-of-the-art methods."}
{"id": "2601.03573", "categories": ["cs.DS", "cs.DB", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03573", "abs": "https://arxiv.org/abs/2601.03573", "authors": ["Daniel Paul-Pena", "Vaishali Surianarayanan", "Deeparnab Chakrabarty", "C. Seshadhri"], "title": "Counting hypertriangles through hypergraph orientations", "comment": null, "summary": "Counting the number of small patterns is a central task in network analysis. While this problem is well studied for graphs, many real-world datasets are naturally modeled as hypergraphs, motivating the need for efficient hypergraph motif counting algorithms. In particular, we study the problem of counting hypertriangles - collections of three pairwise-intersecting hyperedges. These hypergraph patterns have a rich structure with multiple distinct intersection patterns unlike graph triangles.\n  Inspired by classical graph algorithms based on orientations and degeneracy, we develop a theoretical framework that generalizes these concepts to hypergraphs and yields provable algorithms for hypertriangle counting. We implement these ideas in DITCH (Degeneracy Inspired Triangle Counter for Hypergraphs) and show experimentally that it is 10-100x faster and more memory efficient than existing state-of-the-art methods."}
{"id": "2601.03608", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03608", "abs": "https://arxiv.org/abs/2601.03608", "authors": ["Ansh Tiwari", "Ayush Chauhan"], "title": "Shielded RecRL: Explanation Generation for Recommender Systems without Ranking Degradation", "comment": null, "summary": "We introduce Shielded RecRL, a reinforcement learning approach to generate personalized explanations for recommender systems without sacrificing the system's original ranking performance. Unlike prior RLHF-based recommender methods that directly optimize item rankings, our two-tower architecture keeps the recommender's ranking model intact while a language model learns to produce helpful explanations. We design a composite reward signal combining explanation length, content relevance, and coherence, and apply proximal policy optimization (PPO) with a KL-divergence constraint to fine-tune a large language model with only 0.4% of its parameters trainable via LoRA adapters. In experiments on an Amazon Books dataset (approximately 50K interactions in the fantasy and romance genres), Shielded RecRL improved the relative click-through rate (CTR) by 22.5% (1.225x over baseline) while keeping the recommender's item-ranking behavior virtually unchanged. An extensive ablation study confirms that our gradient shielding strategy and reward design effectively balance explanation quality and policy drift. Our results demonstrate that Shielded RecRL enhances user-facing aspects of recommendations through rich, personalized explanations without degrading core recommendation accuracy."}
{"id": "2601.03831", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03831", "abs": "https://arxiv.org/abs/2601.03831", "authors": ["Matteo Nerini", "Zheyu Wu", "Shanpu Shen", "Bruno Clerckx"], "title": "Low-Complexity Planar Beyond-Diagonal RIS Architecture Design Using Graph Theory", "comment": "Submitted to IEEE for publication", "summary": "Reconfigurable intelligent surfaces (RISs) enable programmable control of the wireless propagation environment and are key enablers for future networks. Beyond-diagonal RIS (BD-RIS) architectures enhance conventional RIS by interconnecting elements through tunable impedance components, offering greater flexibility with higher circuit complexity. However, excessive interconnections between BD-RIS elements require multi-layer printed circuit board (PCB) designs, increasing fabrication difficulty. In this letter, we use graph theory to characterize the BD-RIS architectures that can be realized on double-layer PCBs, denoted as planar-connected RISs. Among the possible planar-connected RISs, we identify the ones with the most degrees of freedom, expected to achieve the best performance under practical constraints."}
{"id": "2601.03853", "categories": ["cs.GT", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2601.03853", "abs": "https://arxiv.org/abs/2601.03853", "authors": ["Junyao Zhao"], "title": "From No-Regret to Strategically Robust Learning in Repeated Auctions", "comment": null, "summary": "In Bayesian single-item auctions, a monotone bidding strategy--one that prescribes a higher bid for a higher value type--can be equivalently represented as a partition of the quantile space into consecutive intervals corresponding to increasing bids. Kumar et al. (2024) prove that agile online gradient descent (OGD), when used to update a monotone bidding strategy through its quantile representation, is strategically robust in repeated first-price auctions: when all bidders employ agile OGD in this way, the auctioneer's average revenue per round is at most the revenue of Myerson's optimal auction, regardless of how she adjusts the reserve price over time.\n  In this work, we show that this strategic robustness guarantee is not unique to agile OGD or to the first-price auction: any no-regret learning algorithm, when fed gradient feedback with respect to the quantile representation, is strategically robust, even if the auction format changes every round, provided the format satisfies allocation monotonicity and voluntary participation. In particular, the multiplicative weights update (MWU) algorithm simultaneously achieves the optimal regret guarantee and the best-known strategic robustness guarantee. At a technical level, our results are established via a simple relation that bridges Myerson's auction theory and standard no-regret learning theory. This showcases the potential of translating standard regret guarantees into strategic robustness guarantees for specific games, without explicitly minimizing any form of swap regret."}
{"id": "2601.03643", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03643", "abs": "https://arxiv.org/abs/2601.03643", "authors": ["Zeev Nutov"], "title": "On $k$-connectivity oracles in $k$-connected graphs", "comment": null, "summary": "A $k$-connectivity oracle for a graph $G=(V,E)$ is a data structure that given $s,t \\in V$ determines whether there are at least $k+1$ internally disjoint $st$-paths in $G$. For undirected graphs, Pettie, Saranurak & Yin [STOC 2022, pp. 151-161] proved that any $k$-connectivity oracle requires $Ω(kn)$ bits of space. They asked whether $Ω(kn)$ bits are still necessary if $G$ is $k$-connected. We will show by a very simple proof that this is so even if $G$ is $k$-connected, answering this open question."}
{"id": "2601.03730", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.03730", "abs": "https://arxiv.org/abs/2601.03730", "authors": ["Fabian Haak", "Philipp Schaer"], "title": "Perception-Aware Bias Detection for Query Suggestions", "comment": "13 pages (pp. 130-142); 2 figures; 2 tables; Workshop paper (BIAS 2021) published in CCIS vol. 1418 (Springer)", "summary": "Bias in web search has been in the spotlight of bias detection research for quite a while. At the same time, little attention has been paid to query suggestions in this regard. Awareness of the problem of biased query suggestions has been raised. Likewise, there is a rising need for automatic bias detection approaches. This paper adds on the bias detection pipeline for bias detection in query suggestions of person-related search developed by Bonart et al. \\cite{Bonart_2019a}. The sparseness and lack of contextual metadata of query suggestions make them a difficult subject for bias detection. Furthermore, query suggestions are perceived very briefly and subliminally. To overcome these issues, perception-aware metrics are introduced. Consequently, the enhanced pipeline is able to better detect systematic topical bias in search engine query suggestions for person-related searches. The results of an analysis performed with the developed pipeline confirm this assumption. Due to the perception-aware bias detection metrics, findings produced by the pipeline can be assumed to reflect bias that users would discern."}
{"id": "2601.03982", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.03982", "abs": "https://arxiv.org/abs/2601.03982", "authors": ["Haojie Gu", "Jun Zhang"], "title": "Unique Decoding of Hyperderivative Reed-Solomon Codes", "comment": null, "summary": "Error-correcting codes are combinatorial objects designed to cope with the problem of reliable transmission of information on a noisy channel. A fundamental problem in coding theory and practice is to efficiently decode the received word with errors to obtain the transmitted codeword. In this paper, we consider the decoding problem of Hyperderivative Reed-Solomon (HRS) codes with respect to the NRT metric. Specifically, we propose a Welch-Berlekamp algorithm for the unique decoding of NRT HRS codes."}
{"id": "2601.03934", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03934", "abs": "https://arxiv.org/abs/2601.03934", "authors": ["Matthias Bentert", "Esra Ceylan-Kettler", "Valentin Hübner", "Stefan Schmid", "Jiří Srba"], "title": "Complexity of Perfect and Ideal Resilience Verification in Fast Re-Route Networks", "comment": null, "summary": "To achieve fast recovery from link failures, most modern communication networks feature fully decentralized fast re-routing mechanisms. These re-routing mechanisms rely on pre-installed static re-routing rules at the nodes (the routers), which depend only on local failure information, namely on the failed links incident to the node. Ideally, a network is perfectly resilient: the re-routing rules ensure that packets are always successfully routed to their destinations as long as the source and the destination are still physically connected in the underlying network after the failures. Unfortunately, there are examples where achieving perfect resilience is not possible. Surprisingly, only very little is known about the algorithmic aspect of when and how perfect resilience can be achieved.\n  We investigate the computational complexity of analyzing such local fast re-routing mechanisms. Our main result is a negative one: we show that even checking whether a given set of static re-routing rules ensures perfect resilience is coNP-complete. We also show coNP-completeness of the so-called ideal resilience, a weaker notion of resilience often considered in the literature. Additionally, we investigate other fundamental variations of the problem. In particular, we show that our coNP-completeness proof also applies to scenarios where the re-routing rules have specific patterns (known as skipping in the literature).\n  On the positive side, for scenarios where nodes do not have information about the link from which a packet arrived (the so-called in-port), we present linear-time algorithms for both the verification and synthesis problem for perfect resilience."}
{"id": "2601.03748", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03748", "abs": "https://arxiv.org/abs/2601.03748", "authors": ["Dario Maio", "Stefano Rizzi"], "title": "Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale."}
{"id": "2601.04011", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04011", "abs": "https://arxiv.org/abs/2601.04011", "authors": ["Wei Shi", "Wei Xu", "Yongming Huang", "Jiacheng Yao", "Wenhao Hu", "Dongming Wang"], "title": "Flexible-Duplex Cell-Free Architecture for Secure Uplink Communications in Low-Altitude Wireless Networks", "comment": "Submitted to an IEEE Journal", "summary": "Low-altitude wireless networks (LAWNs) are expected to play a central role in future 6G infrastructures, yet uplink transmissions of uncrewed aerial vehicles (UAVs) remain vulnerable to eavesdropping due to their limited transmit power, constrained antenna resources, and highly exposed air-ground propagation conditions. To address this fundamental bottleneck, we propose a flexible-duplex cell-free (CF) architecture in which each distributed access point (AP) can dynamically operate either as a receive AP for UAV uplink collection or as a transmit AP that generates cooperative artificial noise (AN) for secrecy enhancement. Such AP-level duplex flexibility introduces an additional spatial degree of freedom that enables distributed and adaptive protection against wiretapping in LAWNs. Building upon this architecture, we formulate a max-min secrecy-rate problem that jointly optimizes AP mode selection, receive combining, and AN covariance design. This tightly coupled and nonconvex optimization is tackled by first deriving the optimal receive combiners in closed form, followed by developing a penalty dual decomposition (PDD) algorithm with guaranteed convergence to a stationary solution. To further reduce computational burden, we propose a low-complexity sequential scheme that determines AP modes via a heuristic metric and then updates the AN covariance matrices through closed-form iterations embedded in the PDD framework. Simulation results show that the proposed flexible-duplex architecture yields substantial secrecy-rate gains over CF systems with fixed AP roles. The joint optimization method attains the highest secrecy performance, while the low-complexity approach achieves over 90% of the optimal performance with an order-of-magnitude lower computational complexity, offering a practical solution for secure uplink communications in LAWNs."}
{"id": "2601.04169", "categories": ["cs.DS", "cs.CC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2601.04169", "abs": "https://arxiv.org/abs/2601.04169", "authors": ["Thekla Hamm", "Sukanya Pandey", "Krisztina Szilágyi"], "title": "A Polynomial Kernel for Face Cover on Non-Embedded Planar Graphs", "comment": "Accepted to STACS 2026", "summary": "Given a planar graph, a subset of its vertices called terminals, and $k \\in \\mathbb{N}$, the Face Cover Number problem asks whether the terminals lie on the boundaries of at most $k$ faces of some embedding of the input graph. When a plane graph is given in the input, the problem is known to have a polynomial kernel~\\cite{GarneroST17}. In this paper, we present the first polynomial kernel for Face Cover Number when the input is a planar graph (without a fixed embedding). Our approach overcomes the challenge of not having a predefined set of face boundaries by building a kernel bottom-up on an SPR-tree while preserving the essential properties of the face cover along the way."}
{"id": "2601.03903", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.03903", "abs": "https://arxiv.org/abs/2601.03903", "authors": ["Yuhan Yang", "Jie Zou", "Guojia An", "Jiwei Wei", "Yang Yang", "Heng Tao Shen"], "title": "Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation", "comment": "This paper has been accepted by KDD 2026", "summary": "Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines."}
{"id": "2601.04041", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.04041", "abs": "https://arxiv.org/abs/2601.04041", "authors": ["Avital Boruchovsky", "Anina Gruica", "Jonathan Niemann", "Eitan Yaakobi"], "title": "Serving Every Symbol: All-Symbol PIR and Batch Codes", "comment": null, "summary": "A $t$-all-symbol PIR code and a $t$-all-symbol batch code of dimension $k$ consist of $n$ servers storing linear combinations of $k$ linearly independent information symbols with the following recovery property: any symbol stored by a server can be recovered from $t$ pairwise disjoint subsets of servers. In the batch setting, we further require that any multiset of size $t$ of stored symbols can be recovered from $t$ disjoint subsets of servers. This framework unifies and extends several well-known code families, including one-step majority-logic decodable codes, (functional) PIR codes, and (functional) batch codes.\n  In this paper, we determine the minimum code length for some small values of $k$ and $t$, characterize structural properties of codes attaining this optimum, and derive bounds that show the trade-offs between length, dimension, minimum distance, and $t$. In addition, we study MDS codes and the simplex code, demonstrating how these classical families fit within our framework, and establish new cases of an open conjecture from \\cite{YAAKOBI2020} concerning the minimal $t$ for which the simplex code is a $t$-functional batch code."}
{"id": "2601.04166", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04166", "abs": "https://arxiv.org/abs/2601.04166", "authors": ["Christian Forsch", "Laura Cottatellucci"], "title": "Expectation Propagation for Distributed Inference in Grant-Free Cell-Free Massive MIMO", "comment": "13 pages, 5 figures, submitted for possible journal publication", "summary": "Grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) systems are anticipated to be a key enabling technology for next-generation Internet-of-Things (IoT) networks, as they support massive connectivity without explicit scheduling. However, the large amount of connected devices prevents the use of orthogonal pilot sequences, resulting in severe pilot contamination (PC) that degrades channel estimation and data detection performance. Furthermore, scalable GF-CF-MaMIMO networks inherently rely on distributed signal processing. In this work, we consider the uplink of a GF-CF-MaMIMO system and propose two novel distributed algorithms for joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP). The first algorithm, denoted as JACD-EP, uses Gaussian approximations for the channel variables, whereas the second, referred to as JACD-EP-BG, models them as Bernoulli-Gaussian (BG) random variables. To integrate the BG distribution into the EP framework, we derive its exponential family representation and develop the two algorithms as efficient message passing over a factor graph constructed from the a posteriori probability (APP) distribution. The proposed framework is inherently scalable with respect to both the number of access points (APs) and user equipments (UEs). Simulation results show the efficient mitigation of PC by the proposed distributed algorithms and their superior detection accuracy compared to (genie-aided) centralized linear detectors."}
{"id": "2601.04193", "categories": ["cs.IT", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04193", "abs": "https://arxiv.org/abs/2601.04193", "authors": ["Kieran Morris", "Oliver Johnson"], "title": "A discrete Benamou-Brenier formulation of Optimal Transport on graphs", "comment": null, "summary": "We propose a discrete transport equation on graphs which connects distributions on both vertices and edges. We then derive a discrete analogue of the Benamou-Brenier formulation for Wasserstein-$1$ distance on a graph and as a result classify all $W_1$ geodesics on graphs."}
