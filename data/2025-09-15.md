<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 6]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 16]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Several new classes of optimal p-ary cyclic codes](https://arxiv.org/abs/2509.09951)
*Mengen Fang,Lanqiang Li,Fuyin Tian,Li Liu*

Main category: cs.IT

TL;DR: 本文构建了三类新的最优p元循环码，参数为[p^m-1, p^m-2m-2, 4]，其中p为≥5的奇质数。当p=5时，还额外提出了四类新的最优循环码。


<details>
  <summary>Details</summary>
Motivation: 循环码因其良好的代数结构在通信系统、数据存储系统和消费电子中具有广泛应用。本文旨在构造新的最优p元循环码来丰富现有码类。

Method: 通过定义特定参数s = (p^m+1)/2 和满足2≤t≤p^m-2的t，构造了三类新的循环码C_p(0,s,t)。对于p=5的特殊情况，额外构建了四类最优循环码。

Result: 成功构造了三类参数为[p^m-1, p^m-2m-2, 4]的最优p元循环码，其中一类包含已知最优五元循环码作为特例。当p=5时，还获得了四类新的最优循环码。

Conclusion: 本文提出的构造方法能够生成新的最优循环码类，丰富了最优循环码的理论体系，其中一些码类包含了已知最优码作为特殊情况，证明了构造方法的有效性。

Abstract: Cyclic codes, as a crucial subclass of linear codes, exhibit broad
applications in communication systems, data storage systems, and consumer
electronics, primarily attributed to their well-structured algebraic
properties. Let $p$ denote an odd prime with $p\geq5$, and let $m$ be a
positive integer. The primary objective of this paper is to construct three
novel classes of optimal $p$-ary cyclic codes, denoted as
${\mathcal{C}_p}(0,s,t)$, which possess the parameters $[{p^m} - 1,{p^m} - 2m -
2,4]$. Here, $s$ is defined as $s = \frac{{{p^m}+1}}{{2}}$, and $t$ satisfies
the condition $2 \le t \le {p^m} - 2$. Notably, one of the constructed classes
includes certain known optimal quinary cyclic codes as special cases.
Furthermore, for the specific case when $p=5$, this paper additionally presents
four new classes of optimal cyclic codes ${\mathcal{C}_5}(0,s,t)$.

</details>


### [2] [Semantic Rate-Distortion Theory with Applications](https://arxiv.org/abs/2509.10061)
*Yi-Qun Zhao,Zhi-Ming Ma,Geoffrey Ye Li,Shuai Yuan,Tong Ye,Chuan Zhou*

Main category: cs.IT

TL;DR: 本文提出了一个基于条件语义概率失真约束的语义压缩率失真框架，通过结合信息论和AI方法，有效解决了语义通信中的歧义和多义性问题，提高了语义传输准确性和比特率效率。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信研究主要关注解码端对内在含义的估计，但忽略了语义固有的歧义和多义性问题。为了建立更实用的AI辅助通信系统语义框架，需要开发能够有效捕捉实际语义交换本质特征的新方法。

Method: 基于现实的语义通信模型，开发了语义压缩的率失真框架。利用率失真感知理论的方法，建立了在语义约束和传统符号约束下的最小可达到率定理，并获得了特定语义场景的闭式极限。通过条件语义概率失真约束来有效捕捉AI辅助通信系统中实际语义交换的本质特征。

Result: 实验表明，限制条件语义概率失真能够有效提高语义传输准确性和比特率效率。建立了信息论与AI之间的桥梁，为带宽高效的语义感知网络、增强的收发器理解和AI驱动系统的优化语义传输提供了潜在应用。

Conclusion: 该框架成功解决了语义通信中的关键挑战，通过结合信息论和AI技术，为构建更高效的语义通信系统提供了理论基础和实践指导，具有重要的理论价值和实际应用前景。

Abstract: Artificial intelligence (AI) is ushering in a new era for communication. As a
result, the establishment of a semantic communication framework is putting on
the agenda. Based on a realistic semantic communication model, this paper
develops a rate-distortion framework for semantic compression. Different from
the existing works primarily focusing on decoder-side estimation of intrinsic
meaning and ignoring its inherent issues, such as ambiguity and polysemy, we
exploit a constraint of conditional semantic probability distortion to
effectively capture the essential features of practical semantic exchanges in
an AI-assisted communication system. With the help of the methods in
rate-distortion-perception theory, we establish a theorem specifying the
minimum achievable rate under this semantic constraint and a traditional
symbolic constraint and obtain its closed-form limit for a particular semantic
scenario. From the experiments in this paper, bounding conditional semantic
probability distortion can effectively improve both semantic transmission
accuracy and bit-rate efficiency. Our framework bridges information theory and
AI, enabling potential applications in bandwidth-efficient semantic-aware
networks, enhanced transceiver understanding, and optimized semantic
transmission for AI-driven systems.

</details>


### [3] [Analog Over-the-Air Federated Learning with Interference-Based Energy Harvesting](https://arxiv.org/abs/2509.10123)
*Ahmad Massud Tota Khel,Aissa Ikhlef,Zhiguo Ding,Hongjian Sun*

Main category: cs.IT

TL;DR: 提出了一种无需CSI的降噪策略和基于能量自适应的调度算法，用于能量收集的空中模拟联邦学习，有效缓解CCI干扰并提升学习性能


<details>
  <summary>Details</summary>
Motivation: 解决设备从射频信号中收集能量进行联邦学习时面临的同信道干扰问题，同时避免对信道状态信息的依赖，提高设备参与度和能效

Method: 设计无需CSI的有效降噪策略来减轻聚合误差；提出自适应调度算法，根据可用能量动态调整本地训练轮数

Result: 仿真和收敛分析表明算法相比传统方法具有鲁棒性能，降噪效果与CSI方法相当，高功率CCI会严重降低性能但可通过增加活跃设备数缓解

Conclusion: 所提方法能有效处理能量收集联邦学习中的干扰问题，自适应算法通过动态调整训练策略显著提升设备参与度和整体学习性能

Abstract: We consider analog over-the-air federated learning, where devices harvest
energy from in-band and out-band radio frequency signals, with the former also
causing co-channel interference (CCI). To mitigate the aggregation error, we
propose an effective denoising policy that does not require channel state
information (CSI). We also propose an adaptive scheduling algorithm that
dynamically adjusts the number of local training epochs based on available
energy, enhancing device participation and learning performance while reducing
energy consumption. Simulation results and convergence analysis confirm the
robust performance of the algorithm compared to conventional methods. It is
shown that the performance of the proposed denoising method is comparable to
that of conventional CSI-based methods. It is observed that high-power CCI
severely degrades the learning performance, which can be mitigated by
increasing the number of active devices, achievable via the adaptive algorithm.

</details>


### [4] [Cooperative Base Station Assignment and Resource Allocation for 6G ISAC Network](https://arxiv.org/abs/2509.10240)
*Jiajia Liao,Luping Xiang,Shida Zhong,Lixia Xiao,Haochen Liu,Kun Yang*

Main category: cs.IT

TL;DR: 本文提出了一种用于6G网络中集成感知与通信的多基站协作分配和资源分配策略，通过交替优化算法解决非凸优化问题，实现了通信速率117%和感知精度40%的性能提升


<details>
  <summary>Details</summary>
Motivation: 在6G网络中，集成感知与通信(ISAC)需要同时优化数据传输和多目标感知性能，多基站架构下的资源分配问题具有挑战性

Method: 提出协作基站分配和资源分配(CBARA)策略，推导后验克拉美罗下界和可达速率作为优化准则，开发启发式交替优化算法解决多耦合变量导致的非凸优化问题

Result: 数值结果显示，相比经典方案，所提方案实现了通信速率117%的提升和感知精度40%的提升

Conclusion: 所提出的CBARA策略能有效提升6G网络中集成感知与通信系统的整体性能，为多基站协作提供了有效的解决方案

Abstract: In the upcoming 6G networks, integrated sensing and communications (ISAC)
will be able to provide a performance boost in both perception and wireless
connectivity. This paper considers a multiple base station (BS) architecture to
support the comprehensive services of data transmission and multi-target
sensing. In this context, a cooperative BS assignment and resource allocation
(CBARA) strategy is proposed in this paper, aiming at jointly optimizing the
communication and sensing (C&S) performance. The posterior Cramer-Rao lower
bound and the achievable rate with respect to transmit power and bandwidth are
derived and utilized as optimization criteria for the CBARA scheme. We develop
a heuristic alternating optimization algorithm to obtain an effective
sub-optimal solution for the non-convex optimization problem caused by multiple
coupled variables. Numerical results show the effectiveness of the proposed
solution, which achieves a performance improvement of 117% in communication
rate and 40% in sensing accuracy, compared to the classic scheme.

</details>


### [5] [Large-scale Aerial Reconfigurable Intelligent Surface-aided Robust Anti-jamming Transmission](https://arxiv.org/abs/2509.10280)
*Junshan Luo,Shilian Wang,Boxiang He*

Main category: cs.IT

TL;DR: 本文提出了一种基于均值场建模的ARIS空间配置方法，通过连续密度函数设计无人机载可重构智能表面部署，解决了大规模ARIS抗干扰通信中的计算复杂度和干扰威胁问题。


<details>
  <summary>Details</summary>
Motivation: 无人机载可重构智能表面(ARIS)能够动态配置信道条件并建立可靠的空地链路，但大规模部署面临传统离散优化方法计算复杂度高和复杂干扰威胁的挑战。

Method: 采用均值场建模方法，通过连续密度函数设计ARIS空间配置；提出鲁棒抗干扰传输框架，联合优化基站波束成形、ARIS反射和空间分布；利用变分优化和黎曼流形方法求解函数优化问题。

Result: 仿真结果表明所提框架显著提高了和速率，算法计算复杂度与无人机数量无关，验证了其在可扩展ARIS辅助抗干扰通信中的有效性。

Conclusion: 最优ARIS部署遵循空间注水原理，将资源集中在高增益区域同时避开干扰易发区域；干扰机的最优策略受路径损耗减少和空间聚焦增强之间的邻近-方向性权衡支配。

Abstract: Aerial reconfigurable intelligent surfaces (ARIS), deployed on unmanned
aerial vehicles (UAVs), could enhance anti-jamming communication performance by
dynamically configuring channel conditions and establishing reliable air-ground
links. However, large-scale ARIS faces critical deployment challenges due to
the prohibitive computational complexity of conventional discrete optimization
methods and sophisticated jamming threats. In this paper, we introduce a mean
field modeling approach to design the spatial configuration of ARIS by a
continuous density function, thus bypassing high-dimensional combinatorial
optimization. We consider an adaptive jammer which adjusts its position and
beamforming to minimize the sum-rate. A key finding reveals that the jammer's
optimal strategy is governed by a proximity-directivity trade-off between
reducing path loss and enhancing spatial focusing. To combat the jamming, we
propose a robust anti-jamming transmission framework that jointly optimizes the
BS beamforming, the ARIS reflection, and the ARIS spatial distribution to
maximize the worst-case sum-rate. By leveraging variational optimization and
Riemannian manifold methods, we efficiently solve the functional optimization
problems. Our analysis further unveils that the optimal ARIS deployment follows
a spatial water-filling principle, concentrating resources in high-gain regions
while avoiding interference-prone areas. Simulation results demonstrate that
the proposed framework remarkably improves the sum-rate. Furthermore, the
computational complexity of the proposed algorithm is independent of the number
of UAVs, validating its effectiveness for scalable ARIS-assisted anti-jamming
communications.

</details>


### [6] [Energy Efficiency for Massive MIMO Integrated Sensing and Communication Systems](https://arxiv.org/abs/2509.10290)
*Huy T. Nguyen,Van-Dinh Nguyen,Nhan Thanh Nguyen,Nguyen Cong Luong,Vo-Nguyen Quoc Bao,Hien Quoc Ngo,Dusit Niyato,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: 该论文研究大规模MIMO ISAC系统的能量效率优化，通过功率分配算法平衡通信和感知性能，提出新颖的初始化策略和迭代优化方法。


<details>
  <summary>Details</summary>
Motivation: 随着集成感知与通信(ISAC)系统的发展，需要在大规模MIMO环境下同时优化通信和感知的能量效率，但现有方法在平衡两者性能和确保算法收敛性方面存在挑战。

Method: 推导通信速率和CRB的闭式表达式，建立能量效率最大化问题，利用Dinkelbach和SCA技术设计迭代算法，并提出高质量初始点策略。

Result: 仿真显示所提方法相比基线有显著性能提升，在高频谱效率下感知EE对系统整体EE影响更加明显，在特定条件下观察到16.7%的EE下降。

Conclusion: 该研究成功解决了MIMO-ISAC系统的能量效率优化问题，证明了感知效率在整体系统性能中的重要性，为未来ISAC系统设计提供了有效解决方案。

Abstract: This paper explores the energy efficiency (EE) of integrated sensing and
communication (ISAC) systems employing massive multiple-input multiple-output
(mMIMO) techniques to leverage spatial beamforming gains for both communication
and sensing. We focus on an mMIMO-ISAC system operating in an orthogonal
frequency-division multiplexing setting with a uniform planar array,
zero-forcing downlink transmission, and mono-static radar sensing to exploit
multi-carrier channel diversity. By deriving closed-form expressions for the
achievable communication rate and Cram\'er-Rao bounds (CRBs), we are able to
determine the overall EE in closed-form. A power allocation problem is then
formulated to maximize the system's EE by balancing communication and sensing
efficiency while satisfying communication rate requirements and CRB
constraints. Through a detailed analysis of CRB properties, we reformulate the
problem into a more manageable form and leverage Dinkelbach's and successive
convex approximation (SCA) techniques to develop an efficient iterative
algorithm. A novel initialization strategy is also proposed to ensure
high-quality feasible starting points for the iterative optimization process.
Extensive simulations demonstrate the significant performance improvement of
the proposed approach over baseline approaches. Results further reveal that as
communication spectral efficiency rises, the influence of sensing EE on the
overall system EE becomes more pronounced, even in sensing-dominated scenarios.
Specifically, in the high $\omega$ regime of $2 \times 10^{-3}$, we observe a
16.7\% reduction in overall EE when spectral efficiency increases from $4$ to
$8$ bps/Hz, despite the system being sensing-dominated.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [7] [Space-Time Tradeoffs for Spatial Conjunctive Queries](https://arxiv.org/abs/2509.10050)
*Aryan Esmailpour,Xiao Hu,Stavros Sintos*

Main category: cs.DB

TL;DR: 本文研究如何为连接查询结果构建高效的空间查询索引，建立了查询时间与空间使用的下界，并构建了针对k-star和k-path查询的最优索引


<details>
  <summary>Details</summary>
Motivation: 现有方法要么查询时间O(N)要么空间使用过大，无法在实际中有效应用。需要构建既能高效回答空间连接查询又节省空间的索引

Method: 建立查询时间与空间使用的下界理论，为k-star和k-path查询构建最优索引，扩展到层次查询和任意连接查询，使用广义超树分解技术

Result: 证明了k-star和k-path查询的空间下界，构建了支持范围空性、范围计数和最近邻查询的最优索引

Conclusion: 提出的索引方法在理论和实践中都表现出色，能够显著改善关系算法中空间查询的运行时间

Abstract: Given a conjunctive query and a database instance, we aim to develop an index
that can efficiently answer spatial queries on the results of a conjunctive
query. We are interested in some commonly used spatial queries, such as range
emptiness, range count, and nearest neighbor queries. These queries have
essential applications in data analytics, such as filtering relational data
based on attribute ranges and temporal graph analysis for counting graph
structures like stars, paths, and cliques. Furthermore, this line of research
can accelerate relational algorithms that incorporate spatial queries in their
workflow, such as relational clustering. Known approaches either have to spend
$\tilde{O}(N)$ query time or use space as large as the number of query results,
which are inefficient or unrealistic to employ in practice. Hence, we aim to
construct an index that answers spatial conjunctive queries in both time- and
space-efficient ways.
  In this paper, we establish lower bounds on the tradeoff between answering
time and space usage. For $k$-star (resp. $k$-path) queries, we show that any
index for range emptiness, range counting or nearest neighbor queries with $T$
answering time requires $\Omega\left(N+\frac{N^k}{T^k}\right)$ (resp.
$\Omega\left(N+\frac{N^2}{T^{2/(k-1)}}\right)$) space. Then, we construct
optimal indexes for answering range emptiness and range counting problems over
$k$-star and $k$-path queries. Extending this result, we build an index for
hierarchical queries. By resorting to the generalized hypertree decomposition,
we can extend our index to arbitrary conjunctive queries for supporting spatial
conjunctive queries. Finally, we show how our new indexes can be used to
improve the running time of known algorithms in the relational setting.

</details>


### [8] [Semi-interval Comparison Constraints in Query Containment and Their Impact on Certain Answer Computation](https://arxiv.org/abs/2509.10138)
*Foto N. Afrati,Matthew Damigos*

Main category: cs.DB

TL;DR: 本文研究带有算术比较的连接查询(CQAC)的包含性测试和确定答案计算的计算复杂度，发现某些半区间算术比较的查询类可以将问题复杂度降低到NP，但某些简单情况仍保持Π₂^p完全性。


<details>
  <summary>Details</summary>
Motivation: 研究CQAC查询包含性问题的计算复杂度，特别关注包含查询具有半区间算术比较时的特殊情况，以及如何在视图环境下高效计算确定答案。

Method: 通过理论分析和复杂度证明，研究不同类别的CQAC查询包含性测试问题的计算复杂度边界，并探讨使用最大包含重写来计算确定答案的方法。

Result: 发现包含查询具有半区间算术比较时，包含性测试问题可在NP内解决；但某些简单情况仍为Π₂^p完全。证明最大包含重写能精确计算所有确定答案，并在某些情况下可在多项式时间内计算。

Conclusion: CQAC查询包含性问题的复杂度取决于算术比较的类型，半区间比较可降低复杂度；最大包含重写是计算确定答案的有效方法，在某些情况下可实现高效计算。

Abstract: We consider conjunctive queries with arithmetic comparisons (CQAC) and
investigate the computational complexity of the problem: Given two CQAC
queries, $Q$ and $Q'$, is $Q'$ contained in $Q$? We know that, for CQAC
queries, the problem of testing containment is $\Pi_2 ^p$ -complete. However,
there are broad classes of queries with semi-interval arithmetic comparisons in
the containing query that render the problem solvable in NP. In all cases
examined the contained query is allowed to be any CQAC. Interestingly, we also
prove that there are simple cases where the problem remains $\Pi_2 ^p$
-complete.
  We also investigate the complexity of computing certain answers in the
framework of answering CQAC queries with semi-interval comparisons using any
CQAC views. We prove that maximally contained rewritings in the language of
union of CQACs always compute exactly all certain answers. We find cases where
we can compute certain answers in polynomial time using maximally contained
rewritings.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates](https://arxiv.org/abs/2509.10036)
*Zhuowei Zhao,Zhuo Zhang,Hanzhi Wang,Junhao Gan,Zhifeng Bao,Jianzhong Qi*

Main category: cs.DS

TL;DR: 本文提出了AGP-Static++和AGP-Dynamic算法，改进了近似图传播框架在动态图和动态参数化查询场景下的性能，显著提升了更新时间和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有AGP-Static算法在动态图和动态参数化查询场景下存在性能瓶颈：查询复杂度依赖不存在的理想子集采样算法，且动态图更新处理时间高达O(n log n)。

Method: 提出AGP-Static++算法简化查询过程，降低O(log² n)的查询复杂度；进一步提出AGP-Dynamic算法实现O(1)摊销时间的动态图更新处理。

Result: 实验验证理论改进：相比基线方法，更新时间提升177倍，查询效率提升10倍，同时保持近似保证。

Conclusion: 新算法有效解决了动态图传播中的性能挑战，为PageRank、GNN特征传播和图基RAG等应用提供了高效解决方案。

Abstract: We revisit Approximate Graph Propagation (AGP), a unified framework which
captures various graph propagation tasks, such as PageRank, feature propagation
in Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation
(RAG). Our work focuses on the settings of dynamic graphs and dynamic
parameterized queries, where the underlying graphs evolve over time (updated by
edge insertions or deletions) and the input query parameters are specified on
the fly to fit application needs. Our first contribution is an interesting
observation that the SOTA solution, AGP-Static, can be adapted to support
dynamic parameterized queries; however several challenges remain unresolved.
Firstly, the query time complexity of AGP-Static is based on an assumption of
using an optimal algorithm for subset sampling in its query algorithm.
Unfortunately, back to that time, such an algorithm did not exist; without such
an optimal algorithm, an extra $O(\log^2 n)$ factor is required in the query
complexity, where $n$ is the number of vertices in the graphs. Secondly,
AGP-Static performs poorly on dynamic graphs, taking $O(n\log n)$ time to
process each update. To address these challenges, we propose a new algorithm,
AGP-Static++, which is simpler yet reduces roughly a factor of $O(\log^2 n)$ in
the query complexity while preserving the approximation guarantees of
AGP-Static. However, AGP-Static++ still requires $O(n)$ time to process each
update. To better support dynamic graphs, we further propose AGP-Dynamic, which
achieves $O(1)$ amortized time per update, significantly improving the
aforementioned $O(n)$ per-update bound, while still preserving the query
complexity and approximation guarantees. Last, our comprehensive experiments
validate the theoretical improvements: compared to the baselines, our algorithm
achieves speedups of up to $177\times$ on update time and $10\times$ on query
efficiency.

</details>


### [10] [Constant Time with Minimal Preprocessing, a Robust and Extensive Complexity Class](https://arxiv.org/abs/2509.10188)
*Étienne Grandjean,Louis Jachiet*

Main category: cs.DS

TL;DR: 本文研究cstPP类运算，这类运算在O(N)预处理后可对小于N^d的输入实现常数时间计算，证明了该类的鲁棒性、闭包性质和对预处理时间变化的稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究在RAM机器上能够通过线性预处理实现常数时间计算的运算类别，探索这类运算的数学性质和计算复杂性特征。

Method: 通过理论分析和数学证明，研究cstPP类的定义、闭包性质、对预处理时间变化的敏感性，以及在不同原始操作集下的不变性。

Result: 证明了cstPP类具有鲁棒性、闭包性质，对预处理时间从N^ε到O(N^c)的变化保持稳定，但在N^{o(1)}预处理时会退化。

Conclusion: cstPP类是一个具有良好数学性质和计算特性的运算类别，其定义对预处理时间具有一定程度的灵活性，但过少的预处理会导致类别退化。

Abstract: In this paper, we study the class $\mathtt{cstPP}$ of operations
$\mathtt{op}: \mathbb{N}^k\to\mathbb{N}$, of any fixed arity $k\ge 1$,
satisfying the following property: for each fixed integer $d\ge 1$, there
exists an algorithm for a RAM machine which, for any input integer $N\ge 2$, -
pre-computes some tables in $O(N)$ time, - then reads $k$ operands
$x_1,\ldots,x_k<N^d$ and computes $\mathtt{op}(x_1,\dots,x_k)$ in constant
time.
  We show that the $\mathtt{cstPP}$ class is robust and extensive and satisfies
several closure properties. It is invariant depending on whether the set of
primitive operations of the RAM is $\{+\}$, or
$\{+,-,\times,\mathtt{div},\mathtt{mod}\}$, or any set of operations in
$\mathtt{cstPP}$ provided it includes $+$. We prove that the $\mathtt{cstPP}$
class is closed under composition and, for fast-growing functions, is closed
under inverse. We also show that in the definition of $\mathtt{cstPP}$ the
constant-time procedure can be reduced to a single return instruction. Finally,
we establish that linear preprocessing time is not essential in the definition
of the $\mathtt{cstPP}$ class: this class is not modified if the preprocessing
time is increased to $O(N^c)$, for any fixed $c>1$, or conversely, is reduced
to $N^{\varepsilon}$, for any positive $\varepsilon<1$ (provided the set of
primitive operation includes $+$, $\mathtt{div}$ and $\mathtt{mod}$). To
complete the picture, we demonstrate that the $\mathtt{cstPP}$ class
degenerates if the preprocessing time reduces to $N^{o(1)}$.

</details>


### [11] [A linear-time algorithm for Chow decompositions](https://arxiv.org/abs/2509.10450)
*Alexander Taveira Blomenhofer,Benjamin Lovitz*

Main category: cs.DS

TL;DR: 提出线性时间算法计算低秩Chow分解，能够分解n变量中秩为n/3的对称3-张量，基于铅笔方法依赖广义特征值计算，并为高阶Chow分解开发亚二次时间算法


<details>
  <summary>Details</summary>
Motivation: 需要高效算法来分解对称3-张量，特别是处理Chow秩为n/3的简洁对称张量，以及扩展到高阶分解和非泛型轨道上的分解问题

Method: 基于铅笔的线性时间算法，利用广义特征值计算，同时开发亚二次时间算法处理高阶Chow分解和W-张量分解

Result: 成功实现了线性时间分解算法，能够处理特定秩的对称3-张量，并为更广泛的分解问题提供了亚二次时间解决方案

Conclusion: 该研究提供了高效的Chow分解算法框架，在张量分解计算效率方面取得重要进展，为对称张量分析提供了新的计算工具

Abstract: We propose a linear-time algorithm to compute low-rank Chow decompositions.
Our algorithm can decompose concise symmetric 3-tensors in n variables of Chow
rank n/3. The algorithm is pencil based, hence it relies on generalized
eigenvalue computations. We also develop sub-quadratic time algorithms for
higher order Chow decompositions, and Chow decompositions of 3-tensors into
products of linear forms which do not lie on the generic orbit. In particular,
we obtain a sub-quadratic-time algorithm for decomposing a symmetric 3-tensor
into a linear combination of W-tensors.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: 大型语言模型平台AutoIND可将IND申请的非临床书面总结初稿撰写时间减少约97%，从约100小时缩短至3-4小时，质量评分达69-78%，无关键监管错误，但仍需专家进行完善。


<details>
  <summary>Details</summary>
Motivation: IND申请准备过程耗时且依赖专业知识，阻碍早期临床开发进程，需要寻找提高效率的方法。

Method: 使用AutoIND LLM平台生成IND非临床书面总结(eCTD模块2.6.2、2.6.4、2.6.6)，记录起草时间并与人工起草时间对比，由盲评监管写作评估员使用7个预设类别进行质量评估。

Result: 起草时间减少97%(从~100小时降至3.7小时和2.6小时)，质量评分分别为69.6%和77.9%，未发现关键监管错误，但在重点突出、简洁性和清晰度方面存在不足。

Conclusion: AutoIND能显著加速IND起草过程，但专家监管写作者仍需将输出完善至提交就绪质量，发现的系统性缺陷为针对性模型改进提供了路线图。

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [13] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: boldsea是一个基于语义事件方法的架构，使用可执行本体来建模复杂动态系统，通过将事件语义与数据流架构集成来解决传统BPM系统和面向对象语义技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理(BPM)系统和面向对象语义技术在动态系统建模中的局限性，提供更灵活和实时的过程执行控制。

Method: 提出boldsea架构和BSL语义语言，包括BNF语法，通过boldsea-engine直接解释语义模型作为可执行算法，无需编译。

Result: 实现了运行时修改事件模型、确保时间透明度、在统一语义框架内无缝合并数据和业务逻辑的能力。

Conclusion: boldsea方法为复杂动态系统的建模和执行提供了一种创新的语义驱动架构，克服了传统方法的限制。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [14] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: 本文评估了大型语言模型(LLMs)和视觉语言模型(VLMs)在符号、语言和连续控制环境中提供规划反馈的能力，发现基础模型能提供高质量反馈，但复杂动态环境会降低反馈质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于环境的规划学习需要精心设计的奖励函数或高质量标注演示，而预训练基础模型可能提供背景知识来减少这些需求。

Method: 评估了LLMs和VLMs在多种环境中的反馈能力，包括二元反馈、偏好反馈、行动建议、目标建议和增量行动反馈，并测试了上下文学习、思维链和环境动态访问等推理方法。

Result: 基础模型能跨领域提供多样化高质量反馈，更大和具备推理能力的模型反馈更准确、偏见更少，且能从增强推理方法中获益更多。

Conclusion: 基础模型是规划任务中有价值的反馈来源，但反馈质量在复杂动态或连续状态/行动空间环境中会下降。

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [15] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: 提出基于生成式AI的模块化多模态框架，从公开住宅信息和图像生成能源建模所需数据，解决数据获取难题


<details>
  <summary>Details</summary>
Motivation: 能源建模计算模型需要大量数据，但部分数据难以获取、成本高昂或存在隐私问题，需要寻找替代数据源

Method: 开发模块化多模态框架，利用生成式人工智能从公开的住宅信息和图像中生成所需数据，并提供完整的实现流程

Result: 实验表明该框架能避免生成模型的常见问题，产生真实且标注良好的数据

Conclusion: 通过减少对昂贵或受限数据源的依赖，为更易获取和可重复的研究开辟了新途径

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [16] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文综述了自动形式化领域的研究进展，提出了统一框架以促进不同领域间的交叉融合，推动下一代AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 自动形式化领域虽然发展迅速，但由于数学形式化和非正式语言到形式表示转换等研究相对独立发展，缺乏共享的方法论、基准和理论框架，限制了该领域的整体进展。

Method: 通过回顾显性和隐性的自动形式化实例，分析深度学习特别是大语言模型在其中的作用，并提出统一的理论框架。

Result: 识别了自动形式化在不同领域（数学定理证明、推理、规划、知识表示等）的应用现状，揭示了领域间缺乏协同的问题。

Conclusion: 需要建立统一的自动形式化框架来促进不同研究领域的交叉融合，这将加速下一代AI系统的开发进程。

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [17] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: 本研究开发了一个基于检索增强生成(RAG)的智能知识助手系统，专门用于山羊养殖健康管理，通过表格文本化和决策树文本化方法处理异构数据，在验证集和测试集上分别达到87.90%和84.22%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在畜牧业应用受限，主要受限于知识源的可用性、多样性和复杂性，特别是在山羊养殖健康管理领域缺乏有效的知识沟通工具。

Method: 采用检索增强生成(RAG)技术，提出表格文本化和决策树文本化两种结构化知识处理方法，建立覆盖疾病防治、营养管理、饲养管理等五个关键领域的山羊养殖知识库，并集成在线搜索模块实现实时信息检索。

Result: 异构知识融合方法效果最佳，验证集平均准确率87.90%，测试集84.22%。在文本、表格、决策树问答任务中准确率均超过85%，错误分析显示遗漏是主要错误类型。

Conclusion: 该系统展现了在实际山羊养殖应用中的鲁棒性和可靠性，结构化知识融合的模块化设计有效提升了模型性能，为进一步改进检索覆盖率和上下文整合提供了方向。

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [18] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 本文研究了LLM在UNO游戏中作为助手的能力，发现虽然所有模型都能超越随机基准表现，但只有少数模型能显著帮助其他玩家获胜


<details>
  <summary>Details</summary>
Motivation: 测试基于大语言模型的智能体是否能作为主动参与者真正帮助人类实现目标，特别是在协作性游戏场景中

Method: 构建工具让仅解码器LLM在RLCard游戏环境中作为智能体参与UNO游戏，使用两种不同的提示策略，评估从1B到70B参数的不同规模模型

Result: 所有模型在玩UNO时都能成功超越随机基准表现，但只有少数模型能够显著帮助其他玩家获胜

Conclusion: 虽然LLM在独立游戏中表现良好，但在协作助人方面的能力仍然有限，模型规模对性能有影响但并非决定性因素

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [19] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: 提出了一个从当前工作流管理系统向完全自主、分布式科学实验室演化的概念框架，包含智能化和组合化两个维度，旨在加速科学发现100倍。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现需要协调分布式设施和异构资源，研究人员被迫成为手动工作流协调者而非科学家。AI智能体技术为科学发现提供了新的加速机会，但需要明确如何在实际中实现和集成。

Method: 提出了一个概念框架，工作流沿着两个维度演化：智能化（从静态到智能）和组合化（从单一到群体），并提供了架构蓝图来指导社区向自主科学发展。

Result: 构建了一个从当前工作流管理系统到完全自主分布式科学实验室的演化路径，为利用AI智能体加速科学发现提供了系统性的方法论。

Conclusion: 该框架为科学社区提供了向自主科学发展的明确路径，具有实现100倍发现加速和变革性科学工作流程的潜力，将重新定义科学研究的方式。

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [20] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: 将WaveFunctionCollapse重新表述为马尔可夫决策过程，使优化算法专注于目标最大化，而WFC传播机制处理约束满足，相比传统联合优化方法表现更优


<details>
  <summary>Details</summary>
Motivation: 程序化内容生成需要同时满足设计者指定的目标和底层瓦片集隐含的邻接约束，传统联合优化方法在任务复杂度增加时表现不佳

Method: 将WaveFunctionCollapse重新表述为马尔可夫决策过程（MDP），分离局部约束满足和全局目标优化，让外部优化算法专注于目标最大化

Result: 在多个不同难度的领域中，WFC-MDP方法始终优于传统的联合优化方法，特别是在任务复杂度增加时优势更加明显

Conclusion: 将局部约束满足与全局目标优化解耦的方法具有显著优势，WFC-MDP框架为程序化内容生成提供了更有效的优化途径

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [21] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: 本文提出基于实际因果关系的变量重要性度量方法，用于评估可解释AI工具在布尔函数预测中的表现，并开发了新的XAI工具B-ReX，在大型基准测试中优于其他黑盒XAI工具。


<details>
  <summary>Details</summary>
Motivation: 由于解释的主观性，评估可解释AI方法具有挑战性。本文专注于表格数据和布尔函数预测的特定用例，旨在建立更精确的评估标准。

Method: 提出基于实际因果关系的正式变量重要性度量方法，评估最先进的XAI工具，并基于现有工具ReX开发新的XAI工具B-ReX。

Result: B-ReX在随机10值布尔公式上实现了0.072±0.012的Jensen-Shannon散度，优于其他黑盒XAI工具。

Conclusion: 基于实际因果关系的度量方法为XAI评估提供了更精确的标准，B-ReX工具在布尔函数预测任务中表现出色，为黑盒XAI工具提供了有效的解决方案。

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [22] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA是一个保护隐私的多智能体系统，通过将工作空间划分为私有和公共区域，使用匿名化机制保护敏感数据，同时通过知识增强和逻辑增强模块减少语义损失。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中的广泛应用，处理涉及隐私数据的任务时需要确保数据安全，但现有系统缺乏有效的隐私保护机制。

Method: 提出GAMA系统，划分私有和公共空间，在私有空间处理敏感数据，公共空间使用匿名化数据。包含DRKE（基于领域规则的知识增强）和DLE（基于反证的逻辑增强）两个关键模块来缓解匿名化带来的语义损失。

Result: 在两个公开问答数据集（Trivia Creative Writing和Logic Grid Puzzle）上表现优于最先进模型。在专门设计的隐私保护数据集（Knowledge Privacy Preservation和Logic Privacy Preservation）上也显示出卓越的隐私保护效果。

Conclusion: GAMA系统在保持任务处理性能的同时，有效实现了隐私保护，为大语言模型在多智能体系统中的安全应用提供了可行方案。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [23] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents是一个基于多极任务处理图和IF-THEN规则的多智能体协作框架，旨在解决复杂不确定任务中的规划问题，在知识和逻辑问答任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型增强了多智能体系统的能力，但在处理高度复杂且不确定的任务时，仍面临规划效率低、输出错误等问题，需要更好的任务规划机制。

Method: 提出XAgents框架，使用多极任务处理图实现动态任务规划和不确定性处理，结合领域特定的IF-THEN规则约束智能体行为，并通过全局规则增强智能体间协作。

Result: 在三个不同数据集上的评估表明，XAgents在知识型和逻辑型问答任务中 consistently 超越最先进的单智能体和多智能体方法。

Conclusion: XAgents通过创新的多极任务处理图和规则集成机制，有效解决了复杂任务规划中的不确定性挑战，为多智能体系统提供了更可靠的协作框架。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [24] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: 提出了AI Harmonics框架，使用基于序数严重性数据的新型AI危害评估指标(AIH)，无需精确数值估计即可捕捉相对影响，重点关注政治和物理危害的紧急缓解需求。


<details>
  <summary>Details</summary>
Motivation: 现有AI风险评估模型过于关注内部合规性，忽视了不同利益相关者视角和现实世界后果，需要转向以人为中心、基于实证事件数据的危害严重性自适应方法。

Method: 开发AI Harmonics框架，包含新颖的AI危害评估指标(AIH)，利用序数严重性数据来评估相对影响，结合稳健的通用方法和数据驱动的利益相关者感知框架。

Result: 在标注事件数据上的实验证实，政治和物理危害具有最高集中度，需要紧急缓解：政治危害侵蚀公众信任，物理危害构成严重甚至危及生命的风险。

Conclusion: AI Harmonics能够一致识别不均匀的危害分布，使政策制定者和组织能够有效针对性地开展缓解工作，具有重要的现实世界相关性。

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [25] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 提出了"沙盒经济"框架来分析新兴的AI代理经济，从起源（涌现vs有意）和与人类经济的分离程度（可渗透vs不可渗透）两个维度进行表征，并讨论了可引导AI代理市场的设计选择。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的快速采用，正在形成超越人类直接监督的新经济层，需要框架来分析和引导这一新兴系统，以应对前所未有的协调机会和系统性经济风险等挑战。

Method: 提出沙盒经济分析框架，从两个关键维度（起源和分离程度）表征AI代理经济，并探讨拍卖机制、AI"使命经济"设计和社会技术基础设施等可能的市场设计选择。

Result: 当前趋势指向自发涌现的庞大且高度可渗透的AI代理经济，提出了确保信任、安全和问责所需的基础设施设计建议。

Conclusion: 主张主动设计可引导的代理市场，确保技术变革与人类长期集体繁荣保持一致，需要通过精心设计的经济机制和社会技术基础设施来实现安全可控的AI代理市场。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [26] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: 提出了Robust Sparse Sampling (RSS)算法，这是首个具有有限样本理论性能保证的在线鲁棒MDP规划算法，能够在模型不确定环境下实现实时鲁棒规划。


<details>
  <summary>Details</summary>
Motivation: 在线MDP规划方法在实际应用中面临生成模型从有限数据学习带来的近似误差问题，可能导致性能下降或不安全行为。现有鲁棒MDP方法计算量大，不适合实时应用。

Method: 基于Sample Average Approximation (SAA)方法，通过计算鲁棒价值函数而非标称价值函数，开发了Robust Sparse Sampling算法，适用于无限或连续状态空间。

Result: RSS的样本和计算复杂度与状态空间大小无关，理论分析证明了其性能保证，实验表明在不确定动态环境中优于标准Sparse Sampling方法。

Conclusion: RSS算法为在线鲁棒规划提供了首个具有理论保证的实用解决方案，能够有效处理模型不确定性，适用于大规模动态环境。

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [27] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: 提出基于LLM的多智能体框架，用于自动化多孔材料表征，通过智能体自主理解任务、规划模拟、组装力场、执行并解释结果，初步实现文献驱动的力场提取和RASPA模拟自动化


<details>
  <summary>Details</summary>
Motivation: 自动化多孔材料表征能加速材料发现，但目前受限于模拟设置和力场选择的复杂性，需要简化流程并提高效率

Method: 采用基于大语言模型的多智能体框架，智能体能够自主理解表征任务、规划适当模拟、组装相关力场、执行模拟并解释结果以指导后续步骤

Result: 初步评估显示该方法具有高正确性和可重现性，验证了文献驱动的力场提取和自动化RASPA模拟设置的有效性

Conclusion: 该方法展示了实现完全自主、可扩展材料表征的潜力，为自动化材料发现提供了有前景的技术路径

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [28] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: CARENLI是一个用于临床自然语言推理的模块化代理推理框架，通过将知识访问与推理分离，在四个推理家族中显著提升了LLM的推理准确性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 传统假设认为扩大数据和参数规模能产生更结构化、可泛化的内部表示，但本文质疑这一假设在临床自然语言推理中的有效性，特别是在推理过程不明确时LLM倾向于使用启发式方法。

Method: 提出CARENLI框架，将前提-陈述对路由到特定推理家族的求解器，通过规划器、验证器和精炼器强制执行可审计程序，分离知识访问与原则性推理。

Result: 在四个LLM上，CARENLI将保真度提升高达42个百分点，在因果归因中达到98.0%，在风险状态抽象中达到81.2%。验证器以接近天花板可靠性标记违规，精炼器纠正了大量认知错误。

Conclusion: LLM通常保留相关事实但在推理不明确时默认使用启发式方法，CARENLI使这种分离显式化，同时为更安全、可审计的推理提供了框架，家族分类是主要瓶颈。

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [29] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: 研究表明用紧凑的逻辑语言替代自然语言可以保持小语言模型在推理任务上的强性能，为小语言模型在ontology工程中的应用提供新方向


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理领域存在明显短板，特别是在ontology工程等任务中表现不佳，研究旨在探索形式化方法如何提升小语言模型的推理能力

Method: 通过一系列初步实验，研究不同语法表达逻辑问题对小语言模型在预定义推理任务上性能的影响，比较自然语言与逻辑语言的效果

Result: 研究发现可以用更紧凑的逻辑语言替代自然语言，同时保持推理任务的强性能表现

Conclusion: 这些结果为在小语言模型中进一步精炼ontology工程的角色提供了基础，展示了形式化方法在提升语言模型推理能力方面的潜力

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [30] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: 该研究通过量化实验分析6个大语言模型在18个道德困境中的表现，发现所有模型都一致偏好关怀和美德价值观，而惩罚自由主义选择。具备推理能力的模型表现出更强的上下文敏感性和解释能力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能快速发展，需要解决机器决策如何与人类道德价值观对齐的紧迫问题，探索AI系统如何优先考虑道德结果以及这对人机共生的前景意味着什么。

Method: 对6个大型语言模型进行定量实验，在代表5种道德框架的18个困境中对结果进行排名和评分，分析模型架构、文化起源和可解释性对道德偏好的影响。

Result: 发现所有模型都存在显著一致的价值偏见：关怀和美德价值观结果被评为最道德，而自由主义选择被一致惩罚。推理模型对上下文更敏感且提供更丰富的解释，非推理模型产生更统一但不透明的判断。

Conclusion: 研究强调了可解释性和文化意识作为关键设计原则的重要性，以指导AI走向透明、对齐和共生的未来，为道德推理的大规模比较、概率模型行为与价值编码的理论联系提供了贡献。

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [31] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: State Algebra是一个使用代数方法表示和操作命题逻辑的新框架，包含集合、坐标和行分解三种层次化表示，在保持语义清晰的同时提供计算灵活性


<details>
  <summary>Details</summary>
Motivation: 开发一个既能保持命题逻辑语义清晰性，又能通过代数方法提供强大计算能力的灵活表示框架

Method: 构建三层表示层次（Set、Coordinate、Row Decomposition），使用状态向量和代数引擎进行计算，通过固定变量顺序获得规范形式

Result: 框架虽然默认归约不是规范的，但通过固定变量顺序可以获得唯一规范形式，在灵活性和紧凑性之间取得平衡

Conclusion: State Algebra为基于搜索和知识编译的算法提供了工具，并能自然扩展到概率逻辑和加权模型计数

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [32] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding框架通过结构化因果推理方法，将多智能体系统中的故障归因从模式识别任务转化为因果推理任务，显著提高了步骤级准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的故障归因方法准确率极低（低于17%），无法进行有效的反事实推理来确定修正单个动作是否能避免任务失败。

Method: 提出Abduct-Act-Predict（A2P）框架，通过三个结构化步骤：溯因推理推断隐藏根本原因，定义最小修正干预，预测后续轨迹验证干预效果。

Result: 在Algorithm-Generated数据集上达到47.46%的步骤级准确率（比基线16.67%提高2.85倍），在Hand-Crafted数据集上达到29.31%（比基线12.07%提高2.43倍）。

Conclusion: 通过因果推理视角重构问题，A2P Scaffolding为自动化故障归因提供了更鲁棒、可验证且准确度显著提升的解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [33] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 本文提出了一个信息论框架，通过分析状态-动作互信息模式来诊断RL系统部署时的异常，能够区分传感器故障和驱动器故障，为自适应RL系统提供自主故障检测基础。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署的强化学习代理面临传感器故障、驱动器磨损和环境变化等问题，但缺乏内在机制来检测和诊断这些故障，需要开发有效的诊断方法。

Method: 采用信息论框架分析状态-动作互信息模式，通过控制扰动实验验证信息指标对系统故障的区分能力，包括状态熵、状态-动作互信息和状态-动作-下一状态联合互信息等指标。

Result: 成功学习表现出特征性信息签名：状态-动作互信息从0.84比特增长到2.83比特（238%增长）；状态-动作-下一状态联合互信息呈倒U型曲线；信息指标能够区分传感器故障（广泛崩溃）和驱动器故障（选择性破坏）。

Conclusion: 信息模式既是学习的签名，也是系统健康的诊断工具，为基于信息论原理的自适应RL系统提供了自主故障检测和政策调整的基础。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑图表示，在精神病诊断中优于现有方法，特别是在小样本数据场景下。


<details>
  <summary>Details</summary>
Motivation: 脑网络标记数据有限，现有自监督学习方法的数据增强策略可能破坏脑图的关键结构语义，需要开发能够保护结构语义的表示学习方法。

Method: 提出两阶段框架：预训练阶段在小标记子集上训练边缘掩码器捕获关键结构语义；自监督学习阶段使用结构先验指导结构感知的数据增强过程。

Result: 在两个真实精神病数据集上的实验表明，SAM-BG优于最先进方法，特别是在小标记数据设置下，并能发现临床相关的连接模式。

Conclusion: SAM-BG通过结构语义保护有效提升了脑图表示学习性能，在小样本精神病诊断中具有优越性和更好的可解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [35] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT是一种解耦跨注意力迁移学习框架，允许在推理时仅使用单一传感器模态，通过跨注意力对齐损失实现模态间特征空间对齐，无需训练和推理时都使用配对传感器数据。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法需要在训练和推理时都使用配对的传感器数据，这在资源受限环境中部署成本高且技术不可行。需要一种方法能够在推理时仅使用单一传感器模态，同时利用其他模态的知识提升性能。

Method: 提出D-CAT框架，包含自注意力模块进行特征提取和新型跨注意力对齐损失。该损失强制对齐不同传感器的特征空间，而不需要耦合两种模态的分类流程。

Result: 在三个多模态人类活动数据集（IMU、视频、音频）上评估，在分布内场景中，从高性能模态（如视频到IMU）迁移可获得10% F1分数提升；在分布外场景中，即使较弱的源模态也能提升目标性能。

Conclusion: D-CAT通过实现单传感器推理和跨模态知识迁移，减少了感知系统的硬件冗余，同时保持准确性，这对于成本敏感或自适应部署（如家庭辅助机器人）至关重要。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [36] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了一个完全自我改进的加密货币交易代理，无需人工监督，在多模态市场输入下表现出色。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测极其困难，价格变动由快速变化的链上活动、新闻流和社交情绪驱动，而标记训练数据稀缺且昂贵。

Method: 从基础指令调优LLM开始，代理在闭环架构中迭代交替扮演三个角色（演员、法官、元法官），利用多模态市场输入和内部偏好反馈，持续优化交易策略和评估标准。

Result: 在不同市场机制下的实验表明，Meta-RL-Crypto在真实市场技术指标上表现良好，并优于其他基于LLM的基线方法。

Conclusion: 该研究提出了一种无需人工监督的自我改进交易代理架构，成功解决了加密货币预测的挑战，为自动化交易系统提供了新思路。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [37] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流中的信息损失来实现动态预算分配，无需训练或多策略组合，在长上下文推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法根据任务需求自适应调整不同层和注意力头的缓存预算。

Method: 通过分析层注意力输出损失，提出新的度量标准来比较不同头的缓存条目，实现层级的动态头预算分配；通过跨层信息对比，实现动态层预算分配。

Result: 在多个基准测试（LongBench、Needle-In-A-Haystack、Ruler、InfiniteBench）上表现出优越性能，发现动态层预算对生成任务关键，动态头预算对抽取任务重要。

Conclusion: LAVa是首个统一的缓存驱逐和动态预算分配策略，无需训练，在各种任务类型中保持顶级性能，为KV缓存压缩提供了新的有效解决方案。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [38] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: 提出HACO框架，结合风险校准和偏好优化，为医疗补助人群提供安全、公平、可审计的决策支持，控制不良事件风险。


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群的健康管理项目需要协调纵向服务和干预措施，必须确保安全、公平且可审计。传统方法难以在控制风险的同时优化服务偏好。

Method: HACO框架：1)训练轻量级风险模型预测不良事件；2)使用保形阈值屏蔽不安全行为；3)在安全子集上学习偏好策略。使用FQE评估策略并进行亚组分析。

Result: 风险区分能力强(AUC~0.81)，校准阈值合适(τ~0.038)，保持高安全覆盖率。亚组分析显示不同人口统计特征的价值估计存在系统性差异。

Conclusion: 保形风险门控与离线强化学习结合良好，能够为人口健康管理团队提供保守、可审计的决策支持，强调公平性审计的重要性。

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [39] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出基于单头交叉注意力机制的统一路由框架，动态选择最优LLM，在RouterBench基准上实现6.6%的质量提升和2.9%的最大性能提升


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在计算成本和性能特征上的多样化给实际部署带来了挑战，需要一种能够动态选择最优模型的成本感知路由方案

Method: 使用单头交叉注意力机制联合建模查询和模型嵌入，显式捕获细粒度查询-模型交互，预测响应质量和生成成本，并采用指数奖励函数平衡性能和成本

Result: 在RouterBench基准测试中，平均质量改进(AIQ)提升6.6%，最大性能提升2.9%，架构轻量且能有效跨域泛化

Conclusion: 该路由框架为成本感知的LLM路由建立了新标准，在保持轻量级的同时实现了性能和效率的显著提升

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [40] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一种联邦多智能体强化学习框架Fed-MARL，用于6G超密集边缘网络中的隐私保护、实时资源管理，通过跨层协调和加密聚合协议实现高效能效和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向超密集智能边缘环境发展，在严格的隐私、移动性和能耗约束下实现高效资源管理变得至关重要。需要解决异构边缘设备间的实时资源分配问题，同时保护用户隐私。

Method: 采用联邦多智能体强化学习框架，每个智能体使用深度循环Q网络(DRQN)学习去中心化策略，包括任务卸载、频谱接入和CPU能耗适应。引入基于椭圆曲线Diffie-Hellman密钥交换的安全聚合协议保护隐私。将问题建模为部分可观测多智能体马尔可夫决策过程。

Result: 仿真结果表明，Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保在动态资源受限的6G边缘网络中具有强大的隐私保护和可扩展性。

Conclusion: 该框架为6G边缘网络提供了一种有效的隐私保护资源管理解决方案，能够同时优化延迟、能效、频谱效率、公平性和可靠性，满足URLLC、eMBB和mMTC等6G特定服务需求。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [41] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步去噪器及其在即插即用算法中的应用，该去噪器被训练为显式函数梯度的精确算子，同时保持最先进的去噪能力


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法通常使用现成的去噪器来替代图像先验的邻近算子或梯度下降算子，但这些先验往往是隐式的。本文旨在开发一种能够同时作为显式函数梯度算子和保持先进去噪性能的去噪器

Method: 提出梯度步去噪器，通过训练使其精确地成为显式函数的梯度下降算子或邻近算子

Result: 梯度步去噪器成功实现了作为显式函数梯度算子的目标，同时保持了最先进的去噪性能

Conclusion: 梯度步去噪器为即插即用算法提供了一种新的有效工具，能够将显式函数优化与先进去噪能力相结合

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [42] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出Vendi信息增益(VIG)主动学习策略，通过考虑数据集整体预测不确定性来选择图像，在Snapshot Serengeti数据集上仅用10%标签就达到接近全监督的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决相机陷阱图像数据中物种识别因标注资源有限而成为瓶颈的问题，传统主动学习方法只关注个体预测不确定性而忽略了整个数据集的不确定性。

Method: 引入Vendi信息增益(VIG)主动学习策略，基于图像对整个数据集预测不确定性的影响来选择图像，同时考虑信息量和多样性。

Result: 在Snapshot Serengeti数据集上，VIG使用不到10%的标签就达到了接近全监督的预测准确率，在不同指标和批次大小下都优于标准基线方法，并在特征空间中收集了更多样化的数据。

Conclusion: VIG方法在数据有限的环境中具有广泛适用性，对生物多样性监测具有重要价值，能够有效解决标注资源受限的问题。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [43] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 该研究使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶事件，最高准确率达85.7%，并能区分惊吓、惊讶和基线状态，准确率达74.9%。


<details>
  <summary>Details</summary>
Motivation: 意外事件会损害注意力并延迟决策，在航空等高危环境中构成严重安全风险。惊吓和惊讶反应以不同方式影响飞行员表现，但在实践中难以区分，现有研究大多单独研究这些反应，缺乏对其综合效应或使用生理数据区分它们的研究。

Method: 使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶事件。采用SVM和XGBoost等算法，以及Late Fusion多模态融合方法。

Result: 惊吓和惊讶事件可以可靠预测，使用SVM和Late Fusion获得最高平均准确率85.7%。扩展评估包括基线条件后，使用XGBoost和Late Fusion成功区分惊吓、惊讶和基线状态，最高平均准确率达74.9%。

Conclusion: 研究表明基于生理信号的机器学习方法能够有效区分惊吓和惊讶反应，为高危环境中的安全风险评估提供了可靠的技术手段。

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [44] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 本文重新审视了离散动作环境中的actor-critic方法，发现DSAC性能不佳的主要原因是actor和critic熵的耦合，通过解耦这两个组件并引入灵活的off-policy框架，可以达到与DQN相当的性能。


<details>
  <summary>Details</summary>
Motivation: 值基方法如DQN是离散动作环境off-policy学习的默认方法，而常见的策略基方法要么无法有效利用off-policy数据，要么在离散动作设置中表现不佳，因此需要重新设计离散动作环境中的actor-critic方法。

Method: 提出灵活的off-policy actor-critic框架，解耦actor和critic的熵组件，使用m步Bellman算子进行critic更新，并将标准策略优化方法与熵正则化结合来实例化actor目标。

Result: 理论证明在表格设置中可以保证收敛到最优正则化值函数，实证表明在标准Atari游戏中可以达到DQN的性能水平，甚至无需熵正则化或显式探索。

Conclusion: 通过解耦actor-critic的熵组件和引入灵活的框架，成功解决了离散SAC的性能问题，为离散动作环境的off-policy学习提供了有效的actor-critic方法。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [45] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN是首个针对异构图设计的集成学习框架，通过元路径和随机丢弃创建多样化的图神经网络，利用残差注意力机制和相关性正则化提升分类精度


<details>
  <summary>Details</summary>
Motivation: 异构图中的节点类型、特征和局部拓扑结构的异质性给集成学习带来挑战，需要适应多样化的图学习器

Method: 使用元路径结合随机丢弃创建Allele GNNs，采用残差注意力机制校准不同元路径的GNNs，并引入相关性正则化项增加嵌入矩阵的差异性

Result: 在五个异构网络上的实验表明，HGEN始终以显著优势超越最先进的竞争对手

Conclusion: HGEN通过有效的集成学习框架成功解决了异构图学习中的挑战，提高了分类准确性

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [46] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如beam search或best-of-N），同时考虑token成本和延迟时间，在推理基准测试中优于静态策略。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时扩展方法主要关注并行生成方法如best-of-N，忽略了增量解码方法如beam search，且大多只关注token使用量而忽略了延迟时间，这对于用户体验和智能体工作流至关重要。

Method: 将推理时扩展建模为动态计算分配和方法选择问题，系统需要基于每个查询决定应用哪种策略以及分配多少计算资源，明确纳入token成本和时钟延迟两个关键指标。

Result: 在推理基准测试中，该方法持续优于静态策略，实现了良好的准确率-成本权衡，同时保持实际部署的可行性。

Conclusion: 动态计算分配框架能够有效提升大语言模型性能，通过智能选择生成策略和计算资源分配，在保证准确率的同时优化成本和延迟，适用于实际部署场景。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [47] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 提出了一种基于可观测变量的数据驱动计算方法，通过热力学拉格朗日量和神经网络来预测耗散动力系统的演化，无需直接观测动量和熵。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统的演化计算需要相空间数据，但在耗散动力系统中，动量和熵等相空间变量通常无法直接观测，这给数据驱动的预测带来了困难。

Method: 构建基于热力学拉格朗日量的新方法，设计神经网络来保持热力学约束并确保熵的非递减演化，仅使用可观测变量进行相空间演化描述。

Result: 该方法能够基于有限的数据点和相对较少的系统参数，有效地描述相空间演化。

Conclusion: 该框架为仅使用可观测变量进行耗散动力系统演化预测提供了一种高效的解决方案，克服了传统方法需要完整相空间数据的限制。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [48] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 提出了LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习中的过自信和低质量伪标签问题，并在开放世界场景下扩展为LoFT-OW，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的长尾半监督学习方法大多从零开始训练模型，容易导致过自信和低质量伪标签问题。为了解决这些问题，作者将长尾半监督学习扩展到基础模型微调范式。

Method: 提出了LoFT框架，通过参数高效微调基础模型来生成更可靠的伪标签。针对开放世界场景，进一步提出了LoFT-OW方法，提高模型的判别能力以处理分布外样本。

Result: 在多个基准测试上，该方法相比先前方法取得了优越的性能，即使只使用1%的无标签数据也能超越之前的工作。

Conclusion: 微调的基础模型能够生成更可靠的伪标签，有利于不平衡学习。该方法为长尾半监督学习提供了有效的解决方案，特别是在开放世界场景下表现出色。

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [49] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 提出多播放组合半赌博机(MP-CSB)模型，扩展传统组合半赌博机到非负整数动作空间，解决最优传输和背包等问题，并提出两种高效算法。


<details>
  <summary>Details</summary>
Motivation: 传统组合半赌博机(CSB)局限于二元决策空间，无法处理非负整数流或分配问题，如最优传输和背包问题，需要扩展模型以覆盖更广泛的应用场景。

Method: 提出MP-CSB模型，允许选择非负整数动作并从单个臂获得多次反馈。开发了两种算法：基于Thompson采样的算法处理指数级大动作空间，以及最佳两界算法在随机和对抗环境下都表现良好。

Result: Thompson采样算法在随机环境下达到O(log T)分布依赖遗憾，计算可行；最佳两界算法在随机环境下达到O(log T)方差依赖遗憾，在对抗环境下达到O(√T)最坏情况遗憾，且具有数据适应性。数值实验显示优于现有方法。

Conclusion: MP-CSB成功扩展了CSB框架，提出的算法在理论和实验上都表现出色，为处理非负整数组合优化问题提供了有效解决方案。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [50] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 本文探讨使用LLMs作为科学机器学习代理，通过生成代码解决ODE问题，而非直接预测目标值。研究引入两个新数据集评估LLM在科学计算中的能力，发现通过适当的提示和微调，LLM可以可靠解决简单ODE问题。


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法（如物理信息神经网络）在准确性和鲁棒性方面面临挑战，本文探索使用LLMs编写数值算法代码的替代方案，将学习负担从函数学习转移到领域感知的数值选择。

Method: 引入诊断性误导问题数据集和大规模1000个ODE任务基准；评估开源和闭源LLM模型，采用无引导vs领域知识引导提示、现成vs微调变体两种策略；测量代码可执行性和数值有效性。

Result: 研究发现，在充足上下文和引导提示下，较新的指令跟随模型在两个评估标准上都达到高准确率；开源系统无需微调表现强劲，而较老或较小模型仍能从微调中受益。

Conclusion: 精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专用LLM代理，为科学机器学习提供了有前景的新方向。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [51] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena模型通过将音频-视觉线索转换为动态的逐token卷积核来直接调制文本特征提取，而不是简单的特征融合，在多模态意图识别任务上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态意图识别方法通过多头注意力等机制融合多模态特征，但这种方法可能会用噪声或无关的非语言信号破坏主要语言特征，无法捕捉细粒度的token级别影响。

Method: 提出DyKen-Hyena模型，将问题从特征融合重新定义为处理调制，将音频-视觉线索转换为动态的逐token卷积核来直接调制文本特征提取过程。

Result: 在MIntRec和MIntRec2.0基准测试中达到最先进水平，在域外检测任务上获得+10.46%的F1分数提升。

Conclusion: 该方法创建了本质上更鲁棒的意图表示，验证了细粒度调制方法的有效性。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [52] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出无需训练的token合并框架，通过自适应合并语义冗余token来压缩transformer表示，在保持精度的同时显著降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 大规模transformer在语义通信中计算和通信成本高，难以部署在资源受限的边缘设备上，需要一种无需重新训练的高效压缩方法

Method: 基于每层相似度阈值选择性合并语义冗余token，将合并策略发现建模为多目标优化问题，使用贝叶斯优化获得精度、推理成本和通信成本之间的帕累托最优权衡

Result: 在ImageNet分类中，以30%更少的FLOPs和低于20%的原始通信成本匹配未修改transformer的精度；在VQA任务中，以不到三分之一计算量和十分之一带宽实现与完整LLaVA模型相当的性能

Conclusion: 该框架为在资源受限的边缘智能场景中部署强大transformer模型提供了实用且通用的解决方案，具有跨信道条件的鲁棒性和隐私保护优势

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [53] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine是一个合成表格数据生成框架，通过从可解释模型提取符号规则嵌入提示词来指导生成，并采用双粒度过滤策略减少分布不平衡，在稀缺数据场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成方法（GANs、扩散模型、微调LLMs）需要充足参考数据，在领域特定数据库记录稀缺时效果受限。基于提示的LLMs虽然灵活但难以捕捉数据集特定的特征-标签依赖关系，且生成冗余数据导致下游任务性能下降。

Method: 提出ReFine框架：(i)从可解释模型推导符号"if-then"规则并嵌入提示词，显式指导生成符合领域特定特征分布；(ii)应用双粒度过滤策略，抑制过采样模式并选择性精炼稀有但信息丰富的样本以减少分布不平衡。

Result: 在多种回归和分类基准测试中，ReFine始终优于最先进方法，回归任务R平方绝对提升达0.44，分类任务F1分数相对提升10.0%。

Conclusion: ReFine通过结合符号规则指导和双粒度过滤，有效解决了稀缺数据场景下的表格生成问题，显著提升了生成数据的质量和下游任务性能。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [54] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的虚拟机能耗估计方法，仅使用客户机资源利用率指标就能准确预测能耗，无需物理功率测量接口或特权主机访问权限。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟化环境（如云平台）中无法直接测量能耗的关键问题，为能源感知调度、成本优化和独立于物理主机的能耗估计提供解决方案。

Method: 使用从客户虚拟机收集的资源利用率指标，训练梯度提升回归器（Gradient Boosting Regressor）来预测通过RAPL在主机上测量的能耗。

Result: 在多样化工作负载实验中实现了高预测精度（0.90 ≤ R² ≤ 0.97），首次证明了无需特权主机访问的纯客户机资源能耗估计的可行性。

Conclusion: 该方法证明了在虚拟化环境中仅通过客户机侧资源指标进行能耗估计的可行性，为云环境中的能源管理和优化提供了实用解决方案。

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [55] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 本文实证研究了深度回归模型中的神经缩放定律，发现在扭曲范德瓦尔斯磁体参数估计任务中，损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的成功凸显了神经缩放定律的重要性，但这些定律在深度回归模型中的应用仍未被充分探索。本文旨在填补这一空白，研究深度回归中的缩放行为。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，采用多种架构（包括全连接网络、残差网络和视觉变换器）在不同训练数据集大小和模型容量范围内进行实证研究。

Result: 观察到损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2，具体值取决于回归参数和模型细节。

Conclusion: 一致的缩放行为和大缩放指数表明，深度回归模型的性能可以随着数据量的增加而显著提高。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [56] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA是一种能够估计数据集内在维度并重建原始数据的自编码器，通过投影重建损失项和重加权双CancelOut层结构，在理论和实际流体动力学数据上都表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够准确估计线性或非线性流形数据集内在维度，同时保持数据重建能力的统一框架，解决现有方法在维度估计和重建能力之间的权衡问题。

Method: 使用重加权双CancelOut层构建潜在空间，引入投影重建损失项来指导训练，通过连续评估在移除额外潜在维度情况下的重建质量来确定内在维度。

Result: 在理论基准测试中表现出良好的准确性和高适应性，在流体动力学数值解数据集上成功估计内在维度并重建原始解。

Conclusion: IDEA提供了一个统一的框架，能够同时准确估计数据集的内在维度和重建原始数据，在理论和实际应用中都显示出优越性能。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [57] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏专家混合变分自编码器(SMoE-VAE)架构，在QuickDraw数据集上发现无监督专家路由比有监督基准表现更好，能够识别超越人工定义类别边界的有意义子类别结构。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络内部组织是深度学习可解释性的基本挑战，需要探索新的架构来揭示数据的内在结构。

Method: 使用稀疏专家混合变分自编码器(SMoE-VAE)，在QuickDraw数据集上比较无监督专家路由和有监督基准，通过t-SNE可视化和重建分析研究模型如何发现数据的基本结构。

Result: 无监督路由始终获得更好的重建性能，专家学会了识别有意义且经常超越人工定义类别边界的子类别结构。数据集大小研究揭示了数据量与专家专业化之间的权衡关系。

Conclusion: MoE模型能够发现比预定义标签更符合模型目标的基本数据结构，为设计高效的MoE架构提供了指导。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [58] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出了一种用于双字典稀疏编码的低秩编码模型AODL，通过凸松弛和交替优化学习可泛化的字典，在保持重建质量的同时获得更稀疏的解


<details>
  <summary>Details</summary>
Motivation: 解决多字典场景下同时学习字典和编码系数的挑战，特别是编码系数对应于所有字典原子组合时的高复杂度问题

Method: 提出低秩编码模型，建立样本复杂度边界，设计凸松弛解决方案AODL，通过稀疏编码矩阵和学习字典之间的交替优化进行求解

Result: AODL相比非低秩和固定分析字典基线学习到稀疏度高90%的解，在合成和真实数据集上展示了良好的数据重建和缺失值填补性能，学习到的字典揭示了训练样本中的可解释模式

Conclusion: 低秩编码模型有效解决了多字典学习的数据复杂度问题，AODL方法能够学习到更稀疏且可解释的表示，在保持重建质量的同时显著提升稀疏性

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [59] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文提出了一个形式化理论，证明概率有限自动机(PFAs)可以通过符号前馈神经网络精确模拟。该架构使用向量表示状态分布，随机矩阵表示转移，通过矩阵-向量乘积实现概率状态传播，提供了并行、可解释、可微分的PFA模拟。


<details>
  <summary>Details</summary>
Motivation: 弥合符号计算与深度学习之间的差距，将概率自动机理论与神经架构在严格的代数框架下统一起来，实现可学习、可解释的符号计算。

Method: 使用符号前馈神经网络架构，将状态分布表示为向量，转移表示为随机矩阵，通过矩阵-向量乘积实现概率状态传播，支持概率子集构造、ε-闭包和精确模拟。

Result: 证明了PFAs与特定类神经网络之间的等价性，展示了这些符号模拟器不仅具有表达力而且可学习：通过标准梯度下降优化在标记序列数据上训练，能够恢复真实PFAs的精确行为。

Conclusion: 该工作通过严格的代数框架统一了概率自动机理论和神经架构，为符号计算与深度学习的融合提供了理论基础，核心贡献在于形式化了这些符号模拟器的可学习性。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [60] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP是一种新颖的联邦学习算法，通过结合随机投影和ADMM优化框架，在保护隐私的同时降低通信成本，并提供强差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护用户隐私方面面临挑战，包括隐私攻击风险和通信成本管理问题，需要一种既能保持模型准确性又能提供强隐私保护的高效方法。

Method: 提出FedRP算法，将随机投影技术与ADMM优化框架结合，通过随机投影降低模型参数的维度后再传输到中央服务器，减少通信成本并提供差分隐私保护。

Result: 实验结果表明FedRP不仅保持高模型准确性，而且在隐私保护和通信效率方面优于传统差分隐私方法和FedADMM等现有方法。

Conclusion: FedRP算法成功解决了联邦学习中的隐私保护和通信效率问题，为物联网和医疗数据分析等敏感领域的应用提供了有效的解决方案。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [61] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 本文评估了在TabPFN中集成VBLL方法对不确定性校准的效果，发现在三个医疗表格数据集上，原始TabPFN在不确定性校准方面始终优于VBLL集成版本。


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等安全关键应用中，可靠的不确定性估计至关重要。TabPFN是新兴的表格数据基础模型，而VBLL是最先进的轻量级变分方法，本研究旨在评估两者结合的效果。

Method: 在三个基准医疗表格数据集上进行实验，比较原始TabPFN和VBLL集成版本的性能，重点关注不确定性校准能力。

Result: 与预期相反，原始TabPFN在所有数据集上的不确定性校准表现都优于VBLL集成版本。

Conclusion: VBLL集成并未改善TabPFN的不确定性校准性能，原始TabPFN在这方面表现更优。

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [62] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR是一个基于Kolmogorov Arnold Networks的新型符号回归框架，采用分治方法，结合深度学习技术和简化策略，能够准确恢复Feynman SRSD数据集中的真实方程，并能精确建模生物过程系统动力学。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归通常使用遗传编程方法，本文旨在利用深度学习技术开发更高效的符号回归框架，提高方程发现的准确性和效率。

Method: 使用Kolmogorov Arnold Networks (KANs)构建符号回归框架，采用分治方法，结合平移对称性和可分离性等简化策略，并与神经控制微分方程结合进行动态建模。

Result: 成功恢复了Feynman SRSD数据集中的真实方程，并精确建模了硅内生物过程系统的动力学行为。

Conclusion: KAN-SR框架为符号回归提供了新的深度学习解决方案，能够有效发现数学方程并建模复杂工程系统的动态行为，具有广泛的应用前景。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [63] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的贝叶斯联邦学习个性化框架，通过将全局模型投影到用户本地模型邻域，实现全局泛化与本地特化的可调权衡，计算等价于统计流形上的重心计算，具有闭式解和零成本个性化优势。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习方法通常依赖MCMC采样或变分推断，需要个性化机制来适应异构数据分布，但计算成本较高。本文旨在开发一种更高效的个人化方法。

Method: 采用信息几何投影框架，将全局模型投影到用户本地模型邻域，证明该投影步骤等价于计算统计流形上的重心，从而获得闭式解。结合改进变分在线牛顿优化器(IVON)应用于变分学习设置。

Result: 在异构数据分布下的实证评估表明，该方法能有效平衡全局和本地性能，且计算开销极小。

Conclusion: 提出的信息几何投影框架为贝叶斯联邦学习提供了一种计算高效的个人化方法，实现了全局泛化与本地特化的最优权衡，适用于各种聚合方案。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [64] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 提出了BenchECG标准化基准和xECG模型，通过统一评估标准促进ECG基础模型发展，xECG在多项任务中表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型研究缺乏一致的评估标准，使用不同的任务和数据集，阻碍了公平比较和进展

Method: 开发BenchECG标准化基准套件，包含公开ECG数据集和多样化任务；提出基于xLSTM的xECG模型，使用SimDINOv2自监督学习训练

Result: xECG在BenchECG基准上取得了最佳成绩，是唯一在所有数据集和任务上都表现优异的公开可用模型

Conclusion: BenchECG通过标准化评估促进了ECG表示学习的严格比较，xECG为未来ECG基础模型设立了新的性能基准

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [65] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF是一种新颖的联邦学习框架，通过在本地训练期间直接学习量化模型参数，实现逐比特更新策略，显著降低通信开销同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在显著的通信开销问题，现有量化方法通常在本地训练后应用量化，导致量化误差影响模型精度。

Method: 服务器先量化模型参数并传输给客户端，每个客户端在每轮通信中只更新多比特参数表示中的单个比特，冻结其余比特。

Result: 在5个常用数据集上的实验表明，FedBiF在IID和非IID设置下都能实现优异的通信压缩，同时促进模型稀疏性，使用1bpp上行和3bpp下行通信时达到与FedAvg相当的精度。

Conclusion: FedBiF通过逐比特更新策略有效解决了联邦学习的通信效率问题，在保持模型精度的同时显著降低了通信成本。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [66] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出了一种通过神经网络连续函数近似来重新优化表面码解码器的技术，解决了传统方法因非唯一性预测只能获取误差概率分布的问题，在多种网络架构和码距下均提高了解码精度。


<details>
  <summary>Details</summary>
Motivation: 量子纠错码中表面码虽然具有高错误阈值，但传统解码器由于输入的非唯一性只能获取误差概率分布，无法获得唯一正确预测，这限制了解码精度。

Method: 通过神经网络数学插值近似综合征测量，将解码问题重构为回归问题，使用多层感知机、卷积神经网络、循环神经网络和Transformer等深度学习架构进行重新优化。

Result: 在码距5和7的所有测试案例中，重新优化的解码器都比原始模型具有更好的精度，证明了该方法在不同码距和网络架构下的普适有效性。

Conclusion: 将表面码解码问题重新构建为可通过深度学习解决的回归问题是一个有效的策略，该方法具有通用性和架构无关性。

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [67] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 该论文研究了深度残差网络在标准随机初始化下的梯度训练动力学，证明了当深度L趋于无穷时，训练动态收敛到神经平均ODE，并分析了不同残差缩放因子对特征学习的影响。


<details>
  <summary>Details</summary>
Motivation: 研究深度残差网络的训练动态，特别是当网络深度趋于无穷时的极限行为，以及不同参数缩放对特征学习能力的影响，为理解Transformer等深层架构提供理论依据。

Method: 采用数学分析方法，通过随机初始化的前向和后向传播行为建立随机近似，利用传播混沌理论证明训练动态收敛到平均ODE，并对不同残差缩放因子进行理论分析和实验验证。

Result: 证明了深度残差网络训练动态收敛到神经平均ODE，获得了输出与极限之间的误差界O(1/L + α/√(LM))，并发现当α=Θ(1)时实现完全特征学习，而α→∞时进入惰性ODE机制。对于两层感知机块，完全特征学习需要残差缩放Θ(√D/(LM))。

Conclusion: 该研究为深度残差网络的训练动态提供了严格的理论框架，揭示了不同参数缩放对特征学习能力的影响，为深层神经网络的理论理解和实践设计提供了重要指导。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [68] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了一个可扩展的框架，用于学习高分辨率3D物理模拟的确定性和概率性神经代理模型，通过混合CNN-Transformer架构在速度和精度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率3D物理模拟计算成本高的问题，通过神经网络代理模型来加速模拟过程，同时保持准确性。

Method: 使用混合CNN-Transformer骨干架构，可以在小型模拟域补丁上进行预训练，然后融合获得全局解，通过序列到序列模型包含长程依赖关系。

Result: 在14种不同类型3D PDE动力学学习中优于基线方法，可扩展到512^3空间分辨率的高分辨率各向同性湍流，并能作为扩散模型生成概率样本。

Conclusion: 该框架为高分辨率3D物理模拟提供了高效准确的神经代理解决方案，具有很好的可扩展性和通用性。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [69] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出了一种新的集成学习框架，通过将边际方差纳入损失函数并重新参数化集成权重到单位球面，解决了传统边际方法忽略方差和计算效率低的问题


<details>
  <summary>Details</summary>
Motivation: 传统基于边际的集成方法主要关注最大化期望边际而忽略边际方差的关键作用，限制了模型的泛化能力并增加了过拟合风险，特别是在噪声或不平衡数据集中。同时，传统概率单纯形中的集成权重优化方法存在计算效率低和可扩展性挑战

Method: 提出新颖的集成学习框架，在损失函数中显式包含边际方差，联合优化负期望边际及其方差。通过将集成权重重新参数化到单位球面，简化优化过程并提高计算效率

Result: 在多个基准数据集上的广泛实验表明，所提出的方法始终优于传统的基于边际的集成技术

Conclusion: 该方法通过同时考虑期望边际和边际方差，显著提高了集成学习的鲁棒性和泛化性能，同时通过权重重新参数化改善了计算效率，具有实际应用价值

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [70] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 提出基于机器学习的飞机机翼疲劳寿命预测管道，替代传统有限元模拟等复杂方法，提高预测效率并降低计算资源需求


<details>
  <summary>Details</summary>
Motivation: 传统飞机疲劳寿命预测方法耗时且复杂，需要多团队协作和大量有限元模拟。机器学习方法可以提供快速预测，作为传统方法的补充，提高效率并减少资源消耗

Method: 开发基于机器学习的管道，根据飞机不同任务的飞行参数来预测机翼各位置的疲劳寿命。包括统计验证和不确定性量化

Result: 在真实疲劳寿命预测用例中验证了管道的有效性，获得了准确的预测结果

Conclusion: 该机器学习管道是对传统方法的有效补充，能够显著减少昂贵模拟的需求，降低计算和人力资源成本

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [71] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统评估发现：简单的提示词注入攻击对LLM同行评审高度有效（可达100%接受率），且LLM评审普遍存在接受偏向（>95%），这对LLM在学术评审中的应用讨论具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 针对作者使用隐藏提示词注入操纵评审分数的报道，研究这种攻击的可行性和技术成功率，以影响关于LLM在科学同行评审中使用的持续讨论。

Method: 使用多种LLM对2024年ICLR论文的1000篇评审进行系统评估，分析简单提示词注入的有效性和LLM评审的偏向性。

Result: 1) 非常简单提示词注入高度有效，达到100%接受率；2) LLM评审普遍偏向接受（许多模型>95%接受率）。

Conclusion: 研究结果对LLM在同行评审中使用的持续讨论具有重大影响，揭示了系统漏洞和固有偏向问题。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [72] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 提出了一个基于神经推荐系统的迁移学习框架，利用COSMO-RS模拟数据和稀疏实验数据，准确预测离子液体的五种关键热物理性质。


<details>
  <summary>Details</summary>
Motivation: 离子液体具有可定制的物理化学性质，但由于化学设计空间巨大和实验数据有限，准确预测其热物理性质仍然具有挑战性。

Method: 采用两阶段方法：首先在固定温度和压力下使用COSMO-RS模拟数据预训练神经推荐系统模型，学习阳离子和阴离子的性质特异性结构嵌入；然后使用这些嵌入和不同温度压力下的实验数据微调简单前馈神经网络。

Result: 该框架支持性质内和跨性质知识迁移，预训练的密度、粘度和热容模型用于微调所有五种目标性质模型，其中四种性质的性能显著提升。模型对未见过的离子液体表现出稳健的外推能力，可为超过70万种离子液体组合进行性质预测。

Conclusion: 这项工作展示了结合模拟数据和迁移学习来克服实验数据稀疏性的有效性，为离子液体在过程设计中的筛选提供了可扩展的解决方案。

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [73] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 提出了一种基于SDN和AutoML的区块链能源交易架构，利用机器学习回归器生成随机数作为nonce候选，称为Proof of AutoML，特别适用于灾难场景下的安全能源交易。


<details>
  <summary>Details</summary>
Motivation: 在灾难场景中传统能源基础设施受损时，需要确保太阳能家庭与移动充电单元之间能源交易的安全性和可追溯性，而区块链网络需要强大的随机nonce生成机制。

Method: 采用SDN架构实现灵活的数据流和能源路由控制，利用五种AutoML选择的回归模型（梯度提升、LightGBM、随机森林、额外树和K近邻）生成随机输出作为nonce候选，通过9000样本数据集评估其随机性。

Result: 随机性分析显示随机森林和额外树回归器完全依赖随机性，梯度提升、K近邻和LightGBM分别达到97.6%、98.8%和99.9%的随机性得分，树集成模型表现出色。

Conclusion: 某些机器学习模型，特别是基于树的集成方法，可以作为轻量级有效的nonce生成器，用于构建抗灾难的区块链安全SDN能源交易基础设施。

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [74] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要大量模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习调度策略的方法

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择，支持从多样化数据源学习

Result: CDQAC显著优于原始数据生成启发式方法，超越最先进的离线和在线RL基线，仅需10-20个训练实例即可学习高质量策略

Conclusion: CDQAC是高效的离线RL调度方法，意外发现在随机启发式生成数据上训练效果优于遗传算法和优先级调度规则生成的高质量数据

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [75] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 提出了GraphCSVAE框架，通过深度学习和图表示方法整合卫星时间序列数据和专家知识，用于建模灾害物理脆弱性，并在孟加拉国和塞拉利昂的灾害案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有灾害风险评估方法在物理脆弱性建模方面进展有限，限制了决策者对联合国仙台框架进展的评估能力，需要新的数据驱动方法来填补这一空白。

Method: 开发了Graph Categorical Structured Variational Autoencoder (GraphCSVAE)概率框架，整合深度学习、图表示和分类概率推理，使用时间序列卫星数据和专家先验知识，引入弱监督一阶转移矩阵来反映物理脆弱性的时空分布变化。

Result: 在两个灾害频发且社会经济弱势地区（孟加拉国Khurushkul社区和塞拉利昂弗里敦市）成功揭示了灾后物理脆弱性的区域动态变化。

Conclusion: 该工作为局部时空审计和灾后风险减少的可持续策略提供了有价值的见解，推动了物理脆弱性建模的发展。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [76] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 提出了一种基于ARIMA模型启发的简单卷积模块ARMA，用于长期时间序列预测，通过两个卷积组件分别捕捉趋势和局部变化，直接进行多步预测，在多个基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型需要迭代多步预测且难以扩展到多变量设置，需要一种简单有效的直接多步预测方法。

Method: 设计包含两个卷积组件的模块：一个用于捕捉趋势（自回归），另一个用于细化局部变化（移动平均），直接进行多步预测。

Result: 在9个基准数据集上实验表明，ARMA方法在具有强趋势变化的数据集上表现优异，同时保持架构简单性。

Conclusion: 该模块不仅实现了竞争性的预测精度，而且内在地编码了绝对位置信息，有潜力作为序列模型中位置嵌入的轻量级替代方案。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [77] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于结构保持数字孪生的自适应源定位机器学习框架，结合条件神经Whitney形式和变换器算子学习，实现实时轨迹规划和数据同化


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂流体输运系统中源定位问题，需要开发能够保持物理约束、适应实时传感器数据并保证数值稳定性的方法

Method: 使用条件神经Whitney形式(CNWF)构建数字孪生，耦合有限元外微积分(FEEC)和变换器算子学习，采用交错方案交替评估数字孪生和应用Lloyd算法指导传感器布置

Result: 实验显示在复杂几何形状中，当强制执行物理约束时，相比物理无关的变换器架构具有更高的精度，结构保持为源识别提供了有效的归纳偏置

Conclusion: 结构保持的数字孪生框架能够有效实现自适应源定位，保持离散守恒特性，并通过正则性作为源定位的充分条件

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [78] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，将数据集压缩任务从传统的泛化性能扩展到包括鲁棒性、隐私性等更多目标，并使用差异度量来形式化定义分布近似问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集压缩方法主要关注模型泛化性能，但缺乏统一的理论框架。本文旨在建立一个更通用的形式化定义，将数据集压缩扩展到更广泛的应用目标。

Method: 提出基于差异度量的统一框架，使用概率分布之间的距离概念来形式化数据集压缩问题，涵盖现有方法并支持多种目标优化。

Result: 建立了数据集压缩的统一理论框架，能够同时处理泛化、鲁棒性、隐私性等多个目标，为后续研究提供了理论基础。

Conclusion: 该框架为数据集压缩领域提供了更全面和形式化的理论基础，支持多种应用场景的扩展，具有重要的理论价值和实践意义。

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [79] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: CAPE模型通过对比学习在520万份心电图数据上预训练，发现预训练队列的人口统计学和健康状况分布影响下游任务性能，多中心多样化队列虽提高分布内精度但降低分布外泛化能力，提出了IDB策略来增强泛化性。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习在自监督预训练中对队列组成的依赖性，特别是不同人群、健康状况和人口多样性如何影响下游心电图预测任务的性能。

Method: 开发CAPE基础模型，在四大洲五个队列（n=5,203,352）上进行预训练，系统评估队列人口统计学、健康状况和多样性对下游性能的影响，并提出In-Distribution Batch (IDB)策略来保持队列内一致性。

Result: 发现下游性能取决于预训练队列的分布特性；多中心多样化队列提高分布内精度但降低分布外泛化能力；IDB策略能有效增强模型的分布外鲁棒性。

Conclusion: 这项工作为开发临床公平且可泛化的基础模型提供了重要见解，强调了预训练队列组成对模型性能的关键影响。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [80] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文建立了无限维希尔伯特空间中整流流的严格函数形式化框架，扩展了有限维生成模型到无限维函数空间的应用


<details>
  <summary>Details</summary>
Motivation: 虽然许多生成模型已在有限维欧几里得空间开发并推广到无限维设置，但整流流在无限维空间的扩展仍未被探索

Method: 基于无限维空间中连续性方程的叠加原理，建立了整流流的函数形式化框架，并扩展到函数流匹配和函数概率流ODE，将其解释为整流流的非线性推广

Result: 实验证明该方法相比现有函数生成模型具有更优越的性能

Conclusion: 提出的函数整流流框架不仅扩展了现有理论，还移除了现有函数流匹配理论中的限制性测度论假设，为无限维生成建模提供了更通用的方法

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [81] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO是一种针对掩码扩散大语言模型的强化学习框架，利用inpainting能力指导探索，通过插入部分真实推理轨迹来提升样本效率和性能


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习在LLM中对齐时的探索挑战，如稀疏奖励和样本浪费问题，利用dLLM独特的inpainting能力来引导探索

Method: 提出IGPO框架，在在线采样时策略性地插入部分真实推理轨迹，结合监督微调在合成重写的简洁轨迹上训练，并采用基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLM实现了新的最先进结果

Conclusion: IGPO成功地将dLLM的inpainting能力与强化学习相结合，有效解决了探索效率问题，为dLLM的强化学习对齐提供了新思路

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [82] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe是一种高效的softmax注意力近似方法，通过结合语义聚类和多极展开技术，将Transformer的计算复杂度从二次降低到线性或准线性，在保持性能的同时显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer中softmax注意力机制在序列长度上的二次计算复杂度问题，传统方法要么只聚类键值，要么使用统一聚类，未能充分考虑注意力机制中查询和键空间的不对称性。

Method: 提出多极语义注意力(MuSe)，在学习的表示空间中分别对查询和键进行聚类，采用分层两阶段注意力机制。在质心(单极)近似基础上增加偶极校正来捕捉簇内方向方差，保持训练过程中的丰富信息。

Result: 在8k上下文长度下，比CUDNN Flash Attention快3倍，相对平方误差低于20%。在16k上下文的30M参数模型预训练中，实现12.2%的运行时间减少，仅损失0.36%的性能。

Conclusion: 多极近似方法为高效Transformer预训练提供了可行方案，通过分离聚类和偶极校正有效平衡了计算效率和模型性能。

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [83] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 使用过程挖掘和机器学习进行铁路系统运行时控制流异常检测与定位


<details>
  <summary>Details</summary>
Motivation: 随着计算机化铁路系统的复杂性和关键性增加，需要提高系统韧性来应对设计时未知的残余故障、系统环境变化和新兴网络威胁

Method: 采用过程挖掘从执行轨迹中学习系统实际控制流，进行运行时一致性检查，并结合无监督机器学习进行异常定位

Result: 在ERTMS/ETCS L2的RBC切换参考场景中测试，显示出高精度、高效性和可解释性的异常检测与定位能力

Conclusion: 过程挖掘结合机器学习的方法能有效增强铁路控制系统的运行时韧性和安全性

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [84] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究了Local SGD中外部优化器的作用，证明了新的收敛保证，发现调整外部学习率可以在优化误差和随机梯度噪声方差之间权衡，并弥补内部学习率的不良调整。理论表明外部学习率有时应大于1，并扩展到使用动量和加速的外部优化器。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大规模批处理、分布式数据和并行计算硬件，通信成为主要瓶颈。Local SGD能减少通信开销，但现有研究主要关注本地优化过程的超参数，而外部优化器及其超参数的选择不够明确。

Method: 通过理论分析证明Local SGD的收敛保证，研究外部学习率的作用机制，扩展到动量优化器和加速方法，并进行数据依赖性分析。使用标准语言模型和各种外部优化器进行实验验证。

Result: 理论证明调整外部学习率可以权衡优化误差和随机梯度噪声方差，弥补内部学习率的不良调整。外部学习率有时应大于1，使用动量和加速能改善收敛速率。实验验证了理论发现。

Conclusion: 外部优化器在Local SGD中起着关键作用，适当调整外部学习率能显著提升算法性能，动量和加速技术的应用能进一步改善收敛效率，为分布式机器学习提供了重要的理论指导。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [85] [Evolution of Coordination Through Institutional Incentives: An Evolutionary Game Theory Approach](https://arxiv.org/abs/2509.10112)
*Ndidi Bianca Ogbo,Zhao Song,The Anh Han*

Main category: cs.GT

TL;DR: 本文通过进化博弈论模型分析有限制度资源在促进参与和确保承诺遵守之间的最优分配策略，发现奖励机制比惩罚机制更有效促进协调合作。


<details>
  <summary>Details</summary>
Motivation: 现有关于承诺机制和合作的研究大多停留在定性分析，缺乏对有限制度资源在促进参与和确保承诺遵守之间分配策略的定量研究，特别是在一次性互动场景中。

Method: 开发了一个进化博弈论模型，明确研究有限预算下制度激励（奖励或惩罚）在预承诺框架内两个关键目标（促进参与和确保遵守）之间的战略分配。

Result: 研究发现基于奖励的激励方法始终比基于惩罚的方法产生更大的协调成功率，当资源在促进参与和确保遵守之间适当分配时获得最优结果。

Conclusion: 这些发现为设计制度激励以促进新技术的广泛协调采用提供了新的见解，强调了奖励机制和资源平衡分配的重要性。

Abstract: There is a broad recognition that commitment-based mechanisms can promote
coordination and cooperative behaviours in both biological populations and
self-organised multi-agent systems by making individuals' intentions explicit
prior to engagement. Yet their effectiveness depends on sustained compliance
supported by institutions, especially in one-off interactions. Despite advances
in quantitative studies of cooperation and commitment, most applied analyses
and policy debates remain largely qualitative, with limited attention to the
allocation of scarce institutional resources between enhancing participation
and ensuring commitment compliance. Herein, we develop an evolutionary
game-theoretic model that explicitly examines the strategic distribution of a
limited budget for institutional incentives, namely rewards or punishments,
aimed at these two critical objectives within pre-commitment frameworks. Our
findings reveal that a reward-based incentive approach consistently yields
greater coordination success than a punishment-based approach, with optimal
outcomes arising when resources are appropriately distributed between
participation promotion and compliance assurance. These findings offer novel
insights for designing institutional incentives to promote broad, coordinated
adoption of new technologies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [86] [TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation](https://arxiv.org/abs/2509.09685)
*Keunwoo Choi,Seungheon Doh,Juhan Nam*

Main category: cs.IR

TL;DR: TalkPlayData 2是一个通过多智能体LLM管道生成的合成数据集，用于多模态对话式音乐推荐，支持音频和图像输入，并通过条件化对话目标覆盖多种对话场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态对话式音乐推荐系统训练数据的稀缺问题，通过合成数据生成方法来创建高质量的多模态对话数据集。

Method: 使用多角色LLM智能体（Listener和Recsys LLM）在专门设计的提示下进行对话，每个对话都基于微调的对话目标进行条件化，所有LLM都支持音频和图像多模态输入。

Result: 在LLM-as-a-judge和主观评估实验中，TalkPlayData 2在训练生成式音乐推荐模型的各个方面都达到了预期目标。

Conclusion: 该研究成功开发了一个开源的多模态对话音乐推荐数据集生成框架，为相关领域的研究提供了宝贵的数据资源和方法论支持。

Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational
music recommendation generated by an agentic data pipeline. In TalkPlayData 2
pipeline, multiple large language model (LLM) agents are created under various
roles with specialized prompts and access to different parts of information,
and the chat data is acquired by logging the conversation between the Listener
LLM and the Recsys LLM. To cover various conversation scenarios, for each
conversation, the Listener LLM is conditioned on a finetuned conversation goal.
Finally, all the LLMs are multimodal with audio and images, allowing a
simulation of multimodal recommendation and conversation. In the LLM-as-a-judge
and subjective evaluation experiments, TalkPlayData 2 achieved the proposed
goal in various aspects related to training a generative recommendation model
for music. TalkPlayData 2 and its generation code are open-sourced at
https://talkpl.ai/talkplaydata2.html.

</details>


### [87] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中获胜的解决方案，通过多模态检索管道和LLM幻觉控制技术，在三个任务中取得优异成绩并获得总冠军


<details>
  <summary>Details</summary>
Motivation: 解决CRAG-MM挑战赛中多模态、多轮问答的独特需求，特别是处理第一人称视角的自我中心查询挑战

Method: 开发了针对不同任务的定制化检索管道（处理图像索引知识图谱、网络来源和多轮对话）以及统一的LLM微调方法（使用SFT、DPO和RL进行拒绝训练）来控制幻觉

Result: 在Task 1获得第2名，Task 2获得第2名，Task 3获得第1名，凭借在自我中心查询方面的卓越表现赢得总冠军

Conclusion: 该综合框架成功整合了多模态检索能力和先进的LLM幻觉控制技术，在处理复杂多轮问答特别是第一人称视角挑战方面表现出色

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [88] [Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs](https://arxiv.org/abs/2509.09682)
*Maxim Zhelnin,Dmitry Redko,Volkov Daniil,Anna Volodkevich,Petr Sokerin,Valeriy Shevchenko,Egor Shvetsov,Alexey Vasilev,Darya Denisova,Ruslan Izmailov,Alexey Zaytsev*

Main category: cs.IR

TL;DR: 提出CCE-方法，通过GPU高效的负采样交叉熵损失实现，在序列推荐系统中将训练速度提升2倍，内存消耗减少10倍以上，同时支持更多负样本和批量大小来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决transformer-based序列推荐模型训练时因交叉熵损失导致的高计算成本和内存消耗问题，特别是在大规模商品目录场景下，传统负采样方法要么性能下降要么超出GPU内存限制。

Method: 开发CCE-方法，使用GPU高效的交叉熵损失实现，通过优化的Triton内核减少显存占用，支持同时增加负样本数量和批量大小来提升模型准确性。

Result: 训练速度提升2倍，内存消耗减少10倍以上，在大规模商品目录数据集上相比原始PyTorch实现获得更好的准确性，证明了同时扩展负样本和批量大小的有效性。

Conclusion: CCE-方法有效解决了序列推荐模型训练中的内存瓶颈问题，实现了计算效率和模型性能的双重提升，为工业级推荐系统提供了实用的解决方案。

Abstract: Sequential recommendations (SR) with transformer-based architectures are
widely adopted in real-world applications, where SR models require frequent
retraining to adapt to ever-changing user preferences. However, training
transformer-based SR models often encounters a high computational cost
associated with scoring extensive item catalogs, often exceeding thousands of
items. This occurs mainly due to the use of cross-entropy loss, where peak
memory scales proportionally to catalog size, batch size, and sequence length.
Recognizing this, practitioners in the field of recommendation systems
typically address memory consumption by integrating the cross-entropy (CE) loss
with negative sampling, thereby reducing the explicit memory demands of the
final layer. However, a small number of negative samples would degrade model
performance, and as we demonstrate in our work, increasing the number of
negative samples and the batch size further improves the model's performance,
but rapidly starts to exceed industrial GPUs' size (~40Gb).
  In this work, we introduce the CCE- method, which offers a GPU-efficient
implementation of the CE loss with negative sampling. Our method accelerates
training by up to two times while reducing memory consumption by more than 10
times. Leveraging the memory savings afforded by using CCE- for model training,
it becomes feasible to enhance its accuracy on datasets with a large item
catalog compared to those trained with original PyTorch-implemented loss
functions. Finally, we perform an analysis of key memory-related
hyperparameters and highlight the necessity of a delicate balance among these
factors. We demonstrate that scaling both the number of negative samples and
batch size leads to better results rather than maximizing only one of them. To
facilitate further adoption of CCE-, we release a Triton kernel that
efficiently implements the proposed method.

</details>


### [89] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 本文介绍了text-2-SQL-4-PM，一个为流程挖掘领域文本到SQL任务设计的双语（葡萄牙语-英语）基准数据集，包含1,655个自然语言语句和205个SQL语句。


<details>
  <summary>Details</summary>
Motivation: 促进自然语言查询数据库，提高非SQL专家用户的可访问性和专家用户的生产力，解决流程挖掘领域特有的挑战。

Method: 通过专家手动整理、专业翻译和详细标注过程构建数据集，并使用GPT-3.5 Turbo进行基线研究。

Result: 数据集支持文本到SQL实现的评估，展示了在流程挖掘领域的可行性和实用性。

Conclusion: text-2-SQL-4-PM为语义解析和其他自然语言处理任务提供了更广泛的适用性。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [90] [Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs](https://arxiv.org/abs/2509.09683)
*Briti Gangopadhyay,Zhao Wang,Shingo Takamatsu*

Main category: cs.IR

TL;DR: 提出了一种结合点击数据和文本日志的多模态预测框架，使用强化学习提升文本信息理解和模态融合效果，在准确性和推理质量上均优于基线方法


<details>
  <summary>Details</summary>
Motivation: 数字广告中的点击量预测对收入和营销策略至关重要，传统时间序列模型仅依赖数值数据，忽略了关键词更新等文本元素中的丰富上下文信息

Method: 多模态预测框架，结合点击数据和广告活动的文本日志，使用强化学习来改进文本信息理解和模态融合，同时生成人类可解释的解释

Result: 在大型行业数据集上的实验表明，该方法在准确性和推理质量方面均优于基线模型

Conclusion: 该多模态框架通过融合文本和数值信息，显著提升了点击量预测的性能，并为预测结果提供了可解释性

Abstract: Forecasting click volume is a key task in digital advertising, influencing
both revenue and campaign strategy. Traditional time series models rely solely
on numerical data, often overlooking rich contextual information embedded in
textual elements, such as keyword updates. We present a multimodal forecasting
framework that combines click data with textual logs from real-world ad
campaigns and generates human-interpretable explanations alongside numeric
predictions. Reinforcement learning is used to improve comprehension of textual
information and enhance fusion of modalities. Experiments on a large-scale
industry dataset show that our method outperforms baselines in both accuracy
and reasoning quality.

</details>


### [91] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: 提出基于波的语义记忆框架，用波模式表示知识并通过共振干涉进行检索，相比传统向量方法能更好地保留振幅和相位信息，实现更强大的语义相似性计算


<details>
  <summary>Details</summary>
Motivation: 传统基于向量的记忆系统依赖余弦或内积相似度，虽然计算高效但本质上是相位不敏感的，无法捕捉对意义表示至关重要的共振现象

Method: 将知识建模为波模式ψ(x)=A(x)e^{iφ(x)}，通过共振干涉进行检索，保留振幅和相位信息

Result: 共振检索在向量方法失效的情况下（如相位偏移、否定和组合查询）表现出更高的判别能力，ResonanceDB实现了百万级模式的可扩展性和毫秒级延迟

Conclusion: 基于波的记忆是向量存储的可行替代方案，适用于AGI导向的推理和知识表示

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [92] [GeoGPT.RAG Technical Report](https://arxiv.org/abs/2509.09686)
*Fei Huang,Fan Wu,Zeqing Zhang,Qihao Wang,Long Zhang,Grant Michael Boquet,Hongyang Chen*

Main category: cs.IR

TL;DR: GeoGPT是一个专为地球科学设计的开源大语言模型系统，通过检索增强生成(RAG)技术从专业知识库中获取信息，提供准确的地球科学领域回答。


<details>
  <summary>Details</summary>
Motivation: 为了提升地球科学领域大语言模型的领域专业能力，解决通用模型在地球科学专业知识上的不足，提供更准确、可信的领域特定输出。

Method: 集成检索增强生成(RAG)技术，构建专业的地球科学知识库(GeoGPT Library)，微调嵌入模型和排序模型来优化检索质量，支持用户上传个性化知识库。

Result: 显著提升了系统在地球科学应用中的检索质量和领域对齐能力，能够生成更精确、可信的输出结果。

Conclusion: GeoGPT通过开源核心RAG组件(GeoEmbedding和GeoReranker)，体现了对开放科学的承诺，为全球地球科学家提供了强大易用的AI工具，推动了地球科学研究的发展。

Abstract: GeoGPT is an open large language model system built to advance research in
the geosciences. To enhance its domain-specific capabilities, we integrated
Retrieval Augmented Generation(RAG), which augments model outputs with relevant
information retrieved from an external knowledge source. GeoGPT uses RAG to
draw from the GeoGPT Library, a specialized corpus curated for geoscientific
content, enabling it to generate accurate, context-specific answers. Users can
also create personalized knowledge bases by uploading their own publication
lists, allowing GeoGPT to retrieve and respond using user-provided materials.
To further improve retrieval quality and domain alignment, we fine-tuned both
the embedding model and a ranking model that scores retrieved passages by
relevance to the query. These enhancements optimize RAG for geoscience
applications and significantly improve the system's ability to deliver precise
and trustworthy outputs. GeoGPT reflects a strong commitment to open science
through its emphasis on collaboration, transparency, and community driven
development. As part of this commitment, we have open-sourced two core RAG
components-GeoEmbedding and GeoReranker-to support geoscientists, researchers,
and professionals worldwide with powerful, accessible AI tools.

</details>


### [93] [Demonstrating Narrative Pattern Discovery from Biomedical Literature](https://arxiv.org/abs/2509.09687)
*Hermann Kroll,Pascal Sackhoff,Bill Matthias Thang,Christin Katharina Kreutz,Wolf-Tilo Balke*

Main category: cs.IR

TL;DR: 本文介绍了PubPharm药学数字图书馆中的叙事模式挖掘新搜索功能，通过专家访谈验证了其有用性


<details>
  <summary>Details</summary>
Motivation: 数字图书馆需要为药学领域提供有效的知识访问路径，传统的关键词搜索和化学结构搜索已不能满足用户对上下文相关实体和交互的探索需求

Method: 开发了叙事模式挖掘功能，允许用户探索上下文相关的实体和实体交互，并通过与五位领域专家的访谈来验证原型的有用性

Result: 专家访谈验证了该叙事模式挖掘功能在药学数字图书馆中的实用性

Conclusion: 叙事模式挖掘为药学数字图书馆提供了新的有效访问路径，能够更好地支持用户探索上下文相关的实体和交互关系

Abstract: Digital libraries maintain extensive collections of knowledge and need to
provide effective access paths for their users. For instance, PubPharm, the
specialized information service for Pharmacy in Germany, provides and develops
access paths to their underlying biomedical document collection. In brief,
PubPharm supports traditional keyword-based search, search for chemical
structures, as well as novel graph-based discovery workflows, e.g., listing or
searching for interactions between different pharmaceutical entities. This
paper introduces a new search functionality, called narrative pattern mining,
allowing users to explore context-relevant entities and entity interactions. We
performed interviews with five domain experts to verify the usefulness of our
prototype.

</details>


### [94] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC数据与分析保存计划开发了一个基于大语言模型的AI助手系统，通过自然语言访问文档、工作流和软件，旨在支持重离子碰撞实验数据的可重复性、教育和未来发现。


<details>
  <summary>Details</summary>
Motivation: 随着RHIC运行25年结束，保存其海量数据（约1EB）和嵌入式科学知识成为关键优先事项，需要确保科学遗产的长期可访问性和可用性。

Method: 基于检索增强生成和模型上下文协议的大语言模型，索引RHIC实验的结构化和非结构化内容，实现领域适应的交互。

Result: 系统已部署并展示了良好的计算性能，正在进行多实验集成，架构设计注重可持续性和可解释的长期AI访问。

Conclusion: 现代AI/ML工具能够显著提升科学遗产数据的可用性和可发现性，为大型科学设施的数据保存提供了有效解决方案。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [95] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 使用冻结的大语言模型提取文本用户表示，通过微调小语言模型构建高效的用户行为代理，采用低秩适配器在可扩展性和性能间取得平衡


<details>
  <summary>Details</summary>
Motivation: 解决用户行为模拟的挑战，传统方法需要有效解析大规模表格交互数据、克服预训练偏差并实现大规模部署，现有方法主要关注复杂提示或微调大模型

Method: 使用冻结LLM提取文本用户表示，用微调SLMs构建高效用户代理，训练多个低秩适配器为用户群体创建个性化代理

Result: 实验证明该方法能有效弥合推荐系统离线指标与实际性能之间的差距

Conclusion: 该方法提供了在可扩展性和性能间取得平衡的有效方案，有望提升推荐系统用户行为模拟的准确性和效率

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [96] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 提出基于大语言模型的统一查询理解框架，替代传统多NER模型架构，通过联合建模查询和上下文信号生成结构化解释，提升推荐准确性和个性化程度


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多个任务特定的命名实体识别模型，存在架构脆弱、维护成本高、难以适应快速变化的分类体系和语言模式等问题

Method: 使用大语言模型构建统一查询理解框架，联合建模用户查询和上下文信号（如个人资料属性），生成结构化解释来驱动推荐系统

Result: 在线A/B测试中提升了相关性质量，同时显著降低了系统复杂度和运维开销

Conclusion: 该解决方案为动态网络应用中的查询理解提供了可扩展和适应性强的技术基础

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [97] [A Research Vision for Web Search on Emerging Topics](https://arxiv.org/abs/2509.10212)
*Alisa Rieger,Stefan Dietze,Ran Yu*

Main category: cs.IR

TL;DR: 本文提出了一个研究愿景，旨在开发支持新兴话题知识获取的搜索系统和界面，帮助用户了解话题动态性并形成负责任的观点。


<details>
  <summary>Details</summary>
Motivation: 新兴话题的信息稀疏且动态演变，质量参差不齐，容易受到操纵、错误信息和偏见的影响，现有搜索系统难以有效支持用户获取可靠知识。

Method: 提出三个核心研究问题：理解现状、确定系统需求、构建系统；通过文献综述和理论分析来探讨这些问题。

Result: 提出了一个完整的研究框架和愿景，指出了相关文献和可能的解决方案方向。

Conclusion: 开发支持新兴话题知识获取的搜索系统具有重要价值，但面临诸多挑战，需要跨学科研究来推进这一愿景的实现。

Abstract: We regularly encounter information on novel, emerging topics for which the
body of knowledge is still evolving, which can be linked, for instance, to
current events. A primary way to learn more about such topics is through web
search. However, information on emerging topics is sparse and evolves
dynamically as knowledge grows, making it uncertain and variable in quality and
trustworthiness and prone to deliberate or accidental manipulation,
misinformation, and bias. In this paper, we outline a research vision towards
search systems and interfaces that support effective knowledge acquisition,
awareness of the dynamic nature of topics, and responsible opinion formation
among people searching the web for information on emerging topics. To realize
this vision, we propose three overarching research questions, aimed at
understanding the status quo, determining requirements of systems aligned with
our vision, and building these systems. For each of the three questions, we
highlight relevant literature, including pointers on how they could be
addressed. Lastly, we discuss the challenges that will potentially arise in
pursuing the proposed vision.

</details>


### [98] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: 该研究提出了一种模型无关的删除诊断方法，用于评估推荐系统中特定用户或项目对模型性能的影响，提高推荐系统的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 推荐系统通常使用复杂的特征嵌入和深度学习算法来提供精准推荐，但这些方法往往降低了系统的可解释性和透明度。为了量化特定观测值对推荐系统的影响，需要开发一种系统性的诊断方法。

Method: 开发了一种删除诊断方法，通过比较原始模型与移除特定用户或项目后训练得到的相似模型的性能差异，来量化该观测值对推荐系统的正面或负面影响。该方法适用于不同推荐模型，包括神经协同过滤(NCF)和奇异值分解(SVD)。

Result: 在MovieLens和Amazon Reviews数据集上的实验表明，该方法能够提供对模型行为的深入洞察，并展示了在不同推荐范式中的通用性。

Conclusion: 提出的删除诊断方法是一种模型无关的工具，能够有效评估推荐系统中特定观测值的影响，为提高推荐系统的可解释性和透明度提供了实用解决方案。

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


### [99] [Diversified recommendations of cultural activities with personalized determinantal point processes](https://arxiv.org/abs/2509.10392)
*Carole Ibrahim,Hiba Bederina,Daniel Cuesta,Laurent Montier,Cyrille Delabre,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 使用个性化行列式点过程(DPP)在推荐系统中平衡相关性和多样性，通过质量-多样性分解核函数来更好地反映用户偏好，并在生产环境中评估了效果


<details>
  <summary>Details</summary>
Motivation: 在优化用户参与度的同时，有效多样化推荐内容而不影响核心业务指标是行业重大挑战，旨在扩大受众的文化实践

Method: 采用个性化DPP采样方法，基于质量-多样性相似性核分解，给予用户偏好更多权重，通过离线和在线指标评估相关性-多样性权衡

Result: 实现了推荐系统的多样化和相关性平衡，发布了完整的平台代码和实验代码以保证可复现性

Conclusion: 个性化DPP方法能够有效解决推荐系统中多样性需求，为实践者提供了在生产环境中使用的见解，代码已开源

Abstract: While optimizing recommendation systems for user engagement is a
well-established practice, effectively diversifying recommendations without
negatively impacting core business metrics remains a significant industry
challenge. In line with our initiative to broaden our audience's cultural
practices, this study investigates using personalized Determinantal Point
Processes (DPPs) to sample diverse and relevant recommendations. We rely on a
well-known quality-diversity decomposition of the similarity kernel to give
more weight to user preferences. In this paper, we present our implementations
of the personalized DPP sampling, evaluate the trade-offs between relevance and
diversity through both offline and online metrics, and give insights for
practitioners on their use in a production environment. For the sake of
reproducibility, we release the full code for our platform and experiments on
GitHub.

</details>


### [100] [RecoWorld: Building Simulated Environments for Agentic Recommender Systems](https://arxiv.org/abs/2509.10397)
*Fei Liu,Xinyu Lin,Hanchao Yu,Mingyuan Wu,Jianyu Wang,Qiang Zhang,Zhuokai Zhao,Yinglong Xia,Yao Zhang,Weiwei Li,Mingze Gao,Qifan Wang,Lizhu Zhang,Benyu Zhang,Xiangjun Fan*

Main category: cs.IR

TL;DR: RecoWorld是一个为智能推荐系统设计的模拟环境框架，通过双视图架构让用户模拟器和推荐代理进行多轮交互，利用LLM的推理能力优化用户留存率。


<details>
  <summary>Details</summary>
Motivation: 为智能推荐系统提供安全的训练环境，避免在真实用户身上试错，同时探索用户与推荐系统协同优化的新型交互范式。

Method: 采用双视图架构：用户模拟器审查推荐项目并更新心态，当检测到用户可能流失时生成反思指令；推荐代理结合用户指令和推理轨迹调整推荐策略，形成动态反馈循环。支持多种内容表示（文本、多模态、语义ID）和多智能体模拟。

Result: 构建了一个能够模拟用户-推荐系统多轮交互的训练环境，支持推荐策略的迭代优化和用户群体的模拟响应。

Conclusion: RecoWorld是迈向用户与智能体共同塑造个性化信息流的重要第一步，为'用户指导、推荐响应'的新型交互范式奠定了基础。

Abstract: We present RecoWorld, a blueprint for building simulated environments
tailored to agentic recommender systems. Such environments give agents a proper
training space where they can learn from errors without impacting real users.
RecoWorld distinguishes itself with a dual-view architecture: a simulated user
and an agentic recommender engage in multi-turn interactions aimed at
maximizing user retention. The user simulator reviews recommended items,
updates its mindset, and when sensing potential user disengagement, generates
reflective instructions. The agentic recommender adapts its recommendations by
incorporating these user instructions and reasoning traces, creating a dynamic
feedback loop that actively engages users. This process leverages the
exceptional reasoning capabilities of modern LLMs. We explore diverse content
representations within the simulator, including text-based, multimodal, and
semantic ID modeling, and discuss how multi-turn RL enables the recommender to
refine its strategies through iterative interactions. RecoWorld also supports
multi-agent simulations, allowing creators to simulate the responses of
targeted user populations. It marks an important first step toward recommender
systems where users and agents collaboratively shape personalized information
streams. We envision new interaction paradigms where "user instructs,
recommender responds," jointly optimizing user retention and engagement.

</details>


### [101] [MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables](https://arxiv.org/abs/2509.10448)
*Kausik Hira,Mohd Zaki,Mausam,N. M. Anoop Krishnan*

Main category: cs.IR

TL;DR: MatSKRAFT是一个从材料科学表格数据中自动提取和整合知识的计算框架，通过图神经网络处理表格数据，在性能和速度上显著优于大型语言模型


<details>
  <summary>Details</summary>
Motivation: 科学进步越来越依赖于从海量文献中综合知识，但大多数实验数据仍被困在半结构化格式中，难以进行系统提取和分析

Method: 将表格转换为基于图的表示，通过约束驱动的图神经网络处理，将科学原理直接编码到模型架构中

Result: F1分数达到88.68（属性提取）和71.35（成分提取），处理速度比现有模型快19-496倍，从47,000多篇研究论文中构建了包含535,000多个条目的综合数据库

Conclusion: 该方法能够发现先前被忽视的材料及其独特属性组合，为数据驱动的材料发现和科学发现奠定了基础

Abstract: Scientific progress increasingly depends on synthesizing knowledge across
vast literature, yet most experimental data remains trapped in semi-structured
formats that resist systematic extraction and analysis. Here, we present
MatSKRAFT, a computational framework that automatically extracts and integrates
materials science knowledge from tabular data at unprecedented scale. Our
approach transforms tables into graph-based representations processed by
constraint-driven GNNs that encode scientific principles directly into model
architecture. MatSKRAFT significantly outperforms state-of-the-art large
language models, achieving F1 scores of 88.68 for property extraction and 71.35
for composition extraction, while processing data $19$-$496\times$ faster than
them (compared to the slowest and the fastest models, respectively) with modest
hardware requirements. Applied to nearly 69,000 tables from more than 47,000
research publications, we construct a comprehensive database containing over
535,000 entries, including 104,000 compositions that expand coverage beyond
major existing databases, pending manual validation. This systematic approach
reveals previously overlooked materials with distinct property combinations and
enables data-driven discovery of composition-property relationships forming the
cornerstone of materials and scientific discovery.

</details>
