{"id": "2512.17967", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.17967", "abs": "https://arxiv.org/abs/2512.17967", "authors": ["Bri Holt"], "title": "Memelang: An Axial Grammar for LLM-Generated Vector-Relational Queries", "comment": null, "summary": "Structured generation for LLM tool use highlights the value of compact DSL intermediate representations (IRs) that can be emitted directly and parsed deterministically. This paper introduces axial grammar: linear token sequences that recover multi-dimensional structure from the placement of rank-specific separator tokens. A single left-to-right pass assigns each token a coordinate in an n-dimensional grid, enabling deterministic parsing without parentheses or clause-heavy surface syntax. This grammar is instantiated in Memelang, a compact query language intended as an LLM-emittable IR whose fixed coordinate roles map directly to table/column/value slots. Memelang supports coordinate-stable relative references, parse-time variable binding, and implicit context carry-forward to reduce repetition in LLM-produced queries. It also encodes grouping, aggregation, and ordering via inline tags on value terms, allowing grouped execution plans to be derived in one streaming pass over the coordinate-indexed representation. Provided are a reference lexer/parser and a compiler that emits parameterized PostgreSQL SQL (optionally using pgvector operators)."}
{"id": "2512.18238", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.18238", "abs": "https://arxiv.org/abs/2512.18238", "authors": ["Ding Jia", "Jingyu Zhu", "Yu Sun", "Aoqian Zhang", "Shaoxu Song", "Haiwei Zhang", "Xiaojie Yuan"], "title": "Sync Without Guesswork: Incomplete Time Series Alignment", "comment": null, "summary": "Multivariate time series alignment is critical for ensuring coherent analysis across variables, but missing values and timestamp inconsistencies make this task highly challenging. Existing approaches often rely on prior imputation, which can introduce errors and lead to suboptimal alignments. To address these limitations, we propose a constraint-based alignment framework for incomplete multivariate time series that avoids imputation and ensures temporal and structural consistency. We further design efficient approximation algorithms to balance accuracy and scalability. Experiments on multiple real-world datasets demonstrate that our approach achieves superior alignment quality compared to existing methods under varying missing rates. Our contributions include: (1) formally defining incomplete multiple temporal data alignment problem; (2) proposing three approximation algorithms balancing accuracy and efficiency; and (3) validating our approach on diverse real-world datasets, where it consistently outperforms existing methods in alignment accuracy and the number of aligned tuples."}
{"id": "2512.18405", "categories": ["cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18405", "abs": "https://arxiv.org/abs/2512.18405", "authors": ["El Kindi Rezig", "Mir Mahathir Mohammad", "Nicolas Baret", "Ricardo Mayerhofer", "Andrew McNutt", "Paul Rosen"], "title": "Towards Scalable Visual Data Wrangling via Direct Manipulation", "comment": "Published in CIDR 2026", "summary": "Data wrangling - the process of cleaning, transforming, and preparing data for analysis - is a well-known bottleneck in data science workflows. Existing tools either rely on manual scripting, which is error-prone and hard to debug, or automate cleaning through opaque black-box pipelines that offer limited control. We present Buckaroo, a scalable visual data wrangling system that restructures data preparation as a direct manipulation task over visualizations. Buckaroo enables users to explore and repair data anomalies - such as missing values, outliers, and type mismatches - by interacting directly with coordinated data visualizations. The system extensibly supports user-defined error detectors and wranglers, tracks provenance for undo/redo, and generates reproducible scripts for downstream tasks. Buckaroo maintains efficient indexing data structures and differential storage to localize anomaly detection and minimize recomputation. To demonstrate the applicability of our model, Buckaroo is integrated with the \\textit{Hopara} pan-and-zoom engine, which enables multi-layered navigation over large datasets without sacrificing interactivity. Through empirical evaluation and an expert review, we show that Buckaroo makes visual data wrangling scalable - bridging the gap between visual inspection and programmable repairs."}
{"id": "2512.18622", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18622", "abs": "https://arxiv.org/abs/2512.18622", "authors": ["Thanh Dat Hoang", "Thanh Trung Huynh", "Matthias Weidlich", "Thanh Tam Nguyen", "Tong Chen", "Hongzhi Yin", "Quoc Viet Hung Nguyen"], "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback", "comment": null, "summary": "Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql."}
{"id": "2512.17950", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17950", "abs": "https://arxiv.org/abs/2512.17950", "authors": ["Zhao Song", "Song Yue", "Jiahao Zhang"], "title": "Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy", "comment": null, "summary": "The rapid growth of AI conference submissions has created an overwhelming reviewing burden. To alleviate this, recent venues such as ICLR 2026 introduced a reviewer nomination policy: each submission must nominate one of its authors as a reviewer, and any paper nominating an irresponsible reviewer is desk-rejected. We study this new policy from the perspective of author welfare. Assuming each author carries a probability of being irresponsible, we ask: how can authors (or automated systems) nominate reviewers to minimize the risk of desk rejections? We formalize and analyze three variants of the desk-rejection risk minimization problem. The basic problem, which minimizes expected desk rejections, is solved optimally by a simple greedy algorithm. We then introduce hard and soft nomination limit variants that constrain how many papers may nominate the same author, preventing widespread failures if one author is irresponsible. These formulations connect to classical optimization frameworks, including minimum-cost flow and linear programming, allowing us to design efficient, principled nomination strategies. Our results provide the first theoretical study for reviewer nomination policies, offering both conceptual insights and practical directions for authors to wisely choose which co-author should serve as the nominated reciprocal reviewer."}
{"id": "2512.18115", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.18115", "abs": "https://arxiv.org/abs/2512.18115", "authors": ["Changxu Duan"], "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown", "comment": "Accepted ICDAR 2025", "summary": "Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced."}
{"id": "2512.18036", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.18036", "abs": "https://arxiv.org/abs/2512.18036", "authors": ["Connor Weyers", "N. V. Vinodchandran"], "title": "Fast Rational Search via Stern-Brocot Tree", "comment": null, "summary": "We revisit the problem of rational search: given an unknown rational number $α= \\frac{a}{b} \\in (0,1)$ with $b \\leq n$, the goal is to identify $α$ using comparison queries of the form ``$β\\leq α$?''. The problem has been studied several decades ago and optimal query algorithms are known. We present a new algorithm for rational search based on a compressed traversal of the Stern--Brocot tree, which appeared to have been overlooked in the literature. This approach also naturally extends to two related problems that, to the best of our knowledge, have not been previously addressed: (i) unbounded rational search, where the bound $n$ is unknown, and (ii) computing the best (in a precise sense) rational approximation of an unknown real number using only comparison queries."}
{"id": "2512.18117", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.18117", "abs": "https://arxiv.org/abs/2512.18117", "authors": ["Xiwen Chen", "Yen-Chieh Lien", "Susan Liu", "María Castaños", "Abolfazl Razi", "Xiaoting Zhao", "Congzhe Su"], "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning", "comment": "Accepted by WSDM'26", "summary": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search."}
{"id": "2512.18025", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.18025", "abs": "https://arxiv.org/abs/2512.18025", "authors": ["Benjamin D. Kim", "Daniel Alabi", "Lav R. Varshney"], "title": "Scalable Multiterminal Key Agreement via Error-Correcting Codes", "comment": "6 pages, 1 figure", "summary": "We explore connections between secret sharing and secret key agreement, which yield a simple and scalable multiterminal key agreement protocol. In our construction, we use error-correcting codes, specifically Reed-Solomon codes with threshold reconstruction, to ensure no information is leaked to an eavesdropper. We then derive novel bounds for both full-rank maximum distance separable codes and our scheme's secret key capacity, using key capacity's duality with multivariate mutual information."}
{"id": "2512.17952", "categories": ["cs.GT", "cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.17952", "abs": "https://arxiv.org/abs/2512.17952", "authors": ["Hanyu Li", "Xiaotie Deng"], "title": "Will AI Trade? A Computational Inversion of the No-Trade Theorem", "comment": "Accepted in WINE 2025", "summary": "Classic no-trade theorems attribute trade to heterogeneous beliefs. We re-examine this conclusion for AI agents, asking if trade can arise from computational limitations, under common beliefs. We model agents' bounded computational rationality within an unfolding game framework, where computational power determines the complexity of its strategy. Our central finding inverts the classic paradigm: a stable no-trade outcome (Nash equilibrium) is reached only when \"almost rational\" agents have slightly different computational power. Paradoxically, when agents possess identical power, they may fail to converge to equilibrium, resulting in persistent strategic adjustments that constitute a form of trade. This instability is exacerbated if agents can strategically under-utilize their computational resources, which eliminates any chance of equilibrium in Matching Pennies scenarios. Our results suggest that the inherent computational limitations of AI agents can lead to situations where equilibrium is not reached, creating a more lively and unpredictable trade environment than traditional models would predict."}
{"id": "2512.18122", "categories": ["cs.MM", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.18122", "abs": "https://arxiv.org/abs/2512.18122", "authors": ["Changxu Duan"], "title": "Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation", "comment": "Accepted NLDB 2025", "summary": "Converting data from machine-unreadable formats like PDFs into Markdown has the potential to enhance the accessibility of scientific research. Existing end-to-end decoder transformer models can transform screenshots of PDFs into Markdown, offering more flexibility than pipeline-based methods. Yet, decoding text token by token from scratch is inefficient, especially when dense text can be directly copied from the PDF. To address this challenge, this paper modifies Prompt Lookup Decoding (PLD) to extract candidate sequences directly from PDF files, leveraging the high n-gram overlap between PDFs and their Markdown equivalents. A new method, Copy Lookup Decoding (CLD), is introduced here to enhance PLD's candidate generation mechanism. Experiments demonstrate that CLD can accelerate the conversion process by up to 1.70$\\times$ at original quality. The codebase for this paper is open-source on GitHub (https://github.com/Fireblossom/CopyLookup)."}
{"id": "2512.18060", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18060", "abs": "https://arxiv.org/abs/2512.18060", "authors": ["Nina Mishra", "Yonatan Naamad", "Tal Wagner", "Lichen Zhang"], "title": "Graph-based Nearest Neighbors with Dynamic Updates via Random Walks", "comment": "37 pages, 23 figures", "summary": "Approximate nearest neighbor search (ANN) is a common way to retrieve relevant search results, especially now in the context of large language models and retrieval augmented generation. One of the most widely used algorithms for ANN is based on constructing a multi-layer graph over the dataset, called the Hierarchical Navigable Small World (HNSW). While this algorithm supports insertion of new data, it does not support deletion of existing data. Moreover, deletion algorithms described by prior work come at the cost of increased query latency, decreased recall, or prolonged deletion time. In this paper, we propose a new theoretical framework for graph-based ANN based on random walks. We then utilize this framework to analyze a randomized deletion approach that preserves hitting time statistics compared to the graph before deleting the point. We then turn this theoretical framework into a deterministic deletion algorithm, and show that it provides better tradeoff between query latency, recall, deletion time, and memory usage through an extensive collection of experiments."}
{"id": "2512.18283", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.18283", "abs": "https://arxiv.org/abs/2512.18283", "authors": ["Tianji Jiang", "Wenqi Li", "Jiqun Liu"], "title": "Improving Data Reusability in Interactive Information Retrieval: Insights from the Community", "comment": "Accepted by CHIIR 2025", "summary": "In this study, we conducted semi-structured interviews with 21 IIR researchers to investigate their data reuse practices. This study aims to expand upon current findings by exploring IIR researchers' information-obtaining behaviors regarding data reuse. We identified the information about shared data characteristics that IIR researchers need when evaluating data reusability, as well as the sources they typically consult to obtain this information. We consider this work to be an initial step toward revealing IIR researchers' data reuse practices and identifying what the community needs to do to promote data reuse. We hope that this study, as well as future research, will inspire more individuals to contribute to ongoing efforts aimed at designing standards, infrastructures, and policies, as well as fostering a sustainable culture of data sharing and reuse in this field."}
{"id": "2512.18332", "categories": ["cs.IT", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.18332", "abs": "https://arxiv.org/abs/2512.18332", "authors": ["Ilya Petrovanov", "Anton Sergeev"], "title": "Implementing Transport Coding in OMNeT++ for Message Delay Reduction", "comment": null, "summary": "Transport coding reduces message delay in packet-switched networks by introducing controlled redundancy at the transport layer: $k$ original packets are encoded into $n\\ge k$ coded packets, and the message is reconstructed after the first $k$ successful deliveries, effectively shifting latency from the maximum packet delay to the $k$-th order statistic. We present a concise, reproducible discrete-event implementation of transport coding in OMNeT++, including a multi-hop Kleinrock-type network, FIFO queues, exponential service and link delays, and explicit receiver-side reconstruction that records message delay and deadline violations. Using paired uncoded ($n{=}k$) and coded ($n{>}k$) configurations at the same message generation rate, we compare delay, reliability, and saturation effects across code rates and input loads. Simulation results show consistent reductions of average delay and late-delivery probability for moderate redundancy, while keeping the saturation throughput close to the uncoded baseline. The proposed model provides a transparent bridge between analytical transport-coding formulas and executable simulation for tuning redundancy in low-latency services."}
{"id": "2512.17979", "categories": ["cs.GT", "cs.AI", "cs.MA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17979", "abs": "https://arxiv.org/abs/2512.17979", "authors": ["Matthieu Mastio", "Paul Saves", "Benoit Gaudou", "Nicolas Verstaevel"], "title": "Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis", "comment": "AAMAS CC-BY 4.0 licence. Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis. Full paper. In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 10 pages", "summary": "Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets."}
{"id": "2512.18318", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.18318", "abs": "https://arxiv.org/abs/2512.18318", "authors": ["Eren Caglar", "Amirkia Rafiei Oskooei", "Mehmet Kutanoglu", "Mustafa Keles", "Mehmet S. Aktas"], "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems", "comment": "Accepted to IEEE Big Data 2025, AIDE4IoT Workshop. Copyright \\c{opyright} 2025 IEEE", "summary": "This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems."}
{"id": "2512.18141", "categories": ["cs.DS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.18141", "abs": "https://arxiv.org/abs/2512.18141", "authors": ["Robert Streit", "Vijay K. Garg"], "title": "Constrained Cuts, Flows, and Lattice-Linearity", "comment": null, "summary": "In a capacitated directed graph, it is known that the set of all min-cuts forms a distributive lattice [1], [2]. Here, we describe this lattice as a regular predicate whose forbidden elements can be advanced in constant parallel time after precomputing a max-flow, so as to obtain parallel algorithms for min-cut problems with additional constraints encoded by lattice-linear predicates [3]. Some nice algorithmic applications follow. First, we use these methods to compute the irreducibles of the sublattice of min-cuts satisfying a regular predicate. By Birkhoff's theorem [4] this gives a succinct representation of such cuts, and so we also obtain a general algorithm for enumerating this sublattice. Finally, though we prove computing min-cuts satisfying additional constraints is NP-hard in general, we use poset slicing [5], [6] for exact algorithms with constraints not necessarily encoded by lattice-linear predicates) with better complexity than exhaustive search. We also introduce $k$-transition predicates and strong advancement for improved complexity analyses of lattice-linear predicate algorithms in parallel settings, which is of independent interest."}
{"id": "2512.18384", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18384", "abs": "https://arxiv.org/abs/2512.18384", "authors": ["Boris Genin", "Alexander Gorbunov", "Dmitry Zolkin", "Igor Nekrasov"], "title": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems", "comment": "14 pages, 3 figures, 2 tables", "summary": "The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search."}
{"id": "2512.18359", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.18359", "abs": "https://arxiv.org/abs/2512.18359", "authors": ["Jun Qian", "Ross Murch", "Khaled B. Letaief"], "title": "Downlink Power Allocation for STAR-RIS-Assisted Cell-Free Massive MIMO with Multi-antenna Users", "comment": "4 figures, 6 pages", "summary": "This paper investigates the downlink power allocation of the simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-assisted cell-free massive multiple-input multiple-output (MIMO) system with multi-antenna users. We introduce downlink spectral efficiency (SE) and derive novel closed-form SE expressions using linear minimum mean squared error (MMSE) detectors. We also address the downlink power allocation via a sum SE maximization problem framed within an alternating direction method of multipliers (ADMM)-based fractional programming (FP) algorithm. Numerical results demonstrate that systems utilizing multi-antenna users significantly enhance SE, achieving at least a 20% SE increase as the number of antennas increases from one to six. Additionally, our proposed ADMM-based FP algorithm outperforms existing fractional power control approaches, yielding a more than 20% SE increase. These results highlight the necessity for adopting multi-antenna users and efficient power allocation algorithms in STAR-RIS-assisted cell-free massive MIMO systems."}
{"id": "2512.18296", "categories": ["cs.GT", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18296", "abs": "https://arxiv.org/abs/2512.18296", "authors": ["Lijun Bo", "Weiqiang Chang"], "title": "Privacy Data Pricing: A Stackelberg Game Approach", "comment": "21 pages", "summary": "Data markets are emerging as key mechanisms for trading personal and organizational data. Traditional data pricing studies -- such as query-based or arbitrage-free pricing models -- mainly emphasize price consistency and profit maximization but often neglect privacy constraints and strategic interactions. The widespread adoption of differential privacy (DP) introduces a fundamental privacy-utility trade-off: noise protects individuals' privacy but reduces data accuracy and market value. This paper develops a Stackelberg game framework for pricing DP data, where the market maker (leader) sets the price function and the data buyer (follower) selects the optimal query precision under DP constraints. We derive the equilibrium strategies for both parties under a balanced pricing function where the pricing decision variable enters linearly into the original pricing model. We obtain closed-form solutions for the optimal variance and pricing level, and determine the boundary conditions for market participation. Furthermore, we extend the analysis to Stackelberg games involving nonlinear power pricing functions. The model bridges DP and economic mechanism design, offering a unified foundation for incentive-compatible and privacy-conscious data pricing in data markets."}
{"id": "2512.19130", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2512.19130", "abs": "https://arxiv.org/abs/2512.19130", "authors": ["Junhao Xiao", "Shun Feng", "Zhiyu Wu", "Jianjun Li", "Zhiyuan Ma", "Yi Chen"], "title": "D$^{2}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection", "comment": null, "summary": "Audio-visual speaker detection aims to identify the active speaker in videos by leveraging complementary audio and visual cues. Existing methods often suffer from computational inefficiency or suboptimal performance due to joint modeling of temporal and speaker interactions. We propose D$^{2}$Stream, a decoupled dual-stream framework that separates cross-frame temporal modeling from within-frame speaker discrimination. Audio and visual features are first aligned via cross-modal attention, then fed into two lightweight streams: a Temporal Interaction Stream captures long-range temporal dependencies, while a Speaker Interaction Stream models per-frame inter-person relationships. The temporal and relational features extracted by the two streams interact via cross-attention to enrich representations. A lightweight Voice Gate module further mitigates false positives from non-speech facial movements. On AVA-ActiveSpeaker, D$^{2}$Stream achieves a new state-of-the-art at 95.6% mAP, with 80% reduction in computation compared to GNN-based models and 30% fewer parameters than attention-based alternatives, while also generalizing well on Columbia ASD. Source code is available at https://anonymous.4open.science/r/D2STREAM."}
{"id": "2512.18204", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.18204", "abs": "https://arxiv.org/abs/2512.18204", "authors": ["Haoda Li", "Jiahui Chen", "Yu Sun", "Shaoxu Song", "Haiwei Zhang", "Xiaojie Yuan"], "title": "Learning Dependency Models for Subset Repair", "comment": null, "summary": "Inconsistent values are commonly encountered in real-world applications, which can negatively impact data analysis and decision-making. While existing research primarily focuses on identifying the smallest removal set to resolve inconsistencies, recent studies have shown that multiple minimum removal sets may exist, making it difficult to make further decisions. While some approaches use the most frequent values as the guidance for the subset repair, this strategy has been criticized for its potential to inaccurately identify errors. To address these issues, we consider the dependencies between attribute values to determine a more appropriate subset repair. Our main contributions include (1) formalizing the optimal subset repair problem with attribute dependencies and analyzing its computational hardness; (2) computing the exact solution using integer linear programming; (3) developing an approximate algorithm with performance guarantees based on cliques and LP relaxation; and (4) designing a probabilistic approach with an approximation bound for efficiency. Experimental results on real-world datasets validate the effectiveness of our methods in both subset repair performance and downstream applications."}
{"id": "2512.18434", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.18434", "abs": "https://arxiv.org/abs/2512.18434", "authors": ["Federica Valeau", "Odysseas Boufalis", "Polytimi Gkotsi", "Joshua Rosenthal", "David Vos"], "title": "Efficient Optimization of Hierarchical Identifiers for Generative Recommendation", "comment": "Accepted at ECIR 2026 Reproducibility Track (to appear)", "summary": "SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater."}
{"id": "2512.18457", "categories": ["cs.IT", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.18457", "abs": "https://arxiv.org/abs/2512.18457", "authors": ["Nail Akar", "Ismail Cosandal", "Sennur Ulukus"], "title": "Age of Information with Age-Dependent Server Selection", "comment": "11 pages, 6 figures, preliminary version presented at Asilomar Conference, 2025", "summary": "In this paper, we consider a single-source multi-server generate-at-will discrete-time non-preemptive status update system where update packets are transmitted using {\\em only one} of the available servers, according to a server selection policy. In particular, when a transmission is complete, the update system makes a threshold-based decision on whether to wait or transmit, and if latter, which server to use for transmissions, on the basis of the instantaneous value of the age of information (AoI) process. In our setting, servers have general heterogeneous discrete phase-type (DPH) distributed service times, and also heterogeneous transmission costs. The goal is to find an age-dependent multi-threshold policy that minimizes the AoI cost with a constraint on transmission costs, the former cost defined in terms of the time average of an arbitrary function of AoI. For this purpose, we propose a novel tool called \\emph{multi-regime absorbing Markov chain} (MR-AMC) in discrete time. Using the MR-AMC framework, we exactly obtain the distribution of AoI, and subsequently the costs associated with AoI and transmissions. With the exact analysis in hand, optimum thresholds can be obtained in the case of a few servers, by exhaustive search. We validate the proposed analytical model, and also demonstrate the benefits of age-dependent server selection, with numerical examples."}
{"id": "2512.18444", "categories": ["cs.GT", "cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18444", "abs": "https://arxiv.org/abs/2512.18444", "authors": ["Grammateia Kotsialou"], "title": "Snowveil: A Framework for Decentralised Preference Discovery", "comment": null, "summary": "Aggregating subjective preferences of a large group is a fundamental challenge in computational social choice, traditionally reliant on central authorities. To address the limitations of this model, this paper introduces Decentralised Preference Discovery (DPD), the problem of determining the collective will of an electorate under constraints of censorship resistance, partial information, and asynchronous communication. We propose Snowveil, a novel framework for this task. Snowveil uses an iterative, gossip-based protocol where voters repeatedly sample the preferences of a small, random subset of the electorate to progressively converge on a collective outcome. We demonstrate the framework's modularity by designing the Constrained Hybrid Borda (CHB), a novel aggregation rule engineered to balance broad consensus with strong plurality support, and provide a rigorous axiomatic analysis of its properties. By applying a potential function and submartingale theory, we develop a multi-level analytical method to show that the system almost surely converges to a stable, single-winner in finite time, a process that can then be iterated to construct a set of winning candidates for multi-winner scenarios. This technique is largely agnostic to the specific aggregation rule, requiring only that it satisfies core social choice axioms like Positive Responsiveness, thus offering a formal toolkit for a wider class of DPD protocols. Furthermore, we present a comprehensive empirical analysis through extensive simulation, validating Snowveil's $O(n)$ scalability. Overall, this work advances the understanding of how a stable consensus can emerge from subjective, complex, and diverse preferences in decentralised systems for large electorates."}
{"id": "2512.18335", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.18335", "abs": "https://arxiv.org/abs/2512.18335", "authors": ["Ishaq Aden-Ali", "Hakan Ferhatosmanoglu", "Alexander Greaves-Tunnell", "Nina Mishra", "Tal Wagner"], "title": "Quantization for Vector Search under Streaming Updates", "comment": null, "summary": "Large-scale vector databases for approximate nearest neighbor (ANN) search typically store a quantized dataset in main memory for fast access, and full precision data on remote disk. State-of-the-art ANN quantization methods are highly data-dependent, rendering them unable to handle point insertions and deletions. This either leads to degraded search quality over time, or forces costly global rebuilds of the entire search index. In this paper, we formally study data-dependent quantization under streaming dataset updates. We formulate a computation model of limited remote disk access and define a dynamic consistency property that guarantees freshness under updates. We use it to obtain the following results: Theoretically, we prove that static data-dependent quantization can be made dynamic with bounded disk I/O per update while retaining formal accuracy guarantees for ANN search. Algorithmically, we develop a practical data-dependent quantization method which is provably dynamically consistent, adapting itself to the dataset as it evolves over time. Our experiments show that the method outperforms baselines in large-scale nearest neighbor search quantization under streaming updates."}
{"id": "2512.18683", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.18683", "abs": "https://arxiv.org/abs/2512.18683", "authors": ["Sebastian Sun"], "title": "CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift", "comment": null, "summary": "Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines."}
{"id": "2512.18529", "categories": ["cs.IT", "cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.18529", "abs": "https://arxiv.org/abs/2512.18529", "authors": ["Mohamed Seif", "Atsutse Kludze", "Yasaman Ghasempour", "H. Vincent Poor", "Doru Calin", "Andrea J. Goldsmith"], "title": "Protecting Human Activity Signatures in Compressed IEEE 802.11 CSI Feedback", "comment": null, "summary": "Explicit channel state information (CSI) feedback in IEEE~802.11 conveys \\emph{transmit beamforming directions} by reporting quantized Givens rotation and phase angles that parametrize the right-singular subspace of the channel matrix. Because these angles encode fine-grained spatial signatures of the propagation environment, recent work have shown that plaintext CSI feedback can inadvertently reveal user activity, identity, and location to passive eavesdroppers. In this work, we introduce a standards-compatible \\emph{differentially private (DP) quantization mechanism} that replaces deterministic angular quantization with an $\\varepsilon$-DP stochastic quantizer applied directly to the Givens parameters of the transmit beamforming matrix. The mechanism preserves the 802.11 feedback structure, admits closed-form sensitivity bounds for the angular representation, and enables principled privacy calibration. Numerical simulations demonstrate strong privacy guarantees with minimal degradation in beamforming performance."}
{"id": "2512.18620", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.18620", "abs": "https://arxiv.org/abs/2512.18620", "authors": ["Hau Chan", "Jianan Lin", "Chenhao Wang"], "title": "Obnoxious Facility Location Problems: Strategyproof Mechanisms Optimizing $L_p$-Aggregated Utilities and Costs", "comment": "To appear in AAMAS 2026", "summary": "We study the problem of locating a single obnoxious facility on the normalized line segment $[0,1]$ with strategic agents from a mechanism design perspective. Each agent has a preference for the undesirable location of the facility and would prefer the facility to be far away from their location. We consider the utility of the agent, defined as the distance between the agent's location and the facility location, and the cost of each agent, equal to one minus the utility. Given this standard setting of obnoxious facility location problems, our goal is to design (group) strategyproof mechanisms to elicit agent locations truthfully and determine facility location approximately optimizing the $L_p$-aggregated utility and cost objectives, which generalizes the $L_p$-norm ($p\\ge 1$) of the agents' utilities and agents' costs to any $p \\in [-\\infty, \\infty]$, respectively. We establish upper and lower bounds on the approximation ratios of deterministic and randomized (group) strategyproof mechanisms for maximizing the $L_p$-aggregated utilities or minimizing the $L_p$-aggregated costs across the range of \\(p\\)-values. While there are gaps between upper and lower bounds for randomized mechanisms, our bounds for deterministic mechanisms are tight."}
{"id": "2512.18416", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.18416", "abs": "https://arxiv.org/abs/2512.18416", "authors": ["Jiangqi Dai", "Mohsen Ghaffari", "Julian Portmann"], "title": "Constant Approximation of Arboricity in Near-Optimal Sublinear Time", "comment": "FOCS 2025", "summary": "We present a randomized algorithm that computes a constant approximation of a graph's arboricity, using $\\tilde{O}(n/λ)$ queries to adjacency lists and in the same time bound. Here, $n$ and $λ$ denote the number of nodes and the graph's arboricity, respectively. The $\\tilde{O}(n/λ)$ query complexity of our algorithm is nearly optimal. Our constant approximation settles a question of Eden, Mossel, and Ron [SODA'22], who achieved an $O(\\log^2 n)$ approximation with the same query and time complexity and asked whether a better approximation can be achieved using near-optimal query complexity.\n  A key technical challenge in the problem is due to recursive algorithms based on probabilistic samplings, each with a non-negligible error probability. In our case, many of the recursions invoked could have bad probabilistic samples and result in high query complexities. The particular difficulty is that those bad recursions are not easy or cheap to detect and discard. Our approach runs multiple recursions in parallel, to attenuate the error probability, using a careful \\textit{scheduling mechanism} that manages the speed at which each of them progresses and makes our overall query complexity competitive with the single good recursion. We find this usage of parallelism and scheduling in a sublinear algorithm remarkable, and we are hopeful that similar ideas may find applications in a wider range of sublinear algorithms that rely on probabilistic recursions."}
{"id": "2512.18996", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18996", "abs": "https://arxiv.org/abs/2512.18996", "authors": ["Chong Liu", "Ming Zhang", "Fei Li", "Hao Zhou", "Xiaoshuang Chen", "Ye Yuan"], "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation", "comment": null, "summary": "Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks"}
{"id": "2512.18535", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.18535", "abs": "https://arxiv.org/abs/2512.18535", "authors": ["Sepehr Jahangiri", "H. Ali Talebi"], "title": "Integrated Control and Communication in LQG Systems", "comment": null, "summary": "In this paper, we study the Integrated Communication and Control (ICAC) problem. Specifically, we investigate how messages can be transmitted from the controller/encoder to the observer/decoder through the control signal in Multiple-Input Multiple-Output (MIMO) vector-state Linear Quadratic Gaussian (LQG) systems under control constraints. We provide a computable capacity expression using semidefinite programming. We further show that it is possible to transmit data at a nonzero rate over an LQG system while maintaining the same optimal control cost as in the case where no information message are transmitted. Finally, we discuss how this framework generalizes communication over MIMO Gaussian channels with feedback, both with and without InterSymbol Interference (ISI)."}
{"id": "2512.18858", "categories": ["cs.GT", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18858", "abs": "https://arxiv.org/abs/2512.18858", "authors": ["Avirup Chakraborty", "Shirsa Maitra", "Tathagata Banerjee", "Diganta Mukherjee", "Tridib Mukherjee"], "title": "Adapting Skill Ratings to Luck-Based Hidden-Information Games", "comment": "13 pages, 4 figures", "summary": "Rating systems play a crucial role in evaluating player skill across competitive environments. The Elo rating system, originally designed for deterministic and information-complete games such as chess, has been widely adopted and modified in various domains. However, the traditional Elo rating system only considers game outcomes for rating calculation and assumes uniform initial states across players. This raises important methodological challenges in skill modelling for popular partially randomized incomplete-information games such as Rummy. In this paper, we examine the limitations of conventional Elo ratings when applied to luck-driven environments and propose a modified Elo framework specifically tailored for Rummy. Our approach incorporates score-based performance metrics and explicitly models the influence of initial hand quality to disentangle skill from luck. Through extensive simulations involving 270,000 games across six strategies of varying sophistication, we demonstrate that our proposed system achieves stable convergence, superior discriminative power, and enhanced predictive accuracy compared to traditional Elo formulations. The framework maintains computational simplicity while effectively capturing the interplay of skill, strategy, and randomness, with broad applicability to other stochastic competitive environments."}
{"id": "2512.19493", "categories": ["cs.DS", "cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.19493", "abs": "https://arxiv.org/abs/2512.19493", "authors": ["Martin Hoefer", "Lennart Kauther", "Philipp Pabst", "Britta Peis", "Khai Van Tran"], "title": "Fare Zone Assignment", "comment": null, "summary": "Tariff setting in public transportation networks is an important challenge. A popular approach is to partition the network into fare zones (\"zoning\") and fix journey prices depending on the number of traversed zones (\"pricing\"). In this paper, we focus on finding revenue-optimal solutions to the zoning problem for a given concave pricing function. We consider tree networks with $n$ vertices, since trees already pose non-trivial algorithmic challenges. Our main results are efficient algorithms that yield a simple $\\mathcal{O}(\\log n)$-approximation as well as a more involved $\\mathcal{O}(\\log n/\\log \\log n)$-approximation. We show how to solve the problem exactly on rooted instances, in which all demand arises at the same source. For paths, we prove strong NP-hardness and outline a PTAS. Moreover, we show that computing an optimal solution is in FPT or XP for several natural problem parameters."}
{"id": "2512.19360", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.19360", "abs": "https://arxiv.org/abs/2512.19360", "authors": ["Markus Ekvall", "Ludvig Bergenstråhle", "Patrick Truong", "Ben Murrell", "Joakim Lundeberg"], "title": "Generative vector search to improve pathology foundation models across multimodal vision-language tasks", "comment": "13 pages main (54 total), 2 main figures (9 total)", "summary": "Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions."}
{"id": "2512.18600", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.18600", "abs": "https://arxiv.org/abs/2512.18600", "authors": ["Juha Park", "Seokho Kim", "Wonjae Shin", "H. Vincent Poor"], "title": "Embracing Beam-Squint Effects for Wideband LEO Satellite Communications: A 3D Rainbow Beamforming Approach", "comment": "39 pages, 10 figures", "summary": "Low Earth Orbit (LEO) satellite communications (SATCOM) offers high-throughput, low-latency global connectivity to a very large number of users. To accommodate this demand with limited hardware resources, beam hopping (BH) has emerged as a prominent approach in LEO SATCOM. However, its time-domain switching mechanism confines coverage to a small fraction of the service area during each time slot, exacerbating uplink throughput bottlenecks and latency issues as the user density increases. Meanwhile, wideband systems experience the beam-squint effect, where analog beamforming (BF) directions vary with subcarrier frequencies, potentially causing misalignment at certain frequencies, thereby hindering the performance of wideband SATCOM. In this paper, we aim to shift the paradigm in wideband LEO SATCOM from beam-squint as an impairment to beam-squint as an asset. Specifically, we put forth 3D rainbow BF employing a joint phase-time array (JPTA) antenna with true time delay (TTD) to intentionally widen the beam-squint angle, steering frequency-dependent beams toward distributed directions. This novel approach enables the satellite to serve its entire coverage area in a single time slot. By doing so, the satellite simultaneously receives uplink signals from a massive number of users, significantly boosting throughput and reducing latency. To realize 3D rainbow BF, we formulate a JPTA beamformer optimization problem and address the non-convex nature of the optimization problem through a novel joint alternating and decomposition-based optimization framework. Through numerical evaluations incorporating realistic 3D LEO SATCOM geometry, our numerical results demonstrate that the proposed rainbow BF-empowered LEO SATCOM achieves up to 2.8-fold increase in uplink throughput compared to conventional BH systems. These results mark a significant breakthrough for 6G wideband LEO SATCOM."}
{"id": "2512.18989", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.18989", "abs": "https://arxiv.org/abs/2512.18989", "authors": ["Youzhi Zhang"], "title": "Considering the Difference in Utility Functions of Team Players in Adversarial Team Games", "comment": null, "summary": "The United Nations' 2030 Agenda for Sustainable Development requires that all countries collaborate to fight adversarial factors to achieve peace and prosperity for humans and the planet. This scenario can be formulated as an adversarial team game in AI literature, where a team of players play against an adversary. However, previous solution concepts for this game assume that team players have the same utility functions, which cannot cover the real-world case that countries do not always have the same utility function. This paper argues that studying adversarial team games should not ignore the difference in utility functions of team players. We show that ignoring the difference in utility functions of team players could cause the computed equilibrium to be unstable. To show the benefit of considering the difference in utility functions of team players, we introduce a novel solution concept called Co-opetition Equilibrium (CoE) for the adversarial team game. In this game, team players with different utility functions (i.e., cooperation between team players) correlate their actions to play against the adversary (i.e., competition between the team and the adversary). We further introduce the team-maximizing CoE, which is a CoE but maximizes the team's utility among all CoEs. Both equilibria can overcome the issue caused by ignoring the difference in utility functions of team players. We further show the opportunities for theoretical and algorithmic contributions based on our position of considering the difference in utility functions of team players."}
{"id": "2512.19521", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.19521", "abs": "https://arxiv.org/abs/2512.19521", "authors": ["Santhoshini Velusamy"], "title": "Near-optimal streaming approximation for Max-DICUT in sublinear space using two passes", "comment": "27 pages", "summary": "The Max-DICUT problem has gained a lot of attention in the streaming setting in recent years, and has so far served as a canonical problem for designing algorithms for general constraint satisfaction problems (CSPs) in this setting. A seminal result of Kapralov and Krachun [STOC 2019] shows that it is impossible to beat $1/2$-approximation for Max-DICUT in sublinear space in the single-pass streaming setting, even on bounded-degree graphs. In a recent work, Saxena, Singer, Sudan, and Velusamy [SODA 2025] prove that the above lower bound is tight by giving a single-pass algorithm for bounded-degree graphs that achieves $(1/2-ε)$-approximation in sublinear space, for every constant $ε>0$. For arbitrary graphs of unbounded degree, they give an $O(1/ε)$-pass $O(\\log n)$ space algorithm. Their work left open the question of obtaining $1/2$-approximation for arbitrary graphs in the single-pass setting in sublinear space. We make progress towards this question and give a two-pass algorithm that achieves $(1/2-ε)$-approximation in sublinear space, for every constant $ε>0$."}
{"id": "2512.18698", "categories": ["cs.IT", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.18698", "abs": "https://arxiv.org/abs/2512.18698", "authors": ["Mehrdad Salimnejad", "Marios Kountouris", "Nikolaos Pappas"], "title": "Real-Time Remote Monitoring of Correlated Markovian Sources", "comment": null, "summary": "We investigate real-time tracking of two correlated stochastic processes over a shared wireless channel. The joint evolution of the processes is modeled as a two-dimensional discrete-time Markov chain. Each process is observed by a dedicated sampler and independently reconstructed at a remote monitor according to a task-specific objective. Although both processes originate from a common underlying phenomenon (e.g., distinct features of the same source), each monitor is interested only in its corresponding feature. A reconstruction error is incurred when the true and reconstructed states mismatch at one or both monitors. To address this problem, we propose an error-aware joint sampling and transmission policy, under which each sampler probabilistically generates samples only when the current process state differs from the most recently reconstructed state at its corresponding monitor. We adopt the time-averaged reconstruction error as the primary performance metric and benchmark the proposed policy against state-of-the-art joint sampling and transmission schemes. For each policy, we derive closed-form expressions for the resulting time-averaged reconstruction error. We further formulate and solve an optimization problem that minimizes the time-averaged reconstruction error subject to an average sampling cost constraint. Analytical and numerical results demonstrate that the proposed error-aware policy achieves the minimum time-averaged reconstruction error among the considered schemes while efficiently utilizing the sampling budget. The performance gains are particularly pronounced in regimes with strong inter-process correlation and stringent tracking requirements, where frequent sampling by both samplers is necessary."}
{"id": "2512.19113", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19113", "abs": "https://arxiv.org/abs/2512.19113", "authors": ["Luca Pennella", "Pietro Saggese", "Fabio Pinelli", "Letterio Galletta"], "title": "A Unified Framework and Comparative Study of Decentralized Finance Derivatives Protocols", "comment": "Submitted; under review", "summary": "Decentralized Finance (DeFi) applications introduce novel financial instruments replicating and extending traditional ones through blockchain-based smart contracts. Among these, derivatives protocols enable the decentralized trading of cryptoassets that are the counterpart of derivative products available in traditional finance. Despite their growing significance, DeFi derivatives protocols remain relatively understudied compared to other DeFi instruments, such as lending protocols and decentralized exchanges with automated market makers. This paper systematically analyzes DeFi derivatives protocols - categorized into perpetual, options, and synthetics - in the field, highlighting similarities, differences, dynamics, and actors. As a result of our study, we provide a formal characterization of decentralized derivative products and introduce a unifying conceptual framework that captures the design principles and core architecture of such protocols. We complement our theoretical analysis with numerical simulations: we evaluate protocol dynamics under various economic conditions, including changes in underlying asset prices, volatility, protocol-specific fees, leverage, and their impact on liquidation and profitability."}
{"id": "2512.19654", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19654", "abs": "https://arxiv.org/abs/2512.19654", "authors": ["Diptarka Chakraborty", "Hendrik Fichtenberger", "Bernhard Haeupler", "Silvio Lattanzi", "Ashkan Norouzi-Fard", "Ola Svensson"], "title": "Clustering with Label Consistency", "comment": null, "summary": "Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems."}
{"id": "2512.19002", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.19002", "abs": "https://arxiv.org/abs/2512.19002", "authors": ["Mokshay Madiman", "James Melbourne", "Cyril Roberto"], "title": "A Quantitative Entropy Power Inequality for Dependent Random Vectors", "comment": null, "summary": "The entropy power inequality for independent random vectors is a foundational result of information theory, with deep connections to probability and geometric functional analysis. Several extensions of the entropy power inequality have been developed for settings with dependence, including by Takano, Johnson, and Rioul. We extend these works by developing a quantitative version of the entropy power inequality for dependent random vectors. A notable consequence is that an entropy power inequality stated using conditional entropies holds for random vectors whose joint density is log-supermodular."}
{"id": "2512.19292", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19292", "abs": "https://arxiv.org/abs/2512.19292", "authors": ["Ruijun Ma", "Xin Chen", "Xiaoqing Wen", "Hui Xu", "Shengnan Ye", "Chuanjian Zhang", "Senling Wang"], "title": "LOCO: A Low-Cost SNU-Self-Resilient Latch Using an Output-Split C-Element", "comment": null, "summary": "As the CMOS technology enters nanometer scales, integrated circuits (ICs) become increasingly sensitive to radiation-induced soft errors, which can corrupt the state of storage elements and cause severe reliability issues. Many hardened designs have been proposed to mitigate soft errors by using filtering elements. However, existing filtering elements only protect their inputs against soft errors and leave their outputs unprotected. Therefore, additional filtering elements must be added to protect outputs, resulting in extra overhead. In this paper, we first propose a novel Output-Split C-element (OSC) to protect both its input and output nodes, and then a novel LOw-COst single-node-upset (SNU) self-resilient latch (LOCO) to use OSCs to achieve both soft error resilience and low overhead. The usage of OSCs effectively reduce the short-circuit current of the LOCO latch during switching activities. Furthermore, the usage of clock gating and high-speed path reduces power consumption and delay, respectively. Compared with state-of-the-art SNU-resilient hardened designs, the LOCO latch achieves 19% fewer transistors, 63.58% lower power, 74% less delay, and 92% lower power-delay-product (PDP) on average. In addition, the LOCO latch exhibits better stability under variations in PVT (Process, Voltage, and Temperature)."}
{"id": "2512.19067", "categories": ["cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19067", "abs": "https://arxiv.org/abs/2512.19067", "authors": ["George Vershinin", "Asaf Cohen", "Omer Gurewitz"], "title": "On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation", "comment": "9 pages, 7 figures", "summary": "We study a variant of cost-aware sequential hypothesis testing in which a single active Decision Maker (DM) selects actions with positive, random costs to identify the true hypothesis under an average error constraint, while minimizing the expected total cost. The DM may abort an in-progress action, yielding no sample, by truncating its realized cost at a smaller, tunable deterministic limit, which we term a per-action deadline. We analyze how this cancellation option can be exploited under two cost-revelation models: ex-post, where the cost is revealed only after the sample is obtained, and ex-ante, where the cost accrues before sample acquisition.\n  In the ex-post model, per-action deadlines do not affect the expected total cost, and the cost-error tradeoffs coincide with the baseline obtained by replacing deterministic costs with cost means. In the ex-ante model, we show how per-action deadlines inflate the expected number of times actions are applied, and that the resulting expected total cost can be reduced to the constant-cost setting by introducing an effective per-action cost. We characterize when deadlines are beneficial and study several families in detail."}
{"id": "2512.19328", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19328", "abs": "https://arxiv.org/abs/2512.19328", "authors": ["Bingqing Liu", "David Watling", "Joseph Y. J. Chow"], "title": "Stochastic assignment games for Mobility-as-a-Service markets", "comment": null, "summary": "We study the stochastic assignment game and extend it to model multimodal mobility markets with a regulator or a Mobility-as-a-Service (MaaS) platform. We start by presenting general forms of one-to-one and many-to-many stochastic assignment games. Optimality conditions are discussed. The core of stochastic assignment games is defined, with expected payoffs of sellers and buyers in stochastic assignment games as payoffs from a hypothetical \"ideal matching\" that represent sellers' and buyers' expectations under imperfect information. To apply stochastic assignment games to the urban mobility markets, we extend the general stochastic many-to-many assignment game into a stochastic Stackelberg game to model MaaS systems, where the platform is the leader, and users and operators are the followers. The platform sets fares to maximize revenue. Users and operator react to the fare settings to form a stochastic many-to-many assignment game considering both fixed-route services and Mobility-on-Demand (MOD). The Stackelberg game is formulated as a bilevel problem. The lower level is the stochastic many-to-many assignment game between users and operators, shown to yield a coalitional logit model. The upper-level problem is a fare adjustment problem maximizing revenue. An iterative balancing algorithm is proposed to solve the lower-level problem exactly. The bilevel problem is solved through an iterative fare adjusting heuristic, whose solution is shown to be equivalent to the bilevel problem with an additional condition when it converges. Two case studies are conducted. The model can be applied to design MaaS fares maximizing income of the platform while anticipating the selfish behavior and heterogeneity of users and operators. Public agencies can also use the model to manage multimodal transportation systems."}
{"id": "2512.19094", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.19094", "abs": "https://arxiv.org/abs/2512.19094", "authors": ["Mengqi Guo", "Ji Zhou", "Haide Wang", "Changyuan Yu", "Xiangjun Xin", "Liangchuan Li"], "title": "Low-Latency and Low-Complexity MLSE for Short-Reach Optical Interconnects", "comment": null, "summary": "To meet the high-speed, low-latency, and low-complexity demand for optical interconnects, simplified layered 2-step maximum likelihood sequence estimation (L2S-MLSE) is proposed in this paper. Simplified L2S-MLSE combines computational simplification and reduced state in L2S-MLSE. L2S-MLSE with a parallel sliding block architecture reduces latency from linear order to logarithmic order. Computational simplification reduces the number of multipliers from exponential order to linear order. Incorporating the reduced state with computational simplification further decreases the number of adders and comparators. The simplified L2S-MLSE is evaluated in a 112-Gbit/s PAM4 transmission over 2-km standard single-mode fiber. Experimental results show that the simplified L2S-MLSE significantly outperforms the FFE-only case in bit error ratio (BER) performance. Compared with simplified 1-step MLSE, the latency of simplified L2S-MLSE is reduced from 34 delay units in linear order to 7 delay units in logarithmic order. The simplified scheme in L2S-MLSE reduces the number of variable multipliers from 512 in exponential order to 33 in linear order without BER performance deterioration, while reducing the number of adders and comparators to 37.2% and 8.4%, respectively, with nearly identical BER performance."}
{"id": "2512.19388", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19388", "abs": "https://arxiv.org/abs/2512.19388", "authors": ["Matteo Castiglioni", "Junjie Chen", "Yingkai Li"], "title": "Fair Team Contracts", "comment": null, "summary": "A principal selects a team of agents for collaborating on a joint project. The principal aims to design a revenue-optimal contract that incentivize the team of agents to exert costly effort while satisfying fairness constraints. We show that the optimal fair contract ensures that there is a minimum share, and every agent receives a linear contract weakly higher than the minimum share that is sufficient to incentivize them to exert costly effort. We utilize this structure to design an FPTAS for additive success functions and a constant approximation algorithm for submodular success functions. Moreover, we show that adopting optimal fair contracts can lead to a 25% revenue increase compared to the optimal non-discriminatory contracts even for additive success functions."}
{"id": "2512.19306", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.19306", "abs": "https://arxiv.org/abs/2512.19306", "authors": ["Shakir Ali", "Atif Ahmad Khan", "Abhishek Kesarwani"], "title": "On the construction of Cauchy MDS matrices over Galois rings via nilpotent elements and Frobenius maps", "comment": null, "summary": "Let $s,m$ be the positive integers and $p$ be any prime number. Next, let $GR(p^s,p^{sm})$ be a Galois ring of characteristic $p^s$ and cardinality $p^{sm}$. In the present paper, we explore the construction of Cauchy MDS matrices over Galois rings. Moreover, we introduce a new approach that considers nilpotent elements and Teichmüller set of Galois ring $GR(p^s,p^{sm})$ to reduce the number of entries in these matrices. Furthermore, we construct $p^{(s-1)m}(p^m-1)$ distinct functions with the help of Frobenius automorphisms. These functions preserve MDS property of matrices. Finally, we prove some results using automorphisms and isomorphisms of the Galois rings that can be used to generate new Cauchy MDS matrices."}
{"id": "2512.19405", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.19405", "abs": "https://arxiv.org/abs/2512.19405", "authors": ["Jussi Keppo", "Yingkai Li"], "title": "Three Tiers and Thresholds: Incentives in Private Market Investing", "comment": null, "summary": "This paper studies optimal contract design in private market investing, focusing on internal decision making in venture capital and private equity firms. A principal relies on an agent who privately exerts costly due diligence effort and then recommends whether to invest. Outcomes are observable ex post even when an opportunity is declined, allowing compensation to reward both successful investments and prudent decisions to pass. We characterize profit maximizing contracts that induce information acquisition and truthful reporting. We show that three tier contracts are sufficient, with payments contingent on the agent's recommendation and the realized return. In symmetric environments satisfying the monotone likelihood ratio property, the optimal contract further simplifies to a threshold contract that pays only when the recommendation is aligned with an extreme realized return. These results provide guidance for performance based compensation that promotes diligent screening while limiting excessive risk taking."}
{"id": "2512.19334", "categories": ["cs.IT", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.19334", "abs": "https://arxiv.org/abs/2512.19334", "authors": ["Haohua Chen", "Songbin Liu", "Junjie Ma"], "title": "Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models", "comment": null, "summary": "We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods."}
{"id": "2512.19493", "categories": ["cs.DS", "cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.19493", "abs": "https://arxiv.org/abs/2512.19493", "authors": ["Martin Hoefer", "Lennart Kauther", "Philipp Pabst", "Britta Peis", "Khai Van Tran"], "title": "Fare Zone Assignment", "comment": null, "summary": "Tariff setting in public transportation networks is an important challenge. A popular approach is to partition the network into fare zones (\"zoning\") and fix journey prices depending on the number of traversed zones (\"pricing\"). In this paper, we focus on finding revenue-optimal solutions to the zoning problem for a given concave pricing function. We consider tree networks with $n$ vertices, since trees already pose non-trivial algorithmic challenges. Our main results are efficient algorithms that yield a simple $\\mathcal{O}(\\log n)$-approximation as well as a more involved $\\mathcal{O}(\\log n/\\log \\log n)$-approximation. We show how to solve the problem exactly on rooted instances, in which all demand arises at the same source. For paths, we prove strong NP-hardness and outline a PTAS. Moreover, we show that computing an optimal solution is in FPT or XP for several natural problem parameters."}
{"id": "2512.19339", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.19339", "abs": "https://arxiv.org/abs/2512.19339", "authors": ["Rashid Iqbal", "Ahmed Zoha", "Salama Ikki", "Muhammad Ali Imran", "Hanaa Abumarshoud"], "title": "Enhancing PLS of Indoor IRS-VLC Systems for Colluding and Non-Colluding Eavesdroppers", "comment": null, "summary": "Most intelligent reflecting surface (IRS)-aided indoor visible light communication (VLC) studies ignore the time delays introduced by reflected paths, even though these delays are inherent in practical wideband systems. In this work, we adopt a realistic assumption of IRS-induced time delay for physical layer security (PLS) enhancement. We consider an indoor VLC system where an IRS is used to shape the channel so that the reflected signals add constructively at the legitimate user and create intersymbol interference at eavesdroppers located inside the coverage area. The resulting secrecy capacity maximisation over the IRS element allocation is formulated as a complex combinatorial optimisation problem and is solved using deep reinforcement learning with proximal policy optimisation (PPO). The approach is evaluated for both colluding eavesdroppers, which combine their received signals, and non-colluding eavesdroppers, which act independently. Simulation results are shown for various simulation setups, which demonstrate significant secrecy capacity gains. In a worst-case scenario, where the eavesdroppers have stronger channels than the legitimate user, the proposed PPO-based IRS allocation improves secrecy capacity by 107\\% and 235\\% in the colluding and non-colluding cases, respectively, compared with allocating all IRS elements to the legitimate user. These results demonstrate that time-delay-based IRS control can provide a strong secrecy advantage in practical indoor VLC scenarios."}
{"id": "2512.19468", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19468", "abs": "https://arxiv.org/abs/2512.19468", "authors": ["Mert Ozates", "Mohammad Kazemi", "Gianluigi Liva", "Deniz Gündüz"], "title": "Fully Asynchronous Unsourced Random Access over Fading Channels", "comment": null, "summary": "We examine unsourced random access in a fully asynchronous setup, where active users transmit their data without restriction on the start time over a fading channel. In the proposed scheme, the transmitted signal consists of a pilot sequence and a polar codeword, with the polar codeword distributed across the data part of the packet in an on-off pattern. The receiver uses a double sliding-window decoder, where the inner window employs iterative decoding with joint timing and pilot detection, channel estimation, single-user decoding, and successive interference cancellation to recover the message bits, while the outer window enhances interference cancellation. The numerical results indicate that the proposed scheme exhibits only a slight performance loss compared to the synchronous benchmark while being more applicable in practice."}
