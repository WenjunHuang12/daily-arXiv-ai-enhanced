<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 9]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Functional Information Decomposition: A First-Principles Approach to Analyzing Functional Relationships](https://arxiv.org/abs/2509.18522)
*Clifford Bohm,Vincent R. Ragusa,Arend Hintze,Christoph Adami*

Main category: cs.IT

TL;DR: 本文提出了功能信息分解（FID）方法，用于解决多变量系统中信息分解的挑战，通过基于功能关系而非统计相关性来精确量化独立和协同贡献。


<details>
  <summary>Details</summary>
Motivation: 信息理论在多变量系统中的应用受限于只能处理两个变量间的互信息，无法分解多个变量对目标的集体信息贡献。

Method: 引入功能信息分解（FID）方法，通过操作完整的功能关系而非统计相关性，实现对独立和协同贡献的精确量化。

Result: FID方法能够清晰分解多个输入变量对输出变量的互信息，区分出各变量的独立贡献和它们之间的协同作用。

Conclusion: 功能信息分解为多变量信息理论分析提供了新工具，有望在神经科学、遗传学、物理学和机器学习等领域产生重要影响。

Abstract: Information theory, originating from Shannon's work on communication systems,
has become a fundamental tool across neuroscience, genetics, physics, and
machine learning. However, the application of information theory is often
limited to the simplest case: mutual information between two variables. A
central challenge in extending information theory to multivariate systems is
decomposition: understanding how the information that multiple variables
collectively provide about a target can be broken down into the distinct
contributions that are assignable to individual variables or their
interactions. To restate the problem clearly, what is sought after is a
decomposition of the mutual information between a set of inputs (or parts) and
an output (or whole). In this work, we introduce Functional Information
Decomposition (FID) a new approach to information decomposition that differs
from prior methods by operating on complete functional relationships rather
than statistical correlations, enabling precise quantification of independent
and synergistic contributions.

</details>


### [2] [Hybrid Neural/Traditional OFDM Receiver with Learnable Decider](https://arxiv.org/abs/2509.18574)
*Mohanad Obeed,Ming Jian*

Main category: cs.IT

TL;DR: 提出一种混合接收器架构，结合传统和神经网络接收器的优势，通过判别器网络动态选择最优接收器，解决DL接收器泛化能力差和难以跟踪快速信道变化的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习接收器在无线OFDM系统中性能优越，但面临对未知信道条件泛化能力差和难以有效跟踪快速信道波动的挑战。

Method: 设计混合接收器架构，训练判别器神经网络根据接收的OFDM块特征动态选择传统或DL接收器，使用包含异常信道场景的标记导频信号进行训练。

Result: 混合接收器实现了稳健的性能，有效克服了独立DL方法的泛化问题。

Conclusion: 所提出的混合接收器架构成功整合了传统和DL接收器的优势，在保持高性能的同时解决了DL方法的泛化限制。

Abstract: Deep learning (DL) methods have emerged as promising solutions for enhancing
receiver performance in wireless orthogonal frequency-division multiplexing
(OFDM) systems, offering significant improvements over traditional estimation
and detection techniques. However, DL-based receivers often face challenges
such as poor generalization to unseen channel conditions and difficulty in
effectively tracking rapid channel fluctuations. To address these limitations,
this paper proposes a hybrid receiver architecture that integrates the
strengths of both traditional and neural receivers. The core innovation is a
discriminator neural network trained to dynamically select the optimal receiver
whether it is the traditional or DL-based receiver according on the received
OFDM block characteristics. This discriminator is trained using labeled pilot
signals that encode the comparative performance of both receivers. By including
anomalous channel scenarios in training, the proposed hybrid receiver achieves
robust performance, effectively overcoming the generalization issues inherent
in standalone DL approaches.

</details>


### [3] [Ruled surfaces over finite fields, and some codes over them](https://arxiv.org/abs/2509.18698)
*Régis Blache,Emmanuel Hallouin*

Main category: cs.IT

TL;DR: 本文研究有限域上的直纹曲面及其不变量，并构建渐近良好的评估码族，其参数优于对应乘积码。


<details>
  <summary>Details</summary>
Motivation: 研究有限域上直纹曲面的几何性质，并利用这些曲面构造性能更优的编码方案。

Method: 第一部分引入直纹曲面的不变量并描述具体构造；第二部分在曲面上构建评估码，分析参数并构造渐近良好码族。

Result: 成功构造了渐近参数优于对应乘积码的评估码族，并研究了这些码的局部性质。

Conclusion: 有限域上直纹曲面为构造高性能编码提供了有效工具，所构建的评估码在渐近性能上具有优势。

Abstract: In the first part of this article, we consider ruled surfaces defined over a
finite field; we introduce invariants for them, and describe some explicit
contructions that illustrate possible behaviour of these invariants. In the
second part, we consider evaluation codes on some such surfaces; we first
estimate their parameters, then we construct asymptotically good families of
such codes, and we show that their asymptotic parameters are better than the
ones of the corresponding product codes. We also consider local properties of
these codes.

</details>


### [4] [New constructions of cyclic constant-dimension subspace codes based on Sidon spaces and subspace polynomials](https://arxiv.org/abs/2509.18704)
*Gang Wang,Ming Xu,You Gao*

Main category: cs.IT

TL;DR: 本文提出了两种新的Sidon空间构造方法，通过巧妙添加参数和灵活变化参数数量，构建了具有较大尺寸的循环常数维码（CDC），其尺寸优于已知最佳结果。


<details>
  <summary>Details</summary>
Motivation: 研究Sidon空间的构造方法，旨在提高循环常数维码的尺寸性能，特别是在Grassmann流形上的编码应用，以突破现有构造的尺寸限制。

Method: 采用两种新的Sidon空间构造技术：第一种基于参数n=(2r+1)k, r≥2和p0的定义；第二种基于n=2rk, r≥2的参数设置。两种方法都通过参数化处理和子空间多项式技术实现循环CDC的构造。

Result: 成功构造出最小距离为2k-2的循环CDC，其尺寸公式复杂但明确。特别地，当n=4k时，随着k趋于无穷大，构造的CDC尺寸与Sphere-packing bound的比值近似为1/2，表明性能接近理论极限。

Conclusion: 新构造方法显著提高了循环CDC的尺寸，推广了先前结果，并在特定参数下提供了比基于三项式的构造更大的尺寸或更多的N可取值，为编码理论提供了有效的工具。

Abstract: In this paper, two new constructions of Sidon spaces are given by tactfully
adding new parameters and flexibly varying the number of parameters. Under the
parameters $ n= (2r+1)k, r \ge2 $ and $p_0=\max \{i\in \mathbb{N}^+: \lfloor
\frac{r}{i}\rfloor>\lfloor \frac{r}{i+1} \rfloor \}$, the first construction
produces a cyclic CDC in $\mathcal{G}_q(n, k)$ with minimum distance $2k-2$ and
size $\frac{\left((r+\sum\limits_{i=2}^{p_0}(\lfloor \frac{r}{i}\rfloor-\lfloor
\frac{r}{i+1} \rfloor))(q^k-1)(q-1)+r\right)(q^k-1)^{r-1}(q^n-1)}{q-1}$. Given
parameters $n=2rk,r\ge 2$ and if $r=2$, $p_0=1$, otherwise, $p_0=\max\{ i\in
\mathbb{N}^+: \lceil\frac{r}{i}\rceil-1>\lfloor \frac{r}{i+1} \rfloor \}$, a
cyclic CDC in $\mathcal{G}_q(n, k)$ with minimum distance $2k-2$ and size
$\frac{\left((r-1+\sum\limits_{i=2}^{p_0}(\lceil \frac{r}{i}\rceil-\lfloor
\frac{r}{i+1} \rfloor-1))(q^k-1)(q-1)+r-1\right)(q^k-1)^{r-2}\lfloor
\frac{q^k-2}{2}\rfloor(q^n-1)}{q-1}$ is produced by the second construction.
The sizes of our cyclic CDCs are larger than the best known results. In
particular, in the case of $n=4k$, when $k$ goes to infinity, the ratio between
the size of our cyclic CDC and the Sphere-packing bound (Johnson bound) is
approximately equal to $\frac{1}{2}$. Moreover, for a prime power $q$ and
positive integers $k,s$ with $1\le s< k-1$, a cyclic CDC in $\mathcal{G}_q(N,
k)$ of size $e\frac{q^N-1}{q-1}$ and minimum distance $\ge 2k-2s$ is provided
by subspace polynomials, where $N,e$ are positive integers. Our construction
generalizes previous results and, under certain parameters, provides cyclic
CDCs with larger sizes or more admissible values of $ N $ than constructions
based on trinomials.

</details>


### [5] [A Convex Demixing Approach for Hybrid-Field Channel Estimation of XL-MIMO Systems via Atomic Norm Minimization](https://arxiv.org/abs/2509.18752)
*Dehui Yang,Feng Xi,Yanxian Zhu*

Main category: cs.IT

TL;DR: 本文提出了一种基于原子范数最小化框架的凸混合方法，用于6G无线通信中XL-MIMO系统的混合场信道估计。


<details>
  <summary>Details</summary>
Motivation: 在6G无线通信的极大规模MIMO系统中，信道估计是关键任务。混合场信道模型能有效表征实际XL-MIMO系统中的远场和近场散射分量混合情况。

Method: 通过在连续参数域中直接促进远场和近场分量的稀疏性，提出了一种最小化两个原子范数加权和的混合方案。该原子范数最小化问题等价于计算可行的半定规划问题。

Result: 在模拟数据上的数值实验表明，该方法在混合场信道估计方面优于现有方法。

Conclusion: 所提出的凸混合方法为XL-MIMO系统的混合场信道估计提供了一种有效的解决方案，通过原子范数最小化框架实现了更好的性能。

Abstract: Channel estimation is a critical task in extremely large-scale multiple-input
multiple-output (XL-MIMO) systems for 6G wireless communications. A
hybrid-field channel model effectively characterizes the mixed far-field and
near-field scattering components in practical XL-MIMO systems. In this paper,
we propose a convex demixing approach for hybrid-field channel estimation
within the atomic norm minimization (ANM) framework. By promoting sparsity of
the far-field and near-field components directly in the continuous parameter
domain, a demixing scheme that minimizes a weighted sum of two atomic norms is
proposed. We show that the resulting ANM is equivalent to a computationally
feasible semi-definite programming (SDP). Numerical experiments on simulated
data demonstrate that our method outperforms existing approaches for
hybrid-field channel estimation.

</details>


### [6] [A Two-Dimensional Super-Resolution Method for Reconfigurable Intelligent Surface-Assisted Near-Field Localization](https://arxiv.org/abs/2509.18774)
*Feng Xi,Dehui Yang*

Main category: cs.IT

TL;DR: 本文提出了一种基于Fresnel近场近似的RIS辅助3D定位方法，通过2D原子范数最小化实现无网格的角度-距离联合估计。


<details>
  <summary>Details</summary>
Motivation: 传统远场平面波模型在RIS辅助近场定位中不适用，需要处理角度和距离耦合的球面波模型，这增加了3D定位的复杂性。

Method: 采用Fresnel近似将RIS响应表示为2D远场导向向量与距离相关二次相位啁啾的乘积，通过低维子空间建模啁啾分量，构建2D原子范数最小化问题，并使用半定规划求解。

Result: 仿真结果表明该方法能够实现精确的3D定位，相比子空间和压缩感知方法具有更强的鲁棒性。

Conclusion: 所提出的2D-ANM框架有效解决了近场球面波模型下的角度-距离耦合问题，为RIS辅助近场定位提供了高精度解决方案。

Abstract: Reconfigurable intelligent surface (RIS)-aided localization in the radiating
near-field requires range-aware spherical-wave models, which inherently couple
angles and ranges and thus complicate accurate 3D positioning. Using the
Fresnel approximation, we show that the RIS response can be expressed as the
element-wise product of a 2D far-field steering vector and a range-dependent
quadratic-phase chirp. By modeling these chirp components within a
low-dimensional subspace, we reformulate the joint recovery of azimuth,
elevation, and range under a 2D super-resolution framework, resulting in a 2D
atomic norm minimization (2D-ANM) problem. Solving this via semi-definite
programming (SDP) yields gridless azimuth-elevation estimation and
high-accuracy range recovery. Simulations demonstrate accurate 3D localization
and enhanced robustness of the proposed scheme, compared with subspace and
compressive sensing methods.

</details>


### [7] [From Fixed to Fluid: Unlocking the New Potential with Fluid RIS (FRIS)](https://arxiv.org/abs/2509.18899)
*Han Xiao,Xiaoyan Hu,Kai-Kit Wong,Xusheng Zhu,Hanjiang Hong,Farshad Rostami Ghadi,Hao Xu,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出了一种新型可重构智能表面系统——流体RIS（FRIS），通过引入流体天线系统的概念，使RIS元件具有动态可重构的位置或辐射模式，从而增强波束成形灵活性和环境适应性。


<details>
  <summary>Details</summary>
Motivation: 当前RIS技术由于固定几何结构和元件模式的限制，其潜力只能部分发挥。为了克服这些限制，受流体天线系统概念的启发，开发了FRIS系统。

Method: FRIS允许元件位置或辐射模式表现出"流体"特性，即动态可重构性，以适应无线环境。本文提供了FRIS的全面概述，包括其分类、基本机制、优势、应用场景和案例研究。

Result: FRIS相比传统RIS具有增强的波束成形灵活性和环境适应性，能够更好地适应动态无线环境。

Conclusion: FRIS技术仍处于起步阶段，本文指出了该领域的主要开放挑战和未来研究方向。

Abstract: Owing to its flexible and intelligent electromagnetic signal manipulation,
the technology of reconfigurable intelligent surfaces (RISs) has attracted
widespread attention. However, the potential of current RISs can only be partly
unlocked due to their fixed geometry and element patterns. Motivated by the
concept of the fluid antenna system (FAS), a novel RIS system, termed fluid RIS
(FRIS), has been developed. Unlike traditional RISs, FRIS allows the element
positions or radiation patterns to exhibit ``fluid" properties, i.e., dynamic
reconfigurability, to adapt to the wireless environment, offering enhanced
beamforming flexibility and environmental adaptability. Given that research on
FRIS is still in its infancy, this paper provides a comprehensive overview of
its current developments and future prospects. Specifically, the key features
of FRIS are first presented, including its classification, fundamental
mechanisms, and advantages. Next, potential application scenarios of FRIS are
analyzed and discussed, followed by two illustrative case studies demonstrating
its potential. Finally, the main open challenges and future research directions
related to FRIS are highlighted.

</details>


### [8] [1-bit RIS-aided Index Modulation with Quantum Annealing](https://arxiv.org/abs/2509.18932)
*Ioannis Krikidis,Constantinos Psomas,Gan Zheng*

Main category: cs.IT

TL;DR: 本文提出了一种用于1位RIS相位分辨率的可重构智能表面辅助通信的新型索引调制方案，通过索引正相位偏移的基数来嵌入额外信息比特，并利用量子退火设备解决组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统的RIS辅助通信系统信息传输效率有限，需要开发新的调制方案来提高容量，同时解决1位RIS相位分辨率下的优化问题。

Method: 提出基于索引调制的RIS向量设计，将问题转化为带等式约束的二次二进制优化问题，采用惩罚方法和增广拉格朗日优化技术，并在D-WAVE量子退火设备上实现。

Result: 实验结果表明D-WAVE启发式算法能有效解决组合优化问题，理论分析显示该设计在平均容量上优于传统方案。

Conclusion: 所提出的索引调制方案结合量子计算技术，显著提升了RIS辅助通信系统的性能，为未来通信系统设计提供了新思路。

Abstract: In this paper, we investigate a new index modulation (IM) scheme for
reconfigurable intelligent surface (RIS)-assisted communications with 1-bit RIS
phase resolution. In addition to the traditional modulated symbols, extra bits
of information are embedded in the binary RIS phase vector by indexing the
cardinality of the positive phases shifts. To maximize capacity, the IM-based
RIS vector is selected so as to maximize the signal-to-noise ratio at the
receiver. The proposed IM design requires the solution of a quadratic binary
optimization problem with an equality constraint at the transmitter as well as
a quadratic unconstrained binary optimization (QUBO) problem at the receiver.
Since commercial solvers cannot directly handle constraints, a penalty method
that embeds the equality constraint in the objective function is investigated.
To overcome the empirical tuning of the penalty parameter, an iterative
Augmented Lagrangian optimization technique is also investigated where a QUBO
problem is solved at each iteration. The proposed design and associated
mathematical framework are tested in a real-world quantum annealing device
provided by D-WAVE. Rigorous experimental results demonstrate that the D-WAVE
heuristic efficiently solves the considered combinatorial problems.
Furthermore, theoretical bounds on the average capacity are provided. Both
experimental and theoretical results show that the proposed design outperforms
conventional counterparts.

</details>


### [9] [Optimum Spectrum Extension for PAPR Reduction of DFT-s-OFDM](https://arxiv.org/abs/2509.19064)
*Renaud-Alexandre Pitaval,Fredrik Berggren,Branislav M. Popovic*

Main category: cs.IT

TL;DR: 本文研究了通过频谱扩展（SE）结合频域频谱整形（FDSS）来降低蜂窝网络上行链路中的峰均功率比（PAPR），并优化了频移和SE大小，发现存在分别优化PAPR和速率的SE大小，这些最优值主要取决于窗口衰减，与带宽比例几乎不变。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络上行链路覆盖受限于最大UE发射功率，因此降低PAPR至关重要。虽然DFT-s-OFDM与FDSS相比OFDM能显著降低PAPR，但在高速传输时PAPR仍然过高。

Method: 采用参数化FDSS窗口和任意子载波系数循环移位的FDSS-SE方法，优化频移和SE大小，通过分析和仿真研究PAPR和速率的最优SE大小。

Result: 发现存在分别优化PAPR和速率的SE大小，这些最优值主要取决于窗口衰减，与带宽比例几乎不变。PAPR最优SE大小对常规QAM的星座阶数几乎不变，而速率最优SE大小还取决于SNR。

Conclusion: 为超越5G的上行链路覆盖增强提供了实用指南，强调应根据用户的FDSS窗口和链路质量单独配置SE大小。

Abstract: Uplink coverage in cellular networks is constrained by the maximum UE
transmit power, making peak-to-average power ratio (PAPR) reduction essential.
While DFT-s-OFDM with frequency-domain spectral shaping (FDSS) achieves
significantly lower PAPR than OFDM, especially with pi/2-BPSK, the PAPR remains
too high for higher-rate transmission. Spectrum extension (SE) combined with
FDSS (FDSS-SE) can further reduce the PAPR for higher-order QAM. This paper
considers FDSS-SE with parametrized FDSS windows spanning a range of possible
power ripples, as well as arbitrary circular shifts of the subcarrier
coefficients. We optimize both the frequency shift and the SE size, and show
that there exists an optimal SE size for reducing the PAPR and another one for
increasing the rate. Analysis and simulations reveal that both optima largely
depend on the window attenuation but are nearly invariant in proportion to the
bandwidth. While the PAPR-optimal SE size is nearly invariant to the
constellation order of regular QAM, the rate-optimal SE size depends also on
the SNR. These insights provide practical guidelines for beyond-5G uplink
coverage enhancement, highlighting that SE size should be individually
configured according to the user's FDSS window and link quality.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [10] [On Sybil-proofness in Restaking Networks](https://arxiv.org/abs/2509.18338)
*Tarun Chitra,Paolo Penna,Manvir Schneider*

Main category: cs.GT

TL;DR: 本文提出了一个重质押网络中Sybil攻击防护的形式化框架，分析了两种Sybil攻击类型，证明了不可能同时防止两种攻击的定理，并研究了网络结构对Sybil攻击盈利能力的影响。


<details>
  <summary>Details</summary>
Motivation: 重质押协议扩展了验证者的责任范围，但其安全性依赖于对Sybil攻击的抵抗能力。当前缺乏对重质押网络中Sybil防护机制的系统性分析框架。

Method: 引入形式化框架区分两种Sybil攻击类型；分析边际和乘法罚没机制；证明不可能定理；通过随机图模型研究网络结构影响。

Result: 证明没有罚没机制能同时防止两种Sybil攻击；Erdős-Rényi网络保持Sybil防护，但双块随机块模型中的最小异质性就会使Sybil攻击有利可图。

Conclusion: 研究揭示了重质押机制设计的基本限制，并强调了网络拓扑结构在Sybil防护中的关键作用。

Abstract: Restaking protocols expand validator responsibilities beyond consensus, but
their security depends on resistance to Sybil attacks. We introduce a formal
framework for Sybil-proofness in restaking networks, distinguishing between two
types of attacks, one in which other Sybil identities are kept out of an attack
and one where multiple Sybil identities attack. We analyze marginal and
multiplicative slashing mechanisms and characterize the conditions under which
each deters Sybil strategies. We then prove an impossibility theorem: no
slashing mechanism can simultaneously prevent both attack types. Finally, we
study the impact of network structure through random graph models: while
Erd\"os-R\'enyi networks remain Sybil-proof, even minimal heterogeneity in a
two-block stochastic block model makes Sybil attacks profitable. These results
reveal fundamental limits of mechanism design for restaking and highlight the
critical role of network topology.

</details>


### [11] [Fair Decisions through Plurality: Results from a Crowdfunding Platform](https://arxiv.org/abs/2509.18343)
*Joel Miller,E. Glen Weyl,Chris Kanich*

Main category: cs.GT

TL;DR: 本文提出了一种名为CO-QF的新算法，旨在解决Quadratic Funding算法在众筹平台中的公平性和效率问题，通过考虑用户的社交连接和亲社会行为，显著提升了资金分配效果。


<details>
  <summary>Details</summary>
Motivation: 传统的Quadratic Funding算法基于个体孤立和自私的模型，忽视了社交连接和亲社会行为，导致在众筹平台中公平性和经济效率不足。

Method: 开发了Connection-Oriented Quadratic Funding算法，结合技术和定性方法，基于多元化和亲社会效用的理论重新设计资金分配机制。

Result: CO-QF在平台上实现了89%的采用率，已分配超过400万美元资金，模拟显示其社会福祉优于QF算法。

Conclusion: CO-QF不仅适用于特定社区需求，还显示出在通用公共决策中的潜在应用价值，为算法设计提供了新的理论视角。

Abstract: We discuss an algorithmic intervention aimed at increasing equity and
economic efficiency at a crowdfunding platform that gives cash subsidies to
grantees. Through a blend of technical and qualitative methods, we show that
the previous algorithm used by the platform -- Quadratic Funding (QF) --
suffered problems because its design was rooted in a model of individuals as
isolated and selfish. We present an alternative algorithm --
Connection-Oriented Quadratic Funding (CO-QF) -- rooted in a theory of
plurality and prosocial utilities, and show that it qualitatively and
quantitatively performs better than QF. CO-QF has achieved an 89% adoption rate
at the platform and has distributed over $4 Million to date. In simulations we
show that it provides better social welfare than QF. While our design for CO-QF
was responsive to the needs of a specific community, we also extrapolate out of
this context to show that CO-QF is a potentially helpful tool for
general-purpose public decision making.

</details>


### [12] [Group Formation through Game Theory and Agent-Based Modeling: Spatial Cohesion, Heterogeneity, and Resource Pooling](https://arxiv.org/abs/2509.18551)
*Chenlan Wang,Jimin Han,Diana Jue-Rajasingh*

Main category: cs.GT

TL;DR: 本文开发了博弈论模型和基于代理的模型，研究由资源池化、空间凝聚力和异质性驱动的群体形成，重点关注公共、私人和非营利组织之间的跨部门合作伙伴关系。


<details>
  <summary>Details</summary>
Motivation: 研究跨部门合作中群体形成的动态机制，探索资源分配、空间距离和异质性如何影响组织间的合作模式。

Method: 结合博弈论模型和基于代理的模型，模拟代理在竞争环境中战略优化选择的过程，证明稳定群体均衡的存在性。

Result: 有限个体资源导致主要形成近距离群体，资源充足时群体可跨越更大距离；资源异质性和空间邻近性促进更大、更多样化群体的形成。

Conclusion: 揭示了影响群体规模和构成的关键权衡，为有效的跨部门合作和多代理系统提供策略指导。

Abstract: This paper develops a game-theoretic model and an agent-based model to study
group formation driven by resource pooling, spatial cohesion, and
heterogeneity. We focus on cross-sector partnerships (CSPs) involving public,
private, and nonprofit organizations, each contributing distinct resources.
Group formation occurs as agents strategically optimize their choices in
response to others within a competitive setting. We prove the existence of
stable group equilibria and simulate formation dynamics under varying spatial
and resource conditions. The results show that limited individual resources
lead to groups that form mainly among nearby actors, while abundant resources
allow groups to move across larger distances. Increased resource heterogeneity
and spatial proximity promote the formation of larger and more diverse groups.
These findings reveal key trade-offs shaping group size and composition,
guiding strategies for effective cross-sector collaborations and multi-agent
systems.

</details>


### [13] [Proximately Envy-Free and Efficient Allocation of Mixed Manna](https://arxiv.org/abs/2509.18673)
*Siddharth Barman,Paritosh Verma*

Main category: cs.GT

TL;DR: 该论文证明了对于混合物品（既有物品又有杂务）存在帕累托最优和自省性嫉妒至多一项的分配方案。


<details>
  <summary>Details</summary>
Motivation: 解决混合物品分配中帕累托最优和嫉妒至多一项分配方案存在性的开放问题，这是公平分配领域的核心挑战。

Method: 引入自省性嫉妒至多一项的概念，即每个代理可以通过在自己的捆绑包中添加或移除一项来消除对其他代理的嫉妒。

Result: 成功证明了混合物品分配中帕累托最优和自省性嫉妒至多一项分配方案的存在性。

Conclusion: 该结果将Mahara关于杂务分配的结果推广到混合物品情形，为公平分配理论做出了重要贡献。

Abstract: The existence of fair and efficient allocations of indivisible items is a
central problem in fair division. For indivisible goods, the existence of
Pareto efficient (PO) and envy free up to one item (EF1) allocations was
established by Caragiannis et al. In a recent breakthrough, Mahara established
the existence of PO and EF1 allocations for indivisible chores.
  However, the existence of PO and EF1 allocations of mixed manna remains an
intriguing open problem. In this paper, we make significant progress in this
direction. We establish the existence of allocations that are PO and
introspective envy free up to one item (IEF1) for mixed manna. In an IEF1
allocation, each agent can eliminate its envy towards all the other agents by
either adding an item or removing an item from its own bundle. The notion of
IEF1 coincides with EF1 for indivisible chores, and hence, our existence result
generalizes the aforementioned result of Mahara.

</details>


### [14] [Approximating Electoral Control Problems](https://arxiv.org/abs/2509.19279)
*Huy Vu Bui,Michael C. Chavrimootoo,Trung Kien Le,Son M. Nguyen*

Main category: cs.GT

TL;DR: 本文研究了选举控制问题的近似算法，针对plurality、approval和Condorcet三种投票规则，在加权和非加权选民设置下，建立了近似算法的可近似性结果，并证明了这些算法的近似比是最优的（除非P=NP）。


<details>
  <summary>Details</summary>
Motivation: 选举控制是研究最广泛的选举攻击形式之一，但之前的研巨主要集中在决策复杂性分析（如P类、NP完全性、固定参数可处理性），而近似算法在选举控制研究中受到较少关注，尽管在其他选举攻击形式（如操纵和贿赂）中很常见。

Method: 利用覆盖整数规划（CIPs）可以在O(log n)因子内近似的特性，设计了近似算法。对于plurality规则，给出了O(m)-近似算法，并利用Minimum k-Union问题的下界证明了Ω(m^{1/4})的下界。还通过公理化方法将算法推广到无限族投票规则。

Result: 对于approval和Condorcet下的每个问题，证明所给近似算法是最优的（除非P=NP）。在plurality下获得了O(m)-近似算法和Ω(m^{1/4})的下界。这是MkU问题在计算社会选择中的首次应用。

Conclusion: 本文填补了选举控制研究中近似算法领域的空白，解决了18年前提出的一系列开放问题，为三种标准投票规则下的控制问题提供了完整的近似性分析。

Abstract: Much research in electoral control -- one of the most studied form of
electoral attacks, in which an entity running an election alters the structure
of that election to yield a preferred outcome -- has focused on giving decision
complexity results, e.g., membership in P, NP-completeness, or fixed-parameter
tractability. Approximation algorithms on the other hand have received little
attention in electoral control, despite their prevalence in the study of other
forms of electoral attacks, such as manipulation and bribery. Early work
established some preliminary results with respect to popular voting rules such
as plurality, approval, and Condorcet. In this paper, we establish for each of
the ``standard'' control problems under plurality, approval, and Condorcet,
whether they are approximable, and we prove our results in both the weighted
and unweighted voter settings. For each problem we study under either approval
or Condorcet, we show that any approximation algorithm we give is optimal,
unless P=NP. Our approximation algorithms leverage the fact that Covering
Integer Programs (CIPs) can be approximated within a factor of $O(\log n)$.
Under plurality, we give an $O(m)$-approximation algorithm, and give as lower
bound $\Omega(m^{1/4})$, by using a known lower bound on the Minimum $k$-Union
(M$k$U) problem. To our knowledge, this is the first application of M$k$U in
computational social choice. We also generalize our $O(m)$-approximation
algorithm to work with respect to an infinite family of voting rules using an
axiomatic approach. Our work closes a long list of open problems established 18
years ago.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 本文提出了一个成本效益分析框架，帮助企业决策何时本地部署开源LLM比商业订阅服务更经济可行


<details>
  <summary>Details</summary>
Motivation: 企业在使用AI时面临选择商业LLM服务还是本地部署的难题。商业服务虽然方便但存在数据隐私、供应商锁定和长期成本等问题，因此需要分析本地部署的经济可行性

Method: 构建成本效益分析框架，考虑硬件需求、运营费用和开源模型性能基准，将本地部署总成本与主要云服务商的订阅费用进行比较

Result: 研究提供了基于使用水平和性能需求的盈亏平衡点估计

Conclusion: 该框架为企业规划LLM战略提供了实用的决策工具

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [16] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE是一个基于大语言模型的土壤湿度时间序列分析框架，能够联合检测灌溉模式和异常，无需特定任务标注或微调即可实现零样本分析。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度时间序列分析方法要么依赖基于阈值的规则，要么使用数据需求高的机器学习或深度学习模型，这些方法在适应性和可解释性方面存在局限。

Method: SPADE利用ChatGPT-4.1的先进推理能力，将时间序列数据转换为文本表示，并通过领域知识提示模板来识别灌溉事件、估计净灌溉增益、检测和分类异常，生成结构化可解释报告。

Result: 在真实世界土壤湿度传感器数据上的实验表明，SPADE在异常检测方面优于现有方法，获得了更高的召回率和F1分数，并能准确分类异常类型。在灌溉事件检测方面也达到了高精度和召回率。

Conclusion: 该研究突显了LLMs作为精准农业可扩展、适应性工具的潜力，能够整合定性知识和数据驱动推理，为准确的土壤湿度监测和改进的灌溉调度提供可操作的见解。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [17] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 本文提出可解释不确定性估计(XUE)方法，将可解释性与不确定性量化相结合，以增强医疗AI的可信度和可用性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统未能以符合临床推理的方式明确量化或传达不确定性，现有可解释AI(XAI)工作缺乏对预测置信度的捕捉，而不确定性估计(UE)技术又缺乏直观解释，这种脱节限制了AI在医学领域的应用。

Method: 系统地将医学不确定性映射到AI不确定性概念，识别XUE实施的关键挑战，提出多模态不确定性量化、模型无关可视化技术和不确定性感知决策支持系统等技术方向。

Result: 分析强调了需要开发不仅能生成可靠预测，还能以临床有意义的方式表达置信度的AI系统。

Conclusion: 这项工作通过桥接可解释性和不确定性，为开发与真实世界临床复杂性相一致的AI系统铺平了道路，有助于构建可信赖的医疗AI。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [18] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: HSGM是一种分层分段图记忆框架，通过将长文档分解为有意义的分段，构建局部语义图并提取摘要节点来形成全局图记忆，显著降低了长文档语义解析的计算复杂度和内存需求。


<details>
  <summary>Details</summary>
Motivation: 长文档语义解析面临二次复杂度增长和内存需求激增的挑战，传统方法难以处理超长文本。

Method: HSGM框架将输入文档分解为M个分段，在每个分段上构建局部语义图，提取摘要节点形成全局图记忆，支持增量更新和分层查询处理。

Result: 在三个基准测试中，HSGM实现了2-4倍的推理加速，峰值内存减少超过60%，同时保持基线准确率的95%以上。

Conclusion: HSGM为超长文本提供了可扩展、准确的语义建模方法，适用于实时和资源受限的NLP应用。

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [19] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多智能体框架，通过单一自然语言提示自动化整个OpenFOAM工作流程，显著降低了CFD仿真的使用门槛。


<details>
  <summary>Details</summary>
Motivation: 解决CFD仿真学习曲线陡峭和手动设置复杂的问题，降低工程仿真的专业门槛。

Method: 采用多智能体框架，包含网格生成代理、HPC脚本自动生成和可视化处理；使用Model Context Protocol实现可组合服务架构；通过分层多索引RAG实现高精度配置生成。

Result: 在110个仿真任务基准测试中，Foam-Agent达到88.2%的成功率，显著优于现有框架（MetaOpenFOAM为55.5%）。

Conclusion: Foam-Agent有效降低了CFD的专业门槛，展示了专用多智能体系统在复杂科学计算民主化方面的潜力。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [20] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 研究探索LLMs能否从数学可解释的推荐模型生成有效的用户导向解释，通过用户研究评估不同解释策略的质量。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI研究多依赖自动评估指标，但这些指标往往无法捕捉用户的实际需求和感知，因此需要采用用户中心的方法来评估解释质量。

Method: 使用基于约束矩阵分解的可解释推荐模型，通过精心设计的LLM提示将模型结构转化为自然语言解释，并在326名参与者中进行用户研究，评估五种关键维度的解释质量。

Result: 所有解释类型都受到良好接受，不同策略之间存在适度的统计差异，用户评论提供了定量结果之外的补充见解。

Conclusion: LLMs能够从数学可解释模型生成有效的用户导向解释，用户中心评估方法为可解释AI提供了更全面的评估视角。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [21] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA是一个新颖的视觉记忆问答任务，Pensieve管道通过记忆增强、时空感知检索和多记忆问答微调，在QA准确率上比现有方法提升高达14%。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界中基于视觉记忆的回忆问答任务面临的挑战，包括任务导向记忆创建、时空信息有效利用以及多记忆融合推理。

Method: 提出Pensieve综合管道，包含记忆特定增强、时空感知多信号检索和多记忆问答微调三个核心模块。

Result: 在多模态基准测试中，Pensieve相比最先进方法在问答准确率上提升高达14%。

Conclusion: Pensieve管道有效解决了Memory-QA任务的独特挑战，为视觉记忆问答提供了可行的解决方案。

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [22] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本文综述了将大语言模型（LLMs）集成到运筹学（OR）中的最新进展，主要涵盖自动建模、辅助优化和直接求解三个方向，并讨论了评估基准、应用领域及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法依赖专家建模和手动参数调整，难以处理大规模、动态和多约束问题。LLMs通过语义理解、结构化生成和推理控制，有望解决这些局限性。

Method: 将LLMs在OR中的应用方法分为三类：自动建模（将自然语言描述转换为数学模型或代码）、辅助优化（生成启发式算法和演化算法）和直接求解（直接处理优化任务）。

Result: LLMs在OR中展现出潜力，能够提升建模效率和优化性能，但目前存在语义到结构映射不稳定、研究进展碎片化、泛化能力有限和评估体系不足等问题。

Conclusion: LLMs为OR提供了新的方法论，未来需进一步研究以解决现有挑战，推动LLMs在OR中的更广泛应用。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [23] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 本文提出了SAPA框架，利用大语言模型合成理论基础的潜在态度来预测网约车选择，显著提升了预测准确率


<details>
  <summary>Details</summary>
Motivation: 现有网约车模式选择模型预测精度有限，无法捕捉关键心理因素，且面临严重的类别不平衡问题

Method: SAPA采用分层方法：首先用LLM从原始旅行调查数据生成定性旅行者画像，然后训练倾向得分模型，再用LLM为理论驱动的潜在变量分配定量分数，最后通过分类器整合所有特征进行预测

Result: 在大规模多年旅行调查上的实验表明，SAPA显著优于现有基线方法，在测试集上的PR-AUC提升了75.9%

Conclusion: 该研究为准确预测网约车模式选择提供了有力工具，且方法可轻松迁移到各种应用中

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [24] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: OBER是一个基于学习成果的教育推荐系统，通过将学习成果和评估项目嵌入数据模式，使任何推荐算法都能根据其促进的掌握程度进行评估。


<details>
  <summary>Details</summary>
Motivation: 大多数教育推荐系统仅基于点击或评分相关性进行调优和判断，其真实的教学影响不明确。需要一种能够直接衡量推荐系统对学习成果影响的方法。

Method: OBER采用极简的实体关系模型、基于日志的掌握度公式和插件架构。在非正式学习领域的电子学习系统中进行了为期两周的随机分组测试，比较了固定专家路径、协同过滤和基于知识的过滤三种方法。

Result: 协同过滤方法最大化了用户留存率，但固定路径实现了最高的掌握度。OBER可以从相同的日志中导出业务、相关性和学习指标，让实践者能够在没有额外测试开销的情况下权衡相关性和参与度与成果掌握度。

Conclusion: OBER框架是方法无关的，可以轻松扩展到未来的自适应或上下文感知推荐系统，为教育推荐系统的真实教学效果评估提供了有效工具。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [25] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 提出了MMCD框架，通过多模态协作决策和跨模态知识蒸馏，解决自动驾驶中传感器故障或连接车辆缺失时的鲁棒性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练和测试时所有数据模态和连接车辆都可用，这在传感器故障或车辆缺失时不切实际，需要更鲁棒的决策框架

Method: 提出MMCD框架，融合自车和协作车的多模态观测数据，采用教师-学生模型的跨模态知识蒸馏方法，教师模型使用完整模态训练，学生模型在模态减少时仍能有效工作

Result: 在连接自动驾驶和空地车辆协作实验中，该方法将驾驶安全性提高了20.7%，在潜在事故检测和安全驾驶决策方面优于现有最佳基线

Conclusion: MMCD框架通过多模态协作和知识蒸馏，显著提升了自动驾驶系统在挑战性环境中的鲁棒性和安全性

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [26] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出了一种形式化方法来解释定量双极论证框架（QBAF）中推理变化的原因，通过追踪论证强度不一致性并识别其解释。


<details>
  <summary>Details</summary>
Motivation: 在QBAF中推理并更新框架后，需要解释论证强度顺序变化的原因，以理解推理变化背后的机制。

Method: 定义强度不一致性概念，追踪其根源到具体论证，识别充分、必要和反事实解释，并提出基于启发式的解释搜索方法。

Result: 证明了强度不一致性解释存在的充要条件，并提供了实现方法。

Conclusion: 该方法能够有效解释QBAF更新导致的推理变化，为理解论证强度变化提供了形式化工具。

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [27] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 该论文提出了神经DNA（nDNA）作为捕捉AI基础模型潜在认知身份的语义基因型表示，通过分析模型的潜在几何结构来揭示其内在认知特征。


<details>
  <summary>Details</summary>
Motivation: 当前AI基础模型的能力不断增强，但现有基准测试仅能测量模型行为，而无法捕捉模型的内在认知身份。论文旨在开发一种能够表征模型潜在几何结构的方法，以理解模型的语义流动和认知特征。

Method: 提出神经DNA（nDNA）方法，基于三个几何维度：谱曲率（揭示概念流动的曲率）、热力学长度（量化语义转换所需的努力）和信念矢量场（描述模型的信念方向场）。这些维度共同构成模型的语义基因型表示。

Result: nDNA能够稳定地捕捉模型的潜在几何特征，实现模型之间的谱系追踪、继承关系测量、漂移检测等生物学类比分析，为研究人工认知的演化提供新工具。

Conclusion: 该工作开创了神经基因组学新领域，将AI模型视为具有可追踪内在认知的数字语义有机体，为模型比较、风险诊断和演化治理提供了新的理论基础和方法论。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [28] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 本文提出了相似性场理论，这是一个数学框架，用于形式化实体间相似性关系及其演化的原则。该理论定义了相似性场、系统演化、概念纤维和生成算子，并基于此形式化地定义了智能的概念。


<details>
  <summary>Details</summary>
Motivation: 作者认为持久化和转换相似性关系构成了任何可理解动态系统的结构基础。需要一个数学框架来形式化相似性值的原则及其演化，为表征、比较和构建智能系统提供基础语言。

Method: 定义了相似性场S: U×U→[0,1]，满足自反性但允许不对称性和非传递性；系统演化序列Zp=(Xp,S(p))；概念K诱导纤维Fα(K)；生成算子G。基于此形式化智能定义：若G能生成属于概念K纤维的新实体，则G相对于K是智能的。

Result: 证明了两个定理：(i)不对称性阻碍相互包含；(ii)稳定性需要锚坐标或最终限制在f的水平集内。这些结果确保了相似性场演化的约束性和可解释性。

Conclusion: 相似性场理论为理解大型语言模型提供了框架，并可作为研究社会认知的实验探针。该理论为智能系统的表征和构建提供了基础语言。

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [29] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer是一个用于预测个体健康风险的多模态AI框架，结合视觉和语言数据，通过分层Transformer架构和LLM推理头实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 随着慢性疾病负担增加和临床数据（医学影像、文本记录、可穿戴设备等）的多模态异质性，需要统一的多模态AI框架来主动预测个体健康风险。

Method: 采用分层堆叠的视觉-语言多模态Transformer，包含四个关键创新：跨模态预训练、时间融合块、疾病本体图适配器，以及基于动量更新编码器和去偏InfoNCE损失的细粒度对齐。

Result: 在MIMIC-IV纵向队列中，VL-RiskFormer实现了平均AUROC为0.90，预期校准误差为2.7%。

Conclusion: VL-RiskFormer通过多模态数据融合和时间序列处理，有效提升了健康风险预测的准确性和可靠性，为临床决策提供了有力支持。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [30] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind是一个混合架构，结合了探索链、知识图谱、检索增强生成和大语言模型，用于解决个性化食谱推荐中的模糊用户意图、语义准确性和细节覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 个性化食谱推荐面临处理模糊用户意图、确保语义准确性和提供足够细节覆盖的挑战。

Method: 提出ChefMind混合架构：探索链（CoE）将模糊查询细化为结构化条件，知识图谱（KG）提供语义推理和可解释性，检索增强生成（RAG）补充上下文烹饪细节，大语言模型（LLM）将输出整合为连贯推荐。

Result: 在下厨房数据集和手动标注查询上的评估显示，ChefMind在准确性、相关性、完整性和清晰度方面表现优异，平均得分8.7，而消融模型为6.4-6.7。未处理查询降至1.6%，证明其在处理模糊需求方面的鲁棒性。

Conclusion: ChefMind通过混合架构有效解决了食谱推荐中的关键挑战，实现了显著优于单一方法的性能。

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [31] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 本文提出了一种"N+1"GPT代理框架，用于机械工程问题的初步分析。该框架通过多个独立求解代理和比较代理来提高GPT在工程分析中的可靠性。


<details>
  <summary>Details</summary>
Motivation: GPT在机械工程分析中表现出不稳定性，成功率仅为85%，这种不可靠性使其不适合直接用于教育或工程实践。需要一种方法来提高GPT解决方案的可靠性。

Method: 采用"N+1"代理框架：首先启动N个独立求解代理生成解决方案，然后通过比较代理汇总和比较这些方案，选出推荐解决方案。基于孔多塞陪审团定理，当每个求解代理的成功概率大于1/2时，多数解决方案很可能是正确的。

Result: 与商业多代理模型Grok Heavy相比，该框架在设计和性能上有相似之处，但更注重透明度和教学价值。

Conclusion: 该代理框架能够有效提高GPT在机械工程分析中的可靠性，特别适合教育应用，通过透明的方法帮助学生理解问题解决过程。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [32] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: ComputerAgent是一个轻量级分层强化学习框架，通过两级选项过程（管理器和子策略）控制桌面应用，使用三模态状态编码器处理视觉和上下文多样性，集成元动作和提前停止机制减少无效交互，实现设备端推理（1500万参数）。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在桌面应用控制中存在推理延迟高、长视野稀疏奖励任务样本效率差、设备端部署不可行等问题，需要更实用的解决方案。

Method: 采用分层强化学习框架，将操作系统控制建模为两级选项过程，使用截图、任务ID和数值状态的三模态编码器，集成元动作和提前停止机制，使用紧凑视觉骨干网络和小型策略网络。

Result: 在135个真实桌面任务测试中，简单任务（<8步）成功率92.1%，困难任务（≥8步）成功率58.8%，性能匹配或超过2000亿参数MLLM基线，模型大小减少四个数量级，推理时间减半。

Conclusion: 分层强化学习为计算机控制提供了一个实用、可扩展的替代方案，优于基于单体MLLM的自动化方法。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [33] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 论文指出当前医学AI基准测试存在缺陷，大型模型虽然在基准测试中得分高，但实际表现脆弱，容易受到输入变化影响，且存在猜测和编造推理的问题。


<details>
  <summary>Details</summary>
Motivation: 揭示医学AI基准测试的局限性，证明高分数并不等同于真实医疗场景中的可靠表现，强调需要更严格的评估标准。

Method: 对六个领先模型在六个广泛使用的医学基准上进行压力测试，包括移除关键输入、改变提示方式等，并通过临床医生指导的评估标准进行分析。

Result: 发现模型在基准测试中表现脆弱，容易因微小变化而改变答案，且基准测试之间测量内容差异大但被等同对待，掩盖了失败模式。

Conclusion: 医学基准测试分数不能直接反映真实世界准备度，AI在医疗领域获得信任需要超越排行榜胜利，要求系统具备鲁棒性、合理推理和与真实医疗需求的一致性。

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [34] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 本文研究了两种计算约束策略（推理长度约束和模型量化）对推理语言模型性能和安全性的影响，旨在降低计算成本的同时保持模型效果。


<details>
  <summary>Details</summary>
Motivation: 测试时计算扩展虽然能通过生成长链思维序列提升推理模型性能，但计算成本显著增加。本研究旨在探索在计算约束下如何平衡模型性能与安全性。

Method: 采用两种方法：1）使用长度控制策略优化的强化学习方法微调推理模型，使其满足用户定义的推理长度；2）应用量化技术，在用户定义的计算约束下最大化链式思维序列的生成。

Result: 研究发现计算约束策略能有效降低推理模型的计算需求，但需要在计算效率与模型安全性之间进行权衡。

Conclusion: 计算约束策略是降低推理模型计算成本的有效方法，但需要仔细平衡计算效率与安全性之间的trade-off。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [35] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 本文提出Gödel测试，评估GPT-5在解决组合优化中新简单猜想的能力，结果显示模型在常规推理上有进步，但跨论文综合能力有限。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型是否能在高级数学领域解决新的简单猜想，而不仅仅是已知数学竞赛问题。

Method: 使用5个组合优化中的未解决猜想，提供相关源论文但不透露猜想本身，详细评估GPT-5的推理过程。

Result: GPT-5在3个较易问题上产生接近正确的解，甚至反驳了一个猜想；但在需要跨论文综合的问题上失败；在无验证猜想的难题上提出正确算法但分析失败。

Conclusion: GPT-5在常规推理上有意义进展，偶尔展现原创性，但跨论文合成能力明显不足，可能是通过Gödel测试的早期步骤。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [36] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 该论文提出了首个HTS代码分类基准，通过微调Atlas模型在10位和6位分类上分别达到40%和57.5%的准确率，相比GPT-5和Gemini有显著提升，且成本更低、可自托管保证数据隐私。


<details>
  <summary>Details</summary>
Motivation: HTS代码分类是全球贸易中的关键瓶颈，但机器学习社区对此关注不足。错误分类可能导致货物运输完全停止，主要邮政运营商因海关文件不完整而暂停向美国发货。

Method: 基于美国海关在线搜索系统(CROSS)构建首个HTS代码分类基准，对领先的LLM进行评估，并微调Atlas模型(基于LLaMA-3.3-70B)。

Result: 微调的Atlas模型在10位分类上达到40%完全正确率，6位分类达到57.5%正确率，比GPT-5-Thinking高15个百分点，比Gemini-2.5-Pro-Thinking高27.5个百分点。成本约为GPT-5的1/5、Gemini的1/8。

Conclusion: Atlas模型为HTS分类设立了强基线，但该基准仍极具挑战性。通过发布数据集和模型，旨在将HTS分类定位为新的社区基准任务，并邀请在检索、推理和对齐方面的未来工作。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [37] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC是一个新的函数调用基准测试，专门评估LLM在函数调用中遵循格式指令的能力，填补了现有基准测试只关注参数正确性而忽略格式要求的空白。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准测试（如BFCL、tau^2-Bench、ACEBench）只评估参数正确性，但不测试参数描述中嵌入的格式指令（如双引号、ISO日期格式等）的遵循情况，这在现实AI代理系统中是一个实际限制。

Method: 受IFEval启发，IFEval-FC在JSON schema描述中直接编码可验证的格式要求（如值不能包含标点符号），包含750个测试用例，每个用例包含一个函数及其输入参数的嵌入格式要求，以及相应的用户查询，评估完全基于算法。

Result: 结果显示，即使是GPT-5和Claude 4.1 Opus等最先进的专有模型也经常无法遵循基本格式规则，突显了现实世界代理系统的实际局限性。

Conclusion: IFEval-FC提供了一个客观、可复现、可扩展的基准测试，揭示了当前LLM在精确指令遵循方面的不足，代码和数据已公开。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [38] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA是一个用于击剑裁判辅助的AI原型系统，通过姿态识别和规则推理来解决击剑裁判中的主观性、人为错误和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 击剑运动在裁判方面面临主观判断、人为错误、偏见以及在训练环境中裁判资源有限等挑战。

Method: 系统从视频中提取2D关节位置，进行归一化处理后计算101维运动学特征，使用Transformer进行多标签动作和剑尖分类，并结合基于规则的推理来确定优先权和得分。

Result: 在有限的手动标注数据下，5折交叉验证的平均宏F1分数达到0.549，优于Temporal Convolutional Network、BiLSTM和普通Transformer等基线模型。

Conclusion: 虽然尚未达到部署水平，但结果表明在击剑自动化裁判辅助方面具有前景，并为AI在击剑领域的应用（如教练辅助）开辟了新机会。

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [39] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: LLMZ+是一种基于提示白名单的新型防御机制，通过仅允许上下文相关的安全消息与智能LLM交互，有效防御越狱攻击，同时保证合法业务通信不受干扰。


<details>
  <summary>Details</summary>
Motivation: 传统检测恶意意图的防御机制存在局限性，智能AI因其特权数据访问和API工具使用而成为高价值攻击目标，其非确定性行为特性带来了重大的安全和信息安全风险。

Method: 采用提示白名单方法，通过上下文特异性确保所有外部用户与LLM的交互都符合预定义用例和操作边界，超越传统的基于检测的防御方法。

Result: 实证评估显示LLMZ+对常见越狱提示具有强韧性，同时合法业务通信不受干扰，在实验环境中误报率和漏报率均可降至0。

Conclusion: LLMZ+方法简化了安全框架，增强了长期韧性，减少了维持LLM信息安全所需的资源，为智能AI安全提供了有效的替代方案。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [40] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: 该论文提出了一种新方法，通过分解数学应用题生成方程，使用外部符号求解器求解，并通过二次估算验证答案准确性，在数值和代数数学应用题上取得了新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决数学应用题方面存在困难，因为需要复杂的推理和数学能力。现有方法通过改进提示来帮助LLMs解决更复杂的数学应用题，但仍有提升空间。

Method: 首先提示LLM从问题分解中创建方程，然后使用外部符号方程求解器生成答案。为确保准确性，让LLM第二次估算正确答案并与生成答案比较验证。如果验证失败，采用迭代修正过程。

Result: 该方法在数值和代数数学应用题数据集上取得了新的最先进结果，平均比之前最佳结果提高了近两个百分点。在三角函数数学应用题上也获得了满意结果。

Conclusion: 提出的验证和修正方法有效提高了LLMs解决数学应用题的准确性，并引入了两个新数据集SVAMPClean和Trig300来进一步测试LLMs的推理能力。

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [41] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 本文提出了一个结合地理空间建模与进化学习的新型基于代理的气候风险评估模型，用于量化直接和级联气候风险。


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要建模空间异质性灾害与自适应经济系统之间的复杂相互作用，现有模型难以捕捉这些动态过程。

Method: 开发了基于Mesa的空间建模框架，整合CLIMADA气候影响评估，引入进化学习机制让企业通过适应性选择进化预算分配、定价、工资和风险适应策略。

Result: 使用RCP8.5情景下的河流洪水预测到2100年，显示进化适应使企业能够在数十年气候压力干扰后恢复到基线生产水平；未直接暴露于洪水的代理也会通过供应链中断受到影响，世纪末商品平均价格比基线高5.6%。

Conclusion: 该开源框架为金融机构和公司提供了量化直接和级联气候风险的工具，同时评估成本效益适应的策略。

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [42] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG是一个低成本图检索增强生成框架，通过个性化PageRank方法在检索阶段构建信息图，仅需3%-11%的输出token即可达到主流图RAG方法80%以上的准确率


<details>
  <summary>Details</summary>
Motivation: 现有图RAG系统忽视了LLM token使用的高成本，阻碍了大规模应用，需要开发更经济的解决方案

Method: 受HippoRAG启发，在检索阶段引入个性化PageRank(PPR)来构建信息图，显著降低token消耗

Result: TERAG框架仅消耗3%-11%的输出token，就能达到广泛使用的图RAG方法至少80%的准确率

Conclusion: TERAG证明了通过优化检索策略可以大幅降低图RAG的成本，为大规模应用提供了可行方案

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [43] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 该论文旨在澄清机器学习模型与其明确描述之间的区别，并完善语义保持的概念以确保模型的准确复制，应用于工业用例来构建和比较目标模型。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在航空系统中的应用增加，需要确保ML系统的安全运行并符合相关指导标准。EASA和EUROCAE/SAE已发布高层目标，但需要更具体的方法来验证ML模型在其目标环境中的性能。

Method: 论文提出了机器学习模型描述（MLMD）的概念，区分ML模型与其明确描述，并完善语义保持的概念。通过多个工业用例应用这些贡献来构建和比较目标模型。

Result: 通过应用MLMD和语义保持概念，论文展示了如何确保ML模型在目标环境中准确复制其训练性能，为航空系统安全认证提供具体方法。

Conclusion: 该研究为ML在航空系统中的安全应用提供了重要框架，通过明确的模型描述和语义保持机制，有助于满足EASA等监管机构的安全要求。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [44] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型在医学领域的最新研究进展，包括医学大模型的训练技术、医疗场景应用、优缺点分析，并创新性地将医学LLMs分为三类，评估方法分为两类，同时提出解决现有挑战的方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，LLMs在医学领域展现出巨大应用潜力，需要系统梳理当前研究进展，为医学LLMs的发展提供指导和方向。

Method: 采用系统性文献综述方法，深入分析医学大模型的训练技术、适应医疗场景的应用方式，并对现有研究进行创新性分类。

Result: 提出了医学LLMs的三类分类体系和两类评估方法，识别了当前领域存在的问题和挑战。

Conclusion: 通过系统综述，强调了开发医学LLMs的必要性，加深了对当前发展状态的理解，为后续研究提供了清晰指导。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [45] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 本文提出自主数据代理（DataAgents）的概念，通过集成LLM推理与任务分解、行动推理和工具调用，实现从数据到知识的自动化转换。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和复杂性的增长，数据准备、转换和分析工作仍然劳动密集、重复且难以扩展。数据与AI之间的对齐至关重要，但现有数据结构往往不适合AI利用。

Method: DataAgents通过动态规划工作流、调用强大工具并适应不同规模的数据任务，能够处理数据收集、集成、预处理、选择、转换、重新加权、增强、重编程、修复和检索等操作。

Result: DataAgents能够将复杂和非结构化数据转化为连贯且可操作的知识，代表了向自主数据到知识系统的范式转变。

Conclusion: 需要协同努力推进行动工作流优化、建立开放数据集和基准生态系统、保护隐私、平衡效率与可扩展性，并开发可信的DataAgent防护措施以防止恶意行为。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [46] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 提出经验扩展框架，通过自主环境交互和协作经验共享实现LLM的持续进化，突破静态人类生成数据的限制


<details>
  <summary>Details</summary>
Motivation: 传统通过扩大模型规模、训练数据和计算能力的方法已接近饱和，人类生成文本资源枯竭，增益递减

Method: 经验扩展框架：捕获原始交互，提炼为紧凑可重用知识，定期优化存储内容以保持相关性和效率

Result: 在模拟真实场景中验证，包括泛化到未见但相关任务、重复查询和过饱和知识存储，在所有设置中提高准确性、维持性能并保持对新情况的增益

Conclusion: 结构化部署后学习可以扩展LLM能力，超越静态人类生成数据的限制，为持续智能进步提供可扩展路径

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [47] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS是一个分布式目录服务，用于AI代理能力、元数据和来源的发现，采用内容寻址存储、层次分类和加密签名技术，支持异构多代理系统中的高效、可验证、多维发现。


<details>
  <summary>Details</summary>
Motivation: 随着多代理系统（MAS）的异构性增加，需要一种能够高效发现代理能力、确保数据可验证性并支持多维查询的分布式目录服务。

Method: 基于Open Agentic Schema Framework（OASF），ADS通过两级映射实现能力索引与内容位置的解耦，采用Kademlia-based分布式哈希表（DHT），复用OCI/ORAS基础设施进行工件分发，集成Sigstore确保来源可验证性，并支持模式驱动的扩展性。

Result: ADS提供了高效的存储和发现层，具备安全性和性能优势，能够适应新兴代理模式（如LLM提示代理、MCP服务器、A2A组件）。

Conclusion: ADS在代理注册和互操作性倡议的广阔前景中定位明确，为异构多代理系统提供了可验证、高效且可扩展的发现机制。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [48] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER是一种基于模型检查的方法，用于验证LLM文本生成过程的概率计算树逻辑(PCTL)属性，通过α-k有界文本生成来限制生成空间。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本生成过程缺乏形式化验证方法，无法确保生成文本的一致性和可靠性，需要一种系统性的验证框架。

Method: 提出α-k有界文本生成方法，在每个生成步骤中只考虑累积概率超过阈值α的top-k个token，然后应用PCTL模型检查技术进行形式化验证。

Result: 该方法在多个LLM模型（Llama、Gemma、Mistral等）上得到验证，能够有效检查文本生成过程的一致性。

Conclusion: 这是首次将PCTL模型检查应用于LLM文本生成验证，为LLM的形式化验证提供了新途径。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [49] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 提出了一个模块化框架用于ICD-10-CM编码预测，通过原则性模型选择、冗余感知数据采样和结构化输入设计来解决LLM在医疗编码中的挑战。


<details>
  <summary>Details</summary>
Motivation: ICD编码对临床文档、计费和医疗分析至关重要，但目前仍是劳动密集且易出错的任务。LLM在自动化ICD编码方面有潜力，但在基础模型选择、输入情境化和训练数据冗余方面存在挑战。

Method: 采用模块化框架，包括LLM作为评判者的评估协议与Plackett-Luce聚合来评估开源LLM对ICD-10-CM代码定义的理解；引入基于嵌入的相似性度量和冗余感知采样策略；利用台湾医院的结构化出院摘要评估情境效应。

Result: 在两个机构数据集上的实验表明，经过微调的基础模型在内部和外部评估中始终优于基线LLM。包含更多临床部分持续改善预测性能。

Conclusion: 该研究使用开源LLM为ICD-10-CM代码预测建立了一个实用且原则性的方法，提出的框架通过结合知情模型选择、高效数据精炼和情境感知提示，为自动化医疗编码系统的实际部署提供了可扩展的机构就绪解决方案。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [50] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种名为Mixed Advantage Policy Optimization (MAPO)的新策略，解决了GRPO中存在的优势函数反转和镜像问题，通过动态重加权优势函数来适应不同轨迹确定性的样本。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在优势函数分配上存在优势反转和优势镜像问题，阻碍了不同查询样本间合理的优势分配，影响了基础模型在推理任务上的性能。

Method: 提出MAPO策略，识别轨迹具有不同确定性，为高确定性轨迹样本引入优势百分比偏差，并动态重加权优势函数以适应不同轨迹确定性的样本特征。

Result: 与相关最先进方法的比较以及不同优势变体的消融研究验证了该方法的有效性。

Conclusion: MAPO是一种简单但有效的GRPO策略，能够通过动态调整优势函数来更好地处理不同确定性的轨迹样本，提升基础模型的推理性能。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [51] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 提出了ProfileBench基准和Conf-Profile框架，通过置信度驱动的两阶段方法解决用户画像中的标签稀缺和噪声问题，显著提升了LLM在用户画像任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 用户画像作为用户理解的核心技术，LLMs提供了有前景的解决方案，但缺乏全面基准和大规模真实标签的挑战阻碍了进展。

Method: 提出Conf-Profile框架：第一阶段利用高级LLMs合成高质量标签，通过置信度加权投票和校准；第二阶段通过置信度引导的无监督强化学习增强推理能力。

Result: 实验结果表明Conf-Profile通过两阶段训练显著提升性能，在Qwen3-8B模型上F1分数提高了13.97。

Conclusion: 该方法有效解决了用户画像中的标签稀缺和噪声问题，为无标签可靠用户画像提供了可行方案。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [52] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 本文提出了一个统一的LLM记忆操作定义和四部分分类法（参数化、上下文、外部、程序性/情景性），建立了记忆四元组框架，并设计了分层评估协议来标准化LLM记忆研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM记忆研究中缺乏统一操作定义、评估标准不一致以及治理框架缺失的问题，需要建立一个可复现、可比较和可治理的坐标系统。

Method: 采用三设置协议（仅参数化、离线检索、在线检索）来解耦能力与信息可用性，构建分层评估框架，并开发DMM Gov治理系统协调多种技术。

Result: 建立了包含时间治理、泄漏审计和不确定性报告的综合框架，提出了四个可测试命题，为LLM记忆研究提供了标准化基础。

Conclusion: 该框架为LLM记忆的研究和部署提供了可复现、可比较和可治理的系统性方法，有助于推动该领域的标准化发展。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [53] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个5600亿参数的开源MoE推理模型，通过精心设计的训练流程实现高效推理能力，包括长链思维数据冷启动和大规模强化学习，在复杂推理任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的大规模开源推理模型，解决现有模型在复杂推理任务中的效率问题，特别是减少智能体推理中的token消耗。

Method: 采用混合专家架构，通过长链思维数据冷启动训练策略增强推理潜力，然后使用领域并行训练方案分离不同领域的优化，最后通过DORA系统进行大规模强化学习训练。

Result: 在复杂推理任务上达到开源模型的最先进性能，在AIME-25上智能体推理平均token消耗减少64.5%（从19,653降至6,965），且不降低任务准确率。

Conclusion: LongCat-Flash-Thinking展示了高效推理模型的可行性，为推理系统和智能体AI研究提供了重要贡献，模型已开源以促进进一步研究。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [54] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 本文系统研究了视觉语言模型中的视觉空间推理能力，提出了空间智能的三级分类框架，并创建了包含20个数据集的SIBench基准测试。研究发现当前模型在基础感知任务上表现良好，但在空间理解和规划任务上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 视觉空间推理是人类核心认知能力，也是推进具身智能和自主系统的关键要求。尽管视觉语言模型取得进展，但由于三维空间表示和推理的复杂性，实现人类水平的视觉空间推理仍极具挑战。

Method: 系统回顾了现有方法，包括输入模态、模型架构、训练策略和推理机制。将空间智能分为基础感知、空间理解、空间规划三个能力层级，并构建了SIBench基准测试，涵盖23个任务设置的近20个开源数据集。

Result: 实验表明，最先进的视觉语言模型在感知和推理之间存在显著差距：模型在基础感知任务上表现良好，但在理解和规划任务上持续表现不佳，特别是在数值估计、多视图推理、时间动态和空间想象方面。

Conclusion: 这些发现强调了实现空间智能仍面临的重大挑战，同时为未来研究提供了系统路线图和全面基准测试。研究资源可在指定网址获取。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [55] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: DEAL框架通过结合LoRA和持续微调策略，解决了传统微调方法中的灾难性遗忘和数据效率低下的问题，在15个数据集上表现出优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法存在灾难性遗忘和次优数据效率的问题，限制了其在现实世界中的应用。

Method: 提出DEAL框架，整合低秩适应（LoRA）与持续微调策略，包含知识保留和自适应参数更新模块。

Result: 在15个不同数据集上的实验表明，DEAL在任务准确性和资源效率方面显著优于基线方法。

Conclusion: 该方法通过提升任务性能和资源效率，展现了在LLMs中推进持续适应的潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [56] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 这是第一篇关于LLM-based agents中幻觉问题的全面综述，提出了新的分类法识别不同阶段出现的幻觉类型，深入分析了18种触发原因，并总结了幻觉缓解和检测方法。


<details>
  <summary>Details</summary>
Motivation: LLM-based agents在现实应用中展现出巨大潜力，但容易产生幻觉问题，影响系统可靠性。需要系统性地理解和整合相关研究进展。

Method: 通过仔细分析agents的完整工作流程，提出新的分类法；深入分析18种幻觉触发原因；通过大量现有研究总结幻觉缓解和检测方法。

Result: 建立了系统的幻觉分类框架，识别了不同阶段出现的幻觉类型，总结了有效的缓解策略和检测技术。

Conclusion: 这项调查为未来研究提供了方向，有助于开发更鲁棒可靠的agent系统，希望激发更多解决LLM-based agents幻觉问题的努力。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [57] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文比较了四种剩余时间预测方法在物流公司出库仓库流程中的性能，发现深度学习模型准确率最高，但浅层方法（如传统提升技术）也能达到竞争性准确率且计算资源需求显著更低。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测正在进行的流程执行的未来，其中剩余时间预测是常见目标。本文旨在在真实物流公司出库流程中比较不同剩余时间预测方法的性能。

Method: 在航空物流公司的真实出库仓库流程中，使用包含169,523条轨迹的原始事件日志，比较了四种剩余时间预测方法，特别关注深度学习模型与传统提升技术等浅层方法的对比。

Result: 深度学习模型达到最高准确率，但浅层方法（如传统提升技术）也能达到竞争性准确率，且计算资源需求显著更少。

Conclusion: 虽然深度学习在剩余时间预测方面表现最佳，但浅层方法在准确率和计算效率之间提供了更好的平衡，特别是在资源受限的实际应用场景中具有重要价值。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [58] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: 该论文提出了基于玩家视角的Landmarks、Monuments和Beacons概念，用于自动分解程序生成内容，以改进算法评估与人类体验的对齐。


<details>
  <summary>Details</summary>
Motivation: 算法评估程序生成内容时难以找到与人类体验一致的指标，特别是对于复合产物。需要能够满足多种属性的概念来实现自动分解。

Method: 借鉴游戏研究和游戏AI研究，引入基于可感知性、唤起性和行动召唤的嵌套概念：Landmarks、Monuments和Beacons。这些概念是通用的，可跨游戏类型使用。

Result: 论证了这些实体可以使用当前研究和工业界的技术进行发现和评估，为完全自动化的PCG分解和重要子组件评估开辟了道路。

Conclusion: 该方法旨在连接人文学科和技术游戏研究，实现更好的计算PCG评估，虽然重点强调混合主动PCG和组合PCG，但相信其应用范围更广。

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [59] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: 该论文提出了一个使用可观测源作为辅助变量的因果表示学习框架，能够识别整个潜在变量，并通过变量选择方案优化潜在因素的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习框架通常将辅助变量限制为混合函数外部的变量，但在某些情况下，系统驱动的潜在因素可以被观测或从数据中提取，这可能有助于识别。

Method: 引入可观测源作为辅助变量的框架，使用保体积编码器识别潜在变量，并提供变量选择方案来选择最大化潜在因素可恢复性的辅助变量。

Result: 实验证明该框架在合成图和图像数据上有效，能够识别整个潜在变量到子空间变换和排列的程度。

Conclusion: 该框架扩展了当前方法的边界，通过利用可观测源作为辅助变量，提高了因果表示学习的识别能力。

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [60] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC提出了一种基于代码驱动规划和领域自适应批评器的LLM规划方法，通过生成多样化高层规划程序并利用训练好的批评器评估候选方案，在减少查询成本的同时提升长期奖励对齐的规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划方法依赖频繁查询进行迭代优化，导致高查询成本且难以实现长期奖励对齐。需要一种既能减少查询次数又能保证规划质量的解决方案。

Method: CoPiC使用LLM生成多样化高层规划程序，通过迭代产生和优化候选计划，然后由训练好的领域自适应批评器评估并选择最符合长期奖励的计划执行。

Result: 在ALFWorld、NetHack和StarCraft II Unit Building三个环境中，CoPiC相比AdaPlanner和Reflexion基线平均提升23.33%的成功率，同时减少91.27%的查询成本。

Conclusion: CoPiC通过代码驱动规划和领域自适应批评器的结合，有效解决了LLM规划中的查询成本高和长期奖励对齐问题，在多个复杂环境中展现出优越性能。

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [61] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit是一种多智能体系统初始化方法，通过优化智能体团队结构来提升系统性能，结合自然语言格式化机制和帕累托平衡选择策略，在多种任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MAS初始化方法未能充分考虑生成智能体在后续阶段的协作需求，需要一种能够优化智能体团队结构的方法来提升系统效率和效果。

Method: AgentInit包含多轮智能体交互和反思，采用自然语言到格式的机制确保一致性，并应用帕累托原则的平衡团队选择策略来兼顾团队多样性和任务相关性。

Result: 实验表明AgentInit在各种框架和任务中始终优于最先进的初始化方法和预定义策略，整体性能提升分别达1.2和1.6倍，同时显著减少token消耗。

Conclusion: AgentInit具有良好的可迁移性，关键组件有效性得到验证，证明其作为可靠MAS初始化方法的能力和适应性。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [62] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: 本文研究了LLMs在阿拉伯世界的跨文化常识推理迁移，发现仅需12个特定文化示例就能平均提升10%性能，且来自印尼和美国的文化外演示也能实现有效迁移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在西方中心偏见，在多元文化环境中效果有限。虽然已有研究探索文化对齐，但跨文化迁移（利用一种文化的对齐来提升其他文化性能）的研究仍不足。

Method: 使用覆盖13个阿拉伯国家的文化基础常识推理数据集，评估轻量级对齐方法（上下文学习、演示强化DITTO）以及监督微调和直接偏好优化等基线方法。

Result: 结果显示，在多语言模型中，仅需来自一个国家的12个文化特定示例就能在其他国家平均提升10%性能。来自印尼和美国的跨文化演示在选择题推理中也能达到或超越文化内对齐效果。

Conclusion: 研究表明高效的跨文化对齐是可行的，为LLMs适应低资源文化环境提供了有前景的方法。

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [63] [Precoloring extension with demands on paths](https://arxiv.org/abs/2509.18936)
*Arun Kumar Das,Michal Opler,Tomáš Valla*

Main category: cs.DS

TL;DR: 本文研究了路径上的距离预着色扩展需求问题（DPED），提出了多项式时间精确算法和近似算法，证明了问题的NP完全性，并分析了参数化复杂度，包括W[1]-难性和固定参数可解性。


<details>
  <summary>Details</summary>
Motivation: 该问题源于商业广播频道中的节目调度，需要考虑内容重复和放置的约束条件，这正好对应路径上的DPED问题。

Method: 针对路径上的DPED问题，开发了当预着色顶点限制在路径两端时的多项式时间精确算法，以及具有加性近似因子的近似算法。通过归约证明了问题的NP完全性，并分析了参数化复杂度。

Result: 证明了DPED在路径上是NP完全的，且当参数化为颜色数和距离时是W[1]-难的。同时设计了当参数为颜色数、距离和预着色顶点数时的FPT算法。

Conclusion: 路径上的DPED问题在一般情况下是难解的，但在特定参数设置下存在高效算法，为实际应用提供了理论指导。

Abstract: Let $G$ be a graph with a set of precolored vertices, and let us be given an
integer distance parameter $d$ and a set of integer demands $d_1,\dots,d_c$.
The Distance Precoloring Extension with Demands (DPED) problem is to compute a
vertex $c$-coloring of $G$ such that the following three conditions hold: (i)
the resulting coloring respects the colors of the precolored vertices, (ii) the
distance of two vertices of the same color is at least $d$, and (iii) the
number of vertices colored by color $i$ is exactly $d_i$. This problem is
motivated by a program scheduling in commercial broadcast channels with
constraints on content repetition and placement, which leads precisely to the
DPED problem for paths.
  In this paper, we study DPED on paths and present a polynomial time exact
algorithm when precolored vertices are restricted to the two ends of the path
and devise an approximation algorithm for DPED with an additive approximation
factor polynomially bounded by $d$ and the number of precolored vertices. Then,
we prove that the Distance Precoloring Extension problem on paths, a less
restrictive version of DPED without the demand constraints, and then DPED
itself, is NP-complete. Motivated by this result, we further study the
parameterized complexity of DPED on paths. We establish that the DPED problem
on paths is $W[1]$-hard when parameterized by the number of colors and the
distance. On the positive side, we devise a fixed parameter tractable (FPT)
algorithm for DPED on paths when the number of colors, the distance, and the
number of precolored vertices are considered as the parameters. Moreover, we
prove that Distance Precoloring Extension is FPT parameterized by the distance.
As a byproduct, we also obtain several results for the Distance List Coloring
problem on paths.

</details>


### [64] [GraphBLAS Mathematical Opportunities: Parallel Hypersparse, Matrix Based Graph Streaming, and Complex-Index Matrices](https://arxiv.org/abs/2509.18984)
*Hayden Jananthan,Jeremy Kepner,Michael Jones,Vijay Gadepally,Michael Houle,Peter Michaleas,Chasen Milner,Alex Pentland*

Main category: cs.DS

TL;DR: 本文正式开发了并行超稀疏矩阵、基于矩阵的图流和复索引矩阵，并通过示例展示了这些概念的潜在价值。


<details>
  <summary>Details</summary>
Motivation: GraphBLAS高性能库标准不仅能够用线性代数语言表达图算法，还催生了新的高性能算法思维方式，包括利用超稀疏矩阵进行并行计算、基于矩阵的图流和复索引矩阵。将这些概念数学形式化可以为GraphBLAS在新领域的应用提供更多机会。

Method: 通过数学形式化方法，系统地开发了并行超稀疏矩阵、矩阵基图流和复索引矩阵的概念框架，并配以各种示例进行说明。

Result: 展示了这些新概念在并行计算和图算法中的潜在优势，为GraphBLAS在更广泛领域的应用奠定了基础。

Conclusion: GraphBLAS的这些扩展能力为高性能图计算提供了新的算法思维方式和实现途径，具有重要的理论和实践价值。

Abstract: The GraphBLAS high performance library standard has yielded capabilities
beyond enabling graph algorithms to be readily expressed in the language of
linear algebra. These GraphBLAS capabilities enable new performant ways of
thinking about algorithms that include leveraging hypersparse matrices for
parallel computation, matrix-based graph streaming, and complex-index matrices.
Formalizing these concepts mathematically provides additional opportunities to
apply GraphBLAS to new areas. This paper formally develops parallel hypersparse
matrices, matrix-based graph streaming, and complex-index matrices and
illustrates these concepts with various examples to demonstrate their potential
merits.

</details>


### [65] [Optimization of Base-n Radix Sort for Skewed Datasets](https://arxiv.org/abs/2509.19021)
*Atharv Pandey,Lakshmanan Kuppusamy*

Main category: cs.DS

TL;DR: 本文分析了一种非比较排序算法Base-n Radix Sort (BNRS)，并提出了优化版本Stable Logical Partition Radix Sort (SLPR)，在特定整数分布下（当k < nlog₂n时）比传统比较排序算法更高效。


<details>
  <summary>Details</summary>
Motivation: 排序算法的重要性不言而喻，本文旨在分析非比较排序算法BNRS，并通过优化提升其在特定数据分布下的性能。

Method: 提出了SLPR优化算法，使用原地稳定分区技术减少每轮处理的问题规模，特别适用于包含大量小数和少量大数的偏斜数据集。

Result: 研究表明，当k < nlog₂n时，BNRS和SLPR算法在时间复杂度上比传统比较排序算法更简洁高效。SLPR优化对偏斜数据集排序效果显著。

Conclusion: SLPR算法通过优化分区策略，在特定整数分布条件下提供了比传统排序算法更高效的排序解决方案。

Abstract: The importance and applications of sorting is apparent and needs no
explanation. In this paper, we analyse a non-comparison sorting algorithm,
Base-n Radix Sort (BNRS) and introduce an optimized vari- ant of BNRS, namely,
Stable Logical Partition Radix Sort (SLPR). The complexity of these algorithms
is measured by the input size $n$ and the maximum value $k$. We show that with
respect to time complexity, these algorithms are more succinct than traditional
comparison-based sorting algorithms for representing the sorted order of
certain integer distribu- tions, specifically, when $k <nlog_2^n$ is met. We
also show that the SLPR optimization, which uses in-place stable partitioning
to reduce the active problem size in each pass, resulting in highly effective
sorting for skewed datasets that contain a majority of small numbers and mix of
very large numbers.

</details>


### [66] [Linear Regression under Missing or Corrupted Coordinates](https://arxiv.org/abs/2509.19242)
*Ilias Diakonikolas,Jelena Diakonikolas,Daniel M. Kane,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.DS

TL;DR: 本文研究了高斯协变量下的多元线性回归，在数据可能被对手按坐标预算删除或损坏的两种设置下。主要贡献是刻画了在整个参数范围内可实现误差的信息论下界，并发现缺失数据设置的最优误差与损坏设置相同。


<details>
  <summary>Details</summary>
Motivation: 研究在对抗性缺失或损坏数据下的线性回归问题，这类问题即使在信息论层面也理解不足，现有工作主要关注干净数据设置。

Method: 建立了信息论下界来分析可实现误差，并将这些下界与（计算高效的）算法的误差进行匹配。

Result: 发现缺失数据设置的最优误差与损坏设置相同，表明知道损坏位置并不能带来一般性优势。

Conclusion: 对抗性缺失数据下的线性回归最优误差在整个参数范围内可被精确刻画，且缺失和损坏设置具有相同的误差界限。

Abstract: We study multivariate linear regression under Gaussian covariates in two
settings, where data may be erased or corrupted by an adversary under a
coordinate-wise budget. In the incomplete data setting, an adversary may
inspect the dataset and delete entries in up to an $\eta$-fraction of samples
per coordinate; a strong form of the Missing Not At Random model. In the
corrupted data setting, the adversary instead replaces values arbitrarily, and
the corruption locations are unknown to the learner. Despite substantial work
on missing data, linear regression under such adversarial missingness remains
poorly understood, even information-theoretically. Unlike the clean setting,
where estimation error vanishes with more samples, here the optimal error
remains a positive function of the problem parameters. Our main contribution is
to characterize this error up to constant factors across essentially the entire
parameter range. Specifically, we establish novel information-theoretic lower
bounds on the achievable error that match the error of (computationally
efficient) algorithms. A key implication is that, perhaps surprisingly, the
optimal error in the missing data setting matches that in the corruption
setting-so knowing the corruption locations offers no general advantage.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 使用图像机器学习模型分析Ulam螺旋中不同区域的质数分布规律性，发现在500m附近的区域比25m以下的区域更容易学习到质数模式，表明高数量级区域存在更有序的质数分布。


<details>
  <summary>Details</summary>
Motivation: 质数在定义上是确定性的，但表现出类似随机过程的统计行为。研究旨在探索机器学习能否作为数论的新实验工具，特别是用于分析质数分布的模式。

Method: 采用图像聚焦的机器学习模型，在Ulam螺旋的不同区域提取数据块进行训练和比较，分析模型在不同区域的分类准确率、精确率和召回率。

Result: 在500m附近区域训练的模型性能优于25m以下区域，表明高数量级区域存在更易学习的质数模式。模型在不同区域采用不同的分类策略：低数量级区域侧重识别质数模式，高数量级区域侧重排除合数。

Conclusion: 机器学习可作为数论研究的新实验工具，特别适用于研究强质数和弱质数的模式，对密码学应用具有潜在价值。研究支持数论猜想，即在更高数量级上质数分布的噪声会减少，平均规律性增强。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [68] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出了一个基于Wasserstein距离的联邦学习框架，用于解决数据市场中数据估值和选择的挑战，通过预测模型性能和数据兼容性来提升联邦学习模型交易的可信度。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，数据市场需要可信的模型交易解决方案。联邦学习虽然能保护数据隐私，但在异构数据源的有效估值和选择方面仍存在挑战。

Method: 提出基于Wasserstein距离的估计器框架，包括分布式近似Wasserstein距离的隐私保护方法，并利用神经缩放定律外推模型性能。

Result: 在标签偏斜、错误标签和无标签等多种场景下的实验表明，该方法能一致识别高性能数据组合。

Conclusion: 该框架为构建更可靠的基于联邦学习的模型市场铺平了道路，解决了数据异构性和算法兼容性的关键问题。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [69] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该研究比较了完全学习的神经ODE和物理信息通用微分方程在连续时间库存动态预测中的表现，发现在结构化的需求模式下UDE泛化更好，而在重尾分布下NODE更灵活。


<details>
  <summary>Details</summary>
Motivation: 研究结构偏差在不同需求机制下对牛鞭效应预测的帮助或阻碍作用，为科学和工程系统中的混合建模提供指导。

Method: 使用单级测试平台，在三种需求机制（AR(1)、i.i.d.高斯、重尾对数正态）下比较NODE和UDE方法，通过变化训练轨迹比例评估多步预测性能。

Result: 在结构化需求机制下，UDE泛化能力更强：使用90%训练数据时，库存RMSE从NODE的4.92降至UDE的0.26（AR(1)），从5.96降至0.95（高斯需求）；在重尾对数正态冲击下，NODE的灵活性更好。

Conclusion: 当噪声为轻尾或时间相关时应强制执行结构约束，当极端事件主导时应放松结构约束。这为科学和工程系统中的混合建模提供了具体指导。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [70] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型的迁移学习方法，使用神经网络代理模型，使在一个桥梁上训练的模型能够适应具有相似特征的另一个桥梁，以解决大型桥梁网络中结构评估的可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着永久监测系统的广泛应用，数据可用性增加，为结构评估提供了新机会，但在大型桥梁网络中面临可扩展性挑战。管理多个结构需要高效跟踪和比较长期行为，因此相似结构之间的知识转移变得至关重要。

Method: 采用基于模型的迁移学习方法，使用神经网络代理模型。将在一个桥梁上训练的模型迁移到具有相似特征的另一个桥梁上，这些模型捕捉共享的损伤机制。验证时将迁移模型集成到贝叶斯推理框架中，基于监测数据的模态特征进行连续损伤评估。

Result: 使用两个桥梁的真实数据进行验证，结果显示该方法对损伤位置、严重程度和范围具有高敏感性。

Conclusion: 该方法增强了实时监测能力，实现了跨结构知识转移，促进了智能监测策略，提高了网络层面的韧性。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [71] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ是一种新的向量压缩技术，通过非线性量化方法在高维嵌入向量压缩中实现高保真度和计算效率


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量在存储和检索时存在成本高、效率低的问题，需要开发更高效的压缩技术

Method: 使用新颖的简约非线性方法构建非均匀向量量化器，每个索引向量单独学习量化器

Result: 实验结果显示NVQ在最小计算成本下相比现有技术提高了准确率

Conclusion: NVQ是一种在高保真度场景下计算和空间效率都很高的向量压缩技术

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [72] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出AdaMixT架构，通过多尺度专家变换器和自适应权重融合机制解决现有时间序列预测方法在捕捉复杂模式方面的局限性


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法主要依赖预定义的单尺度补丁或缺乏有效的多尺度特征融合机制，无法充分捕捉时间序列中的复杂模式，导致性能受限和泛化能力不足

Method: AdaMixT引入多种补丁，利用通用预训练模型和领域特定模型进行多尺度特征提取，并通过门控网络动态分配不同专家权重，实现自适应多尺度融合

Result: 在八个广泛使用的基准数据集上的综合实验表明，AdaMixT在真实场景中具有显著有效性

Conclusion: AdaMixT通过自适应多尺度专家融合机制，能够更好地捕捉时间序列的复杂模式，提升预测性能和泛化能力

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [73] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen是一个基于生成式AI和势博弈理论的协作学习框架，用于解决跨机构联邦学习中的统计异质性和经济竞争问题，通过数据生成策略最大化社会福利。


<details>
  <summary>Details</summary>
Motivation: 现有的跨机构联邦学习主要关注统计异质性，但忽略了机构间的经济竞争关系。竞争机构可能因担心效用损失而不愿参与联合训练，且统计异质性和竞争对组织行为和系统福利的联合影响尚未充分研究。

Method: CoCoGen框架通过学习和效用建模来表征竞争和统计异质性，将每个训练轮次建模为加权势博弈，并推导基于生成式AI的数据生成策略来最大化社会福利。

Result: 在Fashion-MNIST数据集上的实验结果表明，CoCoGen能够有效分析不同异质性和竞争水平对组织行为的影响，并且始终优于基线方法。

Conclusion: CoCoGen为异构和竞争环境下的协作学习提供了有效的建模、分析和优化方法，通过生成式AI和博弈论解决了跨机构联邦学习中的关键挑战。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [74] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源的模块化框架，利用大语言模型进行迭代式算法解决方案生成，集成了生成、测试、分析和评估功能。


<details>
  <summary>Details</summary>
Motivation: 简化算法设计过程，通过抽象提示设计和模型管理的复杂性，为研究人员和从业者提供透明可扩展的平台。

Method: 采用多LLM协同架构，将LLM分配为生成器、分析师和评估器等互补角色，构建可重复的反馈循环。

Result: 开发了一个支持用户完全控制错误处理、分析和质量评估的框架，支持跨领域的算法和生成解决方案协同设计。

Conclusion: EASE框架通过模块化设计和多LLM协同，有效降低了算法生成的门槛，为生成式解决方案研究提供了实用工具。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [75] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 基于AIS轨迹的轻量级特征可实现海峡中船舶类型的实时分类，随机森林模型在测试集上达到92.15%的准确率


<details>
  <summary>Details</summary>
Motivation: 准确识别船舶类型对于安全监管和打击非法、未报告和无管制(IUU)活动至关重要

Method: 使用机器学习管道分析AIS数据，提取31个轨迹级特征(运动学、时空、地理空间和船形属性)，采用分组训练/测试分割和分层5折交叉验证

Result: 随机森林模型在五类船舶分类任务中达到92.15%准确率，宏精度94.11%，宏召回率92.51%，宏F1分数93.27%

Conclusion: 基于AIS轨迹的轻量级特征能够实现海峡中船舶类型的实时分类，桥位比和最大航速是最具区分性的特征

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [76] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于patch的PCA-Net框架，通过将解场分解为小块并在每个patch内应用PCA来降低计算复杂度，相比全局PCA可将端到端处理时间减少3.7-4倍。


<details>
  <summary>Details</summary>
Motivation: 解决在高维解场上应用主成分分析(PCA)时计算开销过大的问题，提高神经算子学习在偏微分方程求解中的计算效率。

Method: 提出两种patch-based PCA方法：局部到全局patch PCA和局部到局部patch PCA，并探索了重叠patch加平滑滤波器和CNN精炼两种改进策略。

Result: 基于patch的PCA显著降低了计算复杂度，同时保持了高精度，端到端管道处理时间比全局PCA减少了3.7-4倍。

Conclusion: patch-based PCA是一种有前景的技术，可在PDE系统中实现高效的算子学习，在计算效率和重构精度之间取得了良好平衡。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [77] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种基于上下文优化的新框架，将子空间表示学习与提示调优相结合，用于提升大尺度视觉语言模型在少样本OOD检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD检测方法仅依赖softmax概率，忽略了视觉语言模型在数百万样本上学习到的丰富特征嵌入的判别潜力。

Method: 通过将ID特征投影到提示向量张成的子空间，同时将ID无关特征投影到正交零空间，设计了一个易于处理的端到端学习准则。

Result: 在真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 该方法通过整合子空间表示学习和提示调优，显著提升了ID-OOD的可分离性，同时保持了高ID分类准确率。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [78] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 该研究首次系统比较了用于自动产前CTG分析的最先进AI方法，发现微调后的LLMs在性能上优于基础模型和领域特定方法，为临床CTG解读提供了有前景的替代路径。


<details>
  <summary>Details</summary>
Motivation: 电子胎儿监护(EFM)/胎心监护(CTG)分析是评估胎儿健康的关键技术，但目前基础模型和LLMs在该领域的潜力尚未充分探索。CTG解读存在主观性强、诊断准确性变异大等问题，需要自动化解决方案。

Method: 系统比较时间序列基础模型、LLMs与已建立的CTG特定架构，评估超过500个不同时长的CTG记录，涵盖真实临床记录场景。

Result: 微调后的LLMs在性能上表现最优，超越了基础模型和领域特定方法。

Conclusion: 研究结果为胎儿监护应用中不同AI方法的相对优势提供了关键见解，为产前护理领域未来临床AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [79] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 本文提出了一种基于BlueField-3 DPU的框架，用于实时检测和缓解多节点张量并行推理中的负载不平衡问题，通过卸载监控任务到DPU来分析GPU遥测和节点间通信模式。


<details>
  <summary>Details</summary>
Motivation: 大型基于transformer的语言模型在自回归推理过程中面临运行时效率挑战，特别是在解码阶段，GPU分片间的负载不平衡会导致吞吐量下降和延迟峰值。

Method: 利用DPU辅助框架，将监控任务卸载到DPU，分析GPU遥测数据和节点间通信模式，为推理控制器和调度器提供可操作的反馈。

Result: 研究旨在识别多GPU执行LLM张量计算时出现的偏斜/不平衡/病态条件，评估其对计算性能的影响，并评估这些条件是否可以从DPU网络进行跟踪和缓解。

Conclusion: DPU辅助框架能够有效检测和缓解负载不平衡问题，提升大型语言模型推理的运行时效率。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [80] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间平衡注意力块用于时空预测，通过将空间图划分为子图来实现局部和全局空间相关性的平衡捕捉，并构建了多尺度预测模型。


<details>
  <summary>Details</summary>
Motivation: 为了在遵循空间邻近性和捕捉全局相关性之间取得平衡，解决现有方法在空间相关性建模方面的不足。

Method: 将空间图划分为子图，使用子图内注意力学习局部空间相关性，通过子图间注意力实现子图间的消息传递以捕捉全局相关性，并构建多尺度模型。

Result: 在现实世界的中大型时空数据集上，相比基线方法性能提升最高达7.7%，且运行成本较低。

Conclusion: 所提出的模型既具有可扩展性，又能产生结构化的空间相关性，且易于实现，在效率和效果上都表现优异。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [81] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: ALS（摊销潜在引导）通过在推理时应用离线计算的单一向量来替代昂贵的逐查询优化循环，实现了2-5倍的加速，同时匹配或超越了贪婪CoT和自一致性基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时优化方法（如迭代优化和多步验证）计算成本过高，难以大规模应用。潜在空间优化方法如LatentSeek虽然更直接，但仍需要昂贵的逐查询优化循环。

Method: ALS计算成功与失败生成之间隐藏状态的平均差异，然后使用这个方向来校准模型的隐藏表示：当解码偏离成功流形时，ALS将激活值推回该流形。

Result: 在GSM8K和MATH-500基准测试中，ALS实现了2-5倍的加速，同时匹配或超越了贪婪CoT和自一致性基线，效率-准确率权衡提升了高达101%。

Conclusion: 研究表明，潜在优化的许多好处可以通过离线方式捕获，使得复杂的推理技术能够用于生产部署。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [82] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 该论文提出了一种基于机器学习的数字界面设计方法，能够动态适应不同用户和使用策略。算法使用贝叶斯统计建模用户浏览行为，关注个人习惯而非群体偏好，具有在线增量学习能力。


<details>
  <summary>Details</summary>
Motivation: 设计能够个性化适应不同用户习惯的数字界面，提升用户体验，帮助用户更有效地导航和操作界面。

Method: 使用贝叶斯统计方法建模用户浏览行为，采用在线增量学习算法，能够在小数据量和环境变化的情况下进行可靠预测，生成任务模型并提供图形化导航表示。

Result: 仿真实验表明该方法在静态和非静态环境中都表现有效，能够学习新任务同时保留先验知识。

Conclusion: 这项研究为开发能够改善用户体验的自适应系统奠定了基础，使界面能够更好地适应用户的个性化需求。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [83] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 本文提出了一种针对RISC-V微控制器的轻量级SGD优化算法，通过8位量化技术实现了内存使用减少4倍和训练速度提升2.2倍，解决了RISC-V平台缺乏FPU支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现代IoT设备缺乏GPU或专用加速器，使得本地训练不可行，而云端训练存在隐私和连接依赖问题。联邦学习需要高效优化算法，但RISC-V架构目前缺乏对设备端训练的稳健支持。

Method: 将L-SGD（轻量级随机梯度下降）扩展到RISC-V MCU平台，开发了8位量化版本的L-SGD算法，以应对RISC-V MCU缺乏浮点运算单元的限制。

Result: 在RISC-V平台上，8位量化L-SGD实现了内存使用减少近4倍，训练速度提升2.2倍，同时精度损失可忽略不计。

Conclusion: 该工作证明了在资源受限的RISC-V MCU上实现高效设备端训练的可行性，为联邦学习在开放架构上的应用提供了重要支持。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [84] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个用于移动GUI代理的在线强化学习框架，通过难度自适应算法和奖励调整策略，在AndroidWorld和AndroidLab上实现了最先进的成功率。


<details>
  <summary>Details</summary>
Motivation: 开发有效的移动GUI代理面临任务难度分布重尾和大规模环境采样效率低下的挑战，需要新的强化学习方法来提升性能。

Method: 提出ADAGRPO算法，包含难度自适应正回放、失败课程过滤和最短路径奖励调整策略，应用于Qwen2.5-VL-7B-Instruct和GLM-4.1V-9B-Base模型。

Result: MOBILERL-9B模型在AndroidWorld上达到75.8%的成功率，在AndroidLab上达到46.8%的成功率，均达到最先进水平。

Conclusion: MOBILERL框架通过难度自适应策略稳定了RL训练，提高了样本效率，在多样化移动应用和任务中表现出色，已被集成到AutoGLM产品中并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [85] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 本研究应用监督机器学习算法预测咖啡评分，通过文本和数值特征分析，发现集成方法和多层感知器在预测性能上优于简单分类器。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用用户评论中的文本和数值特征来预测咖啡评分，为传统咖啡品鉴提供数据驱动的补充方法。

Method: 使用TF-IDF进行文本特征提取，SelectKBest进行特征选择，训练六种机器学习模型（决策树、K近邻、多层感知器、随机森林、极端随机树和XGBoost）并进行超参数优化。

Result: 集成方法（极端随机树、随机森林和XGBoost）以及多层感知器在F1分数、G-mean和AUC等评估指标上表现优于决策树和K近邻等简单分类器。

Conclusion: 严格的特征选择和超参数调优对于构建稳健的感官产品评估预测系统至关重要，为传统专业咖啡品鉴提供了数据驱动的补充方法。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [86] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL是一个基于强化学习的护士排班框架，通过结构化状态编码、约束动作掩码和注意力机制，解决传统方法在动态多约束医疗环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 医疗系统面临有限护理资源的高效分配压力，传统优化和启发式方法难以处理技能异质性、患者病情、员工疲劳和护理连续性等多重动态约束。

Method: 使用近端策略优化（PPO）算法，结合可行性掩码确保分配符合现实约束，通过注意力机制表示技能、疲劳和地理上下文，动态适应患者到达和护士可用性变化。

Result: 在真实护士和患者数据的模拟中，NurseSchedRL相比基线启发式和无约束RL方法，实现了更高的排班效率、更好的技能与患者需求匹配以及更低的疲劳度。

Conclusion: 强化学习在复杂高风险医疗人力管理决策支持中具有巨大潜力，能够有效处理动态多约束环境。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [87] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 本文评估了联邦学习在电动汽车充电站异常检测中的性能，特别是在系统异构性和非独立同分布数据下的表现，发现FedAvgM在异构环境下优于FedAvg。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车基础设施的快速扩张，保护基于物联网的充电站免受网络威胁变得至关重要。集中式入侵检测系统存在隐私问题，而联邦学习是很有前景的替代方案，但现有评估忽略了系统异构性和非独立同分布数据等实际问题。

Method: 通过实验评估联邦学习在电动汽车充电站异常检测中的性能，使用FedAvg和FedAvgM两种优化方法，分析它们在异构系统和数据环境下的有效性。

Result: 在独立同分布设置下，FedAvg使用相同神经网络时性能优于集中式模型。但在非独立同分布数据和系统异构性下性能下降。FedAvgM在异构环境下始终优于FedAvg，表现出更好的收敛性和更高的异常检测准确率。

Conclusion: 联邦学习能够处理物联网电动汽车充电站的异构性而不会显著损失性能，FedAvgM是构建稳健、隐私保护的电动汽车充电站安全的有前景解决方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [88] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL是一个用于解释大型语言模型中稀疏自编码器（SAE）特征的框架，旨在提升对安全相关行为的机制理解。


<details>
  <summary>Details</summary>
Motivation: 现有安全研究主要关注评估LLM输出或特定安全任务，无法有效应对更广泛、未定义的风险。SAE虽然有助于解释模型行为，但之前的研究未能将特征与细粒度的安全概念联系起来，无法充分解决安全关键行为问题。

Method: 提出Safe-SAIL框架，系统性地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略来扩展解释过程。

Result: 将发布包含SAE检查点和人类可读神经元解释的全面工具包，支持对安全风险的实证分析。

Conclusion: 该框架有助于促进LLM安全研究，通过机制性理解来识别和缓解高风险行为。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [89] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出一种Gauss-Hermite求积方法，用于解耦机器学习代理模型中的认知不确定性和偶然不确定性，提高可靠性分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在物理可靠性分析中应用广泛，但其模型近似误差引入的认知不确定性会与输入参数的偶然不确定性耦合，影响可靠性预测的准确性。

Method: 使用Gauss-Hermite求积方法，先通过一阶和二阶可靠性方法评估偶然不确定性下的条件失效概率，然后在整个认知不确定性实现范围内积分这些概率。

Result: 三个示例表明，该方法在保持计算效率的同时，比忽略模型不确定性的传统方法产生更可信的预测结果。

Conclusion: 所提出的方法能够有效解耦嵌套不确定性，为基于机器学习代理模型的可靠性分析提供更准确的预测。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [90] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出了一种结合STL时间序列分解和GRU神经网络的模型，用于地铁换乘客流预测，相比传统方法显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 在地铁智能交通系统中，准确的换乘客流预测是优化运营计划、提高运输效率的关键环节，需要改进现有预测理论为智能运营决策提供更可靠支持。

Method: 首先使用Keras构建GRU模型，然后预处理地铁刷卡数据，通过深度优先搜索算法识别乘客出行路径并构建换乘客流时间序列，接着用STL算法将时间序列分解为趋势、周期和残差分量，用3σ原则处理异常值，最后完成预测。

Result: 实验结果表明，相比LSTM、GRU和STL-LSTM模型，STL-GRU模型在工作日（除周五）、周五和休息日的预测精度显著提升，MAPE分别降低了至少2.3、1.36和6.42个百分点。

Conclusion: STL-GRU组合预测模型能有效提高地铁换乘客流预测的准确性，为地铁智能运营决策提供了可靠的技术支持。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [91] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: 论文显示，基于Transformer的机器学习应用在解决物理问题时，其权重矩阵呈现随机性特征，与物理问题的数学结构无直接关联，这表明机器学习与科学方法可能是两种不同但互补的知识获取路径。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习模型（特别是Transformer）在解决物理问题时的内部工作机制，探索其权重矩阵是否反映物理问题的数学结构，以及机器学习与科学方法在知识获取上的关系。

Method: 分析Transformer模型在解决两个代表性物理应用时的权重矩阵特征，观察其与物理问题数学结构的对应关系。

Result: 发现权重矩阵呈现随机性特征，与物理问题的数学结构没有直接可识别的联系，但可以与广义路径积分技术进行类比。

Conclusion: 机器学习与科学方法可能是两种互补的知识获取路径，但严格的解释性（网络参数与物理结构的直接对应）可能难以实现，需要警惕缺乏洞察力的知识获取风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [92] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL是一个参数高效的对抗性专家混合框架，用于工业规模的大语言模型持续指令调优，通过双专家设计解决灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现实工业环境中，大语言模型需要持续学习以适应动态变化的任务，但现有持续学习方法存在灾难性遗忘问题，新任务训练会降低对早期任务的性能

Method: 采用双LoRA专家设计：专用专家保留任务特定知识，共享专家实现跨任务迁移；集成任务感知判别器进行对抗学习，确保共享专家只传递任务相关信息

Result: 在MTL5基准和腾讯工业基准上的实验验证了MoE-CL的有效性，在腾讯视频平台的内容合规审查A/B测试中，将人工审核成本降低了15.3%

Conclusion: MoE-CL适用于大规模工业部署，在持续适应和稳定迁移方面具有实际应用价值

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [93] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文提出了一种加权梯度跟踪的分布式隐私保护算法，解决了梯度跟踪技术中的隐私泄露风险，并在时变异构步长下证明了算法的精确收敛性。


<details>
  <summary>Details</summary>
Motivation: 梯度跟踪技术虽然能提高分布式优化的收敛速度，但存在固有的隐私泄露风险，需要保护智能体在优化过程中的私有信息免受潜在攻击者的侵害。

Method: 提出加权梯度跟踪分布式隐私保护算法，通过衰减权重因子消除梯度跟踪中的隐私泄露风险，并在时变异构步长条件下分析算法收敛性。

Result: 证明了所提算法在温和假设下能精确收敛到最优解，并通过经典分布式估计问题和卷积神经网络分布式训练的数值仿真验证了算法的有效性。

Conclusion: 该算法成功解决了梯度跟踪中的隐私保护问题，为隐私保护的分布式优化提供了有效的解决方案。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [94] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 本文提出了一种静态-动态图融合网络（SDGF），通过双路径图结构学习方法捕捉多尺度时间序列间的相关性，解决现有方法难以建模复杂多尺度依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，序列间的相关性对于准确性至关重要，但这些关系在不同时间尺度上表现出复杂的动态特性。现有方法在建模这些多尺度依赖关系方面存在局限，难以捕捉其复杂且演化的本质。

Method: SDGF模型采用双路径图结构学习方法：1）基于先验知识的静态图用于锚定长期稳定依赖关系；2）通过多级小波分解提取多尺度特征，构建自适应学习的动态图来捕捉不同尺度的关联。设计了注意力门控模块智能融合这两种互补信息源，并使用多核扩张卷积网络加深对时间模式的理解。

Result: 在多个广泛使用的真实世界基准数据集上的综合实验证明了所提出模型的有效性。

Conclusion: SDGF模型能够有效捕捉多尺度时间序列间的复杂动态相关性，在多元时间序列预测任务中表现出优越性能。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [95] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 本文提出了一个大规模数据集，系统分析了大型语言模型（LLMs）的结构配置与性能之间的关系，通过数据挖掘和机制可解释性技术验证了结构选择对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在规模和能力上快速增长，但关于结构配置如何影响性能的系统性、数据驱动研究仍然稀缺。本文旨在填补这一空白。

Method: 构建包含多样化开源LLM结构及其在多个基准测试中性能的大规模数据集，采用数据挖掘驱动的方法进行系统性分析，并使用机制可解释性技术验证发现。

Result: 研究量化了结构配置与性能之间的关系，分析了各种结构选择在不同基准测试中的影响，并通过可解释性技术进一步证实了这些发现。

Conclusion: 这项工作为LLM优化提供了数据驱动的见解，旨在指导未来模型的针对性开发和应用，相关数据集将公开发布。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [96] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出了LoRALib统一基准，标准化了40个下游任务的数据集和超参数，生成了680个LoRA模块，并对3种代表性LoRA-MoE方法进行了大规模实验比较。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA-MoE方法在模型、数据集、超参数和评估方法上缺乏统一标准，难以进行公平比较。

Method: 构建LoRALib基准，标准化40个下游任务数据集，使用相同超参数微调得到680个LoRA模块，基于OpenCompass测试工具对3种LoRA-MoE方法和不同LoRA选择机制进行大规模实验。

Result: 实验表明LoRAMoE方法表现最佳，优先选择与目标任务相关的LoRA能进一步提升MoE性能。

Conclusion: 这些发现将为未来工作提供启发，相关数据集和LoRA库已开源。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [97] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM算法通过利用排名基准与分布基准的结构等价性，直接在排名诱导的Plackett-Luce参数化中更新，确保算法在每轮中保持排名诱导分布的特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在专家身份上操作，而RIPLM旨在直接在排名诱导的PL参数化中更新，以保持与排名基准的等价性。

Method: RIPLM算法利用排名基准与分布基准的结构等价性，在排名诱导的Plackett-Luce参数化中直接更新，确保算法在每轮中保持排名诱导分布的特性。

Result: RIPLM是第一个在睡眠专家设置中既保持排名忠实性又具有方差自适应性的算法。

Conclusion: RIPLM算法通过直接在排名诱导的PL参数化中更新，成功实现了排名忠实性和方差自适应性，为睡眠专家设置提供了新的解决方案。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [98] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: 本研究比较了规则分类器FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。结果表明FOLD-SE在保持可解释性的同时，性能表现优于对比算法。


<details>
  <summary>Details</summary>
Motivation: 随着对兼具准确性、效率和可解释性的机器学习模型需求增长，需要开发能在准确性和可解释性之间取得平衡的新算法。传统模型如神经网络虽然准确性高但缺乏透明度。

Method: 使用分类数据集，以准确率、F1分数和处理时间作为主要性能指标，比较FOLD-SE与FOLD-R++在二分类任务的表现，以及FOLD-SE与XGBoost在多分类任务的表现。

Result: FOLD-SE在二分类中优于FOLD-R++，提供更少的规则但仅损失少量准确性和处理效率；在多分类中比XGBoost更精确高效，同时生成可理解的规则集。

Conclusion: 规则基方法如FOLD-SE能够弥合可解释性与性能之间的差距，是多样化分类任务中黑盒模型的可行替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [99] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本研究开发了一个结合机器学习预测建模和基因无关通路映射的新框架，用于识别2型糖尿病高风险个体并发现潜在治疗靶点。


<details>
  <summary>Details</summary>
Motivation: 代谢性疾病特别是2型糖尿病对遗传易感人群如皮马印第安人造成重大健康负担，需要开发可解释且可扩展的精准医疗解决方案。

Method: 使用皮马印第安人数据集，应用逻辑回归和t检验识别T2DM关键预测因子，开发通路映射策略将预测因子与胰岛素信号、AMPK和PPAR等关键信号网络关联。

Result: 模型整体准确率达到78.43%，通过通路富集分析验证了双GLP-1/GIP受体激动剂、AMPK激活剂、SIRT1调节剂和植物化学物等治疗策略。

Conclusion: 该框架通过提供可解释和可扩展的解决方案，推动了代谢性疾病早期检测和靶向干预的精准医学发展。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [100] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个全自动AI驱动的管道，用于从Kaplan-Meier图中高精度重建个体患者数据，解决了传统手动数字化方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动数字化，容易出错且缺乏可扩展性，需要自动化解决方案来提高临床研究中证据合成的效率和准确性。

Method: KM-GPT集成了先进的图像预处理、GPT-5驱动的多模态推理和迭代重建算法，采用混合推理架构自动化将非结构化信息转换为结构化数据流。

Result: 在合成和真实数据集上的严格评估显示KM-GPT具有卓越的准确性，在胃癌免疫治疗试验的荟萃分析中成功应用，促进了证据合成和基于生物标志物的亚组分析。

Conclusion: KM-GPT通过自动化传统手动流程并提供基于Web的可扩展解决方案，利用重建的个体患者数据支持更明智的下游分析和循证决策，改变了临床研究方式。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [101] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI是一种基于条件扩散模型的新型时空数据插补方法，通过双向S4模型进行预插补，并使用噪声感知时空网络捕获不同扩散步骤中的变异性依赖关系，在三个真实世界数据集上实现了高达46.4%的插补误差降低。


<details>
  <summary>Details</summary>
Motivation: 时空数据常因传感器故障等原因存在缺失值，现有扩散模型方法在提取时空依赖关系作为条件信息时存在误差累积问题，且忽略了噪声数据在不同扩散步骤中依赖关系的变异性。

Method: 提出AdaSTI框架，包含基于双向S4模型的BiS4PI网络进行预插补，设计时空条件化器（STC）网络提取条件信息，以及带有门控注意力机制的噪声感知时空（NAST）网络来捕获跨扩散步骤的变异性依赖关系。

Result: 在三个真实世界数据集上的广泛实验表明，AdaSTI在所有设置下都优于现有方法，插补误差最多降低46.4%。

Conclusion: AdaSTI通过自适应依赖建模有效解决了扩散模型中时空依赖关系的误差累积和变异性问题，显著提升了时空数据插补性能。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [102] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 本研究提出了一种多标签分类框架，用于预测ICU患者的护理升级触发因素（CETs），包括呼吸衰竭、血流动力学不稳定、肾功能损害和神经功能恶化，使用ICU入院前24小时的数据。


<details>
  <summary>Details</summary>
Motivation: 传统的早期预警系统（如SOFA或MEWS）局限于单一结果预测，无法捕捉临床恶化的多维度特征。ICU患者常出现复杂、重叠的生理恶化迹象，需要及时升级护理。

Method: 使用MIMIC-IV数据库，基于规则定义CETs（如血氧饱和度低于90%、平均动脉压低于65 mmHg等）。从ICU入院前24小时提取特征（生命体征汇总、实验室值、静态人口统计学数据），在85,242例ICU住院数据上训练和评估多个分类模型。

Result: XGBoost模型表现最佳，F1分数分别为：呼吸0.66、血流动力学0.72、肾功能0.76、神经系统0.62。特征分析显示呼吸频率、血压和肌酐等临床相关参数是最有影响力的预测因子。

Conclusion: 该框架展示了早期、可解释临床警报的实际潜力，无需复杂的时序建模或自然语言处理。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [103] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow是一个基于概念的可解释性框架，通过追踪概念在CNN各层中的出现和演化来模拟模型的内部"思考路径"。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了单个过滤器的语义角色和概念在层间的动态传播，需要更结构化的模型内部推理视图。

Method: 包含两个关键组件：概念注意力（将过滤器与高层概念关联）和概念路径（通过概念转移矩阵量化概念在过滤器间的传播和转换）。

Result: 实验结果表明ConceptFlow能够产生语义上有意义的模型推理洞察，验证了概念注意力和概念路径在解释决策行为方面的有效性。

Conclusion: 通过建模层次化概念路径，ConceptFlow为CNN内部逻辑提供了更深层次的洞察，支持生成更忠实且与人类对齐的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [104] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏表示的高效训练框架STS，通过视觉令牌压缩器和层动态跳过器来减少多模态大语言模型的训练计算开销


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型训练效率低下，主要由于多模态数据引入的长输入序列和层间计算利用率低

Method: STS框架包含两个关键组件：视觉令牌压缩器（减少视觉令牌信息负载）和层动态跳过器（在前后向传播中动态跳过语言模型中不必要的层）

Result: 该方法在多个基准测试上进行了广泛评估，证明了其有效性和效率

Conclusion: 该稀疏训练方案适用于多种MLLM架构，能够显著提升训练效率

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [105] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS是一种新颖的神经架构搜索预测器，通过全局编码方案和共享超网络增强架构表示学习，在少量样本情况下实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索的性能评估耗时严重，现有神经预测器由于难以捕捉架构间复杂关系而泛化能力差

Method: 提出HyperNAS框架，包含全局编码方案捕获宏观结构信息，共享超网络作为辅助任务增强架构间模式探索，并开发动态自适应多任务损失确保训练稳定性

Result: 在5个代表性搜索空间（包括ViT）上的实验显示，HyperNAS在少量样本场景下优势明显，在CIFAR-10上达到97.60% top-1准确率，ImageNet上达到82.4% top-1准确率，样本使用量减少至少5倍

Conclusion: HyperNAS通过增强架构表示学习，显著提升了神经预测器的性能，特别是在数据稀缺情况下表现出色

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [106] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM是一个用于测井解释的基础模型，通过三阶段训练（标记化、自监督预训练、多任务适应）在1200口井的多曲线测井数据上预训练，在孔隙度估计和岩性分类任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 测井解释面临工具响应异质性、噪声信号和有限标签的挑战，需要开发能够处理这些问题的可扩展AI方法。

Method: 三阶段方法：1）将测井曲线片段标记化为地质标记；2）使用掩码标记建模和地层感知对比学习进行自监督预训练；3）通过少样本微调进行多任务适应。

Result: WLFM在孔隙度估计上达到0.0041 MSE，岩性分类准确率74.13%；微调后分别提升至0.0038 MSE和78.10%准确率。模型还表现出层感知能力和可重用地质词汇学习。

Conclusion: WLFM为地质AI提供了一个可扩展、可解释和可迁移的骨干网络，为测井、地震和文本数据的多模态集成奠定了基础。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [107] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion是一个深度学习框架，通过结合64亿参数蛋白质语言模型和强化学习，实现了抗生素的从头设计。该框架在体外评估中表现出100%的成功率和优异的抗菌活性。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性（AMR）预计到205年每年将导致1000万人死亡，迫切需要开发新的抗生素。现有抗生素开发方法效率低下，需要更快速、可扩展的设计平台。

Method: 使用64亿参数蛋白质语言模型，先在精选肽数据上进行微调以捕捉抗菌序列规律，然后通过近端策略优化结合MIC分类器预测和可微分物理化学目标进行优化。

Result: 体外评估100种设计肽显示所有候选物都具有低MIC值（某些情况下达到纳摩尔范围），100%命中率。99/100化合物对至少两种临床相关细菌表现出广谱抗菌活性。

Conclusion: 通过将生成、评分和多目标优化与深度强化学习统一在单一流程中，该方法能够快速产生多样化、强效的候选物，为肽抗生素提供了可扩展的途径，并在数小时内实现效力和可开发性的迭代优化。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [108] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的多模态大语言模型，通过架构创新、数据策略和训练方法优化，实现了高效能和高效率，在多项评测中超越了GPT-4o-latest等专有模型和更大的开源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的训练和推理效率已成为其普及和扩展的核心瓶颈，需要开发更高效的模型来解决这一问题。

Method: 采用统一的3D-Resampler模型架构实现图像和视频的紧凑编码；使用统一学习范式处理文档知识和文本识别；采用混合强化学习策略支持短推理和长推理模式。

Result: 在OpenCompass评测中超越GPT-4o-latest和Qwen2.5-VL 72B等模型；在VideoMME基准测试中，仅使用Qwen2.5-VL 7B 46.7%的GPU内存和8.7%的推理时间就达到了30B以下模型的最优性能。

Conclusion: MiniCPM-V 4.5证明了通过精心设计的架构和训练策略，小规模模型也能实现超越大型模型的性能，同时显著提升效率。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [109] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 本文提出并比较了9种训练方法，用于探索具有参数化线性B样条激活函数的神经网络中的双重优化动态，相比传统ReLU模型，在FNN中实现了高达94%的误差率降低，在CNN中实现了51%的误差率降低。


<details>
  <summary>Details</summary>
Motivation: 传统的激活函数（如ReLU、tanh、sigmoid）是静态选择的，通过优化激活函数的形状，可以为神经元分配更优的激活函数，从而训练出更参数高效和准确的模型。

Method: 使用参数化线性B样条激活函数，提出并比较了9种训练方法，探索神经网络中的双重优化动态。

Result: 实验结果显示，与传统基于ReLU的模型相比，该方法在FNN中实现了高达94%的误差率降低，在CNN中实现了51%的误差率降低。

Conclusion: 通过优化激活函数形状可以显著提高模型性能，但需要付出额外的开发、训练复杂度以及模型延迟的代价。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [110] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 本文研究卡车与无人机协同的最后一段配送问题，提出混合强化学习方法优化配送时间，在50个客户点实例上比传统ALNS方法提升2.73%。


<details>
  <summary>Details</summary>
Motivation: 解决卡车与无人机协同配送中的电池管理问题，通过精确的时间线模拟和强化学习调度来最小化总完成时间。

Method: 结合ALNS卡车路径规划和基于指针/注意力机制的无人机调度策略，使用精确时间线模拟器确保电池续航和充电约束。

Result: 在N=50、E=0.7、R=0.1的欧几里得实例上，平均完工时间为5.203±0.093，比ALNS方法提升2.73%，与神经网络方法相当。

Conclusion: 混合强化学习方法能有效平衡卡车等待时间和无人机飞行时间，显著提升配送效率，并提供可复现的实现方案。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [111] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: 本文提出DSFT策略，通过调整掩码策略和损失函数来提升扩散大语言模型在数学和逻辑任务上的表现，在小规模数据上实现5-10%和约2%的改进。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在生成方面具有优势，但在学习数值敏感的数学和顺序敏感的逻辑任务时面临挑战，现有训练方法缺乏对数学和逻辑模式的全面理解。

Method: 提出DSFT策略，通过调整掩码策略和损失函数来引导模型理解数学和逻辑模式，可与预训练、强化学习等方法灵活结合。

Result: 在LLaDA和Dream系列模型上验证，DSFT在小规模数据上能在数学问题上实现5-10%的改进，在逻辑问题上实现约2%的改进。

Conclusion: 这种掩码方法为未来学习特定模式提供了思路，可轻松高效地与其他训练方法结合并应用于各种扩散大语言模型。

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [112] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT是一个用于移动数据预测的基础模型，能够统一预测基站流量、用户应用使用和信道质量三种数据类型，通过软提示学习和时序掩码机制实现多任务预测，在真实数据集上表现出优异的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动数据预测方法需要为不同数据类型定制专门模型，在大规模异构网络环境下增加了复杂性和部署成本。需要开发一个统一的基础模型来处理多种移动数据预测任务。

Method: 提出MobiGPT基础模型，采用软提示学习方法帮助模型理解不同数据类型的特征，引入时序掩码机制支持短期预测、长期预测和分布生成三种预测任务。

Result: 在包含10万+样本的真实数据集上评估，MobiGPT相比现有模型预测准确率分别提升27.37%、20.08%和7.27%，在零样本/少样本场景下性能提升超过21.51%。

Conclusion: MobiGPT作为一个基础模型，在移动数据预测方面展现出强大的泛化能力和迁移能力，能够有效支持多样化的优化场景。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [113] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE是一种新的训练和推理架构，通过在神经网络中内在地集成计算能力，实现计算与推理的融合，相比微调LLMs和多智能体系统具有更高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型无法将高精度数值计算作为内在可解释能力集成，而主流多智能体方法存在通信开销大、多模态能力效率低和可扩展性有限的问题。

Method: 提出物理隔离的专家混合模型(PiMoE)，通过分别训练专家、文本到计算模块和路由器，在推理时路由器在token级别指导计算和推理，实现单链思维中的迭代交替。

Result: 在两个推理-计算任务上的评估显示，PiMoE不仅比直接微调LLMs准确率更高，而且在响应延迟、token使用和GPU能耗方面相比主流多智能体方法有显著改进。

Conclusion: PiMoE为下一代科学或工业智能系统提供了高效、可解释和可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [114] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA是一个联邦图学习框架，采用投影优先策略解决领域偏移问题，通过重要性感知的两阶段管道（top-ρ掩码和影响力正则化动量权重）来去噪客户端更新，实现更稳定收敛和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习在领域偏移场景下（如Twitch Gamers和多语言Wikipedia网络），客户端模型趋向不兼容表示，导致朴素聚合方法不稳定且无效。研究发现问题根源是噪声梯度信号而非权重方案。

Method: 提出投影优先策略，在聚合前对客户端更新进行去噪。FedIA框架包含两阶段管道：1）服务器端top-ρ掩码保留约5%最信息丰富的坐标；2）轻量级影响力正则化动量权重抑制异常客户端。

Result: 在同构（Twitch Gamers）和异构（Wikipedia）图上，FedIA相比9个强基线方法实现了更平滑、更稳定的收敛和更高的最终准确率。动态投影保持了最优的O(σ²/√T)收敛速率。

Conclusion: FedIA无需额外上行流量和可忽略的服务器内存开销，易于部署。重要性感知的投影优先策略有效解决了联邦图学习中的领域偏移问题，优于现有聚合方法。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [115] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR是一种新型的大语言模型量化方法，通过高斯分布感知的码本表示和定制CUDA内核，在4位量化下实现最先进的性能，同时获得2.21-3.04倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法存在局限性：RTN方法无法处理LLM权重的类高斯分布，码本方法虽然考虑了分布但存在内存访问模式问题导致推理速度下降。需要一种既能准确压缩又能高效推理的量化方案。

Method: 提出SBVR方法，将权重值映射到非均匀表示点，其分布遵循LLM权重的实际高斯分布。设计定制CUDA内核，支持在SBVR格式下直接进行矩阵向量乘法而无需解压缩。

Result: 在各种模型上的评估显示，SBVR在4位量化下实现了最先进的困惑度和准确度基准性能，同时相比原生FP16模型获得了2.21-3.04倍的端到端token生成加速。

Conclusion: SBVR成功解决了现有量化方法的局限性，通过硬件友好的高斯分布码本表示实现了准确压缩和高效推理的统一，为大语言模型的部署提供了有效的解决方案。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [116] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 该论文提出了一个大规模基准测试，用于评估大语言模型在地理空间路线认知方面的能力，发现LLMs在路线反转任务中存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过自然语言理解地理空间信息，但大语言模型的地理空间认知能力尚未得到充分探索。现有研究受限于不可量化的指标、有限的评估数据集和不清晰的研究层次。

Method: 创建了包含全球12个大都市36000条路线的大规模评估数据集；开发了PathBuilder工具，用于自然语言指令与导航路线之间的双向转换；提出了新的评估框架和指标，对11个SOTA LLMs进行路线反转任务的严格评估。

Result: 基准测试显示LLMs在路线反转方面存在明显限制：大多数反转路线既未返回起点，也与最优路线不相似；LLMs还面临路线生成鲁棒性低和对错误答案置信度高的问题。

Conclusion: 该研究揭示了LLMs在地理空间认知方面的局限性，为未来改进LLMs的地理空间理解能力提供了重要基准和方向。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [117] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 本文提出了多模态思维链（MCoT）框架，用于解决对话式导航中的自我中心到全局中心的空间定向问题，在传统中文环境中实现了高精度的方向推理。


<details>
  <summary>Details</summary>
Motivation: 解决室内或复杂设施中GPS信号弱、详细地图不可用时，对话代理如何将自我中心表达（如"在我右边"）转换为全局方向（N/E/S/W）的挑战。

Method: 提出多模态思维链框架，整合ASR转录语音和地标坐标，通过三步推理过程：提取空间关系、坐标映射到绝对方向、推断用户朝向，并采用课程学习策略在Taiwan-LLM-13B-v2.0-Chat模型上实现。

Result: MCoT在干净转录本上达到100%定向准确率，在ASR转录本上达到98.1%，显著优于单模态和非结构化基线，且在噪声对话条件下表现出鲁棒性。

Conclusion: 结构化MCoT空间推理为实现可解释且资源高效的具身导航提供了有前景的路径。

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [118] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 本文提出了一种变分任务向量组合方法，通过贝叶斯推断框架将组合系数作为潜变量进行估计，实现样本特定的任务组合，相比传统任务级别方法具有更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量组合方法在任务级别操作，忽略了样本特定的需求。同时观察到任务向量存在结构冗余，需要更稀疏和高效的选择机制。

Method: 引入Spike-and-Slab先验促进稀疏性，开发门控采样机制基于不确定性和重要性过滤组合系数，构建可控后验分布。

Result: 实验结果表明该方法在所有数据集上均优于现有方法，能够选择性地利用任务向量中最可靠和信息丰富的组件。

Conclusion: 该方法为高效有效的任务向量组合建立了新标准，通过确定性选择可靠任务组件提高了稳定性和可解释性。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [119] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE是一个包含2.22亿个化合物的大规模、多样化、严格筛选的数据集，旨在解决分子表示学习中现有数据集不足的问题，为化学信息学提供类似ImageNet的标准化资源。


<details>
  <summary>Details</summary>
Motivation: 现有小分子数据集在规模、多样性和质量上的局限性阻碍了分子表示学习的效果，需要构建更高质量的数据集来提升基础模型的泛化能力。

Method: 从6个大规模数据库中通过自动化筛选流程构建MolPILE数据集，并对现有预训练数据集进行全面分析，展示在MolPILE上重新训练现有模型的效果。

Result: 在MolPILE上重新训练现有模型能够显著提升模型的泛化性能。

Conclusion: MolPILE为分子化学领域提供了标准化的模型训练资源，解决了该领域对高质量数据集的迫切需求。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [120] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP是一种通过多令牌预测训练与推理模式对齐来提升推测解码性能的方法，在保持无损输出质量的同时实现2.03倍平均加速


<details>
  <summary>Details</summary>
Motivation: 自回归生成的顺序性造成了吞吐量瓶颈，限制了LLM的实际部署。多令牌预测在训练效率和性能方面表现出色，但其推理加速潜力尚未充分探索

Method: 通过位置共享权重在自蒸馏数据上微调单个MTP头，使其能够捕捉连续未来令牌的依赖关系；集成语言感知动态词汇压缩以减少计算开销

Result: 在七个不同基准测试中，FastMTP相比标准下一个令牌预测实现2.03倍平均加速，比原始MTP性能提升82%，且保持无损输出质量

Conclusion: FastMTP仅需轻量级训练即可无缝集成到现有推理框架中，为加速LLM推理提供了实用且快速部署的解决方案

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [121] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出了一种新的多工作者选择算法M-DSL，用于解决分布式群学习中的非独立同分布数据挑战，通过引入非i.i.d.程度度量来指导工作者选择，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 分布式群学习在边缘物联网中面临非i.i.d.数据的挑战，这会降低学习性能并导致训练行为发散，目前缺乏关于数据异质性如何影响模型训练准确性的理论指导。

Method: 提出M-DSL算法，引入新的非i.i.d.程度度量来量化局部数据集间的统计差异，建立数据异质性与DSL性能评估的联系，指导选择对全局模型更新贡献显著的多工作者。

Result: 在不同异构数据集和非i.i.d.数据设置下的实验验证了M-DSL在性能提升和网络智能增强方面的优势，超越了基准方法。

Conclusion: M-DSL算法有效解决了分布式群学习中的数据异质性问题，提供了理论收敛性分析，并在实验中展现出优于基准方法的性能。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [122] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar是一种新颖的全局图神经网络解释器，基于认知科学中的范例理论，通过识别代表性节点和生成自然语言规则来解释GNN预测，在保真度、可扩展性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的全局解释方法依赖小图中的模式发现，但在大型现实场景中，子图重复罕见、节点属性高维、预测来自复杂的结构-属性交互，这些方法会失效。

Method: GnnXemplar在GNN嵌入空间中识别代表性节点（范例），将其反向k近邻覆盖最大化问题转化为高效贪心近似，并使用LLM的自优化提示策略生成可解释的自然语言规则。

Result: 在多样化基准测试中，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，60名参与者的用户研究验证了其有效性。

Conclusion: GnnXemplar为GNN全局解释提供了有效的解决方案，特别适用于大型现实图数据，通过范例理论和LLM的结合实现了高解释质量。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [123] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: 提出了GETAD框架，通过整合道路网络拓扑、路段语义和历史出行模式来检测轨迹异常，使用图注意力网络学习道路感知嵌入，结合Transformer解码器实现更精确的异常检测


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法仅考虑轨迹的有限方面，在欧几里得空间中分析轨迹，忽略了底层移动网络（如道路或交通网络）的约束和连通性信息

Method: GETAD框架使用图注意力网络学习道路感知嵌入，结合基于图的位置编码，采用Transformer解码器建模序列移动，使用结合自回归预测和监督链接预测的多目标损失函数

Result: 在真实世界和合成数据集上的实验表明，GETAD相比现有方法取得了持续改进，特别是在道路约束环境中检测细微异常方面表现优异

Conclusion: 将图结构和上下文语义整合到轨迹建模中，能够实现更精确和上下文感知的异常检测

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [124] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文探讨了为什么强化学习预训练算法能够生成支持上下文强化学习的网络参数，并通过案例研究证明，当Transformer被预训练用于策略评估时，预训练损失的全局最小值解之一能够实现上下文时序差分学习。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习代理通常通过更新神经网络参数来适应任务，但最近发现一些预训练的代理能够在参数固定的情况下，仅通过上下文信息解决新的分布外任务。本文旨在研究为什么标准的RL预训练算法能够产生支持这种上下文学习能力的网络参数。

Method: 通过理论分析的方法，研究Transformer在策略评估预训练中的损失函数最小化问题，证明预训练损失的全局最小值解具备上下文时序差分学习的能力。

Result: 研究证明，当Transformer被预训练用于策略评估时，预训练损失的全局最小值解之一确实能够实现上下文时序差分学习，为假设提供了初步支持。

Conclusion: 本文通过理论分析证实了预训练损失的最小值解能够支持上下文强化学习，为理解ICRL现象提供了理论基础，并指出了未来研究的方向。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [125] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 这篇论文对深度学习优化器进行了全面综述，从随机梯度下降到最新的Momentum、AdamW、Sophia和Muon等优化器，按时间顺序分析其特点、更新规则、技术贡献和超参数设置。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的快速发展，各种优化器不断涌现，但缺乏系统的综述来帮助研究人员理解当前优化器的状态和识别未来发展方向。

Method: 采用文献综述方法，按时间顺序分析各种优化器的更新规则、相关概念和变量，讨论它们的技术特点、对优化过程的贡献以及默认超参数设置。

Result: 提供了一个全面的优化器资源，详细介绍了从基础到最新的各种优化器，包括它们的技术细节和实际应用特点。

Conclusion: 该研究为理解当前优化器状态和识别未来潜在发展领域提供了有价值的资源，同时指出了深度学习模型优化中面临的开放挑战。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [126] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了一种新颖的信息保持混沌游戏表示方法（R-CGR），解决了传统CGR方法在几何映射过程中丢失序列信息的根本限制，实现了完整的序列恢复能力。


<details>
  <summary>Details</summary>
Motivation: 传统CGR方法在生物序列分析中存在序列信息丢失的问题，这限制了其在需要完整序列信息的生物信息学应用中的使用。

Method: 通过显式路径编码结合有理数精度控制，实现了完美的序列重建。该方法存储完整的路径信息，在每一步都保持位置和字符信息。

Result: 在生物序列分类任务中表现出与传统序列方法相竞争的性能，同时提供可解释的几何可视化。生成的图像适合深度学习应用。

Conclusion: 该方法为可解释的生物信息学分析开辟了新途径，在需要准确性和序列恢复的应用中具有重要价值。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [127] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: KANDI方法结合Kolmogorov-Arnold网络和扩散策略，解决了离线强化学习在医疗健康应用中奖励函数定义困难和政策对齐的挑战，特别是在老年人跌倒风险干预中表现出色。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗健康应用中面临奖励函数定义困难和政策与人类行为对齐的挑战，特别是在复杂的真实世界临床数据环境中。

Method: 提出KANDI方法，利用Kolmogorov-Arnold网络的灵活函数逼近能力估计奖励函数，并结合扩散策略在Actor-Critic框架中进行动作优化。

Result: KANDI在D4RL基准测试中优于现有最先进方法，并在老年人跌倒风险干预临床试验中表现出实际应用价值。

Conclusion: KANDI为医疗健康应用中的离线强化学习提供了有效解决方案，特别适用于活动促进干预策略。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [128] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: 提出了MeshODENet框架，将图神经网络的空间推理能力与神经常微分方程的连续时间建模相结合，用于复杂结构力学系统的长期预测，显著提高了预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在复杂物理系统模拟中计算成本高昂，而标准自回归图神经网络在长期预测中存在误差累积和不稳定性问题。

Method: MeshODENet框架结合图神经网络（GNN）的空间推理能力和神经常微分方程（Neural ODE）的连续时间建模，用于处理一维和二维弹性体的大非线性变形问题。

Result: 该方法在长期预测精度和稳定性方面显著优于基线模型，同时相比传统求解器实现了大幅计算加速。

Conclusion: MeshODENet为开发数据驱动代理模型提供了一种强大且通用的方法，可加速复杂结构系统的分析和建模。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [129] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: 提出了一种AI驱动的框架，用于为给定线性系统推荐MCMC参数，以加速Krylov子空间求解器的收敛。该框架通过图神经网络预测预处理速度，并使用贝叶斯采集函数选择最优参数集，在50%的搜索预算下实现了约10%的收敛迭代次数减少。


<details>
  <summary>Details</summary>
Motivation: Krylov子空间求解器在求解大型稀疏线性系统时，对于病态矩阵收敛缓慢，需要预处理。MCMC矩阵求逆可以生成预处理器，但其效果取决于参数，而最优参数随矩阵变化，手动或网格搜索成本高昂。

Method: 开发了一个AI驱动框架：1）使用图神经网络代理模型从矩阵A和MCMC参数预测预处理速度；2）采用贝叶斯采集函数选择最有可能最小化迭代次数的参数集。

Result: 在未见过的病态系统上，该框架仅使用传统方法50%的搜索预算就实现了更好的预处理效果，收敛迭代次数减少了约10%。

Conclusion: 这项工作为将MCMC基预处理器集成到大规模系统中提供了一条可行路径，展示了AI方法在优化数值线性代数求解器参数方面的潜力。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [130] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind是一个基于Transformer的多模态框架，用于持续和长期血糖预测，通过交叉注意力和多尺度注意力机制整合生理和行为信号，并采用知识保留技术防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决血糖预测中多源信号采样率不一致、长期时间依赖性建模困难以及持续学习中的灾难性遗忘问题。

Method: 设计并行运行的交叉注意力和多尺度注意力机制，交叉注意力整合血糖数据与其他生理行为信号，多尺度注意力捕捉长期时间依赖；在Transformer预测模型中加入知识保留模块。

Result: 在AIREADI数据集上评估，GluMind在RMSE和MAE指标上分别比现有最优模型提升约15%和9%，表现出稳定的性能和适应性。

Conclusion: GluMind框架能有效处理多模态血糖预测问题，在持续学习场景下保持高性能，为糖尿病管理提供了可靠的技术支持。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [131] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA是一种新的降维算法，将PPCA推广到非线性流形，通过结合几何坐标系统更好地描述分布在非线性流形周围的数据分布。


<details>
  <summary>Details</summary>
Motivation: 传统PPCA及其扩展主要基于线性模型，只能描述欧几里得坐标系中的数据。但在神经科学等应用中，数据可能分布在非线性流形周围而非欧几里得空间中。

Method: 开发了PGPCA算法，显式地结合给定的非线性流形知识，推导出几何坐标系统来捕捉数据与流形的偏差和噪声，并提出了数据驱动的EM算法来学习模型参数。

Result: 在模拟和脑数据分析中，PGPCA能有效建模各种给定流形周围的数据分布，且在此类数据上优于PPCA。PGPCA还提供了测试几何坐标系统是否比欧几里得系统更好地描述数据的能力。

Conclusion: PGPCA通过结合非线性流形几何，增强了高维数据分析中降维的有效性，特别适用于具有噪声且分布在非线性流形周围的数据。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [132] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文探索了离散时间扩散过程及其变体，包括加性高斯噪声、乘性高斯噪声、模糊噪声以及混合噪声，实验表明离散时间过程在语音质量上与连续对应物相当，但具有更高效和一致的训练推理模式。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用连续时间过程，存在训练推理条件不匹配的问题，而离散时间过程可以避免这些限制，实现更少的推理步骤和完全一致的训练推理条件。

Method: 提出了几种扩散式离散时间过程变体：加性高斯噪声、乘性高斯噪声、模糊噪声以及模糊与高斯噪声的混合。

Result: 实验结果显示，离散时间过程在主观和客观语音质量评估上与流行的连续对应物相当。

Conclusion: 离散时间扩散过程提供了与连续对应物相当的语音质量，同时具有更高效和一致的训练推理方案。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [133] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold是首个基于流匹配的蛋白质折叠模型，仅使用通用Transformer块，挑战了传统依赖复杂领域特定架构的设计理念。


<details>
  <summary>Details</summary>
Motivation: 质疑蛋白质折叠模型中复杂的领域特定架构是否是构建高性能模型的必要条件，探索是否可以使用通用架构实现竞争性表现。

Method: 使用标准Transformer块配合自适应层，通过生成流匹配目标训练，包含额外的结构项，模型规模达30亿参数，在约900万个蒸馏蛋白质结构和实验PDB数据上训练。

Result: 在标准折叠基准测试中，SimpleFold-3B达到与最先进基线竞争的性能，在集成预测方面表现强劲，在消费级硬件上部署和推理效率高。

Conclusion: SimpleFold挑战了蛋白质折叠中对复杂领域特定架构设计的依赖，为未来进展开辟了替代设计空间。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [134] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 本文提出了一种基于Kolmogorov Arnold Networks（KANs）的新方法，结合物理信息损失函数来预测量子动力学响应，相比传统神经网络方法显著减少了训练数据需求并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 量子系统在高维希尔伯特空间中的演化使得传统数值方法计算成本高昂，而现有神经网络架构需要大量训练数据且存在虚假振荡问题，影响了物理可解释性。

Method: 使用Kolmogorov Arnold Networks（KANs）增强物理信息损失函数，强制执行Ehrenfest定理，并引入Chain of KANs架构直接嵌入时间因果关系。

Result: 该方法仅需200个样本（传统Temporal Convolution Networks需要3,700个），准确率显著提升，数据需求减少至5.4%。

Conclusion: 物理信息KANs相比传统黑盒模型具有明显优势，在保持数学严谨性和物理一致性的同时大幅降低了数据需求。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [135] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 本文提出使用混合数据集来增强合成数据集的效用，通过结合公开可用的真实世界特征，既保护隐私又提高反洗钱模型的性能。


<details>
  <summary>Details</summary>
Motivation: 反洗钱模型训练面临数据隐私和机密性问题，合成数据虽然能保护隐私但存在效用限制，需要找到平衡隐私保护和模型性能的解决方案。

Method: 采用混合数据集方法，将合成数据与公开可用的真实世界特征相结合，构建既能保护隐私又具有实用性的训练数据集。

Result: 实验表明混合数据集不仅能够有效保护隐私，还能显著提升反洗钱模型的检测性能，为金融机构提供了实用的解决方案。

Conclusion: 混合数据集方法为解决反洗钱模型训练中的数据隐私问题提供了可行路径，在保护隐私的同时提升了模型效用，具有重要的实践价值。

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [136] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL是一种新的强化学习框架，通过主动部分rollout策略解决RL训练中长尾响应分布导致的GPU空闲问题，提高训练效率


<details>
  <summary>Details</summary>
Motivation: 当前RL训练中rollout生成占90%以上运行时间，且长尾响应分布导致GPU利用率低下，限制了RL训练的扩展性

Method: APRIL在rollout阶段过度配置请求，达到目标响应数量后终止，将未完成的响应回收用于后续步骤继续生成

Result: APRIL将rollout吞吐量最多提升44%，加速收敛，在多个任务上实现最多8%的最终准确率提升

Conclusion: APRIL统一了系统级和算法级考虑，可部署在不同硬件上，有望推动RL训练效率的进一步优化

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [137] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 本文提出了一种名为反向互补一致性正则化（RCCR）的简单且模型无关的微调目标，用于解决DNA语言模型无法捕捉DNA序列反向互补对称性的问题，通过惩罚模型对序列及其反向互补序列预测的差异来提高模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: DNA序列的反向互补序列通常具有相同的生物学意义，但现有的DNA语言模型经常无法捕捉这种对称性，导致对序列及其反向互补序列的预测不一致，这削弱了模型的可靠性。

Method: 引入反向互补一致性正则化（RCCR），这是一种直接惩罚模型对序列及其反向互补序列预测差异的微调目标。该方法在三个不同的骨干网络（Nucleotide Transformer、HyenaDNA、DNABERT-2）上进行了评估，涵盖了序列分类、标量回归和轮廓预测等多种基因组任务。

Result: 实验表明，RCCR显著提高了反向互补鲁棒性，大幅减少了预测翻转和错误，同时在任务准确性上保持或优于基线方法（如反向互补数据增强和测试时平均）。

Conclusion: 通过将关键的生物学先验直接整合到学习过程中，RCCR为多样化的生物学任务提供了一种单一、内在鲁棒且计算高效的模型微调方案。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [138] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: 本文提出Symphony-MoE框架，通过融合多个异构预训练模型的专家来构建强大的MoE模型，解决了传统方法专家多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型通过复制单个预训练密集模型的FFN层来构建专家，限制了专家多样性。本文旨在利用多个异构预训练模型来构建更具多样性的MoE模型。

Method: 提出两阶段框架：1）训练无关的层感知融合策略构建共享骨干网络，并通过基于激活的功能对齐缓解参数错位；2）轻量级路由器训练协调整个架构。

Result: 实验表明该方法成功整合了异构源专家，在多领域任务和分布外泛化方面显著超越基线方法。

Conclusion: Symphony-MoE框架有效解决了多源专家融合的挑战，为构建高性能MoE模型提供了新思路。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [139] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 本文从理论上解释了在Sigmoid损失下同步可训练逆温度和偏置的优势，提出了(m, b_rel)-Constellations这一新的组合对象，并用其理论证明了SigLIP在检索任务上的成功，解释了模态间隙，并提出了具有显式相对偏置的Sigmoid损失重参数化方法。


<details>
  <summary>Details</summary>
Motivation: 对比预训练中的表示获取和对齐任务日益重要，需要从理论上理解SigLIP和SigLIP2模型中使用的可训练温度和偏置参数的优势。

Method: 提出了(m, b_rel)-Constellations这一与球面码相关的新组合对象，用于理论分析Sigmoid损失下的训练动态，并提出了具有显式相对偏置的Sigmoid损失重参数化方法。

Result: 理论证明了SigLIP在检索任务上的成功，解释了模态间隙现象，识别了产生高质量表示所需的维度，并在合成数据实验中改善了训练动态。

Conclusion: 同步可训练温度和偏置在Sigmoid损失下具有理论优势，提出的(m, b_rel)-Constellations为理解对比学习提供了新的理论框架，显式相对偏置的重参数化方法能改善训练效果。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [140] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 本文是对可解释图神经网络（XGNNs）在痴呆症研究中应用的首次全面综述，涵盖阿尔茨海默病、帕金森病、轻度认知障碍等疾病的诊断，提出了针对痴呆症任务的解释性方法分类体系，并讨论了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 痴呆症具有临床和生物学异质性，使得诊断和亚型区分极具挑战性。传统图神经网络在建模脑连接方面显示出潜力，但其鲁棒性不足、数据稀缺和缺乏可解释性限制了临床应用。XGNNs通过结合图学习和可解释性来解决这些障碍。

Method: 本文采用综述方法，系统梳理XGNNs在痴呆症研究中的应用，提出专门针对痴呆症任务的解释性方法分类体系，比较现有模型在临床场景中的表现，并分析整合大型语言模型进行早期检测的可能性。

Result: 综述发现XGNNs能够识别疾病相关生物标志物、分析脑网络破坏，并为临床医生提供透明见解。同时识别出当前面临的挑战包括有限泛化性、未充分探索的领域以及LLMs整合等问题。

Conclusion: 本综述旨在指导未来工作朝着可信赖、临床有意义且可扩展的XGNNs在痴呆症研究中的应用方向发展，为解决痴呆症诊断和治疗的复杂挑战提供新思路。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [141] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出了Interaction Topological Transformer（ITT）框架，用于多尺度预测多孔材料的结构-性能关系，通过结合局部化学环境和全局孔网络拓扑来解决数据稀疏和泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 多孔材料在气体存储、分离和催化中应用广泛，但由于结构-性能关系的多尺度特性（局部化学环境和全局孔网络拓扑共同决定性能），以及标记数据稀疏且分布不均，预测建模面临挑战。

Method: ITT框架利用新颖的相互作用拓扑在多尺度（结构、元素、原子、成对元素组织）捕获材料信息，提取反映复杂多孔框架内组成和关系结构的尺度感知特征，并通过内置Transformer架构支持跨尺度联合推理。采用两阶段训练策略：在60万未标记结构上进行自监督预训练，然后进行监督微调。

Result: ITT在吸附、传输和稳定性性能预测方面实现了最先进、准确且可迁移的预测结果。

Conclusion: 该框架为结构和化学多样性多孔材料的学习引导发现提供了原则性和可扩展的路径。

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [142] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: 本文提出DS-Diffusion模型，通过风格引导核和分层去噪机制解决现有扩散模型在时间序列生成中的条件引导需重新训练、分布偏差和推理过程不透明等问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列生成中存在三个主要问题：需要重新训练才能引入特定条件引导；生成数据与真实数据存在分布偏差；模型复杂性和潜在空间导致推理过程不透明。

Method: 提出数据风格引导扩散模型（DS-Diffusion），包括基于风格引导核的扩散框架避免特定条件重新训练，以及基于时间信息的分层去噪机制（THD）减少分布偏差。

Result: 实验结果显示，相比最先进的ImagenTime模型，预测分数和判别分数分别降低5.56%和61.55%，分布偏差进一步减小，推理过程更可解释，模型灵活性增强。

Conclusion: DS-Diffusion有效解决了现有扩散模型在时间序列生成中的关键问题，提高了生成质量、可解释性和适应性。

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [143] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT方法通过在LLM决策过程中引入反思步骤，显著提升了交互式决策任务的性能，在多个环境中都取得了明显的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在交互决策任务中存在错误累积和缺乏自我纠正机制的问题，需要一种能够及时修正错误的方法。

Method: 提出"Reflect before Act"（REBACT）方法，在执行下一个动作前增加关键的反思步骤，实现即时错误纠正和适应环境反馈。

Result: 在ALFWorld、WebShop和TextCraft三个环境中，REBACT显著优于基线方法，成功率分别提升6.72%（达到98.51%）、24%（达到61%）和0.5%（达到99.5%）。

Conclusion: REBACT通过少量修改步骤就能实现性能提升，证明了其计算效率，为LLM交互决策提供了有效的自我纠正机制。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [144] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: 提出Flow Marching算法，将神经算子学习与流匹配相结合，构建生成式PDE基础模型，通过联合采样噪声水平和物理时间步长来减少长期滚动漂移并实现不确定性感知的集成生成。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型主要依赖确定性Transformer架构，缺乏生成灵活性，无法满足科学和工程应用中对不确定性建模的需求。

Method: 1. Flow Marching算法：联合采样噪声水平和物理时间步长，学习统一速度场；2. P2VAE：将物理状态嵌入紧凑潜在空间；3. FMT：结合扩散强迫方案和潜在时间金字塔，提高计算效率。

Result: 在12个不同PDE家族的约250万轨迹上训练，在未见过的Kolmogorov湍流上表现出良好的少样本适应能力，长期滚动稳定性优于确定性模型，并能提供不确定性分层的集成结果。

Conclusion: 生成式PDE基础模型对于现实世界应用具有重要意义，Flow Marching方法在减少计算成本的同时实现了更好的生成性能和不确定性建模能力。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [145] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt是一种参数高效微调方法，通过行列缩放矩阵实现高秩更新，仅需n+m个可训练参数，在保持性能的同时大幅减少参数数量


<details>
  <summary>Details</summary>
Motivation: 基础模型在多样化任务中表现出色，但适应专业应用通常需要微调，这种方法内存和计算成本高。参数高效微调方法通过仅更新少量权重来缓解这一问题

Method: HyperAdapt通过对预训练权重矩阵应用行列缩放（使用对角矩阵），实现高秩更新。对于n×m矩阵，仅需n+m个可训练参数

Result: 在GLUE、算术推理和常识推理基准测试中，使用高达140亿参数的模型进行实验，HyperAdapt在性能上匹配或接近全微调及最先进PEFT方法，同时使用数量级更少的可训练参数

Conclusion: HyperAdapt是一种高效的参数微调方法，理论分析建立了更新秩的上界，实证确认其在各模型层中一致诱导高秩变换，显著优于现有方法

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [146] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 本文提出了一种新的高矩阵聚类框架SCoS，直接对矩阵数据进行子空间聚类，而非传统向量化方法。通过块项分解构建三阶张量，实现聚类成员和部分共享子空间的联合估计。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法假设向量化数据，无法有效处理矩阵形式的高维数据。本文旨在直接对矩阵样本进行聚类，捕捉超越单个数据向量的结构信息。

Method: 基于块项分解构建三阶张量，联合估计聚类成员和部分共享子空间。提出了可扩展的优化算法，适用于大规模数据集。

Result: 在高光谱成像数据集上的实验表明，该方法在高噪声和干扰下具有优越的聚类准确性和鲁棒性，优于现有子空间聚类技术。

Conclusion: 该框架在高维应用中具有重要潜力，特别是在存在超越单个数据向量结构信息的挑战性场景中。

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [147] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 该研究利用图机器学习进行理性农药设计，开发更安全环保的农用化学品，创建了最大的蜜蜂毒性数据集ApisTox，评估了多种机器学习模型在农药毒性预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 受药物发现中计算机模拟方法的启发，旨在加速开发更安全、环保的农用化学品，特别关注生态毒理学问题。

Method: 创建ApisTox数据集（最大的蜜蜂毒性数据集），广泛评估分子指纹、图核、图神经网络和预训练变换器等多种机器学习模型在分子图分类任务中的表现。

Result: 发现在药物化学中成功的方法往往无法推广到农用化学品领域，凸显了需要领域特定模型和基准测试的必要性。

Conclusion: 未来工作将专注于开发全面的基准测试套件，并设计专门针对农药发现独特挑战的机器学习模型。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [148] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于衡量不同马尔可夫决策过程（MDP）之间的状态相似性，解决了传统双模拟度量在多MDP场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量（BSM）在单一MDP内有效，但在多MDP场景（如策略迁移）中应用困难。现有方法缺乏对数学性质的严格分析，限制了理论发展。

Method: 通过形式化建立MDP对之间的广义双模拟度量，严格证明其三个基本性质：对称性、MDP间三角不等式和相同状态空间上的距离边界。

Result: GBSM在策略迁移、状态聚合和基于采样的估计中获得了比标准BSM更严格的理论边界，并提供了闭式样本复杂度估计。数值结果验证了理论发现。

Conclusion: GBSM为多MDP场景提供了坚实的理论基础，在理论和实践上都优于传统BSM方法。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [149] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 提出了一种结合强化学习和大型语言模型的新颖电子商务支付欺诈检测方法，通过将交易风险建模为多步马尔可夫决策过程，利用LLM迭代优化奖励函数，提高欺诈检测准确率。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法依赖人工设计复杂的奖励函数，需要大量专家知识。LLM具有先进推理和编码能力，适合优化这些函数，克服传统方法的局限性。

Method: 将交易风险建模为多步MDP，使用强化学习优化风险检测。利用LLM迭代优化奖励函数设计，实现零样本能力。

Result: 在真实世界数据上的实验证实了该方法的有效性、鲁棒性和韧性，长期评估显示欺诈检测准确率显著提升。

Conclusion: LLM增强的强化学习框架在工业应用中具有巨大潜力，为支付欺诈检测提供了创新解决方案。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [150] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种具有周期边界条件的卷积神经网络（周期CNN），证明了其在d维输入空间中能够逼近依赖于d-1个线性变量的脊函数，而低维脊设置（d-2或更少变量）无法实现这种逼近。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理具有周期边界条件的数据时存在局限性，特别是在图像分析、物理信息学习和材料科学等领域，数据往往具有高内在维度的脊状结构。

Method: 设计了一种新型的周期CNN架构，将周期边界条件融入卷积层，并建立了严格的逼近定理来理论分析其表达能力。

Result: 理论证明周期CNN能够精确逼近高维脊函数，填补了CNN逼近理论在周期边界条件下的空白。

Conclusion: 周期CNN不仅扩展了CNN逼近理论的数学基础，还为处理具有周期结构数据的实际问题提供了一类具有实用价值的架构。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [151] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO是一个基于时间序列基础模型（TFM）的异常检测方法，通过引入基于补丁的内存模块来缓解模型过度泛化问题，能够在多个数据集上联合微调并提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重构的深度模型在时间序列异常检测中容易过度泛化，准确重构未见过的异常。现有内存架构方法训练成本高且难以与时间序列基础模型有效集成。

Method: 提出MOMEMTO方法：1）使用基于补丁的内存模块捕获多领域正常模式；2）通过多领域训练策略实现单一模型在多个数据集上的联合微调；3）用预训练编码器的潜在表示初始化内存项，组织为补丁级单元，通过注意力机制更新。

Result: 在23个单变量基准数据集上的实验表明，MOMEMTO作为单一模型在AUC和VUS指标上优于基线方法，特别是在少样本学习场景中显著提升了其骨干TFM的性能。

Conclusion: MOMEMTO成功解决了时间序列异常检测中的过度泛化问题，通过内存模块和多领域训练策略实现了高效且有效的异常检测，为时间序列基础模型在异常检测任务中的应用提供了新思路。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [152] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: 本文深入分析了对角线性网络的训练轨迹与lasso正则化路径之间的紧密联系，表明训练时间扮演了逆正则化参数的角色。


<details>
  <summary>Details</summary>
Motivation: 对角线性网络的理论兴趣在于其隐式正则化可以被严格分析，本文旨在深化这一分析，探索训练轨迹与lasso路径的关系。

Method: 通过理论分析和数值模拟，研究对角线性网络从小的初始化开始的训练过程，并与lasso正则化路径进行比较。

Result: 在lasso路径单调的假设下，这种联系是精确的；在一般情况下，则显示出近似联系。

Conclusion: 对角线性网络的训练轨迹与lasso正则化路径密切相关，训练时间起到了逆正则化参数的作用。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [153] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成概率机器学习的诊断框架，通过量化和自动化预测不确定性来改进数据驱动的一致性诊断特性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在故障诊断中应用广泛，但模型往往难以评估其置信度，这在基于一致性的诊断中尤为重要，因为决策逻辑对误报高度敏感。

Method: 使用集成概率机器学习方法来量化和自动化预测不确定性，提高数据驱动一致性诊断的性能。

Result: 通过消融分析和比较分析在多个案例研究中评估，显示在一系列诊断指标上都有持续改进。

Conclusion: 所提出的方法能够有效改善数据驱动一致性诊断的诊断特性，特别是在处理预测不确定性方面表现出色。

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [154] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出了一种基于预训练扩散模型的轻量级通用数据同化方法，无需额外训练即可应用于动力系统状态估计


<details>
  <summary>Details</summary>
Motivation: 数据同化在气象学、海洋学和机器人学等领域广泛应用，但现有方法需要大量计算资源。本文旨在利用预训练的扩散模型开发一种轻量级的数据同化解决方案

Method: 基于粒子滤波器算法，利用为模拟动力系统预训练的扩散模型进行数据同化，以GenCast（基于扩散的全球集合天气预报模型）为例进行说明

Result: 该方法不需要额外训练，可以直接利用预训练模型进行数据同化，具有轻量级和通用性的特点

Conclusion: 提出的方法为数据同化提供了一种新的轻量级解决方案，特别适用于利用预训练扩散模型进行动力系统状态估计的应用场景

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [155] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO算法解决了GRPO在处理同质错误样本时学习信号丢失的问题，通过优势校准和非对称裁剪技术，将同质错误转化为有效的学习信号，在数学推理任务上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在面对全部正确或全部错误的同质响应组时，优势函数值为零，导致梯度消失和学习信号丢失，特别是在同质错误情况下会失去宝贵的学习机会。

Method: 1. 优势校准：在优势计算中引入虚拟最大奖励样本假设，改变组内奖励的均值和方差，确保同质错误样本的优势值不再为零；2. 非对称裁剪：放宽正样本的更新幅度，同时对负样本施加更严格的约束，稳定探索压力。

Result: 在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试中显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法。

Conclusion: NGRPO能够有效学习同质错误，实现数学推理能力的稳定和显著提升，为解决RLVR算法中的同质响应学习问题提供了有效方案。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [156] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 本文提出了LoRD和B-LoRD两种图聚类方法，通过减少对低秩、非负、双随机和正交约束的过度松弛，仅放松正交约束来获得概率聚类结果，并通过块对角正则化进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图聚类方法（如谱聚类、对称非负矩阵分解等）为了数值可行性过度放松了内在约束，可能限制了聚类效果。作者希望通过更精确的约束松弛来提升聚类性能。

Method: 提出LoRD模型仅放松正交约束，保持低秩、非负和双随机约束；进一步引入块对角正则化得到B-LoRD；通过引入类概率参数将非凸双随机约束转化为线性凸约束；提出全局收敛的投影梯度下降算法进行优化。

Result: 理论分析证明了正交性与块对角性在双随机约束下的等价性，并证明了LoRD和B-LoRD的梯度Lipschitz连续性。大量实验验证了方法的有效性。

Conclusion: LoRD和B-LoRD通过更精确的约束松弛和块对角正则化，显著提升了图聚类性能，同时保证了算法的数值可解性和全局收敛性。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [157] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 论文提出GNARL框架，将神经算法推理（NAR）重新构建为马尔可夫决策过程，结合模仿学习和强化学习来解决传统NAR方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统NAR方法存在无法构建有效解、无法处理多个正确解、在NP难问题上表现差、以及缺乏专家算法时无法应用等局限性。

Method: 将算法轨迹学习问题重新构建为马尔可夫决策过程，提出GNARL框架，结合模仿学习和强化学习方法，适用于广泛的图基问题。

Result: 在多个CLRS-30问题上获得很高的图精度，在NP难问题上性能匹配或超越传统NAR方法，甚至在缺乏专家算法时也能应用。

Conclusion: GNARL框架成功解决了传统NAR方法的局限性，为算法学习提供了更灵活和强大的解决方案。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [158] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出Shared-Weights Extender（SWE）和Steepest Voting Distributor（SVoD）两种方法，通过在训练过程中扩展神经网络来增加容量，同时防止新神经元变得不活跃。


<details>
  <summary>Details</summary>
Motivation: 在训练过程中扩展神经网络是一种有前景的方法，可以在不从头训练更大模型的情况下增加容量。但新添加的神经元往往无法适应已训练的网络而变得不活跃，无法为容量增长做出贡献。

Method: SWE方法通过将新神经元与现有神经元耦合来实现平滑集成，防止新神经元不活跃；SVoD是一种基于梯度的神经元分配方法，用于在深度网络扩展过程中跨层分配神经元。

Result: 在四个数据集上的广泛基准测试表明，该方法能有效抑制神经元不活跃现象，并比其他扩展方法和基线获得更好的性能。

Conclusion: 提出的SWE和SVoD方法能够有效解决神经网络扩展过程中新神经元不活跃的问题，实现更好的性能表现。

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [159] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 该论文提出使用置信网络（CN）作为贝叶斯网络（BN）的隐私保护替代方案，在保持模型效用的同时防止追踪攻击。


<details>
  <summary>Details</summary>
Motivation: 随着隐私问题日益严重，公开发布的贝叶斯网络容易受到追踪攻击，现有噪声添加技术虽然提供隐私保护但严重损害模型效用。

Method: 将贝叶斯网络转换为置信网络，通过参数模糊化而非添加噪声来保护隐私，同时识别并隐藏关键学习信息以防止攻击者恢复原始BN。

Result: 数值实验表明置信网络能够有效调节隐私增益，在保持有意义的推理能力的同时显著降低追踪攻击的成功概率。

Conclusion: 置信网络为开发隐私感知的概率图模型提供了一种原则性、实用且有效的方法，能够平衡隐私保护和模型效用。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [160] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 本文提出了一种完全可学习的神经奖励机（FLNRM），能够端到端学习符号接地函数和自动机，无需依赖预定义符号映射或先验任务知识，在非马尔可夫强化学习任务中表现优于基于RNN的方法。


<details>
  <summary>Details</summary>
Motivation: 解决非马尔可夫强化学习任务中传统方法依赖预定义符号接地函数和先验任务知识的限制问题。

Method: 提出完全可学习的神经奖励机（FLNRM），集成深度强化学习，端到端学习符号接地函数和自动机表示。

Result: FLNRM方法在非马尔可夫强化学习任务中优于基于循环神经网络（RNN）的先前方法。

Conclusion: FLNRM既保持了经典深度强化学习的易用性，又因自动机的有限紧凑特性而更具可解释性，为复杂时序任务提供了有效解决方案。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [161] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 该论文首次分析了异质性在图级学习中的影响，揭示了与节点级任务不同的机制：图级任务需要混合频率动态而非频率主导机制，特别是在基于motif的检测任务中。


<details>
  <summary>Details</summary>
Motivation: 虽然异质性在节点级任务中已被广泛研究，但其对图级任务的影响尚不清楚。本文旨在填补这一空白，建立图级学习中异质性的理论理解。

Method: 结合理论分析和实证验证：1）提出图级标注方案的分类法；2）聚焦局部结构标注中的motif任务；3）使用基于能量的梯度流分析；4）在合成数据集和真实分子属性预测数据集上进行实验。

Result: 理论分析表明motif目标与全局频率主导机制存在固有错位，需要不同的架构考虑。实验证明频率自适应模型优于频率主导模型。

Conclusion: 这项工作建立了图级学习中异质性的新理论理解，并为设计有效的GNN架构提供了指导，强调了频率自适应机制的重要性。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [162] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: 本文系统比较了三种免反向传播训练方法（FF、CaFo、MF），发现MF算法在MLP架构上不仅分类精度优于BP，还能降低41%能耗和34%训练时间，为可持续AI发展提供了数据驱动的路线图。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的计算和能耗需求不断增长，主要受反向传播驱动，这对可持续AI发展构成挑战。研究旨在寻找更节能高效的训练方法。

Method: 建立了严格的比较框架：在原生架构上实现三种BP-free算法（FF和MF用MLP，CaFo用CNN），与等效BP模型对比。使用Optuna优化超参数，基于验证性能应用一致的早停标准。

Result: MF在MLP上的分类精度超越BP，能耗降低41%，训练时间缩短34%。其优越泛化能力源于在验证损失景观中收敛到更有利的最小值。硬件级分析揭示了FF的架构低效性和MF的计算精简设计。

Conclusion: MF算法在精度和可持续性方面实现了最佳平衡，挑战了全局优化必要性的假设，为未来节能深度学习提供了清晰的数据驱动路线图。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [163] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出了一种动态优化后门触发器的方法，通过min-max框架将后门任务与主任务解耦，提高联邦学习中的后门攻击持久性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性暴露了新的攻击面，现有后门攻击依赖固定模式或对抗扰动作为触发器，容易受到诚实更新的稀释和联邦防御的影响。

Method: 采用min-max框架动态优化后门触发器，内层最大化中毒样本与良性样本的性能差距，外层将自适应触发器注入本地模型。

Result: 在计算机视觉和自然语言任务上评估，与6种后门攻击方法和6种防御算法比较，显示该方法具有良好的攻击性能。

Conclusion: 该方法能有效解耦后门任务与主任务，提高攻击持久性，且易于集成到现有后门攻击技术中。

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [164] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN是一种新颖的图神经网络架构，基于Axelrod文化传播模型，解决了GNN中的特征过度平滑、异构关系处理和特征向量不可分等问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN存在三个基本限制：深层网络中的特征过度平滑导致节点表示难以区分；难以有效处理连接节点差异显著的异构关系；将整个特征向量作为不可分割单元处理，限制了灵活性。

Method: AxelGNN采用相似性门控概率交互机制，根据节点相似性自适应促进收敛或发散；实现特征级复制机制，在片段级别进行细粒度特征聚合；保持全局极化以在多个表示簇中保持节点独特性。

Result: 在节点分类和影响力估计基准测试中，AxelGNN在不同图结构上始终优于或匹配最先进的GNN方法，特别是在各种同质性-异质性特征的图上表现优异。

Conclusion: AxelGNN的双稳态收敛动力学自然地在单一架构中处理同质性和异质性图，为GNN提供了更灵活和有效的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [165] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 该论文讨论了深度学习模型在安全关键应用中的鲁棒性问题，涵盖了对抗样本、领域泛化和大型语言模型越狱三个关键领域，提出了新的算法和防御方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在安全关键应用中的广泛使用，确保模型决策对对抗性攻击具有鲁棒性变得至关重要。论文旨在设计具有理想鲁棒性特性的算法。

Method: 1) 针对计算机视觉中的对抗样本问题，引入了新的技术结果、训练范式和认证算法；2) 针对领域泛化问题，提出了在医学影像、分子识别和图像分类中实现最先进泛化能力的新算法；3) 针对LLM越狱问题，提出了新的攻击和防御方法。

Result: 论文在三个关键领域都取得了显著进展：在对抗样本方面提出了新的技术方案；在领域泛化方面实现了最先进的性能；在LLM安全方面代表了设计鲁棒语言智能体的前沿进展。

Conclusion: 该论文通过系统性地研究深度学习模型在不同应用场景中的鲁棒性问题，为构建更安全可靠的AI系统提供了重要的理论和技术支持，代表了该领域的前沿研究方向。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [166] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO是一种可扩展的多标签因果发现方法，用于处理稀疏高维事件序列，通过预训练的因果Transformer和自适应频率融合来推断因果图。


<details>
  <summary>Details</summary>
Motivation: 理解事件序列中的因果关系对于医疗、车辆诊断等领域至关重要，但目前仍是一个未解决的挑战。

Method: 使用两个预训练的因果Transformer作为领域特定的基础模型，通过并行推断每个序列的因果图，并使用自适应频率融合来重建标签的全局马尔可夫边界。

Result: 在包含29,100个唯一事件类型和474个不平衡标签的真实世界汽车故障预测数据集上，CARGO表现出结构化推理能力。

Conclusion: CARGO的两阶段方法能够在保持可扩展性的同时进行概率推理，避免了全数据集条件独立性测试的不可行成本。

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [167] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: 提出HEROS异构在线集成方法，在流数据挖掘中平衡预测性能和计算资源消耗，通过ζ策略选择最优模型子集进行训练，实现绿色在线学习


<details>
  <summary>Details</summary>
Motivation: 现有集成方法过度关注预测能力而忽视计算成本，不符合可持续发展需求，需要解决资源约束下的模型选择问题

Method: 基于马尔可夫决策过程理论框架，提出ζ策略从多样化超参数模型池中选择子集进行训练，平衡性能和资源消耗

Result: 在11个基准数据集上的实验表明，ζ策略在保持高准确率的同时显著减少资源使用，有时甚至优于现有方法

Conclusion: HEROS框架和ζ策略为绿色在线学习提供了有效解决方案，在预测性能和可持续性之间实现了良好平衡

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [168] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS是一个结合信任和公平的联邦学习框架，通过基于适应度的客户端选举和分槽聚合来提升FedFaSt方法，在医疗等敏感领域解决非IID数据、客户端不可靠性和对抗性攻击问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗等敏感领域的部署面临非IID数据、客户端不可靠性和对抗性操纵等持续挑战，需要平衡收敛效率与鲁棒性。

Method: FedFiTS采用三阶段参与策略：自由训练、自然选择和分槽团队参与，结合动态客户端评分、自适应阈值和基于队列的调度。

Result: 在医学影像、视觉基准和表格农业数据上的实验表明，FedFiTS在准确性、达到目标时间和对投毒攻击的韧性方面持续优于FedAvg、FedRand和FedPow。

Conclusion: 通过整合信任感知聚合和公平导向的客户端选择，FedFiTS推进了可扩展和安全的联邦学习，使其非常适合现实世界的医疗和跨领域部署。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [169] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文为异步更新的Polyak-Ruppert平均Q学习建立了中心极限定理，包括非渐近中心极限定理和函数中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 研究异步更新环境下Q学习的统计性质，特别是其收敛速率和极限分布，以更好地理解强化学习算法的理论性质。

Method: 采用Polyak-Ruppert平均技术，分析异步Q学习算法在Wasserstein距离下的收敛速率，并推导部分和过程的弱收敛性质。

Result: 得到了非渐近中心极限定理，收敛速率明确依赖于迭代次数、状态-动作空间大小、折扣因子和探索质量；同时证明了部分和过程弱收敛于布朗运动。

Conclusion: 该研究为异步Q学习提供了严格的统计理论基础，揭示了算法收敛的极限分布特性，对强化学习理论分析具有重要意义。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [170] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 本文提出了两种向量（标准差向量和聚类向量）来分析大语言模型的权重特征，能够有效区分不同模型并显示同系列模型的相似性。研究发现LoRA微调后，标准差向量受数据集直接影响，而聚类向量保持与预训练模型的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的架构和参数特征，特别是权重特征，以分析模型间的相关性和差异。

Method: 提出标准差向量（假设权重服从正态分布，对投影矩阵标准差进行归一化）和聚类向量（对权重投影矩阵奇异值进行K-Means聚类分组），用于描述模型特征。

Result: 两种向量能有效区分不同模型，清晰显示同系列模型的相似性。LoRA微调后，标准差向量受数据集影响，聚类向量保持与预训练模型的一致性。

Conclusion: 标准差向量和聚类向量是分析大语言模型权重特征的有效工具，能揭示模型间的差异和相关性，对模型理解和优化具有重要意义。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [171] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出了一种名为Otters的新型光电突触设计，利用光电设备的自然信号衰减特性来实现时间到首次脉冲编码，从而显著提高脉冲神经网络的能效。


<details>
  <summary>Details</summary>
Motivation: 传统脉冲神经网络虽然理论上具有高能效，但在实际推理过程中需要计算时间衰减函数和突触权重乘法，这些操作消耗大量能量。本文旨在通过硬件-软件协同设计解决这一问题。

Method: 1）制造定制氧化铟光电突触，利用其自然物理衰减直接实现时间函数；2）将设备模拟输出视为突触权重和时间衰减的融合产物；3）开发量化神经网络到SNN的转换算法，用于复杂架构如Transformer；4）完整的硬件-软件协同设计方法。

Result: 在七个GLUE基准数据集上达到最先进准确率，基于22nm商用工艺的能耗分析显示比先前领先SNNs能效提高1.77倍。

Conclusion: 本研究建立了一种新的能效SNN范式，将基础设备物理特性直接转化为强大的计算原语，所有代码和数据均已开源。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [172] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU是一个生成式时空框架，通过显式建模人类移动的语义和时间复杂性来推进移动性分析。该框架包含四个关键创新：时空概念编码器、认知轨迹记忆、生活方式概念库和任务导向生成头。在四个真实数据集上的实验表明，该方法在三个基准任务上均取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 人类移动轨迹记录了短期访问模式和持久生活方式规律，但现有方法难以有效建模移动的语义和时间复杂性。本文旨在开发一个能够更好地捕捉用户意图和生活方式规律的综合框架。

Method: 提出GSTM-HMU框架，包含四个核心组件：1）STCE编码器整合地理位置、POI类别语义和周期性时间节奏；2）CTM记忆模块自适应过滤历史访问，强调近期和行为显著事件；3）LCB概念库提供结构化的人类偏好线索；4）任务导向生成头将学习表示转化为下游任务预测。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare四个数据集上的实验表明，GSTM-HMU在下一个位置预测、轨迹用户识别和时间估计三个基准任务上均取得了持续且显著的性能提升，超越了强基线方法。

Conclusion: 生成式建模为构建更鲁棒、可解释和可泛化的人类移动智能系统提供了有前景的基础。GSTM-HMU框架有效提取了复杂移动数据中的语义规律，证明了其在移动性分析中的有效性。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [173] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 本文提出了模拟基础神经网络（SGNNs）的理论基础，证明其在模拟先验下实现摊销贝叶斯推断，收敛到贝叶斯最优预测器，并能在模型错误设定下学习经验方法无法观测的科学量。


<details>
  <summary>Details</summary>
Motivation: SGNNs在真实标签有限或不可观测的领域已达到最先进性能，但缺乏理论基础。本文旨在建立模拟基础学习的正式理论框架。

Method: 通过理论分析证明SGNNs实现摊销贝叶斯推断，推导模型错误设定下的泛化界限，并形式化SGNNs特有的机制可解释性方法。通过数值实验验证理论预测。

Result: SGNNs能够恢复潜在参数，在模型不匹配下保持鲁棒性，在模型选择任务中比AIC误差减少一半。

Conclusion: SGNNs为数据受限场景下的科学预测提供了一个原则性和实用的框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [174] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion是一个联邦迁移学习框架，通过统一域适应和节俭标注技术，结合多样性/聚类感知编码器，解决联邦学习中的异构特征空间、严重非IID数据和标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 实际联邦学习面临异构特征空间、严重非IID数据和跨客户端标签稀缺的挑战，需要一种能够协调个性化、域适应和标签效率的解决方案。

Method: 使用标记的教师客户端通过置信度过滤的伪标签和域自适应迁移指导学习者客户端，同时客户端维护针对本地数据定制的个性化编码器。采用相似性加权分类器耦合（可选聚类平均）保持全局一致性，结合自监督/半监督预训练与选择性微调的节俭标注流程。

Result: 在表格和图像基准测试中，FedFusion在IID、非IID和标签稀缺场景下，在准确性、鲁棒性和公平性方面持续优于最先进的基线方法，同时保持相当的通信和计算预算。

Conclusion: 协调个性化、域适应和标签效率是现实约束下实现鲁棒联邦学习的有效策略。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [175] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net是一种创新的参数高效框架，通过利用层间激活残差的低秩特性，采用双路径架构重构层激活，在减少参数复杂度和内存需求的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前低秩方法存在三个关键缺陷：模型性能受损、计算开销大、激活内存节省有限。需要一种既能保持高效性又不牺牲性能的低秩架构。

Method: 提出跨层低秩残差网络（CR-Net），基于层间激活残差具有低秩特性的发现，采用双路径架构将前一层输出与其低秩差异相结合来高效重构层激活，并开发专门的重计算策略降低内存需求。

Result: 在60M到7B参数规模的预训练实验中，CR-Net持续优于最先进的低秩框架，同时需要更少的计算资源和内存。

Conclusion: CR-Net成功解决了当前低秩方法的局限性，在保持高效性的同时实现了更好的性能表现，为大规模语言模型的高效预训练提供了有效解决方案。

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [176] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文综述了无标签数据表示学习的最新理论进展，特别是深度学习模型的新原理与传统统计方法的差异，以及作者在该方向上的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型使用自监督或去噪/掩码自编码器等新原理进行无监督表示学习，但这些方法难以用经典理论分析。需要结合统计和优化的数学工具来理解这些模型学到的表示及其在多样化预测任务中的表现。

Method: 结合统计学和优化的数学工具，对无标签数据表示学习的理论进行分析，特别是针对视觉基础模型中的自监督和去噪/掩码自编码器等方法。

Result: 提供了无标签数据表示学习最新理论进展的综述，并阐述了作者在该方向的具体贡献。

Conclusion: 需要新的理论框架来分析和理解深度学习模型在无监督表示学习中的工作原理和性能表现。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [177] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge是一个统一的多模态框架，支持视觉语言理解、生成和检索任务，通过轻量级双向潜在对齐模块和两阶段解耦训练策略实现多任务统一建模。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型解决方案通常孤立处理不同任务或需要从头训练，导致计算成本高且跨模态泛化能力有限。

Method: 采用语言中心设计，重用预训练LLM，引入轻量级双向潜在对齐模块，提出两阶段解耦训练策略：监督微调+潜在空间对齐，以及语义引导的扩散训练。

Result: 在广泛基准测试中，OmniBridge在所有三个任务上都达到了竞争性或最先进的性能。

Conclusion: 潜在空间对齐在共享表示空间下统一多模态建模方面具有有效性，代码和模型已开源。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [178] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 本文提出了一种结合GAN和Transformer编码器的混合方法，用于生成高质量的信用卡欺诈交易样本，以解决数据集高度不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临数据集高度不平衡的挑战，传统过采样方法（如SMOTE）生成的样本过于简单，而现有生成模型（如CTGAN、TVAE）在高维依赖建模方面仍存在问题。

Method: 采用生成对抗网络（GAN）结合Transformer编码器块，GAN用于对抗训练生成真实样本，Transformer通过自注意力机制学习丰富的特征交互。

Result: 在公开的信用卡欺诈检测数据集上测试表明，该方法在Recall、F1-score和AUC指标上相比传统方法和生成式重采样策略均有显著提升。

Conclusion: 基于Transformer的GAN方法能有效克服欺诈检测中严重的类别不平衡问题，生成高质量的少数类样本。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [179] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一个基于潜在扩散模型的黑盒对抗攻击框架，通过联合注意力蒸馏策略生成对抗样本，实现跨架构的高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒对抗攻击方法存在跨架构迁移性差和查询成本高的问题，需要一种架构无关且高效的攻击方法。

Method: 利用CNN和ViT模型的注意力图指导潜在扩散模型生成对抗样本，通过联合注意力蒸馏聚焦于跨架构敏感区域。

Result: 实验表明JAD在攻击泛化性、生成效率和跨架构迁移性方面优于现有方法。

Conclusion: JAD为黑盒对抗攻击提供了一个有前景的有效范式，具有优异的跨模型攻击能力。

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [180] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出了Diffusion Bridge Variational Inference (DBVI)，一种改进的变分推断方法，通过可学习的、数据依赖的初始分布来初始化反向扩散过程，显著提高了深度高斯过程后验推断的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Denoising diffusion variational inference (DDVI)方法使用固定的无条件起始分布，与真实复杂后验分布差距较大，导致推断轨迹效率低下和收敛缓慢。

Method: DBVI通过摊销神经网络参数化数据依赖的初始分布，利用ELBO目标的梯度逐步调整初始化，减少后验差距。网络设计为在诱导输入上操作，利用其作为数据集的结构化低维摘要。

Result: 在回归、分类和图像重建任务中，DBVI在预测准确性、收敛速度和后验质量方面均优于DDVI和其他变分基线方法。

Conclusion: DBVI在保持DDVI数学优雅性的同时，通过可学习的扩散桥接过程显著改进了深度高斯过程的后验推断性能，为大规模DGP提供了可扩展的推断解决方案。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [181] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 本文研究了带迁移学习的非上下文多臂老虎机问题，在已知源分布与目标分布距离约束下，推导了累积遗憾的渐进下界，并提出了匹配该界的KL-UCB-Transfer算法。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法未充分利用先验知识，而实际应用中常存在相关但不同的源任务数据。本文旨在利用源分布信息提升目标任务的决策效率。

Method: 首先推导包含迁移参数(d_k, L_k, N'_k)的渐进遗憾下界，然后设计KL-UCB-Transfer索引策略，在Gaussian情况下匹配该下界。

Result: 理论分析表明KL-UCB-Transfer达到最优渐进遗憾界，仿真验证当源目标分布足够接近时，该算法显著优于无先验基线方法。

Conclusion: 迁移学习框架能有效利用源任务信息提升多臂老虎机性能，所提算法在分布相似度较高时具有明显优势。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [182] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: DRO-REBEL是一个鲁棒的离线RLHF方法家族，通过Wasserstein、KL和χ²模糊集解决奖励错误指定导致的过优化问题，在保持可扩展性的同时实现最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有离线RLHF方法存在过优化问题，模型会过度拟合错误的奖励函数并偏离训练时观察到的偏好行为。

Method: 使用Fenchel对偶将鲁棒更新简化为相对奖励回归，避免了PPO风格的裁剪或辅助价值网络，提出了三种分歧的实用SGD算法。

Result: 在Emotion Alignment、ArmoRM多目标基准和HH-Alignment上的实验显示，该方法在未见偏好混合、模型规模和数据尺度上具有强鲁棒性，χ²-REBEL表现最佳。

Conclusion: 研究验证了无免费午餐权衡：半径收缩快于经验分歧集中率可实现最优参数率但丧失覆盖保证，而保证覆盖的半径只能达到O(n^{-1/4})速率。

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [183] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL是一种用于大语言模型强化学习训练的新方法，通过并发异步数据生成和模型训练，实现了硬件效率和数据时效性的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的强化学习方法在扩展性方面面临挑战，主要问题是在保持高AI加速器利用率的同时避免产生过时的离策略数据，这些数据会损害常见的RL算法性能。

Method: PipelineRL采用并发异步数据生成和模型训练，核心创新是飞行中权重更新机制。该机制允许LLM生成引擎在生成token序列过程中以最小中断接收更新的模型权重，从而最大化加速器利用率和训练数据的新鲜度。

Result: 在128个H100 GPU上进行的实验表明，PipelineRL在长形式推理任务上相比传统RL基线实现了约2倍的学习速度提升，同时保持了高度在策略的训练数据。

Conclusion: PipelineRL为解决大语言模型强化学习训练的扩展性问题提供了有效的解决方案，其开源实现也为社区做出了重要贡献。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [184] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 本文研究了激活函数在神经网络训练动态中的作用及其对强化学习中灾难性遗忘的影响，提出了能够同时产生稀疏输出和稀疏梯度的新激活函数类别——大象激活函数。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是强化学习中长期存在的挑战，现有研究主要关注算法层面，而对神经网络架构特性如何导致灾难性遗忘的理解不足。本文旨在填补这一空白，特别关注激活函数的作用。

Method: 通过分析激活函数的训练动态，发现梯度稀疏性对减少遗忘的重要性。基于此提出大象激活函数，该函数能同时产生稀疏输出和稀疏梯度。在基于价值的算法中简单替换传统激活函数即可应用。

Result: 实验表明，使用大象激活函数可以显著提高神经网络对灾难性遗忘的抵抗力，使强化学习更加样本高效和内存高效。

Conclusion: 激活函数的梯度稀疏性对减少灾难性遗忘具有重要作用，大象激活函数为缓解强化学习中的遗忘问题提供了一种简单有效的解决方案。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [185] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 本文提出了功能缩放定律（FSL），通过随机微分方程建模SGD训练过程，揭示了学习率调度对LLM预训练损失动态的影响，为优化大规模模型训练提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要关注最终损失，忽略了训练过程中的损失动态和学习率调度的影响，本文旨在填补这一空白。

Method: 采用师生核回归设置，通过在线随机梯度下降训练，利用内在时间视角和随机微分方程建模，推导出功能缩放定律。

Result: FSL能够准确描述不同学习率调度下的损失演化，理论验证了LLM预训练中的经验实践，并在0.1B到1B参数规模的实验中表现出良好的拟合和预测能力。

Conclusion: FSL框架深化了对LLM预训练动态的理解，为改进大规模模型训练提供了理论指导和实用工具。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [186] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 提出一种基于训练数据中弱鲁棒样本的模型鲁棒性验证方法，通过局部鲁棒性分析识别最易受扰动的样本作为早期脆弱性指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习分类器在干净数据集上表现优异，但对对抗性扰动和常见数据失真等扰动敏感，传统方法依赖扰动测试集评估鲁棒性，需要更早、更敏感的验证方法。

Method: 从训练数据中提取弱鲁棒样本作为验证集，这些样本是对扰动最敏感的训练实例，通过评估模型在这些样本上的表现来理解其鲁棒性弱点。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上验证了方法的有效性，表明基于弱鲁棒样本的鲁棒性验证能显著提升模型在对抗性和常见失真场景下的可靠性。

Conclusion: 该方法提供了一种更细致、更早期的模型鲁棒性评估方式，能够指导有针对性的性能改进，提升模型在实际应用中的可靠性。

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [187] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill是一个知识蒸馏框架，通过预测、特征和补丁级别的蒸馏，将大型PPG基础模型的知识转移到资源受限设备上，实现高效PPG分析。


<details>
  <summary>Details</summary>
Motivation: 光电容积脉搏波（PPG）广泛应用于可穿戴健康监测，但大型PPG基础模型难以在资源受限设备上部署，需要开发高效的模型压缩方法。

Method: PPG-Distill采用知识蒸馏框架，通过形态蒸馏保留局部波形模式，通过节律蒸馏捕捉补丁间时间结构，实现全局和局部知识的转移。

Result: 在心率估计和房颤检测任务上，PPG-Distill将学生模型性能提升高达21.8%，推理速度提升7倍，内存使用减少19倍。

Conclusion: PPG-Distill框架成功实现了在可穿戴设备上高效PPG分析，为资源受限环境下的健康监测提供了可行的解决方案。

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [188] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本文对最先进的开源文本到视频生成模型进行了系统性的延迟和能耗研究，建立了计算受限的分析模型，并通过实验验证了空间分辨率、时间长度和去噪步骤对性能的影响规律。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成系统虽然能创建高保真、时间连贯的视频片段，但计算成本巨大且能耗需求未被充分理解，需要系统研究其延迟和能耗特性。

Method: 首先开发了计算受限的分析模型来预测空间分辨率、时间长度和去噪步骤的缩放规律，然后在WAN2.1-T2V模型上进行细粒度实验验证，最后扩展到六个不同的T2V模型进行运行时和能耗分析。

Result: 实验验证了空间和时间维度的二次增长规律，以及去噪步骤的线性缩放规律，为六个不同T2V模型提供了基准参考和能耗分析。

Conclusion: 研究结果为设计和部署更可持续的生成视频系统提供了基准参考和实用见解。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [189] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 本文通过消融研究分析了电力系统中物理约束机器学习模型的混合策略，评估了从正则化到无监督损失等多种方法，使用LIPS基准测试框架在准确性、物理合规性、工业准备度和分布外泛化四个维度进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和跨境电力交换的增加，电网面临更大的不确定性和运行风险。传统物理求解器虽然准确但速度慢，机器学习模型作为快速替代方案需要更好地遵守物理定律（如基尔霍夫定律）。

Method: 采用消融研究方法，比较了将物理约束作为正则化项或无监督损失的混合策略，探索了从简单多层感知器到先进图神经网络等多种架构，使用自定义的LIPS基准测试管道进行系统评估。

Result: 研究结果揭示了物理知识整合如何影响模型在准确性、物理合规性、工业准备度和分布外泛化四个维度上的性能表现。

Conclusion: 物理约束的集成策略对机器学习模型的性能有显著影响，研究为电力系统物理约束机器学习模型的设计提供了系统指导，所有实现均可复现。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [190] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文提出了基于稳定性的对抗训练泛化分析，针对扩散策略下的凸损失函数，揭示了泛化误差随对抗扰动强度和训练步数增长的规律。


<details>
  <summary>Details</summary>
Motivation: 虽然对抗训练增强了模型鲁棒性，但存在鲁棒过拟合和泛化差距扩大的问题。现有研究已证明去中心化网络中对抗训练的收敛性，但其泛化特性尚未被探索。

Method: 采用基于稳定性的泛化分析方法，针对扩散策略下的凸损失函数进行理论推导，并通过逻辑回归进行数值实验验证。

Result: 推导出泛化误差随对抗扰动强度和训练步数增长的边界，这一发现在单智能体情况下一致，但在去中心化设置中是新颖的。数值实验验证了理论预测。

Conclusion: 该研究为去中心化对抗训练的泛化特性提供了理论分析框架，揭示了关键影响因素，对理解对抗训练的泛化行为具有重要意义。

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [191] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 该论文研究发现，在大型推理模型中，更长的思维链（CoT）和更多的回顾步骤反而会降低准确性。作者提出了"失败步骤比例（FSF）"这一指标，能更好地预测CoT的有效性，并证明删除失败分支可以显著提高推理准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在测试时需要大量计算资源用于生成长思维链，但什么构成有效的思维链仍不清楚。现有研究对"更长更好"的观点存在矛盾，需要系统性地评估思维链长度与推理质量的关系。

Method: 在10个大型推理模型上进行数学和科学推理的系统评估；引入思维链的图结构视图；提出失败步骤比例（FSF）指标；设计两种干预实验：基于指标排名候选思维链和编辑思维链删除失败分支。

Result: 发现思维链长度增加和回顾比例提高与准确性降低相关；FSF指标在预测正确性方面优于长度和回顾比例；删除失败分支的干预显著提高了推理准确性。

Conclusion: 有效的思维链是那些失败步骤较少的思维链，支持基于结构感知的测试时扩展策略，而不是盲目生成长思维链。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [192] [Understand your Users, An Ensemble Learning Framework for Natural Noise Filtering in Recommender Systems](https://arxiv.org/abs/2509.18560)
*Clarita Hawat,Wissam Al Jurdi,Jacques Bou Abdo,Jacques Demerjian,Abdallah Makhoul*

Main category: cs.IR

TL;DR: 该论文提出了一种新的框架来识别推荐系统中的噪声评分，通过区分三种用户行为现象（外部因素、偶然性和偶然交互）来提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着网络内容的指数级增长，推荐系统面临噪声定义的挑战，噪声与人类偏好和行为的变异性密切相关。需要区分不同类型的变化现象来提高推荐质量。

Method: 提出了一个模块化框架，包含三个层次：已知的自然噪声算法用于项目分类，集成学习模型用于项目精炼评估，以及基于签名的噪声识别。还提出了量化偶然性和群体验证的指标。

Result: 该方法旨在提供更干净的训练数据集，从而提高推荐系统的用户满意度和参与度。

Conclusion: 该框架通过有效识别和处理噪声评分，能够显著提升推荐系统的鲁棒性和准确性。

Abstract: The exponential growth of web content is a major key to the success for
Recommender Systems. This paper addresses the challenge of defining noise,
which is inherently related to variability in human preferences and behaviors.
In classifying changes in user tendencies, we distinguish three kinds of
phenomena: external factors that directly influence users' sentiment,
serendipity causing unexpected preference, and incidental interaction perceived
as noise. To overcome these problems, we present a new framework that
identifies noisy ratings. In this context, the proposed framework is modular,
consisting of three layers: known natural noise algorithms for item
classification, an Ensemble learning model for refined evaluation of the items
and signature-based noise identification. We further advocate the metrics that
quantitatively assess serendipity and group validation, offering higher
robustness in recommendation accuracy. Our approach aims to provide a cleaner
training dataset that would inherently improve user satisfaction and engagement
with Recommender Systems.

</details>


### [193] [The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking](https://arxiv.org/abs/2509.18575)
*Yaoyao Qian,Yifan Zeng,Yuchao Jiang,Chelsi Jain,Huazheng Wang*

Main category: cs.IR

TL;DR: 本文研究发现LLMs在多文档比较任务中存在"排序盲点"，恶意内容提供者可通过决策目标劫持和决策标准劫持两种方式操纵LLM排序系统，使特定文档获得更高排名。实验表明这些攻击在各种LLM中都有效，且更强的LLM更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在信息检索任务中的指令跟随能力如何与多文档比较任务交互，识别LLM决策过程中存在的"排序盲点"特性，探索内容提供者如何利用这一弱点影响文档排序。

Method: 通过两种攻击方法：决策目标劫持（改变成对排序系统的评估目标）和决策标准劫持（修改不同排序方案的相关性标准），在多种LLM上进行实证研究。

Result: 实验证明所提出的攻击方法在各种LLM中都有效，可以推广到多种排序方案，且发现更强的LLM更容易受到这些攻击的影响。

Conclusion: LLM排序系统存在安全漏洞，恶意行为者可以利用排序盲点操纵文档排名，这为LLM在信息检索应用中的安全性提出了重要警示。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

</details>


### [194] [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
*Yixin Liu,Yonghui Wu,Denghui Zhang,Lichao Sun*

Main category: cs.IR

TL;DR: Agentic AutoSurvey是一个多智能体框架，用于自动生成文献综述，通过四个专业智能体协作，在合成质量上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数级增长给研究人员在快速发展的领域中综合知识带来了前所未有的挑战，需要更高效的自动化文献综述方法。

Method: 采用四智能体架构：论文搜索专家、主题挖掘与聚类、学术综述撰写者和质量评估者，协同处理75-443篇论文，针对高引用覆盖率进行优化。

Result: 在COLM 2024六个代表性LLM研究主题上的实验显示，多智能体方法得分8.18/10，显著优于AutoSurvey的4.77/10，处理了847篇论文。

Conclusion: 多智能体架构代表了在快速发展的科学领域中自动文献综述生成的有意义进展。

Abstract: The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

</details>


### [195] [Robust Denoising Neural Reranker for Recommender Systems](https://arxiv.org/abs/2509.18736)
*Wenyu Mao,Shuchang Liu,Hailan Yang,Xiaobei Wang,Xiaoyu Yang,Xu Gao,Xiang Li,Lantao Hu,Han Li,Kun Gai,An Zhang,Xiang Wang*

Main category: cs.IR

TL;DR: 本文提出DNR框架，将多阶段推荐系统中的重排序任务视为检索器分数的降噪问题，通过对抗性训练优化重排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段检索-重排序框架中，检索器分数的重要性未被充分探索，而重排序本质上是从检索器分数中去除噪声的问题。

Method: 提出DNR对抗性框架，包含降噪重排序器和噪声生成模块，扩展了传统的分数误差最小化目标，增加了降噪目标、对抗性检索器分数生成目标和分布正则化项。

Result: 在三个公开数据集上的大量实验验证了DNR的有效性。

Conclusion: DNR框架成功地将重排序任务建模为检索器分数的降噪问题，通过对抗性训练显著提升了重排序性能。

Abstract: For multi-stage recommenders in industry, a user request would first trigger
a simple and efficient retriever module that selects and ranks a list of
relevant items, then calls a slower but more sophisticated deep reranking model
that refines the item arrangement before exposure to the user. The latter model
typically reranks the item list conditioned on the user's history content and
the initial ranking from retrievers. Although this two-stage retrieval-ranking
framework demonstrates practical effectiveness, the significance of retriever
scores from the previous stage has been limitedly explored, which is
informative. In this work, we first theoretically analyze the limitations of
using retriever scores as the rerankers' input directly and argue that the
reranking task is essentially a noise reduction problem from the retriever
scores. Following this notion, we derive an adversarial framework, DNR, that
associates the denoising reranker with a carefully designed noise generation
module. We extend the conventional score error minimization term with three
augmented objectives, including: 1) a denoising objective that aims to denoise
the noisy retriever scores to align with the user feedback; 2) an adversarial
retriever score generation objective that improves the exploration in the
retriever score space; and 3) a distribution regularization term that aims to
align the distribution of generated noisy retriever scores with the real ones.
Extensive experiments are conducted on three public datasets, together with
analytical support, validating the effectiveness of the proposed DNR.

</details>


### [196] [Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation](https://arxiv.org/abs/2509.18807)
*Christian Ganhör,Marta Moscati,Anna Hausberger,Shah Nawaz,Markus Schedl*

Main category: cs.IR

TL;DR: 该论文提出使用单分支神经网络结合权重共享、模态采样和对比损失来解决混合推荐系统中模态缺失问题，通过缩小模态差距来提高推荐质量。


<details>
  <summary>Details</summary>
Motivation: 传统混合推荐系统在模态缺失时推荐质量会下降，需要一种能够有效处理模态缺失情况的方法。

Method: 使用单分支神经网络，结合权重共享、模态采样和对比损失技术，与多分支网络进行对比实验。

Result: 单分支网络在热启动场景中表现有竞争力，在模态缺失场景中显著优于其他方法，且能在嵌入空间中实现模态的更好对齐。

Conclusion: 单分支网络是处理模态缺失推荐问题的有效解决方案，能够显著提升推荐系统在模态不完整情况下的性能。

Abstract: Traditional recommender systems rely on collaborative filtering, using past
user-item interactions to help users discover new items in a vast collection.
In cold start, i.e., when interaction histories of users or items are not
available, content-based recommender systems use side information instead.
Hybrid recommender systems (HRSs) often employ multimodal learning to combine
collaborative and side information, which we jointly refer to as modalities.
Though HRSs can provide recommendations when some modalities are missing, their
quality degrades. In this work, we utilize single-branch neural networks
equipped with weight sharing, modality sampling, and contrastive loss to
provide accurate recommendations even in missing modality scenarios by
narrowing the modality gap. We compare these networks with multi-branch
alternatives and conduct extensive experiments on three datasets. Six
accuracy-based and four beyond-accuracy-based metrics help assess the
recommendation quality for the different training paradigms and their
hyperparameters in warm-start and missing modality scenarios. We quantitatively
and qualitatively study the effects of these different aspects on bridging the
modality gap. Our results show that single-branch networks achieve competitive
performance in warm-start scenarios and are significantly better in missing
modality settings. Moreover, our approach leads to closer proximity of an
item's modalities in the embedding space. Our full experimental setup is
available at https://github.com/hcai-mms/single-branch-networks.

</details>


### [197] [RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints](https://arxiv.org/abs/2509.19057)
*Olawumi Olasunkanmi,Mathew Satursky,Hong Yi,Chris Bizon,Harlin Lee,Stanley Ahalt*

Main category: cs.IR

TL;DR: RELATE是一个三阶段管道，将LLM提取的生物医学关系映射到标准化本体谓词，解决了LLM输出缺乏标准化和本体对齐的问题，实现了从自由文本到结构化本体约束表示的转换。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱对药物发现和临床决策支持至关重要，但存在不完整性。LLM擅长提取生物医学关系，但其输出缺乏标准化和本体对齐，限制了知识图谱的集成。

Method: 三阶段管道：(1)使用谓词嵌入进行本体预处理，(2)基于相似性的检索增强SapBERT，(3)基于LLM的重排序并处理显式否定。结合向量搜索和上下文LLM推理。

Result: 在ChemProt基准测试中，RELATE达到52%的精确匹配和94%的准确率@10；在2,400篇HEAL项目摘要中，有效拒绝不相关关联（0.4%）并识别否定断言。

Conclusion: RELATE通过结合向量搜索和上下文LLM推理，提供了一个可扩展、语义准确的框架，将非结构化生物医学文献转换为标准化知识图谱，同时确保知识图谱增强的质量。

Abstract: Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical
decision support but remain incomplete. Large language models (LLMs) excel at
extracting biomedical relations, yet their outputs lack standardization and
alignment with ontologies, limiting KG integration. We introduce RELATE, a
three-stage pipeline that maps LLM-extracted relations to standardized ontology
predicates using ChemProt and the Biolink Model. The pipeline includes: (1)
ontology preprocessing with predicate embeddings, (2) similarity-based
retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit
negation handling. This approach transforms relation extraction from free-text
outputs to structured, ontology-constrained representations. On the ChemProt
benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400
HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)
and identifies negated assertions. RELATE captures nuanced biomedical
relationships while ensuring quality for KG augmentation. By combining vector
search with contextual LLM reasoning, RELATE provides a scalable, semantically
accurate framework for converting unstructured biomedical literature into
standardized KGs.

</details>


### [198] [A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent](https://arxiv.org/abs/2509.19209)
*Olalekan K. Akindele,Bhupesh Kumar Mishra,Kenneth Y. Wertheim*

Main category: cs.IR

TL;DR: 本文提出了一种基于知识图谱和向量搜索的RAG聊天机器人，并开发了RAG-Eval评估框架，用于在高容量工程邮件场景中提供精确、上下文丰富的响应，同时通过置信度评分增强用户信任。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话AI中表现出色，但在特定领域的准确性和避免事实不一致方面仍面临挑战，特别是在处理大型数据集时。设计有效的聊天机器人并评估其性能是该领域的关键挑战。

Method: 采用检索增强生成（RAG）方法，结合知识图谱和向量搜索检索技术，减少文档分块需求。创新性地提出了RAG-Eval评估框架，这是一个基于思维链的三方评估系统，同时评估用户查询、检索文档和生成响应。

Result: 实验表明，RAG-Eval在摘要评估任务中优于BERTScore和G-EVAL，能够可靠检测事实差距和查询不匹配问题。该方法提供了1-100%的置信度评分，增强了系统的透明度和可验证性。

Conclusion: 该方法为开发准确、用户可验证的聊天机器人提供了一条可扩展的路径，有效弥合了高层次对话流畅性和事实准确性之间的差距，特别适用于高需求、以数据为中心的环境。

Abstract: Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [199] [ExtGraph: A Fast Extraction Method of User-intended Graphs from a Relational Database](https://arxiv.org/abs/2509.18534)
*Jeongho Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: 提出了一种高效的图提取方法ExtGraph，通过外连接和物化视图的混合查询处理，能够高效地从关系数据库中提取用户意图的图结构


<details>
  <summary>Details</summary>
Motivation: 企业重要数据通常存储在关系数据库中，需要从RDBMS中提取图结构进行分析，但现有方法由于需要复杂的连接查询处理，往往无法准确提取用户意图的图

Method: ExtGraph方法采用外连接和物化视图的混合查询处理技术

Result: 在TPC-DS、DBLP和IMDB数据集上的实验表明，ExtGraph在图提取时间上比现有最优方法快2.78倍

Conclusion: ExtGraph方法能够高效准确地从关系数据库中提取用户意图的图结构，显著提升了图提取性能

Abstract: Graph analytics is widely used in many fields to analyze various complex
patterns. However, in most cases, important data in companies is stored in
RDBMS's, and so, it is necessary to extract graphs from relational databases to
perform graph analysis. Most of the existing methods do not extract a
user-intended graph since it typically requires complex join query processing.
We propose an efficient graph extraction method, \textit{ExtGraph}, which can
extract user-intended graphs efficiently by hybrid query processing of outer
join and materialized view. Through experiments using the TPC-DS, DBLP, and
IMDB datasets, we have shown that \textit{ExtGraph} outperforms the
state-of-the-art methods up to by 2.78x in terms of graph extraction time.

</details>


### [200] [CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases](https://arxiv.org/abs/2509.18670)
*Yeonwoo Jeong,Hyunji Cho,Kyuri Park,Youngjae Kim,Sungyong Park*

Main category: cs.DB

TL;DR: CALL是一种上下文感知的查询分组机制，通过基于共享集群访问模式组织查询来减少缓存未命中惩罚，从而降低搜索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入模型将不同查询映射到向量空间的相似区域，导致现代基于磁盘的向量数据库中出现非均匀的集群访问模式。现有方法优化单个查询但忽略了集群访问模式的影响，增加了缓存未命中惩罚。

Method: 提出CALL机制：1）基于共享集群访问模式进行查询分组；2）组感知预取方法减少查询组间转换时的缓存未命中；3）延迟感知集群加载。

Result: 实验结果显示，CALL将第99百分位尾部延迟降低高达33%，同时持续保持更高的缓存命中率，显著减少搜索延迟。

Conclusion: CALL通过考虑查询间的集群访问模式局部性效应，有效优化了向量数据库的查询性能，特别是在缓存管理方面取得了显著改进。

Abstract: Embedding models capture both semantic and syntactic structures of queries,
often mapping different queries to similar regions in vector space. This
results in non-uniform cluster access patterns in modern disk-based vector
databases. While existing approaches optimize individual queries, they overlook
the impact of cluster access patterns, failing to account for the locality
effects of queries that access similar clusters. This oversight increases cache
miss penalty. To minimize the cache miss penalty, we propose CALL, a
context-aware query grouping mechanism that organizes queries based on shared
cluster access patterns. Additionally, CALL incorporates a group-aware
prefetching method to minimize cache misses during transitions between query
groups and latency-aware cluster loading. Experimental results show that CALL
reduces the 99th percentile tail latency by up to 33% while consistently
maintaining a higher cache hit ratio, substantially reducing search latency.

</details>


### [201] [Teaching RDM in a smart advanced inorganic lab course and its provision in the DALIA platform](https://arxiv.org/abs/2509.18902)
*Alexander Hoffmann,Jochen Ortmeyer,Fabian Fink,Charles Tapley Hoyt,Jonathan D. Geiger,Paul Kehrein,Torsten Schrade,Sonja Herres-Pawlis*

Main category: cs.DB

TL;DR: 介绍在化学本科教学中引入电子实验室笔记本Chemotion来实施FAIR数据管理原则，通过数字化实验规划、记录和评估，提升研究数据管理能力。


<details>
  <summary>Details</summary>
Motivation: 传统手写实验记录和PDF存储方式限制了数据的重用和机器学习应用，需要培养学生掌握FAIR数据原则等关键数据素养技能。

Method: 在第五学期实验课程中使用开源电子实验室笔记本Chemotion，结合研讨会和在线培训视频，让学生数字化管理实验数据。

Result: 学生能够通过Chemotion的直观界面和存储库进行可持续的数据共享，确保元数据和分析的长期可重用性。

Conclusion: 电子实验室笔记本Chemotion是有效实施研究数据管理教育的工具，有助于培养化学学生的数据管理能力。

Abstract: Research data management (RDM) is a key data literacy skill that chemistry
students must acquire. Concepts such as the FAIR data principles (Findable,
Accessible, Interoperable, Reusable) should be taught and applied in
undergraduate studies already. Traditionally, research data from labs, theses,
and internships were handwritten and stored in inaccessible formats such as
PDFs, limiting reuse and machine learning applications. At RWTH Aachen
University, a fifth-semester lab course introduces students to the electronic
laboratory notebook (ELN) Chemotion, an open-source DFG-funded tool linked to
the national NFDI4Chem initiative. Students plan, document, and evaluate
experiments digitally, ensuring metadata and analysis are captured for
long-term reuse. Chemotion's intuitive interface and repository enable
sustainable data sharing. To reinforce RDM, students receive a seminar and
access to online training videos with interactive Moodle elements. Herein we
highlight the use of the DALIA platform as a discovery tool for the students.

</details>


### [202] [A decentralized future for the open-science databases](https://arxiv.org/abs/2509.19206)
*Gaurav Sharma,Viorel Munteanu,Nika Mansouri Ghiasi,Jineta Banerjee,Susheel Varma,Luca Foschini,Kyle Ellrott,Onur Mutlu,Dumitru Ciorbă,Roel A. Ophoff,Viorel Bostan,Christopher E Mason,Jason H. Moore,Despoina Sousoni,Arunkumar Krishnan,Christopher E. Mason,Mihai Dimian,Gustavo Stolovitzky,Fabio G. Liberante,Taras K. Oleksyk,Serghei Mangul*

Main category: cs.DB

TL;DR: 本文分析了集中式生物数据存储库的脆弱性，提出采用联邦和去中心化架构的混合框架来增强科学数据基础设施的韧性，确保数据的长期完整性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 集中式数据存储库存在单点故障风险，易受网络攻击、技术故障、自然灾害、资金和政治不确定性影响，可能导致数据不可用、丢失或完整性受损，阻碍科学进步。

Method: 通过分析集中式存储库的结构局限性，评估联邦和去中心化模型，提出一个混合框架，结合FAIR原则（可查找、可访问、可互操作、可重用）实现可持续的科学数据管理。

Result: 提出的混合框架能显著降低治理不稳定、基础设施脆弱性和资金波动带来的风险，同时促进公平性和全球可访问性。

Conclusion: 开放科学的未来依赖于整合联邦和去中心化方法，建立全球分布式、经济可持续且制度稳健的基础设施，将科学数据作为公共产品加以保护，确保其长期可访问性、互操作性和保存。

Abstract: Continuous and reliable access to curated biological data repositories is
indispensable for accelerating rigorous scientific inquiry and fostering
reproducible research. Centralized repositories, though widely used, are
vulnerable to single points of failure arising from cyberattacks, technical
faults, natural disasters, or funding and political uncertainties. This can
lead to widespread data unavailability, data loss, integrity compromises, and
substantial delays in critical research, ultimately impeding scientific
progress. Centralizing essential scientific resources in a single geopolitical
or institutional hub is inherently dangerous, as any disruption can paralyze
diverse ongoing research. The rapid acceleration of data generation, combined
with an increasingly volatile global landscape, necessitates a critical
re-evaluation of the sustainability of centralized models. Implementing
federated and decentralized architectures presents a compelling and
future-oriented pathway to substantially strengthen the resilience of
scientific data infrastructures, thereby mitigating vulnerabilities and
ensuring the long-term integrity of data. Here, we examine the structural
limitations of centralized repositories, evaluate federated and decentralized
models, and propose a hybrid framework for resilient, FAIR, and sustainable
scientific data stewardship. Such an approach offers a significant reduction in
exposure to governance instability, infrastructural fragility, and funding
volatility, and also fosters fairness and global accessibility. The future of
open science depends on integrating these complementary approaches to establish
a globally distributed, economically sustainable, and institutionally robust
infrastructure that safeguards scientific data as a public good, further
ensuring continued accessibility, interoperability, and preservation for
generations to come.

</details>


### [203] [Gate-Based and Annealing-Based Quantum Algorithms for the Maximum K-Plex Problem](https://arxiv.org/abs/2509.19214)
*Xiaofan Li,Gao Cong,Rui Zhou*

Main category: cs.DB

TL;DR: 本文研究了最大k-plex问题，提出了两种量子算法：基于门的qTKP和qMKP算法，以及基于退火的qaMKP算法，显著降低了时间复杂度。


<details>
  <summary>Details</summary>
Motivation: k-plex模型作为团的松弛形式，更适合分析现实世界图中的噪声和不完美数据。最大k-plex问题在社交网络分析、社区检测等领域有重要应用，但现有经典算法时间复杂度较高。

Method: 提出了两种量子算法：1）基于门的qTKP算法，结合量子搜索与图编码、度计数等技术；2）基于退火的qaMKP算法，将问题重新表述为二次无约束二进制优化问题。

Result: qTKP和qMKP算法实现了O*(1.42^n)的时间复杂度，相比经典算法的O*(c_k^n)（c_k>1.94）有显著提升。qaMKP算法能更高效地利用量子比特资源。

Conclusion: 量子算法在解决最大k-plex问题上展现出显著优势，该工作可推广到其他团松弛问题如n-clan和n-club。

Abstract: The $ k $-plex model, which allows each vertex to miss connections with up to
$ k $ neighbors, serves as a relaxation of the clique. Its adaptability makes
it more suitable for analyzing real-world graphs where noise and imperfect data
are common and the ideal clique model is often impractical. The problem of
identifying the maximum $ k $-plex (MKP, which is NP-hard) is gaining attention
in fields such as social network analysis, community detection, terrorist
network identification, and graph clustering. Recent works have focused on
optimizing the time complexity of MKP algorithms. The state-of-the-art has
reduced the complexity from a trivial $ O^*(2^n) $ to $ O^*(c_k^n) $, with $
c_k > 1.94 $ for $ k \geq 3 $, where $ n $ denotes the vertex number. This
paper investigates the MKP using two quantum models: gate-based model and
annealing-based model. Two gate-based algorithms, qTKP and qMKP, are proposed
to achieve $ O^*(1.42^n) $ time complexity. qTKP integrates quantum search with
graph encoding, degree counting, degree comparison, and size determination to
find a $ k $-plex of a given size; qMKP uses binary search to progressively
identify the maximum solution. Furthermore, by reformulating MKP as a quadratic
unconstrained binary optimization problem, we propose qaMKP, the first
annealing-based approximation algorithm, which utilizes qubit resources more
efficiently than gate-based algorithms. To validate the practical performance,
proof-of-principle experiments were conducted using the latest IBM gate-based
quantum simulator and D-Wave adiabatic quantum computer. This work holds
potential to be applied to a wide range of clique relaxations, e.g., $ n $-clan
and $ n $-club.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [204] [CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese Patronizing and Condescending Language Detection](https://arxiv.org/abs/2509.18562)
*Jiaxun Yang,Yifei Han,Long Zhang,Liu Yujie,Bin Li,Bo Gao,Yangfan He,Kejia Zhan*

Main category: cs.MM

TL;DR: 该研究构建了包含10.3万条评论的新数据集PCLMMPLUS，并提出了CPCLDetector模型，用于更准确地检测中文视频平台上的傲慢与居高临下语言(CPCL)。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏用户评论数据，这影响了模型对视频内容的理解，导致无法有效检测CPCL视频。

Method: 提出CPCLDetector模型，包含对齐选择和知识增强的评论内容模块，并构建了包含评论的新数据集PCLMMPLUS。

Result: 实验表明CPCLDetector在PCLMM数据集上优于现有最优方法，在PCLMMPLUS数据集上表现更佳。

Conclusion: 该方法能更准确地检测CPCL视频，有助于内容治理和保护弱势群体。

Abstract: Chinese Patronizing and Condescending Language (CPCL) is an implicitly
discriminatory toxic speech targeting vulnerable groups on Chinese video
platforms. The existing dataset lacks user comments, which are a direct
reflection of video content. This undermines the model's understanding of video
content and results in the failure to detect some CPLC videos. To make up for
this loss, this research reconstructs a new dataset PCLMMPLUS that includes
103k comment entries and expands the dataset size. We also propose the
CPCLDetector model with alignment selection and knowledge-enhanced comment
content modules. Extensive experiments show the proposed CPCLDetector
outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS .
CPLC videos are detected more accurately, supporting content governance and
protecting vulnerable groups. Code and dataset are available at
https://github.com/jiaxunyang256/PCLD.

</details>


### [205] [Harnessing Multimodal Large Language Models for Personalized Product Search with Query-aware Refinement](https://arxiv.org/abs/2509.18682)
*Beibei Zhang,Yanan Lu,Ruobing Xie,Zongyi Li,Siyuan Xing,Tongwei Ren,Fen Lin*

Main category: cs.MM

TL;DR: HMPPS是一个利用多模态大语言模型处理个性化产品搜索的新框架，通过查询感知的细化模块解决多模态内容中的冗余和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-based个性化产品搜索方法仅考虑文本内容，忽略了多模态内容在产品搜索中的关键作用。

Method: 提出HMPPS框架，包含两个查询感知细化模块：1）基于核心视角的产品描述总结模块；2）基于多模态表示的两阶段训练范式。

Result: 在四个公共数据集上的广泛实验证明了HMPPS的有效性，并在拥有数十亿日活用户的在线搜索系统中通过A/B测试显示出明显增益。

Conclusion: HMPPS成功地将多模态大语言模型应用于个性化产品搜索，有效解决了多模态内容中的噪声和冗余问题，显著提升了搜索性能。

Abstract: Personalized product search (PPS) aims to retrieve products relevant to the
given query considering user preferences within their purchase histories. Since
large language models (LLM) exhibit impressive potential in content
understanding and reasoning, current methods explore to leverage LLM to
comprehend the complicated relationships among user, query and product to
improve the search performance of PPS. Despite the progress, LLM-based PPS
solutions merely take textual contents into consideration, neglecting
multimodal contents which play a critical role for product search. Motivated by
this, we propose a novel framework, HMPPS, for \textbf{H}arnessing
\textbf{M}ultimodal large language models (MLLM) to deal with
\textbf{P}ersonalized \textbf{P}roduct \textbf{S}earch based on multimodal
contents. Nevertheless, the redundancy and noise in PPS input stand for a great
challenge to apply MLLM for PPS, which not only misleads MLLM to generate
inaccurate search results but also increases the computation expense of MLLM.
To deal with this problem, we additionally design two query-aware refinement
modules for HMPPS: 1) a perspective-guided summarization module that generates
refined product descriptions around core perspectives relevant to search query,
reducing noise and redundancy within textual contents; and 2) a two-stage
training paradigm that introduces search query for user history filtering based
on multimodal representations, capturing precise user preferences and
decreasing the inference cost. Extensive experiments are conducted on four
public datasets to demonstrate the effectiveness of HMPPS. Furthermore, HMPPS
is deployed on an online search system with billion-level daily active users
and achieves an evident gain in A/B testing.

</details>
