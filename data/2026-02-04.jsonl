{"id": "2602.02630", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02630", "abs": "https://arxiv.org/abs/2602.02630", "authors": ["Roberto Balestri", "Pasquale Cascarano", "Mirko Degli Esposti", "Guglielmo Pescatore"], "title": "Trailer Reimagined: An Innovative, Llm-DRiven, Expressive Automated Movie Summary framework (TRAILDREAMS)", "comment": null, "summary": "This paper introduces TRAILDREAMS, a framework that uses a large language model (LLM) to automate the production of movie trailers. The purpose of LLM is to select key visual sequences and impactful dialogues, and to help TRAILDREAMS to generate audio elements such as music and voiceovers. The goal is to produce engaging and visually appealing trailers efficiently. In comparative evaluations, TRAILDREAMS surpasses current state-of-the-art trailer generation methods in viewer ratings. However, it still falls short when compared to real, human-crafted trailers. While TRAILDREAMS demonstrates significant promise and marks an advancement in automated creative processes, further improvements are necessary to bridge the quality gap with traditional trailers."}
{"id": "2602.02999", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.02999", "abs": "https://arxiv.org/abs/2602.02999", "authors": ["Zhengle Wang", "Yanfei Zhang", "Chunwei Liu"], "title": "ResQ: Realistic Performance-Aware Query Generation", "comment": "13 pages, 4 figures", "summary": "Database research and development rely heavily on realistic user workloads for benchmarking, instance optimization, migration testing, and database tuning. However, acquiring real-world SQL queries is notoriously challenging due to strict privacy regulations. While cloud database vendors have begun releasing anonymized performance traces to the research community, these traces typi- cally provide only high-level execution statistics without the origi- nal query text or data, which is insufficient for scenarios that require actual execution. Existing tools fail to capture fine-grained perfor- mance patterns or generate runnable workloads that reproduce these public traces with both high fidelity and efficiency. To bridge this gap, we propose ResQ, a fine-grained workload synthesis sys- tem designed to generate executable SQL workloads that faithfully match the per-query execution targets and operator distributions of production traces. ResQ constructs execution-aware query graphs, instantiates them into SQL via Bayesian Optimization-driven pred- icate search, and explicitly models workload repetition through reuse at both exact-query and parameterized-template levels. To ensure practical scalability, ResQ combines search-space bounding with lightweight local cost models to accelerate optimization. Ex- periments on public cloud traces (Snowset, Redset) and a newly released industrial trace (Bendset) demonstrate that ResQ signif- icantly outperforms state-of-the-art baselines, achieving 96.71% token savings and a 86.97% reduction in runtime, while lowering maximum Q-error by 14.8x on CPU time and 997.7x on scanned bytes, and closely matching operator composition."}
{"id": "2602.03069", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.03069", "abs": "https://arxiv.org/abs/2602.03069", "authors": ["Yue Wu", "Tianhao Su", "Shunbo Hu", "Deng Pan"], "title": "Skill-Based Autonomous Agents for Material Creep Database Construction", "comment": null, "summary": "The advancement of data-driven materials science is currently constrained by a fundamental bottleneck: the vast majority of historical experimental data remains locked within the unstructured text and rasterized figures of legacy scientific literature. Manual curation of this knowledge is prohibitively labor-intensive and prone to human error. To address this challenge, we introduce an autonomous, agent-based framework powered by Large Language Models (LLMs) designed to excavate high-fidelity datasets from scientific PDFs without human intervention. By deploying a modular \"skill-based\" architecture, the agent orchestrates complex cognitive tasks - including semantic filtering, multi-modal information extraction, and physics-informed validation. We demonstrate the efficacy of this framework by constructing a physically self-consistent database for material creep mechanics, a domain characterized by complex graphical trajectories and heterogeneous constitutive models. Applying the pipeline to 243 publications, the agent achieved a verified extraction success rate exceeding 90% for graphical data digitization. Crucially, we introduce a cross-modal verification protocol, demonstrating that the agent can autonomously align visually extracted data points with textually extracted constitutive parameters ($R^2 > 0.99$), ensuring the physical self-consistency of the database. This work not only provides a critical resource for investigating time-dependent deformation across diverse material systems but also establishes a scalable paradigm for autonomous knowledge acquisition, paving the way for the next generation of self-driving laboratories."}
{"id": "2602.03189", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.03189", "abs": "https://arxiv.org/abs/2602.03189", "authors": ["Yong Fang", "Yuxing Han", "Meng Wang", "Yifan Zhang", "Yue Ma", "Chi Zhang"], "title": "StreamShield: A Production-Proven Resiliency Solution for Apache Flink at ByteDance", "comment": null, "summary": "Distributed Stream Processing Systems (DSPSs) form the backbone of real-time processing and analytics at ByteDance, where Apache Flink powers one of the largest production clusters worldwide. Ensuring resiliency, the ability to withstand and rapidly recover from failures, together with operational stability, which provides consistent and predictable performance under normal conditions, is essential for meeting strict Service Level Objectives (SLOs). However, achieving resiliency and stability in large-scale production environments remains challenging due to the cluster scale, business diversity, and significant operational overhead. In this work, we present StreamShield, a production-proven resiliency solution deployed in ByteDance's Flink clusters. Designed along complementary perspectives of the engine and cluster, StreamShield introduces key techniques to enhance resiliency, covering runtime optimization, fine-grained fault-tolerance, hybrid replication strategy, and high availability under external systems. Furthermore, StreamShield proposes a robust testing and deployment pipeline that ensures reliability and robustness in production releases. Extensive evaluations on a production cluster demonstrate the efficiency and effectiveness of techniques proposed by StreamShield."}
{"id": "2602.03278", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.03278", "abs": "https://arxiv.org/abs/2602.03278", "authors": ["Saige Rutherford", "Zeshawn Zahid", "Robert C. Welsh", "Andrea Avena-Koenigsberger", "Vincent Koppelmans", "Amanda F. Mejia"], "title": "A Pipeline for ADNI Resting-State Functional MRI Processing and Quality Control", "comment": null, "summary": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a comprehensive multimodal neuroimaging resource for studying aging and Alzheimer's disease (AD). Since its second wave, ADNI has increasingly collected resting-state functional MRI (rs-fMRI), a valuable resource for discovering brain connectivity changes predictive of cognitive decline and AD. A major barrier to its use is the considerable variability in acquisition protocols and data quality, compounded by missing imaging sessions and inconsistencies in how functional scans temporally align with clinical assessments. As a result, many studies only utilize a small subset of the total rs-fMRI data, limiting statistical power, reproducibility, and the ability to study longitudinal functional brain changes at scale. Here, we describe a pipeline for ADNI rs-fMRI data that encompasses the download of necessary imaging and clinical data, temporally aligning the clinical and imaging data, preprocessing, and quality control. We integrate data curation and preprocessing across all ADNI sites and scanner types using a combination of open-source software (Clinica, fMRIPrep, and MRIQC) and bespoke tools. Quality metrics and reports are generated for each subject and session to facilitate rigorous data screening. All scripts and configuration files are available to enable reproducibility. The pipeline, which currently supports ADNI-GO, ADNI-2, and ADNI-3 data releases, outputs high-quality rs-fMRI time series data adhering to the BIDS-derivatives specification. This protocol provides a transparent and scalable framework for curating and utilizing ADNI fMRI data, empowering large-scale functional biomarker discovery and integrative multimodal analyses in Alzheimer's disease research."}
{"id": "2602.02514", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02514", "abs": "https://arxiv.org/abs/2602.02514", "authors": ["Pratik Lahiri", "Bingqing Ge", "Zhou Qin", "Aditya Jumde", "Shuning Huo", "Lucas Scottini", "Yi Liu", "Mahmoud Mamlouk", "Wenyang Liu"], "title": "Design and Evaluation of Whole-Page Experience Optimization for E-commerce Search", "comment": null, "summary": "E-commerce Search Results Pages (SRPs) are evolving from linear lists to complex, non-linear layouts, rendering traditional position-biased ranking models insufficient. Moreover, existing optimization frameworks typically maximize short-term signals (e.g., clicks, same-day revenue) because long-term satisfaction metrics (e.g., expected two-week revenue) involve delayed feedback and challenging long-horizon credit attribution. To bridge these gaps, we propose a novel Whole-Page Experience Optimization Framework. Unlike traditional list-wise rankers, our approach explicitly models the interplay between item relevance, 2D positional layout, and visual elements. We use a causal framework to develop metrics for measuring long-term user satisfaction based on quasi-experimental data. We validate our approach through industry-scale A/B testing, where the model demonstrated a 1.86% improvement in brand relevance (our primary customer experience metric) while simultaneously achieving a statistically significant revenue uplift of +0.05%"}
{"id": "2602.02628", "categories": ["cs.GT", "cs.CC", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.02628", "abs": "https://arxiv.org/abs/2602.02628", "authors": ["Florian Galliot", "Nacim Oijid", "Jonas Sénizergues"], "title": "A two-player version of the assignment problem", "comment": null, "summary": "We introduce the competitive assignment problem, a two-player version of the well-known assignment problem. Given a set of tasks and a set of agents with different efficiencies for different tasks, Alice and Bob take turns picking agents one by one. Once all agents have been picked, Alice and Bob compute the optimal values $s_A$ and $s_B$ for the assignment problem on their respective sets of agents, i.e. they assign their own agents to tasks (with at most one agent per task and at most one task per agent) so as to maximize the sum of the efficiencies. The score of the game is then defined as $s_A-s_B$. Alice aims at maximizing the score, while Bob aims at minimizing it. This problem can model drafts in sports and card games, or more generally situations where two entities fight for the same resources and then use them to compete against each other. We show that the problem is PSPACE-complete, even restricted to agents that have at most two nonzero efficiencies. On the other hand, in the case of agents having at most one nonzero efficiency, the problem lies in XP parameterized by the number of tasks, and the optimal score can be computed in linear time when there are only two tasks."}
{"id": "2602.02508", "categories": ["cs.IT", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.02508", "abs": "https://arxiv.org/abs/2602.02508", "authors": ["Xi Chen", "Homa Esfahanizadeh", "Foad Sohrabi"], "title": "Precoding-Oriented CSI Feedback Design with Mutual Information Regularized VQ-VAE", "comment": "5 pages, submitted to IEEE VTC conference", "summary": "Efficient channel state information (CSI) compression at the user equipment plays a key role in enabling accurate channel reconstruction and precoder design in massive multiple-input multiple-output systems. A key challenge lies in balancing the CSI feedback overhead with the achievable downlink rate, i.e., maximizing the utility of limited feedback to maintain high system performance. In this work, we propose a precoding-oriented CSI feedback framework based on a vector quantized variational autoencoder, augmented with an information-theoretic regularization. To achieve this, we introduce a differentiable mutual information lower-bound estimator as a training regularizer to promote effective utilization of the learned codebook under a fixed feedback budget. Numerical results demonstrate that the proposed method achieves rates comparable to variable-length neural compression schemes, while operating with fixed-length feedback. Furthermore, the learned codewords exhibit significantly more uniform usage and capture interpretable structures that are strongly correlated with the underlying channel state information."}
{"id": "2602.02505", "categories": ["cs.DS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02505", "abs": "https://arxiv.org/abs/2602.02505", "authors": ["Hao-Yuan He", "Ming Li"], "title": "Learning-augmented smooth integer programs with PAC-learnable oracles", "comment": null, "summary": "This paper investigates learning-augmented algorithms for smooth integer programs, covering canonical problems such as MAX-CUT and MAX-k-SAT. We introduce a framework that incorporates a predictive oracle to construct a linear surrogate of the objective, which is then solved via linear programming followed by a rounding procedure. Crucially, our framework ensures that the solution quality is both consistent and smooth against prediction errors. We demonstrate that this approach effectively extends tractable approximations from the classical dense regime to the near-dense regime. Furthermore, we go beyond the assumption of oracle existence by establishing its PAC-learnability. We prove that the induced algorithm class possesses a bounded pseudo-dimension, thereby ensuring that an oracle with near-optimal expected performance can be learned with polynomial samples."}
{"id": "2602.02827", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02827", "abs": "https://arxiv.org/abs/2602.02827", "authors": ["Roi Pony", "Adi Raz", "Oshri Naparstek", "Idan Friedman", "Udi Barzelay"], "title": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval", "comment": null, "summary": "Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-$K$ identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5$\\times$, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time."}
{"id": "2602.03145", "categories": ["cs.GT", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03145", "abs": "https://arxiv.org/abs/2602.03145", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "title": "Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow", "comment": null, "summary": "Large language models (LLMs) have enabled a new class of agentic AI systems that reason, plan, and act by invoking external tools. However, most existing agentic architectures remain centralized and monolithic, limiting scalability, specialization, and interoperability. This paper proposes a framework for scalable agentic intelligence, termed the Internet of Agentic AI, in which autonomous, heterogeneous agents distributed across cloud and edge infrastructure dynamically form coalitions to execute task-driven workflows. We formalize a network-native model of agentic collaboration and introduce an incentive-compatible workflow-coalition feasibility framework that integrates capability coverage, network locality, and economic implementability. To enable scalable coordination, we formulate a minimum-effort coalition selection problem and propose a decentralized coalition formation algorithm. The proposed framework can operate as a coordination layer above the Model Context Protocol (MCP). A healthcare case study demonstrates how domain specialization, cloud-edge heterogeneity, and dynamic coalition formation enable scalable, resilient, and economically viable agentic workflows. This work lays the foundation for principled coordination and scalability in the emerging era of Internet of Agentic AI."}
{"id": "2602.02768", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02768", "abs": "https://arxiv.org/abs/2602.02768", "authors": ["Ronald Ogden", "David Fridovich-Keil", "Takashi Tanaka"], "title": "Rate-Distortion Analysis of Optically Passive Vision Compression", "comment": null, "summary": "The use of remote vision sensors for autonomous decision-making poses the challenge of transmitting high-volume visual data over resource-constrained channels in real-time. In robotics and control applications, many systems can quickly destabilize, which can exacerbate the issue by necessitating higher sampling frequencies. This work proposes a novel sensing paradigm in which an event camera observes the optically generated cosine transform of a visual scene, enabling high-speed, computation-free video compression inspired by modern video codecs. In this study, we simulate this optically passive vision compression (OPVC) scheme and compare its rate-distortion performance to that of a standalone event camera (SAEC). We find that the rate-distortion performance of the OPVC scheme surpasses that of the SAEC and that this performance gap increases as the spatial resolution of the event camera increases."}
{"id": "2602.03436", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.03436", "abs": "https://arxiv.org/abs/2602.03436", "authors": ["Kenta Komoto", "Kazuhiro Kurita", "Hirotaka Ono"], "title": "On the Complexity of Maximal/Closed Frequent Tree Mining for Bounded Height Trees", "comment": null, "summary": "In this paper, we address the problem of enumerating all frequent maximal/closed trees. This is a classical and central problem in data mining. Although many practical algorithms have been developed for this problem, its complexity under ``realistic assumptions'' on tree height has not been clarified. More specifically, while it was known that the mining problem becomes hard when the tree height is at least 60, the complexity for cases where the tree height is smaller has not yet been clarified. We resolve this gap by establishing results for these tree mining problems under several settings, including ordered and unordered trees, as well as maximal and closed variants."}
{"id": "2602.02883", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02883", "abs": "https://arxiv.org/abs/2602.02883", "authors": ["Parker Carlson", "Wentai Xie", "Rohil Shah", "Tao Yang"], "title": "Efficiency Optimizations for Superblock-based Sparse Retrieval", "comment": "11 pages, 5 figures, 9 tables. Under review", "summary": "Learned sparse retrieval (LSR) is a popular method for first-stage retrieval because it combines the semantic matching of language models with efficient CPU-friendly algorithms. Previous work aggregates blocks into \"superblocks\" to quickly skip the visitation of blocks during query processing by using an advanced pruning heuristic. This paper proposes a simple and effective superblock pruning scheme that reduces the overhead of superblock score computation while preserving competitive relevance. It combines this scheme with a compact index structure and a robust zero-shot configuration that is effective across LSR models and multiple datasets. This paper provides an analytical justification and evaluation on the MS MARCO and BEIR datasets, demonstrating that the proposed scheme can be a strong alternative for efficient sparse retrieval."}
{"id": "2602.03381", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.03381", "abs": "https://arxiv.org/abs/2602.03381", "authors": ["Axel Benyamine", "Julien Grand-Clément", "Marek Petrik", "Michael I. Jordan", "Alain Durmus"], "title": "Dynamic Programming for Epistemic Uncertainty in Markov Decision Processes", "comment": null, "summary": "In this paper, we propose a general theory of ambiguity-averse MDPs, which treats the uncertain transition probabilities as random variables and evaluates a policy via a risk measure applied to its random return. This ambiguity-averse MDP framework unifies several models of MDPs with epistemic uncertainty for specific choices of risk measures. We extend the concepts of value functions and Bellman operators to our setting. Based on these objects, we establish the consequences of dynamic programming principles in this framework (existence of stationary policies, value and policy iteration algorithms), and we completely characterize law-invariant risk measures compatible with dynamic programming. Our work draws connections among several variants of MDP models and fully delineates what is possible under the dynamic programming paradigm and which risk measures require leaving it."}
{"id": "2602.03074", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.03074", "abs": "https://arxiv.org/abs/2602.03074", "authors": ["Xi Zhong", "Jörg Kliewer", "Mingyue Ji"], "title": "Straggler-Aware Coded Polynomial Aggregation", "comment": "6 pages, 1 figure", "summary": "Coded polynomial aggregation (CPA) in distributed computing systems enables the master to directly recover a weighted aggregation of polynomial computations without individually decoding each term, thereby reducing the number of required worker responses. However, existing CPA schemes are restricted to an idealized setting in which the system cannot tolerate stragglers. In this paper, we extend CPA to straggler-aware distributed computing systems with a pre-specified non-straggler pattern, where exact recovery is required for a given collection of admissible non-straggler sets. Our main results show that exact recovery of the desired aggregation is achievable with fewer worker responses than that required by polynomial codes based on individual decoding, and that feasibility is characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA. We identify an intersection-size threshold that is sufficient to guarantee exact recovery. When the number of admissible non-straggler sets is sufficiently large, we further show that this threshold is necessary in a generic sense. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations verify our theoretical results by demonstrating a sharp feasibility transition at the predicted intersection threshold."}
{"id": "2602.03525", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.03525", "abs": "https://arxiv.org/abs/2602.03525", "authors": ["Antoine Limasset"], "title": "ZOR filters: fast and smaller than fuse filters", "comment": null, "summary": "Probabilistic membership filters support fast approximate membership queries with a controlled false-positive probability $\\varepsilon$ and are widely used across storage, analytics, networking, and bioinformatics \\cite{chang2008bigtable,dayan2018optimalbloom,broder2004network,harris2020improved,marchet2023scalable,chikhi2025logan,hernandez2025reindeer2}. In the static setting, state-of-the-art designs such as XOR and fuse filters achieve low overhead and very fast queries, but their peeling-based construction succeeds only with high probability, which complicates deterministic builds \\cite{graf2020xor,graf2022binary,ulrich2023taxor}.\n  We introduce \\emph{ZOR filters}, a deterministic continuation of XOR/fuse filters that guarantees construction termination while preserving the same XOR-based query mechanism. ZOR replaces restart-on-failure with deterministic peeling that abandons a small fraction of keys, and restores false-positive-only semantics by storing the remainder in a compact auxiliary structure. In our experiments, the abandoned fraction drops below $1\\%$ for moderate arity (e.g., $N\\ge 5$), so the auxiliary handles a negligible fraction of keys. As a result, ZOR filters can achieve overhead within $1\\%$ of the information-theoretic lower bound $\\log_2(1/\\varepsilon)$ while retaining fuse-like query performance; the additional cost is concentrated on negative queries due to the auxiliary check. Our current prototype builds several-fold slower than highly optimized fuse builders because it maintains explicit incidence information during deterministic peeling; closing this optimisation gap is an engineering target."}
{"id": "2602.03056", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03056", "abs": "https://arxiv.org/abs/2602.03056", "authors": ["Lu Ren", "Junda She", "Xinchen Luo", "Tao Wang", "Xin Ye", "Xu Zhang", "Muxuan Wang", "Xiao Yang", "Chenguang Wang", "Fei Xie", "Yiwei Zhou", "Danjun Wu", "Guodong Zhang", "Yifei Hu", "Guoying Zheng", "Shujie Yang", "Xingmei Wang", "Shiyao Wang", "Yukun Zhou", "Fan Yang", "Size Li", "Kuo Cai", "Qiang Luo", "Ruiming Tang", "Han Li", "Kun Gai"], "title": "ALPBench: A Benchmark for Attribution-level Long-term Personal Behavior Understanding", "comment": null, "summary": "Recent advances in large language models have highlighted their potential for personalized recommendation, where accurately capturing user preferences remains a key challenge. Leveraging their strong reasoning and generalization capabilities, LLMs offer new opportunities for modeling long-term user behavior. To systematically evaluate this, we introduce ALPBench, a Benchmark for Attribution-level Long-term Personal Behavior Understanding. Unlike item-focused benchmarks, ALPBench predicts user-interested attribute combinations, enabling ground-truth evaluation even for newly introduced items. It models preferences from long-term historical behaviors rather than users' explicitly expressed requests, better reflecting enduring interests. User histories are represented as natural language sequences, allowing interpretable, reasoning-based personalization. ALPBench enables fine-grained evaluation of personalization by focusing on the prediction of attribute combinations task that remains highly challenging for current LLMs due to the need to capture complex interactions among multiple attributes and reason over long-term user behavior sequences."}
{"id": "2602.03387", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03387", "abs": "https://arxiv.org/abs/2602.03387", "authors": ["Zhengwei Ni", "Zhidu Li", "Wei Chen", "Zhaoyang Zhang", "Zehua Wang", "F. Richard Yu", "Victor C. M. Leung"], "title": "Toward a Sustainable Federated Learning Ecosystem: A Practical Least Core Mechanism for Payoff Allocation", "comment": "7 pages, 3 figures, submitted to IEEE Network", "summary": "Emerging network paradigms and applications increasingly rely on federated learning (FL) to enable collaborative intelligence while preserving privacy. However, the sustainability of such collaborative environments hinges on a fair and stable payoff allocation mechanism. Focusing on coalition stability, this paper introduces a payoff allocation framework based on the least core (LC) concept. Unlike traditional methods, the LC prioritizes the cohesion of the federation by minimizing the maximum dissatisfaction among all potential subgroups, ensuring that no participant has an incentive to break away. To adapt this game-theoretic concept to practical, large-scale networks, we propose a streamlined implementation with a stack-based pruning algorithm, effectively balancing computational efficiency with allocation precision. Case studies in federated intrusion detection demonstrate that our mechanism correctly identifies pivotal contributors and strategic alliances. The results confirm that the practical LC framework promotes stable collaboration and fosters a sustainable FL ecosystem."}
{"id": "2602.03363", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.03363", "abs": "https://arxiv.org/abs/2602.03363", "authors": ["Kaizhe He", "Qi Chen"], "title": "Entropy Functions on Two-Dimensional Faces of Polymatroid Region with One Extreme Ray Containing Rank-One Matroid", "comment": null, "summary": "Characterization of entropy functions is of fundamental importance in information theory. By imposing constraints on their Shannon outer bound, i.e., the polymatroidal region, one obtains the faces of the region and entropy functions on them with special structures. In this paper, we characterize entropy functions on 2-dimensional faces of polymatroid region of degree n with one extreme ray containing rank-1 matroid. We classify all such 2-dimensional faces with another extreme ray containing a matroid into four types."}
{"id": "2602.03827", "categories": ["cs.DS", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.03827", "abs": "https://arxiv.org/abs/2602.03827", "authors": ["Matthias Bentert", "Stefan Schmid"], "title": "Perfect Network Resilience in Polynomial Time", "comment": null, "summary": "Modern communication networks support local fast rerouting mechanisms to quickly react to link failures: nodes store a set of conditional rerouting rules which define how to forward an incoming packet in case of incident link failures. The rerouting decisions at any node $v$ must rely solely on local information available at $v$: the link from which a packet arrived at $v$, the target of the packet, and the incident link failures at $v$. Ideally, such rerouting mechanisms provide perfect resilience: any packet is routed from its source to its target as long as the two are connected in the underlying graph after the link failures. Already in their seminal paper at ACM PODC '12, Feigenbaum, Godfrey, Panda, Schapira, Shenker, and Singla showed that perfect resilience cannot always be achieved. While the design of local rerouting algorithms has received much attention since then, we still lack a detailed understanding of when perfect resilience is achievable.\n  This paper closes this gap and presents a complete characterization of when perfect resilience can be achieved. This characterization also allows us to design an $O(n)$-time algorithm to decide whether a given instance is perfectly resilient and an $O(nm)$-time algorithm to compute perfectly resilient rerouting rules whenever it is. Our algorithm is also attractive for the simple structure of the rerouting rules it uses, known as skipping in the literature: alternative links are chosen according to an ordered priority list (per in-port), where failed links are simply skipped. Intriguingly, our result also implies that in the context of perfect resilience, skipping rerouting rules are as powerful as more general rerouting rules. This partially answers a long-standing open question by Chiesa, Nikolaevskiy, Mitrovic, Gurtov, Madry, Schapira, and Shenker [IEEE/ACM Transactions on Networking, 2017] in the affirmative."}
{"id": "2602.03158", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03158", "abs": "https://arxiv.org/abs/2602.03158", "authors": ["Zongwei Wang", "Min Gao", "Junliang Yu", "Tong Chen", "Chenghua Lin"], "title": "PAMAS: Self-Adaptive Multi-Agent System with Perspective Aggregation for Misinformation Detection", "comment": "12 pages", "summary": "Misinformation on social media poses a critical threat to information credibility, as its diverse and context-dependent nature complicates detection. Large language model-empowered multi-agent systems (MAS) present a promising paradigm that enables cooperative reasoning and collective intelligence to combat this threat. However, conventional MAS suffer from an information-drowning problem, where abundant truthful content overwhelms sparse and weak deceptive cues. With full input access, agents tend to focus on dominant patterns, and inter-agent communication further amplifies this bias. To tackle this issue, we propose PAMAS, a multi-agent framework with perspective aggregation, which employs hierarchical, perspective-aware aggregation to highlight anomaly cues and alleviate information drowning. PAMAS organizes agents into three roles: Auditors, Coordinators, and a Decision-Maker. Auditors capture anomaly cues from specialized feature subsets; Coordinators aggregate their perspectives to enhance coverage while maintaining diversity; and the Decision-Maker, equipped with evolving memory and full contextual access, synthesizes all subordinate insights to produce the final judgment. Furthermore, to improve efficiency in multi-agent collaboration, PAMAS incorporates self-adaptive mechanisms for dynamic topology optimization and routing-based inference, enhancing both efficiency and scalability. Extensive experiments on multiple benchmark datasets demonstrate that PAMAS achieves superior accuracy and efficiency, offering a scalable and trustworthy way for misinformation detection."}
{"id": "2602.03543", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.03543", "abs": "https://arxiv.org/abs/2602.03543", "authors": ["Kanstantsin Pashkovich", "Jacob Skitsko", "Yun Xing"], "title": "Sequential Linear Contracts on Matroids", "comment": null, "summary": "In this work, we study sequential contracts under matroid constraints. In the sequential setting, an agent can take actions one by one. After each action, the agent observes the stochastic value of the action and then decides which action to take next, if any. At the end, the agent decides what subset of taken actions to use for the principal's reward; and the principal receives the total value of this subset as a reward. Taking each action induces a certain cost for the agent. Thus, to motivate the agent to take actions the principal is expected to offer an appropriate contract. A contract describes the payment from the principal to the agent as a function of the principal's reward obtained through the agent's actions. In this work, we concentrate on studying linear contracts, i.e.\\ the contracts where the principal transfers a fraction of their total reward to the agent. We assume that the total principal's reward is calculated based on a subset of actions that forms an independent set in a given matroid. We establish a relationship between the problem of finding an optimal linear contract (or computing the corresponding principal's utility) and the so called matroid (un)reliability problem. Generally, the above problems turn out to be equivalent subject to adding parallel copies of elements to the given matroid."}
{"id": "2602.03407", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.03407", "abs": "https://arxiv.org/abs/2602.03407", "authors": ["Fatih Gulec", "Vahid Abolghasemi"], "title": "Universal Costas Matrices: Towards a General Framework for Costas Array Construction", "comment": "Accepted for IEEE International Conference on Communications (ICC) 2026", "summary": "Costas arrays are a special type of permutation matrices with ideal autocorrelation and low cross-correlation properties, making them valuable for radar, wireless communication, and integrated sensing and communication applications. This paper presents a novel unified framework for analyzing and discovering new Costas arrays. We introduce Universal Costas Matrices (UCMs) and Universal Costas Frequency Matrices (UCFMs) and investigate their structural characteristics. A framework integrating UCMs and UCFMs is proposed to pave the way for future artificial intelligence-assisted Costas array discovery. Leveraging the structural properties of UCMs and UCFMs, a reconstruction-based search method is developed to generate UCMs from UCFMs. Numerical results demonstrate that the proposed approach significantly accelerates the search process and enhances structural insight into Costas array generation."}
{"id": "2602.03223", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03223", "abs": "https://arxiv.org/abs/2602.03223", "authors": ["Jiahao Liu", "Hongji Ruan", "Weimin Zhang", "Ziye Tong", "Derick Tang", "Zhanpeng Zeng", "Qinsong Zeng", "Peng Zhang", "Tun Lu", "Ning Gu"], "title": "Distribution-Aware End-to-End Embedding for Streaming Numerical Features in Click-Through Rate Prediction", "comment": "Under review", "summary": "This paper explores effective numerical feature embedding for Click-Through Rate prediction in streaming environments. Conventional static binning methods rely on offline statistics of numerical distributions; however, this inherently two-stage process often triggers semantic drift during bin boundary updates. While neural embedding methods enable end-to-end learning, they often discard explicit distributional information. Integrating such information end-to-end is challenging because streaming features often violate the i.i.d. assumption, precluding unbiased estimation of the population distribution via the expectation of order statistics. Furthermore, the critical context dependency of numerical distributions is often neglected. To this end, we propose DAES, an end-to-end framework designed to tackle numerical feature embedding in streaming training scenarios by integrating distributional information with an adaptive modulation mechanism. Specifically, we introduce an efficient reservoir-sampling-based distribution estimation method and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics. DAES significantly outperforms existing approaches as demonstrated by extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users."}
{"id": "2602.03687", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03687", "abs": "https://arxiv.org/abs/2602.03687", "authors": ["Martin Bullinger", "Edith Elkind", "Kassian Köck"], "title": "Efficient Investment in Multi-Agent Models of Public Transportation", "comment": null, "summary": "We study two stylized, multi-agent models aimed at investing a limited, indivisible resource in public transportation. In the first model, we face the decision of which potential stops to open along a (e.g., bus) path, given agents' travel demands. While it is known that utilitarian optimal solutions can be identified in polynomial time, we find that computing approximately optimal solutions with respect to egalitarian welfare is NP-complete. This is surprising as we operate on the simple topology of a line graph.\n  In the second model, agents navigate a more complex network modeled by a weighted graph where edge weights represent distances. We face the decision of improving travel time along a fixed number of edges. We provide a polynomial-time algorithm that combines Dijkstra's algorithm with a dynamical program to find the optimal decision for one or two agents. By contrast, if the number of agents is variable, we find \\np-completeness and inapproximability results for utilitarian and egalitarian welfare. Moreover, we demonstrate implications of our results for a related model of railway network design."}
{"id": "2602.03421", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.03421", "abs": "https://arxiv.org/abs/2602.03421", "authors": ["Hadi Aghaee", "Christian Deppe", "Holger Boche"], "title": "On (Im)possibility of Network Oblivious Transfer via Noisy Channels and Non-Signaling Correlations", "comment": null, "summary": "This work investigates the fundamental limits of implementing network oblivious transfer via noisy multiple access channels and broadcast channels between honest-but-curious parties when the parties have access to general tripartite non-signaling correlations. By modeling the shared resource as an arbitrary tripartite non-signaling box, we obtain a unified perspective on both the channel behavior and the resulting correlations. Our main result demonstrates that perfect oblivious transfer is impossible. In the asymptotic regime, we further show that even negligible leakage cannot be achieved, as repeated use of the resource amplifies the receiver(s)'s ability to distinguish messages that were not intended for him/them. In contrast, the receiver(s)'s own privacy is not subject to a universal impossibility limitation."}
{"id": "2602.03304", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03304", "abs": "https://arxiv.org/abs/2602.03304", "authors": ["Wenlin Zhang", "Kuicai Dong", "Junyi Li", "Yingyi Zhang", "Xiaopeng Li", "Pengyue Jia", "Yi Wen", "Derong Xu", "Maolin Wang", "Yichao Wang", "Yong Liu", "Xiangyu Zhao"], "title": "To Search or Not to Search: Aligning the Decision Boundary of Deep Search Agents via Causal Intervention", "comment": null, "summary": "Deep search agents, which autonomously iterate through multi-turn web-based reasoning, represent a promising paradigm for complex information-seeking tasks. However, current agents suffer from critical inefficiency: they conduct excessive searches as they cannot accurately judge when to stop searching and start answering. This stems from outcome-centric training that prioritize final results over the search process itself. We identify the root cause as misaligned decision boundaries, the threshold determining when accumulated information suffices to answer. This causes over-search (redundant searching despite sufficient knowledge) and under-search (premature termination yielding incorrect answers). To address these errors, we propose a comprehensive framework comprising two key components. First, we introduce causal intervention-based diagnosis that identifies boundary errors by comparing factual and counterfactual trajectories at each decision point. Second, we develop Decision Boundary Alignment for Deep Search agents (DAS), which constructs preference datasets from causal feedback and aligns policies via preference optimization. Experiments on public datasets demonstrate that decision boundary errors are pervasive across state-of-the-art agents. Our DAS method effectively calibrates these boundaries, mitigating both over-search and under-search to achieve substantial gains in accuracy and efficiency. Our code and data are publicly available at: https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS."}
{"id": "2602.03505", "categories": ["cs.IT", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03505", "abs": "https://arxiv.org/abs/2602.03505", "authors": ["Saeed R. Khosravirad", "Ahmed Alkhateeb", "Ingrid van de Voorde"], "title": "Generative Decompression: Optimal Lossy Decoding Against Distribution Mismatch", "comment": null, "summary": "This paper addresses optimal decoding strategies in lossy compression where the assumed distribution for compressor design mismatches the actual (true) distribution of the source. This problem has immediate relevance in standardized communication systems where the decoder acquires side information or priors about the true distribution that are unavailable to the fixed encoder. We formally define the mismatched quantization problem, demonstrating that the optimal reconstruction rule, termed generative decompression, aligns with classical Bayesian estimation by taking the conditional expectation under the true distribution given the quantization indices and adapting it to fixed-encoder constraints. This strategy effectively performs a generative Bayesian correction on the decoder side, strictly outperforming the conventional centroid rule. We extend this framework to transmission over noisy channels, deriving a robust soft-decoding rule that quantifies the inefficiency of standard modular source--channel separation architectures under mismatch. Furthermore, we generalize the approach to task-oriented decoding, showing that the optimal strategy shifts from conditional mean estimation to maximum a posteriori (MAP) detection. Experimental results on Gaussian sources and deep-learning-based semantic classification demonstrate that generative decompression closes a vast majority of the performance gap to the ideal joint-optimization benchmark, enabling adaptive, high-fidelity reconstruction without modifying the encoder."}
{"id": "2602.03306", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03306", "abs": "https://arxiv.org/abs/2602.03306", "authors": ["Zhanyu Wu", "Richong Zhang", "Zhijie Nie"], "title": "Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval", "comment": null, "summary": "Dense retrieval represents queries and docu-002 ments as high-dimensional embeddings, but003 these representations can be redundant at the004 query level: for a given information need, only005 a subset of dimensions is consistently help-006 ful for ranking. Prior work addresses this via007 pseudo-relevance feedback (PRF) based dimen-008 sion importance estimation, which can produce009 query-aware masks without labeled data but010 often relies on noisy pseudo signals and heuris-011 tic test-time procedures. In contrast, super-012 vised adapter methods leverage relevance labels013 to improve embedding quality, yet they learn014 global transformations shared across queries015 and do not explicitly model query-aware di-016 mension importance. We propose a Query-017 Aware Adaptive Dimension Selection frame-018 work that learns to predict per-dimension im-019 portance directly from query embedding. We020 first construct oracle dimension importance dis-021 tributions over embedding dimensions using022 supervised relevance labels, and then train a023 predictor to map a query embedding to these024 label-distilled importance scores. At inference,025 the predictor selects a query-aware subset of026 dimensions for similarity computation based027 solely on the query embedding, without pseudo-028 relevance feedback. Experiments across multi-029 ple dense retrievers and benchmarks show that030 our learned dimension selector improves re-031 trieval effectiveness over the full-dimensional032 baseline as well as PRF-based masking and033 supervised adapter baselines."}
{"id": "2602.03579", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.03579", "abs": "https://arxiv.org/abs/2602.03579", "authors": ["Anjali Padmanabhan", "Danya Arun Bindhu", "Nujoom Sageer Karat", "Shanuja Sasi"], "title": "Secure Decentralized Pliable Index Coding for Target Data Size", "comment": "12 pages", "summary": "Decentralized Pliable Index Coding (DPIC) problem addresses efficient information exchange in distributed systems where clients communicate among themselves without a central server. An important consideration in DPIC is the heterogeneity of side-information and demand sizes. Although many prior works assume homogeneous settings with identical side-information cardinality and single message demands, these assumptions limit real-world applicability where clients typically possess unequal amounts of prior information. In this paper, we study DPIC problem under heterogeneous side-information cardinalities. We propose a transmission scheme that coordinates client broadcasts to maximize coding efficiency while ensuring that each client achieves a common target level $T$. In addition, we impose a strict security constraint that no client acquires more than the target $T$ number of messages, guaranteeing that each client ends up with exactly $T$ messages. We analyze the communication cost incurred by the proposed scheme under this security constraint."}
{"id": "2602.03324", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03324", "abs": "https://arxiv.org/abs/2602.03324", "authors": ["Chao Chen", "Longfei Xu", "Daohan Su", "Tengfei Liu", "Hanyu Guo", "Yihai Duan", "Kaikui Liu", "Xiangxiang Chu"], "title": "SCASRec: A Self-Correcting and Auto-Stopping Model for Generative Route List Recommendation", "comment": null, "summary": "Route recommendation systems commonly adopt a multi-stage pipeline involving fine-ranking and re-ranking to produce high-quality ordered recommendations. However, this paradigm faces three critical limitations. First, there is a misalignment between offline training objectives and online metrics. Offline gains do not necessarily translate to online improvements. Actual performance must be validated through A/B testing, which may potentially compromise the user experience. Second, redundancy elimination relies on rigid, handcrafted rules that lack adaptability to the high variance in user intent and the unstructured complexity of real-world scenarios. Third, the strict separation between fine-ranking and re-ranking stages leads to sub-optimal performance. Since each module is optimized in isolation, the fine-ranking stage remains oblivious to the list-level objectives (e.g., diversity) targeted by the re-ranker, thereby preventing the system from achieving a jointly optimized global optimum. To overcome these intertwined challenges, we propose \\textbf{SCASRec} (\\textbf{S}elf-\\textbf{C}orrecting and \\textbf{A}uto-\\textbf{S}topping \\textbf{Rec}ommendation), a unified generative framework that integrates ranking and redundancy elimination into a single end-to-end process. SCASRec introduces a stepwise corrective reward (SCR) to guide list-wise refinement by focusing on hard samples, and employs a learnable End-of-Recommendation (EOR) token to terminate generation adaptively when no further improvement is expected. Experiments on two large-scale, open-sourced route recommendation datasets demonstrate that SCASRec establishes an SOTA in offline and online settings. SCASRec has been fully deployed in a real-world navigation app, demonstrating its effectiveness."}
{"id": "2602.03607", "categories": ["cs.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.03607", "abs": "https://arxiv.org/abs/2602.03607", "authors": ["Hajar El Hassani", "Mikael Gidlund"], "title": "Sleep or Transmit: Dual-Mode Energy-Efficient Design for NOMA-Enabled Backscatter Networks", "comment": null, "summary": "The rapid growth of Internet-of-Things (IoT) devices demands communication systems that are both spectrally efficient and energy frugal. Backscatter communication (BackCom) is an attractive low-power paradigm, but its spectral efficiency declines in dense deployments. This paper presents an uplink BackCom design that integrates non-orthogonal multiple access (NOMA) and maximizes system energy efficiency (EE). In a bistatic network where multiple backscatter nodes (BNs) harvest RF energy and alternate between sleep and active modes, we formulate a fractional program with coupled time, power, and reflection variables and develop a Dinkelbach-based alternating optimization (AO) algorithm with closed-form updates. Analysis reveals two operating modes depending on power availability, circuit demands and propagation conditions. Simulations show the proposed design adapts the time allocation, achieving up to 8% higher EE than fixed-power and 68% than no-sleep baselines, and delivering up to 127% EE gains over orthogonal multiple access (OMA). These results establish NOMA-enabled BackCom as a scalable, energy efficient solution for large-scale IoT deployments."}
{"id": "2602.03345", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03345", "abs": "https://arxiv.org/abs/2602.03345", "authors": ["Xuancheng Li", "Tao Yang", "Yujia Zhou", "Qingyao Ai", "Yiqun Liu"], "title": "Beyond Exposure: Optimizing Ranking Fairness with Non-linear Time-Income Functions", "comment": null, "summary": "Ranking is central to information distribution in web search and recommendation. Nowadays, in ranking optimization, the fairness to item providers is viewed as a crucial factor alongside ranking relevance for users. There are currently numerous concepts of fairness and one widely recognized fairness concept is Exposure Fairness. However, it relies primarily on exposure determined solely by position, overlooking other factors that significantly influence income, such as time. To address this limitation, we propose to study ranking fairness when the provider utility is influenced by other contextual factors and is neither equal to nor proportional to item exposure. We give a formal definition of Income Fairness and develop a corresponding measurement metric. Simulated experiments show that existing-exposure-fairness-based ranking algorithms fail to optimize the proposed income fairness. Therefore, we propose the Dynamic-Income-Derivative-aware Ranking Fairness algorithm, which, based on the marginal income gain at the present timestep, uses Taylor-expansion-based gradients to simultaneously optimize effectiveness and income fairness. In both offline and online settings with diverse time-income functions, DIDRF consistently outperforms state-of-the-art methods."}
{"id": "2602.03416", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03416", "abs": "https://arxiv.org/abs/2602.03416", "authors": ["Wenxin Ye", "Lin Li", "Ming Li", "Yang Shen", "Kanghong Wang", "Jimmy Xiangji Huang"], "title": "AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation", "comment": null, "summary": "Clothing recommendation extends beyond merely generating personalized outfits; it serves as a crucial medium for aesthetic guidance. However, existing methods predominantly rely on user-item-outfit interaction behaviors while overlooking explicit representations of clothing aesthetics. To bridge this gap, we present the AesRec benchmark dataset featuring systematic quantitative aesthetic annotations, thereby enabling the development of aesthetics-aligned recommendation systems. Grounded in professional apparel quality standards and fashion aesthetic principles, we define a multidimensional set of indicators. At the item level, six dimensions are independently assessed: silhouette, chromaticity, materiality, craftsmanship, wearability, and item-level impression. Transitioning to the outfit level, the evaluation retains the first five core attributes while introducing stylistic synergy, visual harmony, and outfit-level impression as distinct metrics to capture the collective aesthetic impact. Given the increasing human-like proficiency of Vision-Language Models in multimodal understanding and interaction, we leverage them for large-scale aesthetic scoring. We conduct rigorous human-machine consistency validation on a fashion dataset, confirming the reliability of the generated ratings. Experimental results based on AesRec further demonstrate that integrating quantified aesthetic information into clothing recommendation models can provide aesthetic guidance for users while fulfilling their personalized requirements."}
{"id": "2602.03422", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03422", "abs": "https://arxiv.org/abs/2602.03422", "authors": ["Yumeng Wang", "Catherine Chen", "Suzan Verberne"], "title": "RankSteer: Activation Steering for Pointwise LLM Ranking", "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance as zero-shot rankers, yet their effectiveness is highly sensitive to prompt formulation, particularly role-play instructions. Prior analyses suggest that role-related signals are encoded along activation channels that are largely separate from query-document representations, raising the possibility of steering ranking behavior directly at the activation level rather than through brittle prompt engineering. In this work, we propose RankSteer, a post-hoc activation steering framework for zero-shot pointwise LLM ranking. We characterize ranking behavior through three disentangled and steerable directions in representation space: a \\textbf{decision direction} that maps hidden states to relevance scores, an \\textbf{evidence direction} that captures relevance signals not directly exploited by the decision head, and a \\textbf{role direction} that modulates model behavior without injecting relevance information. Using projection-based interventions at inference time, RankSteer jointly controls these directions to calibrate ranking behavior without modifying model weights or introducing explicit cross-document comparisons. Experiments on TREC DL 20 and multiple BEIR benchmarks show that RankSteer consistently improves ranking quality using only a small number of anchor queries, demonstrating that substantial ranking capacity remains under-utilized in pointwise LLM rankers. We further provide a geometric analysis revealing that steering improves ranking by stabilizing ranking geometry and reducing dispersion, offering new insight into how LLMs internally represent and calibrate relevance judgments."}
{"id": "2602.03432", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03432", "abs": "https://arxiv.org/abs/2602.03432", "authors": ["Joohyung Yun", "Doyup Lee", "Wook-Shin Han"], "title": "Failure is Feedback: History-Aware Backtracking for Agentic Traversal in Multimodal Graphs", "comment": "Project page: https://failureisfeedback.github.io/", "summary": "Open-domain multimodal document retrieval aims to retrieve specific components (paragraphs, tables, or images) from large and interconnected document corpora. Existing graph-based retrieval approaches typically rely on a uniform similarity metric that overlooks hop-specific semantics, and their rigid pre-defined plans hinder dynamic error correction. These limitations suggest that a retriever should adapt its reasoning to the evolving context and recover intelligently from dead ends. To address these needs, we propose Failure is Feedback (FiF), which casts subgraph retrieval as a sequential decision process and introduces two key innovations. (i) We introduce a history-aware backtracking mechanism; unlike standard backtracking that simply reverts the state, our approach piggybacks on the context of failed traversals, leveraging insights from previous failures. (ii) We implement an economically-rational agentic workflow. Unlike conventional agents with static strategies, our orchestrator employs a cost-aware traversal method to dynamically manage the trade-off between retrieval accuracy and inference costs, escalating to intensive LLM-based reasoning only when the prior failure justifies the additional computational investment. Extensive experiments show that FiF achieves state-of-the-art retrieval on the benchmarks of MultimodalQA, MMCoQA and WebQA."}
{"id": "2602.03640", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03640", "abs": "https://arxiv.org/abs/2602.03640", "authors": ["Mohanna Hoveyda", "Panagiotis Efstratiadis", "Arjen de Vries", "Maarten de Rijke"], "title": "Tutorial on Reasoning for IR & IR for Reasoning", "comment": "Accepted to ECIR 2026", "summary": "Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies."}
{"id": "2602.03692", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03692", "abs": "https://arxiv.org/abs/2602.03692", "authors": ["Xinyu Lin", "Pengyuan Liu", "Wenjie Wang", "Yicheng Hu", "Chen Xu", "Fuli Feng", "Qifan Wang", "Tat-Seng Chua"], "title": "Bringing Reasoning to Generative Recommendation Through the Lens of Cascaded Ranking", "comment": "Accepted by WWW2026", "summary": "Generative Recommendation (GR) has become a promising end-to-end approach with high FLOPS utilization for resource-efficient recommendation. Despite the effectiveness, we show that current GR models suffer from a critical \\textbf{bias amplification} issue, where token-level bias escalates as token generation progresses, ultimately limiting the recommendation diversity and hurting the user experience. By comparing against the key factor behind the success of traditional multi-stage pipelines, we reveal two limitations in GR that can amplify the bias: homogeneous reliance on the encoded history, and fixed computational budgets that prevent deeper user preference understanding.\n  To combat the bias amplification issue, it is crucial for GR to 1) incorporate more heterogeneous information, and 2) allocate greater computational resources at each token generation step. To this end, we propose CARE, a simple yet effective cascaded reasoning framework for debiased GR. To incorporate heterogeneous information, we introduce a progressive history encoding mechanism, which progressively incorporates increasingly fine-grained history information as the generation process advances. To allocate more computations, we propose a query-anchored reasoning mechanism, which seeks to perform a deeper understanding of historical information through parallel reasoning steps. We instantiate CARE on three GR backbones. Empirical results on four datasets show the superiority of CARE in recommendation accuracy, diversity, efficiency, and promising scalability. The codes and datasets are available at https://github.com/Linxyhaha/CARE."}
{"id": "2602.03713", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03713", "abs": "https://arxiv.org/abs/2602.03713", "authors": ["Moritz Vandenhirtz", "Kaveh Hassani", "Shervin Ghasemlou", "Shuai Shao", "Hamid Eghbalzadeh", "Fuchun Peng", "Jun Liu", "Michael Louis Iuzzolino"], "title": "Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals", "comment": null, "summary": "Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. To resolve this, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. Additionally, MSCGRec fuses collaborative and semantic signals by extracting collaborative features from sequential recommenders and treating them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines and provide an extensive ablation study to validate the impact of each component."}
