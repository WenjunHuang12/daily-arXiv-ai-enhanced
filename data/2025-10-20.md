<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 8]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Combinatorial Contract Design: Recent Progress and Emerging Frontiers](https://arxiv.org/abs/2510.15065)
*Michal Feldman*

Main category: cs.GT

TL;DR: 本文综述了算法合同设计中的组合合同领域，重点关注三种组合设置：单代理多行动、多代理二元行动、多代理多行动，分析了结构洞察、算法技术和复杂性障碍。


<details>
  <summary>Details</summary>
Motivation: 传统经济模型提供了最优解的优雅特征，但现代应用（如在线劳动力市场、医疗保健、AI委托和区块链协议）需要算法视角，关注合同能否高效计算。

Method: 回顾三种组合合同设置：单代理选择多行动、多代理二元行动、多代理各选多行动，分析每种情况下的结构特性、算法技术和计算复杂性。

Result: 发现了可处理情况（如总替代奖励函数）、硬度结果以及在价值和需求预言机访问下的近似保证。

Conclusion: 通过梳理这些进展，文章描绘了组合合同设计的新兴格局，并突出了基本开放问题和未来工作的有前景方向。

Abstract: Contract theory studies how a principal can incentivize agents to exert
costly, unobservable effort through performance-based payments. While classical
economic models provide elegant characterizations of optimal solutions, modern
applications, ranging from online labor markets and healthcare to AI delegation
and blockchain protocols, call for an algorithmic perspective. The challenge is
no longer only which contracts induce desired behavior, but whether such
contracts can be computed efficiently. This viewpoint has given rise to
\emph{algorithmic contract design}, paralleling the rise of algorithmic
mechanism design two decades ago.
  This article focuses on \emph{combinatorial contracts}, an emerging frontier
within algorithmic contract design, where agents may choose among exponentially
many combinations of actions, or where multiple agents must work together as a
team, and the challenge lies in selecting the right composition. These models
capture a wide variety of real-world contracting environments, from hospitals
coordinating physicians across treatment protocols to firms hiring teams of
engineers for interdependent tasks. We review three combinatorial settings: (i)
a single agent choosing multiple actions, (ii) multiple agents with binary
actions, and (iii) multiple agents each selecting multiple actions. For each,
we highlight structural insights, algorithmic techniques, and complexity
barriers. Results include tractable cases such as gross substitutes reward
functions, hardness results, and approximation guarantees under value- and
demand-oracle access. By charting these advances, the article maps the emerging
landscape of combinatorial contract design, and highlights fundamental open
questions and promising directions for future work.

</details>


### [2] [Beyond Outcome-Based Imperfect-Recall: Higher-Resolution Abstractions for Imperfect-Information Games](https://arxiv.org/abs/2510.15094)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.GT

TL;DR: 提出了信号观测有序博弈(SOOGs)作为德州扑克类游戏的形式化框架，定义了分辨率界限来衡量信号抽象的性能上限，并提出了全记忆结果同构(FROI)方法来改进传统结果基不完全记忆算法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决不完全信息博弈中手牌抽象缺乏形式化任务模型和评估需要大量策略求解资源的问题，为德州扑克类游戏提供精确的数学基础。

Method: 引入信号观测有序博弈(SOOGs)框架，定义分辨率界限，提出全记忆结果同构(FROI)来整合历史信息，提高抽象质量。

Result: 实验证明FROI在德州扑克类基准测试中持续优于传统结果基不完全记忆基线方法。

Conclusion: 为手牌抽象提供了统一的形式化处理，并为设计更高分辨率的不完全信息博弈抽象提供了实用指导。

Abstract: Hand abstraction is crucial for scaling imperfect-information games (IIGs)
such as Texas Hold'em, yet progress is limited by the lack of a formal task
model and by evaluations that require resource-intensive strategy solving. We
introduce signal observation ordered games (SOOGs), a subclass of IIGs tailored
to hold'em-style games that cleanly separates signal from player action
sequences, providing a precise mathematical foundation for hand abstraction.
Within this framework, we define a resolution bound-an information-theoretic
upper bound on achievable performance under a given signal abstraction. Using
the bound, we show that mainstream outcome-based imperfect-recall algorithms
suffer substantial losses by arbitrarily discarding historical information; we
formalize this behavior via potential-aware outcome Isomorphism (PAOI) and
prove that PAOI characterizes their resolution bound. To overcome this
limitation, we propose full-recall outcome isomorphism (FROI), which integrates
historical information to raise the bound and improve policy quality.
Experiments on hold'em-style benchmarks confirm that FROI consistently
outperforms outcome-based imperfect-recall baselines. Our results provide a
unified formal treatment of hand abstraction and practical guidance for
designing higher-resolution abstractions in IIGs.

</details>


### [3] [How to Sell High-Dimensional Data Optimally](https://arxiv.org/abs/2510.15214)
*Andrew Li,R. Ravi,Karan Singh,Zihong Yi,Weizhong Zhang*

Main category: cs.GT

TL;DR: 提出了一种高效的数据产品定价算法，通过采样方法在高维状态下生成近似最优的菜单，解决了传统方法在指数级状态空间中的计算难题。


<details>
  <summary>Details</summary>
Motivation: 解决大型专有数据销售中的信息定价问题，特别是当卖家不完全了解买家私人偏好时，如何设计收益最大化的统计实验菜单。

Method: 使用采样方法独立于状态空间生成近似最优菜单；针对高维高斯数据，证明只需考虑标量高斯实验，并通过半定规划高效求解最优菜单。

Result: 算法在采样数量独立于状态空间的情况下生成近似最优菜单；在高斯数据情况下，实现了高效的最优菜单设计，并确定了完全剩余提取的条件。

Conclusion: 提出的采样方法有效解决了高维数据定价的计算挑战，为大数据环境下的信息销售提供了实用的解决方案。

Abstract: Motivated by the problem of selling large, proprietary data, we consider an
information pricing problem proposed by Bergemann et al. that involves a
decision-making buyer and a monopolistic seller. The seller has access to the
underlying state of the world that determines the utility of the various
actions the buyer may take. Since the buyer gains greater utility through
better decisions resulting from more accurate assessments of the state, the
seller can therefore promise the buyer supplemental information at a price. To
contend with the fact that the seller may not be perfectly informed about the
buyer's private preferences (or utility), we frame the problem of designing a
data product as one where the seller designs a revenue-maximizing menu of
statistical experiments.
  Prior work by Cai et al. showed that an optimal menu can be found in time
polynomial in the state space, whereas we observe that the state space is
naturally exponential in the dimension of the data. We propose an algorithm
which, given only sampling access to the state space, provably generates a
near-optimal menu with a number of samples independent of the state space. We
then analyze a special case of high-dimensional Gaussian data, showing that (a)
it suffices to consider scalar Gaussian experiments, (b) the optimal menu of
such experiments can be found efficiently via a semidefinite program, and (c)
full surplus extraction occurs if and only if a natural separation condition
holds on the set of potential preferences of the buyer.

</details>


### [4] [HOB: A Holistically Optimized Bidding Strategy under Heterogeneous Auction Mechanisms with Organic Traffic](https://arxiv.org/abs/2510.15238)
*Qi Li,Wendong Huang,Qichen Ye,Wutong Xu,Cheems Wang,Rongquan Bai,Wei Yuan,Guan Wang,Chuan Yu,Jian Xu*

Main category: cs.GT

TL;DR: 该论文提出了在电子商务广告平台中优化跨异构拍卖渠道（一价和二价拍卖）的竞价策略，特别是针对平台级营销解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着广告主从仅购买商业流量转向竞标全部流量以最大化整体销售额，自动化竞价系统面临在异构拍卖渠道中确定最优策略的挑战。

Method: 1. 推导一价拍卖渠道下的最优竞价解决方案，考虑有机流量的存在；2. 提出边际成本对齐策略，确保跨异构拍卖机制的竞价效率。

Result: 在公共数据集和大规模在线A/B测试中进行的综合实验表明，所开发框架相比现有方法具有一致的改进。

Conclusion: 该工作为电子商务广告平台中的跨渠道竞价优化提供了有效的解决方案，能够满足广告主的不同目标，如最大化回报或达到目标广告支出回报率。

Abstract: The E-commerce advertising platforms typically sell commercial traffic
through either second-price auction (SPA) or first-price auction (FPA). SPA was
historically prevalent due to its dominant strategy incentive-compatible (DSIC)
for bidders with quasi-linear utilities, especially when budgets are not a
binding constraint, while FPA has gained more prominence for offering higher
revenue potential to publishers and avoiding the possibility for discriminatory
treatment in personalized reserve prices. Meanwhile, on the demand side,
advertisers are increasingly adopting platform-wide marketing solutions akin to
QuanZhanTui, shifting from spending budgets solely on commercial traffic to
bidding on the entire traffic for the purpose of maximizing overall sales. For
automated bidding systems, such a trend poses a critical challenge: determining
optimal strategies across heterogeneous auction channels to fulfill diverse
advertiser objectives, such as maximizing return (MaxReturn) or meeting target
return on ad spend (TargetROAS). To overcome this challenge, this work makes
two key contributions. First, we derive an efficient solution for optimal
bidding under FPA channels, which takes into account the presence of organic
traffic - traffic can be won for free. Second, we introduce a marginal cost
alignment (MCA) strategy that provably secures bidding efficiency across
heterogeneous auction mechanisms. To validate performance of our developed
framework, we conduct comprehensive offline experiments on public datasets and
large-scale online A/B testing, which demonstrate consistent improvements over
existing methods.

</details>


### [5] [A Renegotiable contract-theoretic incentive mechanism for Federated learning](https://arxiv.org/abs/2510.15344)
*Xavier Tan,Xiaoli Tang,Han Yu*

Main category: cs.GT

TL;DR: 提出了可重协商的合同理论激励机制RC-TIM，用于联邦学习中的动态激励管理，相比静态合同方法能更好地适应数据所有者的行为变化和预算约束。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习激励机制假设合同一旦签订就保持不变，但实际情况中数据所有者可能无法履行合同，导致数据消费者的预算使用效率低下。

Method: 设计支持合同重协商的激励机制RC-TIM，能够根据数据所有者行为变化和预算约束动态调整激励策略。

Result: 在三个基准数据集上的实验表明，RC-TIM显著优于四种最先进方法，平均效用提升达45.76%。

Conclusion: RC-TIM通过支持合同重协商，使联邦学习系统对运行环境中的不可预测变化更具适应性，提高了激励机制的灵活性和效率。

Abstract: Federated learning (FL) has gained prominence due to heightened concerns over
data privacy. Privacy restrictions limit the visibility for data consumers
(DCs) to accurately assess the capabilities and efforts of data owners (DOs).
Thus, for open collaborative FL markets to thrive, effective incentive
mechanisms are key as they can motivate data owners (DOs) to contribute to FL
tasks. Contract theory is a useful technique for developing FL incentive
mechanisms. Existing approaches generally assume that once the contract between
a DC and a DO is signed, it remains unchanged until the FL task is finished.
However, unforeseen circumstances might force a DO to be unable to fulfill the
current contract, resulting in inefficient utilization of DCs' budgets. To
address this limitation, we propose the Renegotiable Contract-Theoretic
Incentive Mechanism (RC-TIM) for FL. Unlike previous approaches, it adapts to
changes in DOs' behavior and budget constraints by supporting the renegotiation
of contracts, providing flexible and dynamic incentives. Under RC-TIM, an FL
system is more adaptive to unpredictable changes in the operating environment
that can affect the quality of the service provided by DOs. Extensive
experiments on three benchmark datasets demonstrate that RC-TIM significantly
outperforms four state-of-the-art related methods, delivering up to a 45.76%
increase in utility on average.

</details>


### [6] [Co-Investment with Dynamic Participation under Unforeseeable Opportunity Costs: A Coalitional Game Approach](https://arxiv.org/abs/2510.15384)
*Amal Sakr,Andrea Araldo,Tijani Chahed,Daniel Kofman*

Main category: cs.GT

TL;DR: 提出了一种基于联盟博弈理论的动态共投资方案，允许基础设施提供商和服务提供商在移动边缘计算环境中灵活加入、停留或退出共投资，并通过计算入场费和退出惩罚来补偿留在共投资中的参与者。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算等技术的部署需要大量基础设施投资，基础设施提供商可能不愿独自承担。同时，长期共投资承诺可能过于约束，参与者可能发现未来有更好的盈利机会（机会成本）。

Method: 基于联盟博弈理论，设计动态共投资方案，允许参与者灵活调整参与状态，动态调整基础设施容量和资源共享，并计算入场费和退出惩罚来补偿留在共投资中的参与者。

Result: 数值模拟显示，该动态方案能够鼓励参与者参与，并在高机会成本情况下增加利润。

Conclusion: 提出的动态共投资方案能够有效解决基础设施投资障碍，为参与者提供灵活性，同时通过适当的补偿机制确保共投资的稳定性。

Abstract: Technologies such as Mobile Edge Computing (MEC) depend on the availability
of infrastructure. We define the Infrastructure Provider (InP) as the actor
responsible for deploying and maintaining this infrastructure, while Service
Providers (SPs) operate applications over it to serve end users and earn
revenues. Deploying such infrastructure requires however a significant
investment, and the InP may be reluctant to bear it alone. We propose
co-investment to overcome this barrier, allowing players, the InP and multiple
SPs, to share costs and revenues. However, committing to a co-investment over a
long period may be too constraining for players: in an unforeseeable future,
players may realize that they could make more profit outside the co-investment
(such a profit is called opportunity cost). For this reason, we propose a
scheme, based on coalitional game theory, which is dynamic in terms of
(i)allowing players to join, remain in, or leave the co-investment, (ii)
adjusting the infrastructure capacity and resource sharing over time. We
propose a method to compute entry fees and exit penalties in order to
appropriately compensate players remaining in the co-investment. We numerically
show that our dynamic scheme encourages player participation and increases
profit (in case of high opportunity cost).

</details>


### [7] [Reviving, reproducing, and revisiting Axelrod's second tournament](https://arxiv.org/abs/2510.15438)
*Vincent Knight,Owen Campbell,Marc Harper,T. J. Gaffney,Nikoleta E. Glynatsi*

Main category: cs.GT

TL;DR: 本文重现了Axelrod的第二次囚徒困境迭代锦标赛，验证了TFT策略的成功，并评估了原始策略在更广泛环境下的稳健性。


<details>
  <summary>Details</summary>
Motivation: Axelrod在1980年代组织的两次IPD计算机锦标赛具有重要历史影响，但档案记录不完整，特别是第二次锦标赛缺少最终比赛代码，这引发了结果可重现性的疑问。

Method: 通过恢复幸存的Fortran实现使其能在现代编译器下编译，并构建Python接口调用原始策略函数，使用开源的Axelrod-Python库运行锦标赛。

Result: 成功重现了Axelrod的主要发现：TFT获胜，成功策略倾向于合作、对背叛做出反应并愿意原谅。策略排名基本保持不变。在更广泛的策略环境中，原始锦标赛对TFT特别有利。

Conclusion: 这是对Axelrod第二次锦标赛的首次系统性重现，为未来研究提供了易于使用的策略实现，并重新评估了原始结果在新策略和环境下的表现。

Abstract: Direct reciprocity, typically studied using the Iterated Prisoner's Dilemma
(IPD), is central to understanding how cooperation evolves. In the 1980s,
Robert Axelrod organized two influential IPD computer tournaments, where Tit
for Tat (TFT) emerged as the winner. Yet the archival record is incomplete: for
the first tournament only a report survives, and for the second the submitted
Fortran strategies remain but not the final tournament code. This gap raises
questions about the reproducibility of these historically influential results.
We recreate the second tournament by restoring the surviving Fortran
implementations to compile with modern compilers and by building a Python
interface that calls the original strategy functions without modification.
Using the open-source Axelrod-Python library to run tournaments, we reproduce
Axelrod's main findings: TFT prevails, and successful play tends to be
cooperative, responsive to defection, and willing to forgive. Strategy rankings
remain mostly unchanged. We then assess the robustness of the originally
submitted strategies by incorporating additional strategies, and we run one of
the largest IPD tournaments to date. We find that the original tournament was
especially favorable to TFT and that it is difficult to dethrone TFT when the
original submissions make up the majority of the field. We also observe that
several lesser-known submissions perform strongly in more diverse settings and
under noise. Our contributions are: (i) the first systematic reproduction of
Axelrod's second tournament; (ii) a contemporary reassessment of the original
results in light of new strategies and settings; and (iii) a preserved,
easy-to-use implementation of the second-tournament strategies within
Axelrod-Python to support future research.

</details>


### [8] [Active Inverse Methods in Stackelberg Games with Bounded Rationality](https://arxiv.org/abs/2510.15582)
*Jianguo Chen,Jinlong Lei,Biqiang Mu,Yiguang Hong,Hongsheng Qi*

Main category: cs.GT

TL;DR: 本文提出主动逆博弈理论，让领导者作为学习者在Stackelberg博弈中主动选择行动来更好地理解追随者的成本函数，通过Fisher信息最大化信息增益，并证明了方法的收敛性和渐近性质。


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈理论方法未将学习者视为博弈的主动参与者，这限制了学习过程的效率。本文旨在扩展逆博弈理论，让领导者主动参与博弈以增强对追随者成本函数的学习。

Method: 开发了两种方法：1）利用Fisher信息最大化未知参数信息增益的主动学习方法；2）考虑领导者自身成本的主动逆博弈方法，平衡探索与利用。两种方法都证明了收敛性和渐近性质。

Result: 在二次成本函数情况下通过仿真验证了方法的性质，证明主动逆博弈方法能够通过主动探索更快地达到Stackelberg均衡。

Conclusion: 主动逆博弈理论能够显著提升学习效率，使领导者更快地理解追随者的行为并达到博弈均衡状态。

Abstract: Inverse game theory is utilized to infer the cost functions of all players
based on game outcomes. However, existing inverse game theory methods do not
consider the learner as an active participant in the game, which could
significantly enhance the learning process. In this paper, we extend inverse
game theory to active inverse methods. For Stackelberg games with bounded
rationality, the leader, acting as a learner, actively chooses actions to
better understand the follower's cost functions. First, we develop a method of
active learning by leveraging Fisher information to maximize information gain
about the unknown parameters and prove the consistency and asymptotic
normality. Additionally, when leaders consider its cost, we develop a method of
active inverse game to balance exploration and exploitation, and prove the
consistency and asymptotic Stackelberg equilibrium with quadratic cost
functions. Finally, we verify the properties of these methods through
simulations in the quadratic case and demonstrate that the active inverse game
method can achieve Stackelberg equilibrium more quickly through active
exploration.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [TKHist: Cardinality Estimation for Join Queries via Histograms with Dominant Attribute Correlation Finding](https://arxiv.org/abs/2510.15368)
*Renrui Li,Qingzhi Ma,Jiajie Xu,Lei Zhao,An Liu*

Main category: cs.DB

TL;DR: TKHist是一种新的基数估计方法，通过放松直方图的均匀性假设来改进多表连接查询的基数估计，同时提出主导连接路径相关性发现算法来处理连接键与过滤谓词之间的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的多表连接查询基数估计方法虽然提高了准确性，但带来了更高的空间开销、延迟和复杂性，特别是在与二元连接框架集成时。

Method: TKHist通过捕获分箱非均匀性信息来放松直方图的均匀性假设，并提出主导连接路径相关性发现算法来管理连接键与过滤谓词之间的相关性。

Result: 在流行基准测试上的广泛实验表明，TKHist相比最先进方法将误差方差降低了2-3个数量级，同时保持相当或更低的内存使用。

Conclusion: TKHist通过放松均匀性假设和相关性管理，显著提高了多表连接查询基数估计的准确性，同时保持了较低的资源开销。

Abstract: Cardinality estimation has long been crucial for cost-based database
optimizers in identifying optimal query execution plans, attracting significant
attention over the past decades. While recent advancements have significantly
improved the accuracy of multi-table join query estimations, these methods
introduce challenges such as higher space overhead, increased latency, and
greater complexity, especially when integrated with the binary join framework.
In this paper, we introduce a novel cardinality estimation method named TKHist,
which addresses these challenges by relaxing the uniformity assumption in
histograms. TKHist captures bin-wise non-uniformity information, enabling
accurate cardinality estimation for join queries without filter predicates.
Furthermore, we explore the attribute independent assumption, which can lead to
significant over-estimation rather than under-estimation in multi-table join
queries. To address this issue, we propose the dominating join path correlation
discovery algorithm to highlight and manage correlations between join keys and
filter predicates. Our extensive experiments on popular benchmarks demonstrate
that TKHist reduces error variance by 2-3 orders of magnitude compared to SOTA
methods, while maintaining comparable or lower memory usage.

</details>


### [10] [Optimizing Data Lakes' Queries](https://arxiv.org/abs/2510.15445)
*Gregory,Weintraub*

Main category: cs.DB

TL;DR: 提出了一种基于查询覆盖集的框架来优化云数据湖查询性能，通过识别最小文件子集来减少网络数据传输。


<details>
  <summary>Details</summary>
Motivation: 云数据湖中计算与存储分离架构导致每次查询都需要通过网络传输数据，严重影响查询性能和网络带宽使用。

Method: 建立理论框架，引入"查询覆盖集"概念，识别每个查询需要访问的最小文件子集，仅在该子集上执行查询。

Result: 通过最小化数据传输显著提升了查询性能。

Conclusion: 查询覆盖集方法能有效优化云数据湖架构中的查询效率，减少网络传输开销。

Abstract: Cloud data lakes provide a modern solution for managing large volumes of
data. The fundamental principle behind these systems is the separation of
compute and storage layers. In this architecture, inexpensive cloud storage is
utilized for data storage, while compute engines are employed to perform
analytics on this data in an "on-demand" mode. However, to execute any
calculations on the data, it must be transferred from the storage layer to the
compute layer over the network for each query. This transfer can negatively
impact calculation performance and requires significant network bandwidth. In
this thesis, we examine various strategies to enhance query performance within
a cloud data lake architecture. We begin by formalizing the problem and
proposing a straightforward yet robust theoretical framework that clearly
outlines the associated trade-offs. Central to our framework is the concept of
a "query coverage set," which is defined as the collection of files that need
to be accessed from storage to fulfill a specific query. Our objective is to
identify the minimal coverage set for each query and execute the query
exclusively on this subset of files. This approach enables us to significantly
improve query performance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Revoke vs. Restart in Unweighted Throughput Scheduling](https://arxiv.org/abs/2510.15318)
*Changdao He*

Main category: cs.DS

TL;DR: 本文研究了抢占撤销模型下的单机无权重吞吐量调度问题，证明了任何确定性在线算法都无法达到常数竞争比。


<details>
  <summary>Details</summary>
Motivation: 研究抢占撤销模型（作业被中止后无法重启）与抢占重启模型的差异，探索在这种更严格的约束条件下在线调度算法的性能极限。

Method: 通过构造对抗性实例：从三个作业的实例开始，其中算法最多完成一个作业而最优离线算法完成所有三个作业，然后迭代嵌套这种构造。

Result: 对于任意k≥3，存在实例使得确定性在线算法最多完成一个作业，而最优离线算法至少完成k个作业，竞争比可被强制为1/k，任意接近零。

Conclusion: 抢占撤销模型与抢占重启模型存在根本性差异，在抢占撤销模型下不存在常数竞争比的确定性在线调度算法。

Abstract: We study the unweighted throughput scheduling problem on a single machine in
the preemption-revoke model, where a running job may be aborted at any time,
but all progress is permanently lost and the job cannot be restarted. Each job
$J_i=(r_i,p_i,s_i)$ is defined by a release time $r_i$, a processing time
$p_i$, and a slack $s_i$, and must start no later than $r_i+s_i$ to be
feasible. We prove that no deterministic online algorithm can achieve a
constant competitive ratio. The lower bound is established via an adversarial
construction: starting from a three-job instance where $\textsf{ALG}$ completes
at most one job while $\textsf{OPT}$ completes all three, we iteratively nest
such constructions. By induction, for every $k\ge 3$, there exists an instance
where $\textsf{ALG}$ completes at most one job, while $\textsf{OPT}$ completes
at least $k$ jobs. Thus, the competitive ratio can be forced to $1/k$, and
hence made arbitrarily close to zero. Our result stands in sharp contrast to
the preemption-restart model, where Hoogeveen, Potts, and Woeginger (2000) gave
a deterministic $1/2$-competitive algorithm.

</details>


### [12] [Temporal Graph Reconfiguration for Always-Connected Graphs](https://arxiv.org/abs/2510.15593)
*Paul Sievers,George Skretas,Georg Tennigkeit*

Main category: cs.DS

TL;DR: 本文研究了时态图重构问题，提出了分层连接重构问题，开发了多项式时间算法来生成有效重构序列，并证明了最小化重构序列长度是NP难问题。


<details>
  <summary>Details</summary>
Motivation: 研究时态图重构问题，因为在实践中网络修改需要逐步进行，而网络在重构过程中必须保持某些性质。

Method: 基于桥梁的可达性分区分析，识别可改变和不可改变的边，构建多项式时间算法生成重构序列。

Result: 提出了一个多项式时间算法，能生成长度最多为2M^2的有效重构序列，或判断重构是否可行。

Conclusion: 时态图重构问题在理论上有重要意义，最小化重构序列长度是NP难问题，但存在有效的重构算法。

Abstract: Network redesign problems ask to modify the edges of a given graph to satisfy
some properties. In temporal graphs, where edges are only active at certain
times, we are sometimes only allowed to modify when the edges are going to be
active. In practice, we might not even be able to perform all of the necessary
modifications at once; changes must be applied step-by-step while the network
is still in operation, meaning that the network must continue to satisfy some
properties. To initiate a study in this area, we introduce the temporal graph
reconfiguration problem. As a starting point, we consider the Layered
Connectivity Reconfiguration problem in which every snapshot of the temporal
graph must remain connected throughout the reconfiguration. We provide insights
into how bridges can be reconfigured into non-bridges based on their
reachability partitions, which lets us identify any edge as either changeable
or unchangeable. From this we construct a polynomial-time algorithm that gives
a valid reconfiguration sequence of length at most 2M^2 (where M is the number
of temporal edges), or determines that reconfiguration is not possible. We also
show that minimizing the length of the reconfiguration sequence is NP-hard via
a reduction from vertex cover.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [13] [DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management](https://arxiv.org/abs/2510.15087)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Allen Lin,Lingfeng Shi,Ali Mostafavi,James Caverlee*

Main category: cs.IR

TL;DR: 提出了DMRetriever，这是首个专门为灾害管理设计的密集检索模型系列，通过三阶段训练框架实现SOTA性能，具有极高的参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有通用检索模型无法有效处理灾害管理场景中的多样化搜索意图，导致性能不一致和不可靠，需要专门的灾害管理检索模型。

Method: 采用三阶段训练框架：双向注意力适应、无监督对比预训练和难度感知渐进指令微调，使用高级数据精炼管道生成的高质量数据进行训练。

Result: DMRetriever在所有六种搜索意图和每个模型规模上都实现了最先进的性能，具有极高的参数效率，596M模型性能超过基线模型13.3倍，33M模型仅用基线7.6%的参数就超越基线。

Conclusion: DMRetriever是首个专门针对灾害管理的检索模型系列，通过创新的训练框架实现了优异的性能和参数效率，为灾害管理信息检索提供了有效解决方案。

Abstract: Effective and efficient access to relevant information is essential for
disaster management. However, no retrieval model is specialized for disaster
management, and existing general-domain models fail to handle the varied search
intents inherent to disaster management scenarios, resulting in inconsistent
and unreliable performance. To this end, we introduce DMRetriever, the first
series of dense retrieval models (33M to 7.6B) tailored for this domain. It is
trained through a novel three-stage framework of bidirectional attention
adaptation, unsupervised contrastive pre-training, and difficulty-aware
progressive instruction fine-tuning, using high-quality data generated through
an advanced data refinement pipeline. Comprehensive experiments demonstrate
that DMRetriever achieves state-of-the-art (SOTA) performance across all six
search intents at every model scale. Moreover, DMRetriever is highly
parameter-efficient, with 596M model outperforming baselines over 13.3 X larger
and 33M model exceeding baselines with only 7.6% of their parameters. All
codes, data, and checkpoints are available at
https://github.com/KaiYin97/DMRETRIEVER

</details>


### [14] [MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation](https://arxiv.org/abs/2510.15286)
*Xianyang Qi,Yuan Tian,Zhaoyu Hu,Zhirui Kuai,Chang Liu,Hongxiang Lin,Lei Wang*

Main category: cs.IR

TL;DR: 提出了MTmixAtt，一种统一的MoE架构，用于大规模推荐任务，通过自动特征聚类和混合注意力机制，在美团TRec数据集上超越了现有最佳模型，并在线上A/B测试中显著提升了商业指标。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖人工特征工程和场景特定架构，阻碍跨场景迁移和大规模部署，需要统一的解决方案。

Method: MTmixAtt包含AutoToken模块自动聚类异构特征为语义连贯的token，以及MTmixAttBlock模块通过可学习混合矩阵、共享稠密专家和场景感知稀疏专家实现高效token交互。

Result: 在TRec数据集上超越Transformer、WuKong、HiFormer等基线模型；MTmixAtt-1B规模下获得单调增益；线上A/B测试在首页场景提升支付PV 3.62%和实际支付GTV 2.54%。

Conclusion: MTmixAtt为跨场景异构特征建模提供了统一且可扩展的解决方案，显著改善了用户体验和商业成果。

Abstract: Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.

</details>


### [15] [GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework](https://arxiv.org/abs/2510.15299)
*Yijia Sun,Shanshan Huang,Zhiyuan Guan,Qiang Luo,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GRank是一种无需结构化索引的检索新范式，将目标感知学习与用户中心检索无缝统一，在工业级推荐系统中显著提升召回率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案存在局限性：双塔架构表达能力有限，无法捕捉细粒度用户-物品交互；生成模型缺乏精确的目标感知匹配能力；结构化索引难以融入动态用户偏好且维护成本高昂。

Method: 提出三个关键创新：(1) 目标感知生成器通过GPU加速MIPS进行个性化候选生成；(2) 轻量级排序器在小规模子集上执行细粒度推理；(3) 端到端多任务学习框架确保生成和排序目标间的语义一致性。

Result: 在两个公共基准测试和十亿级生产语料上的实验表明，GRank将Recall@500提升30%以上，P99 QPS达到最先进树和图检索器的1.7倍。

Conclusion: GRank已在生产环境中部署，服务4亿活跃用户，在线A/B测试证实核心参与度指标显著提升，总应用使用时间增加0.160%-0.165%。

Abstract: Industrial-scale recommender systems rely on a cascade pipeline in which the
retrieval stage must return a high-recall candidate set from billions of items
under tight latency. Existing solutions ei- ther (i) suffer from limited
expressiveness in capturing fine-grained user-item interactions, as seen in
decoupled dual-tower architectures that rely on separate encoders, or
generative models that lack precise target-aware matching capabilities, or (ii)
build structured indices (tree, graph, quantization) whose item-centric
topologies struggle to incorporate dynamic user preferences and incur
prohibitive construction and maintenance costs.
  We present GRank, a novel structured-index-free retrieval paradigm that
seamlessly unifies target-aware learning with user-centric retrieval. Our key
innovations include: (1) A target-aware Generator trained to perform
personalized candidate generation via GPU-accelerated MIPS, eliminating
semantic drift and maintenance costs of structured indexing; (2) A lightweight
but powerful Ranker that performs fine-grained, candidate-specific inference on
small subsets; (3) An end-to-end multi-task learning framework that ensures
semantic consistency between generation and ranking objectives.
  Extensive experiments on two public benchmarks and a billion-item production
corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\times$
the P99 QPS of state-of-the-art tree- and graph-based retrievers.
  GRank has been fully deployed in production in our recommendation platform
since Q2 2025, serving 400 million active users with 99.95% service
availability. Online A/B tests confirm significant improvements in core
engagement metrics, with Total App Usage Time increasing by 0.160% in the main
app and 0.165% in the Lite version.

</details>


### [16] [Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable ID-based Models](https://arxiv.org/abs/2510.15308)
*Srijan Saket,Ikuhiro Ihara,Vaibhav Sharma,Danish Kalim*

Main category: cs.IR

TL;DR: 提出一种自动确定ID特征最佳嵌入尺寸的方法，通过维度掩码层将嵌入向量维度减少40-50%，显著降低模型内存占用同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统和社交媒体平台中，大规模ID特征需要消耗大量内存的嵌入表，导致模型臃肿难以部署和维护。

Method: 定义自定义Keras维度掩码层，放置在嵌入查找层之后，仅允许前N个维度通过，从而修剪嵌入向量。

Result: 在公共数据集和生产环境的A/B测试中，有效嵌入维度减少40-50%，内存效率显著提升，模型性能指标无损失或损失极小。

Conclusion: 该方法为处理大量ID特征的平台提供了可扩展解决方案，优化了资源使用和模型性能。

Abstract: In modern recommendation systems and social media platforms like Meta,
TikTok, and Instagram, large-scale ID-based features often require embedding
tables that consume significant memory. Managing these embedding sizes can be
challenging, leading to bulky models that are harder to deploy and maintain. In
this paper, we introduce a method to automatically determine the optimal
embedding size for ID features, significantly reducing the model size while
maintaining performance.
  Our approach involves defining a custom Keras layer called the dimension mask
layer, which sits directly after the embedding lookup. This layer trims the
embedding vector by allowing only the first N dimensions to pass through. By
doing this, we can reduce the input feature dimension by more than half with
minimal or no loss in model performance metrics. This reduction helps cut down
the memory footprint of the model and lowers the risk of overfitting due to
multicollinearity.
  Through offline experiments on public datasets and an online A/B test on a
real production dataset, we demonstrate that using a dimension mask layer can
shrink the effective embedding dimension by 40-50\%, leading to substantial
improvements in memory efficiency. This method provides a scalable solution for
platforms dealing with a high volume of ID features, optimizing both resource
usage and model performance.

</details>


### [17] [Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs](https://arxiv.org/abs/2510.15428)
*Sho Okazaki,Kohei Kaminishi,Takuma Fujiu,Yusheng Wang,Jun Ota*

Main category: cs.IR

TL;DR: 提出了一种结合制造领域概念化和图神经网络推理的过程感知框架，通过本体引导的LLM提取将FMEA工作表转换为统一知识图，使用RGCN学习嵌入，并通过链接预测推断故障原因，在汽车压力传感器装配线上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化制造生产线中的故障原因识别面临系统复杂性、频繁重新配置以及现有FMEA知识可重用性有限的挑战。FMEA工作表包含有价值的专家见解，但由于自然语言变异性、术语不一致和流程差异，在异构生产线间的重用受到阻碍。

Method: 1) 通过本体引导的LLM提取将多个制造生产线的FMEA工作表转换为统一知识图；2) 使用具有过程感知评分函数的RGCN学习同时考虑语义关系和顺序流程的嵌入；3) 采用链接预测来推断和排序与目标生产线流程一致的候选故障原因。

Result: 在汽车压力传感器装配线的案例研究中，该方法优于最先进的检索增强生成基线(F1@20=0.267)和RGCN方法(0.400)，实现了最佳性能(0.523)。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。

Conclusion: 该框架显著提高了FMEA知识在异构生产线间的可转移性，从而支持操作员更可靠地诊断故障，并为智能制造中未来领域自适应LLM应用铺平了道路。

Abstract: Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.

</details>


### [18] [Enhance Large Language Models as Recommendation Systems with Collaborative Filtering](https://arxiv.org/abs/2510.15647)
*Zhisheng Yang,Xiaofei Xu,Ke Deng,Li Li*

Main category: cs.IR

TL;DR: 提出Critic-LLM-RS方法，通过训练独立的协同过滤模型Critic为LLMs提供反馈，解决非调优策略缺乏特定领域知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有非调优策略的LLM推荐系统缺乏特定业务知识，且未明确整合协同过滤这一成功推荐技术。

Method: 训练独立的协同过滤模型Critic学习用户-物品交互，为LLMs提供反馈来优化推荐。

Result: 在真实数据集上的大量实验验证了Critic-LLM-RS的有效性。

Conclusion: Critic-LLM-RS成功填补了非调优策略LLM推荐系统中协同过滤整合的空白。

Abstract: As powerful tools in Natural Language Processing (NLP), Large Language Models
(LLMs) have been leveraged for crafting recommendations to achieve precise
alignment with user preferences and elevate the quality of the recommendations.
The existing approaches implement both non-tuning and tuning strategies.
Compared to following the tuning strategy, the approaches following the
non-tuning strategy avoid the relatively costly, time-consuming, and
expertise-requiring process of further training pre-trained LLMs on
task-specific datasets, but they suffer the issue of not having the
task-specific business or local enterprise knowledge. To the best of our
knowledge, none of the existing approaches following the non-tuning strategy
explicitly integrates collaborative filtering, one of the most successful
recommendation techniques. This study aims to fill the gap by proposing
critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,
we train a separate machine-learning model called Critic that implements
collaborative filtering for recommendations by learning from the interactions
between many users and items. The Critic provides critiques to LLMs to
significantly refine the recommendations. Extensive experiments have verified
the effectiveness of Critic-LLM-RS on real datasets.

</details>


### [19] [SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15682)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael Färber*

Main category: cs.IR

TL;DR: SQuAI是一个可扩展且可信赖的多智能体检索增强生成框架，用于科学问答，通过分解复杂问题、混合检索和自适应文档过滤来提高答案质量和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在科学领域中的局限性，包括复杂开放域问题的准确回答、带引用的明确声明以及数百万科学文档的检索需求。

Method: 使用四个协作智能体：分解复杂问题为子问题，通过混合稀疏-稠密检索获取目标证据，自适应过滤文档以提高上下文相关性，并为每个生成声明提供内联引用。

Result: 相比强大的RAG基线，在忠实性、答案相关性和上下文相关性方面提升了高达+0.088（12%），并发布了包含1000个科学问答证据三元组的基准数据集。

Conclusion: SQuAI通过透明推理、可验证引用和领域级可扩展性，展示了多智能体RAG如何实现更可信赖的科学问答。

Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

</details>


### [20] [Mixture of Experts Approaches in Dense Retrieval Tasks](https://arxiv.org/abs/2510.15683)
*Effrosyni Sokli,Pranav Kasela,Georgios Peikos,Gabriella Pasi*

Main category: cs.IR

TL;DR: 提出了一种更高效的MoE设计SB-MoE，在最终Transformer层后引入单个MoE块，显著减少参数量的同时保持检索性能，特别适用于轻量级DRM模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统DRM模型泛化能力差的问题，同时避免现有MoE方法参数过多的问题，寻求更高效的模型设计。

Method: 在DRM的最终Transformer层后添加单个MoE块，通过两种评估设置（领域内评估和零样本泛化）验证效果，并分析超参数影响。

Result: SB-MoE对轻量级DRM模型（如TinyBERT、BERT-Small）效果显著，在多个基准测试中持续超越标准微调；对于参数更多的模型需要更多训练样本才能提升性能。

Conclusion: SB-MoE是一种高效且有效的MoE设计，特别适合资源受限的轻量级DRM模型，在保持性能的同时大幅减少参数数量。

Abstract: Dense Retrieval Models (DRMs) are a prominent development in Information
Retrieval (IR). A key challenge with these neural Transformer-based models is
that they often struggle to generalize beyond the specific tasks and domains
they were trained on. To address this challenge, prior research in IR
incorporated the Mixture-of-Experts (MoE) framework within each Transformer
layer of a DRM, which, though effective, substantially increased the number of
additional parameters. In this paper, we propose a more efficient design, which
introduces a single MoE block (SB-MoE) after the final Transformer layer. To
assess the retrieval effectiveness of SB-MoE, we perform an empirical
evaluation across three IR tasks. Our experiments involve two evaluation
setups, aiming to assess both in-domain effectiveness and the model's zero-shot
generalizability. In the first setup, we fine-tune SB-MoE with four different
underlying DRMs on seven IR benchmarks and evaluate them on their respective
test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform
zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform
further experiments to analyze the model's dependency on its hyperparameters
(i.e., the number of employed and activated experts) and investigate how this
variation affects SB-MoE's performance. The obtained results show that SB-MoE
is particularly effective for DRMs with lightweight base models, such as
TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning
across benchmarks. For DRMs with more parameters, such as BERT-Base and
Contriever, our model requires a larger number of training samples to achieve
improved retrieval performance. Our code is available online at:
https://github.com/FaySokli/SB-MoE.

</details>


### [21] [GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery](https://arxiv.org/abs/2510.15706)
*Italo Luis da Silva,Hanqi Yan,Lin Gui,Yulan He*

Main category: cs.IR

TL;DR: GraphMind是一个交互式网络工具，通过整合外部API和LLMs，帮助用户评估科学论文的新颖性，提供可验证的上下文洞察和结果可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助科学文献分析方法透明度有限，缺乏通过信息检索模块实现结果可追溯性的机制，无法满足同行评审中对论文新颖性评估的需求。

Method: 开发GraphMind工具，整合arXiv和Semantic Scholar等外部API与LLMs，支持论文的注释、提取、检索和分类，让用户能够标注论文关键元素，通过多种关系探索相关论文。

Result: GraphMind提供了一个易于使用的交互式网络工具，能够捕捉科学论文的主要结构，从多个角度探索相关想法，并通过提供可验证的上下文洞察来评估新颖性。

Conclusion: GraphMind通过结合外部API和LLMs，为用户提供了科学论文核心贡献及其与现有工作联系的丰富结构化视图，有效解决了科学文献新颖性评估中的透明度和可追溯性问题。

Abstract: Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

</details>


### [22] [The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation](https://arxiv.org/abs/2510.15722)
*Da Li,Zecheng Fang,Qiang Yan,Wei Huang,Xuanpu Luo*

Main category: cs.IR

TL;DR: 本文介绍了在CCIR CUP 2025中提出的法律知识检索与生成方法，利用大语言模型和信息检索系统基于法律条文回答用户问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成在自然语言处理领域取得了显著进展，但在法律领域的应用仍处于探索阶段。

Method: 结合信息检索和大语言模型的优势，从可靠来源检索项目并生成相关且上下文适当的响应。

Result: 该方法在多个领域表现出色性能，但在法律领域的应用效果未具体说明。

Conclusion: 提出了基于法律条文的法律知识检索与生成方法，为法律领域的RAG应用提供了探索性方案。

Abstract: Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.

</details>


### [23] [FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens](https://arxiv.org/abs/2510.15729)
*Chao Wang,Yixin Song,Jinhui Ye,Chuan Qin,Dazhong Shen,Lingfeng Liu,Xiang Wang,Yanyong Zhang*

Main category: cs.IR

TL;DR: FACE框架将协同过滤嵌入映射到预训练LLM的token中，通过解耦投影和量化自编码器实现语义对齐，提升推荐性能且无需微调LLM。


<details>
  <summary>Details</summary>
Motivation: 解决LLM难以解释CF方法产生的潜在非语义嵌入的问题，这限制了推荐效果和进一步应用。

Method: 提出FACE框架：1) 解耦投影模块分解CF嵌入为概念特定向量；2) 量化自编码器将连续嵌入转换为LLM token；3) 对比对齐目标确保token与文本信号对齐。

Result: 在三个真实世界推荐数据集上的实证结果显示基准模型性能提升，可解释性研究证实了描述符的可解释性。

Conclusion: 模型无关的FACE框架无需微调LLM即可实现语义对齐，通过利用预训练能力增强推荐性能。

Abstract: Recently, large language models (LLMs) have been explored for integration
with collaborative filtering (CF)-based recommendation systems, which are
crucial for personalizing user experiences. However, a key challenge is that
LLMs struggle to interpret the latent, non-semantic embeddings produced by CF
approaches, limiting recommendation effectiveness and further applications. To
address this, we propose FACE, a general interpretable framework that maps CF
embeddings into pre-trained LLM tokens. Specifically, we introduce a
disentangled projection module to decompose CF embeddings into concept-specific
vectors, followed by a quantized autoencoder to convert continuous embeddings
into LLM tokens (descriptors). Then, we design a contrastive alignment
objective to ensure that the tokens align with corresponding textual signals.
Hence, the model-agnostic FACE framework achieves semantic alignment without
fine-tuning LLMs and enhances recommendation performance by leveraging their
pre-trained capabilities. Empirical results on three real-world recommendation
datasets demonstrate performance improvements in benchmark models, with
interpretability studies confirming the interpretability of the descriptors.
Code is available in https://github.com/YixinRoll/FACE.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [24] [Game mechanics for cyber-harm awareness in the metaverse](https://arxiv.org/abs/2510.15180)
*Sophie McKenzie,Jeb Webb,Robin Doss*

Main category: cs.MM

TL;DR: 开发CyberNinjas VR体验，教育8-16岁儿童在元宇宙中的安全行为，分析游戏机制如何促进网络安全行为。


<details>
  <summary>Details</summary>
Motivation: 随着元宇宙作为下一代互联网融合沉浸式技术，重塑儿童互动并放大体验，虚拟现实虽然提供完全沉浸式、高度互动和多感官参与，但也增加了年轻或脆弱用户的网络伤害风险。

Method: 开发CyberNinjas VR体验，为8-16岁儿童提供元宇宙安全行为教育，包括有害互动的明确转介步骤，并分析游戏机制。

Result: 该项目分析CyberNinjas以理解游戏机制如何培养网络安全行为，为未来优先考虑安全性和包容性的VR环境设计提供参考。

Conclusion: 理解用户在元宇宙游戏中的参与度将有助于设计未来优先考虑安全和包容性的VR环境。

Abstract: Educating children and young people to be safe online is essential,
especially as the metaverse, a next-generation internet blending immersive
technologies, promises to reshape their interactions and amplify their
experiences. While virtual reality offers fully immersive, highly interactive,
and multi-sensory engagement, it also heightens cyber harm risks for young or
vulnerable users. To address this, the CyberNinjas VR experience was developed
to educate children aged 8 to 16 on safe metaverse behaviours, providing clear
referral steps for harmful interactions. Understanding user engagement in
metaverse gaming will aid the design of future VR environments which prioritize
safety and inclusivity. This project analyses CyberNinjas to understand how
game mechanics can foster cyber-safe behaviours.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [25] [Adaptive Base Representation Theorem: An Alternative to Binary Number System](https://arxiv.org/abs/2510.15099)
*Ravin Kumar*

Main category: cs.IT

TL;DR: 本文提出了自适应基数表示定理和一种新的数字系统，作为二进制系统的结构化替代方案，能够在相同比特数下唯一表示十进制数，并兼容现有压缩和纠错算法。


<details>
  <summary>Details</summary>
Motivation: 为数字计算机提供二进制系统的结构化替代方案，探索新的数字数据表示方法。

Method: 基于自适应基数表示定理，开发新的数字系统，确保在相同比特数下唯一表示十进制数，并与现有算法兼容。

Result: ABR数字系统能够编码与二进制相同的整数范围，验证了其作为可行替代方案的潜力。

Conclusion: ABR数字系统在数字数据表示和计算设计方面具有应用前景，可激发新的方法探索。

Abstract: This paper introduces the Adaptive Base Representation (ABR) Theorem and
proposes a novel number system that offers a structured alternative to the
binary number system for digital computers. The ABR number system enables each
decimal number to be represented uniquely and using the same number of bits,
$n$, as the binary encoding. Theoretical foundations and mathematical
formulations demonstrate that ABR can encode the same integer range as binary,
validating its potential as a viable alternative. Additionally, the ABR number
system is compatible with existing data compression algorithms like Huffman
coding and arithmetic coding, as well as error detection and correction
mechanisms such as Hamming codes. We further explore practical applications,
including digital steganography, to illustrate the utility of ABR in
information theory and digital encoding, suggesting that the ABR number system
could inspire new approaches in digital data representation and computational
design.

</details>


### [26] [Outage-Aware Sum Rate Maximization in Movable Antennas-Enabled Systems](https://arxiv.org/abs/2510.15292)
*Guojie Hu,Qingqing Wu,Ming-Min Zhao,Wen Chen,Zhenyu Xiao,Kui Xu,Jiangbo Si*

Main category: cs.IT

TL;DR: 本文研究了可移动天线(MAs)支持的多输入单输出(MISO)系统，在延迟敏感场景下，通过联合优化天线位置和发射波束成形来最大化中断感知的和速率。


<details>
  <summary>Details</summary>
Motivation: 在延迟敏感场景中，用户避免周期性发送训练信号以避免额外延迟，基站仅依赖统计信道状态信息(CSI)进行数据传输，需要优化系统性能。

Method: 采用基于统计CSI的迫零波束成形设计，利用Laguerre级数近似推导SINR的闭式CDF，并开发投影梯度上升(PGA)方法迭代优化天线位置。

Result: 数值结果表明，所提方案相比传统固定位置天线(FPA)和其他竞争基准方案具有更好的性能。

Conclusion: 可移动天线系统在仅使用统计CSI的情况下，通过联合优化天线位置和波束成形，能够显著提升中断感知的和速率性能。

Abstract: In this paper, we investigate the movable antennas (MAs)-enabled
multiple-input-single-output (MISO) systems, where the base station (BS)
equipped with multiple MAs serves multiple single-antenna user. The
delay-sensitive scenario is considered, where users refrain from periodically
sending training signals to the BS for channel estimations to avoid additional
latency. As a result, the BS relies solely on the statistical channel state
information (CSI) to transmit data with a fixed rate. Under this setup, we aim
to maximize the outage-aware sum rate of all users, by jointly optimizing
antenna positions and the transmit beamforming at the BS, while satisfying the
given target outage probability requirement at each user. The problem is highly
non-convex, primarily because the exact cumulative distribution function (CDF)
of the received signal-to-interference-plus-noise ratio (SINR) of each user is
difficult to derive. To simplify analysis and without comprising performance,
we adopt the statistical CSI based zero-forcing beamforming design. We then
introduce one important lemma to derive the tight mean and variance of the
SINR. Leveraging these results, we further exploit the Laguerre series
approximation to successfully derive the closedform and tight CDF of the SINR.
Subsequently, the outageaware sum rate expression is presented but still
includes complex structure with respect to antenna positions. Facing this
challenge, the projected gradient ascent (PGA) method is developed to
iteratively update antenna positions until convergence. Numerical results
demonstrate the effectiveness of our proposed schemes compared to conventional
fixed-position antenna (FPA) and other competitive benchmarks.

</details>


### [27] [Rotatable Antenna Meets UAV: Towards Dual-Level Channel Reconfiguration Paradigm for ISAC](https://arxiv.org/abs/2510.15295)
*Shiying Chen,Guangji Chen,Long Shi,Qingqing Wu,Kang Wei*

Main category: cs.IT

TL;DR: 本文提出了一种用于无人机集成感知与通信的双层信道重构框架，通过部署可旋转天线主动控制感知与通信信道的路径损耗和相关性，实现性能权衡优化。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)需要在共享无线资源的情况下实现感知与通信功能的平衡，但现有方法难以灵活控制两者之间的性能权衡。

Method: 提出双层信道重构框架，在无人机上部署可旋转天线，联合优化天线旋转、发射波束成形和无人机轨迹，最大化通信速率同时满足感知性能要求。

Result: 对于静态无人机场景推导出最优天线旋转、波束成形和悬停位置的闭式解；对于移动无人机场景证明最优轨迹遵循悬停-飞行-悬停结构，并获得全局最优解。

Conclusion: 仿真结果表明所提设计相比基准方案显著提高了可实现的感知与通信权衡区域。

Abstract: Integrated sensing and communication (ISAC) is viewed as a key enabler for
future wireless networks by sharing the hardware and wireless resources between
the functionalities of sensing and communication (S&C). Due to the shared
wireless resources for both S&C, it is challenging to achieve a critical
trade-off between these two integrated functionalities. To address this issue,
this paper proposes a novel dual-level channel reconfiguration framework for
ISAC by deploying rotatable antennas at an unmanned aerial vehicle (UAV), where
both the large-scale path loss and the correlation of S&C channels can be
proactively controlled, thereby allowing a flexible trade-off between S&C
performance. To characterize the S&C tradeoff, we aim to maximize the
communication rate by jointly optimizing the RA rotation, the transmit
beamforming, and the UAV trajectory, subject to the given requirement of
sensing performance. For the typical scenario of static UAV deployment, we
introduce the concept of subspace correlation coefficient to derive closed-form
solutions for the optimal RA rotation, transmit beamforming, and UAV hovering
location. For the scenario of a fully mobile UAV, we prove that the optimal
trajectory of a UAV follows a hover-fly-hover (HFH) structure, thereby
obtaining its global optimal solution. Simulation results show that the
proposed design significantly improves the achievable S&C trade-off region
compared to benchmark schemes.

</details>


### [28] [Subverting Flexible Multiuser Communications via Movable Antenna-Enabled Jammer](https://arxiv.org/abs/2510.15298)
*Guojie Hu,Qingqing Wu,Lipeng Zhu,Kui Xu,Guoxin Li,Jiangbo Si,Jian Ouyang,Tong-Xing Zheng*

Main category: cs.IT

TL;DR: 本文研究如何利用可移动天线技术，通过优化天线位置和干扰波束成形来破坏可疑多用户下行通信，最小化可疑接收器的总速率或最小速率。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术能够通过自适应调整天线位置来重构无线信道，为系统性能提升带来额外的空间自由度。从安全角度出发，利用可移动天线合法干扰器来破坏可疑的多用户下行通信。

Method: 联合优化可移动天线干扰器的天线位置和干扰波束成形，考虑可疑发射器会反应性地调整功率分配来对抗干扰。开发了基于交替优化的有效算法来求解简化后的问题。

Result: 数值结果表明，与传统的固定位置天线和其他竞争基准相比，所提出的方案具有有效性。对于两个可疑接收器的特殊情况，揭示了天线位置部署规则的深刻结论。

Conclusion: 可移动天线技术能够有效破坏可疑通信，通过优化天线位置和干扰波束成形可以显著降低可疑接收器的通信性能，同时分析了实现全局性能下界的理想天线部署方案。

Abstract: Movable antenna (MA) is an emerging technology which can reconfigure wireless
channels via adaptive antenna position adjustments at transceivers, thereby
bringing additional spatial degrees of freedom for improving system
performance. In this paper, from a security perspective, we exploit the
MAenabled legitimate jammer (MAJ) to subvert suspicious multiuser downlink
communications consisting of one suspicious transmitter (ST) and multiple
suspicious receivers (SRs). Specifically, our objective is to minimize the
benefit (the sum rate of all SRs or the minimum rate among all SRs) of such
suspicious communications, by jointly optimizing antenna positions and the
jamming beamforming at the MAJ. However, the key challenge lies in that given
the MAJ's actions, the ST can reactively adjust its power allocations to
instead maximize its benefit for mitigating the unfavorable interference. Such
flexible behavior of the ST confuses the optimization design of the MAJ to a
certain extent. Facing this difficulty, corresponding to the above two
different benefits: i) we respectively determine the optimal behavior of the ST
given the MAJ's actions; ii) armed with these, we arrive at two simplified
problems and then develop effective alternating optimization based algorithms
to iteratively solve them. In addition to these, we also focus on the special
case of two SRs, and reveal insightful conclusions about the deployment rule of
antenna positions at the MAJ. Furthermore, we analyze the ideal antenna
deployment scheme at the MAJ for achieving the globally performance lower
bound. Numerical results demonstrate the effectiveness of our proposed schemes
compared to conventional fixed-position antenna (FPA) and other competitive
benchmarks.

</details>


### [29] [ProxySelect: Frequency Selectivity-Aware Scheduling for Joint OFDMA and MU-MIMO in 802.11ax WiFi](https://arxiv.org/abs/2510.15452)
*Xiang Zhang,Michail Palaiologos,Christian Bluemm,Giuseppe Caire*

Main category: cs.IT

TL;DR: ProxySelect是一种针对802.11ax WiFi中OFDMA和MU-MIMO联合使用的可扩展用户调度算法，通过代理速率和采样候选组生成来解决频率选择性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着WiFi带宽增长，信道频率选择性增强，导致传统窄带用户选择算法在802.11ax中效率低下，需要新的调度方案来应对频率选择性挑战。

Method: 将调度任务建模为整数线性规划问题，引入代理速率来近似ZFBF速率而无需复杂矩阵求逆，并开发基于采样的候选组生成方案来限制问题规模。

Result: 使用基于射线追踪的真实信道模型仿真表明，ProxySelect在显著降低复杂度的同时实现了接近最优的速率性能。

Conclusion: ProxySelect为802.11ax中的OFDMA和MU-MIMO联合调度提供了一种高效且可扩展的解决方案，能够有效应对频率选择性带来的挑战。

Abstract: IEEE 802.11ax introduces orthogonal frequency division multiple access
(OFDMA) to WiFi to support concurrent transmissions to a larger number of
users. As bandwidth continues to grow, WiFi channels exhibit increased
frequency selectivity, which poses new challenges for MU-MIMO user selection:
the optimal user set varies across frequency and is interleaved over subbands
(called resource units, or RUs). This frequency selectivity, coupled with the
complex subband allocation pattern, renders conventional narrowband user
selection algorithms inefficient for 802.11ax. In this paper, we propose
\emph{ProxySelect}, a scalable and frequency selectivity-aware user scheduling
algorithm for joint OFDMA and MU-MIMO usage in 802.11ax under zero-forcing
beamforming (ZFBF). The scheduling task is formulated as an integer linear
program (ILP) with binary variables indicating user (group)-RU associations,
and linear constraints ensuring standard compatibility. To reduce complexity,
we introduce a novel proxy rate--a function of individual channel strengths and
their correlations--that approximates the ZFBF rate without requiring
cubic-complexity matrix inversion. Additionally, we develop a sampling-based
candidate group generation scheme that selects up to $T$ near-orthogonal user
groups for each RU, thereby bounding the ILP size and ensuring scalability.
Simulations using realistic ray-tracing-based channel models show that
ProxySelect achieves near-optimal rate performance with significantly lower
complexity.

</details>


### [30] [Near-Field Imaging by Exploiting Frequency Correlation in Wireless Communication Networks](https://arxiv.org/abs/2510.15459)
*Tianyu Yang,Kangda Zhi,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了一种基于稀疏贝叶斯学习的近场成像方法，利用多频段无线通信网络和图像相关性，通过优化照明模式提高成像性能


<details>
  <summary>Details</summary>
Motivation: 解决宽带无线通信网络中的近场成像问题，利用均匀线性阵列的近场信道和频域图像相关性来提升成像质量

Method: 将图像恢复建模为特殊的多测量向量压缩感知问题，提出基于稀疏贝叶斯学习的解决方案，同时估计所有图像系数及其相关性，并设计了两种照明模式优化方案

Result: 数值结果表明所提出的SBL方法和照明设计具有有效性和优越性

Conclusion: 该方法成功解决了多频段近场成像问题，通过利用图像相关性和优化照明模式显著提升了成像性能

Abstract: In this work, we address the near-field imaging under a wideband wireless
communication network by exploiting both the near-field channel of a uniform
linear array (ULA) and the image correlation in the frequency domain. We first
formulate the image recovery as a special multiple measurement vector (MMV)
compressed sensing (CS) problem, where at various frequencies the sensing
matrices can be different, and the image coefficients are correlated. To solve
such an MMV problem with various sensing matrices and correlated coefficients,
we propose a sparse Bayesian learning (SBL)-based solution to simultaneously
estimate all image coefficients and their correlation on multiple frequencies.
Moreover, to enhance estimation performance, we design two illumination
patterns following two different criteria. From the CS perspective, the first
design minimizes the total coherence of the sensing matrix to increase the
mutual orthogonality of the basis vectors. Alternatively, to improve SNR, the
second design maximizes the illumination power of the imaging area. Numerical
results demonstrate the effectiveness of the proposed SBL-based method and the
superiority of the illumination designs.

</details>


### [31] [New generalizations of circular complex fuzzy sets and Gaussian weighted aggregation operators](https://arxiv.org/abs/2510.15605)
*Yelda Gülfırat,Mehmet Ünver*

Main category: cs.IT

TL;DR: 提出了一种新的圆形复q阶正交对模糊集(CCq-ROFS)，统一了现有圆形复直觉模糊集和复q阶正交对模糊集框架，并基于高斯函数构建了新的聚合算子。


<details>
  <summary>Details</summary>
Motivation: 现有模糊集框架存在局限性，需要一种更通用的统一模型来更平滑、统计意义更强地表示不确定性。

Method: 扩展高斯框架到CCq-ROFS，使用高斯三角范数和余范数构建高斯加权算术和几何聚合算子。

Result: 开发了新的高斯基聚合算子，能够一致地整合隶属度和非隶属度信息，用于模糊建模和决策。

Conclusion: CCq-ROFS提供了一个统一且统计意义强的模糊集框架，其高斯基聚合算子为模糊建模和决策提供了有效工具。

Abstract: In this paper, we introduce the concept of the circular complex $q$-rung
orthopair fuzzy set (CC$q$-ROFS) as a novel generalization that unifies the
existing frameworks of circular complex intuitionistic fuzzy sets (CCIFSs) and
complex $q$-rung orthopair fuzzy sets. If $q = 2$, the structure is referred to
as a circular complex Pythagorean fuzzy set, and if $q = 3$, it is called a
circular complex Fermatean fuzzy set. The proposed approach extends the
Gaussian-based framework to the CC$q$-ROFSs, aiming to achieve a smoother and
statistically meaningful representation of uncertainty. Within this setting,
new Gaussian-based aggregation operators for CC$q$-ROFSs are constructed by
employing the Gaussian triangular norm and conorm. Furthermore,
Gaussian-weighted arithmetic and Gaussian-weighted geometric aggregation
operators are formulated to enable consistent integration of membership and
non-membership information for fuzzy modeling and decision-making.

</details>


### [32] [Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization](https://arxiv.org/abs/2510.15701)
*Binggui Zhou,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出学习型双层架构发现框架(LTTADF)来解决非理想BD-RIS的架构发现挑战，通过架构生成器和性能优化器联合发现给定电路复杂度下的最优架构。


<details>
  <summary>Details</summary>
Motivation: 传统BD-RIS设计存在性能与电路复杂度的权衡问题，非理想BD-RIS的架构发现尚未研究，非理想性和电路复杂度如何联合影响性能尚不清楚。

Method: 提出LTTADF框架，包含架构生成器和性能优化器，在大规模架构空间中有效探索，避免陷入局部最优，实现性能优化的近最优解。

Result: 数值结果为考虑性能-电路复杂度权衡的非理想BD-RIS部署提供了有价值的见解。

Conclusion: LTTADF能够有效解决非理想BD-RIS架构发现的计算复杂度和全局优化难题，实现性能与电路复杂度的良好权衡。

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been
introduced to enable advanced control over electromagnetic waves to further
increase the benefits of traditional RIS in enhancing signal quality and
improving spectral and energy efficiency for next-generation wireless networks.
A significant issue in designing and deploying BD-RIS is the tradeoff between
its performance and circuit complexity. Despite some efforts in exploring
optimal architectures with the lowest circuit complexities for ideal BD-RIS,
architecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,
how non-idealities and circuit complexity jointly affect the performance of
BD-RIS remains unclear, making it difficult to achieve the performance -
circuit complexity tradeoff in the presence of non-idealities. Essentially,
architecture discovery for non-ideal BD-RIS faces challenges from both the
computational complexity of global architecture search and the difficulty in
achieving global optima. To tackle these challenges, we propose a
learning-based two-tier architecture discovery framework (LTTADF) consisting of
an architecture generator and a performance optimizer to jointly discover
optimal architectures of non-ideal BD-RIS given specific circuit complexities,
which can effectively explore over a large architecture space while avoiding
getting trapped in poor local optima and thus achieving near-optimal solutions
for the performance optimization. Numerical results provide valuable insights
for deploying non-ideal BD-RIS considering the performance - circuit complexity
tradeoff.

</details>
