<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [MS-Index: Fast Top-k Subsequence Search for Multivariate Time Series under Euclidean Distance](https://arxiv.org/abs/2512.14723)
*Jens E. d'Hondt,Teun Kortekaas,Odysseas Papapetrou,Themis Palpanas*

Main category: cs.DB

TL;DR: 提出MS-Index算法，用于支持查询时选择相关通道的多变量时间序列子序列最近邻搜索，性能比现有方法快1-2个数量级。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，多变量时间序列的查询通常只涉及部分相关通道，而非所有通道。现有方法无法有效支持查询时动态选择相关通道，导致查询效率低下。

Method: 提出MS-Index算法，支持查询时选择相关通道的多变量时间序列子序列最近邻搜索。算法基于欧氏距离，具有精确性，查询性能随查询通道数呈亚线性增长。

Result: 在34个数据集上的实验表明，MS-Index在原始和标准化子序列搜索中，性能比现有最优方法快1-2个数量级。

Conclusion: MS-Index算法有效解决了多变量时间序列子序列搜索中查询时通道选择的问题，显著提升了查询性能，具有实际应用价值。

Abstract: Modern applications frequently collect and analyze temporal data in the form of multivariate time series (MTS) -- time series that contain multiple channels. A common task in this context is subsequence search, which involves identifying all MTS that contain subsequences highly similar to a query time series. In practical scenarios, not all channels of an MTS are relevant to every query. For instance, airplane sensors may gather data on a plethora of components and subsystems, but only a few of these are relevant to a specific query, such as identifying the cause of a malfunctioning landing gear, or a specific flight maneuver. Consequently, the relevant query channels are often specified at query time. In this work, we introduce the Multivariate Subsequence Index (MS-Index), a novel algorithm for nearest neighbor MTS subsequence search under Euclidean distance that supports ad-hoc selection of query channels. The algorithm is exact and demonstrates query performance that scales sublinearly to the number of query channels. We examine the properties of \name with a thorough experimental evaluation over 34 datasets, and show that it outperforms the state-of-the-art one to two orders of magnitude for both raw and normalized subsequences.

</details>


### [2] [Extracting node comparison insights for the interactive exploration of property graphs](https://arxiv.org/abs/2512.15157)
*Cristina Aguiar,Jacques Chabin,Alexandre Chanson,Mirian Halfeld-Ferrari,Nicolas Hiot,Nicolas Labroche,Patrick Marcel,Verónika Peralta,Felipe Vasconcelos*

Main category: cs.DB

TL;DR: 提出一种从属性图中自动提取节点比较的方法，支持交互式探索分析


<details>
  <summary>Details</summary>
Motivation: 虽然图节点评分（如中心性）研究已有数十年，但基于节点属性比较属性图中的节点尚未被充分研究

Method: 1. 利用节点上下文设计比较指标；2. 正式定义使用这些指标对节点进行分组的问题；3. 提出多种启发式算法解决该问题

Result: 在真实属性图数据库上的测试表明：简单启发式算法可在几分钟内获得洞察，而较慢的启发式算法可获得更高质量的洞察

Conclusion: 该方法能有效支持属性图的交互式探索分析，提供有意义的节点比较

Abstract: While scoring nodes in graphs to understand their importance (e.g., in terms of centrality) has been investigated for decades, comparing nodes in property graphs based on their properties has not, to our knowledge, yet been addressed. In this paper, we propose an approach to automatically extract comparison of nodes in property graphs, to support the interactive exploratory analysis of said graphs. We first present a way of devising comparison indicators using the context of nodes to be compared. Then, we formally define the problem of using these indicators to group the nodes so that the comparisons extracted are both significant and not straightforward. We propose various heuristics for solving this problem. Our tests on real property graph databases show that simple heuristics can be used to obtain insights within minutes while slower heuristics are needed to obtain insights of higher quality.

</details>


### [3] [Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting](https://arxiv.org/abs/2512.15308)
*Basil Ell*

Main category: cs.DB

TL;DR: 提出用于有向标记多重图（如RDF图）的图模式关联规则（GPARs），支持生成性和评估性任务，超越现有形式化方法，在概率空间中定义置信度、提升度、杠杆度和确信度等度量。


<details>
  <summary>Details</summary>
Motivation: 现有图关联规则形式化方法（如图函数依赖、图实体依赖、关系关联规则等）在处理有向标记多重图时存在局限性，需要更强大的框架来支持图的扩展和评估任务。

Method: 引入图模式关联规则（GPARs），采用无重复语义评估图模式，定义概率空间并推导置信度、提升度、杠杆度和确信度等概率度量，分析这些度量与传统项集度量的关系。

Result: GPARs框架超越了现有形式化方法，能够更有效地考虑图拓扑结构，在概率设置中定义了关联规则度量，并确定了这些度量保持特征性质的条件。

Conclusion: GPARs为有向标记多重图提供了强大的关联规则框架，支持生成和评估任务，通过概率度量和拓扑感知方法改进了现有技术。

Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.

</details>


### [4] [Revisiting Task-Oriented Dataset Search in the Era of Large Language Models: Challenges, Benchmark, and Solution](https://arxiv.org/abs/2512.15363)
*Zixin Wei,Yucan Guo,Jinyang Li,Xiaolin Han,Xiaolong Jin,Chenhao Ma*

Main category: cs.DB

TL;DR: KATS是一个端到端的任务导向数据集搜索系统，通过构建任务-数据集知识图谱和混合查询引擎，解决科学文献中数据集搜索的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据驱动研究中寻找合适数据集是关键的"第一步"，但现有搜索系统面临用户意图模糊、任务-数据集映射差距、实体歧义等挑战，缺乏标准化评估基准。

Method: KATS包含离线知识库构建和在线查询处理：1) 使用协作多智能体框架构建动态更新的任务-数据集知识图谱；2) 基于语义的实体链接和消歧机制；3) 结合向量搜索和图排名的混合查询引擎；4) 提出CS-TDS评估基准套件。

Result: 在CS-TDS基准测试中，KATS在效果和效率上都显著优于最先进的检索增强生成框架，为下一代数据集发现系统提供了稳健蓝图。

Conclusion: KATS通过创新的知识图谱构建和混合检索方法，有效解决了任务导向数据集搜索的关键挑战，填补了评估基准的空白，为数据驱动研究提供了强大的数据集发现工具。

Abstract: The search for suitable datasets is the critical "first step" in data-driven research, but it remains a great challenge. Researchers often need to search for datasets based on high-level task descriptions. However, existing search systems struggle with this task due to ambiguous user intent, task-to-dataset mapping and benchmark gaps, and entity ambiguity. To address these challenges, we introduce KATS, a novel end-to-end system for task-oriented dataset search from unstructured scientific literature. KATS consists of two key components, i.e., offline knowledge base construction and online query processing. The sophisticated offline pipeline automatically constructs a high-quality, dynamically updatable task-dataset knowledge graph by employing a collaborative multi-agent framework for information extraction, thereby filling the task-to-dataset mapping gap. To further address the challenge of entity ambiguity, a unique semantic-based mechanism is used for task entity linking and dataset entity resolution. For online retrieval, KATS utilizes a specialized hybrid query engine that combines vector search with graph-based ranking to generate highly relevant results. Additionally, we introduce CS-TDS, a tailored benchmark suite for evaluating task-oriented dataset search systems, addressing the critical gap in standardized evaluation. Experiments on our benchmark suite show that KATS significantly outperforms state-of-the-art retrieval-augmented generation frameworks in both effectiveness and efficiency, providing a robust blueprint for the next generation of dataset discovery systems.

</details>


### [5] [ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata](https://arxiv.org/abs/2512.15365)
*Gajendra Doniparthi,Shashank Balu Pandhare,Stefan Deßloch,Timo Mühlhaus*

Main category: cs.DB

TL;DR: ArcBERT是一个基于大语言模型的系统，用于研究数据管理生态系统中的元数据探索，能够理解自然语言查询和元数据结构层次。


<details>
  <summary>Details</summary>
Motivation: 传统研究数据管理中的搜索应用通常需要用户提交基于关键词的查询，而不是自然语言。随着大语言模型在领域特定NLP任务中的应用越来越普遍，需要开发能够理解自然语言查询和元数据结构的智能搜索系统。

Method: 开发了ArcBERT系统，这是一个基于大语言模型的系统，专门用于集成元数据探索。该系统采用语义匹配而非传统关键词匹配，能够理解自然语言查询，并特别能够理解元数据中的结构和层次关系。

Result: ArcBERT能够有效处理多样化的用户查询模式，通过理解元数据的结构和层次，提供比传统关键词搜索更智能的元数据探索体验。

Conclusion: 基于大语言模型的ArcBERT系统为研究数据管理生态系统提供了更自然、更智能的元数据探索方式，能够理解自然语言查询和元数据结构，代表了该领域搜索应用的重要进步。

Abstract: Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [6] [A Preprocessing Framework for Video Machine Vision under Compression](https://arxiv.org/abs/2512.15331)
*Fei Zhao,Mengxi Guo,Shijie Zhao,Junlin Li,Li Zhang,Xiaodong Xie*

Main category: cs.MM

TL;DR: 提出针对机器视觉任务的视频预处理框架，通过神经预处理器保留关键信息，提升率-精度性能，相比标准编解码器可节省超过15%的码率。


<details>
  <summary>Details</summary>
Motivation: 现有视频编码优化方法主要关注人类感知指标的失真最小化，忽视了机器视觉系统的更高要求。需要专门针对机器视觉任务的视频预处理解决方案。

Method: 提出包含神经预处理器的视频预处理框架，保留后续任务的关键信息；引入可微分虚拟编解码器在训练阶段提供率和失真约束；测试时直接应用广泛使用的标准编解码器。

Result: 在两个典型下游任务和多种骨干网络上进行广泛实验，结果表明相比仅使用标准编解码器基准版本，该方法可以节省超过15%的码率。

Conclusion: 该方法能有效提升机器视觉任务的率-精度性能，且易于应用于实际场景，为机器视觉优化的视频压缩提供了实用解决方案。

Abstract: There has been a growing trend in compressing and transmitting videos from terminals for machine vision tasks. Nevertheless, most video coding optimization method focus on minimizing distortion according to human perceptual metrics, overlooking the heightened demands posed by machine vision systems. In this paper, we propose a video preprocessing framework tailored for machine vision tasks to address this challenge. The proposed method incorporates a neural preprocessor which retaining crucial information for subsequent tasks, resulting in the boosting of rate-accuracy performance. We further introduce a differentiable virtual codec to provide constraints on rate and distortion during the training stage. We directly apply widely used standard codecs for testing. Therefore, our solution can be easily applied to real-world scenarios. We conducted extensive experiments evaluating our compression method on two typical downstream tasks with various backbone networks. The experimental results indicate that our approach can save over 15% of bitrate compared to using only the standard codec anchor version.

</details>


### [7] [One Size Doesn't Fit All: Age-Aware Gamification Mechanics for Multimedia Learning Environments](https://arxiv.org/abs/2512.15630)
*Sarah Kaißer,Markus Kleffmann,Kristina Schaaff*

Main category: cs.MM

TL;DR: 本文探讨了如何设计年龄感知的游戏化学习系统，以满足不同年龄段学习者的动机和认知需求，提出了年龄群体、机制和效果的映射关系，并制定了五项设计原则和三种技术实现模式。


<details>
  <summary>Details</summary>
Motivation: 当前数字学习中的游戏化应用普遍忽视年龄差异，而不同年龄段的学习者在动机和认知需求上存在显著差异。为了提升游戏化学习的有效性和包容性，需要开发能够适应不同年龄特点的年龄感知游戏化设计方法。

Method: 通过针对性的文献综述，建立了年龄群体、游戏化机制和效果之间的映射关系。基于此分析，推导出五项年龄特定游戏化设计原则，并识别了三种在多媒体学习环境中实现这些原则的技术模式。

Result: 研究发现游戏化并非普遍有效，而是需要差异化的设计来支持不同年龄段的学习者。提出的年龄-机制-效果映射为设计者提供了理论框架，五项设计原则和三种技术模式为实际应用提供了具体指导。

Conclusion: 年龄感知的游戏化设计对于提升数字学习的参与度和包容性至关重要。通过考虑不同年龄段的动机和认知需求，可以开发出更有效、更具包容性的游戏化学习系统，支持全生命周期的学习体验。

Abstract: Gamification is widely used in digital learning. However, most systems neglect age-related differences. This paper investigates how gamification can be designed in an age-aware way to address learners' diverse motivational and cognitive needs. Based on a targeted literature review, we present a mapping of age groups, mechanics, and effects. Furthermore, we derive five design principles for age-specific gamification and identify three technical patterns for implementation in multimedia learning environments. The results indicate that gamification is not universally effective, but rather requires a differentiated design to support engagement and inclusivity across the lifespan.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [8] [A Joint Auction Framework with Externalities and Adaptation](https://arxiv.org/abs/2512.15043)
*Chun Fang,Luowen Liu,Kun Huang,Tao Ruan,Sheng Yan,Zhen Wang,Huan Li,Qiang Liu,Xingxing Wang*

Main category: cs.GT

TL;DR: 提出JEANet框架，首个将全局外部性融入联合拍卖的自动化机制设计方法，解决传统广告与联合广告的统一拍卖问题


<details>
  <summary>Details</summary>
Motivation: 现有联合广告方法无法同时处理传统广告和联合广告框架，且忽视全局外部性和多方广告主竞价变化，限制了收入潜力和通用性

Method: 提出包含外部性和自适应的联合拍卖框架(JEANet)，利用自动化机制设计方法计算满足个体理性和近似占优策略激励相容的联合拍卖机制

Result: 实验结果表明JEANet在多槽位联合拍卖中优于现有基准方法，能够动态适应多方广告主竞价特性，实现传统与联合广告的统一拍卖

Conclusion: JEANet是首个将全局外部性融入联合拍卖的AMD方法，解决了现有方法的局限性，提升了拍卖效率和收入潜力

Abstract: Recently, joint advertising has gained significant attention as an effective approach to enhancing the efficiency and revenue of advertising slot allocation. Unlike traditional advertising, which allocates advertising slots exclusively to a single advertiser, joint advertising displays advertisements from brands and stores that have established a joint selling relationship within the same advertising slot. However, existing approaches often struggle to accommodate both joint and traditional advertising frameworks, thereby limiting the revenue potential and generalizability of joint advertising. Furthermore, these methods are constrained by two critical limitations: they generally neglect the influence of global externalities, and they fail to address the bidding variability stemming from multi-party advertiser participation. Collectively, these limitations present substantial challenges to the design of joint auction mechanisms. To address these challenges, we propose a Joint Auction Framework incorporating Externalities and Adaptation, and leverage the automated mechanism design (AMD) method through our proposed JEANet to compute joint auction mechanisms that satisfy the conditions of individual rationality (IR) and approximate dominant strategy incentive compatibility (DSIC). As the first AMD method to integrate global externalities into joint auctions, JEANet dynamically adapts to the bidding characteristics of multi-party advertiser and enables unified auctions that integrate both joint and traditional advertising. Extensive experimental results demonstrate that JEANet outperforms state-of-the-art baselines in multi-slot joint auctions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [Label-consistent clustering for evolving data](https://arxiv.org/abs/2512.15210)
*Ameet Gadekar,Aristides Gionis,Thibault Marette*

Main category: cs.DS

TL;DR: 提出标签一致k中心问题：在更新聚类时限制对先前解决方案的修改，保证解决方案的平滑演进


<details>
  <summary>Details</summary>
Motivation: 数据分析通常是迭代过程，新数据出现时需要更新现有解决方案，但需要保持一致性，避免剧烈变化，确保解决方案平滑演进

Method: 研究标签一致k中心问题：给定点集X、参数k和b、先前聚类H，计算新聚类C（k个中心），最小化聚类成本同时最多引入b个相对于H的变化；提出两个常数因子近似算法

Result: 提出了标签一致k中心问题的两个常数因子近似算法，并在真实数据集上通过实验验证了方法的有效性

Conclusion: 标签一致k中心问题在迭代数据分析中具有重要意义，提出的算法能够在保证聚类质量的同时控制解决方案的变化，实现平滑演进

Abstract: Data analysis often involves an iterative process, where solutions must be continuously refined in response to new data. Typically, as new data becomes available, an existing solution must be updated to incorporate the latest information. In addition to seeking a high-quality solution for the task at hand, it is also crucial to ensure consistency by minimizing drastic changes from previous solutions. Applying this approach across many iterations, ensures that the solution evolves gradually and smoothly.
  In this paper, we study the above problem in the context of clustering, specifically focusing on the $k$-center problem. More precisely, we study the following problem: Given a set of points $X$, parameters $k$ and $b$, and a prior clustering solution $H$ for $X$, our goal is to compute a new solution $C$ for $X$, consisting of $k$ centers, which minimizes the clustering cost while introducing at most $b$ changes from $H$. We refer to this problem as label-consistent $k$-center, and we propose two constant-factor approximation algorithms for it. We complement our theoretical findings with an experimental evaluation demonstrating the effectiveness of our methods on real-world datasets.

</details>


### [10] [A Constant-Factor Approximation for Directed Latency](https://arxiv.org/abs/2512.15473)
*Jannis Blauth,Ramin Mousavi*

Main category: cs.DS

TL;DR: 本文提出了第一个多项式时间内解决有向延迟问题的常数因子近似算法，通过新的分桶方法和改进的LP松弛技术。


<details>
  <summary>Details</summary>
Motivation: 有向延迟问题（Directed Latency）与对称版本相比，理解存在显著差距。最佳近似因子十多年来一直停留在O(log n)，最近虽有常数因子近似但需要拟多项式时间。需要开发多项式时间的常数因子近似算法。

Method: 引入全新的分桶方法，通过限制LP可行域来加强标准LP松弛，采用较少的猜测。虽然结果LP不再是原始问题的松弛，但仍能获得良好解。设计了针对该LP分数解的舍入算法。

Result: 首次实现了有向延迟问题的多项式时间常数因子近似算法，突破了之前O(log n)近似因子和拟多项式时间常数的限制。

Conclusion: 通过创新的分桶策略和受限LP松弛，成功解决了有向延迟问题的多项式时间常数近似难题，为该领域提供了重要进展。

Abstract: In the Directed Latency problem, we are given an asymmetric metric on a set of vertices (or clients), and a given depot $s$. We seek a path $P$ starting at $s$ and visiting all the clients so as to minimize the sum of client waiting times (also known as latency) before being visited on the path.
  In contrast to the symmetric version of this problem (also known as the Deliveryperson problem and the Repairperson problem in the literature), there are significant gaps in our understanding of Directed Latency. The best approximation factor has remained at $O(\log n)$, where $n$ is the number of clients, for more than a decade [Friggstad, Salavatipour, and Svitkina, '13]. Only recently, [Friggstad and Swamy, '22] presented a constant-factor approximation but in quasi-polynomial time. Both results follow similar ideas: they consider buckets with geometrically-increasing distances, build paths in each bucket, and then stitch together all these paths to get a feasible solution. [Friggstad and Swamy, '22] showed if we guess a vertex from each bucket and augment a standard LP relaxation with these guesses, then one can reduce the stitching cost. Unfortunately, there are logarithmically many buckets so the running time of their algorithm is quasi-polynomial.
  In this paper, we present the first constant-factor approximation for Directed Latency in polynomial time by introducing a completely new way of bucketing which helps us strengthen a standard LP relaxation with less aggressive guessing. Although the resulting LP is no longer a relaxation of Directed Latency, it still admits a good solution. We present a rounding algorithm for fractional solutions of our LP, crucially exploiting the way we restricted the feasibility region of the LP formulation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [11] [Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems](https://arxiv.org/abs/2512.14733)
*Qiang Chen,Venkatesh Ganapati Hegde*

Main category: cs.IR

TL;DR: 在流媒体平台中，通过优化探索内容的展示位置（基于覆盖面和机会成本），在低参与度的滚动区域引入"完全不同的内容"行，既保持业务指标又收集无偏交互数据，最终提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 探索对提升长期推荐质量至关重要，但在远程优先的电视环境中，探索往往会损害短期业务表现，因为用户被动参与、期望即时相关性且几乎没有修正机会。

Method: 识别参与度较低的滚动深度区域，策略性地引入包含随机化内容的专用容器"完全不同的内容"行。不强制在UI中统一实施探索，而是根据经验确定的低成本、高覆盖位置条件化展示，以最小化与平台观看时间目标的权衡。

Result: 大规模A/B测试显示，该策略在保持业务指标的同时收集了无偏交互数据。收集的无偏数据整合到下游候选生成中，显著提升了用户参与度。

Conclusion: 该方法通过引入可部署的、行为感知的机制，在规模上展示探索性内容，补充了现有的行内多样化和基于bandit的探索技术，验证了无偏数据对推荐系统的价值。

Abstract: Exploration is essential to improve long-term recommendation quality, but it often degrades short-term business performance, especially in remote-first TV environments where users engage passively, expect instant relevance, and offer few chances for correction. This paper introduces an approach for delivering content-level exploration safely and efficiently by optimizing its placement based on reach and opportunity cost. Deployed on a large-scale streaming platform with over 100 million monthly active users, our approach identifies scroll-depth regions with lower engagement and strategically introduces a dedicated container, the "Something Completely Different" row containing randomized content. Rather than enforcing exploration uniformly across the user interface (UI), we condition its appearance on empirically low-cost, high-reach positions to ensure minimal tradeoff against platform-level watch time goals. Extensive A/B testing shows that this strategy preserves business metrics while collecting unbiased interaction data. Our method complements existing intra-row diversification and bandit-based exploration techniques by introducing a deployable, behaviorally informed mechanism for surfacing exploratory content at scale. Moreover, we demonstrate that the collected unbiased data, integrated into downstream candidate generation, significantly improves user engagement, validating its value for recommender systems.

</details>


### [12] [Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models](https://arxiv.org/abs/2512.15372)
*Mikel Williams-Lekuona,Georgina Cosma*

Main category: cs.IR

TL;DR: ICAR提出了一种图像复杂度感知检索方法，让视觉Transformer根据图像复杂度动态调整计算量，简单图像少计算，复杂图像全计算，同时保持跨模态对齐，实现20%加速且性能损失很小。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型中的视觉Transformer对所有图像都使用相同的计算量（175.33 GFLOPs），无论图像简单还是复杂，这造成了计算资源的浪费。需要一种能根据图像复杂度自适应调整计算量的方法。

Method: 1. ICAR框架：通过双路径训练，使不同计算深度产生的图像嵌入都能与文本嵌入保持兼容；2. ConvNeXt-IC：将图像复杂度评估作为分类任务，使用现代分类器骨干网络，实现高效复杂度评估；3. 直接图像-文本匹配，无需昂贵的重排序。

Result: ConvNeXt-IC在图像复杂度评估上达到SOTA性能（0.959 Pearson相关性，4.4倍加速）。ICAR在标准基准测试和真实网络数据上实现20%实际加速，保持类别级性能，达到实例级性能的95%。

Conclusion: ICAR通过图像复杂度感知的动态计算分配，在保持跨模态对齐的前提下显著减少计算开销，为实现可持续扩展的视觉-语言系统提供了有效方案。

Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.

</details>


### [13] [MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical Documents](https://arxiv.org/abs/2512.15384)
*Gregor Donabauer,Samy Ateia,Udo Kruschwitz,Maximilian Burger,Matthias May,Christian Gilfrich,Maximilian Haas,Julio Ruben Rodas Garzaro,Christoph Eckl*

Main category: cs.IR

TL;DR: MedNuggetizer是一个基于大语言模型的工具，用于从医学文档中提取和聚类信息块，帮助临床医生探索医学证据


<details>
  <summary>Details</summary>
Motivation: 临床医生需要从大量医学文档中快速提取可靠证据，但现有方法效率低下，难以处理长文档和跨文档信息整合

Method: 基于大语言模型重复提取信息块，然后将这些信息块进行分组，实现文档内和跨文档的可靠证据生成

Result: 在"前列腺活检前抗生素预防"临床用例中，使用主要泌尿科指南和PubMed研究作为信息源，领域专家评估显示工具能有效帮助临床医生探索长文档并提取可靠证据

Conclusion: MedNuggetizer为临床医生和研究人员提供了一种高效探索长文档并轻松提取可靠、查询聚焦的医学证据的方法

Abstract: We present MedNuggetizer, https://mednugget-ai.de/; access is available upon request.}, a tool for query-driven extraction and clustering of information nuggets from medical documents to support clinicians in exploring underlying medical evidence. Backed by a large language model (LLM), \textit{MedNuggetizer} performs repeated extractions of information nuggets that are then grouped to generate reliable evidence within and across multiple documents. We demonstrate its utility on the clinical use case of \textit{antibiotic prophylaxis before prostate biopsy} by using major urological guidelines and recent PubMed studies as sources of information. Evaluation by domain experts shows that \textit{MedNuggetizer} provides clinicians and researchers with an efficient way to explore long documents and easily extract reliable, query-focused medical evidence.

</details>


### [14] [BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems](https://arxiv.org/abs/2512.15526)
*Abdullah Al Munem,Sumona Yeasmin,Mohammad Rezwanul Huq*

Main category: cs.IR

TL;DR: 提出结合BERT和CNN的神经协同过滤模型，能处理数值、分类和图像数据，在MovieLens数据集上表现优于传统NCF和BERT-NCF模型。


<details>
  <summary>Details</summary>
Motivation: 网站通过用户与内容互动获利，强大的推荐系统能根据用户偏好推荐项目，增加用户互动和网站收益。

Method: 提出BERT和CNN集成的神经协同过滤模型，从用户和项目配置文件中提取潜在特征，能处理数值、分类和图像数据。

Result: 在MovieLens数据集上，提出的模型在25个epochs训练后，召回率达到0.72，Hit Ratio@10为0.486，优于简单的NCF和BERT-NCF基线模型。

Conclusion: 同时考虑分类和图像数据可以提升推荐系统的性能，多模态特征提取对推荐效果有积极影响。

Abstract: Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [15] [Low-Complexity Channel Estimation for Internet of Vehicles AFDM Communications With Sparse Bayesian Learning](https://arxiv.org/abs/2512.14776)
*Xiangxiang Li,Haiyan Wang,Yao Ge,Xiaohong Shen,Miaowen Wen,Shun Zhang,Yong Liang Guan*

Main category: cs.IT

TL;DR: 提出基于稀疏贝叶斯学习的AFDM系统信道估计框架，包含网格细化SBL和网格演化SBL两种离网格估计方法，并开发分布式计算方案降低复杂度。


<details>
  <summary>Details</summary>
Motivation: AFDM（仿射频分复用）是车联网中实现高可靠连接的有前景波形，但在双弥散信道中，准确的信道估计对于实现AFDM系统预期性能至关重要且具有挑战性。

Method: 提出稀疏贝叶斯学习框架，开发两种离网格信道估计方法：网格细化SBL（GR-SBL）采用局部网格细化方法动态更新网格；网格演化SBL（GE-SBL）通过一阶线性近似逼近离网格分量并实现渐进网格演化。同时开发分布式计算方案，将大维信道估计模型分解为多个可管理的小维子模型。

Result: 仿真结果表明，所提信道估计器优于现有竞争方案。GR-SBL以高复杂度为代价实现高精度估计，GE-SBL在性能和复杂度之间提供更好权衡。分布式D-GR-SBL和D-GE-SBL有效降低复杂度，同时保持与GR-SBL和GE-SBL相当的性能。

Conclusion: 提出的SBL框架和分布式计算方案为AFDM系统在双弥散信道中提供了高效、高精度的信道估计解决方案，平衡了性能与计算复杂度。

Abstract: Affine frequency division multiplexing (AFDM) has been considered as a promising waveform to enable high-reliable connectivity in the internet of vehicles. However, accurate channel estimation is critical and challenging to achieve the expected performance of the AFDM systems in doubly-dispersive channels. In this paper, we propose a sparse Bayesian learning (SBL) framework for AFDM systems and develop a dynamic grid update strategy with two off-grid channel estimation methods, i.e., grid-refinement SBL (GR-SBL) and grid-evolution SBL (GE-SBL) estimators. Specifically, the GR-SBL employs a localized grid refinement method and dynamically updates grid for a high-precision estimation. The GE-SBL estimator approximates the off-grid components via first-order linear approximation and enables gradual grid evolution for estimation accuracy enhancement. Furthermore, we develop a distributed computing scheme to decompose the large-dimensional channel estimation model into multiple manageable small-dimensional sub-models for complexity reduction of GR-SBL and GE-SBL, denoted as distributed GR-SBL (D-GR-SBL) and distributed GE-SBL (D-GE-SBL) estimators, which also support parallel processing to reduce the computational latency. Finally, simulation results demonstrate that the proposed channel estimators outperform existing competitive schemes. The GR-SBL estimator achieves high-precision estimation with fine step sizes at the cost of high complexity, while the GE-SBL estimator provides a better trade-off between performance and complexity. The proposed D-GR-SBL and D-GE-SBL estimators effectively reduce complexity and maintain comparable performance to GR-SBL and GE-SBL estimators, respectively.

</details>


### [16] [On the Stochastic Analysis of Random Linear Streaming Codes in Multi-Hop Relay Networks](https://arxiv.org/abs/2512.15049)
*Kai Huang,Xinyu Xie,Chunpeng Chen,Wenjie Guan,Xiaoran Wang,Jinbei Zhang*

Main category: cs.IT

TL;DR: 本文分析了多跳中继网络中大规模随机线性流码的随机性能极限，通过量化网络中每个节点的信息流，推导了多跳网络中的错误概率表达式。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注确定性对抗信道下的最优容量实现编码，而本文聚焦于随机信道（每跳独立同分布包擦除），对多跳随机线性流码进行随机性能分析。

Method: 1) 在两跳中继网络中，通过量化每个节点的信息流，将"信息债务"概念扩展到两跳网络；2) 基于错误事件，通过构造"带状"嵌套结构的转移矩阵，推导平均错误概率表达式；3) 将两跳结果推广到任意跳数的中继网络。

Result: 建立了多跳随机线性流码的随机分析框架，推导了错误概率的精确表达式，并通过仿真验证了分析准确性，与现有对抗信道流码进行了比较。

Conclusion: 本文成功将点对点网络的"信息债务"概念扩展到多跳中继网络，为随机信道下的流码性能分析提供了系统框架，填补了随机性能分析的研究空白。

Abstract: In this paper, we aim to explore the stochastic performance limit of large-field-size Random Linear Streaming Codes (RLSCs) in multi-hop relay networks. In our model, a source transmits a sequence of streaming messages to a destination through multiple relays subject to a delay constraint. Most previous research focused on deterministic adversarial channel which introduces only restricted types of erasure patterns, and aimed to design the optimal capacity-achieving codes. In this paper, we focus on stochastic channel where each hop is subject to i.i.d. packet erasures, and carry out stochastic analysis on the error probability of multi-hop RLSCs. Our contributions are three-folds. Firstly, the error event of large-field-size RLSCs is characterized in two-hop relay network with a novel framework, which features quantification of information flowing through each node in the network. Due to the erasures in different hops, some source symbols can be "detained" at the source or relay while others have arrived at the destination. By iteratively computing the number of detained symbols at each node, this framework extends the concept "information debt" from point-to-point network [Pinwen Su et al. 2022] into two-hop relay networks. Secondly, based on the error event, the expression of average error probability in two-hop network is derived by carefully analyzing the expectation terms. To handle the expectation over all possible erasure patterns along two hops of the network, the transition matrices of the detained symbols are novelly constructed in a "band fashion" with nested structure. Thirdly, the derived results in two-hop network are further generalized into relay networks with arbitrary number of hops. Furthermore, simulations are conducted to verify the accuracy of our stochastic analysis, and compare with some existing streaming codes for the adversarial channels.

</details>


### [17] [Rotatable IRS-Assisted 6DMA Communications: A Two-timescale Design](https://arxiv.org/abs/2512.15092)
*Chao Zhou,Changsheng You,Cong Zhou,Liujia Yao,Weijie Yuan,Beixiong Zheng,Nan Wu*

Main category: cs.IT

TL;DR: 提出了一种结合可旋转智能反射面(R-IRS)和六维可移动天线(6DMA)的多功能天线/表面系统，采用双时间尺度传输协议，通过优化天线配置和波束成形来最大化平均和速率。


<details>
  <summary>Details</summary>
Motivation: 智能反射面(IRS)和可移动天线(MA)技术在实际应用中存在性能限制，需要结合两者的互补优势来增强无线通信性能。

Method: 部署可旋转IRS(R-IRS)与配备6DMA的基站协同工作，采用双时间尺度传输协议：长时尺度基于统计CSI优化天线配置和IRS旋转反射，短时尺度基于瞬时CSI设计波束成形。针对多用户非凸优化问题，提出了结合WMMSE和SSCA的高效算法。

Result: 单用户情况下，6DMA应形成稀疏阵列进行多波束传输，IRS旋转实现多径对齐。多用户情况下，所提算法显著提升了系统性能，验证了联合利用6DMA-BS和R-IRS空间自由度的有效性。

Conclusion: 提出的多功能天线/表面系统通过结合6DMA和R-IRS的互补优势，在双时间尺度协议下实现了显著的性能增益，为未来无线通信系统设计提供了新思路。

Abstract: Intelligent reflecting surface (IRS) and movable antenna (MA) are promising technologies to enhance wireless communication by reconfiguring channels at the environment and transceiver sides. However, their performance is constrained by practical limitations. To address this, we propose a multi-functional antenna/surface system that leverages their complementary advantages. A rotatable IRS (R-IRS) is deployed to enhance downlink communications from a six-dimensional MA (6DMA)-equipped base station (BS) to multiple single-antenna users. To reduce the complexity of real-time channel estimation and beamforming, we formulate an optimization problem to maximize the average sum-rate using a two-timescale (TTS) transmission protocol. Specifically, the BS antenna configuration (including position and rotation) and IRS rotation and reflection are optimized based on statistical channel state information (S-CSI), while BS transmit beamforming is designed using instantaneous CSI (I-CSI) in the short timescale. We first consider a single-user case and show that the 6DMA at the BS should form a sparse array for multi-beam transmission towards both the IRS and the user, allowing efficient coordination of direct and reflected channels, while the IRS rotation achieves effective multi-path alignment. For the general multi-user case, the optimization problem is non-convex and challenging to solve. To tackle this, we propose an efficient algorithm combining weighted minimum mean-square error (WMMSE) and stochastic successive convex approximation (SSCA) techniques. A low-complexity algorithm is also proposed to reduce computational complexity. Numerical results validate the proposed system, showing significant performance gains by jointly exploiting the spatial degrees of freedom of the 6DMA-BS and R-IRS under the TTS protocol.

</details>


### [18] [Sparse Principal Component Analysis with Energy Profile Dependent Sample Complexity](https://arxiv.org/abs/2512.15191)
*Mengchu Xu,Jian Wang,Yonina C. Eldar*

Main category: cs.IT

TL;DR: 提出SEP算法用于稀疏主成分分析，能自适应能量分布，样本复杂度优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有稀疏PCA方法主要针对能量均匀分布的"平坦尖峰"情况，对能量分布不均匀的情况缺乏有效指导

Method: 提出谱能量追踪(SEP)迭代方案，通过重复筛选和重选坐标，使用结构函数s(p)量化尖峰能量在top p个条目上的累积

Result: SEP样本复杂度为max_{1≤p≤k} p·s²(p)·log n，匹配平坦尖峰的k²log n，在能量集中时接近klog n；截断幂迭代使估计器达到一致统计误差界

Conclusion: SEP能自适应能量分布结构无需调参，在平坦、幂律和指数信号上优于现有算法

Abstract: We study sparse principal component analysis in the high-dimensional, sample-limited regime, aiming to recover a leading component supported on a few coordinates. Despite extensive progress, most methods and analyses are tailored to the flat-spike case, offering little guidance when spike energy is unevenly distributed across the support. Motivated by this, we propose Spectral Energy Pursuit (SEP), an effective iterative scheme that repeatedly screens and reselects coordinates, with a sample complexity that adapts to the energy profile. We develop our framework around a structure function \(s(p)\) that quantifies how spike energy accumulates over its top \(p\) entries. We establish that SEP succeeds with a sample size of order \(\max_{1\le p\le k} p\,s^2(p)\,\log n\), which matches the classical \(k^2\log n\) sample complexity for flat spikes and improves toward the \(k\log n\) regime as the profile becomes more concentrated. As a lightweight post-processing, a single truncated power iteration is proven to enable the final estimator to attain a uniform statistical error bound. Empirical simulations across flat, power-law, and exponential signals validate that SEP adapts to profile structure without tuning and outperforms existing algorithms.

</details>


### [19] [Symbol Detection in Ambient Backscatter Communications Under Residual Time Synchronization Errors](https://arxiv.org/abs/2512.15241)
*Yinghui Ye,Ying Li,Xiaoli Chu,Gan Zheng,Sumei Sun*

Main category: cs.IT

TL;DR: 提出了一种考虑残余时间同步误差的AmBC符号检测框架，推导了BER表达式并优化了检测阈值


<details>
  <summary>Details</summary>
Motivation: 实际AmBC系统中存在残余时间同步误差，导致部分样本失配，降低符号检测性能，而现有研究假设完美时间同步不现实

Method: 提出新的AmBC符号检测框架，考虑BT当前和相邻符号以及信道系数；以能量检测器为例，推导精确和近似BER表达式；推导闭式近最优检测阈值；提出利用BR接收信号样本属性的参数估计方法

Result: 残余时间同步误差显著降低能量检测器的BER性能；优化后的检测阈值能最小化BER；仿真验证了分析结果

Conclusion: 提出的框架能有效处理残余时间同步误差，通过优化检测阈值和参数估计方法，显著提升AmBC系统的符号检测性能

Abstract: Ambient backscatter communications (AmBC), where a backscatter transmitter (BT) modulates and reflects ambient signals to a backscatter receiver (BR), have been deemed a low-power communication technology for the Internet of Things. Previous work on symbol detection in AmBC assumed perfect time synchronization (TS), which is unrealistic in practice. The residual TS errors (RTSE) cause \emph{partial sample mismatch}, degrading symbol detection performance. To address this, we propose a new AmBC symbol detection framework that incorporates the BT's current and adjacent symbols, as well as channel coefficients. Using energy detector (ED) as a case study, we derive both exact and approximate bit error rate (BER) expressions. Our results show that the ED's BER performance degrades significantly under RTSE, with the symbol detection threshold optimized under the assumption of perfect TS. We then derive a closed-form expression for a near-optimal symbol detection threshold that minimizes BER under RTSE. To estimate the required parameters for the detection threshold, we propose a novel method exploiting the attributes of the BR's received signal samples. The analytical results are verified by simulation results.

</details>


### [20] [Joint Activity Detection and Channel Estimation For Fluid Antenna System Exploiting Geographical and Angular Information](https://arxiv.org/abs/2512.15342)
*Zhentian Zhang,Jian Dang,David Morales-Jimenez,Hao Jiang,Zaichen Zhang,Christos Masouros,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 提出基于近似消息传递(AMP)与自适应期望最大化(EM)结合的EM-AMP框架，用于流体天线系统(FAS)的信道估计，解决现有方法在灵活性、可行性和复杂度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统(FAS)为未来通信网络提供物理层自由度，但信道状态信息(CSI)获取存在严重缺陷：贪婪算法性能受信号假设影响大，无模型方法计算复杂度过高，需要平衡灵活性、可行性和低复杂度的解决方案来支持大规模连接。

Method: 提出EM-AMP框架，将近似消息传递(AMP)与自适应期望最大化(EM)结合，实现高效的大矩阵计算和自适应学习能力，不依赖模型或参数先验知识。引入两种变体：利用FAS网络中地理特征和角度特征的算法。

Result: 提出的算法在大型活动区域表现出改进的估计精度、快速收敛和低计算复杂度。数值结果验证了算法设计的有效性，并分析了贪婪方法固有性能平台的原因，强调了角度信息在算法设计中的关键作用。

Conclusion: EM-AMP框架为FAS网络提供了鲁棒的信道估计解决方案，平衡了灵活性、可行性和复杂度需求，支持大规模连接，是FAS网络的有力候选方案。

Abstract: The fluid antenna system (FAS) refers to a family of reconfigurable antenna technologies that provide substantial spatial gains within a compact, predefined small space, thereby offering extensive degrees of freedom in the physical layer for future communication networks. The acquisition of channel state information (CSI) is critical, as it determines the placement of ports/antennas, which directly impacts FAS-based optimization. Although various channel estimation methods have been developed, significant flaws persist. For instance, the performance of greedy-based algorithms is heavily influenced by signal assumptions, and current model-free methods are infeasible due to prohibitively high computational complexity issue. Consequently, there is a pressing need for a well-balanced solution that exhibits flexibility, feasibility, and low complexity to support massive connectivity in FAS. In this work, we propose methods based on approximate message passing (AMP) integrated with adaptive expectation maximization (EM). The EM-AMP framework uniquely enables efficient large matrix computations with adaptive learning capabilities, independent of prior knowledge of the model or parameters within potential distributions, making it a robust candidate for FAS networks. We introduce two variants of the EM-AMP framework that leverage geographical and angular features in a FAS network. These proposed algorithms demonstrate improved estimation precision, fast convergence, and low computational complexity in large activity regions. Additionally, we analytically elucidate the reasons behind the inherent performance floor of greedy-based methods and highlight the critical role of angular information in algorithm design. Extensive numerical results validate the promising efficacy of the proposed algorithm designs and the derived analytical findings.

</details>


### [21] [Three-Dimensional Radio Localization: A Channel Charting-Based Approach](https://arxiv.org/abs/2512.15399)
*Phillip Stephan,Florian Euchner,Stephan ten Brink*

Main category: cs.IT

TL;DR: 该论文研究三维室内定位场景下的信道图表技术，提出针对三维工厂环境和多层建筑的两种解决方案，并引入新的稀疏特征提取方法。


<details>
  <summary>Details</summary>
Motivation: 现有信道图表研究主要关注二维场景，而现实世界环境本质上是三维的。需要解决三维室内定位问题，特别是三维空间分布和多层建筑这两种典型场景。

Method: 1. 针对三维工厂场景：应用增强信道图表概念，将经典定位与信道图表结合到三维设置中。2. 针对多层建筑：提出多层信道图表方法，包括通过聚类进行楼层分类，然后为每个楼层训练专门的专家神经网络进行信道图表。3. 提出新的特征工程方法，从波束空间信道状态信息中提取适合定位的稀疏特征。

Result: 论文使用基于射线追踪的模拟但真实的数据集验证了两种三维室内定位场景的有效性。增强信道图表适用于三维空间分布场景，多层信道图表方法提高了多层建筑中的信道图表性能。

Conclusion: 信道图表技术可以扩展到三维室内定位场景。针对不同三维场景需要采用不同的方法：三维空间分布场景适合增强信道图表，多层建筑场景适合多层信道图表方法。提出的稀疏特征提取方法有助于提高定位性能。

Abstract: Channel charting creates a low-dimensional representation of the radio environment in a self-supervised manner using manifold learning. Preserving relative spatial distances in the latent space, channel charting is well suited to support user localization. While prior work on channel charting has mainly focused on two-dimensional scenarios, real-world environments are inherently three-dimensional. In this work, we investigate two distinct three-dimensional indoor localization scenarios using simulated, but realistic ray tracing-based datasets: a factory hall with a three-dimensional spatial distribution of datapoints, and a multistory building where each floor exhibits a two-dimensional datapoint distribution. For the first scenario, we apply the concept of augmented channel charting, which combines classical localization and channel charting, to a three-dimensional setting. For the second scenario, we introduce multistory channel charting, a two-stage approach consisting of floor classification via clustering followed by the training of a dedicated expert neural network for channel charting on each individual floor, thereby enhancing the channel charting performance. In addition, we propose a novel feature engineering method designed to extract sparse features from the beamspace channel state information that are suitable for localization.

</details>


### [22] [Variational Robust Kalman Filters: A Unified Framework](https://arxiv.org/abs/2512.15419)
*Shilei Li,Dawei Shi,Hao Yu,Ling Shi*

Main category: cs.IT

TL;DR: 提出一种基于学生t分布和变分推理的统一变分鲁棒卡尔曼滤波器，通过切换规则将鲁棒性和自适应性融合，能处理复杂噪声环境。


<details>
  <summary>Details</summary>
Motivation: 卡尔曼滤波中鲁棒性和自适应性是两个相互竞争的目标。鲁棒性需要临时放大噪声协方差估计，而自适应性则利用实时信息更新先验信念。实际应用中，过程和测量噪声都可能受到异常值影响、时变或两者兼有。现有方法难以有效处理这些复杂噪声场景，因为鲁棒滤波器和自适应滤波器之间存在内在不兼容性。

Method: 提出统一的变分鲁棒卡尔曼滤波器，基于学生t分布诱导的损失函数和变分推理，通过固定点迭代以计算高效的方式求解。将鲁棒性理解为自适应性的前提条件，通过切换规则将两个竞争目标融合到单一框架中。

Result: 所提出的滤波器可以通过调整参数恢复传统卡尔曼滤波器、鲁棒卡尔曼滤波器和自适应卡尔曼滤波器，能够同时抑制不完美的过程和测量噪声，在复杂噪声环境中表现优异。仿真验证了该方法的有效性。

Conclusion: 通过将鲁棒性作为自适应性的前提条件，成功将两个相互竞争的目标融合到统一的变分鲁棒卡尔曼滤波框架中，该框架能够有效处理实际应用中复杂的噪声场景，包括异常值和时变噪声。

Abstract: Robustness and adaptivity are two competing objectives in Kalman filters (KF). Robustness involves temporarily inflating prior estimates of noise covariances, while adaptivity updates prior beliefs using real-time information. In practical applications, both process and measurement noise can be influenced by outliers, be time-varying, or both. Existing works may not effectively address the above complex noise scenarios, as there is an intrinsic incompatibility between robust filters and adaptive filters. In this work, we propose a unified variational robust Kalman filter, built on a Student's t-distribution induced loss function and variational inference, and solved through fixed-point iteration in a computationally efficient manner. We demonstrate that robustness can be understood as a prerequisite for adaptivity, making it possible to merge the above two competing goals into a single framework through switching rules. Additionally, our proposed filter can recover conventional KF, robust KF, and adaptive KF by adjusting parameters, and can suppress both the imperfect process and measurement noise, enabling it to perform superiorly in complex noise environments. Simulations verify the effectiveness of the proposed method.

</details>


### [23] [An Anti-Interference AFDM System: Interference Impacts Analyses and Parameter Optimization](https://arxiv.org/abs/2512.15425)
*Peng Yuan,Zulin Wang,Tao Luo,Yuanhan Ni*

Main category: cs.IT

TL;DR: 提出抗干扰仿射频分复用系统，在高移动性场景下对抗恶意高功率干扰，通过参数优化算法最大化包吞吐量，并设计线性复杂度相关检测器实现全分集增益。


<details>
  <summary>Details</summary>
Motivation: 在高移动性场景中，对抗性设备产生的恶意高功率干扰会严重影响通信系统的可靠性和资源效率，需要设计有效的抗干扰方案。

Method: 1) 推导离散仿射傅里叶变换域中干扰的闭式表达式；2) 基于扩频和纠错编码参数设计包吞吐量优化算法；3) 设计线性复杂度相关检测器，利用扩频序列自相关函数和AFDM输入输出关系的循环移位特性。

Result: 数值结果验证了闭式表达式的准确性，证明所提出的抗干扰AFDM系统在高移动性干扰场景下能够实现高包吞吐量。

Conclusion: 该抗干扰AFDM系统通过参数优化和相关检测器设计，有效提升了高移动性场景下对抗恶意干扰的通信可靠性和资源效率。

Abstract: This paper proposes an anti-interference affine frequency division multiplexing (AFDM) system to ensure reliability and resource efficiency under malicious high-power interference originating from adversarial devices in high-mobility scenarios. Closed-form expressions of interferences in the discrete affine Fourier transform (DAFT) domain are derived by utilizing the stationary phase principle and the Affine Fourier transform convolution theorem, which indicates that interference impacts can be classified into stationary and non-stationary categories. On this basis, we reveal the analytical relationship between packet throughput and the paramerters of spread spectrum and error correction coding in our proposed anti-interference system, which enables the design of a parameter optimization algorithm that maximizes packet throughput. For reception, by jointly utilizing the autocorrelation function of spreading sequence and the cyclic-shift property of AFDM input-output relation, we design a linear-complexity correlation-based DAFT domain detector (CDD) capable of achieving full diversity gain, which performs correlation-based equalization to avoid matrix inversion. Numerical results validate the accuracy of the derived closed-form expressions and verify that the proposed anti-interference AFDM system could achieve high packet throughput under interference in high-mobility scenarios.

</details>


### [24] [Reducing Pilots in Channel Estimation With Predictive Foundation Models](https://arxiv.org/abs/2512.15562)
*Xingyu Zhou,Le Liang,Hao Ye,Jing Zhang,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于预测基础模型的信道估计框架，通过大规模跨域CSI数据训练提取通用信道表示，结合视觉Transformer处理导频观测，实现准确、低开销、可泛化的CSI获取。


<details>
  <summary>Details</summary>
Motivation: 现代无线系统在大规模天线阵列、严格导频开销约束和多样化部署环境下，准确获取信道状态信息(CSI)变得越来越困难。现有基于人工智能的解决方案往往缺乏鲁棒性，无法跨场景泛化。

Method: 提出预测基础模型框架：1) 在大规模跨域CSI数据上训练预测基础模型，提取通用信道表示并提供具有强跨场景可迁移性的预测先验；2) 基于视觉Transformer架构设计导频处理网络，从导频观测中捕获空间、时间和频率相关性；3) 设计高效融合机制，将预测先验与实时测量值集成，即使在稀疏或噪声条件下也能实现可靠的CSI重建。

Result: 在多样化配置下的广泛评估表明，所提出的估计器在准确性、鲁棒性和泛化能力方面显著优于经典方法和数据驱动基线方法。

Conclusion: 提出的预测基础模型框架能够实现准确、低开销、可泛化的CSI获取，解决了现有AI方法在鲁棒性和跨场景泛化方面的局限性。

Abstract: Accurate channel state information (CSI) acquisition is essential for modern wireless systems, which becomes increasingly difficult under large antenna arrays, strict pilot overhead constraints, and diverse deployment environments. Existing artificial intelligence-based solutions often lack robustness and fail to generalize across scenarios. To address this limitation, this paper introduces a predictive-foundation-model-based channel estimation framework that enables accurate, low-overhead, and generalizable CSI acquisition. The proposed framework employs a predictive foundation model trained on large-scale cross-domain CSI data to extract universal channel representations and provide predictive priors with strong cross-scenario transferability. A pilot processing network based on a vision transformer architecture is further designed to capture spatial, temporal, and frequency correlations from pilot observations. An efficient fusion mechanism integrates predictive priors with real-time measurements, enabling reliable CSI reconstruction even under sparse or noisy conditions. Extensive evaluations across diverse configurations demonstrate that the proposed estimator significantly outperforms both classical and data-driven baselines in accuracy, robustness, and generalization capability.

</details>
