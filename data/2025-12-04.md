<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.IT](#cs.IT) [Total: 10]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Sponsored Questions and How to Auction Them](https://arxiv.org/abs/2512.03975)
*Kshipra Bhawalkar,Alexandros Psomas,Di Wang*

Main category: cs.GT

TL;DR: 论文研究在线平台中LLM澄清性提示的赞助机制设计，比较端到端联合优化与模块化分离两种方案，证明VCG机制可实现高效真实，而模块化方法存在无界策略性效率损失。


<details>
  <summary>Details</summary>
Motivation: 传统搜索平台面对用户查询意图模糊时，通常被动预测相关性或提供查询优化。随着对话式AI的发展，LLM可以主动提供澄清性提示，其中部分提示可被"赞助"用于广告目的。需要研究如何分配这些"建议槽位"，以及新机制与传统广告拍卖的交互关系。

Method: 建立形式化模型设计分析交互式平台，比较两种工程选择：1）端到端管道联合优化用户交互和最终广告拍卖；2）将建议槽位和后续广告槽位解耦为独立机制。采用VCG机制进行联合优化，分析模块化方法的策略效率问题。

Result: VCG机制虽然更复杂，但能实现高效和真实的结果。而简单易实现的模块化方法存在策略性效率损失，其无政府状态价格是无界的。

Conclusion: 在LLM驱动的交互式平台中，端到端联合优化机制（如VCG）优于模块化分离方法，尽管实施更复杂，但能保证效率和真实性，避免策略性效率损失。

Abstract: Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow?
  This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [2] [On the Complexity of the Ordered Covering Problem in Distance Geometry](https://arxiv.org/abs/2512.03124)
*Michael Souza,Júlio Araújo,John Kesley Costa,Carlile Lavor*

Main category: cs.DS

TL;DR: 本文证明了有序覆盖问题（OCP）是NP完全问题，通过从强NP完全的3-划分问题进行多项式时间归约，为蛋白质结构确定中的边排序问题建立了计算复杂性理论边界。


<details>
  <summary>Details</summary>
Motivation: 在可离散化分子距离几何问题（DMDGP）中，剪枝边的排序对SBBU算法的性能有重要影响。虽然已有启发式方法在实践中表现良好，但寻找最优解的计算复杂性一直未解决，需要理论分析。

Method: 通过从强NP完全的3-划分问题到有序覆盖问题的多项式时间归约来证明NP完全性。归约构造了严格的预算约束，使得最优解必须对应有效的3-划分。

Result: 成功证明了有序覆盖问题是NP完全问题，为最优边排序建立了计算复杂性理论边界，为实践中使用的启发式方法提供了理论依据。

Conclusion: OCP的NP完全性结果解释了为什么在实践中需要依赖启发式方法而非寻求最优解，为蛋白质结构确定中的边排序问题提供了重要的理论见解。

Abstract: The Ordered Covering Problem (OCP) arises in the context of the Discretizable Molecular Distance Geometry Problem (DMDGP), where the ordering of pruning edges significantly impacts the performance of the SBBU algorithm for protein structure determination. In recent work, Souza et al. (2023) formalized OCP as a hypergraph covering problem with ordered, exponential costs, and proposed a greedy heuristic that outperforms the original SBBU ordering by orders of magnitude. However, the computational complexity of finding optimal solutions remained open. In this paper, we prove that OCP is NP-complete through a polynomial-time reduction from the strongly NP-complete 3-Partition problem. Our reduction constructs a tight budget that forces optimal solutions to correspond exactly to valid 3-partitions. This result establishes a computational barrier for optimal edge ordering and provides theoretical justification for the heuristic approaches currently used in practice.

</details>


### [3] [Complexity of Local Search for CSPs Parameterized by Constraint Difference](https://arxiv.org/abs/2512.03275)
*Aditya Anand,Vincent Cohen-Addad,Tommaso d'Orsi,Anupam Gupta,Euiwoong Lee,Debmalya Panigrahi,Sijin Peng*

Main category: cs.DS

TL;DR: 本文研究了局部搜索的参数化复杂度，提出了一个通用模型：给定当前解P和最优解S*，目标是找到与S*一样好的可行解，参数k = |PΔS*|。该模型推广了经典参数化优化问题，并应用于布尔对称CSPs，给出了完整的参数化复杂度分类。


<details>
  <summary>Details</summary>
Motivation: 局部搜索是解决优化问题的常用启发式方法，但缺乏系统的参数化复杂度分析。现有参数化问题通常关注从全集U中删除最少元素使其可行的情况（即P=U），本文旨在建立一个更通用的局部搜索参数化模型，涵盖更广泛的实际场景。

Method: 提出一个通用参数化局部搜索模型：输入包括当前解P和问题实例，参数k定义为当前解P与最优解S*的对称差大小|PΔS*|。将该模型应用于布尔对称约束满足问题（CSPs），其中U是约束集合，可行子集U'存在满足所有约束的变量赋值。通过系统分析布尔对称CSPs的谓词特性，建立完整的参数化复杂度分类。

Result: 建立了布尔对称CSPs在局部搜索参数化模型下的完整复杂度分类：确定了哪些问题在参数化时间内可解（FPT），哪些问题是W[1]-难的。该分类揭示了局部搜索参数化复杂度的内在结构，为理解优化问题的局部搜索行为提供了理论框架。

Conclusion: 本文提出的参数化局部搜索模型成功推广了经典参数化优化问题，并在布尔对称CSPs上获得了完整的复杂度分类。该工作为局部搜索算法的参数化分析提供了系统框架，有助于理解优化问题在局部搜索下的计算复杂性。

Abstract: In this paper, we study the parameterized complexity of local search, whose goal is to find a good nearby solution from the given current solution. Formally, given an optimization problem where the goal is to find the largest feasible subset $S$ of a universe $U$, the new input consists of a current solution $P$ (not necessarily feasible) as well as an ordinary input for the problem.
  Given the existence of a feasible solution $S^*$, the goal is to find a feasible solution as good as $S^*$ in parameterized time $f(k) \cdot n^{O(1)}$, where $k$ denotes the distance $|PΔS^*|$. This model generalizes numerous classical parameterized optimization problems whose parameter $k$ is the minimum number of elements removed from $U$ to make it feasible, which corresponds to the case $P = U$.
  We apply this model to widely studied Constraint Satisfaction Problems (CSPs), where $U$ is the set of constraints, and a subset $U'$ of constraints is feasible if there is an assignment to the variables satisfying all constraints in $U'$. We give a complete characterization of the parameterized complexity of all boolean-alphabet symmetric CSPs, where the predicate's acceptance depends on the number of true literals.

</details>


### [4] [Fast approximate $\ell$-center clustering in high dimensional spaces](https://arxiv.org/abs/2512.03304)
*Mirosław Kowaluk,Andrzej Lingas,Mia Persson*

Main category: cs.DS

TL;DR: 本文提出了一种基于随机降维的高效近似算法，用于高维欧几里得空间和汉明空间中的ℓ-center聚类和最小直径ℓ-聚类问题，显著提升了现有算法的运行速度。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中，ℓ-center聚类和最小直径ℓ-聚类问题的现有算法运行时间严重依赖于维度大小，当ℓ和维度都超对数增长时，算法效率低下。需要设计更高效的近似算法来解决这一瓶颈。

Method: 主要工具是随机降维技术。首先提出了一种通用方法，用于减少高维欧几里得空间中ℓ-center问题算法运行时间对维度大小的依赖。利用该方法，为欧几里得和汉明空间中的ℓ-center聚类和最小直径ℓ-聚类问题设计了(2+ε)-近似算法。还将该方法应用于最近具有更高近似保证的快速近似算法，并对包含z个异常值的ℓ-center聚类问题的O(1)-近似方法进行了加速。

Result: 提出的(2+ε)-近似算法在ℓ和维度都超对数增长时，比已知的2-近似算法显著更快。通用方法成功应用于加速现有高维欧几里得空间ℓ-center聚类问题的近似算法，并对包含异常值的聚类问题的O(1)-近似方法实现了加速。

Conclusion: 随机降维是解决高维聚类问题计算效率瓶颈的有效工具。提出的方法显著提升了ℓ-center聚类和最小直径ℓ-聚类问题在高维空间中的算法效率，为大规模高维数据聚类提供了实用的解决方案。

Abstract: We study the design of efficient approximation algorithms for the
  $\ell$-center clustering and minimum-diameter $\ell$-clustering
  problems in high dimensional Euclidean and Hamming spaces. Our main
  tool is randomized dimension reduction. First, we present a general
  method of reducing the dependency of the running time of a
  hypothetical algorithm for the $\ell$-center problem in a high
  dimensional Euclidean space on the dimension size. Utilizing in
  part this method, we provide $(2+ε)$- approximation
  algorithms for the $\ell$-center clustering and minimum-diameter
  $\ell$-clustering problems in Euclidean and Hamming spaces that are
  substantially faster than the known $2$-approximation ones when both
  $\ell$ and the dimension are super-logarithmic. Next, we apply the
  general method to the recent fast approximation algorithms with
  higher approximation guarantees for the $\ell$-center clustering
  problem in a high dimensional Euclidean space. Finally, we provide a
  speed-up of the known $O(1)$-approximation method for the
  generalization of the $\ell$-center clustering problem to include
  $z$ outliers (i.e., $z$ input points can be ignored while computing
  the maximum distance of an input point to a center) in high
  dimensional Euclidean and Hamming spaces.

</details>


### [5] [Singing a MIS](https://arxiv.org/abs/2512.03311)
*Sandy Irani,Michael Luby*

Main category: cs.DS

TL;DR: 提出唱歌模型作为蜂鸣模型的泛化，允许代理唱多个音符，并给出在动态网络中O(log(n))时间内收敛到MIS的协议


<details>
  <summary>Details</summary>
Motivation: 现有蜂鸣模型只能发射单一频率，限制了通信能力。需要一种更强大的广播模型，使代理能在不知道网络拓扑的情况下进行更丰富的通信，特别是在动态和异步环境中。

Method: 提出唱歌模型，代理可以唱多个音符（反映其强度）。设计简单自然的协议，代理与邻居竞争，通过唱歌音符数量体现强度。协议在异步、动态网络中工作，支持故障代理。

Result: 协议以高概率在O(log(n))时间内收敛到最大独立集(MIS)，n为网络中的代理数量。这是首个在网络无感知模型中为动态网络提供对数时间MIS收敛的协议。

Conclusion: 唱歌模型是蜂鸣模型的有效泛化，提出的协议在异步、动态网络中实现了高效的MIS计算，为网络无感知模型中的分布式算法提供了新方法。

Abstract: We introduce a broadcast model called the singing model, where agents are oblivious of the size and structure of the communication network, even their immediate neighborhood. Agents can sing multiple notes which are heard by their neighbors. The model is a generalization of the beeping model, where agents can only emit sound at a single frequency. We give a simple and natural protocol where agents compete with their neighbors and their strength is reflected in the number of notes they sing. It converges in $O(log(n))$ time with high probability, where $n$ is the number of agents in the network. The protocol works in an asynchronous model where rounds vary in length and have different start times. It works with completely dynamic networks where agents can be faulty. The protocol is the first to converge to an MIS in logarithmic time for dynamic networks in a network oblivious model.

</details>


### [6] [Comparative algorithm performance evaluation and prediction for the maximum clique problem using instance space analysis](https://arxiv.org/abs/2512.03419)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.DS

TL;DR: 使用实例空间分析方法评估最大团问题的算法性能，发现MOMC算法在74.7%的实例上表现最优，基于ISA的预测模型在挑战性测试集上达到88%的top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 最大团问题作为经典的组合优化问题，虽然已有多种算法解决方案，但缺乏对问题实例的系统性分析。本研究旨在填补这一空白，通过实例空间分析方法全面评估不同算法的性能表现。

Method: 采用实例空间分析(ISA)方法，收集TWITTER、COLLAB和IMDB-BINARY基准数据集中的图实例，使用35个多项式时间可计算的图特征（包括谱特征），构建包含解质量和运行时间的综合性能度量，评估精确算法、启发式算法和GNN方法。

Result: MOMC算法在74.7%的实例空间上表现最优，Gurobi和CliSAT分别在13.8%和11%的实例上表现最佳。基于ISA的预测模型在BHOSLIB和DIMACS的34个挑战性测试实例上，top-1和top-2预测准确率分别达到88%和97%。

Conclusion: 实例空间分析为最大团问题的算法性能评估提供了系统性框架，MOMC算法在大多数实例上表现最优，基于ISA的预测模型能够有效预测最佳算法选择，为实际问题求解提供了实用指导。

Abstract: The maximum clique problem, a well-known graph-based combinatorial optimization problem, has been addressed through various algorithmic approaches, though systematic analyses of the problem instances remain sparse. This study employs the instance space analysis (ISA) methodology to systematically analyze the instance space of this problem and assess & predict the performance of state-of-the-art (SOTA) algorithms, including exact, heuristic, and graph neural network (GNN)-based methods. A dataset was compiled using graph instances from TWITTER, COLLAB and IMDB-BINARY benchmarks commonly used in graph machine learning research. A set of 33 generic and 2 problem-specific polynomial-time-computable graph-based features, including several spectral properties, was employed for the ISA. A composite performance mea- sure incorporating both solution quality and algorithm runtime was utilized. The comparative analysis demonstrated that the exact algorithm Mixed Order Maximum Clique (MOMC) exhib- ited superior performance across approximately 74.7% of the instance space constituted by the compiled dataset. Gurobi & CliSAT accounted for superior performance in 13.8% and 11% of the instance space, respectively. The ISA-based algorithm performance prediction model run on 34 challenging test instances compiled from the BHOSLIB and DIMACS datasets yielded top-1 and top-2 best performing algorithm prediction accuracies of 88% and 97%, respectively.

</details>


### [7] [Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity](https://arxiv.org/abs/2512.03718)
*Robert Ganian,Hung P. Hoang,Simon Wietheger*

Main category: cs.DS

TL;DR: 本文研究了公平均值聚类问题的计算复杂性，证明了即使对高度受限的实例也不存在固定参数算法，但通过三种规避策略（额外约束、固定参数近似、树状矩阵参数化）可以获得可解性结果。


<details>
  <summary>Details</summary>
Motivation: 研究公平均值聚类问题的计算复杂性，该问题在公平设置下比传统聚类问题更具挑战性。虽然传统（非公平）均值聚类问题已知存在固定参数算法，但需要探究公平设置下的计算复杂性。

Method: 将公平均值聚类问题形式化为编辑彩色矩阵使其具有少量颜色平衡行的计算问题。通过理论分析建立完整的复杂性图景，包括下界证明和三种规避策略的可行性分析。

Result: 证明了即使对高度受限的公平均值聚类实例也不存在固定参数算法。但通过三种策略可以绕过这一下界：对实例施加额外约束、使用固定参数近似算法、或采用针对树状矩阵的替代参数化方法。

Conclusion: 公平均值聚类问题在一般情况下是计算困难的，但通过适当的约束、近似方法或特殊参数化可以获得可解性，为实际问题提供了可行的计算途径。

Abstract: We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.

</details>


### [8] [Robust Algorithms for Path and Cycle Problems in Geometric Intersection Graphs](https://arxiv.org/abs/2512.03843)
*Malory Marin,Jean-Florent Raymond,Rémi Watrigant*

Main category: cs.DS

TL;DR: 提出λ-linked partition新工具，用于设计几何相交图上哈密顿路径/环和长路径问题的鲁棒次指数算法，解决了de Berg等人的开放问题。


<details>
  <summary>Details</summary>
Motivation: 研究在R^d中相似尺寸胖对象相交图上的经典连通性问题的鲁棒次指数算法设计。这类图中顶点对应几何对象，当对象相交时顶点相邻。需要解决de Berg等人提出的开放问题，完成从ETH紧确算法角度对几何相交图上这些问题的研究。

Method: 引入λ-linked partition新工具，将顶点集划分为高度连通的组，可在多项式时间内计算且不需要几何表示。结合低树宽模式覆盖定理（对Marx-Pilipczuk结果的改进），应用于路径和环相关问题。

Result: 1. 在R^d相似尺寸胖对象相交图上获得首个ETH紧确的哈密顿路径和哈密顿环鲁棒算法，时间复杂度2^{O(n^{1-1/d})}。2. 在参数化设置中获得首个鲁棒次指数参数化算法用于长路径问题，时间复杂度2^{O(k^{1-1/d}log^2 k)} n^{O(1)}。

Conclusion: λ-linked partition是设计几何相交图上鲁棒次指数算法的有效工具，解决了哈密顿路径/环问题的ETH紧确算法开放问题，并扩展到参数化长路径问题。低树宽模式覆盖定理作为独立的结构性结果也有重要价值。

Abstract: We study the design of robust subexponential algorithms for classical connectivity problems on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$. In this setting, each vertex corresponds to a geometric object, and two vertices are adjacent if and only if their objects intersect. We introduce a new tool for designing such algorithms, which we call a $λ$-linked partition. This is a partition of the vertex set into groups of highly connected vertices. Crucially, such a partition can be computed in polynomial time and does not require access to the geometric representation of the graph. We apply this framework to problems related to paths and cycles in graphs. First, we obtain the first robust ETH-tight algorithms for Hamiltonian Path and Hamiltonian Cycle, running in time $2^{O(n^{1-1/d})}$ on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$. This resolves an open problem of de Berg et al. [STOC 2018] and completes the study of these problems on geometric intersection graphs from the viewpoint of ETH-tight exact algorithms. We further extend our approach to the parameterized setting and design the first robust subexponential parameterized algorithm for Long Path in any fixed dimension $d$. More precisely, we obtain a randomized robust algorithm running in time $2^{O(k^{1-1/d}\log^2 k)}\, n^{O(1)}$ on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$, where $k$ is the natural parameter. Besides $λ$-linked partitions, our algorithm also relies on a low-treewidth pattern covering theorem that we establish for geometric intersection graphs, which may be viewed as a refinement of a result of Marx-Pilipczuk [ESA 2017]. This structural result may be of independent interest.

</details>


### [9] [Aggregating maximal cliques in real-world graphs](https://arxiv.org/abs/2512.03960)
*Noga Alon,Sabyasachi Basu,Shweta Jain,Haim Kaplan,Jakub Łącki,Blair D. Sullivan*

Main category: cs.DS

TL;DR: 提出ρ-dense aggregators方法，用少量高密度聚类替代枚举所有极大团，显著降低计算复杂度和输出冗余


<details>
  <summary>Details</summary>
Motivation: 传统极大团枚举存在计算不可行性和高度冗余输出的问题，需要更简洁有效的图结构表示方法

Method: 引入ρ-dense aggregators概念，寻找边密度至少为ρ的小型聚类集合，这些聚类共同包含所有极大团

Result: 证明所有图都存在亚指数大小的aggregator，对有界退化图实现近线性时间和大小，实验显示比现有算法快6-300倍

Conclusion: ρ-dense aggregators提供了一种实用且理论保证的图结构摘要方法，显著优于传统极大团枚举

Abstract: Maximal clique enumeration is a fundamental graph mining task, but its utility is often limited by computational intractability and highly redundant output. To address these challenges, we introduce \emph{$ρ$-dense aggregators}, a novel approach that succinctly captures maximal clique structure. Instead of listing all cliques, we identify a small collection of clusters with edge density at least $ρ$ that collectively contain every maximal clique.
  In contrast to maximal clique enumeration, we prove that for all $ρ< 1$, every graph admits a $ρ$-dense aggregator of \emph{sub-exponential} size, $n^{O(\log_{1/ρ}n)}$, and provide an algorithm achieving this bound. For graphs with bounded degeneracy, a typical characteristic of real-world networks, our algorithm runs in near-linear time and produces near-linear size aggregators. We also establish a matching lower bound on aggregator size, proving our results are essentially tight. In an empirical evaluation on real-world networks, we demonstrate significant practical benefits for the use of aggregators: our algorithm is consistently faster than the state-of-the-art clique enumeration algorithm, with median speedups over $6\times$ for $ρ=0.1$ (and over $300\times$ in an extreme case), while delivering a much more concise structural summary.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [10] [CFO-Robust Detection for 5G PRACH under Fading Channels: Analytical Modeling and Performance Evaluation](https://arxiv.org/abs/2512.03096)
*Daniel Alarcón-Martín,Mari Carmen Aguayo-Torres,Francisco J. Martín-Vega,Gerardo Gómez*

Main category: cs.IT

TL;DR: 提出一个统一的PRACH检测分析框架，推导最优阈值和闭式检测概率，并设计一种利用CFO相关性的低复杂度检测器


<details>
  <summary>Details</summary>
Motivation: PRACH检测对高用户密度、大载波频率偏移和快速衰落等损伤高度敏感，现有研究局限于特定场景或缺乏全面的性能分析表征

Method: 1) 建立统一分析框架，在平坦瑞利衰落下表征接收功率延迟剖面的统计分布，支持相干合并和功率合并两种重复策略；2) 推导每种策略的最优阈值表达式和闭式检测概率；3) 分析相干时间相关的两种关键情况：每重复的相同和独立信道实现；4) 利用CFO在循环移位上引起的相关性，设计新型低复杂度检测器

Result: 数值结果表明：在独立信道条件下功率合并优于相干合并，而在相同信道实现的有限设置中相干合并可能更优；提出的CFO感知检测器在严重CFO条件下表现出更好的鲁棒性

Conclusion: 该研究为PRACH检测提供了统一的分析框架和性能表征，提出的CFO感知检测器在恶劣CFO条件下具有更好的鲁棒性，为5G/6G网络中的初始接入设计提供了理论指导

Abstract: The Physical Random Access Channel (PRACH) is essential for initial access and synchronization in both 5G and future 6G networks; however, its detection is highly sensitive to impairments such as high user density, large carrier frequency offset (CFO), and fast fading. Although prior studies have examined PRACH detection, they are often restricted to specific scenarios or lack a comprehensive analytical characterization of performance. We introduce a unified analytical framework that characterizes the statistical distribution of the received power delay profile (PDP) under flat Rayleigh fading and supports both coherent combining (CC) and power combining (PC) repetition strategies. For each strategy, we derive optimal threshold expressions and closed-form detection probabilities. Furthermore, we analyze two key cases depending on the coherence time: identical and independent channel realizations per repetition. Secondly, we exploit the correlation induced by CFO across cyclic shifts to design a novel low-complexity detector that exploits PDP dependencies. Numerical results indicate that PC outperforms CC when repetitions experience independent channels, while CC can be preferable under identical realizations in limited settings. On the other hand, the proposed CFO-aware detector delivers improved robustness under severe CFO conditions.

</details>


### [11] [Strengthening Han's Fourier Entropy-Influence Inequality via an Information-Theoretic Proof](https://arxiv.org/abs/2512.03117)
*Peijie Li,Guangyue Han*

Main category: cs.IT

TL;DR: 本文改进了Han的傅里叶熵-影响不等式，将常数优化到最优值C₁=C₂=1，适用于所有单位L²范数的实值布尔函数。


<details>
  <summary>Details</summary>
Motivation: Han之前证明的傅里叶熵-影响不等式虽然重要，但其常数C₁=3+2ln2和C₂=1并非最优。本文旨在通过信息论方法证明该不等式实际上可以以最优常数C₁=C₂=1成立，从而揭示该不等式作为香农熵和影响的基本结构特性。

Method: 采用简短的信息论证明方法，针对所有具有单位L²范数的实值布尔函数，优化了Han不等式的常数。

Result: 成功证明了傅里叶熵-影响不等式以最优常数C₁=C₂=1成立，该结果适用于所有单位L²范数的实值布尔函数，比Han原来的结果更强。

Conclusion: 该不等式实际上是一个关于香农熵和影响的基本结构性质，最优常数的发现强化了我们对布尔函数傅里叶熵与影响之间关系的理解。

Abstract: We strengthen Han's Fourier entropy-influence inequality $$ H[\widehat{f}] \leq C_{1}I(f) + C_{2}\sum_{i\in [n]}I_{i}(f)\ln\frac{1}{I_{i}(f)} $$ originally proved for $\{-1,1\}$-valued Boolean functions with $C_{1}=3+2\ln 2$ and $C_{2}=1$. We show, by a short information-theoretic proof, that it in fact holds with sharp constants $C_{1}=C_{2}=1$ for all real-valued Boolean functions of unit $L^{2}$-norm, thereby establishing the inequality as an elementary structural property of Shannon entropy and influence.

</details>


### [12] [Multi-Source M/G/1/1 Queues with Probabilistic Preemption](https://arxiv.org/abs/2512.03241)
*Mohammad Moltafet,Hamid R. Sadjadpour,Zouheir Rezki,Marian Codreanu,Roy D. Yates*

Main category: cs.IT

TL;DR: 提出一种概率抢占式包管理策略，用于多源状态更新系统，推导了AoI和PAoI的矩生成函数，并通过数值结果验证策略有效性。


<details>
  <summary>Details</summary>
Motivation: 在多源状态更新系统中，如何有效管理数据包以降低信息年龄(AoI)是一个重要问题。现有系统通常采用完全抢占或非抢占策略，但缺乏灵活的概率抢占机制。

Method: 建立多源M/G/1/1排队系统模型，提出概率抢占式包管理策略：当同一源的新包到达时，以固定概率替换系统中已有的同源包。推导了各源AoI和PAoI的矩生成函数。

Result: 数值结果表明，所提出的概率抢占式包管理策略能有效降低信息年龄，相比传统策略具有更好的性能表现。

Conclusion: 概率抢占式包管理策略为多源状态更新系统提供了一种灵活有效的包管理方案，能显著改善信息新鲜度性能。

Abstract: We consider a multi-source status update system consisting of multiple independent sources, a single server, and a single sink. Each source generates packets according to a Poisson process, and packets are served according to a general service time distribution. The system has a capacity of one packet, i.e., no waiting buffer, and is modeled as a multi-source M/G/1/1 queueing system. We introduce a probabilistically preemptive packet management policy, under which an existing packet from the same source in the system is replaced by an arriving packet with a fixed probability. We derive the moment generating functions (MGFs) of the age of information (AoI) and peak AoI (PAoI) for each source under this policy. Numerical results demonstrate the effectiveness of the proposed packet management policy.

</details>


### [13] [Generalized Orthogonal Approximate Message-Passing for Sublinear Sparsity](https://arxiv.org/abs/2512.03326)
*Keigo Takeuchi*

Main category: cs.IT

TL;DR: 提出广义正交AMP(GOAMP)算法，用于从广义线性测量中重建亚线性稀疏信号，通过Onsager校正解决非高斯测量矩阵下的收敛问题，在亚线性稀疏极限下实现误差自由重建。


<details>
  <summary>Details</summary>
Motivation: 传统AMP算法仅适用于标准高斯测量矩阵，对于更一般的正交不变测量矩阵收敛性差。同时，现有研究主要关注信号稀疏度与维度成比例的情况，而实际中信号可能具有亚线性稀疏度（稀疏度随维度增长但慢于线性）。

Method: 提出广义正交AMP(GOAMP)算法，核心是通过状态演化设计Onsager校正项，确保在正交不变测量矩阵和亚线性稀疏极限下估计误差保持渐近高斯性。算法使用贝叶斯去噪器，并针对亚线性稀疏度进行优化。

Result: 当非零信号支撑不包含原点邻域时，GOAMP在线性测量下能实现误差自由重建，且所需测量维度阈值与标准高斯矩阵下的AMP相同。数值模拟显示，对于病态测量矩阵，GOAMP在亚线性稀疏度下优于现有算法包括广义AMP。

Conclusion: GOAMP算法成功解决了亚线性稀疏信号在广义线性测量下的重建问题，通过精心设计的Onsager校正实现了对正交不变测量矩阵的良好适应性，在病态矩阵情况下表现优于现有方法。

Abstract: This paper addresses the reconstruction of sparse signals from generalized linear measurements. Signal sparsity is assumed to be sublinear in the signal dimension while it was proportional to the signal dimension in conventional research. Approximate message-passing (AMP) has poor convergence properties for sensing matrices beyond standard Gaussian matrices. To solve this convergence issue, generalized orthogonal AMP (GOAMP) is proposed for signals with sublinear sparsity. The main feature of GOAMP is the so-called Onsager correction to realize asymptotic Gaussianity of estimation errors. The Onsager correction in GOAMP is designed via state evolution for orthogonally invariant sensing matrices in the sublinear sparsity limit, where the signal sparsity and measurement dimension tend to infinity at sublinear speed in the signal dimension. When the support of non-zero signals does not contain a neighborhood of the origin, GOAMP using Bayesian denoisers is proved to achieve error-free signal reconstruction for linear measurements if and only if the measurement dimension is larger than a threshold, which is equal to that of AMP for standard Gaussian sensing matrices. Numerical simulations are also presented for linear measurements and 1-bit compressed sensing. When ill-conditioned sensing matrices are used, GOAMP for sublinear sparsity is shown to outperform existing reconstruction algorithms, including generalized AMP for sublinear sparsity.

</details>


### [14] [From Reliability to Security: How RIS-Assisted Adaptive SM and SSK Enhances Wireless Systems](https://arxiv.org/abs/2512.03518)
*Chaorong Zhang,Benjamin K. Ng,Ke Wang,Hui Xu,Chan-Tong Lam*

Main category: cs.IT

TL;DR: 本文提出了两种新型无线传输方案：RIS辅助接收自适应空间调制(RASM)和RIS辅助接收自适应空间移位键控(RASSK)，旨在提高频谱效率和物理层安全性。


<details>
  <summary>Details</summary>
Motivation: 传统无线通信系统在频谱效率和物理层安全性方面存在局限，需要更高效、安全的传输方案。RIS技术为动态调整无线信道提供了新可能，但如何充分利用RIS特性实现高效安全的通信仍需探索。

Method: 1. 提出RIS辅助接收自适应空间调制(RASM)方案
2. 提出RIS辅助接收自适应空间移位键控(RASSK)方案
3. 利用RIS特性在每个时隙动态映射接收天线上的传输比特
4. 以极少功耗增强特定选定天线的信噪比
5. 通过自适应方法向接收器传递额外比特

Result: 1. 提高了频谱效率(SE)
2. 降低了发射机射频链成本
3. 增强了物理层安全(PLS)，窃听者无法完全检测RIS反射信号
4. 提供了详细的频谱效率、检测复杂度、误码率和保密率分析
5. 仿真和分析结果显示了优越的误码性能和抗窃听鲁棒性

Conclusion: 提出的RASM和RASSK方案在频谱效率、物理层安全性和能效方面表现出色，为未来无线通信应用提供了有前景的解决方案，展示了RIS技术在实现可靠、绿色无线通信方面的潜力。

Abstract: This paper proposes two novel wireless transmission schemes, namely reconfigurable intelligent surface (RIS)-assisted received adaptive spatial modulation (RASM) scheme and RIS-assisted received adaptive space shift keying (RASSK) scheme, designed to enhance spectral efficiency (SE) and physical layer security (PLS).In both proposed schemes, transmitting bits are dynamically mapped at receive antennas by leveraging the characteristics of the RIS in each time slot, which enables the enhancement of signal-to-noise ratio (SNR) at specific selected antennas with near few power, thus leading a reliable and green wireless communication. This adaptive approach facilitates the conveyance of extra bits to the receiver, which means it needs less cost of radio-frequency chains at transmitter while improving SE. Besides, the proposed schemes offer an inherent PLS security advantage, as the eavesdropper is unable to completely detect signals reflected from the RIS. To comprehensively evaluate the performance of the proposed RASM and RASSK schemes, this paper presents a detailed analytical performance of their spectral efficiency, detection complexity, bit error rate, and secrecy rate, which are accompanied by insightful findings and conclusions. Simulation and analytical results demonstrate the superiority of the proposed schemes, showcasing their improved error performance and robustness against wiretapping, while also highlighting the potential of the RASM and RASSK schemes for future wireless applications.

</details>


### [15] [Expected Confidence Dependency: A Novel Rough Set-Based Approach to Feature Selection](https://arxiv.org/abs/2512.03612)
*Saeed Rasouli,Hamid Karamikabir*

Main category: cs.IT

TL;DR: 提出了一种新的特征选择依赖度量方法ECD，基于置信度贡献和归一化期望算子，相比传统粗糙集方法具有更好的软计算特性。


<details>
  <summary>Details</summary>
Motivation: 传统粗糙集依赖度量方法基于条件块的二元表征，存在局限性。需要一种更灵活的、软计算导向的、精度驱动的特征选择依赖度量方法。

Method: 提出期望置信依赖(ECD)方法，为单个等价块分配基于置信度的贡献，并通过归一化期望算子进行聚合。

Result: ECD具有归一化、与经典依赖兼容、单调性、在结构和标签保持变换下不变等理想性质。

Conclusion: ECD是一种新颖有效的特征选择依赖度量方法，在粗糙集理论框架下提供了更好的软计算解决方案。

Abstract: This paper proposes Expected Confidence Dependency (ECD), a novel, soft computing-oriented, accuracy driven dependency measure for feature selection within the rough set theory framework. Unlike traditional rough set dependency measures that rely on binary characterizations of conditional blocks, ECD assigns confidence-based contributions to individual equivalence blocks and aggregates them through a normalized expectation operator. We formally establish several desirable properties of ECD, including normalization, compatibility with classical dependency, monotonicity, and invariance under structural and label-preserving transformations.

</details>


### [16] [Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing](https://arxiv.org/abs/2512.03719)
*Seyed Mohammad Azimi-Abarghouyi,Carlo Fischione,Kaibin Huang*

Main category: cs.IT

TL;DR: AirFL是一种新兴的联邦学习范式，利用无线信号的叠加特性同时进行通信和模型聚合，显著降低延迟、带宽和能耗。本文提供了AirFL的教程式介绍，将其分为三种设计方法：CSIT感知、盲式和加权AirFL。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算和分布式机器学习的发展，需要在网络边缘实现可扩展的AI。传统联邦学习面临通信开销大、延迟高、能耗多的问题。AirFL通过利用无线信号的物理层特性，将通信和模型聚合紧密结合，旨在解决这些挑战。

Method: 提出三种AirFL设计方法：1) CSIT感知AirFL - 利用信道状态信息进行优化设计；2) 盲式AirFL - 无需信道状态信息；3) 加权AirFL - 采用加权聚合策略。文章提供了理论基础、性能分析、复杂度考虑和实践限制的全面指南。

Result: AirFL通过无线信号的叠加特性实现通信和模型聚合的同时进行，显著减少了联邦学习的延迟、带宽需求和能耗。三种设计方法各有适用场景，为不同条件下的边缘AI部署提供了灵活选择。

Conclusion: AirFL是边缘计算中联邦学习的重要发展方向，通过无线通信和机器学习的深度融合，为实现可扩展的边缘AI提供了高效解决方案。文章为研究人员和从业者提供了全面的指导，并指出了未来的研究方向。

Abstract: Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.

</details>


### [17] [Movable Signals with Dual-Polarized Fixed Intelligent Surfaces: Beyond Diagonal Reflection Matrices](https://arxiv.org/abs/2512.03872)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文研究了双极化智能表面辅助的无线系统，比较了可重构智能表面（RIS）和固定智能表面（FIS）配合可调信号频率的性能，发现FIS始终优于RIS，至少获得四倍增益。


<details>
  <summary>Details</summary>
Motivation: 研究双极化智能表面在无线系统中的性能差异，比较可重构智能表面（RIS）与固定智能表面（FIS）两种不同架构的优劣，探索不同反射矩阵配置对系统性能的影响。

Method: 将智能表面分为两类：RIS（可调整反射矩阵）和FIS（表面特性固定但信号频率可调）。每类又细分为对角型（反射矩阵为对角）和非对角型（反射矩阵不受对角限制）。通过理论分析和性能比较来评估不同配置下的系统性能。

Result: FIS配合可调信号频率始终优于RIS，至少获得四倍性能增益。当发射机和接收机极化不同时，非对角型FIS能进一步提升性能表现。

Conclusion: 在双极化智能表面辅助的无线系统中，固定智能表面（FIS）配合可调信号频率的方案优于可重构智能表面（RIS），特别是在发射接收极化不同的场景下，非对角型FIS能提供最佳性能。

Abstract: This paper investigates wireless systems aided by dual-polarized intelligent surfaces. We compare reconfigurable intelligent surface (RIS), which adjust their reflection matrices, with movable signals operating with fixed intelligent surface (FIS), which adjust the signal frequency while the surface properties remain fixed. For both RIS and FIS, we consider surfaces with a diagonal reflection matrix, named diagonal RIS/FIS, and surfaces with a reflection matrix not limited to being diagonal, named beyond-diagonal RIS/FIS. Movable signals with FIS always outperform RIS, achieving at least a fourfold gain. When transmitter and receiver polarizations differ, beyond-diagonal FIS further enhances performance.

</details>


### [18] [On topological and algebraic structures of categorical random variables](https://arxiv.org/abs/2512.04020)
*Inocencio Ortiz,Santiago Gómez-Guerrero,Christian E. Schaerer*

Main category: cs.IT

TL;DR: 基于熵和对称不确定性定义了分类随机变量的度量，该度量可提升为分类随机变量商空间的适当度量，并证明该商空间存在自然的交换幺半群结构，且与度量诱导的拓扑相容。


<details>
  <summary>Details</summary>
Motivation: 为分类随机变量建立严格的数学框架，将信息论度量（熵和对称不确定性）与代数结构（交换幺半群）和拓扑结构相结合，提供统一的数学处理工具。

Method: 基于熵和对称不确定性定义分类随机变量的度量，构造商空间使该度量成为适当度量，并在该商空间中定义自然的交换幺半群结构。

Result: 成功构建了分类随机变量的度量空间和交换幺半群结构，证明幺半群运算在度量诱导的拓扑下是连续的，实现了代数结构与拓扑结构的相容性。

Conclusion: 为分类随机变量建立了包含度量、代数和拓扑结构的统一数学框架，为信息论和概率论中的分类变量分析提供了新的理论基础。

Abstract: Based on entropy and symmetrical uncertainty (SU), we define a metric for categorical random variables and show that this metric can be promoted into an appropriate quotient space of categorical random variables. Moreover, we also show that there is a natural commutative monoid structure in the same quotient space, which is compatible with the topology induced by the metric, in the sense that the monoid operation is continuous.

</details>


### [19] [Semi-Markov Decision Process Framework for Age of Incorrect Information Minimization](https://arxiv.org/abs/2512.04077)
*Ismail Cosandal,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 研究远程估计系统中的错误信息年龄(AoII)优化问题，采用多阈值传输策略，通过半马尔可夫决策过程(SMDP)和新型随机工具DR-AMC/DR-DPH来最小化AoII函数和传输成本的加权和。


<details>
  <summary>Details</summary>
Motivation: 远程估计系统中需要优化语义感知的新鲜度度量AoII，在具有一般离散时间相型分布信道延迟和完美反向信道信息的复杂环境下，传统方法难以处理，需要新的分析框架。

Method: 采用多阈值传输策略，将问题建模为半马尔可夫决策过程(SMDP)，使用新型随机工具双机制吸收马尔可夫链(DR-AMC)及其吸收时间分布DR-DPH来获取SMDP参数，从而优化阈值选择。

Result: 提出了一个完整的分析框架，能够在具有一般信道延迟分布和完美反馈信息的复杂远程估计系统中，通过优化多阈值策略来最小化AoII相关成本和传输成本。

Conclusion: 该研究为具有复杂信道特性和语义感知需求的远程估计系统提供了一个有效的优化框架，通过SMDP建模和新型随机工具DR-AMC/DR-DPH实现了多阈值策略的最优设计。

Abstract: For a remote estimation system, we study age of incorrect information (AoII), which is a recently proposed semantic-aware freshness metric. In particular, we assume an information source observing a discrete-time finite-state Markov chain (DTMC) and employing push-based transmissions of status update packets towards the monitor which is tasked with remote estimation of the source. The source-to-monitor channel delay is assumed to have a general discrete-time phase-type (DPH) distribution, whereas the zero-delay reverse channel ensures that the source has perfect information on AoII and the remote estimate. A multi-threshold transmission policy is employed where packet transmissions are initiated when the AoII process exceeds a threshold which may be different for each estimation value. In this general setting, our goal is to minimize the weighted sum of time average of an arbitrary function of AoII and estimation, and transmission costs, by suitable choice of the thresholds. We formulate the problem as a semi-Markov decision process (SMDP) with the same state-space as the original DTMC to obtain the optimum multi-threshold policy whereas the parameters of the SMDP are obtained by using a novel stochastic tool called dual-regime absorbing Markov chain (DR-AMC), and its corresponding absorption time distribution named as dual-regime DPH (DR-DPH).

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [20] [When Harmful Content Gets Camouflaged: Unveiling Perception Failure of LVLMs with CamHarmTI](https://arxiv.org/abs/2512.03087)
*Yanhui Li,Qi Zhou,Zhihong Xu,Huizhong Guo,Wenhai Wang,Dongxia Wang*

Main category: cs.MM

TL;DR: LVLMs在检测伪装有害内容（如梗图、含恶意文本的图像）方面存在显著感知差距，人类准确率超95%，而ChatGPT-4o仅2.1%。作者提出CamHarmTI基准，包含4500+样本，用于评估和改进LVLMs的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界的有害内容往往通过文本-图像的微妙交互进行伪装（如梗图、嵌入恶意文本的图像），以逃避检测。现有大型视觉语言模型（LVLMs）是否能像人类一样敏感地感知这种伪装有害内容，是一个关键问题。

Method: 提出CamHarmTI基准，包含4500多个样本，涵盖三种类型的图文帖子。在100名人类用户和12个主流LVLMs上进行实验，并进行微调实验、注意力分析和分层探测，研究模型感知机制。

Result: 发现明显的感知差距：人类准确率超过95.75%，而当前LVLMs表现不佳（如ChatGPT-4o仅2.10%准确率）。微调实验显示CamHarmTI能有效提升模型感知能力，使Qwen2.5VL-7B准确率提升55.94%。注意力分析表明微调主要增强了视觉编码器早期层的敏感性。

Conclusion: LVLMs在感知伪装有害内容方面存在固有局限性，与人类感知能力存在显著差距。CamHarmTI基准可作为改进模型感知能力的有效资源，通过增强视觉编码器早期层的敏感性，促进更整合的场景理解，为开发更符合人类视觉推理的系统提供见解。

Abstract: Large vision-language models (LVLMs) are increasingly used for tasks where detecting multimodal harmful content is crucial, such as online content moderation. However, real-world harmful content is often camouflaged, relying on nuanced text-image interplay, such as memes or images with embedded malicious text, to evade detection. This raises a key question: \textbf{can LVLMs perceive such camouflaged harmful content as sensitively as humans do?} In this paper, we introduce CamHarmTI, a benchmark for evaluating LVLM ability to perceive and interpret camouflaged harmful content within text-image compositions. CamHarmTI consists of over 4,500 samples across three types of image-text posts. Experiments on 100 human users and 12 mainstream LVLMs reveal a clear perceptual gap: humans easily recognize such content (e.g., over 95.75\% accuracy), whereas current LVLMs often fail (e.g., ChatGPT-4o achieves only 2.10\% accuracy). Moreover, fine-tuning experiments demonstrate that \bench serves as an effective resource for improving model perception, increasing accuracy by 55.94\% for Qwen2.5VL-7B. Attention analysis and layer-wise probing further reveal that fine-tuning enhances sensitivity primarily in the early layers of the vision encoder, promoting a more integrated scene understanding. These findings highlight the inherent perceptual limitations in LVLMs and offer insight into more human-aligned visual reasoning systems.

</details>


### [21] [Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation](https://arxiv.org/abs/2512.03521)
*Xiaosen Lyu,Jiayu Xiong,Yuren Chen,Wanlong Wang,Xiaoqing Dai,Jing Wang*

Main category: cs.MM

TL;DR: 提出CSS框架，通过SPF高效捕捉高阶跨模态交互，PGM缓解梯度冲突，提升MERC任务性能与训练稳定性


<details>
  <summary>Details</summary>
Motivation: 现有MERC方法难以捕捉复杂的跨模态交互，或在深层架构中面临梯度冲突和训练不稳定问题

Method: 提出Cross-Space Synergy (CSS)框架，包含Synergistic Polynomial Fusion (SPF)用于高效捕捉高阶跨模态交互，Pareto Gradient Modulator (PGM)用于优化方向，缓解梯度冲突

Result: 在IEMOCAP和MELD数据集上超越现有代表性方法，在准确率和训练稳定性方面均表现优异

Conclusion: CSS框架有效解决了复杂多模态场景中的跨模态交互和训练稳定性问题，为MERC任务提供了有效解决方案

Abstract: Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers' emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases](https://arxiv.org/abs/2512.03278)
*Michael Theologitis,Dan Suciu*

Main category: cs.DB

TL;DR: Thucy：首个跨数据库、跨表格的多智能体声明验证系统，通过自主发现和推理关系数据库来验证声明，并提供支持验证结果的SQL查询证据


<details>
  <summary>Details</summary>
Motivation: 当前验证系统仅能在小型单表数据库上操作，无法处理现实世界中跨多个数据库和表格的复杂声明验证需求。需要开发能够自主发现、检查并推理所有可用关系数据库的验证系统。

Method: 开发了Thucy系统，这是一个完全与底层数据源无关的多智能体声明验证系统。系统能够自主发现、检查并推理所有可用关系数据库，通过生成和执行SQL查询来验证声明，并提供完整的SQL查询证据。

Result: 在TabFact数据集（结构化数据事实验证的标准基准）上，Thucy以94.3%的准确率超越了之前88.7%的最优结果，提升了5.6个百分点。

Conclusion: Thucy展示了跨数据库、跨表格声明验证的可行性，通过提供透明的SQL查询证据增强了系统的可信度，为自动事实核查系统的发展提供了重要进展。

Abstract: In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\unicode{x2014}$typically a few hundred rows$\unicode{x2014}$that conveniently fit within an LLM's context window.
  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\unicode{x2014}$the standard benchmark for fact verification over structured data$\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).

</details>


### [23] [Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams](https://arxiv.org/abs/2512.03389)
*Shu Chen,Deepti Raghavan,Uğur Çetintemel*

Main category: cs.DB

TL;DR: 提出了Continuous Prompts (CPs)框架，将LLM推理引入连续流处理，通过动态优化在准确性和效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM框架是状态无关和一次性的，无法满足对非结构化流进行持久、语义感知计算的需求，限制了其在长期运行分析中的应用。

Method: 1. 提出Continuous Prompts (CPs)框架，将RAG扩展到流处理场景
2. 定义连续语义操作符，提供多种实现（主要基于LLM，也有嵌入变体）
3. 研究两种LLM优化：元组批处理和操作符融合
4. 开发动态优化框架，使用轻量级影子执行和成本感知多目标贝叶斯优化来学习吞吐量-准确性边界

Result: 在VectraFlow流处理系统中实现CPs，通过微基准测试和真实数据集上的流管道实验，证明系统能够适应工作负载动态变化，导航准确性-效率权衡，并在不断演化的非结构化流上维持持久语义查询。

Conclusion: CPs框架成功将LLM推理引入连续流处理，通过动态优化机制在保持语义准确性的同时显著提升处理效率，为非结构化流的实时语义分析提供了有效解决方案。

Abstract: Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.
  Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.
  We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.

</details>


### [24] [Enterprise Data Science Platform: A Unified Architecture for Federated Data Access](https://arxiv.org/abs/2512.03401)
*Ryoto Miyamoto,Akira Kasuga*

Main category: cs.DB

TL;DR: EDSP是一个基于数据湖仓架构的企业数据科学平台，采用"一次写入、随处读取"原则，通过集中式数据管理和联邦访问机制，解决多查询引擎环境中的数据孤岛问题，减少数据复制和厂商锁定。


<details>
  <summary>Details</summary>
Motivation: 组织在跨部门共享数据时面临挑战，不同部门采用不同的数据分析平台导致数据孤岛。传统数据仓库需要将数据复制到特定厂商的存储中，跨平台访问困难，可能产生n*m个数据副本，增加不一致性和成本。

Method: 提出企业数据科学平台（EDSP），基于数据湖仓架构，采用四层架构：数据准备层、数据存储层、访问接口层和查询引擎层。遵循"一次写入、随处读取"原则，实现联邦数据访问，支持多查询引擎环境。

Result: 实验结果表明，主要云数据仓库和编程环境可以直接查询EDSP管理的数据集。生产部署证实了跨多查询引擎的互操作性。与传统需要数据迁移的方法相比，EDSP将操作步骤减少了33-44%。虽然查询延迟可能增加最多2.6倍，但端到端完成时间仍在秒级，保持分析用例的实用性能。

Conclusion: EDSP为多查询引擎环境中的数据孤岛问题提供了实用的设计指南，通过集中式数据管理和联邦访问机制，消除了数据复制和厂商锁定，同时保持了分析用例的实用性能。

Abstract: Organizations struggle to share data across departments that have adopted different data analytics platforms. If n datasets must serve m environments, up to n*m replicas can emerge, increasing inconsistency and cost. Traditional warehouses copy data into vendor-specific stores; cross-platform access is hard. This study proposes the Enterprise Data Science Platform (EDSP), which builds on data lakehouse architecture and follows a Write-Once, Read-Anywhere principle. EDSP enables federated data access for multi-query engine environments, targeting data science workloads with periodic data updates and query response times ranging from seconds to minutes. By providing centralized data management with federated access from multiple query engines to the same data sources, EDSP eliminates data duplication and vendor lock-in inherent in traditional data warehouses. The platform employs a four-layer architecture: Data Preparation, Data Store, Access Interface, and Query Engines. This design enforces separation of concerns and reduces the need for data migration when integrating additional analytical environments. Experimental results demonstrate that major cloud data warehouses and programming environments can directly query EDSP-managed datasets. We implemented and deployed EDSP in production, confirming interoperability across multiple query engines. For data sharing across different analytical environments, EDSP achieves a 33-44% reduction in operational steps compared with conventional approaches requiring data migration. Although query latency may increase by up to a factor of 2.6 compared with native tables, end-to-end completion times remain on the order of seconds, maintaining practical performance for analytical use cases. Based on our production experience, EDSP provides practical design guidelines for addressing the data-silo problem in multi-query engine environments.

</details>


### [25] [ExOAR: Expert-Guided Object and Activity Recognition from Textual Data](https://arxiv.org/abs/2512.03790)
*Iris Beerepoot,Vinicius Stein Dani,Xixi Lu*

Main category: cs.DB

TL;DR: ExOAR是一个交互式方法，结合大语言模型和人工验证，从非结构化文本中提取对象和活动信息，用于以对象为中心的过程挖掘。


<details>
  <summary>Details</summary>
Motivation: 以对象为中心的过程挖掘需要结构化数据，但从非结构化文本中提取这种数据仍然是一个挑战。现有方法难以有效处理文本数据到结构化日志的转换。

Method: ExOAR是一个交互式方法，结合LLM和人工验证。它引导用户通过连续阶段：LLM基于上下文输入（如用户职业）和文本数据生成候选对象类型、活动和对象实例，用户审查并完善这些建议后再进入下一阶段。

Result: ExOAR被实现为实用工具，通过演示进行初步验证，然后使用来自五个用户的真实世界活动窗口跟踪数据进行评估。结果表明，ExOAR能有效弥合非结构化文本数据与以对象为中心过程分析所需的结构化日志之间的差距，同时保持灵活性和人工监督。

Conclusion: ExOAR能够有效解决从非结构化文本到结构化过程日志的转换问题，为以对象为中心的过程挖掘提供了实用的交互式解决方案，结合了LLM的自动化能力和人工监督的准确性。

Abstract: Object-centric process mining requires structured data, but extracting it from unstructured text remains a challenge. We introduce ExOAR (Expert-Guided Object and Activity Recognition), an interactive method that combines large language models (LLMs) with human verification to identify objects and activities from textual data. ExOAR guides users through consecutive stages in which an LLM generates candidate object types, activities, and object instances based on contextual input, such as a user's profession, and textual data. Users review and refine these suggestions before proceeding to the next stage. Implemented as a practical tool, ExOAR is initially validated through a demonstration and then evaluated with real-world Active Window Tracking data from five users. Our results show that ExOAR can effectively bridge the gap between unstructured textual data and the structured log with clear semantics needed for object-centric process analysis, while it maintains flexibility and human oversight.

</details>


### [26] [IBM Multilevel Process Mining vs de facto Object-Centric Process Mining approaches](https://arxiv.org/abs/2512.03906)
*Alberto Ronzoni,Anina Antony,Anjana M R,Francesca De Leo,Jesna Jose,Mattia Freda,Nandini Narayanankutty,Rafflesia Khan,Raji RV,Thomas Diacci*

Main category: cs.DB

TL;DR: 本文比较了对象中心过程挖掘与IBM多级过程挖掘两种方法，并基于比较开发了结合两者优势的组织挖掘新功能。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘领域正在向对象中心过程挖掘发展，IBM开发了多级过程挖掘方法，需要比较这两种方法的优劣以指导产品演进。

Method: 对对象中心过程挖掘和IBM多级过程挖掘进行描述和比较分析，基于比较结果开发新的组织挖掘功能，并通过示例展示该方法。

Result: IBM基于比较分析开发了组织挖掘功能，该功能结合了两种方法的优势，创建了创新且独特的方法论。

Conclusion: 通过比较两种过程挖掘方法并开发组织挖掘功能，IBM创建了结合两者优势的创新方法论，展示了其在过程挖掘产品演进中的价值。

Abstract: The academic evolution of process mining is moving toward object centric process mining, marking a significant shift in how processes are modeled and analyzed. IBM has developed its own distinctive approach called Multilevel Process Mining. This paper provides a description of the two approaches and presents a comparative analysis of their respective advantages and limitations. IBM leveraged this comparison to drive the evolution of IBM Process Mining product, creating the new Organizational Mining feature, an innovation that combines the best of the two approaches. Demonstrate the potential of this novel, innovative and distinct methodology with an example.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [27] [BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents](https://arxiv.org/abs/2512.03413)
*Shu Wang,Yingli Zhou,Yixiang Fang*

Main category: cs.IR

TL;DR: BookRAG：针对层次结构文档的新型RAG方法，通过提取层次树、构建实体关系图，并基于信息觅食理论设计智能体查询机制，显著提升检索召回率和QA准确率。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要针对通用文档，忽略了现实世界中许多文档（如书籍、手册等）具有层次结构的特点，导致在QA任务上表现不佳。需要专门针对层次结构文档的RAG方法。

Method: 1. 构建BookIndex索引结构：从文档中提取层次树作为目录，用图捕捉实体间复杂关系，并将实体映射到树节点。2. 基于信息觅食理论的智能体查询方法：动态分类查询并采用定制化的检索工作流。

Result: 在三个广泛采用的基准测试中，BookRAG实现了最先进的性能，在检索召回率和QA准确率方面显著优于基线方法，同时保持竞争性的效率。

Conclusion: BookRAG是针对层次结构文档的有效RAG方法，通过利用文档的逻辑层次结构和实体关系，显著提升了QA任务的性能，为处理结构化文档提供了新的解决方案。

Abstract: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.

</details>


### [28] [LLM as Explainable Re-Ranker for Recommendation System](https://arxiv.org/abs/2512.03439)
*Yaqi Wang,Haojia Sun,Shuting Zhang*

Main category: cs.IR

TL;DR: 使用LLM作为可解释的重新排序器，结合传统推荐模型提升准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统缺乏可解释性且存在流行度偏差问题，而LLM单独作为预测器时准确性不如传统模型

Method: 提出混合方法：使用LLM作为可解释的重新排序器，结合传统推荐模型；构建数据集训练重新排序器LLM，采用两阶段训练过程

Result: 模型显著提升了NDCG排名指标，在排名准确性和可解释性方面优于零样本基线

Conclusion: 整合传统推荐模型与LLM能够解决现有系统的局限性，为更可解释和公平的推荐框架铺平道路

Abstract: The application of large language models (LLMs) in recommendation systems has recently gained traction. Traditional recommendation systems often lack explainability and suffer from issues such as popularity bias. Previous research has also indicated that LLMs, when used as standalone predictors, fail to achieve accuracy comparable to traditional models. To address these challenges, we propose to use LLM as an explainable re-ranker, a hybrid approach that combines traditional recommendation models with LLMs to enhance both accuracy and interpretability. We constructed a dataset to train the re-ranker LLM and evaluated the alignment between the generated dataset and human expectations. Leveraging a two-stage training process, our model significantly improved NDCG, a key ranking metric. Moreover, the re-ranker outperformed a zero-shot baseline in ranking accuracy and interpretability. These results highlight the potential of integrating traditional recommendation models with LLMs to address limitations in existing systems and pave the way for more explainable and fair recommendation frameworks.

</details>


### [29] [M3DR: Towards Universal Multilingual Multimodal Document Retrieval](https://arxiv.org/abs/2512.03514)
*Adithya S Kolavi,Vyoman Jain*

Main category: cs.IR

TL;DR: M3DR是一个多语言多模态文档检索框架，通过合成多语言数据和对比学习实现跨语言跨模态对齐，在22种语言上验证了性能，并在跨语言检索上取得了约150%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索系统主要针对英语，在多语言环境中的效果有限。需要开发一个能够跨越不同语言和文化背景的框架，以适应全球化的文档检索需求。

Method: M3DR框架利用合成多语言文档数据，采用对比训练方法学习文本和文档图像的统一表示。该框架可泛化到不同的视觉-语言架构和模型规模，支持单密集向量和ColBERT风格的多向量检索范式。

Result: 在22种类型多样的语言上验证了模型能力，展示了跨语言和文字变体的稳定性能。NetraEmbed和ColNetraEmbed模型在跨语言检索任务上实现了约150%的相对改进，达到了最先进水平。

Conclusion: M3DR成功解决了多语言多模态文档检索的挑战，通过统一的表示学习框架实现了有效的跨语言跨模态对齐，为实际多语言场景提供了强大的检索解决方案。

Abstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.

</details>


### [30] [Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics](https://arxiv.org/abs/2512.03807)
*Christos Kolomvakis,Thomas Bobille,Arnaud Vandaele,Nicolas Gillis*

Main category: cs.IR

TL;DR: 本文提出新的布尔矩阵分解算法，包括交替优化、整数规划、贪婪和局部搜索启发式方法，以及高效的C++数据结构，在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 布尔矩阵分解（BMF）使用布尔OR和AND运算进行矩阵乘积，相比基于标准算术的二进制矩阵分解具有更好的可解释性和更低的近似误差，在角色挖掘和计算机视觉中有应用。现有方法在可扩展性方面存在限制。

Method: 1) 提出基于交替优化（AO）的算法，每个子问题通过整数规划（IP）求解；2) 设计从多次运行中选择最优秩一因子子集的方法增强AO算法；3) 针对IP方法可扩展性限制，引入新的贪婪和局部搜索启发式方法；4) 构建新的C++数据结构，显著加速布尔向量和矩阵运算。

Result: 提出的方法在各种真实数据集上（包括有缺失数据和无缺失数据的情况）表现出优越性能，在主题建模和成像应用中验证了有效性，新数据结构使启发式方法能够扩展到大型数据集。

Conclusion: 本文提出的布尔矩阵分解算法框架结合了精确优化和启发式方法，通过高效的数据结构实现可扩展性，在多个应用领域优于现有技术，为大规模布尔矩阵分解提供了有效的解决方案。

Abstract: Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.

</details>


### [31] [Learning to Comparison-Shop](https://arxiv.org/abs/2512.04009)
*Jie Tang,Daochen Zha,Xin Liu,Huiji Gao,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 提出LTCS系统，通过显式建模用户比较购物行为来改进电商搜索排名，在NDCG和预订转化率上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有电商搜索引擎与用户比较购物需求存在脱节，传统排名模型孤立评估商品，忽略了用户在搜索结果页面对多个商品进行比较的上下文。

Method: 提出Learning-to-Comparison-Shop (LTCS)系统，显式建模和学习用户的比较购物行为，通过离线实验和在线A/B测试验证。

Result: NDCG提升1.7%，预订转化率提升0.6%，显著优于现有先进方法，同时改善了用户体验。

Conclusion: LTCS系统通过建模比较购物行为有效解决了传统排名模型与用户实际购物行为的脱节问题，在业务指标和用户体验上均取得显著改善。

Abstract: In online marketplaces like Airbnb, users frequently engage in comparison shopping before making purchase decisions. Despite the prevalence of this behavior, a significant disconnect persists between mainstream e-commerce search engines and users' comparison needs. Traditional ranking models often evaluate items in isolation, disregarding the context in which users compare multiple items on a search results page. While recent advances in deep learning have sought to improve ranking accuracy, diversity, and fairness by encoding listwise context, the challenge of aligning search rankings with user comparison shopping behavior remains inadequately addressed. In this paper, we propose a novel ranking architecture - Learning-to-Comparison-Shop (LTCS) System - that explicitly models and learns users' comparison shopping behaviors. Through extensive offline and online experiments, we demonstrate that our approach yields statistically significant gains in key business metrics - improving NDCG by 1.7% and boosting booking conversion rate by 0.6% in A/B testing - while also enhancing user experience. We also compare our model against state-of-the-art approaches and demonstrate that LTCS significantly outperforms them.

</details>
