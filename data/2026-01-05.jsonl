{"id": "2601.00510", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00510", "abs": "https://arxiv.org/abs/2601.00510", "authors": ["Jetlir Duraj", "Ishita Khan", "Kilian Merkelbach", "Mehran Elyasi"], "title": "A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies", "comment": "9 pages, accepted at SIGIR eCom 2025", "summary": "Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries."}
{"id": "2601.00567", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00567", "abs": "https://arxiv.org/abs/2601.00567", "authors": ["Jeyun Lee", "Junhyoung Lee", "Wonbin Kweon", "Bowen Jin", "Yu Zhang", "Susik Yoon", "Dongha Lee", "Hwanjo Yu", "Jiawei Han", "Seongku Kang"], "title": "Improving Scientific Document Retrieval with Academic Concept Index", "comment": null, "summary": "Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance."}
{"id": "2601.00002", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.00002", "abs": "https://arxiv.org/abs/2601.00002", "authors": ["Tarek Al Mustafa"], "title": "From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories", "comment": "Master's thesis", "summary": "Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata."}
{"id": "2601.00094", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00094", "abs": "https://arxiv.org/abs/2601.00094", "authors": ["Ali Dasdan"], "title": "Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means", "comment": "17 pages, 2 figures", "summary": "The problem of finding the longest simple cycle in a directed graph is NP-hard, with critical applications in computational biology, scheduling, and network analysis. While polynomial-time approximation algorithms exist for restricted graph classes, general bounds remain loose or computationally expensive. In this paper, we exploit optimum cycle means (minimum and maximum cycle means), which are computable in strongly polynomial time, to derive both strict algebraic bounds and heuristic approximations for the weight and length of the longest simple cycle. We rigorously analyze the algebraic relationships between these mean statistics and the properties of longest cycles, and present dual results for shortest cycles. While the strict bounds provide polynomial-time computable constraints suitable for pruning search spaces in branch-and-bound algorithms, our proposed heuristic approximations offer precise estimates for the objective value. Experimental evaluation on ISCAS benchmark circuits demonstrates this trade-off: while the strict algebraic lower bounds are often loose (median 85--93% below true values), the heuristic approximations achieve median errors of only 6--14%. We also observe that maximum weight and maximum length cycles frequently coincide, suggesting that long cycles tend to accumulate large weights."}
{"id": "2601.00329", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00329", "abs": "https://arxiv.org/abs/2601.00329", "authors": ["Angshul Majumdar"], "title": "Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\\ell_1$ Relaxations", "comment": null, "summary": "We study coalition structure generation (CSG) when coalition values are not given but must be learned from episodic observations. We model each episode as a sparse linear regression problem, where the realised payoff \\(Y_t\\) is a noisy linear combination of a small number of coalition contributions. This yields a probabilistic CSG framework in which the planner first estimates a sparse value function from \\(T\\) episodes, then runs a CSG solver on the inferred coalition set. We analyse two estimation schemes. The first, Bayesian Greedy Coalition Pursuit (BGCP), is a greedy procedure that mimics orthogonal matching pursuit. Under a coherence condition and a minimum signal assumption, BGCP recovers the true set of profitable coalitions with high probability once \\(T \\gtrsim K \\log m\\), and hence yields welfare-optimal structures. The second scheme uses an \\(\\ell_1\\)-penalised estimator; under a restricted eigenvalue condition, we derive \\(\\ell_1\\) and prediction error bounds and translate them into welfare gap guarantees. We compare both methods to probabilistic baselines and identify regimes where sparse probabilistic CSG is superior, as well as dense regimes where classical least-squares approaches are competitive."}
{"id": "2601.00120", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.00120", "abs": "https://arxiv.org/abs/2601.00120", "authors": ["Hiram H. López", "Gretchen L. Matthews", "Daniel Valvo"], "title": "A repair scheme for a distributed storage system based on multivariate polynomials", "comment": null, "summary": "A distributed storage system stores data across multiple nodes, with the primary objective of enabling efficient data recovery even in the event of node failures. The main goal of an exact repair scheme is to recover the data from a failed node by accessing and downloading information from the rest of the nodes. In a groundbreaking paper, ~\\cite{GW} developed an exact repair scheme for a distributed storage system that is based on Reed-Solomon codes, which depend on single-variable polynomials. In these notes, we extend the repair scheme to the family of distributed storage systems based on Reed-Muller codes, which are linear codes based on multivariate polynomials. The repair scheme we propose repairs any single node failure and multiple node failures, provided the positions satisfy certain conditions."}
{"id": "2601.00098", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.00098", "abs": "https://arxiv.org/abs/2601.00098", "authors": ["Paraschos Koutris", "Stijn Vansummeren", "Qichen Wang", "Yisu Remy Wang", "Xiangyao Yu"], "title": "Database Theory in Action: Yannakakis' Algorithm", "comment": null, "summary": "Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research."}
{"id": "2601.00272", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00272", "abs": "https://arxiv.org/abs/2601.00272", "authors": ["Alexandr Andoni", "Themistoklis Haris", "Esty Kelman", "Krzysztof Onak"], "title": "Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search", "comment": "36 pages, 3 figures", "summary": "We study the Approximate Nearest Neighbor (ANN) problem under a powerful adaptive adversary that controls both the dataset and a sequence of $Q$ queries.\n  Primarily, for the high-dimensional regime of $d = ω(\\sqrt{Q})$, we introduce a sequence of algorithms with progressively stronger guarantees. We first establish a novel connection between adaptive security and \\textit{fairness}, leveraging fair ANN search to hide internal randomness from the adversary with information-theoretic guarantees. To achieve data-independent performance, we then reduce the search problem to a robust decision primitive, solved using a differentially private mechanism on a Locality-Sensitive Hashing (LSH) data structure. This approach, however, faces an inherent $\\sqrt{n}$ query time barrier. To break the barrier, we propose a novel concentric-annuli LSH construction that synthesizes these fairness and differential privacy techniques. The analysis introduces a new method for robustly releasing timing information from the underlying algorithm instances and, as a corollary, also improves existing results for fair ANN.\n  In addition, for the low-dimensional regime $d = O(\\sqrt{Q})$, we propose specialized algorithms that provide a strong ``for-all'' guarantee: correctness on \\textit{every} possible query with high probability. We introduce novel metric covering constructions that simplify and improve prior approaches for ANN in Hamming and $\\ell_p$ spaces."}
{"id": "2601.00447", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.00447", "abs": "https://arxiv.org/abs/2601.00447", "authors": ["Benjamin Cookson", "Nisarg Shah", "Ziqi Yu"], "title": "Unifying Proportional Fairness in Centroid and Non-Centroid Clustering", "comment": "Appeared in NeurIPS 2025", "summary": "Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study centroid clustering, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study non-centroid clustering, in which each data point's loss is determined by its maximum distance to any other data point in its cluster.\n  We generalize both paradigms to introduce semi-centroid clustering, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria -- the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case."}
{"id": "2601.00122", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.00122", "abs": "https://arxiv.org/abs/2601.00122", "authors": ["Eduardo Camps-Moreno", "Jun Bo Lau", "Hiram H. López", "Welington Santos"], "title": "The permutation group of Reed-Solomon codes over arbitrary points", "comment": null, "summary": "In this work, we prove that the permutation group of a Reed-Solomon code is given by the polynomials of degree one that leave the set of evaluation points invariant. Our results provide a straightforward proof of the well-known cases of the permutation group of the Reed-Solomon code when the set of evaluation points is the whole finite field or the multiplicative group."}
{"id": "2601.00208", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.00208", "abs": "https://arxiv.org/abs/2601.00208", "authors": ["David Lomet", "Rui Wang"], "title": "Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More", "comment": "6 pages, 4 figures", "summary": "A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed."}
{"id": "2601.00361", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00361", "abs": "https://arxiv.org/abs/2601.00361", "authors": ["Rachit Chhaya", "Anirban Dasgupta", "Dan Feldman", "Supratim Shit"], "title": "Deterministic Coreset for Lp Subspace", "comment": null, "summary": "We introduce the first iterative algorithm for constructing a $\\varepsilon$-coreset that guarantees deterministic $\\ell_p$ subspace embedding for any $p \\in [1,\\infty)$ and any $\\varepsilon > 0$. For a given full rank matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where $n \\gg d$, $\\mathbf{X}' \\in \\mathbb{R}^{m \\times d}$ is an $(\\varepsilon,\\ell_p)$-subspace embedding of $\\mathbf{X}$, if for every $\\mathbf{q} \\in \\mathbb{R}^d$, $(1-\\varepsilon)\\|\\mathbf{Xq}\\|_{p}^{p} \\leq \\|\\mathbf{X'q}\\|_{p}^{p} \\leq (1+\\varepsilon)\\|\\mathbf{Xq}\\|_{p}^{p}$. Specifically, in this paper, $\\mathbf{X}'$ is a weighted subset of rows of $\\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\\ell_p$ subspace embedding. For an error parameter $\\varepsilon$, our algorithm takes $O(\\mathrm{poly}(n,d,\\varepsilon^{-1}))$ time and returns a deterministic $\\varepsilon$-coreset, for $\\ell_p$ subspace embedding whose size is $O\\left(\\frac{d^{\\max\\{1,p/2\\}}}{\\varepsilon^{2}}\\right)$. Here, we remove the $\\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\\ell_p$ regression problem in a deterministic manner."}
{"id": "2601.00523", "categories": ["cs.GT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00523", "abs": "https://arxiv.org/abs/2601.00523", "authors": ["Andrés Fábrega", "James Austgen", "Samuel Breckenridge", "Jay Yu", "Amy Zhao", "Sarah Allen", "Aditya Saraf", "Ari Juels"], "title": "The CoinAlg Bind: Profitability-Fairness Tradeoffs in Collective Investment Algorithms", "comment": null, "summary": "Collective Investment Algorithms (CoinAlgs) are increasingly popular systems that deploy shared trading strategies for investor communities. Their goal is to democratize sophisticated -- often AI-based -- investing tools. We identify and demonstrate a fundamental profitability-fairness tradeoff in CoinAlgs that we call the CoinAlg Bind: CoinAlgs cannot ensure economic fairness without losing profit to arbitrage. We present a formal model of CoinAlgs, with definitions of privacy (incomplete algorithm disclosure) and economic fairness (value extraction by an adversarial insider). We prove two complementary results that together demonstrate the CoinAlg Bind. First, privacy in a CoinAlg is a precondition for insider attacks on economic fairness. Conversely, in a game-theoretic model, lack of privacy, i.e., transparency, enables arbitrageurs to erode the profitability of a CoinAlg. Using data from Uniswap, a decentralized exchange, we empirically study both sides of the CoinAlg Bind. We quantify the impact of arbitrage against transparent CoinAlgs. We show the risks posed by a private CoinAlg: Even low-bandwidth covert-channel information leakage enables unfair value extraction."}
{"id": "2601.00251", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.00251", "abs": "https://arxiv.org/abs/2601.00251", "authors": ["Kwonyeol Park", "Hyuckjin Choi", "Geonho Han", "Gyoseung Lee", "Yeonjoon Choi", "Sunwoo Park", "Junil Choi"], "title": "Evolution of UE in Massive MIMO Systems for 6G: From Passive to Active", "comment": "7 pages, 4 figures", "summary": "As wireless networks continue to evolve, stringent latency and reliability requirements and highly dynamic channels expose fundamental limitations of gNB-centric massive multiple-input multiple-output (mMIMO) architectures, motivating a rethinking of the user equipment (UE) role. In response, the UE is transitioning from a passive transceiver into an active entity that directly contributes to system-level performance. In this context, this article examines the evolving role of the UE in mMIMO systems during the transition from fifth-generation (5G) to sixth-generation (6G), bridging third generation partnership project (3GPP) standardization, device implementation, and architectural innovation. Through a chronological review of 3GPP Releases 15 to 19, we highlight the progression of UE functionalities from basic channel state information (CSI) reporting to artificial intelligence (AI) and machine learning (ML)-based CSI enhancement and UE-initiated beam management. We further examine key implementation challenges, including multi-panel UE (MPUE) architectures, on-device intelligent processing, and energy-efficient operation, and then discuss corresponding architectural innovations under practical constraints. Using digital-twin-based evaluations, we validate the impact of emerging UE-centric functionalities, illustrating that UE-initiated beam reporting improves throughput in realistic mobility scenarios, while a multi-panel architecture enhances link robustness compared with a single-panel UE."}
{"id": "2601.00304", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.00304", "abs": "https://arxiv.org/abs/2601.00304", "authors": ["Mouna Ammar", "Marvin Hofer", "Erhard Rahm"], "title": "Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches", "comment": null, "summary": "We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness."}
{"id": "2601.00768", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00768", "abs": "https://arxiv.org/abs/2601.00768", "authors": ["Mihail Stoian"], "title": "Mind the Gap. Doubling Constant Parametrization of Weighted Problems: TSP, Max-Cut, and More", "comment": "To appear at STACS 2026", "summary": "Despite much research, hard weighted problems still resist super-polynomial improvements over their textbook solution. On the other hand, the unweighted versions of these problems have recently witnessed the sought-after speedups. Currently, the only way to repurpose the algorithm of the unweighted version for the weighted version is to employ a polynomial embedding of the input weights. This, however, introduces a pseudo-polynomial factor into the running time, which becomes impractical for arbitrarily weighted instances.\n  In this paper, we introduce a new way to repurpose the algorithm of the unweighted problem. Specifically, we show that the time complexity of several well-known NP-hard problems operating over the $(\\min, +)$ and $(\\max, +)$ semirings, such as TSP, Weighted Max-Cut, and Edge-Weighted $k$-Clique, is proportional to that of their unweighted versions when the set of input weights has small doubling. We achieve this by a meta-algorithm that converts the input weights into polynomially bounded integers using the recent constructive Freiman's theorem by Randolph and Węgrzycki [ESA 2024] before applying the polynomial embedding."}
{"id": "2601.00381", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.00381", "abs": "https://arxiv.org/abs/2601.00381", "authors": ["Chong Huang", "Xuyang Chen", "Jingfu Li", "Pei Xiao", "Gaojie Chen", "Rahim Tafazolli"], "title": "Semantic Transmission Framework in Direct Satellite Communications", "comment": "5 pages", "summary": "Insufficient link budget has become a bottleneck problem for direct access in current satellite communications. In this paper, we develop a semantic transmission framework for direct satellite communications as an effective and viable solution to tackle this problem. To measure the tradeoffs between communication, computation, and generation quality, we introduce a semantic efficiency metric with optimized weights. The optimization aims to maximize the average semantic efficiency metric by jointly optimizing transmission mode selection, satellite-user association, ISL task migration, denoising steps, and adaptive weights, which is a complex nonlinear integer programming problem. To maximize the average semantic efficiency metric, we propose a decision-assisted REINFORCE++ algorithm that utilizes feasibility-aware action space and a critic-free stabilized policy update. Numerical results show that the proposed algorithm achieves higher semantic efficiency than baselines."}
{"id": "2601.00633", "categories": ["cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00633", "abs": "https://arxiv.org/abs/2601.00633", "authors": ["Satyam Singh", "Sai Niranjan Ramachandran"], "title": "KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees", "comment": null, "summary": "Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \\textbf{KELP} (\\textbf{K}elp \\textbf{E}volutionary \\textbf{L}og \\textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp"}
{"id": "2601.00435", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.00435", "abs": "https://arxiv.org/abs/2601.00435", "authors": ["Gabriel Sac Himelfarb", "Moshe Schwartz"], "title": "On the burst-covering radius of binary cyclic codes", "comment": "15 pages", "summary": "We define and study burst-covering codes. We provide some general bounds connecting the code parameters with its burst-covering radius. We then provide stronger bounds on the burst-covering radius of cyclic codes, by employing linear-feedback shift-register (LFSR) sequences. For the case of BCH codes we prove a new bound on pattern frequencies in LFSR sequences, which is of independent interest. Using this tool, we can bound the covering-radius of binary primitive BCH codes and Melas codes. We conclude with an efficient algorithm for burst-covering cyclic codes."}
{"id": "2601.00695", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.00695", "abs": "https://arxiv.org/abs/2601.00695", "authors": ["Chuanyi Lv", "Huan Li", "Dingyu Yang", "Zhongle Xie", "Lu Chen", "Christian S. Jensen"], "title": "DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data", "comment": "This paper has been accepted for publication in PVLDB Volume 19(VLDB 2026)", "summary": "With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail."}
{"id": "2601.00549", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00549", "abs": "https://arxiv.org/abs/2601.00549", "authors": ["Zhiheng Guo", "Zhaoyang Liu", "Zihan Cen", "Chenyuan Feng", "Xinghua Sun", "Xiang Chen", "Tony Q. S. Quek", "Xijun Wang"], "title": "CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge", "comment": "7 pages, 3 figures, 1 algorithm", "summary": "The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings."}
{"id": "2601.00712", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.00712", "abs": "https://arxiv.org/abs/2601.00712", "authors": ["Bernhard C. Geiger", "Tobias Koch", "Josipa Mihaljević", "Maximilian Toller"], "title": "Universal Outlier Hypothesis Testing via Mean- and Median-Based Tests", "comment": "8 pages, 3 figures; accepted for publication at the International Zurich Seminar on Information and Communication", "summary": "Universal outlier hypothesis testing refers to a hypothesis testing problem where one observes a large number of length-$n$ sequences -- the majority of which are distributed according to the typical distribution $π$ and a small number are distributed according to the outlier distribution $μ$ -- and one wishes to decide, which of these sequences are outliers without having knowledge of $π$ and $μ$. In contrast to previous works, in this paper it is assumed that both the number of observation sequences and the number of outlier sequences grow with the sequence length. In this case, the typical distribution $π$ can be estimated by computing the mean over all observation sequences, provided that the number of outlier sequences is sublinear in the total number of sequences. It is demonstrated that, in this case, one can achieve the error exponent of the maximum likelihood test that has access to both $π$ and $μ$. However, this mean-based test performs poorly when the number of outlier sequences is proportional to the total number of sequences. For this case, a median-based test is proposed that estimates $π$ as the median of all observation sequences. It is demonstrated that the median-based test achieves again the error exponent of the maximum likelihood test that has access to both $π$ and $μ$, but only with probability approaching one. To formalize this case, the typical error exponent -- similar to the typical random coding exponent introduced in the context of random coding for channel coding -- is proposed."}
