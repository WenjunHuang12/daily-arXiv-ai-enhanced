<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 4]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [On the Existence of Fair Allocations for Goods and Chores under Dissimilar Preferences](https://arxiv.org/abs/2511.03810)
*Egor Gagushin,Marios Mertzanidis,Alexandros Psomas*

Main category: cs.GT

TL;DR: 本文解决了Gorantla等人提出的主要开放问题，为任意数量的组和物品类型推导出了μ的显式上界，并引入了更简单但强大的技术，可扩展到杂务和连续域。


<details>
  <summary>Details</summary>
Motivation: 解决Gorantla等人[GMV23]提出的主要开放问题，即对于任意数量的组和物品类型，找到保证存在无嫉妒分配的显式上界μ。

Method: 引入了一种显著更简单但强大的技术，不仅为不可分割物品提供构造性保证，还自然地扩展到杂务和连续域。

Result: 为任意数量的组和物品类型推导出了μ的显式上界，解决了Gorantla等人的开放问题。

Conclusion: 新方法不仅解决了不可分割物品的公平分配问题，还扩展到杂务和连续域，为相关公平分配设置（如切蛋糕）带来了新结果。

Abstract: We study the fundamental problem of fairly allocating a multiset
$\mathcal{M}$ of $t$ types of indivisible items among $d$ groups of agents,
where all agents within a group have identical additive valuations. Gorantla et
al. [GMV23] showed that for every such instance, there exists a finite number
$\mu$ such that, if each item type appears at least $\mu$ times, an envy-free
allocation exists. Their proof is non-constructive and only provides explicit
upper bounds on $\mu$ for the cases of two groups ($d=2$) or two item types
($t=2$).
  In this work, we resolve one of the main open questions posed by Gorantla et
al. [GMV23] by deriving explicit upper bounds on $\mu$ that hold for arbitrary
numbers of groups and item types. We introduce a significantly simpler, yet
powerful technique that not only yields constructive guarantees for indivisible
goods but also extends naturally to chores and continuous domains, leading to
new results in related fair division settings such as cake cutting.

</details>


### [2] [The Complexity of Equilibrium Refinements in Potential Games](https://arxiv.org/abs/2511.03968)
*Ioannis Anagnostides,Maria-Florina Balcan,Kiriaki Fragkia,Tuomas Sandholm,Emanuel Tewolde,Brian Hu Zhang*

Main category: cs.GT

TL;DR: 本文解决了潜在博弈中均衡精炼计算复杂性的基本开放问题，证明了不同博弈表示下纯策略完美均衡的PLS完备性，以及扩展式博弈中正规形式适当均衡的FIXP_a完备性。


<details>
  <summary>Details</summary>
Motivation: 均衡精炼的计算复杂性在算法博弈论中一直是个核心问题，但在经典的潜在博弈类中仍保持开放状态，本文旨在填补这一基础空白。

Method: 通过分析不同博弈表示（扩展式博弈、多面体博弈等）下的均衡精炼计算问题，建立复杂性结果，并在结构化博弈类中设计多项式时间算法。

Result: 证明了纯策略完美均衡在多种博弈表示下是PLS完备的；正规形式适当均衡在扩展式博弈中是FIXP_a完备的；在多面体潜在博弈中计算正规形式适当均衡是NP-hard和coNP-hard的；在对称网络拥塞和对称拟阵拥塞博弈中，完美纯策略均衡可在多项式时间内计算。

Conclusion: 本文系统性地解决了潜在博弈中均衡精炼的计算复杂性，揭示了不同博弈类和均衡概念之间的复杂性分离，为算法博弈论提供了重要的理论基础。

Abstract: The complexity of computing equilibrium refinements has been at the forefront
of algorithmic game theory research, but it has remained open in the seminal
class of potential games; we close this fundamental gap in this paper.
  We first establish that computing a pure-strategy perfect equilibrium is
$\mathsf{PLS}$-complete under different game representations -- including
extensive-form games and general polytope games, thereby being polynomial-time
equivalent to pure Nash equilibria. For normal-form proper equilibria, our main
result is that a perturbed (proper) best response can be computed efficiently
in extensive-form games. As a byproduct, we establish
$\mathsf{FIXP}_a$-completeness of normal-form proper equilibria in
extensive-form games, resolving a long-standing open problem. In stark
contrast, we show that computing a normal-form proper equilibrium in polytope
potential games is both $\mathsf{NP}$-hard and $\mathsf{coNP}$-hard.
  We next turn to more structured classes of games, namely symmetric network
congestion and symmetric matroid congestion games. For both classes, we show
that a perfect pure-strategy equilibrium can be computed in polynomial time,
strengthening the existing results for pure Nash equilibria. On the other hand,
we establish that, for a certain class of potential games, there is an
exponential separation in the length of the best-response path between perfect
and Nash equilibria.
  Finally, for mixed strategies, we prove that computing a point geometrically
near a perfect equilibrium requires a doubly exponentially small perturbation
even in $3$-player potential games in normal form. On the flip side, in the
special case of polymatrix potential games, we show that equilibrium
refinements are amenable to perturbed gradient descent dynamics, thereby
belonging to the complexity class $\mathsf{CLS}$.

</details>


### [3] [Fraud-Proof Revenue Division on Subscription Platforms](https://arxiv.org/abs/2511.04465)
*Abheek Ghosh,Tzeh Yuan Neoh,Nicholas Teh,Giannis Tyrovolas*

Main category: cs.GT

TL;DR: 该论文研究订阅制平台中的收入分配机制，提出了三种防操纵公理，发现现有流媒体平台广泛使用的机制不仅无法防止欺诈，还使检测操纵变得计算困难，并提出了满足所有防操纵公理的新规则ScaledUserProp。


<details>
  <summary>Details</summary>
Motivation: 现有欺诈检测方法主要依赖机器学习，与恶意行为者进行持续的军备竞赛，需要探索能够从根本上抑制操纵的收入分配机制。

Method: 形式化三种防操纵公理，分析现有规则是否满足这些公理，提出新的ScaledUserProp规则，并在真实和合成流媒体数据上进行实验验证。

Result: 发现流媒体平台广泛使用的机制无法防止欺诈且检测操纵计算困难，而ScaledUserProp规则满足所有防操纵公理，在实验中表现出比现有规则更公平的特性。

Conclusion: ScaledUserProp是一个满足所有防操纵公理的公平替代方案，能够从根本上抑制平台操纵行为，避免与恶意行为者的持续军备竞赛。

Abstract: We study a model of subscription-based platforms where users pay a fixed fee
for unlimited access to content, and creators receive a share of the revenue.
Existing approaches to detecting fraud predominantly rely on machine learning
methods, engaging in an ongoing arms race with bad actors. We explore revenue
division mechanisms that inherently disincentivize manipulation. We formalize
three types of manipulation-resistance axioms and examine which existing rules
satisfy these. We show that a mechanism widely used by streaming platforms, not
only fails to prevent fraud, but also makes detecting manipulation
computationally intractable. We also introduce a novel rule, ScaledUserProp,
that satisfies all three manipulation-resistance axioms. Finally, experiments
with both real-world and synthetic streaming data support ScaledUserProp as a
fairer alternative compared to existing rules.

</details>


### [4] [Fisher Meets Lindahl: A Unified Duality Framework for Market Equilibrium](https://arxiv.org/abs/2511.04572)
*Yixin Tao,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 提出了一个统一的对偶框架，将公共品的Lindahl均衡与私人品的Fisher市场均衡联系起来，通过交换分配和价格的角色建立对应关系。


<details>
  <summary>Details</summary>
Motivation: Fisher市场均衡已有深入研究，但Lindahl均衡的理论基础相对薄弱，需要统一框架来填补计算和动态分析方面的空白。

Method: 建立对偶框架，将Lindahl均衡映射到具有对偶效用的Fisher市场均衡，利用间接效用函数交换分配和价格的角色。

Result: 证明了Lindahl均衡在凹齐次效用下最大化Nash社会福利，在凹非齐次效用下达到(1/e)^{1/e}近似比；扩展了市场动态分析；提出了避免"极点"问题的私人劳务市场规划。

Conclusion: 对偶框架为Lindahl均衡提供了统一的理论基础，填补了计算和动态分析空白，并获得了Fisher市场均衡的新见解和发展。

Abstract: The Fisher market equilibrium for private goods and the Lindahl equilibrium
for public goods are classic and fundamental solution concepts for market
equilibria. While Fisher market equilibria have been well-studied, the
theoretical foundations for Lindahl equilibria remain substantially
underdeveloped.
  In this work, we propose a unified duality framework for market equilibria.
We show that Lindahl equilibria of a public goods market correspond to Fisher
market equilibria in a dual Fisher market with dual utilities, and vice versa.
The dual utility is based on the indirect utility, and the correspondence
between the two equilibria works by exchanging the roles of allocations and
prices.
  Using the duality framework, we address the gaps concerning the computation
and dynamics for Lindahl equilibria and obtain new insights and developments
for Fisher market equilibria. First, we leverage this duality to analyze
welfare properties of Lindahl equilibria. For concave homogeneous utilities, we
prove that a Lindahl equilibrium maximizes Nash Social Welfare (NSW). For
concave non-homogeneous utilities, we show that a Lindahl equilibrium achieves
$(1/e)^{1/e}$ approximation to the optimal NSW, and the approximation ratio is
tight. Second, we apply the duality framework to market dynamics, including
proportional response dynamics (PRD) and t\^atonnement. We obtain new market
dynamics for the Lindahl equilibria from market dynamics in the dual Fisher
market. We also use duality to extend PRD to markets with total complements
utilities, the dual class of gross substitutes utilities. Finally, we apply the
duality framework to markets with chores. We propose a program for private
chores for general convex homogeneous disutilities that avoids the "poles"
issue, whose KKT points correspond to Fisher market equilibria. We also
initiate the study of the Lindahl equilibrium for public chores.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [5] [Attractors Is All You Need: Parity Games In Polynomial Time](https://arxiv.org/abs/2511.03752)
*Rick van der Heijden*

Main category: cs.DS

TL;DR: 提出了一个多项式时间算法解决奇偶博弈，运行时间为O(n²·(n+m))，结束了数十年的研究。


<details>
  <summary>Details</summary>
Motivation: 解决奇偶博弈问题，寻找多项式时间算法，结束长期的研究探索。

Method: 引入新型吸引子，只移除已确定获胜者的区域，能够保证找到奇偶博弈的最小支配域。

Result: 算法在多项式时间内运行，能够将图完全剥离，解决了奇偶博弈问题。

Conclusion: 成功开发出多项式时间算法解决奇偶博弈，为这一长期开放问题提供了解决方案。

Abstract: This paper provides a polynomial-time algorithm for solving parity games that
runs in $\mathcal{O}(n^{2}\cdot(n + m))$ time-ending a search that has taken
decades. Unlike previous attractor-based algorithms, the presented algorithm
only removes regions with a determined winner. The paper introduces a new type
of attractor that can guarantee finding the minimal dominion of a parity game.
The attractor runs in polynomial time and can peel the graph empty.

</details>


### [6] [Multi-Pass Streaming Lower Bounds for Uniformity Testing](https://arxiv.org/abs/2511.03960)
*Qian Li,Xin Lyu*

Main category: cs.DS

TL;DR: 本文证明了在大小为2m的域上进行均匀性测试的多通流算法下界。任何使用空间s和ℓ通且具有常数优势的算法必须满足snℓ = Ω̃(m/ε²)的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 将Diakonikolas等人(2019)的单通下界扩展到多通情况，研究流算法在均匀性测试问题中的基本限制。

Method: 开发了一种混合论证方法，将流算法问题归约到双玩家通信问题。证明的关键在于识别出困难来源是偏置方向的不确定性，而非碰撞位置。

Result: 建立了多通流算法的空间-样本-通数权衡下界：snℓ = Ω̃(m/ε²)，扩展了之前单通情况的结果。

Conclusion: 本文的技术可能对其他具有随机输入的流问题具有独立应用价值，为理解流算法在分布测试中的基本限制提供了新的视角。

Abstract: We prove multi-pass streaming lower bounds for uniformity testing over a
domain of size $2m$. The tester receives a stream of $n$ i.i.d. samples and
must distinguish (i) the uniform distribution on $[2m]$ from (ii) a
Paninski-style planted distribution in which, for each pair $(2i-1,2i)$, the
probabilities are biased left or right by $\epsilon/2m$. We show that any
$\ell$-pass streaming algorithm using space $s$ and achieving constant
advantage must satisfy the tradeoff $sn\ell=\tilde{\Omega}(m/\epsilon^2)$. This
extends the one-pass lower bound of Diakonikolas, Gouleakis, Kane, and Rao
(2019) to multiple passes.
  Our proof has two components. First, we develop a hybrid argument, inspired
by Dinur (2020), that reduces streaming to two-player communication problems.
This reduction relies on a new perspective on hardness: we identify the source
of hardness as uncertainty in the bias directions, rather than the collision
locations. Second, we prove a strong lower bound for a basic two-player
communication task, in which Alice and Bob must decide whether two random sign
vectors $Y^a,Y^b\in\{\pm 1\}^m$ are independent or identical, yet they cannot
observe the signs directly--only noisy local views of each coordinate. Our
techniques may be of independent use for other streaming problems with
stochastic inputs.

</details>


### [7] [HART: A Hybrid Addressing Scheme for Self-Balancing Binary Search Trees in Phase Change Memory (PCM)](https://arxiv.org/abs/2511.03994)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.DS

TL;DR: HART是一种用于自平衡二叉搜索树的混合寻址方案，通过结合DFATGray码寻址和线性寻址来优化相变存储器的特性，减少位翻转并提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前内存密集型算法忽视了相变存储器的耐久性限制和写入不对称性，而自平衡二叉搜索树在开发时未考虑PCM的独特属性，导致潜在性能下降。

Method: 提出HART混合寻址方案，对深层节点使用DFATGray码寻址以减少频繁旋转时的位翻转，对浅层节点使用线性寻址以保持计算简单性。

Result: 在PCM感知的AVL树上的实验结果显示，位翻转显著减少，从而提高了耐久性、增加了寿命，并降低了写入能量和延迟。

Conclusion: HART在不增加显著计算开销的情况下，为大数据应用提供了高效的解决方案，优化了PCM的性能特性。

Abstract: As DRAM and other transistor-based memory technologies approach their
scalability limits, alternative storage solutions like Phase-Change Memory
(PCM) are gaining attention for their scalability, fast access times, and zero
leakage power. However, current memory-intensive algorithms, especially those
used in big data systems, often overlook PCM's endurance limitations (10^6 to
10^8 writes before degradation) and write asymmetry. Self-balancing binary
search trees (BSTs), which are widely used for large-scale data management,
were developed without considering PCM's unique properties, leading to
potential performance degradation. This paper introduces HART, a novel hybrid
addressing scheme for self-balancing BSTs, designed to optimize PCM's
characteristics. By combining DFATGray code addressing for deeper nodes with
linear addressing for shallower nodes, HART balances reduced bit flips during
frequent rotations at deeper levels with computational simplicity at shallow
levels. Experimental results on PCM-aware AVL trees demonstrate significant
improvements in performance, with a reduction in bit flips leading to enhanced
endurance, increased lifetime, and lower write energy and latency. Notably,
these benefits are achieved without imposing substantial computational
overhead, making HART an efficient solution for big data applications.

</details>


### [8] [Depth-13 Sorting Networks for 28 Channels](https://arxiv.org/abs/2511.04107)
*Chengu Wang*

Main category: cs.DS

TL;DR: 提出了27和28通道排序网络的新深度上界，从14改进到13


<details>
  <summary>Details</summary>
Motivation: 改进排序网络的深度上界，特别是在27和28通道的情况下

Method: 利用反射对称性构建28通道网络，结合16和12通道网络的高质量前缀，逐层贪婪扩展，并使用SAT求解器完成剩余层

Result: 成功将27和28通道排序网络的深度从14改进到13

Conclusion: 通过结合多种技术方法，成功降低了排序网络的深度上界

Abstract: We establish new depth upper bounds for sorting networks on 27 and 28
channels, improving the previous best bound of 14 to 13. Our 28-channel network
is constructed with reflectional symmetry by combining high-quality prefixes of
16- and 12-channel networks, extending them greedily one comparator at a time,
and using a SAT solver to complete the remaining layers.

</details>


### [9] [Counting Patterns in Degenerate Graphs in Constant Space](https://arxiv.org/abs/2511.04258)
*Balagopal Komarath,Anant Kumar,Akash Pareek*

Main category: cs.DS

TL;DR: 本文引入新的图参数DAG treedepth，开发了使用常数空间的算法来计数同态、子图同构和诱导子图同构，相比现有方法在空间复杂度上有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于DAG treewidth的方法虽然高效但需要多项式空间，希望开发空间复杂度更低的算法来处理图计数问题。

Method: 引入DAG treedepth参数，设计基于分治策略的常数空间算法，同时优化DAG treewidth方法的运行时间。

Result: 实现了常数空间的子图计数算法；对最多9个顶点的模式图，可在O(n³)时间内计数诱导子图；对最多11个顶点的模式图，可在二次时间内计数诱导子图。

Conclusion: DAG treedepth参数为图计数问题提供了空间高效的解决方案，同时DAG treewidth方法在时间复杂度上也有进一步优化。

Abstract: For an arbitrary, fixed graph (pattern graph), we study the algorithmic
complexity of counting homomorphisms, subgraph isomorphisms, and induced
subgraph isomorphisms from the pattern graph to $n$-vertex, $d$-degenerate
graphs as input. Recent work by Bressan (Algorithmica, 2021) has shown that
this problem has efficient dynamic programming algorithms using a graph
parameter called DAG treewidth. Bressan used DAG treewidth to design a fast
algorithm for counting homomorphisms, subgraph isomorphisms, and induced
subgraph isomorphisms that use polynomial space. Bera, Gishboliner, Levanzov,
Seshadhri, and Shapira (SODA, 2021) provided a characterization of graphs with
DAG treewidth one.
  In this paper, we introduce a new graph parameter called DAG treedepth and
show that it yields efficient divide and conquer algorithms that use only
constant space (in the unit-cost RAM model). Specifically, we show:
  An algorithm for counting subgraphs isomorphic to sparse pattern graphs using
only constant space.
  We derive an induced minor-based characterization for graphs of DAG treedepth
up to two.
  For pattern graphs upto nine vertices, the induced subgraphs can be counted
in $O(n^3)$ time using constant space.
  An algorithm for counting induced subgraphs that matches the running time
given by Bressan but only uses constant space.
  Apart from the DAG treedepth result, we also focus on DAG treewidth. For DAG
treewidth, we show that we can count homomorphisms, subgraph isomorphisms, and
induced subgraph isomorphisms faster than Bressan's algorithm (2021). We
further show that for all pattern graphs up to 11 vertices, we can count
induced subgraphs in quadratic time.

</details>


### [10] [Estimating Hitting Times Locally At Scale](https://arxiv.org/abs/2511.04343)
*Themistoklis Haris,Fabian Spaeh,Spyros Dragazis,Charalampos Tsourakakis*

Main category: cs.DS

TL;DR: 本文提出了两种局部算法来估计图中节点间的命中时间，无需访问完整图结构，解决了传统全局方法的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 命中时间是随机过程中的基本距离度量，在中心性分析、推荐系统和流行病学中有广泛应用。传统全局方法存在可扩展性问题，需要开发局部算法来估计节点间的命中时间。

Method: 第一种算法利用两个独立随机游走相遇时间截断命中时间计算，通过Kronecker积图和马尔可夫链Chernoff界分析。第二种算法扩展了Peng等人的工作，引入谱截断技术的创新适配来处理命中时间的不对称性。

Result: 除了算法上界外，还提供了紧渐近下界，并揭示了命中时间估计与分布测试之间的联系。在真实和合成数据上的实验验证了算法有效性。

Conclusion: 开发的局部算法能够高效估计命中时间，克服了全局方法的可扩展性限制，为大规模网络分析提供了实用工具。

Abstract: Hitting times provide a fundamental measure of distance in random processes,
quantifying the expected number of steps for a random walk starting at node $u$
to reach node $v$. They have broad applications across domains such as network
centrality analysis, ranking and recommendation systems, and epidemiology. In
this work, we develop local algorithms for estimating hitting times between a
pair of vertices $u,v$ without accessing the full graph, overcoming scalability
issues of prior global methods. Our first algorithm uses the key insight that
hitting time computations can be truncated at the meeting time of two
independent random walks from $u$ and $v$. This leads to an efficient estimator
analyzed via the Kronecker product graph and Markov Chain Chernoff bounds. We
also present an algorithm extending the work of [Peng et al.; KDD 2021], that
introduces a novel adaptation of the spectral cutoff technique to account for
the asymmetry of hitting times. This adaptation captures the directionality of
the underlying random walk and requires non-trivial modifications to ensure
accuracy and efficiency. In addition to the algorithmic upper bounds, we also
provide tight asymptotic lower bounds. We also reveal a connection between
hitting time estimation and distribution testing, and validate our algorithms
using experiments on both real and synthetic data.

</details>


### [11] [A Polynomial-Time Algorithm for the Next-to-Shortest Path Problem on Positively Weighted Directed Graphs](https://arxiv.org/abs/2511.04345)
*Kuowen Chen,Nicole Wein,Yiran Zhang*

Main category: cs.DS

TL;DR: 本文解决了有向图中正边权重的次短路径问题，这是一个近30年未解决的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 次短路径问题自1996年提出以来，对于有向图中非负边权重的情况已被证明是NP完全问题，但对于正边权重的情况一直未解决。之前的工作已经为无向图和平面有向图开发了多项式时间算法。

Method: 作者提供了一种针对有向图中正边权重次短路径问题的算法。

Result: 该算法成功解决了这个近30年的开放性问题，为有向图中正边权重的次短路径问题提供了解决方案。

Conclusion: 这项工作最终解决了有向图中正边权重次短路径问题的计算复杂性，填补了该领域的重要空白。

Abstract: Given a graph and a pair of terminals $s$, $t$, the next-to-shortest path
problem asks for an $s\!\to \!t$ (simple) path that is shortest among all not
shortest $s\!\to \!t$ paths (if one exists). This problem was introduced in
1996, and soon after was shown to be NP-complete for directed graphs with
non-negative edge weights, leaving open the case of positive edge weights.
Subsequent work investigated this open question, and developed polynomial-time
algorithms for the cases of undirected graphs and planar directed graphs. In
this work, we resolve this nearly 30-year-old open problem by providing an
algorithm for the next-to-shortest path problem on directed graphs with
positive edge weights.

</details>


### [12] [Free-order secretary for two-sided independence systems](https://arxiv.org/abs/2511.04390)
*Kristóf Bérczi,Vasilis Livanos,José A. Soto,Victor Verdugo*

Main category: cs.DS

TL;DR: 提出了一个统一的二分图框架来研究Matroid Secretary问题，设计了针对k-增长系统的Ω(1/k²)竞争比算法，并扩展到代理到达模型和多物品选择场景。


<details>
  <summary>Details</summary>
Motivation: 统一和扩展已知的Matroid Secretary问题表述，包括二分图匹配、拟阵交和随机顺序拟阵秘书问题，研究自由顺序设置下的算法设计。

Method: 引入二分图框架和k-增长系统概念，利用核心引理设计自适应算法，扩展到代理到达模型和多物品选择场景。

Result: 为k-拟阵交设计了Ω(1/k²)竞争比算法，为k-增长系统在边到达模型提供Ω(1/k²)竞争比，在代理到达模型提供Ω(β/k²)竞争比，其中β是特定算法的竞争比。

Conclusion: 提出的框架统一了多种Matroid Secretary问题变体，核心引理和k-增长系统概念为设计竞争比算法提供了通用工具，结果可扩展到更一般的约束设置。

Abstract: The Matroid Secretary Problem is a central question in online optimization,
modeling sequential decision-making under combinatorial constraints. We
introduce a bipartite graph framework that unifies and extends several known
formulations, including the bipartite matching, matroid intersection, and
random-order matroid secretary problems. In this model, elements form a
bipartite graph between agents and items, and the objective is to select a
matching that satisfies feasibility constraints on both sides, given by two
independence systems.
  We study the free-order setting, where the algorithm may adaptively choose
the next element to reveal. For $k$-matroid intersection, we leverage a core
lemma by (Feldman, Svensson and Zenklusen, 2022) to design an
$\Omega(1/k^2)$-competitive algorithm, extending known results for single
matroids. Building on this, we identify the structural property underlying our
approach and introduce $k$-growth systems. We establish a generalized core
lemma for $k$-growth systems, showing that a suitably defined set of critical
elements retains a $\Omega(1/k^2)$ fraction of the optimal weight. Using this
lemma, we extend our $\Omega(1/k^2)$-competitive algorithm to $k$-growth
systems for the edge-arrival model.
  We then study the agent-arrival model, which presents unique challenges to
our framework. We extend the core lemma to this model and then apply it to
obtain an $\Omega(\beta/k^2)$-competitive algorithm for $k$-growth systems,
where $\beta$ denotes the competitiveness of a special type of order-oblivious
algorithm for the item-side constraint. Finally, we relax the matching
assumption and extend our results to the case of multiple item selection, where
agents have individual independence systems coupled by a global item-side
constraint. We obtain constant-competitive algorithms for fundamental cases
such as partition matroids and $k$-matching constraints.

</details>


### [13] [Online Algorithms for Repeated Optimal Stopping: Achieving Both Competitive Ratio and Regret Bounds](https://arxiv.org/abs/2511.04484)
*Tsubasa Harada,Yasushi Kawase,Hanna Sumita*

Main category: cs.DS

TL;DR: 提出了一个解决重复最优停止问题的通用算法框架，该框架在每轮保证竞争比的同时实现跨轮次的次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 将经典最优停止问题扩展到重复设置，旨在设计既能保证每轮竞争比又能实现跨轮次低遗憾的算法。

Method: 动态选择算法：在每轮从两个候选算法中选择——基于历史观测的经验最优算法和具有理论竞争比保证的样本算法。

Result: 算法在每轮表现不低于基线样本算法，总遗憾上界为$\tilde{O}(\sqrt{T})$。应用于先知不等式、秘书问题等典型问题，在重复先知不等式中从第二轮起达到1/2竞争比和$\tilde{O}(\sqrt{T})$遗憾。

Conclusion: 该框架具有广泛适用性，且$\Omega(\sqrt{T})$的遗憾下界表明算法在轮数方面几乎最优。

Abstract: We study the repeated optimal stopping problem, which generalizes the
classical optimal stopping problem with an unknown distribution to a setting
where the same problem is solved repeatedly over $T$ rounds. In this framework,
we aim to design algorithms that guarantee a competitive ratio in each round
while also achieving sublinear regret across all rounds.
  Our primary contribution is a general algorithmic framework that achieves
these objectives simultaneously for a wide array of repeated optimal stopping
problems. The core idea is to dynamically select an algorithm for each round,
choosing between two candidates: (1) an empirically optimal algorithm derived
from the history of observations, and (2) a sample-based algorithm with a
proven competitive ratio guarantee. Based on this approach, we design an
algorithm that performs no worse than the baseline sample-based algorithm in
every round, while ensuring that the total regret is bounded by
$\tilde{O}(\sqrt{T})$.
  We demonstrate the broad applicability of our framework to canonical
problems, including the prophet inequality, the secretary problem, and their
variants under adversarial, random, and i.i.d. input models. For example, for
the repeated prophet inequality problem, our method achieves a
$1/2$-competitive ratio from the second round on and an $\tilde{O}(\sqrt{T})$
regret. Furthermore, we establish a regret lower bound of $\Omega(\sqrt{T})$
even in the i.i.d. model, confirming that our algorithm's performance is almost
optimal with respect to the number of rounds.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [14] [Environment Division Multiple Access (EDMA): A Feasibility Study via Pinching Antennas](https://arxiv.org/abs/2511.03820)
*Zhiguo Ding,Robert Schober,H. V. Poor*

Main category: cs.IT

TL;DR: 提出了一种名为环境分割多址接入(EDMA)的新多址技术，利用无线传播环境的动态特性，通过捏合天线智能重构多用户传播环境，无需复杂信号处理即可增强目标接收器信号并抑制多址干扰。


<details>
  <summary>Details</summary>
Motivation: 利用无线传播环境的动态特性作为多址技术的基础，避免传统多址技术中复杂的信号处理需求，如预编码、波束成形或多用户检测。

Method: 采用捏合天线来重构视距链路，通过将捏合天线放置在特定位置来有意阻断干扰链路。开发了两种低复杂度算法分别用于上行和下行传输。

Result: 推导了EDMA相比传统多址的遍历和速率增益的闭式表达式，以及EDMA实现更大瞬时和速率的概率。仿真结果表明所提算法相比穷举搜索具有最优性。

Conclusion: EDMA在支持多用户通信方面具有显著潜力，通过智能重构传播环境可以有效提升系统性能。

Abstract: This paper exploits the dynamic features of wireless propagation environments
as the basis for a new multiple access technique, termed environment division
multiple access (EDMA). In particular, with the proposed
pinching-antenna-assisted EDMA, the multi-user propagation environment is
intelligently reconfigured to improve signal strength at intended receivers and
simultaneously suppress multiple-access interference, without requiring complex
signal processing, e.g., precoding, beamforming, or multi-user detection. The
key to creating a favorable propagation environment is to utilize the
capability of pinching antennas to reconfigure line-of-sight (LoS) links, e.g.,
pinching antennas are placed at specific locations, such that interference
links are blocked on purpose. Based on a straightforward choice of
pinching-antenna locations, the ergodic sum-rate gain of EDMA over conventional
multiple access and the probability that EDMA achieves a larger instantaneous
sum rate than the considered benchmarking scheme are derived in closed form.
The obtained analytical results demonstrate the significant potential of EDMA
for supporting multi-user communications. Furthermore, pinching antenna
location optimization is also investigated, since the locations of pinching
antennas are critical for reconfiguring LoS links and large-scale path losses.
Two low-complexity algorithms are developed for uplink and downlink
transmission, respectively, and simulation results are provided to show their
optimality in comparison to exhaustive searches.

</details>


### [15] [Which Similarity-Sensitive Entropy?](https://arxiv.org/abs/2511.03849)
*Phuc Nguyen,Josiah Couch,Rahul Bansal,Alexandra Morgan,Chris Tam,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.IT

TL;DR: 本文比较了Leinster-Cobbold-Reeve (LCR)相似性敏感熵和Vendi评分(VS)两种方法，通过概念、分析和实验验证发现两者在量级上可能相差数个数量级，并能捕获系统的互补信息。


<details>
  <summary>Details</summary>
Motivation: 量化系统时测量熵是关键步骤，传统熵度量只捕获元素频率信息，而LCR和VS方法能捕获元素间相似性和差异性的丰富信息，但两者比较和选择标准尚不明确。

Method: 使用概念分析、理论证明和53个机器学习数据集的实验验证，引入"半距离"概念来参数化相似性缩放的影响，比较LCR和VS在不同Rényi-Hill阶参数下的表现。

Result: LCR和VS在量级上可能相差数个数量级，能捕获互补信息；VS为LCR提供上界；两种方法都依赖于相似性缩放方式；VS仅在特定情况下更优。

Conclusion: 当将元素解释为更基本"原始元素"的线性组合或系统具有量子力学特性时，VS更优；在更广泛情况下，为捕获相似性编码的丰富信息，LCR更受青睐；在某些半距离下两种方法可互补使用。

Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon
entropy and other traditional entropy measures capture only the information
encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,
and Reeve (LCR) introduced a method that also captures the rich information
encoded in the similarities and differences among elements, yielding
similarity-sensitive entropy. More recently, the Vendi score (VS) was
introduced as an alternative, raising the question of how LCR and VS compare,
and which is preferable. Here we address these questions conceptually,
analytically, and experimentally, using 53 machine-learning datasets. We show
that LCR and VS can differ by orders of magnitude and can capture complementary
information about a system, except in limiting cases. We demonstrate that both
LCR and VS depend on how similarities are scaled and introduce the concept of
``half distance'' to parameterize this dependence. We prove that VS provides an
upper bound on LCR for several values of the R\'enyi-Hill order parameter and
conjecture that this bound holds for all values. We conclude that VS is
preferable only when interpreting elements as linear combinations of a more
fundamental set of ``ur-elements'' or when the system or dataset possesses a
quantum-mechanical character. In the broader circumstance where one seeks
simply to capture the rich information encoded by similarity, LCR is favored;
nevertheless, for certain half-distances the two methods can complement each
other.

</details>


### [16] [Efficient and rate-optimal list-decoding in the presence of minimal feedback: Weldon and Slepian-Wolf in sheep's clothing](https://arxiv.org/abs/2511.04088)
*Pranav Joshi,Daniel McMorrow,Yihan Zhang,Amitalok J. Budkuley,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 本文提出了第一个适用于任意q≥2的列表可译码方案，在存在低速率反馈的情况下，能够以接近信息论最优的速率进行通信，同时确保接收方能够从一个小列表中推断出发送方的消息。


<details>
  <summary>Details</summary>
Motivation: 现有方案主要针对大q值，缺乏适用于任意q≥2的高效编码/解码方案。本文旨在填补这一空白，利用低速率反馈实现接近信息论最优的通信速率。

Method: 设计了一种最小反馈方案，利用渐近可忽略的反馈速率（O(1/log n)），结合特定的编码参数η来权衡存储复杂度和错误概率。

Result: 方案实现了速率1-H_q(ϱ)-ε（接近信息论最优），列表大小exp(O(ε^{-3/2}log²(1/ε)))，编码/解码计算复杂度n^{O(ε^{-1}log(1/ε))}，存储复杂度O(n^{η+1}log n)，错误概率O(n^{-η})。

Conclusion: 该工作首次为任意q≥2提供了在低速率反馈下接近信息论最优的列表可译码方案，解决了现有技术在大q值限制下的局限性。

Abstract: Given a channel with length-$n$ inputs and outputs over the alphabet
$\{0,1,\ldots,q-1\}$, and of which a fraction $\varrho \in (0,1-1/q)$ of
symbols can be arbitrarily corrupted by an adversary, a fundamental problem is
that of communicating at rates close to the information-theoretically optimal
values, while ensuring the receiver can infer that the transmitter's message is
from a ``small" set. While the existence of such codes is known, and
constructions with computationally tractable encoding/decoding procedures are
known for large $q$, we provide the first schemes that attain this performance
for any $q \geq 2$, as long as low-rate feedback (asymptotically negligible
relative to the number of transmissions) from the receiver to the transmitter
is available. For any sufficiently small $\varepsilon > 0$ and $\varrho \in
(1-1/q-\Theta(\sqrt{\varepsilon})$ our minimal feedback scheme has the
following parameters: Rate $1-H_q(\varrho) - \varepsilon$ (i.e.,
$\varepsilon$-close to information-theoretically optimal -- here $H_q(\varrho)$
is the $q$-ary entropy function), list-size
$\exp(\mathcal{O}(\varepsilon^{-3/2}\log^2(1/\varepsilon))$, computational
complexity of encoding/decoding
$n^{\mathcal{O}(\varepsilon^{-1}\log(1/\varepsilon))}$, storage complexity
$\mathcal{O}(n^{\eta+1}\log n)$ for a code design parameter $\eta>1$ that
trades off storage complexity with the probability of error. The error
probability is $\mathcal{O}(n^{-\eta})$, and the (vanishing) feedback rate is
$\mathcal{O}(1/ \log n)$.

</details>


### [17] [List Decoding of Folded Reed-Solomon Codes Over Galois Ring](https://arxiv.org/abs/2511.04135)
*Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文扩展了Guruswami-Sudan列表解码算法到伽罗瓦环上的Reed-Solomon码，证明了速率为r的RS码可列表解码到半径1-√r，并研究了伽罗瓦环上折叠RS码的列表解码，使其解码半径达到Singleton界，同时将列表大小改进为O(1/ε²)。


<details>
  <summary>Details</summary>
Motivation: 由于零知识系统的最新进展，需要研究伽罗瓦环上码的邻近间隙问题。有限域上的RS码邻近间隙在列表解码下可改进为1-√r，但对伽罗瓦环上RS码了解甚少，这阻碍了基于环的算术电路的零知识证明系统发展。

Method: 首先将Guruswami-Sudan列表解码程序扩展到伽罗瓦环上的Reed-Solomon码，然后研究伽罗瓦环上折叠Reed-Solomon码的列表解码，最后通过扩展Shashank Srivastava(2025)的工作改进折叠RS码的列表大小。

Result: 证明了伽罗瓦环上速率为r的RS码可列表解码到半径1-√r；折叠RS码的列表解码半径可达Singleton界；折叠RS码的列表大小改进为O(1/ε²)。

Conclusion: 成功将列表解码技术扩展到伽罗瓦环上的RS码和折叠RS码，为基于环的零知识证明系统提供了重要的编码理论基础。

Abstract: List decoding of codes can be seen as the generalization of unique decoding
of codes While list decoding over finite fields has been extensively studied,
extending these results to more general algebraic structures such as Galois
rings remains an important challenge. Due to recent progress in zero knowledge
systems, there is a growing demand to investigate the proximity gap of codes
over Galois rings in Yizhou Yao and coauthors(2025), Alexander Golovne and
coauthors(2023), Yuanju Wei and coauthors(2025). The proximity gap is closely
related to the decoding capability of codes. It was shown in Eli Ben-Sasson and
coauthors(2020) that the proximity gap for RS codes over finite field can be
improved to $1-\sqrt{r}$ if one consider list decoding instead of unique
decoding. However, we know very little about RS codes over Galois ring which
might hinder the development of zero knowledge proof system for ring-based
arithmetic circuit. In this work, we first extend the list decoding procedure
of Guruswami and Sudan to Reed-Solomon codes over Galois rings, which shows
that RS codes with rate $r$ can be list decoded up to radius $1-\sqrt{r}$.
Then, we investigate the list decoding of folded Reed-Solomon codes over Galois
rings. We show that the list decoding radius of folded Reed-Solomon codes can
reach the Singlton bound as its counterpart over finite field. Finally, we
improve the list size of our folded Reed-Solomon code to
$O(\frac{1}{\varepsilon^2})$ by extending recent work in Shashank
Srivastava(2025) to Galois Rings.

</details>


### [18] [Affine Frequency Division Multiplexing: From Communication to Sensing](https://arxiv.org/abs/2511.04471)
*Ali Bemani,Nassar Ksairi,Marios Kountouris*

Main category: cs.IT

TL;DR: AFDM波形在集成感知与通信系统中解决了两个关键挑战：大带宽下的接收机复杂度和多雷达环境干扰问题。在单静态感知中通过自干扰消除和模拟去啁啾降低采样率，在双静态感知中支持亚奈奎斯特采样，并通过离散仿射傅里叶变换的灵活性来管理干扰。


<details>
  <summary>Details</summary>
Motivation: 解决集成感知与通信系统中两个主要挑战：(1) 支持高延迟/距离分辨率所需的大带宽时保持接收机复杂度和能耗在可接受水平；(2) 在多雷达环境中减轻干扰。

Method: 在单静态感知中使用AFDM兼容的低复杂度自干扰消除方案和模拟去啁啾降低采样率；在双静态感知中采用亚奈奎斯特采样技术；利用离散仿射傅里叶变换的资源分配灵活性管理干扰。

Result: AFDM波形能够在单静态感知中有效处理发射机-接收机直接泄漏，在双静态感知中无需硬件修改即可实现亚奈奎斯特采样并保持延迟分辨率，同时通过DAFT灵活分配资源来减轻多雷达环境干扰。

Conclusion: AFDM波形通过其独特的特性为集成感知与通信系统提供了有效的解决方案，能够在大带宽需求下控制接收机复杂度，并在多雷达环境中有效管理干扰。

Abstract: Affine Frequency Division Multiplexing (AFDM) has been proposed as an
effective waveform for achieving the full diversity of doubly-dispersive
(delay-Doppler) channels. While this property is closely related to range and
velocity estimation in sensing, this article focuses on other AFDM features
that are particularly relevant for addressing two challenges in integrated
sensing and communication (ISAC) systems: (1) maintaining receiver complexity
and energy consumption at acceptable levels while supporting the large
bandwidths required for high delay/range resolution, and (2) mitigating
interference in multiradar environments. In monostatic sensing, where direct
transmitter-receiver leakage is a major impairment, we show that AFDM-based
ISAC receivers can address the first challenge through their compatibility with
low-complexity self-interference cancellation (SIC) schemes and reduced
sampling rates via analog dechirping. In bistatic sensing, where such analog
solutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist
sampling without requiring hardware modifications while preserving delay
resolution. Finally, we show that the second challenge can be addressed by
leveraging the resource-assignment flexibility of the discrete affine Fourier
transform (DAFT) underlying the AFDM waveform.

</details>


### [19] [Age of Job Completion Minimization with Stable Queues](https://arxiv.org/abs/2511.04630)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了具有马尔可夫状态机器的作业分配系统，提出了最小化作业完成时间和采样成本的策略，并分析了队列稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 在中央服务器、多用户和状态变化的机器组成的系统中，需要优化作业完成效率并控制采样成本，以最大化单位时间内完成的作业数量。

Method: 引入作业完成时间的新度量指标，提出两种策略来最小化该指标和采样成本，并通过数值评估其性能。

Result: 为两种策略找到了保证作业队列稳定的充分条件，并通过数值实验验证了策略的有效性。

Conclusion: 提出的策略能够有效管理作业分配，在保证队列稳定的同时优化作业完成时间和采样成本。

Abstract: We consider a time-slotted job-assignment system with a central server, N
users and a machine which changes its state according to a Markov chain (hence
called a Markov machine). The users submit their jobs to the central server
according to a stochastic job arrival process. For each user, the server has a
dedicated job queue. Upon receiving a job from a user, the server stores that
job in the corresponding queue. When the machine is not working on a job
assigned by the server, the machine can be either in internally busy or in free
state, and the dynamics of these states follow a binary symmetric Markov chain.
Upon sampling the state information of the machine, if the server identifies
that the machine is in the free state, it schedules a user and submits a job to
the machine from the job queue of the scheduled user. To maximize the number of
jobs completed per unit time, we introduce a new metric, referred to as the age
of job completion. To minimize the age of job completion and the sampling cost,
we propose two policies and numerically evaluate their performance. For both of
these policies, we find sufficient conditions under which the job queues will
remain stable.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [20] [Caption Injection for Optimization in Generative Search Engine](https://arxiv.org/abs/2511.04080)
*Xiaolu Chen,Yong Liao*

Main category: cs.IR

TL;DR: 提出了Caption Injection，首个多模态G-SEO方法，通过从图像提取标题并注入文本内容来增强生成式搜索引擎中的内容主观可见性。


<details>
  <summary>Details</summary>
Motivation: 现有G-SEO方法仅限于文本优化，无法充分利用多模态数据。随着多模态检索增强生成技术的发展，需要开发能够整合视觉语义的多模态优化方法。

Method: Caption Injection方法从图像中提取标题，并将其注入到文本内容中，从而在生成式搜索场景中整合视觉语义来增强内容的主观可见性。

Result: 在MRAMG基准测试中，Caption Injection在G-Eval指标下显著优于仅文本的G-SEO基线方法，证明了多模态整合在G-SEO中的必要性和有效性。

Conclusion: 多模态整合对于提高用户感知的内容可见性至关重要，Caption Injection为生成式搜索引擎优化提供了有效的多模态解决方案。

Abstract: Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation
(RAG) techniques and Large Language Models (LLMs) to integrate multi-source
information and provide users with accurate and comprehensive responses. Unlike
traditional search engines that present results in ranked lists, GSEs shift
users' attention from sequential browsing to content-driven subjective
perception, driving a paradigm shift in information retrieval. In this context,
enhancing the subjective visibility of content through Generative Search Engine
Optimization (G-SEO) methods has emerged as a new research focus. With the
rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)
techniques, GSEs can now efficiently integrate text, images, audio, and video,
producing richer responses that better satisfy complex information needs.
Existing G-SEO methods, however, remain limited to text-based optimization and
fail to fully exploit multimodal data. To address this gap, we propose Caption
Injection, the first multimodal G-SEO approach, which extracts captions from
images and injects them into textual content, integrating visual semantics to
enhance the subjective visibility of content in generative search scenarios. We
systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under
both unimodal and multimodal settings. Experimental results show that Caption
Injection significantly outperforms text-only G-SEO baselines under the G-Eval
metric, demonstrating the necessity and effectiveness of multimodal integration
in G-SEO to improve user-perceived content visibility.

</details>


### [21] [E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce](https://arxiv.org/abs/2511.04087)
*Ge Zhang,Rohan Deepak Ajwani,Tony Zheng,Hongjian Gu,Yaochen Hu,Wei Guo,Mark Coates,Yingxue Zhang*

Main category: cs.IR

TL;DR: 提出E-CARE方法，通过构建常识推理因子图，在单次LLM前向传播中实现高效的常识增强推荐，避免实时LLM推理的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 解决电商平台中基于LLM的查询-产品相关性预测方法存在的高成本问题，包括实时LLM推理开销、人工标注和潜在监督微调需求。

Method: 构建常识推理因子图，编码强大LLM的推理模式，使模型在推理时仅需单次LLM前向传播即可访问常识推理能力。

Result: 在2个下游任务中，precision@5指标提升高达12.1%。

Conclusion: E-CARE方法能够有效利用LLM的常识推理能力，同时显著提升效率，为电商推荐任务提供高效解决方案。

Abstract: Finding relevant products given a user query plays a pivotal role in an
e-commerce platform, as it can spark shopping behaviors and result in revenue
gains. The challenge lies in accurately predicting the correlation between
queries and products. Recently, mining the cross-features between queries and
products based on the commonsense reasoning capacity of Large Language Models
(LLMs) has shown promising performance. However, such methods suffer from high
costs due to intensive real-time LLM inference during serving, as well as human
annotations and potential Supervised Fine Tuning (SFT). To boost efficiency
while leveraging the commonsense reasoning capacity of LLMs for various
e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation
Enhancer (E-CARE). During inference, models augmented with E-CARE can access
commonsense reasoning with only a single LLM forward pass per query by
utilizing a commonsense reasoning factor graph that encodes most of the
reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show
an improvement of up to 12.1% on precision@5.

</details>


### [22] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 开发了一个AI聊天机器人作为BRAC大学学生的导师，通过混合检索方法（BM25和ChromaDB）结合LLaMA-3.3-70B模型，提供个性化指导。


<details>
  <summary>Details</summary>
Motivation: 大学生在本科阶段面临巨大挑战，缺乏个性化的按需指导，现有数字工具无法提供规模化定制化辅导。

Method: 构建数据摄取管道处理多样信息源，采用BM25词汇排名和ChromaDB语义检索的混合方法，使用LLaMA-3.3-70B生成对话响应。

Result: 生成文本语义相关性高，BERTScore为0.831，METEOR得分为0.809；数据管道更新效率高（106.82秒 vs 新数据368.62秒）。

Conclusion: 该聊天机器人能够帮助学生解答疑问，更好地理解大学生活，并在开放学分制大学中规划更好的学期安排。

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


### [23] [Coordination-Free Lane Partitioning for Convergent ANN Search](https://arxiv.org/abs/2511.04221)
*Carl Kugblenu,Petri Vuorimaa*

Main category: cs.IR

TL;DR: 提出了一种无协调的分区方法，将向量搜索系统中的重复计算转化为互补工作，在相同成本和时限下显著提升召回率和命中率。


<details>
  <summary>Details</summary>
Motivation: 生产向量搜索系统通常通过并行通道（线程、副本或分片）来满足延迟SLO，但这些通道会重复发现相同候选，额外计算无法增加覆盖范围。

Method: 为每个查询：(1)构建确定性的候选池，大小等于总top-k预算；(2)应用每查询伪随机排列；(3)为每个通道分配不相交的位置切片。通道无需运行时协调即可返回不同结果。

Result: 在4个通道、总候选预算64的情况下，SIFT1M数据集上recall@10从0.249提升到0.999，通道重叠从近100%降至0%；MS MARCO数据集上hit@10从0.200提升到0.601，MRR@10从0.133提升到0.330。

Conclusion: 操作指南：将每查询池大小设为总预算，确定性地跨通道分区位置，在不改变预算或时限的情况下将冗余扇出转化为互补覆盖。

Abstract: Production vector search systems often fan out each query across parallel
lanes (threads, replicas, or shards) to meet latency service-level objectives
(SLOs). In practice, these lanes rediscover the same candidates, so extra
compute does not increase coverage. We present a coordination-free lane
partitioner that turns duplication into complementary work at the same cost and
deadline. For each query we (1) build a deterministic candidate pool sized to
the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)
assign each lane a disjoint slice of positions. Lanes then return different
results by construction, with no runtime coordination.
  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT
feature vectors) with Hierarchical Navigable Small World graphs (HNSW)
recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%
to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to
0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted
file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS
MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead
of ~37 microseconds per query (mean at the main setting) with linear growth in
the number of merged candidates.
  These results yield a simple operational guideline: size the per-query pool
to the total budget, deterministically partition positions across lanes, and
turn redundant fan-out into complementary coverage without changing budget or
deadline.

</details>


### [24] [Denoised Recommendation Model with Collaborative Signal Decoupling](https://arxiv.org/abs/2511.04237)
*Zefeng Li,Ning Yang*

Main category: cs.IR

TL;DR: 提出DRCSD模型，通过协作信号解耦和分阶去噪解决推荐系统中用户-物品交互矩阵的噪声问题，避免传统单图去噪方法导致的协作信号衰减。


<details>
  <summary>Details</summary>
Motivation: 传统协同过滤算法因用户-物品交互矩阵中的噪声导致推荐性能不佳，现有去噪方法多在单图上进行，可能中断节点间路径，削弱路径依赖的协作信息。

Method: DRCSD包含协作信号解耦模块（按结构特征分解信号为不同阶次）和分阶去噪模块（对每阶进行针对性去噪），并改进传统GNN聚合机制以避免交叉阶信号干扰。

Result: 在三个公开真实数据集上的实验表明，DRCSD对不稳定交互具有优越鲁棒性，在推荐准确性指标上相比最先进基线模型获得统计显著性能提升。

Conclusion: DRCSD通过解耦协作信号和分阶去噪，有效解决了传统去噪方法的局限性，显著提升了推荐系统的鲁棒性和准确性。

Abstract: Although the collaborative filtering (CF) algorithm has achieved remarkable
performance in recommendation systems, it suffers from suboptimal
recommendation performance due to noise in the user-item interaction matrix.
Numerous noise-removal studies have improved recommendation models, but most
existing approaches conduct denoising on a single graph. This may cause
attenuation of collaborative signals: removing edges between two nodes can
interrupt paths between other nodes, weakening path-dependent collaborative
information. To address these limitations, this study proposes a novel
GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD
includes two core modules: a collaborative signal decoupling module (decomposes
signals into distinct orders by structural characteristics) and an order-wise
denoising module (performs targeted denoising on each order). Additionally, the
information aggregation mechanism of traditional GNN-based CF models is
modified to avoid cross-order signal interference until the final pooling
operation. Extensive experiments on three public real-world datasets show that
DRCSD has superior robustness against unstable interactions and achieves
statistically significant performance improvements in recommendation accuracy
metrics compared to state-of-the-art baseline models.

</details>


### [25] [LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems](https://arxiv.org/abs/2511.04541)
*Baptiste Bonin,Maxime Heuillet,Audrey Durand*

Main category: cs.IR

TL;DR: 研究探索LLM如何通过slate的成对推理作为用户偏好的世界模型，在三个任务上的实证研究揭示了任务性能与LLM捕获的偏好函数特性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 跨领域建模用户偏好在slate推荐（推荐有序物品序列）中仍是一个关键挑战，需要探索LLM作为用户偏好世界模型的有效性。

Method: 使用多个LLM在三个不同数据集的任务上进行实证研究，通过成对推理分析slate来建模用户偏好。

Result: 研究结果揭示了任务性能与LLM捕获的偏好函数特性之间的关系，指出了改进方向。

Conclusion: LLM在推荐系统中作为世界模型具有潜力，研究为其应用提供了实证基础。

Abstract: Modeling user preferences across domains remains a key challenge in slate
recommendation (i.e. recommending an ordered sequence of items) research. We
investigate how Large Language Models (LLM) can effectively act as world models
of user preferences through pairwise reasoning over slates. We conduct an
empirical study involving several LLMs on three tasks spanning different
datasets. Our results reveal relationships between task performance and
properties of the preference function captured by LLMs, hinting towards areas
for improvement and highlighting the potential of LLMs as world models in
recommender systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [26] [On the Brittleness of CLIP Text Encoders](https://arxiv.org/abs/2511.04247)
*Allie Tran,Luca Rossetto*

Main category: cs.MM

TL;DR: 本文系统分析了CLIP等多模态嵌入模型在多媒体信息检索中对非语义查询扰动的稳定性问题，发现语法和语义扰动导致最大不稳定性，而标点符号等表面编辑的脆弱性最为集中。


<details>
  <summary>Details</summary>
Motivation: 虽然CLIP等对比对齐模型在零样本分类和多模态检索中表现出色，但它们对输入的小扰动缺乏稳定性，特别是在处理手动表达查询时，微小的查询变化可能导致检索结果排序的巨大差异。

Method: 使用TRECVID Ad-Hoc视频搜索查询和V3C1视频数据集，评估多种CLIP变体对词汇、语法和语义扰动的响应，系统分析三类非语义查询扰动的影响。

Result: 在所有模型中，语法和语义扰动驱动最大的不稳定性，而脆弱性集中在标点符号、大小写等表面编辑上。

Conclusion: 稳健性是评估视觉语言模型超越基准准确性的关键维度，当前模型对查询扰动存在显著的脆弱性。

Abstract: Multimodal co-embedding models, especially CLIP, have advanced the state of
the art in zero-shot classification and multimedia information retrieval in
recent years by aligning images and text in a shared representation space.
However, such modals trained on a contrastive alignment can lack stability
towards small input perturbations. Especially when dealing with manually
expressed queries, minor variations in the query can cause large differences in
the ranking of the best-matching results. In this paper, we present a
systematic analysis of the effect of multiple classes of non-semantic query
perturbations in an multimedia information retrieval scenario. We evaluate a
diverse set of lexical, syntactic, and semantic perturbations across multiple
CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video
collection. Across models, we find that syntactic and semantic perturbations
drive the largest instabilities, while brittleness is concentrated in trivial
surface edits such as punctuation and case. Our results highlight robustness as
a critical dimension for evaluating vision-language models beyond benchmark
accuracy.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li,Weiyan Wang,Ruiyuan Li,Chao Chen,Xianlei Long,Linjiang Zheng,Quanqing Xu,Chuanhui Yang*

Main category: cs.DB

TL;DR: Falcon是一个基于GPU的浮点自适应无损压缩框架，通过异步流水线、精确的浮点-整数转换和自适应稀疏位平面编码，解决了GPU压缩中的三个关键挑战，在12个数据集上相比最先进的CPU方法压缩率提升9.1%，压缩和解压缩吞吐量分别比最快的GPU竞争者高2.43倍和2.4倍。


<details>
  <summary>Details</summary>
Motivation: 物联网和高性能计算产生大量浮点时间序列数据，需要保持绝对保真度的压缩。利用现代GPU的大规模并行性可以实现前所未有的吞吐量，但面临数据移动瓶颈、精度保持转换复杂度和异常值导致的稀疏性退化三个关键挑战。

Method: 1) 轻量级异步流水线隐藏CPU-GPU数据传输的I/O延迟；2) 具有理论保证的精确快速浮点-整数转换方法，消除浮点运算误差；3) 自适应稀疏位平面无损编码策略，减少异常值引起的稀疏性。

Result: 在12个多样化数据集上的广泛实验表明，压缩率比最先进的CPU方法提高9.1%，压缩吞吐量比最快的GPU竞争者高2.43倍，解压缩吞吐量高2.4倍。

Conclusion: Falcon框架成功解决了GPU浮点无损压缩的三个关键挑战，在压缩率和吞吐量方面都显著优于现有方法，为大规模浮点时间序列数据的高效压缩提供了有效解决方案。

Abstract: Domains such as IoT (Internet of Things) and HPC (High Performance Computing)
generate a torrential influx of floating-point time-series data. Compressing
these data while preserving their absolute fidelity is critical, and leveraging
the massive parallelism of modern GPUs offers a path to unprecedented
throughput. Nevertheless, designing such a high-performance GPU-based lossless
compressor faces three key challenges: 1) heterogeneous data movement
bottlenecks, 2) precision-preserving conversion complexity, and 3)
anomaly-induced sparsity degradation. To address these challenges, this paper
proposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN
framework. Specifically, Falcon first introduces a lightweight asynchronous
pipeline, which hides the I/O latency during the data transmission between the
CPU and GPU. Then, we propose an accurate and fast float-to-integer
transformation method with theoretical guarantees, which eliminates the errors
caused by floating-point arithmetic. Moreover, we devise an adaptive sparse
bit-plane lossless encoding strategy, which reduces the sparsity caused by
outliers. Extensive experiments on 12 diverse datasets show that our
compression ratio improves by 9.1% over the most advanced CPU-based method,
with compression throughput 2.43X higher and decompression throughput 2.4X
higher than the fastest GPU-based competitors, respectively.

</details>


### [28] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao,Daniel E. Lucani*

Main category: cs.DB

TL;DR: EntroGD是一个基于熵引导的广义去重压缩框架，将复杂度从O(nd²)降低到O(nd)，在保持压缩性能的同时显著提升配置速度和聚类分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统广义去重算法在高维数据上面临可扩展性挑战，GreedyGD算法的O(nd²)复杂度限制了其在大规模高维数据上的应用。

Method: 采用两步法：首先生成保留分析保真度的浓缩样本，然后应用熵引导的位选择来最大化压缩效率。

Result: 在18个不同数据集上，EntroGD达到与GD和通用压缩器相当的压缩性能，配置时间比GreedyGD快53.5倍，聚类分析比原始数据快31.6倍，精度损失可忽略。

Conclusion: EntroGD为直接在压缩数据上执行分析提供了高效且可扩展的解决方案。

Abstract: Generalized Deduplication (GD) enables lossless compression with direct
analytics on compressed data by dividing data into \emph{bases} and
\emph{deviations} and performing dictionary encoding on the former. However, GD
algorithms face scalability challenges for high-dimensional data. For example,
the GreedyGD algorithm relies on an iterative bit-selection process across
$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to
select bits to be used as bases and deviations. Although the $n$ data rows can
be reduced during training at the expense of performance, highly dimensional
data still experiences a marked loss in performance. This paper introduces
EntroGD, an entropy-guided GD framework that reduces complexity of the
bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step
process. First, it generates condensed samples to preserve analytic fidelity.
Second, it applies entropy-guided bit selection to maximize compression
efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD
achieves compression performance comparable to GD-based and universal
compressors, while reducing configuration time by up to 53.5$\times$ over
GreedyGD and accelerating clustering by up to 31.6$\times$ over the original
data with negligible accuracy loss by performing analytics on the condensed
samples, which are much fewer than original samples. Thus, EntroGD provides an
efficient and scalable solution to performing analytics directly on compressed
data.

</details>
