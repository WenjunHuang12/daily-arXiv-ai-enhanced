{"id": "2510.21342", "categories": ["cs.DB", "cs.AI", "cs.CG", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.21342", "abs": "https://arxiv.org/abs/2510.21342", "authors": ["Hossein Amiri", "Mohammad Hashemi", "Andreas Züfle"], "title": "World-POI: Global Point-of-Interest Data Enriched from Foursquare and OpenStreetMap as Tabular and Graph Data", "comment": null, "summary": "Recently, Foursquare released a global dataset with more than 100 million\npoints of interest (POIs), each representing a real-world business on its\nplatform. However, many entries lack complete metadata such as addresses or\ncategories, and some correspond to non-existent or fictional locations. In\ncontrast, OpenStreetMap (OSM) offers a rich, user-contributed POI dataset with\ndetailed and frequently updated metadata, though it does not formally verify\nwhether a POI represents an actual business. In this data paper, we present a\nmethodology that integrates the strengths of both datasets: Foursquare as a\ncomprehensive baseline of commercial POIs and OSM as a source of enriched\nmetadata. The combined dataset totals approximately 1 TB. While this full\nversion is not publicly released, we provide filtered releases with adjustable\nthresholds that reduce storage needs and make the data practical to download\nand use across domains. We also provide step-by-step instructions to reproduce\nthe full 631 GB build. Record linkage is achieved by computing name similarity\nscores and spatial distances between Foursquare and OSM POIs. These measures\nidentify and retain high-confidence matches that correspond to real businesses\nin Foursquare, have representations in OSM, and show strong name similarity.\nFinally, we use this filtered dataset to construct a graph-based representation\nof POIs enriched with attributes from both sources, enabling advanced spatial\nanalyses and a range of downstream applications."}
{"id": "2510.21572", "categories": ["cs.DB", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.21572", "abs": "https://arxiv.org/abs/2510.21572", "authors": ["Raktim Mukhopadhyay", "Marianthi Markatou"], "title": "SurVigilance: An Application for Accessing Global Pharmacovigilance Data", "comment": null, "summary": "Even though several publicly accessible pharmacovigilance databases are\navailable, extracting data from them is a technically challenging process.\nExisting tools typically focus on a single database. We present SurVigilance,\nan open-source tool that streamlines the process of retrieving safety data from\nseven major pharmacovigilance databases. SurVigilance provides a graphical user\ninterface as well as functions for programmatic access, thus enabling\nintegration into existing research workflows. SurVigilance utilizes a modular\narchitecture to provide access to the heterogeneous sources. By reducing the\ntechnical barriers to accessing safety data, SurVigilance aims to facilitate\npharmacovigilance research."}
{"id": "2510.21021", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21021", "abs": "https://arxiv.org/abs/2510.21021", "authors": ["Xiaoxin Ye", "Chengkai Huang", "Hongtao Huang", "Lina Yao"], "title": "Gaussian Mixture Flow Matching with Domain Alignment for Multi-Domain Sequential Recommendation", "comment": null, "summary": "Users increasingly interact with content across multiple domains, resulting\nin sequential behaviors marked by frequent and complex transitions. While\nCross-Domain Sequential Recommendation (CDSR) models two-domain interactions,\nMulti-Domain Sequential Recommendation (MDSR) introduces significantly more\ndomain transitions, compounded by challenges such as domain heterogeneity and\nimbalance. Existing approaches often overlook the intricacies of domain\ntransitions, tend to overfit to dense domains while underfitting sparse ones,\nand struggle to scale effectively as the number of domains increases. We\npropose \\textit{GMFlowRec}, an efficient generative framework for MDSR that\nmodels domain-aware transition trajectories via Gaussian Mixture Flow Matching.\nGMFlowRec integrates: (1) a unified dual-masked Transformer to disentangle\ndomain-invariant and domain-specific intents, (2) a Gaussian Mixture flow field\nto capture diverse behavioral patterns, and (3) a domain-aligned prior to\nsupport frequent and sparse transitions. Extensive experiments on JD and Amazon\ndatasets demonstrate that GMFlowRec achieves state-of-the-art performance with\nup to 44\\% improvement in NDCG@5, while maintaining high efficiency via a\nsingle unified backbone, making it scalable for real-world multi-domain\nsequential recommendation."}
{"id": "2510.20903", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.20903", "abs": "https://arxiv.org/abs/2510.20903", "authors": ["Yirong Shen", "Lu Gan", "Cong Ling"], "title": "Information Theoretic Learning for Diffusion Models with Warm Start", "comment": "NeurIPS 2025", "summary": "Generative models that maximize model likelihood have gained traction in many\npractical settings. Among them, perturbation based approaches underpin many\nstrong likelihood estimation models, yet they often face slow convergence and\nlimited theoretical understanding. In this paper, we derive a tighter\nlikelihood bound for noise driven models to improve both the accuracy and\nefficiency of maximum likelihood learning. Our key insight extends the\nclassical KL divergence Fisher information relationship to arbitrary noise\nperturbations, going beyond the Gaussian assumption and enabling structured\nnoise distributions. This formulation allows flexible use of randomized noise\ndistributions that naturally account for sensor artifacts, quantization\neffects, and data distribution smoothing, while remaining compatible with\nstandard diffusion training. Treating the diffusion process as a Gaussian\nchannel, we further express the mismatched entropy between data and model,\nshowing that the proposed objective upper bounds the negative log-likelihood\n(NLL). In experiments, our models achieve competitive NLL on CIFAR-10 and SOTA\nresults on ImageNet across multiple resolutions, all without data augmentation,\nand the framework extends naturally to discrete data."}
{"id": "2510.21039", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.21039", "abs": "https://arxiv.org/abs/2510.21039", "authors": ["Christopher Jerrett", "Elliot Anshelevich"], "title": "Low Cost, Fair, and Representative Committees in a Metric Space", "comment": null, "summary": "We study the problem of selecting a representative committee of $k$ agents\nfrom a collection of $n$ agents in a common metric space. This problem is\nrelated to choosing $k$ facilities in facility location and $k$-median\nproblems. However, unlike in more traditional facility location where each\nagent only cares about the closest selected facility, in the settings we\nconsider each agent desires that all selected committee members are close to\nthem. More precisely, we look at the sum objective, in which the goal is to\nminimize the total distance from all agents to all members of the chosen\ncommittee. We show that it is always possible to find a committee which is both\nlow-cost according to this objective, and also fair according to many existing\nnotions of fairness and proportionality defined for clustering settings.\nMoreover, we introduce a new desirable axiom for representative committees we\ncall NORP, which prevents over-representation of any subset of agents. While\nall existing algorithms for fair committee selection do not satisfy this\nintuitive property, we provide new algorithms which form simultaneously\nlow-cost, fair, and NORP solutions, thus showing that it is always possible to\nform low-cost, fair, and representative committees for our settings."}
{"id": "2510.21058", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.21058", "abs": "https://arxiv.org/abs/2510.21058", "authors": ["Charlie Carlson", "Yury Makarychev", "Ron Mosenzon"], "title": "Hardness of Approximation for Shortest Path with Vector Costs", "comment": "33 pages, 1 figure, to be published in SODA 2026", "summary": "We obtain hardness of approximation results for the $\\ell_p$-Shortest Path\nproblem, a variant of the classic Shortest Path problem with vector costs. For\nevery integer $p \\in [2,\\infty)$, we show a hardness of $\\Omega(p(\\log n /\n\\log^2\\log n)^{1-1/p})$ for both polynomial- and quasi-polynomial-time\napproximation algorithms. This nearly matches the approximation factor of\n$O(p(\\log n / \\log\\log n)^{1-1/p})$ achieved by a quasi-polynomial-time\nalgorithm of Makarychev, Ovsiankin, and Tani (ICALP 2025). No hardness of\napproximation results were previously known for any $p < \\infty$. We also\npresent results for the case where $p$ is a function of $n$. For $p = \\infty$,\nwe establish a hardness of $\\tilde\\Omega(\\log^2 n)$, improving upon the\nprevious $\\tilde\\Omega(\\log n)$ hardness result. Our result nearly matches the\n$O(\\log^2 n)$ approximation guarantee of the quasi-polynomial-time algorithm by\nLi, Xu, and Zhang (ICALP 2025). Finally, we present asymptotic bounds on\nhigher-order Bell numbers, which might be of independent interest."}
{"id": "2510.21028", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21028", "abs": "https://arxiv.org/abs/2510.21028", "authors": ["Amna Al-Araimi", "Yue Zheng", "Haiming Liu"], "title": "Communication Platform for Non-verbal Autistic children in Oman using Android mobile", "comment": null, "summary": "This paper discusses the issue regarding Non-verbal Autism Spectrum Disorder.\nIt has been observed that this mental disorder is listed in major parts of the\nworld including the US, UK, and India. To mitigate this type of disorder, a\nwide range of smartphones, computers, and artificial intelligence technologies\nhave been used. This technology has helped the population cope with\nsocialization and communication needs. Many applications have been developed to\nenhance the communication capabilities of non-verbal autistic children. This\nthesis project proposes the development of a platform that includes a web panel\nand an Android mobile application to assist non-verbal autistic children in\ncommunication, especially in Oman. Different interventions have been merged to\nimprove the quality of life for people on the autism spectrum. The main problem\nidentified in this case is that fragmented approaches are not suitable for\nautistic children. The augmented reality framework provides the capability to\nengage autistic children in creative play and self-reflection through\ninteractive screen-based activities."}
{"id": "2510.21030", "categories": ["cs.IT", "math.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.21030", "abs": "https://arxiv.org/abs/2510.21030", "authors": ["En-Jui Chang"], "title": "Overlapped-repetition Shor codes achieving fourfold asymptotic rate", "comment": "4 pages", "summary": "The standard Shor code employs two repetition codes as inner and outer codes,\nyielding a simple structure but a relatively low code rate. By overlapping a\nsmall number of repetition codes, we enhance the asymptotic code rate fourfold.\nIn the minimal-distance case $d = 3$, this construction reduces the overhead\nfrom $[[9,1,3]]$ to the more efficient $[[7,1,3]]$ configuration."}
{"id": "2510.21178", "categories": ["cs.GT", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.21178", "abs": "https://arxiv.org/abs/2510.21178", "authors": ["Flora C. Shi", "Martin J. Wainwright", "Stephen Bates"], "title": "Instance-Adaptive Hypothesis Tests with Heterogeneous Agents", "comment": null, "summary": "We study hypothesis testing over a heterogeneous population of strategic\nagents with private information. Any single test applied uniformly across the\npopulation yields statistical error that is sub-optimal relative to the\nperformance of an oracle given access to the private information. We show how\nit is possible to design menus of statistical contracts that pair type-optimal\ntests with payoff structures, inducing agents to self-select according to their\nprivate information. This separating menu elicits agent types and enables the\nprincipal to match the oracle performance even without a priori knowledge of\nthe agent type. Our main result fully characterizes the collection of all\nseparating menus that are instance-adaptive, matching oracle performance for an\narbitrary population of heterogeneous agents. We identify designs where\ninformation elicitation is essentially costless, requiring negligible\nadditional expense relative to a single-test benchmark, while improving\nstatistical performance. Our work establishes a connection between proper\nscoring rules and menu design, showing how the structure of the hypothesis test\nconstrains the elicitable information. Numerical examples illustrate the\ngeometry of separating menus and the improvements they deliver in error\ntrade-offs. Overall, our results connect statistical decision theory with\nmechanism design, demonstrating how heterogeneity and strategic participation\ncan be harnessed to improve efficiency in hypothesis testing."}
{"id": "2510.21128", "categories": ["cs.DS", "cs.CC", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21128", "abs": "https://arxiv.org/abs/2510.21128", "authors": ["Kshipra Bhawalkar", "Yang Cai", "Zhe Feng", "Christopher Liaw", "Tao Lin"], "title": "A Unified Approach to Submodular Maximization Under Noise", "comment": "Accepted by NeurIPS 2025", "summary": "We consider the problem of maximizing a submodular function with access to a\nnoisy value oracle for the function instead of an exact value oracle. Similar\nto prior work, we assume that the noisy oracle is persistent in that multiple\ncalls to the oracle for a specific set always return the same value. In this\nmodel, Hassidim and Singer (2017) design a $(1-1/e)$-approximation algorithm\nfor monotone submodular maximization subject to a cardinality constraint, and\nHuang et al (2022) design a $(1-1/e)/2$-approximation algorithm for monotone\nsubmodular maximization subject to any arbitrary matroid constraint. In this\npaper, we design a meta-algorithm that allows us to take any \"robust\" algorithm\nfor exact submodular maximization as a black box and transform it into an\nalgorithm for the noisy setting while retaining the approximation guarantee. By\nusing the meta-algorithm with the measured continuous greedy algorithm, we\nobtain a $(1-1/e)$-approximation (resp. $1/e$-approximation) for monotone\n(resp. non-monotone) submodular maximization subject to a matroid constraint\nunder noise. Furthermore, by using the meta-algorithm with the double greedy\nalgorithm, we obtain a $1/2$-approximation for unconstrained (non-monotone)\nsubmodular maximization under noise."}
{"id": "2510.21151", "categories": ["cs.IR", "H.5.2; H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.21151", "abs": "https://arxiv.org/abs/2510.21151", "authors": ["David Guo", "Minqi Sun", "Yilun Jiang", "Jiazhou Liang", "Scott Sanner"], "title": "VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion", "comment": null, "summary": "Multimodal conversational recommendation has emerged as a promising paradigm\nfor delivering personalized experiences through natural dialogue enriched by\nvisual and contextual grounding. Yet, current multimodal conversational\nrecommendation datasets remain limited: existing resources either simulate\nconversations, omit user history, or fail to collect sufficiently detailed\nfeedback, all of which constrain the types of research and evaluation they\nsupport.\n  To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman\ndialogues in realistic fashion shopping scenarios. Each dialogue is paired with\na shared visual catalogue, item metadata, user fashion profiles and histories,\nand post-conversation ratings from both Seekers and Assistants. This design\nenables rigorous evaluation of conversational inference, including not only\nalignment between predicted and ground-truth preferences, but also calibration\nagainst full rating distributions and comparison with explicit and implicit\nuser satisfaction signals.\n  Our initial analyses of VOGUE reveal distinctive dynamics of visually\ngrounded dialogue. For example, recommenders frequently suggest items\nsimultaneously in feature-based groups, which creates distinct conversational\nphases bridged by Seeker critiques and refinements. Benchmarking multimodal\nlarge language models against human recommenders shows that while MLLMs\napproach human-level alignment in aggregate, they exhibit systematic\ndistribution errors in reproducing human ratings and struggle to generalize\npreference inference beyond explicitly discussed items. These findings\nestablish VOGUE as both a unique resource for studying multimodal\nconversational systems and as a challenge dataset beyond the current\nrecommendation capabilities of existing top-tier multimodal foundation models\nsuch as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash."}
{"id": "2510.21253", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21253", "abs": "https://arxiv.org/abs/2510.21253", "authors": ["Boaz Moav", "Ryan Gabrys", "Eitan Yaakobi"], "title": "Complex DNA Synthesis Sequences", "comment": null, "summary": "DNA-based storage offers unprecedented density and durability, but its\nscalability is fundamentally limited by the efficiency of parallel strand\nsynthesis. Existing methods either allow unconstrained nucleotide additions to\nindividual strands, such as enzymatic synthesis, or enforce identical additions\nacross many strands, such as photolithographic synthesis. We introduce and\nanalyze a hybrid synthesis framework that generalizes both approaches: in each\ncycle, a nucleotide is selected from a restricted subset and incorporated in\nparallel. This model gives rise to a new notion of a complex synthesis\nsequence. Building on this framework, we extend the information rate definition\nof Lenz et al. and analyze an analog of the deletion ball, defined and studied\nin this setting, deriving tight expressions for the maximal information rate\nand its asymptotic behavior. These results bridge the theoretical gap between\nconstrained models and the idealized setting in which every nucleotide is\nalways available. For the case of known strands, we design a dynamic\nprogramming algorithm that computes an optimal complex synthesis sequence,\nhighlighting structural similarities to the shortest common supersequence\nproblem. We also define a distinct two-dimensional array model with synthesis\nconstraints over the rows, which extends previous synthesis models in the\nliterature and captures new structural limitations in large-scale strand\narrays. Additionally, we develop a dynamic programming algorithm for this\nproblem as well. Our results establish a new and comprehensive theoretical\nframework for constrained DNA, subsuming prior models and setting the stage for\nfuture advances in the field."}
{"id": "2510.21200", "categories": ["cs.GT", "cs.CY", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.21200", "abs": "https://arxiv.org/abs/2510.21200", "authors": ["Ashlesha Hota", "Susobhan Bandopadhyay", "Palash Dey", "Shruti Thiagu"], "title": "Shift Bribery over Social Networks", "comment": null, "summary": "In shift bribery, a briber seeks to promote his preferred candidate by paying\nvoters to raise their ranking. Classical models of shift bribery assume voters\nact independently, overlooking the role of social influence. However, in\nreality, individuals are social beings and are often represented as part of a\nsocial network, where bribed voters may influence their neighbors, thereby\namplifying the effect of persuasion. We study Shift bribery over Networks,\nwhere voters are modeled as nodes in a directed weighted graph, and arcs\nrepresent social influence between them. In this setting, bribery is not\nconfined to directly targeted voters its effects can propagate through the\nnetwork, influencing neighbors and amplifying persuasion. Given a budget and\nindividual cost functions for shifting each voter's preference toward a\ndesignated candidate, the goal is to determine whether a shift strategy exists\nwithin budget that ensures the preferred candidate wins after both direct and\nnetwork-propagated influence takes effect. We show that the problem is\nNP-Complete even with two candidates and unit costs, and W[2]-hard when\nparameterized by budget or maximum degree. On the positive side, we design\npolynomial-time algorithms for complete graphs under plurality and majority\nrules and path graphs for uniform edge weights, linear-time algorithms for\ntransitive tournaments for two candidates, linear cost functions and uniform\narc weights, and pseudo-polynomial algorithms for cluster graphs. We further\nprove the existence of fixed-parameter tractable algorithms with treewidth as\nparameter for two candidates, linear cost functions and uniform arc weights and\npseudo-FPT with cluster vertex deletion number for two candidates and uniform\narc weights. Together, these results give a detailed complexity landscape for\nshift bribery in social networks."}
{"id": "2510.21287", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.21287", "abs": "https://arxiv.org/abs/2510.21287", "authors": ["Chaitanya Swamy", "Vera Traub", "Laura Vargas Koch", "Rico Zenklusen"], "title": "Unsplittable Cost Flows from Unweighted Error-Bounded Variants", "comment": null, "summary": "A famous conjecture of Goemans on single-source unsplittable flows states\nthat one can turn any fractional flow into an unsplittable one of no higher\ncost, while increasing the load on any arc by at most the maximum demand.\nDespite extensive work on the topic, only limited progress has been made.\nRecently, Morell and Skutella suggested an alternative conjecture, stating that\none can turn any fractional flow into an unsplittable one without changing the\nload on any arc by more than the maximum demand.\n  We show that their conjecture implies Goemans' conjecture (with a violation\nof twice the maximum demand). To this end, we generalize a technique of\nLinhares and Swamy, used to obtain a low-cost chain-constrained spanning tree\nfrom an algorithm without cost guarantees. Whereas Linhares and Swamy's proof\nrelies on Langrangian duality, we provide a very simple elementary proof of a\ngeneralized version, which we hope to be of independent interest. Moreover, we\nshow how this technique can also be used in the context of the weighted ring\nloading problem, showing that cost-unaware approximation algorithms can be\ntransformed into approximation algorithms with additional cost guarantees."}
{"id": "2510.21242", "categories": ["cs.IR", "H.3.3; H.3.5"], "pdf": "https://arxiv.org/pdf/2510.21242", "abs": "https://arxiv.org/abs/2510.21242", "authors": ["Yimeng Bai", "Chang Liu", "Yang Zhang", "Dingxian Wang", "Frank Yang", "Andrew Rabinovich", "Wenge Rong", "Fuli Feng"], "title": "Bi-Level Optimization for Generative Recommendation: Bridging Tokenization and Generation", "comment": null, "summary": "Generative recommendation is emerging as a transformative paradigm by\ndirectly generating recommended items, rather than relying on matching.\nBuilding such a system typically involves two key components: (1) optimizing\nthe tokenizer to derive suitable item identifiers, and (2) training the\nrecommender based on those identifiers. Existing approaches often treat these\ncomponents separately--either sequentially or in alternation--overlooking their\ninterdependence. This separation can lead to misalignment: the tokenizer is\ntrained without direct guidance from the recommendation objective, potentially\nyielding suboptimal identifiers that degrade recommendation performance.\n  To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative\nRecommendation framework, which explicitly models the interdependence between\nthe tokenizer and the recommender in a unified optimization process. The lower\nlevel trains the recommender using tokenized sequences, while the upper level\noptimizes the tokenizer based on both the tokenization loss and recommendation\nloss. We adopt a meta-learning approach to solve this bi-level optimization\nefficiently, and introduce gradient surgery to mitigate gradient conflicts in\nthe upper-level updates, thereby ensuring that item identifiers are both\ninformative and recommendation-aligned. Extensive experiments on real-world\ndatasets demonstrate that BLOGER consistently outperforms state-of-the-art\ngenerative recommendation methods while maintaining practical efficiency with\nno significant additional computational overhead, effectively bridging the gap\nbetween item tokenization and autoregressive generation."}
{"id": "2510.21299", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21299", "abs": "https://arxiv.org/abs/2510.21299", "authors": ["Shengkang Chen", "Tong Wu", "Zhiyong Chen", "Feng Yang", "Meixia Tao", "Wenjun Zhang"], "title": "Text-Guided Diffusion Model-based Generative Communication for Wireless Image Transmission", "comment": "submitted to IEEE journal", "summary": "Reliable image transmission over wireless channels is particularly\nchallenging at extremely low transmission rates, where conventional compression\nand channel coding schemes fail to preserve adequate visual quality. To address\nthis issue, we propose a generative communication framework based on diffusion\nmodels, which integrates joint source channel coding (JSCC) with\nsemantic-guided reconstruction leveraging a pre-trained generative model.\nUnlike conventional architectures that aim to recover exact pixel values of the\noriginal image, the proposed method focuses on preserving and reconstructing\nsemantically meaningful visual content under severely constrained rates,\nensuring perceptual plausibility and faithfulness to the scene intent.\nSpecifically, the transmitter encodes the source image via JSCC and jointly\ntransmits it with a textual prompt over the wireless channel. At the receiver,\nthe corrupted low-rate representation is fused with the prompt and\nreconstructed through a Stable Diffusion model with ControlNet, enabling\nhigh-quality visual recovery. Leveraging both generative priors and semantic\nguidance, the proposed framework produces perceptually convincing images even\nunder extreme bandwidth limitations. Experimental results demonstrate that the\nproposed method consistently outperforms conventional coding-based schemes and\ndeep learning baselines, achieving superior perceptual quality and robustness\nacross various channel conditions."}
{"id": "2510.21231", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.21231", "abs": "https://arxiv.org/abs/2510.21231", "authors": ["Jason Hartline", "Aleck Johnsen", "Yingkai Li"], "title": "Scale-robust Auctions", "comment": "Part of the results from this work has appeared in FOCS'20 under the\n  title \"Benchmark Design and Prior-independent Optimization\"", "summary": "We study auctions that are robust at any scale, i.e., they can be applied to\nsell both expensive and cheap items and achieve the best multiplicative\napproximations of the optimal revenue in the worst case. We show that the\noptimal mechanism is scale invariant, which randomizes between selling at the\nsecond-price and a 2.45 multiple of the second-price."}
{"id": "2510.21327", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.21327", "abs": "https://arxiv.org/abs/2510.21327", "authors": ["Sebastian Brandt", "Fabian Kuhn", "Zahra Parsaeian"], "title": "On the Complexity of Distributed Edge Coloring and Orientation Problems", "comment": null, "summary": "Understanding the role of randomness when solving locally checkable labeling\n(LCL) problems in the LOCAL model has been one of the top priorities in the\nresearch on distributed graph algorithms in recent years. For LCL problems in\nbounded-degree graphs, it is known that randomness cannot help more than\npolynomially, except in one case: if the deterministic complexity of an LCL\nproblem is in $\\Omega(\\log n)$ and its randomized complexity is in $o(\\log n)$,\nthen the randomized complexity is guaranteed to be $poly(\\log \\log n)$.\nHowever, the fundamental question of \\emph{which} problems with a deterministic\ncomplexity of $\\Omega(\\log n)$ can be solved exponentially faster using\nrandomization still remains wide open.\n  We make a step towards answering this question by studying a simple, but\nnatural class of LCL problems: so-called degree splitting problems. These\nproblems come in two varieties: coloring problems where the edges of a graph\nhave to be colored with $2$ colors and orientation problems where each edge\nneeds to be oriented. For $\\Delta$-regular graphs (where $\\Delta=O(1)$), we\nobtain the following results.\n  - We gave an exact characterization of the randomized complexity of all\nproblems where the edges need to be colored with two colors, say red and blue,\nand which have a deterministic complexity of $O(\\log n)$. - For edge\norientation problems, we give a partial characterization of the problems that\nhave a randomized complexity of $\\Omega(\\log n)$ and the problems that have a\nrandomized complexity of $poly\\log\\log n$.\n  While our results are cleanest to state for the $\\Delta$-regular case, all\nour algorithms naturally generalize to nodes of any degree $d<\\Delta$ in\ngeneral graphs of maximum degree $\\Delta$."}
{"id": "2510.21276", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21276", "abs": "https://arxiv.org/abs/2510.21276", "authors": ["Qiyong Zhong", "Jiajie Su", "Yunshan Ma", "Julian McAuley", "Yupeng Hou"], "title": "Pctx: Tokenizing Personalized Context for Generative Recommendation", "comment": null, "summary": "Generative recommendation (GR) models tokenize each action into a few\ndiscrete tokens (called semantic IDs) and autoregressively generate the next\ntokens as predictions, showing advantages such as memory efficiency,\nscalability, and the potential to unify retrieval and ranking. Despite these\nbenefits, existing tokenization methods are static and non-personalized. They\ntypically derive semantic IDs solely from item features, assuming a universal\nitem similarity that overlooks user-specific perspectives. However, under the\nautoregressive paradigm, semantic IDs with the same prefixes always receive\nsimilar probabilities, so a single fixed mapping implicitly enforces a\nuniversal item similarity standard across all users. In practice, the same item\nmay be interpreted differently depending on user intentions and preferences. To\naddress this issue, we propose a personalized context-aware tokenizer that\nincorporates a user's historical interactions when generating semantic IDs.\nThis design allows the same item to be tokenized into different semantic IDs\nunder different user contexts, enabling GR models to capture multiple\ninterpretive standards and produce more personalized predictions. Experiments\non three public datasets demonstrate up to 11.44% improvement in NDCG@10 over\nnon-personalized action tokenization baselines. Our code is available at\nhttps://github.com/YoungZ365/Pctx."}
{"id": "2510.21386", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21386", "abs": "https://arxiv.org/abs/2510.21386", "authors": ["Xiaotian Fan", "Xingyu Zhou", "Le Liang", "Shi Jin"], "title": "Low-Complexity MIMO Channel Estimation with Latent Diffusion Models", "comment": null, "summary": "Deep generative models offer a powerful alternative to conventional channel\nestimation by learning the complex prior distribution of wireless channels.\nCapitalizing on this potential, this paper proposes a novel channel estimation\nalgorithm based on latent diffusion models (LDMs), termed posterior sampling\nwith latent diffusion for channel estimation (PSLD-CE). The core of our\napproach is a lightweight LDM architecture specifically designed for channel\nestimation, which serves as a powerful generative prior to capture the\nintricate channel distribution. Furthermore, we enhance the diffusion posterior\nsampling process by introducing an effective approximation for the likelihood\nterm and a tailored self-consistency constraint on the variational autoencoder\nlatent space. Extensive experimental results demonstrate that PSLD-CE\nconsistently outperforms a wide range of existing methods. Notably, these\nsignificant performance gains are achieved while maintaining low computational\ncomplexity and fast inference speed, establishing our method as a highly\npromising and practical solution for next-generation wireless systems."}
{"id": "2510.21442", "categories": ["cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.21442", "abs": "https://arxiv.org/abs/2510.21442", "authors": ["Nathan Corecco", "Batuhan Yardim", "Vinzenz Thoma", "Zebang Shen", "Niao He"], "title": "Scalable Neural Incentive Design with Parameterized Mean-Field Approximation", "comment": "52 pages, to appear at NeurIPS 2025", "summary": "Designing incentives for a multi-agent system to induce a desirable Nash\nequilibrium is both a crucial and challenging problem appearing in many\ndecision-making domains, especially for a large number of agents $N$. Under the\nexchangeability assumption, we formalize this incentive design (ID) problem as\na parameterized mean-field game (PMFG), aiming to reduce complexity via an\ninfinite-population limit. We first show that when dynamics and rewards are\nLipschitz, the finite-$N$ ID objective is approximated by the PMFG at rate\n$\\mathscr{O}(\\frac{1}{\\sqrt{N}})$. Moreover, beyond the Lipschitz-continuous\nsetting, we prove the same $\\mathscr{O}(\\frac{1}{\\sqrt{N}})$ decay for the\nimportant special case of sequential auctions, despite discontinuities in\ndynamics, through a tailored auction-specific analysis. Built on our novel\napproximation results, we further introduce our Adjoint Mean-Field Incentive\nDesign (AMID) algorithm, which uses explicit differentiation of iterated\nequilibrium operators to compute gradients efficiently. By uniting\napproximation bounds with optimization guarantees, AMID delivers a powerful,\nscalable algorithmic tool for many-agent (large $N$) ID. Across diverse auction\nsettings, the proposed AMID method substantially increases revenue over\nfirst-price formats and outperforms existing benchmark methods."}
{"id": "2510.21423", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.21423", "abs": "https://arxiv.org/abs/2510.21423", "authors": ["Linh Anh Nguyen"], "title": "Approximate minimization of interpretations in fuzzy description logics under the Gödel semantics", "comment": null, "summary": "The problem of minimizing fuzzy interpretations in fuzzy description logics\n(FDLs) is important both theoretically and practically. For instance, fuzzy or\nweighted social networks can be modeled as fuzzy interpretations, where\nindividuals represent actors and roles capture interactions. Minimizing such\ninterpretations yields more compact representations, which can significantly\nimprove the efficiency of reasoning and analysis tasks in knowledge-based\nsystems. We present the first algorithm that minimizes a finite fuzzy\ninterpretation while preserving fuzzy concept assertions in FDLs without the\nBaaz projection operator and the universal role, under the G\\\"odel semantics.\nThe considered class of FDLs ranges from the sublogic of $f\\!\\mathcal{ALC}$\nwithout the union operator and universal restriction to the FDL that extends\n$f\\!\\mathcal{ALC}_{reg}$ with inverse roles and nominals. Our algorithm is\ngiven in an extended form that supports approximate preservation: it minimizes\na finite fuzzy interpretation $\\mathcal{I}$ while preserving fuzzy concept\nassertions up to a degree $\\gamma \\in (0,1]$. Its time complexity is\n$O((m\\log{l} + n)\\log{n})$, where $n$ is the size of the domain of\n$\\mathcal{I}$, $m$ is the number of nonzero instances of atomic roles in\n$\\mathcal{I}$, and $l$ is the number of distinct fuzzy values used in such\ninstances plus 2."}
{"id": "2510.21333", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21333", "abs": "https://arxiv.org/abs/2510.21333", "authors": ["Yunbo Hou", "Tianle Yang", "Ruijie Li", "Li He", "Liang Wang", "Weiping Li", "Bo Zheng", "Guojie Song"], "title": "CausalRec: A CausalBoost Attention Model for Sequential Recommendation", "comment": "11 pages, 3 figures", "summary": "Recent advances in correlation-based sequential recommendation systems have\ndemonstrated substantial success. Specifically, the attention-based model\noutperforms other RNN-based and Markov chains-based models by capturing both\nshort- and long-term dependencies more effectively. However, solely focusing on\nitem co-occurrences overlooks the underlying motivations behind user behaviors,\nleading to spurious correlations and potentially inaccurate recommendations. To\naddress this limitation, we present a novel framework that integrates causal\nattention for sequential recommendation, CausalRec. It incorporates a causal\ndiscovery block and a CausalBooster. The causal discovery block learns the\ncausal graph in user behavior sequences, and we provide a theory to guarantee\nthe identifiability of the learned causal graph. The CausalBooster utilizes the\ndiscovered causal graph to refine the attention mechanism, prioritizing\nbehaviors with causal significance. Experimental evaluations on real-world\ndatasets indicate that CausalRec outperforms several state-of-the-art methods,\nwith average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized\nDiscounted Cumulative Gain (NDCG). To the best of our knowledge, this is the\nfirst model to incorporate causality through the attention mechanism in\nsequential recommendation, demonstrating the value of causality in generating\nmore accurate and reliable recommendations."}
{"id": "2510.21414", "categories": ["cs.IT", "cs.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21414", "abs": "https://arxiv.org/abs/2510.21414", "authors": ["Hoang Ly", "Emina Soljanin"], "title": "Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication", "comment": null, "summary": "Maximum-likelihood (ML) decoding for arbitrary block codes remains\nfundamentally hard, with worst-case time complexity-measured by the total\nnumber of multiplications-being no better than straightforward exhaustive\nsearch, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper\nintroduces a simple, code-agnostic framework that reduces the worst-case\ncomplexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable\nreduction in practice. The result holds for both linear and nonlinear block\ncodes over general memoryless channels and under both hard-decision and\nsoft-decision decoding. It naturally extends to intersymbol-interference (ISI)\nchannels and ML list decoding with only a negligible increase in complexity.\nOur core insight is that, upon receipt of each sequence at the receiver, the\nconditional probability of that sequence for each codeword in the codebook\n(i.e., the \\emph{likelihood}) can be expressed as the inner product of two\ncarefully constructed vectors -- the first depending on the received sequence,\nand the second on that codeword itself. As a result, evaluating the likelihoods\nfor all codewords in the codebook reduces to a single vector-matrix\nmultiplication, and ML decoding (MLD) becomes the simple task of picking the\nmaximum entry in the resulting vector. The only non-trivial cost lies in the\nvector-matrix product. However, our matrix construction allows the use of the\nMailman algorithm to reduce this cost. This time reduction is achieved at the\ncost of high space complexity, requiring $\\mathcal{O}(q^{k+1} n)$ space to\nstore the pre-computed codebook matrix."}
{"id": "2510.21668", "categories": ["cs.GT", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21668", "abs": "https://arxiv.org/abs/2510.21668", "authors": ["Zhaoyang Cheng", "Guanpu Chen", "Tobias J. Oechtering", "Mikael Skoglund"], "title": "Privacy Guarantee for Nash Equilibrium Computation of Aggregative Games Based on Pointwise Maximal Leakage", "comment": null, "summary": "Privacy preservation has served as a key metric in designing Nash equilibrium\n(NE) computation algorithms. Although differential privacy (DP) has been widely\nemployed for privacy guarantees, it does not exploit prior distributional\nknowledge of datasets and is ineffective in assessing information leakage for\ncorrelated datasets. To address these concerns, we establish a pointwise\nmaximal leakage (PML) framework when computing NE in aggregative games. By\nincorporating prior knowledge of players' cost function datasets, we obtain a\nprecise and computable upper bound of privacy leakage with PML guarantees. In\nthe entire view, we show PML refines DP by offering a tighter privacy\nguarantee, enabling flexibility in designing NE computation. Also, in the\nindividual view, we reveal that the lower bound of PML can exceed the upper\nbound of DP by constructing specific correlated datasets. The results emphasize\nthat PML is a more proper privacy measure than DP since the latter fails to\nadequately capture privacy leakage in correlated datasets. Moreover, we conduct\nexperiments with adversaries who attempt to infer players' private information\nto illustrate the effectiveness of our framework."}
{"id": "2510.21540", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.21540", "abs": "https://arxiv.org/abs/2510.21540", "authors": ["Christian Wallisch", "Till Fluschnik", "Leon Kellerhals"], "title": "Placing Green Bridges Optimally, with Close-Range Habitats in Sparse Graphs", "comment": null, "summary": "We study a network design problem motivated by the challenge of placing\nwildlife crossings to reconnect fragmented habitats of animal species, which is\namong the 17 goals towards sustainable development by the UN: Given a graph,\nwhose vertices represent the fragmented habitat areas and whose edges represent\npossible green bridge locations (with costs), and the habitable vertex set for\neach species' habitat, the goal is to find the cheapest set of edges such that\neach species' habitat is sufficiently connected. We focus on the established\nvariant where a habitat is considered sufficiently connected if it has diameter\ntwo in the solution and study its complexity in cases justified by our setting\nnamely small habitat sizes on planar graphs and graphs of small maximum degree\n$\\Delta$. We provide efficient algorithms and NP-hardness results for different\nvalues of $\\Delta$ and maximum habitat sizes on general and planar graphs."}
{"id": "2510.21352", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21352", "abs": "https://arxiv.org/abs/2510.21352", "authors": ["Mariam Arustashvili", "Krisztian Balog"], "title": "SciNUP: Natural Language User Interest Profiles for Scientific Literature Recommendation", "comment": null, "summary": "The use of natural language (NL) user profiles in recommender systems offers\ngreater transparency and user control compared to traditional representations.\nHowever, there is scarcity of large-scale, publicly available test collections\nfor evaluating NL profile-based recommendation. To address this gap, we\nintroduce SciNUP, a novel synthetic dataset for scholarly recommendation that\nleverages authors' publication histories to generate NL profiles and\ncorresponding ground truth items. We use this dataset to conduct a comparison\nof baseline methods, ranging from sparse and dense retrieval approaches to\nstate-of-the-art LLM-based rerankers. Our results show that while baseline\nmethods achieve comparable performance, they often retrieve different items,\nindicating complementary behaviors. At the same time, considerable headroom for\nimprovement remains, highlighting the need for effective NL-based\nrecommendation approaches. The SciNUP dataset thus serves as a valuable\nresource for fostering future research and development in this area."}
{"id": "2510.21587", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21587", "abs": "https://arxiv.org/abs/2510.21587", "authors": ["Bho Matthiesen", "Armin Dekorsy", "Petar Popovski"], "title": "Resilient Radio Access Networks: AI and the Unknown Unknowns", "comment": "Accepted for presentation at 2025 IEEE Globecom Workshop on\n  Resilience in Next-Generation Wireless Communication Networks", "summary": "5G networks offer exceptional reliability and availability, ensuring\nconsistent performance and user satisfaction. Yet they might still fail when\nconfronted with the unexpected. A resilient system is able to adapt to\nreal-world complexity, including operating conditions completely unanticipated\nduring system design. This makes resilience a vital attribute for communication\nsystems that must sustain service in scenarios where models are absent or too\nintricate to provide statistical guarantees. Such considerations indicate that\nartifical intelligence (AI) will play a major role in delivering resilience. In\nthis paper, we examine the challenges of designing AIs for resilient radio\naccess networks, especially with respect to unanticipated and rare disruptions.\nOur theoretical results indicate strong limitations of current statistical\nlearning methods for resilience and suggest connections to online learning and\ncausal inference."}
{"id": "2510.21613", "categories": ["cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.21613", "abs": "https://arxiv.org/abs/2510.21613", "authors": ["Eleon Bach", "Alexander E. Black", "Sophie Huiberts", "Sean Kafer"], "title": "Beyond Smoothed Analysis: Analyzing the Simplex Method by the Book", "comment": "42 pages", "summary": "Narrowing the gap between theory and practice is a longstanding goal of the\nalgorithm analysis community. To further progress our understanding of how\nalgorithms work in practice, we propose a new algorithm analysis framework that\nwe call by the book analysis. In contrast to earlier frameworks, by the book\nanalysis not only models an algorithm's input data, but also the algorithm\nitself. Results from by the book analysis are meant to correspond well with\nestablished knowledge of an algorithm's practical behavior, as they are meant\nto be grounded in observations from implementations, input modeling best\npractices, and measurements on practical benchmark instances. We apply our\nframework to the simplex method, an algorithm which is beloved for its\nexcellent performance in practice and notorious for its high running time under\nworst-case analysis. The simplex method similarly showcased the state of the\nart framework smoothed analysis (Spielman and Teng, STOC'01). We explain how\nour framework overcomes several weaknesses of smoothed analysis and we prove\nthat under input scaling assumptions, feasibility tolerances and other design\nprinciples used by simplex method implementations, the simplex method indeed\nattains a polynomial running time."}
{"id": "2510.21603", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21603", "abs": "https://arxiv.org/abs/2510.21603", "authors": ["Kuicai Dong", "Shurui Huang", "Fangda Ye", "Wei Han", "Zhi Zhang", "Dexun Li", "Wenjun Li", "Qu Yang", "Gang Wang", "Yichao Wang", "Chen Zhang", "Yong Liu"], "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research", "comment": "preprint", "summary": "Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections."}
{"id": "2510.21668", "categories": ["cs.GT", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21668", "abs": "https://arxiv.org/abs/2510.21668", "authors": ["Zhaoyang Cheng", "Guanpu Chen", "Tobias J. Oechtering", "Mikael Skoglund"], "title": "Privacy Guarantee for Nash Equilibrium Computation of Aggregative Games Based on Pointwise Maximal Leakage", "comment": null, "summary": "Privacy preservation has served as a key metric in designing Nash equilibrium\n(NE) computation algorithms. Although differential privacy (DP) has been widely\nemployed for privacy guarantees, it does not exploit prior distributional\nknowledge of datasets and is ineffective in assessing information leakage for\ncorrelated datasets. To address these concerns, we establish a pointwise\nmaximal leakage (PML) framework when computing NE in aggregative games. By\nincorporating prior knowledge of players' cost function datasets, we obtain a\nprecise and computable upper bound of privacy leakage with PML guarantees. In\nthe entire view, we show PML refines DP by offering a tighter privacy\nguarantee, enabling flexibility in designing NE computation. Also, in the\nindividual view, we reveal that the lower bound of PML can exceed the upper\nbound of DP by constructing specific correlated datasets. The results emphasize\nthat PML is a more proper privacy measure than DP since the latter fails to\nadequately capture privacy leakage in correlated datasets. Moreover, we conduct\nexperiments with adversaries who attempt to infer players' private information\nto illustrate the effectiveness of our framework."}
{"id": "2510.21700", "categories": ["cs.DS", "cs.CG", "cs.DM", "math.CO", "math.MG"], "pdf": "https://arxiv.org/pdf/2510.21700", "abs": "https://arxiv.org/abs/2510.21700", "authors": ["Hsien-Chih Chang", "Jonathan Conroy", "Zihan Tan", "Da Wei Zheng"], "title": "O(1)-Distortion Planar Emulators for String Graphs", "comment": null, "summary": "We show that every unweighted string graph $G$ has an $O(1)$-distortion\nplanar emulator: that is, there exists an (edge-weighted) planar graph $H$ with\n$V(H) = V(G)$, such that every pair of vertices $(u,v)$ satisfies\n$\\delta_G(u,v) \\le \\delta_H(u,v) \\le O(1) \\cdot \\delta_G(u,v).$"}
{"id": "2510.21671", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21671", "abs": "https://arxiv.org/abs/2510.21671", "authors": ["Yabo Yin", "Yang Xi", "Jialong Wang", "Shanqi Wang", "Jiateng Hu"], "title": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study on Query-Category and Query-Item Relevance", "comment": null, "summary": "Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings."}
{"id": "2510.21414", "categories": ["cs.IT", "cs.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21414", "abs": "https://arxiv.org/abs/2510.21414", "authors": ["Hoang Ly", "Emina Soljanin"], "title": "Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication", "comment": null, "summary": "Maximum-likelihood (ML) decoding for arbitrary block codes remains\nfundamentally hard, with worst-case time complexity-measured by the total\nnumber of multiplications-being no better than straightforward exhaustive\nsearch, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper\nintroduces a simple, code-agnostic framework that reduces the worst-case\ncomplexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable\nreduction in practice. The result holds for both linear and nonlinear block\ncodes over general memoryless channels and under both hard-decision and\nsoft-decision decoding. It naturally extends to intersymbol-interference (ISI)\nchannels and ML list decoding with only a negligible increase in complexity.\nOur core insight is that, upon receipt of each sequence at the receiver, the\nconditional probability of that sequence for each codeword in the codebook\n(i.e., the \\emph{likelihood}) can be expressed as the inner product of two\ncarefully constructed vectors -- the first depending on the received sequence,\nand the second on that codeword itself. As a result, evaluating the likelihoods\nfor all codewords in the codebook reduces to a single vector-matrix\nmultiplication, and ML decoding (MLD) becomes the simple task of picking the\nmaximum entry in the resulting vector. The only non-trivial cost lies in the\nvector-matrix product. However, our matrix construction allows the use of the\nMailman algorithm to reduce this cost. This time reduction is achieved at the\ncost of high space complexity, requiring $\\mathcal{O}(q^{k+1} n)$ space to\nstore the pre-computed codebook matrix."}
