{"id": "2511.17913", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17913", "abs": "https://arxiv.org/abs/2511.17913", "authors": ["Wenxi Dai", "Wujiang Xu", "Pinhuan Wang", "Dimitris N. Metaxas"], "title": "Token-Controlled Re-ranking for Sequential Recommendation via LLMs", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) as re-rankers is shifting recommender systems towards a user-centric paradigm. However, a significant gap remains: current re-rankers often lack mechanisms for fine-grained user control. They struggle to balance inherent user preferences with multiple attribute-based constraints, often resorting to simplistic hard filtering that can excessively narrow the recommendation pool and yield suboptimal results. This limitation leaves users as passive recipients rather than active collaborators in the recommendation process. To bridge this gap, we propose COREC, a novel token-augmented re-ranking framework that incorporates specific user requirements in co-creating the recommendation outcome. COREC empowers users to steer re-ranking results with precise and flexible control via explicit, attribute-based signals. The framework learns to balance these commands against latent preferences, yielding rankings that adhere to user instructions without sacrificing personalization. Experiments show that COREC: (1) exceeds state-of-the-art baselines on standard recommendation effectiveness and (2) demonstrates superior adherence to specific attribute requirements, proving that COREC enables fine-grained and predictable manipulation of the rankings."}
{"id": "2511.18013", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18013", "abs": "https://arxiv.org/abs/2511.18013", "authors": ["Weijie Jiang", "Armando Ordorica", "Jaewon Yang", "Olafur Gudmundsson", "Yucheng Tu", "Huizhong Duan"], "title": "Save, Revisit, Retain: A Scalable Framework for Enhancing User Retention in Large-Scale Recommender Systems", "comment": null, "summary": "User retention is a critical objective for online platforms like Pinterest, as it strengthens user loyalty and drives growth through repeated engagement. A key indicator of retention is revisitation, i.e., when users return to view previously saved content, a behavior often sparked by personalized recommendations and user satisfaction. However, modeling and optimizing revisitation poses significant challenges. One core difficulty is accurate attribution: it is often unclear which specific user actions or content exposures trigger a revisit, since many confounding factors (e.g., content quality, user interface, notifications, or even changing user intent) can influence return behavior. Additionally, the scale and timing of revisitations introduce further complexity; users may revisit content days or even weeks after their initial interaction, requiring the system to maintain and associate extensive historical records across millions of users and sessions. These complexities render existing methods insufficient for robustly capturing and optimizing long-term revisitation. To address these gaps, we introduce a novel, lightweight, and interpretable framework for modeling revisitation behavior and optimizing long-term user retention in Pinterest's search-based recommendation context. By defining a surrogate attribution process that links saves to subsequent revisitations, we reduce noise in the causal relationship between user actions and return visits. Our scalable event aggregation pipeline enables large-scale analysis of user revisitation patterns and enhances the ranking system's ability to surface items with high retention value. Deployed on Pinterest's Related Pins surface to serve 500+ million users, the framework led to a significant lift of 0.1% in active users without additional computational costs."}
{"id": "2511.18024", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18024", "abs": "https://arxiv.org/abs/2511.18024", "authors": ["Dor Arviv", "Yehonatan Elisha", "Oren Barkan", "Noam Koenigstein"], "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems", "comment": null, "summary": "We present a method for extracting \\emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \\emph{prediction aware} training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec."}
{"id": "2511.18047", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18047", "abs": "https://arxiv.org/abs/2511.18047", "authors": ["Oren Barkan", "Yahlly Schein", "Yehonatan Elisha", "Veronika Bogina", "Mikhail Baklanov", "Noam Koenigstein"], "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration", "comment": null, "summary": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec."}
{"id": "2511.18415", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18415", "abs": "https://arxiv.org/abs/2511.18415", "authors": ["Wei Yang", "Yiran Zhu", "Zilin Li", "Xunjia Zhang", "Hongtao Wang"], "title": "Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation", "comment": "21 pages, 18 tables, 6 figures", "summary": "Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks, where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels. We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher. It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route to imbue compact VLMs with dependency-aware multi-step reasoning."}
{"id": "2511.17733", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.17733", "abs": "https://arxiv.org/abs/2511.17733", "authors": ["Tristan Mott", "Caleb Bradshaw", "David Grimsman", "Christopher Archibald"], "title": "The Impacts of Increasingly Complex Matchup Models on Baseball Win Probability", "comment": "Undergraduate Honors Thesis, Brigham Young University", "summary": "Baseball is a game of strategic decisions including bullpen usage, pinch-hitting and intentional walks. Managers must adjust their strategies based on the changing state of the game in order to give their team the best chance of winning. In this thesis, we investigate how matchup models -- tools that predict the probabilities of plate appearance outcomes -- impact in-game strategy and ultimately affect win probability. We develop four progressively complex, hierarchical Bayesian models that predict plate appearance outcomes by combining information from both pitchers and batters, their handedness, and recent data, along with base running probabilities calibrated to a player's base-stealing tendencies.\n  Using each model within a game-theoretic framework, we approximate subgame perfect Nash equilibria for in-game decisions, including substitutions and intentional walks. Simulations of the 2024 MLB postseason show that more accurate matchup models can yield tangible gains in win probability -- as much as one additional victory per 162-game season. Furthermore, employing the most detailed model to generate win predictions for actual playoff games demonstrates alignment with market expectations, underscoring both the power and potential of advanced matchup modeling for on-field strategy and prediction."}
{"id": "2511.17707", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.17707", "abs": "https://arxiv.org/abs/2511.17707", "authors": ["Elise Tate", "Joshua A. Grochow"], "title": "Reconstructing Sets of Strings from Their k-way Projections: Algorithms & Complexity", "comment": null, "summary": "Graphs are a powerful tool for analyzing large data sets, but many real-world phenomena involve interactions that go beyond the simple pairwise relationships captured by a graph. In this paper we introduce and study a simple combinatorial model to capture higher order dependencies from an algorithms and computational complexity perspective. Specifically, we introduce the String Set Reconstruction problem, which asks when a set of strings can be reconstructed from seeing only the k-way projections of strings in the set. This problem is distinguished from genetic reconstruction problems in that we allow projections from any k indices and we maintain knowledge of those indices, but not which k-mer came from which string. We give several results on the complexity of this problem, including hardness results, inapproximability, and parametrized complexity.\n  Our main result is the introduction of a new algorithm for this problem using a modified version of overlap graphs from genetic reconstruction algorithms. A key difference we must overcome is that in our setting the k-mers need not be contiguous, unlike the setting of genetic reconstruction. We exhibit our algorithm's efficiency in a variety of experiments, and give high-level explanations for how its complexity is observed to scale with various parameters. We back up these explanation with analytic approximations. We also consider the related problems of: whether a single string can be reconstructed from the k-way projections of a given set of strings, and finding the largest k at which we get no information about the original data set from its k-way projections (i.e., the largest $k$ for which it is \"k-wise independent\")."}
{"id": "2511.17503", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.17503", "abs": "https://arxiv.org/abs/2511.17503", "authors": ["Jon-Lark Kim", "Hongwei Liu", "Jinquan Luo"], "title": "How to Expand a Self-orthogonal Code", "comment": null, "summary": "In this paper, we show how to expand Euclidean/Hermitian self-orthogonal code preserving their orthogonal property. Our results show that every $k$-dimension Hermitian self-orthogonal code is contained in a $(k+1)$-dimensional Hermitian self-orthogonal code. Also, for $k< n/2-1$, every $[n,k]$ Euclidean self-orthogonal code is contained in an $[n,k+1]$ Euclidean self-orthogonal code. Moreover, for $k=n/2-1$ and $p=2$, we can also fulfill the expanding process. But for $k=n/2-1$ and $p$ odd prime, the expanding process can be fulfilled if and only if an extra condition must be satisfied. We also propose two feasible algorithms on these expanding procedures."}
{"id": "2511.17676", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17676", "abs": "https://arxiv.org/abs/2511.17676", "authors": ["Xi Wang", "Xianyao Ling", "Kun Li", "Gang Yin", "Liang Zhang", "Jiang Wu", "Annie Wang", "Weizhe Wang"], "title": "LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment", "comment": null, "summary": "The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed."}
{"id": "2511.18207", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18207", "abs": "https://arxiv.org/abs/2511.18207", "authors": ["Jiuzhou Fu", "Luanzheng Guo", "Nathan R. Tallent", "Dongfang Zhao"], "title": "ProHD: Projection-Based Hausdorff Distance Approximation", "comment": null, "summary": "The Hausdorff distance (HD) is a robust measure of set dissimilarity, but computing it exactly on large, high-dimensional datasets is prohibitively expensive. We propose \\textbf{ProHD}, a projection-guided approximation algorithm that dramatically accelerates HD computation while maintaining high accuracy. ProHD identifies a small subset of candidate \"extreme\" points by projecting the data onto a few informative directions (such as the centroid axis and top principal components) and computing the HD on this subset. This approach guarantees an underestimate of the true HD with a bounded additive error and typically achieves results within a few percent of the exact value. In extensive experiments on image, physics, and synthetic datasets (up to two million points in $D=256$), ProHD runs 10--100$\\times$ faster than exact algorithms while attaining 5--20$\\times$ lower error than random sampling-based approximations. Our method enables practical HD calculations in scenarios like large vector databases and streaming data, where quick and reliable set distance estimation is needed."}
{"id": "2511.18700", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18700", "abs": "https://arxiv.org/abs/2511.18700", "authors": ["Siran Chen", "Boyu Chen", "Chenyun Yu", "Yi Ouyang", "Cheng Lei", "Chengxiang Zhuo", "Zang Li", "Yali Wang"], "title": "When Top-ranked Recommendations Fail: Modeling Multi-Granular Negative Feedback for Explainable and Robust Video Recommendation", "comment": "Accepted in AAAI 2026", "summary": "Existing video recommendation systems, relying mainly on ID-based embedding mapping and collaborative filtering, often fail to capture in-depth video content semantics. Moreover, most struggle to address biased user behaviors (e.g., accidental clicks, fast skips), leading to inaccurate interest modeling and frequent negative feedback in top recommendations with unclear causes. To tackle this issue, we collect real-world user video-watching sequences, annotate the reasons for users' dislikes, and construct a benchmark dataset for personalized explanations. We then introduce the Agentic Explainable Negative Feedback (ENF) framework, which integrates three core components: (1) the Profile Agent, extracting behavioral cues from users' historical data to derive psychological and personality profiles; (2) the Video Agent, performing comprehensive multimodal video analysis; and (3) the Reason Agent, synthesizing information from the other two agents to predict user engagement and generate explanations. Additionally, we propose the S-GRPO algorithm, enabling the model to progressively address complex tasks during reinforcement fine-tuning. Experimental results on the collected dataset show that our method significantly outperforms state-of-the-art baselines in negative feedback prediction and reason explanation. Notably, it achieves an 8.6% improvement over GPT-4o in reason classification. Deployment on the business platform further validates its benefits: increasing average user watch time by 6.2%, reducing the fast-skip rate by 9.4%, and significantly enhancing user satisfaction."}
{"id": "2511.18328", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.18328", "abs": "https://arxiv.org/abs/2511.18328", "authors": ["Akaki Mamageishvili", "Christoph Schlegel", "Ko Sunghun", "Jinsuk Park", "Ali Taslimi"], "title": "TimeBoost: Do Ahead-of-Time Auctions Work?", "comment": null, "summary": "We study the performance of the TimeBoost auction, by comparing cumulative fixed time markout of fast lane trades over the TimeBoost interval to bids for the fast lane. Such comparison allows us to assess how well bids predict future extracted value from the time advantage. The correlation between winning bids and markouts is weak across bidders, suggesting that bids are a noisy predictor of extracted value. The correlation slightly improves when comparing paid bids (the second highest bid) instead of winning bids to markouts, which we attribute to the fact that the auction is more of a common value type. In all settings, the relative order of the most frequent bidder performance remains the same, together with their absolute profits. Bids and markouts aggregated over long time intervals exhibit much higher correlation, indicating that bidders detect trends much better than identify when the high arbitrage value is exactly available. One possible explanation for this is the fact that the correlation between previous minute markouts and current minute bids is significant, suggesting that the previous minute markouts is used to predict the next minute value when bidding."}
{"id": "2511.18253", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.18253", "abs": "https://arxiv.org/abs/2511.18253", "authors": ["Kent Quanrud", "Navid Tajkhorshid"], "title": "From Hop Reduction to Sparsification for Negative Length Shortest Paths", "comment": "Original STOC submission uploaded for archival purposes. A version 2 with updated running times will be uploaded promptly", "summary": "The textbook algorithm for real-weighted single-source shortest paths takes $O(m n)$ time on a graph with $m$ edges and $n$ vertices. A recent breakthrough algorithm by [Fin24] takes $\\tilde{O}(m n^{8/9})$ randomized time. The running time was subsequently improved to $\\tilde{O}(mn^{4/5})$ [HJQ25] and then $\\tilde{O}(m n^{3/4} + m^{4/5} n)$ [HJQ26].\n  We build on the algorithms of [Fin24, HJQ25, HJQ26] to obtain faster strongly-polynomial randomized-time algorithms for negative-length shortest paths. An important new technique in this algorithm repurposes previous \"hop-reducers\" from [Fin24, HJQ26] into \"negative edge sparsifiers\", reducing the number of negative edges by essentially the same factor by which the \"hops\" were previously reduced. A simple recursive algorithm based on sparsifying the layered hop reducers of [Fin24] already gives an $\\tilde{O}(m n^{\\smash{\\sqrt{3}}-1}) < O(mn^{.7321})$ randomized running time, improving [HJQ26] uniformly.\n  We also improve the construction of the bootstrapped hop reducers in [HJQ26] by proposing new sparse shortcut graphs replacing the dense shortcut graphs in [HJQ26]. Integrating all three of layered sparsification, recursion, and sparse bootstrapping into the algorithm of [HJQ26] gives new upper bounds of $O(mn^{.7193})$ randomized time for $m \\geq n^{1.03456}$ and $O((mn)^{.8620})$ randomized time for $m \\leq n^{1.03456}$."}
{"id": "2511.17504", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.17504", "abs": "https://arxiv.org/abs/2511.17504", "authors": ["Hassan ZivariFard", "Rémi A. Chou", "Xiaodong Wang"], "title": "Covert Communication and Key Generation Over Quantum State-Dependent Channels", "comment": "18 pages, 3 figures, two-column, accepted to IEEE Transactions on Information Theory, part of the results were presented at the 2024 IEEE Information Theory Workshop (ITW)", "summary": "We study covert communication and covert secret key generation with positive rates over quantum state-dependent channels. Specifically, we consider fully quantum state-dependent channels when the transmitter shares an entangled state with the channel. We study this problem setting under two security metrics. For the first security metric, the transmitter aims to communicate covertly with the receiver while simultaneously generating a covert secret key, and for the second security metric, the transmitter aims to transmit a secure message covertly and generate a covert secret key with the receiver simultaneously. Our main results include one-shot and asymptotic achievable positive covert-secret key rate pairs for both security metrics. Our results recover as a special case the best-known results for covert communication over state-dependent classical channels. To the best of our knowledge, our results are the first instance of achieving a positive rate for covert secret key generation and the first instance of achieving a positive covert rate over a quantum channel. Additionally, we show that our results are optimal when the channel is classical and the state is available non-causally at both the transmitter and the receiver."}
{"id": "2511.19008", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.19008", "abs": "https://arxiv.org/abs/2511.19008", "authors": ["Liuyi Chen", "Yuchen Hu", "Zhengyi Yang", "Xu Zhou", "Wenjie Zhang", "Kenli Li"], "title": "Efficient Partition-based Approaches for Diversified Top-k Subgraph Matching", "comment": null, "summary": "Subgraph matching is a core task in graph analytics, widely used in domains such as biology, finance, and social networks. Existing top-k diversified methods typically focus on maximizing vertex coverage, but often return results in the same region, limiting topological diversity. We propose the Distance-Diversified Top-k Subgraph Matching (DTkSM) problem, which selects k isomorphic matches with maximal pairwise topological distances to better capture global graph structure. To address its computational challenges, we introduce the Partition-based Distance Diversity (PDD) framework, which partitions the graph and retrieves diverse matches from distant regions. To enhance efficiency, we develop two optimizations: embedding-driven partition filtering and densest-based partition selection over a Partition Adjacency Graph. Experiments on 12 real world datasets show our approach achieves up to four orders of magnitude speedup over baselines, with 95% of results reaching 80% of optimal distance diversity and 100% coverage diversity."}
{"id": "2511.18261", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18261", "abs": "https://arxiv.org/abs/2511.18261", "authors": ["Shijun Li", "Yu Wang", "Jin Wang", "Ying Li", "Joydeep Ghosh", "Anne Cocos"], "title": "LLM Reasoning for Cold-Start Item Recommendation", "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases."}
{"id": "2511.19080", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19080", "abs": "https://arxiv.org/abs/2511.19080", "authors": ["Fan Nie", "Jiangqun Ni", "Jian Zhang", "Bin Zhang", "Weizhe Zhang", "Bin Li"], "title": "Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach", "comment": "TIFS AQE", "summary": "The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks."}
{"id": "2511.18418", "categories": ["cs.GT", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.18418", "abs": "https://arxiv.org/abs/2511.18418", "authors": ["Georgios C. Chasparis"], "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part B: Stochastic Stability in Weakly Acyclic Games", "comment": null, "summary": "Reinforcement-based learning dynamics may exhibit several limitations when applied in a distributed setup. In (repeatedly-played) multi-player/action strategic-form games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. Furthermore, strong convergence guarantees (i.e., almost sure convergence or weak convergence) are mostly restricted to two-player games. To address this main limitation of reinforcement-based learning in repeatedly-played strategic-form games, this paper introduces a novel payoff-based learning scheme for distributed optimization in multi-player/action strategic-form games. We present an extension of perturbed learning automata (PLA), namely aspiration-based perturbed learning automata (APLA), in which each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This paper is the second part of this study that analyzes stochastic stability in multi-player/action weakly-acyclic games in the presence of noisy observations. We provide conditions under which convergence is attained (in weak sense) to the set of pure Nash equilibria and payoff-dominant equilibria. To the best of our knowledge, this is the first reinforcement-based learning scheme that addresses convergence in weakly-acyclic games. Lastly, we provide a specialization of the results to the classical Stag-Hunt game, supported by a simulation study."}
{"id": "2511.18263", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.18263", "abs": "https://arxiv.org/abs/2511.18263", "authors": ["Yuhang Bai", "Kristóf Bérczi", "Johanna K. Siemelink"], "title": "Approximating maximum properly colored forests via degree bounded independent sets", "comment": null, "summary": "In the Maximum-size Properly Colored Forest problem, we are given an edge-colored undirected graph and the goal is to find a properly colored forest with as many edges as possible. We study this problem within a broader framework by introducing the Maximum-size Degree Bounded Matroid Independent Set problem: given a matroid, a hypergraph on its ground set with maximum degree $Δ$, and an upper bound $g(e)$ for each hyperedge $e$, the task is to find a maximum-size independent set that contains at most $g(e)$ elements from each hyperedge $e$. We present approximation algorithms for this problem whose guarantees depend only on $Δ$. When applied to the Maximum-size Properly Colored Forest problem, this yields a $2/3$-approximation on multigraphs, improving the $5/9$ factor of Bai, Bérczi, Csáji, and Schwarcz [Eur. J. Comb. 132 (2026) 104269]."}
{"id": "2511.17718", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.17718", "abs": "https://arxiv.org/abs/2511.17718", "authors": ["Nazanin Mirhosseini", "Jie Luo"], "title": "Unified Error Analysis for Synchronous and Asynchronous Two-User Random Access", "comment": "A short version of this paper is submitted to Information Theory symposium 2026", "summary": "We consider a two-user random access system in which each user independently selects a coding scheme from a finite set for every message, without sharing these choices with the other user or with the receiver. The receiver aims to decode only user 1 message but may also decode user 2 message when beneficial. In the synchronous setting, the receiver employs two parallel sub-decoders: one dedicated to decoding user 1 message and another that jointly decodes both users messages. Their outputs are synthesized to produce the final decoding or collision decision. For the asynchronous setting, we examine a time interval containing $L$ consecutive codewords from each user. The receiver deploys $2^{2L}$ parallel sub-decoders, each responsible for decoding a subset of the message-code index pairs. In both synchronous and asynchronous cases, every sub-decoder partitions the coding space into three disjoint regions: operation, margin, and collision, and outputs either decoded messages or a collision report according to the region in which the estimated code index vector lies. Error events are defined for each sub-decoder and for the overall receiver whenever the expected output is not produced. We derive achievable upper bounds on the generalized error performance, defined as a weighted sum of incorrect-decoding, collision, and miss-detection probabilities, for both synchronous and asynchronous scenarios."}
{"id": "2511.19015", "categories": ["cs.DB", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.19015", "abs": "https://arxiv.org/abs/2511.19015", "authors": ["Xinghe Chen", "Dajun Sun", "Quanqing Xu", "Wei Dong"], "title": "A General Framework for Per-record Differential Privacy", "comment": "SIGMOD 2026", "summary": "Differential Privacy (DP) is a widely adopted standard for privacy-preserving data analysis, but it assumes a uniform privacy budget across all records, limiting its applicability when privacy requirements vary with data values. Per-record Differential Privacy (PrDP) addresses this by defining the privacy budget as a function of each record, offering better alignment with real-world needs. However, the dependency between the privacy budget and the data value introduces challenges in protecting the budget's privacy itself. Existing solutions either handle specific privacy functions or adopt relaxed PrDP definitions. A simple workaround is to use the global minimum of the privacy function, but this severely degrades utility, as the minimum is often set extremely low to account for rare records with high privacy needs. In this work, we propose a general and practical framework that enables any standard DP mechanism to support PrDP, with error depending only on the minimal privacy requirement among records actually present in the dataset. Since directly revealing this minimum may leak information, we introduce a core technique called privacy-specified domain partitioning, which ensures accurate estimation without compromising privacy. We also extend our framework to the local DP setting via a novel technique, privacy-specified query augmentation. Using our framework, we present the first PrDP solutions for fundamental tasks such as count, sum, and maximum estimation. Experimental results show that our mechanisms achieve high utility and significantly outperform existing Personalized DP (PDP) methods, which can be viewed as a special case of PrDP with relaxed privacy protection."}
{"id": "2511.18279", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18279", "abs": "https://arxiv.org/abs/2511.18279", "authors": ["Jiahao Liang", "Haoran Yang", "Xiangyu Zhao", "Zhiwen Yu", "Guandong Xu", "Wanyu Wang", "Kaixiang Yang"], "title": "Democratic Recommendation with User and Item Representatives Produced by Graph Condensation", "comment": null, "summary": "The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \\textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods."}
{"id": "2511.19162", "categories": ["cs.IR", "cs.CY", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.19162", "abs": "https://arxiv.org/abs/2511.19162", "authors": ["Joonhyung Bae"], "title": "BioArtlas: Computational Clustering of Multi-Dimensional Complexity in Bioart", "comment": null, "summary": "Bioart's hybrid nature spanning art, science, technology, ethics, and politics defies traditional single-axis categorization. I present BioArtlas, analyzing 81 bioart works across thirteen curated dimensions using novel axis-aware representations that preserve semantic distinctions while enabling cross-dimensional comparison. Our codebook-based approach groups related concepts into unified clusters, addressing polysemy in cultural terminology. Comprehensive evaluation of up to 800 representation-space-algorithm combinations identifies Agglomerative clustering at k=15 on 4D UMAP as optimal (silhouette 0.664 +/- 0.008, trustworthiness/continuity 0.805/0.812). The approach reveals four organizational patterns: artist-specific methodological cohesion, technique-based segmentation, temporal artistic evolution, and trans-temporal conceptual affinities. By separating analytical optimization from public communication, I provide rigorous analysis and accessible exploration through an interactive web interface (https://www.bioartlas.com) with the dataset publicly available (https://github.com/joonhyungbae/BioArtlas)."}
{"id": "2511.18658", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.18658", "abs": "https://arxiv.org/abs/2511.18658", "authors": ["Karolina Drabent", "Ondřej Kubíček", "Viliam Lisý"], "title": "Understanding Optimal Portfolios of Strategies for Solving Two-player Zero-sum Games", "comment": null, "summary": "In large-scale games, approximating the opponent's strategy space with a small portfolio of representative strategies is a common and powerful technique. However, the construction of these portfolios often relies on domain-specific knowledge or heuristics with no theoretical guarantees. This paper establishes a formal foundation for portfolio-based strategy approximation. We define the problem of finding an optimal portfolio in two-player zero-sum games and prove that this optimization problem is NP-hard. We demonstrate that several intuitive heuristics-such as using the support of a Nash Equilibrium or building portfolios incrementally - can lead to highly suboptimal solutions. These negative results underscore the problem's difficulty and motivate the need for robust, empirically-validated heuristics. To this end, we introduce an analytical framework to bound portfolio quality and propose a methodology for evaluating heuristic approaches. Our evaluation of several heuristics shows that their success heavily depends on the specific game being solved. Our code is publicly available."}
{"id": "2511.18460", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.18460", "abs": "https://arxiv.org/abs/2511.18460", "authors": ["Anupam Gupta", "Vera Traub"], "title": "Steiner Forest: A Simplified Better-Than-2 Approximation", "comment": null, "summary": "In the Steiner Forest problem, we are given a graph with edge lengths, and a collection of demand pairs; the goal is to find a subgraph of least total length such that each demand pair is connected in this subgraph. For over twenty years, the best approximation ratio known for the problem was a $2$-approximation due to Agrawal, Klein, and Ravi (STOC 1991), despite many attempts to surpass this bound. Finally, in a recent breakthrough, Ahmadi, Gholami, Hajiaghayi, Jabbarzade, and Mahdavi (FOCS 2025) gave a $2-\\varepsilon$-approximation, where $\\varepsilon \\approx 10^{-11}$.\n  In this work, we show how to simplify and extend the work of Ahmadi et al. to obtain an improved $1.994$-approximation. We combine some ideas from their work (e.g., an extended run of the moat-growing primal-dual algorithm, and identifying autarkic pairs) with other ideas -- submodular maximization to find components to contract, as in the relative greedy algorithms for Steiner tree, and the use of autarkic triples. We hope that our cleaner abstraction will open the way for further improvements."}
{"id": "2511.17897", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.17897", "abs": "https://arxiv.org/abs/2511.17897", "authors": ["Hanjiang Hong", "Kai-Kit Wong", "Xusheng Zhu", "Hao Xu", "Han Xiao", "Farshad Rostami Ghadi", "Hyundong Shin"], "title": "Multi-Port Selection for FAMA: Massive Connectivity with Fewer RF Chains than Users", "comment": null, "summary": "Fluid antenna multiple access (FAMA) is an emerging technology in massive access designed to meet the demands of future wireless communication networks by naturally mitigating multiuser interference through the utilization of the fluid antenna system (FAS) at RF-chain-limited mobile device. The transition from single-active-port to multi-active-port on a shared RF chain for slow FAMA can greatly enhance its multiplexing capability but is not well understood. Motivated by this, this paper proposes and studies three port selection methods: the optimal exhaustive-search port selection (EPS) as a performance upper bound, and two suboptimal, low-complexity algorithms, namely incremental port selection (IPS) and decremental port selection (DPS). Then the performance of multi-active-port slow FAMA is analyzed, and the complexity of the proposed methods is compared. Simulation results indicate that the proposed methods outperform current state-of-the-art multi-port FAMA techniques. In particular, IPS achieves near-optimal performance while maintaining manageable computational complexity. This research provides a more general framework for port selection in FAMA systems."}
{"id": "2511.18282", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18282", "abs": "https://arxiv.org/abs/2511.18282", "authors": ["Jiahao Liang", "Haoran Yang", "Xiangyu Zhao", "Zhiwen Yu", "Mianjie Li", "Chuan Shi", "Kaixiang Yang"], "title": "Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation", "comment": null, "summary": "Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\\textbf{Inv}$ariant $\\textbf{G}$raph $\\textbf{C}$ontrastive Learning with $\\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines."}
{"id": "2511.19225", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.19225", "abs": "https://arxiv.org/abs/2511.19225", "authors": ["Jordana Blazek", "Frederick C. Harris"], "title": "Bipartiteness in Progressive Second-Price Multi-Auction Networks with Perfect Substitute", "comment": null, "summary": "We consider a bipartite network of buyers and sellers, where the sellers run locally independent Progressive Second-Price (PSP) auctions, and buyers may participate in multiple auctions, forming a multi-auction market with perfect substitute. The paper develops a projection-based influence framework for decentralized PSP auctions. We formalize primary and expanded influence sets using projections on the active bid index set and show how partial orders on bid prices govern allocation, market shifts, and the emergence of saturated one-hop shells. Our results highlight the robustness of PSP auctions in decentralized environments by introducing saturated components and a structured framework for phase transitions in multi-auction dynamics. This structure ensures deterministic coverage of the strategy space, enabling stable and truthful embedding in the larger game. We further model intra-round dynamics using an index to capture coordinated asynchronous seller updates coupled through buyers' joint constraints. Together, these constructions explain how local interactions propagate across auctions and gives premise for coherent equilibria--without requiring global information or centralized control."}
{"id": "2511.18546", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.18546", "abs": "https://arxiv.org/abs/2511.18546", "authors": ["Siyue Liu", "Victor Reis"], "title": "Weighted Chairman Assignment and Flow-Time Scheduling", "comment": null, "summary": "Given positive integers $m, n$, a fractional assignment $x \\in [0,1]^{m \\times n}$ and weights $d \\in \\mathbb{R}^n_{>0}$, we show that there exists an assignment $y \\in \\{0,1\\}^{m \\times n}$ so that for every $i\\in[m]$ and $t\\in [n]$, \\[ \\Big|\\sum_{j \\in [t]} d_j (x_{ij} - y_{ij}) \\Big| < \\max_{j \\in [n]} d_j. \\] This generalizes a result of Tijdeman (1973) on the unweighted version, known as the chairman assignment problem. This also confirms a special case of the single-source unsplittable flow conjecture with arc-wise lower and upper bounds due to Morell and Skutella (IPCO 2020). As an application, we consider a scheduling problem where jobs have release times and machines have closing times, and a job can only be scheduled on a machine if it is released before the machine closes. We give a $3$-approximation algorithm for maximum flow-time minimization."}
{"id": "2511.17916", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.17916", "abs": "https://arxiv.org/abs/2511.17916", "authors": ["Yi Zhang", "Jintao Wang", "Zheng Shi", "Xu Wang", "Guanghua Yang", "Shaodan Ma", "Kai-Kit Wong"], "title": "Asymptotic Performance Analysis of Fluid Antenna Systems: An Extreme Value Theory Perspective", "comment": null, "summary": "Fluid antenna systems (FAS) allow dynamic reconfiguration to achieve superior diversity gains and reliability. To quantify the performance scaling of FAS with a large number of antenna ports, this paper leverages extreme value theory (EVT) to conduct an asymptotic analysis of the outage probability (OP) and ergodic capacity (EC). The analysis reveals that the OP decays approximately exponentially with the number of antenna ports. Moreover, we establish upper and lower bounds for the asymptotic EC, uncovering its double-logarithmic scaling law. Furthermore, we re-substantiate these scaling laws by exploiting the fact that the mode of the Gumbel distribution scales logarithmically. Besides, we theoretically prove that spatial correlation among antenna ports degrades both OP and EC. All analytical findings are conclusively validated by numerical results."}
{"id": "2511.18342", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18342", "abs": "https://arxiv.org/abs/2511.18342", "authors": ["Jiaming Zhang", "Yuyuan Li", "Xiaohua Feng", "Zhifei Ren", "Li Zhang", "Chaochao Chen"], "title": "UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning", "comment": null, "summary": "Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \\textit{judger}, which identifies unfairness from both pre-training and SFT, and the \\textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance."}
{"id": "2511.19307", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.19307", "abs": "https://arxiv.org/abs/2511.19307", "authors": ["Michail Fasoulakis", "Leonidas Bakopoulos", "Charilaos Akasiadis", "Georgios Chalkiadakis"], "title": "On Altruism and Spite in Bimatrix Games", "comment": null, "summary": "One common assumption in game theory is that any player optimizes a utility function that takes into account only its own payoff. However, it has long been observed that in real life players may adopt an altruistic or even spiteful behaviour. As such, there are numerous attempts in the economics literature that strive to explain the fact that players are not entirely selfish, but most of these works do not focus on the algorithmic implications of altruism or spite in games. In this paper, we relax the aforementioned ``self-interest'' assumption, and initiate the study of algorithmic aspects of bimatrix games -- such as the complexity and the quality of their (approximate) Nash equilibria -- under altruism or spite. We provide both a theoretical and an experimental treatment of these topics. Moreover, we demonstrate the potential for learning the degree of an opponent's altruistic/spiteful behaviour, and employing this for opponent selection and transfer of knowledge in bimatrix games."}
{"id": "2511.18554", "categories": ["cs.DS", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18554", "abs": "https://arxiv.org/abs/2511.18554", "authors": ["Adam Lechowicz", "Nicolas Christianson", "Mohammad Hajiesmaili", "Adam Wierman", "Prashant Shenoy"], "title": "Online Smoothed Demand Management", "comment": "69 pages, 12 figures", "summary": "We introduce and study a class of online problems called online smoothed demand management $(\\texttt{OSDM})$, motivated by paradigm shifts in grid integration and energy storage for large energy consumers such as data centers. In $\\texttt{OSDM}$, an operator makes two decisions at each time step: an amount of energy to be purchased, and an amount of energy to be delivered (i.e., used for computation). The difference between these decisions charges (or discharges) the operator's energy storage (e.g., a battery). Two types of demand arrive online: base demand, which must be covered at the current time, and flexible demand, which can be satisfied at any time steps before a demand-specific deadline $Δ_t$. The operator's goal is to minimize a cost (subject to the constraints above) that combines a cost of purchasing energy, a cost for delivering energy (if applicable), and smoothness penalties on the purchasing and delivery rates to discourage fluctuations and encourage ``grid healthy'' decisions. $\\texttt{OSDM}$ generalizes several problems in the online algorithms literature while being the first to fully model applications of interest. We propose a competitive algorithm called $\\texttt{PAAD}$ (partitioned accounting \\& aggregated decisions) and show it achieves the optimal competitive ratio. To overcome the pessimism typical of worst-case analysis, we also propose a novel learning framework that provides guarantees on the worst-case competitive ratio (i.e., to provide robustness against nonstationarity) while allowing end-to-end differentiable learning of the best algorithm on historical instances of the problem. We evaluate our algorithms in a case study of a grid-integrated data center with battery storage, showing that $\\texttt{PAAD}$ effectively solves the problem and end-to-end learning achieves substantial performance improvements compared to $\\texttt{PAAD}$."}
{"id": "2511.17931", "categories": ["cs.IT", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17931", "abs": "https://arxiv.org/abs/2511.17931", "authors": ["Jaswanth Bodempudi", "Batta Siva Sairam", "Madepalli Haritha", "Sandesh Rao Mattu", "Ananthanarayanan Chockalingam"], "title": "A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference", "comment": "Accepted in IEEE Trans. on Machine Learning in Communications and Networking", "summary": "Carrier aggregation (CA) is a technique that allows mobile networks to combine multiple carriers to increase user data rate. On the uplink, for power constrained users, this translates to the need for an efficient resource allocation scheme, where each user distributes its available power among its assigned uplink carriers. Choosing a good set of carriers and allocating appropriate power on the carriers is important. If the carrier allocation on the uplink is such that a harmonic of a user's uplink carrier falls on the downlink frequency of that user, it leads to a self coupling-induced sensitivity degradation of that user's downlink receiver. In this paper, we model the uplink carrier aggregation problem as an optimal resource allocation problem with the associated constraints of non-linearities induced self interference (SI). This involves optimization over a discrete variable (which carriers need to be turned on) and a continuous variable (what power needs to be allocated on the selected carriers) in dynamic environments, a problem which is hard to solve using traditional methods owing to the mixed nature of the optimization variables and the additional need to consider the SI constraint. We adopt a reinforcement learning (RL) framework involving a compound-action actor-critic (CA2C) algorithm for the uplink carrier aggregation problem. We propose a novel reward function that is critical for enabling the proposed CA2C algorithm to efficiently handle SI. The CA2C algorithm along with the proposed reward function learns to assign and activate suitable carriers in an online fashion. Numerical results demonstrate that the proposed RL based scheme is able to achieve higher sum throughputs compared to naive schemes. The results also demonstrate that the proposed reward function allows the CA2C algorithm to adapt the optimization both in the presence and absence of SI."}
{"id": "2511.18347", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18347", "abs": "https://arxiv.org/abs/2511.18347", "authors": ["Haoyan Fu", "Zhida Qin", "Shixiao Yang", "Haoyao Zhang", "Bin Lu", "Shuang Li", "Tianyu Huang", "John C. S. Lui"], "title": "Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs", "comment": null, "summary": "Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%."}
{"id": "2511.19346", "categories": ["cs.GT", "math.DS", "math.SP"], "pdf": "https://arxiv.org/pdf/2511.19346", "abs": "https://arxiv.org/abs/2511.19346", "authors": ["Pablo Lechon-Alonso", "Andrew Dennehy", "Ruizheng Bai", "Nicolas Sanchez", "Derek K. Wise", "David Sewell", "David Rosenbluth", "Alexander Strang"], "title": "Disc Game Dynamics: A Latent Space Perspective on Selection and Learning in Games", "comment": null, "summary": "Evolutionary game theory studies populations that change in response to an underlying game. Often, the functional form relating outcome to player attributes or strategy is complex, preventing mathematical progress. In this work, we axiomatically derive a latent space representation for pairwise, symmetric, zero-sum games by seeking a coordinate space in which the optimal training direction for an agent responding to an opponent depends only on their opponent's coordinates. The associated embedding represents the original game as a linear combination of copies of a simple game, the disc game, in a new coordinate space. In this article, we show that disc-game embedding is useful for studying learning dynamics. We demonstrate that a series of classical evolutionary processes simplify to constrained oscillator equations in the latent space. In particular, the continuous replicator equation reduces to a Hamiltonian system of coupled oscillators that exhibit Poincaré recurrence. This reduction allows exact, finite-dimensional closure when the underlying game is finite-rank, and optimal approximation otherwise. It also establishes an exact equivalence between the continuous replicator equation and adaptive dynamics in the transformed coordinates. By identifying a minimal rank representation, the disc game embedding offers numerical methods that could decouple the cost of simulation from the number of attributes used to define agents. These results generalize to metapopulation models that mix inhomogeneously, and to any time-differentiable dynamic where the rate of growth of a type, relative to its expected payout, is a nonnegative function of its frequency. We recommend disc-game embedding as an organizing paradigm for learning and selection in response to symmetric two-player zero-sum games."}
{"id": "2511.18666", "categories": ["cs.DS", "math.CO", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.18666", "abs": "https://arxiv.org/abs/2511.18666", "authors": ["Frederic Koehler", "Joonhyung Shin"], "title": "Overlap Analysis of the Shortest Path Problem: Local Search, Landscapes, and Franz--Parisi Potential", "comment": "Abstract shortened for arxiv", "summary": "Two directions in algorithms and complexity involve: (1) classifying which optimization problems can be solved in polynomial time, and (2) understanding which computational problems are hard to solve \\emph{on average} in addition to the worst case. For many average-case problems, there does not currently exist strong evidence via reductions that they are hard. However, we can still attempt to predict their polynomial time tractability by proving lower bounds against restricted classes of algorithms.\n  Geometric approaches to predicting tractability typically study the \\emph{optimization landscape}. For optimization problems with random objectives or constraints, ideas originating in statistical physics suggest we should study the \\emph{overlap} between approximately-optimal solutions. Formally, properties of \\emph{Gibbs measures} and the \\emph{Franz--Parisi potential} imply lower bounds against natural local search algorithms, such as Langevin dynamics. A related theory, the \\emph{Overlap Gap Property (OGP)}, proves rigorous lower bounds against classes of algorithms which are stable functions of their input.\n  A remarkable recent work of Li and Schramm showed that the shortest path problem in random graphs admits lower bounds against a class of stable algorithms, via the OGP. Yet this problem is polynomial time tractable. We further investigate this. We find that both the OGP and the Franz--Parisi potential predict that: (1) local search will fail in the optimization landscape of shortest paths, but (2) local search should succeed in the optimization landscape for shortest path \\emph{trees}, which is true. Using the Franz--Parisi potential, we explain an analogy with results from combinatorial optimization -- submodular minimization is tractable via local search on the Lovász extension, even though ``naive'' local search over sets or the multilinear extension provably fails."}
{"id": "2511.18027", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18027", "abs": "https://arxiv.org/abs/2511.18027", "authors": ["Yu-Ting Lin", "Hsin-Po Wang", "Venkatesan Guruswami"], "title": "Block Length Gain for Nanopore Channels", "comment": "9 pages, 19 figures", "summary": "DNA is an attractive candidate for data storage. Its millennial durability and nanometer scale offer exceptional data density and longevity. Its relevance to medical applications also drives advances in DNA-related biotechnology.\n  To protect our data against errors, a straightforward approach uses one error-correcting code per DNA strand, with a Reed--Solomon code protecting the collection of strands. A downside is that current technology can only synthesize strands 200--300 nucleotides long. At this block length, the inner code rate suffers a significant finite-length penalty, making its effective capacity hard to characterize.\n  Last year, we proposed $\\textit{Geno-Weaving}$ in a JSAIT publication. The idea is to protect the same position across multiple strands using one code; this provably achieves capacity against substitution errors. In this paper, we extend the idea to combat deletion errors and show two more advantages of Geno-Weaving: (1) Because the number of strands is 3--4 orders of magnitude larger than the strand length, the finite-length penalty vanishes. (2) At realistic deletion rates $0.1\\%$--$10\\%$, Geno-Weaving designed for BSCs works well empirically, bypassing the need to tailor the design for deletion channels."}
{"id": "2511.18645", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18645", "abs": "https://arxiv.org/abs/2511.18645", "authors": ["Raoul H. Kutil", "Georg Zimmermann", "Christian Borgelt"], "title": "A Recommender System Based on Binary Matrix Representations for Cognitive Disorders", "comment": "19 pages, 1 figure, 3 tables", "summary": "Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy."}
{"id": "2511.19358", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.19358", "abs": "https://arxiv.org/abs/2511.19358", "authors": ["Paul Dütting", "Tomer Ezra", "Michal Feldman", "Thomas Kesselheim"], "title": "Black-Box Lifting and Robustness Theorems for Multi-Agent Contracts", "comment": null, "summary": "Multi-agent contract design has largely evaluated contracts through the lens of pure Nash equilibria (PNE). This focus, however, is not without loss: In general, the principal can strictly gain by recommending a complex, possibly correlated, distribution over actions, while preserving incentive compatibility. In this work, we extend the analysis of multi-agent contracts beyond pure Nash equilibria to encompass more general equilibrium notions, including mixed Nash equilibria as well as (coarse-)correlated equilibria (CCE). The latter, in particular, captures the limiting outcome of agents engaged in learning dynamics.\n  Our main result shows that for submodular and, more generally, XOS rewards, such complex recommendations yield at most a constant-factor gain: there exists a contract and a PNE whose utility is within a constant factor of the best CCE achievable by any contract. This provides a black-box lifting: results established against the best PNE automatically apply with respect to the best CCE, with only a constant factor loss. For submodular rewards, we further show how to transform a contract and a PNE of that contract into a new contract such that any of its CCEs gives a constant approximation to the PNE. This yields black-box robustness: up to constant factors, guarantees established for a specific contract and PNE automatically extend to the modified contract and any of its CCEs. We thus expand prior guarantees for multi-agent contracts and lower the barrier to new ones. As an important corollary, we obtain poly-time algorithms for submodular rewards that achieve constant approximations in any CCE, against the best CCE under the best contract. Such worst-case guarantees are provably unattainable for XOS rewards. Finally, we bound the gap between different equilibrium notions for subadditive, supermodular, and general rewards."}
{"id": "2511.19027", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.19027", "abs": "https://arxiv.org/abs/2511.19027", "authors": ["Christine Awofeso", "Patrick Greaves", "Oded Lachish", "Amit Levi", "Felix Reidl"], "title": "A sufficient condition for characterizing the one-sided testable properties of families of graphs in the Random Neighbour Oracle Model", "comment": null, "summary": "We study property testing in the \\emph{random neighbor oracle} model for graphs, originally introduced by Czumaj and Sohler [STOC 2019]. Specifically, we initiate the study of characterizing the graph families that are $H$-\\emph{testable} in this model. A graph family $\\mathcal{F}$ is $H$-testable if, for every graph $H$, $H$-\\emph{freeness} (that is, not having a subgraph isomorphic to $H$) is testable with one-sided error on all inputs from $\\mathcal{F}$.\n  Czumaj and Sohler showed that for any $H$-testable family of graphs $\\mathcal{F}$, the family of testable properties of $\\mathcal{F}$ has a known characterization, a major goal in the study of property testing. Consequently, characterizing the collection of $H$-testable graph families will not only result in new characterizations, but will also exhaust this method of characterizing testable properties. We believe that our result is a substantial step towards this goal.\n  Czumaj and Sohler further showed that the family of planar graphs is $H$-testable, as is any family of minor-free graphs. In this paper, we provide a sufficient and much broader criterion under which a family of graphs is $H$-testable. As a corollary, we obtain new characterizations for many families of graphs including: families that are closed under taking topological minors or immersions, geometric intersection graphs of low-density objects, euclidean nearest-neighbour graphs with bounded clique number, graphs with bounded crossing number (per edge), graphs with bounded queue- and stack number, and more.\n  The criterion we provide is based on the \\emph{$r$-admissibility} graph measure from the theory of sparse graph families initiated by Nesetril and Ossona de Mendez. Proving that specific families of graphs satisfy this criterion is an active area of research, consequently, the implications of this paper may be strengthened in the future."}
{"id": "2511.18097", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18097", "abs": "https://arxiv.org/abs/2511.18097", "authors": ["Pengchuan Jiang", "Quanzhong Li", "Lifeng Mai", "Qi Zhang"], "title": "Average Secrecy Capacity Maximization of Rotatable Antenna-Assisted Secure Communications", "comment": null, "summary": "A rotatable antenna, which is able to dynamically adjust its deflection angle, is promising to achieve better physical layer security performance for wireless communications. In this paper, considering practical scenarios with non-real-time rotatable antenna adjustment, we investigate the average secrecy rate maximization problem of a rotatable antenna-assisted secure communication system. We theoretically prove that the objective function of the average secrecy rate maximization problem is quasi-concave with respect to an adjustment factor of the rotatable antenna. Under this condition, the optimal solution can be found by the bisection search. Furthermore, we derive the closed-form optimal deflection angle for the secrecy capacity maximization problem, considering the existence of only line-of-sight components of wireless channels. This solution serves as a near optimal solution to the average secrecy rate maximization problem. Based on the closed-form near optimal solution, we obtain the system secrecy outage probability at high signal-to-noise ratio (SNR). It is shown through simulation results that the near optimal solution achieves almost the same average secrecy capacity as the optimal solution. It is also found that at high SNR, the theoretical secrecy outage probabilities match the simulation ones."}
{"id": "2511.18717", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18717", "abs": "https://arxiv.org/abs/2511.18717", "authors": ["Jin Chai", "Xiaoxiao Ma", "Jian Yang", "Jia Wu"], "title": "When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation", "comment": "10 pages, 5 figures. Submitted to arXiv", "summary": "Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits."}
{"id": "2511.19085", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.19085", "abs": "https://arxiv.org/abs/2511.19085", "authors": ["Jan Eube", "Heiko Röglin"], "title": "New Algorithms and Hardness Results for Connected Clustering", "comment": null, "summary": "Connected clustering denotes a family of constrained clustering problems in which we are given a distance metric and an undirected connectivity graph $G$ that can be completely unrelated to the metric. The aim is to partition the $n$ vertices into a given number $k$ of clusters such that every cluster forms a connected subgraph of $G$ and a given clustering objective gets minimized. The constraint that the clusters are connected has applications in many different fields, like for example community detection and geodesy.\n  So far, $k$-center and $k$-median have been studied in this setting. It has been shown that connected $k$-median is $Ω(n^{1- ε})$-hard to approximate which also carries over to the connected $k$-means problem, while for connected $k$-center it remained an open question whether one can find a constant approximation in polynomial time. We answer this question by providing an $Ω(\\log^*(k))$-hardness result for the problem. Given these hardness results, we study the problems on graphs with bounded treewidth. We provide exact algorithms that run in polynomial time if the treewidth $w$ is a constant. Furthermore, we obtain constant approximation algorithms that run in FPT time with respect to the parameter $\\max(w,k)$.\n  Additionally, we consider the min-sum-radii (MSR) and min-sum-diameter (MSD) objective. We prove that on general graphs connected MSR can be approximated with an approximation factor of $(3 + ε)$ and connected MSD with an approximation factor of $(4 + ε)$. The latter also directly improves the best known approximation guarantee for unconstrained MSD from $(6 + ε)$ to $(4 + ε)$."}
{"id": "2511.18250", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18250", "abs": "https://arxiv.org/abs/2511.18250", "authors": ["Dongmei Huang", "Qunying Liao", "Sihem Mesnager", "Gaohua Tang", "Haode Yan"], "title": "On the Hamming Weight Functions of Linear Codes", "comment": null, "summary": "Currently known secondary construction techniques for linear codes mainly include puncturing, shortening, and extending. In this paper, we propose a novel method for the secondary construction of linear codes based on their weight functions. Specifically, we develop a general framework that constructs new linear codes from the set of codewords in a given code having a fixed Hamming weight. We analyze the dimension, number of weights, and weight distribution of the constructed codes, and establish connections with the extendability of the original codes as well as the partial weight distribution of the derived codes. As a new tool, this framework enables us to establish an upper bound on the minimum weight of two-weight codes and to characterize all two-weight codes attaining this bound. Moreover, several divisibility properties concerning the parameters of two-weight codes are derived. The proposed method not only generates new families of linear codes but also provides a powerful approach for exploring the intrinsic combinatorial and geometric structures of existing codes."}
{"id": "2511.18740", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18740", "abs": "https://arxiv.org/abs/2511.18740", "authors": ["Yu Wang", "Yonghui Yang", "Le Wu", "Yi Zhang", "Richang Hong"], "title": "Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation", "comment": "11 pages,6 figures", "summary": "Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model."}
{"id": "2511.19153", "categories": ["cs.DS", "math.OC", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.19153", "abs": "https://arxiv.org/abs/2511.19153", "authors": ["Francisco Sena", "Alexandru I. Tomescu"], "title": "Fast and Flexible Flow Decompositions in General Graphs via Dominators", "comment": null, "summary": "Multi-assembly methods rely at their core on a flow decomposition problem, namely, decomposing a weighted graph into weighted paths or walks. However, most results over the past decade have focused on decompositions over directed acyclic graphs (DAGs). This limitation has lead to either purely heuristic methods, or in applications transforming a graph with cycles into a DAG via preprocessing heuristics. In this paper we show that flow decomposition problems can be solved in practice also on general graphs with cycles, via a framework that yields fast and flexible Mixed Integer Linear Programming (MILP) formulations.\n  Our key technique relies on the graph-theoretic notion of dominator tree, which we use to find all safe sequences of edges, that are guaranteed to appear in some walk of any flow decomposition solution. We generalize previous results from DAGs to cyclic graphs, by showing that maximal safe sequences correspond to extensions of common leaves of two dominator trees, and that we can find all of them in time linear in their size. Using these, we can accelerate MILPs for any flow decomposition into walks in general graphs, by setting to (at least) 1 suitable variables encoding solution walks, and by setting to 0 other walks variables non-reachable to and from safe sequences. This reduces model size and eliminates costly linearizations of MILP variable products.\n  We experiment with three decomposition models (Minimum Flow Decomposition, Least Absolute Errors and Minimum Path Error), on four bacterial datasets. Our pre-processing enables up to thousand-fold speedups and solves even under 30 seconds many instances otherwise timing out. We thus hope that our dominator-based MILP simplification framework, and the accompanying software library can become building blocks in multi-assembly applications."}
{"id": "2511.18420", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18420", "abs": "https://arxiv.org/abs/2511.18420", "authors": ["Charul Rajput", "B. Sundar Rajan", "Ragnar Freij-Hollanti", "Camilla Hollanti"], "title": "Function-Correcting Codes With Data Protection", "comment": null, "summary": "Function-correcting codes (FCCs) are designed to provide error protection for the value of a function computed on the data. Existing work typically focuses solely on protecting the function value and not the underlying data. In this work, we propose a general framework that offers protection for both the data and the function values. Since protecting the data inherently contributes to protecting the function value, we focus on scenarios where the function value requires stronger protection than the data itself. We first introduce a more general approach and a framework for function-correcting codes that incorporates data protection along with protection of function values. A two-step construction procedure for such codes is proposed, and bounds on the optimal redundancy of general FCCs with data protection are reported. Using these results, we exhibit examples that show that data protection can be added to existing FCCs without increasing redundancy. Using our two-step construction procedure, we present explicit constructions of FCCs with data protection for specific families of functions, such as locally bounded functions and the Hamming weight function. We associate a graph called minimum-distance graph to a code and use it to show that perfect codes and maximum distance separable (MDS) codes cannot provide additional protection to function values over and above the amount of protection for data for any function. Then we focus on linear FCCs and provide some results for linear functions, leveraging their inherent structural properties. To the best of our knowledge, this is the first instance of FCCs with a linear structure. Finally, we generalize the Plotkin and Hamming bounds well known in classical error-correcting coding theory to FCCs with data protection."}
{"id": "2511.18805", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18805", "abs": "https://arxiv.org/abs/2511.18805", "authors": ["Yi Xu", "Chaofan Fan", "Jinxin Hu", "Yu Zhang", "Zeng Xiaoyi", "Jing Zhang"], "title": "STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models", "comment": null, "summary": "Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like \"One-Epoch\" and \"Interaction-Collapse,\" ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput)."}
{"id": "2511.19398", "categories": ["cs.DS", "cs.IT", "cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19398", "abs": "https://arxiv.org/abs/2511.19398", "authors": ["Ilias Diakonikolas", "Daniel M. Kane", "Sihan Liu", "Thanasis Pittas"], "title": "PTF Testing Lower Bounds for Non-Gaussian Component Analysis", "comment": null, "summary": "This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.\n  In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions."}
{"id": "2511.18456", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18456", "abs": "https://arxiv.org/abs/2511.18456", "authors": ["Yanbo Yin", "Dingzhu Wen", "Changsheng You", "XiaoWen Cao", "Tat-Ming Lok", "Dusit Niyato"], "title": "Aerial Semantic Relay-Enabled SAGIN: Joint UAV Deployment and Resource Allocation", "comment": null, "summary": "Space-Air-Ground Integrated Networks (SAGINs) are pivotal for enabling ubiquitous connectivity in 6G systems, yet they face significant challenges due to severe satellite-to-ground link impairments. Although Unmanned Aerial Vehicles (UAVs) can function as relay nodes to compensate for air-to-ground channel degradation, the satellite-to-UAV link remains a critical bottleneck. Semantic Communication (SemCom) emerges as a promising solution to enhance spectral efficiency by transmitting essential semantic information. This paper proposes a novel multi-cluster UAV-aided SAGIN SemCom architecture that supports both semantic users (SemUsers) and conventional users (ConUsers). While SemCom is employed in the satellite-to-UAV link to improve transmission efficiency, the UAVs implement an intelligent adaptive relay strategy, capable of either directly forwarding semantic data to SemUsers or converting it into bit-level data for ConUsers. Compared to existing similar schemes, this design guarantees the high-efficiency advantages of SemCom while enabling network access for larger coverage area. A joint optimization problem is formulated to maximize the system's sum-rate through coordinated allocation of power, bandwidth, and UAV positions. To address this non-convex problem, we develop an efficient alternating optimization (AO) algorithm, which decomposes the original problem into tractable subproblems. Numerical results demonstrate that the proposed algorithm significantly outperforms baseline schemes in terms of both sum-rate and spectral efficiency across various channel conditions and user distributions, underscoring the importance of joint resource allocation and intelligent UAV deployment."}
{"id": "2511.18997", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18997", "abs": "https://arxiv.org/abs/2511.18997", "authors": ["Chenhao Zhai", "Chang Meng", "Xueliang Wang", "Shuchang Liu", "Xiaolong Hu", "Shisong Tang", "Xiaoqiang Feng", "Xiu Li"], "title": "Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation", "comment": "Accepted by KDD 2026", "summary": "The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users."}
{"id": "2511.19225", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.19225", "abs": "https://arxiv.org/abs/2511.19225", "authors": ["Jordana Blazek", "Frederick C. Harris"], "title": "Bipartiteness in Progressive Second-Price Multi-Auction Networks with Perfect Substitute", "comment": null, "summary": "We consider a bipartite network of buyers and sellers, where the sellers run locally independent Progressive Second-Price (PSP) auctions, and buyers may participate in multiple auctions, forming a multi-auction market with perfect substitute. The paper develops a projection-based influence framework for decentralized PSP auctions. We formalize primary and expanded influence sets using projections on the active bid index set and show how partial orders on bid prices govern allocation, market shifts, and the emergence of saturated one-hop shells. Our results highlight the robustness of PSP auctions in decentralized environments by introducing saturated components and a structured framework for phase transitions in multi-auction dynamics. This structure ensures deterministic coverage of the strategy space, enabling stable and truthful embedding in the larger game. We further model intra-round dynamics using an index to capture coordinated asynchronous seller updates coupled through buyers' joint constraints. Together, these constructions explain how local interactions propagate across auctions and gives premise for coherent equilibria--without requiring global information or centralized control."}
{"id": "2511.18610", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18610", "abs": "https://arxiv.org/abs/2511.18610", "authors": ["Ferhat Bayar", "Haci Ilhan", "Erdogan Aydin"], "title": "Performance Evaluation of Dual RIS-Assisted Received Space Shift Keying Modulation", "comment": null, "summary": "Reconfigurable intelligent surfaces (RISs) are gaining traction for their ability to reshape wireless environments with low energy consumption. However, prior studies primarily explore single-RIS deployments with static or semi-static reflection control. In this paper, we propose a novel dual-RIS-assisted architecture for smart indoor wireless signal routing, wherein the second RIS (RIS$_2$) is dynamically configured based on source data bits to steer signals toward specific receivers or indoor zones. The first RIS (RIS$_1$), positioned near a fed antenna or access point, passively reflects the incident signal. RIS$_2$, equipped with a lightweight controller, performs bit-driven spatial modulation to enable data-dependent direction selection at the physical layer. We develop a complete end-to-end system model, including multi-hop channel representation, RIS phase configuration mapping, and signal detection based on space shift keying (SSK). Performance analysis is evaluated in terms of achievable capacity and outage probability under varying inter-RIS distances and carrier frequencies."}
{"id": "2511.19162", "categories": ["cs.IR", "cs.CY", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.19162", "abs": "https://arxiv.org/abs/2511.19162", "authors": ["Joonhyung Bae"], "title": "BioArtlas: Computational Clustering of Multi-Dimensional Complexity in Bioart", "comment": null, "summary": "Bioart's hybrid nature spanning art, science, technology, ethics, and politics defies traditional single-axis categorization. I present BioArtlas, analyzing 81 bioart works across thirteen curated dimensions using novel axis-aware representations that preserve semantic distinctions while enabling cross-dimensional comparison. Our codebook-based approach groups related concepts into unified clusters, addressing polysemy in cultural terminology. Comprehensive evaluation of up to 800 representation-space-algorithm combinations identifies Agglomerative clustering at k=15 on 4D UMAP as optimal (silhouette 0.664 +/- 0.008, trustworthiness/continuity 0.805/0.812). The approach reveals four organizational patterns: artist-specific methodological cohesion, technique-based segmentation, temporal artistic evolution, and trans-temporal conceptual affinities. By separating analytical optimization from public communication, I provide rigorous analysis and accessible exploration through an interactive web interface (https://www.bioartlas.com) with the dataset publicly available (https://github.com/joonhyungbae/BioArtlas)."}
{"id": "2511.18663", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18663", "abs": "https://arxiv.org/abs/2511.18663", "authors": ["J. D. Vega-Sánchez", "V. H. Garzón Pacheco", "N. V. Orozco Garzón", "H. R. Carvajal Mora", "F. J. López-Martínez"], "title": "Understanding the Role of Phase and Position Design in Fluid Reconfigurable Intelligent Surfaces", "comment": null, "summary": "Fluid Reconfigurable Intelligent Surfaces (FRISs) are gaining momentum as an improved alternative over classical RIS. However, it remains unclear whether their performance gains can be entirely attributed to spatial flexibility, or instead to differences in equivalent aperture or phase design. In this work, we shed light onto this problem by benchmarking FRIS vs. RIS performances in two practical scenarios: conventional RIS (same number of active elements and same overall aperture) and compact RIS (same number of active elements, and smaller aperture with sub-λ inter-element spacing). Statistical analysis demonstrates that: (i) spatial position optimization in FRIS provides noticeable gains over conventional RIS in the absence of phase-shift design; (ii) such benefits vanish when FRIS and conventional RIS employ optimal beamforming (BF) and phase shift (PS) design, making position optimization irrelevant; (iii) FRIS consistently outperforms compact RIS with optimized BF and PS design, owing to spatial correlation and smaller aperture."}
{"id": "2511.19324", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19324", "abs": "https://arxiv.org/abs/2511.19324", "authors": ["Roksana Goworek", "Olivia Macmillan-Scott", "Eda B. Özyiğit"], "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models", "comment": null, "summary": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages."}
{"id": "2511.18669", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18669", "abs": "https://arxiv.org/abs/2511.18669", "authors": ["Roberto C. G. Porto", "Rodrigo C. de Lamare"], "title": "Study of Iterative Dynamic Channel Tracking for Multiple RIS-Assisted MIMO Systems", "comment": "7 pages, 7 figures", "summary": "The use of multiple Reconfigurable Intelligent Sur- faces (RIS) has gained attention in 6G networks to enhance coverage. However, the feasibility of deploying multiple RIS relies on efficient channel estimation and reduced pilot overhead. To address these challenges, this work proposes an iterative channel estimation scheme that exploits low-density parity-check (LDPC) codes, channel coherence time, and iterative processing to improve estimation accuracy while minimizing pilot length. Encoded pilots are used to strengthen the iterative processing, leveraging both pilot and parity bits, while previous estimates are incorporated to further reduce overhead. Simulations consider a sub-6 GHz scenario with non-sparse channels and multiple RIS under both LOS and NLOS conditions. The results show that the proposed method outperforms existing approaches, achieving significant gains with substantially lower pilot overhead."}
{"id": "2511.19325", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19325", "abs": "https://arxiv.org/abs/2511.19325", "authors": ["Olivia Macmillan-Scott", "Roksana Goworek", "Eda B. Özyiğit"], "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval", "comment": null, "summary": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources."}
{"id": "2511.18675", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18675", "abs": "https://arxiv.org/abs/2511.18675", "authors": ["J. D. Vega-Sánchez", "V. H. Garzón Pacheco", "N. V. Orozco Garzón", "D. A. Riofrío Almeida", "D. P. Moya Osorio"], "title": "Exploring Spatial Flexibility and Phase Design in Fluid Reconfigurable Intelligent Surfaces: A Physical Layer Security Perspective", "comment": null, "summary": "This work examines the secrecy outage probability (SOP) in Fluid Reconfigurable Intelligent Surfaces (FRIS) and contrasts their performance against two alternative RIS architectures: a traditional planar RIS and a compact RIS layout. To characterize the end-to-end FRIS channel, a maximum likelihood estimation (MLE) approach is introduced, while a Q-learning algorithm is employed to adaptively select the spatial positions of FRIS elements. Numerical evaluations show that optimizing element placement in FRIS significantly improves SOP compared to conventional RIS without phase adaptation. However, these improvements become less evident once the conventional RIS implements optimized beamforming (BF) and phase-shift (PS) controlling. In addition, FRIS maintains a clear advantage over compact RIS designs with optimized BF and PS, mainly due to its lower spatial correlation. Results further indicate that reducing the inter-element distance negatively impacts SOP, highlighting the importance of spatial diversity."}
{"id": "2511.19349", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19349", "abs": "https://arxiv.org/abs/2511.19349", "authors": ["Nour Jedidi", "Jimmy Lin"], "title": "Revisiting Feedback Models for HyDE", "comment": null, "summary": "Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods."}
{"id": "2511.18779", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18779", "abs": "https://arxiv.org/abs/2511.18779", "authors": ["Sanjit Bhowmick", "Deepak Kumar Dalai", "Sihem Mesnager"], "title": "On Construction of Linear (Euclidean) Hull Codes over Finite Extensions Binary Fields", "comment": null, "summary": "The hull of a linear code is defined as the intersection of the code and its dual. This concept was initially introduced to classify finite projective planes. The hull plays a crucial role in determining the complexity of algorithms used to check the permutation equivalence of two linear codes and compute a linear code's automorphism group. Research has shown that these algorithms are very effective when the hull size is small. Linear complementary dual (LCD) codes have the smallest hulls, while codes with a one-dimensional hull have the second smallest.\n  A recent notable paper that directs our investigation is authored by H. Chen, titled ``On the Hull-Variation Problem of Equivalent Linear Codes\", published in IEEE Transactions on Information Theory, volume 69, issue 5, in 2023. In this paper, we first explore the one-dimensional hull of a linear code over finite fields. Additionally, we demonstrate that any LCD code over an extended binary field \\( \\FF_q \\) (where \\( q > 3 \\)) with a minimum distance of at least $2$ is equivalent to the one-dimensional hull of a linear code under a specific weak condition. Furthermore, we provide a construction for creating hulls with \\( \\ell + 1 \\)-dimensionality from an \\( \\ell \\)-dimensional hull of a linear code, again under a weak condition. This corresponds to a particularly challenging direction, as creating \\( \\ell \\)-dimensional hulls from \\( \\ell + 1 \\)-dimensional hulls. Finally, we derive several constructions for the \\( \\ell \\)-dimensional hulls of linear codes as a consequence of our results."}
{"id": "2511.19020", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.19020", "abs": "https://arxiv.org/abs/2511.19020", "authors": ["Vishnu Priya Chekuru", "Ganapathiraju S S Ananya Varma", "Arti Yardi", "Praful Mankar"], "title": "Detection of Number of Subcarriers of OFDM Systems using Eigen-Spectral Analysis", "comment": null, "summary": "Orthogonal Frequency-Division Multiplexing (OFDM) is widely used in modern wireless communication systems due to its robustness against time-dispersive channels. In this work, we consider a non-cooperative scenario where the receiver does not have prior knowledge of the OFDM parameters such as the number of subcarriers and the aim is to estimate them using the received data. Such a setup has applications in cognitive radio networks. For this blind OFDM parameter estimation problem, we provide a novel method based on eigen-spectral analysis of the covariance matrix corresponding to the received data. In particular, we show that the covariance matrix exhibits a distinctive rank property under correct segmentation of the received symbols, reflecting a characteristic behavior in its eigenvalue spectrum that facilitates accurate estimation of the number of subcarriers. The proposed method is more general than existing approaches in the literature, as it can detect an arbitrary number of subcarriers and its performance remains independent of the modulation scheme. The numerical results show that the proposed method accurately detects the number of subcarriers with high probability even at low SNR."}
{"id": "2511.19074", "categories": ["cs.IT", "eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.19074", "abs": "https://arxiv.org/abs/2511.19074", "authors": ["Yen-Chi Lee"], "title": "On the Tail Transition of First Arrival Position Channels: From Cauchy to Exponential Decay", "comment": "10 pages, 3 figures. Preprint submitted to IEEE Communications Letters", "summary": "While the zero-drift First Arrival Position (FAP) channel is rigorously known to be Cauchy-distributed, practical molecular communication systems typically operate with non-zero drift. This letter characterizes the transition from heavy-tailed Cauchy behavior to light-tailed exponential decay. Through asymptotic analysis, we identify a critical spatial scale $n_c=σ^2/v$ separating diffusion- and drift-dominated regimes, revealing that the channel effectively behaves as a ``Truncated Cauchy'' model. Numerical results show that Gaussian approximations severely underestimate capacity at low drift, while the zero-drift case provides the appropriate performance lower bound for systems where drift assists particle transport."}
{"id": "2511.19133", "categories": ["cs.IT", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19133", "abs": "https://arxiv.org/abs/2511.19133", "authors": ["Runxin Zhang", "Yulin Shao", "Yuanwei Liu"], "title": "Directional Pinching-Antenna Systems", "comment": null, "summary": "We propose a directional pinching-antenna system (DiPASS), a comprehensive framework that transitions PASS modeling from idealized abstraction to physical consistency. DiPASS introduces the first channel model that accurately captures the directional, pencil-like radiation of pinching antennas, incorporates a practical waveguide attenuation of 1.3 dB/m, and accounts for stochastic line-of-sight blockage. A key enabler of DiPASS is our new \"equal quota division\" power allocation strategy, which guarantees predetermined coupling lengths independent of antenna positions, thereby overcoming a critical barrier to practical deployment. Our analysis yields foundational insights: we derive closed-form solutions for optimal antenna placement and orientation in single-PA scenarios, quantifying the core trade-off between waveguide and free-space losses. For multi-PA systems, we develop a scalable optimization framework that leverages directional sparsity, revealing that waveguide diversity surpasses antenna density in enhancing system capacity. Extensive simulations validate our analysis and demonstrate that DiPASS provides a realistic performance benchmark, fundamentally reshaping the understanding and design principles for future PASS-enabled 6G networks."}
{"id": "2511.19156", "categories": ["cs.IT", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.19156", "abs": "https://arxiv.org/abs/2511.19156", "authors": ["Jianfeng Xu", "Zeyan Li"], "title": "Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints", "comment": null, "summary": "The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This \"Energy-Time-Space\" conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence."}
{"id": "2511.19249", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.19249", "abs": "https://arxiv.org/abs/2511.19249", "authors": ["Yuan Li", "Zicheng Ye", "Huazi Zhang", "Jun Wang", "Wen Tong", "Guiying Yan", "Zhiming Ma"], "title": "Stitched Polar Codes", "comment": null, "summary": "In this paper, we introduce stitched polar codes, a novel generalization of Arıkan's regular polar codes. Our core methodology reconfigures the fundamental polarization process by stitching additional structures to enhance the reliability of less reliable information bits in the original code. This approach preserves the polar transformation structure and maintains the same encoding and decoding complexity. Thanks to the flexible configuration, stitched polar codes consistently outperform regular polar codes, effectively solving the performance degradation issue in rate-matched scenarios. Furthermore, we provide theoretical analysis on the weight spectrum and the polarization speed of stitched polar codes to prove their superiority."}
{"id": "2511.19398", "categories": ["cs.DS", "cs.IT", "cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19398", "abs": "https://arxiv.org/abs/2511.19398", "authors": ["Ilias Diakonikolas", "Daniel M. Kane", "Sihan Liu", "Thanasis Pittas"], "title": "PTF Testing Lower Bounds for Non-Gaussian Component Analysis", "comment": null, "summary": "This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.\n  In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions."}
