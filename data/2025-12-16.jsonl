{"id": "2512.12624", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12624", "abs": "https://arxiv.org/abs/2512.12624", "authors": ["Lankadinee Rathuwadu", "Guanli Liu", "Christopher Leckie", "Renata Borovica-Gajic"], "title": "CoLSE: A Lightweight and Robust Hybrid Learned Model for Single-Table Cardinality Estimation using Joint CDF", "comment": null, "summary": "Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance.\n  To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods."}
{"id": "2512.12957", "categories": ["cs.DB", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.12957", "abs": "https://arxiv.org/abs/2512.12957", "authors": ["Wolfgang Gatterbauer", "Diandre Miguel Sabale"], "title": "Database Research needs an Abstract Relational Query Language", "comment": "CIDR 2026. 16th Annual Conference on Innovative Data Systems Research (CIDR '26). January 18-21, 2026, Chaminade, USA. 16 pages, 21 figures", "summary": "For decades, SQL has been the default language for composing queries, but it is increasingly used as an artifact to be read and verified rather than authored. With Large Language Models (LLMs), queries are increasingly machine-generated, while humans read, validate, and debug them. This shift turns relational query languages into interfaces for back-and-forth communication about intent, which will lead to a rethinking of relational language design, and more broadly, relational interface design.\n  We argue that this rethinking needs support from an Abstract Relational Query Language (ARQL): a semantics-first reference metalanguage that separates query intent from user-facing syntax and makes underlying relational patterns explicit and comparable across user-facing languages. An ARQL separates a query into (i) a relational core (the compositional structure that determines intent), (ii) modalities (alternative representations of that core tailored to different audiences), and (iii) conventions (orthogonal environment-level semantic parameters under which the core is interpreted, e.g., set vs. bag semantics, or treatment of null values). Usability for humans or machines then depends less on choosing a particular language and more on choosing an appropriate modality. Comparing languages becomes a question of which relational patterns they support and what conventions they choose.\n  We introduce Abstract Relational Calculus (ARC), a strict generalization of Tuple Relational Calculus (TRC), as a concrete instance of ARQL. ARC comes in three modalities: (i) a comprehension-style textual notation, (ii) an Abstract Language Tree (ALT) for machine reasoning about meaning, and (iii) a diagrammatic hierarchical graph (higraph) representation for humans. ARC provides the missing vocabulary and acts as a Rosetta Stone for relational querying."}
{"id": "2512.12458", "categories": ["cs.IR", "cs.CG", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12458", "abs": "https://arxiv.org/abs/2512.12458", "authors": ["Vihan Lakshman", "Blaise Munyampirwa", "Julian Shun", "Benjamin Coleman"], "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval", "comment": "27 pages", "summary": "Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality."}
{"id": "2512.12980", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12980", "abs": "https://arxiv.org/abs/2512.12980", "authors": ["Tingyang Chen", "Cong Fu", "Jiahua Wu", "Haotian Wu", "Hua Fan", "Xiangyu Ke", "Yunjun Gao", "Yabo Ni", "Anxiang Zeng"], "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views", "comment": "SIGMOD2026", "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads."}
{"id": "2512.12196", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.12196", "abs": "https://arxiv.org/abs/2512.12196", "authors": ["Xiaoxuan Tang", "Xinping Lei", "Chaoran Zhu", "Shiyun Chen", "Ruibin Yuan", "Yizhi Li", "Changjae Oh", "Ge Zhang", "Wenhao Huang", "Emmanouil Benetos", "Yang Liu", "Jiaheng Liu", "Yinghao Ma"], "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation", "comment": null, "summary": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work."}
{"id": "2512.11994", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.11994", "abs": "https://arxiv.org/abs/2512.11994", "authors": ["Arijit Bishnu", "Debarshi Chanda", "Buddha Dev Das", "Arijit Ghosh", "Gopinath Mishra"], "title": "Optimal non-adaptive algorithm for edge estimation", "comment": "15 pages", "summary": "We present a simple nonadaptive randomized algorithm that estimates the number of edges in a simple, unweighted, undirected graph, possibly containing isolated vertices, using only degree and random edge queries. For an $n$-vertex graph, our method requires only $\\widetilde{O}(\\sqrt{n})$ queries, achieving sublinear query complexity. The algorithm independently samples a set of vertices and queries their degrees, and also independently samples a set of edges, using the answers to these queries to estimate the total number of edges in the graph. We further prove a matching lower bound, establishing the optimality of our algorithm and resolving the non-adaptive query complexity of this problem with respect to degree and random-edge queries."}
{"id": "2512.12031", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.12031", "abs": "https://arxiv.org/abs/2512.12031", "authors": ["Javad Zahedi Moghaddam", "Aria Nosratinia"], "title": "Differentially Private Community Detection in $h$-uniform Hypergraphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper studies the exact recovery threshold subject to preserving the privacy of connections in $h$-uniform hypergraphs. Privacy is characterized by the $(ε, δ)$-hyperedge differential privacy (DP), an extension of the notion of $(ε, δ)$-edge DP in the literature. The hypergraph observations are modeled through a $h$-uniform stochastic block model ($h$-HSBM) in the dense regime. We investigate three differentially private mechanisms: stability-based, sampling-based, and perturbation-based mechanisms. We calculate the exact recovery threshold for each mechanism and study the contraction of the exact recovery region due to the privacy budget, $(ε, δ)$. Sampling-based mechanisms and randomized response mechanisms guarantee pure $ε$-hyperedge DP where $δ=0$, while the stability-based mechanisms cannot achieve this level of privacy. The dependence of the limits of the privacy budget on the parameters of the $h$-uniform hypergraph is studied. More precisely, it is proven rigorously that the minimum privacy budget scales logarithmically with the ratio between the density of in-cluster hyperedges and the cross-cluster hyperedges for stability-based and Bayesian sampling-based mechanisms, while this budget depends only on the size of the hypergraph for the randomized response mechanism."}
{"id": "2512.12084", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12084", "abs": "https://arxiv.org/abs/2512.12084", "authors": ["Hanzhou Liu", "Kai Yin", "Zhitong Chen", "Chenyue Liu", "Ali Mostafavi"], "title": "FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL", "comment": null, "summary": "Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains."}
{"id": "2512.12344", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.12344", "abs": "https://arxiv.org/abs/2512.12344", "authors": ["Olusola Odeyomi", "Tokunbo Ogunfunmi", "Adjovi Laba"], "title": "Differentially Private Online Distributed Aggregative Games With Time-Varying and Non-Identical Communication and Feedback Delays", "comment": null, "summary": "This paper investigates online distributed aggregative games with time-varying cost functions, where agents are interconnected through an unbalanced communication graph. Due to the distributed and noncooperative nature of the game, some curious agents may wish to steal sensitive information from neighboring agents during parameter exchanges. Additionally, communication delays arising from network congestion, particularly in wireless settings, as well as feedback delays, can hinder the convergence of agents to a Nash equilibrium. Although a recent work addressed both communication and feedback delays in aggregative games, it is based on the unrealistic assumption that the delays are fixed over time and identical across agents. Hence, the case of time-varying and non-identical delays across agents has never been considered in aggregative games. In this work, we address the combined challenges of privacy leakage with time-varying and non-identical communication and feedback delays for the first time. We propose an online distributed dual averaging algorithm that simultaneously tackles these challenges while achieving a provably low regret bound. Our simulation result shows that the running average of each client's local action converges over time."}
{"id": "2512.12772", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12772", "abs": "https://arxiv.org/abs/2512.12772", "authors": ["Jianghan Chao", "Jianzhang Gao", "Wenhui Tan", "Yuchong Sun", "Ruihua Song", "Liyun Ru"], "title": "JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation", "comment": null, "summary": "Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning."}
{"id": "2512.12202", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.12202", "abs": "https://arxiv.org/abs/2512.12202", "authors": ["Yossi Azar", "Niv Buchbinder", "Tomer Epshtein"], "title": "Load Balancing with Duration Predictions", "comment": "Brief announcement presented at SPAA 25 (2025)", "summary": "We study the classic fully dynamic load balancing problem on unrelated machines where jobs arrive and depart over time and the goal is minimizing the maximum load, or more generally the l_p-norm of the load vector. Previous work either studied the clairvoyant setting in which exact durations are known to the algorithm, or the unknown duration setting in which no information on the duration is given to the algorithm. For the clairvoyant setting algorithms with polylogarithmic competitive ratios were designed, while for the unknown duration setting strong lower bounds exist and only polynomial competitive factors are possible.\n  We bridge this gap by studying a more realistic model in which some estimate/prediction of the duration is available to the algorithm. We observe that directly incorporating predictions into classical load balancing algorithms designed for the clairvoyant setting can lead to a notable decline in performance. We design better algorithms whose performance depends smoothly on the accuracy of the available prediction. We also prove lower bounds on the competitiveness of algorithms that use such inaccurate predictions."}
{"id": "2512.12149", "categories": ["cs.IT", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.12149", "abs": "https://arxiv.org/abs/2512.12149", "authors": ["Thyda Siv"], "title": "A Framework for Scalable Digital Twin Deployment in Smart Campus Building Facility Management", "comment": null, "summary": "Digital twin (DT) offers significant opportunities for enhancing facility management (FM) in campus environments. However, existing research often focuses narrowly on isolated domains, such as point-cloud geometry or energy analytics, without providing a scalable and interoperable workflow that integrates building geometry, equipment metadata, and operational data into a unified FM platform. This study proposes a comprehensive framework for scalable digital-twin deployment in smart campus buildings by integrating 3D laser scanning, BIM modeling, and IoT-enabled data visualization to support facility operations and maintenance. The methodology includes: (1) reality capture using terrestrial laser scanning and structured point-cloud processing; (2) development of an enriched BIM model incorporating architectural, mechanical, electrical, plumbing, conveying, and sensor systems; and (3) creation of a digital-twin environment that links equipment metadata, maintenance policies, and simulated IoT data within a digital-twin management platform. A case study of the Price Gilbert Building at Georgia Tech demonstrates the implementation of this workflow. A total of 509 equipment items were modeled and embedded with OmniClass classifications into the digital twin. Ten interactive dashboards were developed to visualize system performance. Results show that the proposed framework enables centralized asset documentation, improved system visibility, and enhanced preventive and reactive maintenance workflows. Although most IoT data were simulated due to limited existing sensor infrastructure, the prototype validates the feasibility of a scalable digital twin for facility management and establishes a reference model for real-time monitoring, analytics integration, and future autonomous building operations."}
{"id": "2512.12458", "categories": ["cs.IR", "cs.CG", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12458", "abs": "https://arxiv.org/abs/2512.12458", "authors": ["Vihan Lakshman", "Blaise Munyampirwa", "Julian Shun", "Benjamin Coleman"], "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval", "comment": "27 pages", "summary": "Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality."}
{"id": "2512.12486", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.12486", "abs": "https://arxiv.org/abs/2512.12486", "authors": ["Tyler Becker", "Zachary Sunberg"], "title": "Simultaneous AlphaZero: Extending Tree Search to Markov Games", "comment": null, "summary": "Simultaneous AlphaZero extends the AlphaZero framework to multistep, two-player zero-sum deterministic Markov games with simultaneous actions. At each decision point, joint action selection is resolved via matrix games whose payoffs incorporate both immediate rewards and future value estimates. To handle uncertainty arising from bandit feedback during Monte Carlo Tree Search (MCTS), Simultaneous AlphaZero incorporates a regret-optimal solver for matrix games with bandit feedback. Simultaneous AlphaZero demonstrates robust strategies in a continuous-state discrete-action pursuit-evasion game and satellite custody maintenance scenarios, even when evaluated against maximally exploitative opponents."}
{"id": "2512.13169", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2512.13169", "abs": "https://arxiv.org/abs/2512.13169", "authors": ["Thanh-Danh Luu", "Le-Vu Nguyen Dinh", "Duc-Thien Tran", "Duy-Bao Bui", "Nam-Tien Le", "Tinh-Anh Nguyen Nhu"], "title": "Integrated Semantic and Temporal Alignment for Interactive Video Retrieval", "comment": null, "summary": "The growing volume of video data and the introduction of complex retrieval challenges, such as the Temporal Retrieval and Alignment of Key Events (TRAKE) task at the Ho Chi Minh City AI Challenge 2025, expose critical limitations in existing systems. Many methodologies lack scalable, holistic architectures and rely on \"frozen\" embedding models that fail on out-of-knowledge (OOK) or real-world queries. This paper introduces the comprehensive video retrieval framework developed by team AIO\\_Owlgorithms to address these gaps. Our system features an architecture integrating TransNetV2 for scene segmentation, BEiT-3 for visual embeddings in Milvus, and Gemini OCR for metadata in Elasticsearch. We propose two components: (1) \\textbf{QUEST} (Query Understanding and External Search for Out-of-Knowledge Tasks), a two-branch framework that leverages a Large Language Model (LLM) for query rewriting and an external image search pathway to resolve OOK queries; and (2) \\textbf{DANTE} (Dynamic Alignment of Narrative Temporal Events), a dynamic programming algorithm that efficiently solves the temporally-incoherent TRAKE task. These contributions form a robust and intelligent system that significantly advances the state-of-the-art in handling complex, real-world video search queries."}
{"id": "2512.12860", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.12860", "abs": "https://arxiv.org/abs/2512.12860", "authors": ["Aritra Banik", "Mano Prakash Parthasarathi", "Venkatesh Raman", "Diya Roy", "Abhishek Sahu"], "title": "Learning with Structure: Computing Consistent Subsets on Structurally-Regular Graphs", "comment": "15 pages, 1 figure", "summary": "The Minimum Consistent Subset (MCS) problem arises naturally in the context of supervised clustering and instance selection. In supervised clustering, one aims to infer a meaningful partitioning of data using a small labeled subset. However, the sheer volume of training data in modern applications poses a significant computational challenge. The MCS problem formalizes this goal: given a labeled dataset $\\mathcal{X}$ in a metric space, the task is to compute a smallest subset $S \\subseteq \\mathcal{X}$ such that every point in $\\mathcal{X}$ shares its label with at least one of its nearest neighbors in $S$.\n  Recently, the MCS problem has been extended to graph metrics, where distances are defined by shortest paths. Prior work has shown that MCS remains NP-hard even on simple graph classes like trees, though an algorithm with runtime $\\mathcal{O}(2^{6c} \\cdot n^6)$ is known for trees, where $c$ is the number of colors and $n$ the number of vertices. This raises the challenge of identifying graph classes that admit algorithms efficient in both $n$ and $c$.\n  In this work, we study the Minimum Consistent Subset problem on graphs, focusing on two well-established measures: the vertex cover number ($vc$) and the neighborhood diversity ($nd$). We develop an algorithm with running time $vc^{\\mathcal{O}(vc)}\\cdot\\text{Poly}(n,c)$, and another algorithm with runtime $nd^{\\mathcal{O}(nd)}\\cdot\\text{Poly}(n,c)$. In the language of parameterized complexity, this implies that MCS is fixed-parameter tractable (FPT) parameterized by the vertex cover number and the neighborhood diversity. Notably, our algorithms remain efficient for arbitrarily many colors, as their complexity is polynomially dependent on the number of colors."}
{"id": "2512.12170", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12170", "abs": "https://arxiv.org/abs/2512.12170", "authors": ["Yiming Cui", "Jiajia Guo", "Xiao Li", "Chao-Kai Wen", "Shi Jin"], "title": "Large and Small Model Collaboration for Air Interface", "comment": null, "summary": "Large artificial intelligence models (LAMs) have shown strong capability in wireless communications, yet existing works mainly rely on their generalized knowledge across environments while overlooking the potential gains of environment-specific adaptation. Directly fine-tuning LAMs for adaptation is often impractical due to prohibitive training costs, low inference efficiency in multi-user scenarios, and the risk of catastrophic forgetting, in addition to the limited accessibility of model parameters. To address these limitations, we establish a collaborative framework for air interface. In this framework, unlike prior approaches that either depend solely on LAMs or require direct fine-tuning, LAMs are exploited as a universal channel knowledge base while small artificial intelligence models (SAMs) are employed as lightweight plugins to capture environment-specific knowledge, facilitating efficient environment-specific adaptation of LAMs. Subsequently, we instantiate this framework for CSI feedback tasks, and develop a large and small collaboration framework for CSI feedback, referred to as LASCO. LASCO operates by letting the base LAM produce an initial CSI reconstruction, learning the environment-induced reconstruction shift through a reference SAM and a proxy SAM, and transferring this shift back to the LAM. To further enhance adaptability, we introduce elastic-LASCO (E-LASCO), which augments LASCO with learnable collaboration coefficients that control the contribution of LAMs and SAMs across different environments. Numerical results demonstrate that LASCO and E-LASCO enables LAMs to achieve environment-specific performance gains with significantly reduced training costs, lower data collection requirements, and faster adaptation speed."}
{"id": "2512.12740", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12740", "abs": "https://arxiv.org/abs/2512.12740", "authors": ["Dezhi Yi", "Wei Guo", "Wenyang Cui", "Wenxuan He", "Huifeng Guo", "Yong Liu", "Zhenhua Dong", "Ye Lu"], "title": "FuXi-$γ$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism", "comment": "Accepted by KDD 2026", "summary": "Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$γ$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$γ$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$γ$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\\times$ and inference by up to 6.18$\\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma."}
{"id": "2512.12582", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.12582", "abs": "https://arxiv.org/abs/2512.12582", "authors": ["Kexin Chen", "Jianwei Huang", "Yuan Luo"], "title": "Generative AI as Digital Representatives in Collective Decision-Making: A Game-Theoretical Approach", "comment": "ECAI 2025", "summary": "Generative Artificial Intelligence (GenAI) enables digital representatives to make decisions on behalf of team members in collaborative tasks, but faces challenges in accurately representing preferences. While supplying GenAI with detailed personal information improves representation fidelity, feasibility constraints make complete information access impractical. We bridge this gap by developing a game-theoretic framework that models strategic information revelation to GenAI in collective decision-making. The technical challenges lie in characterizing members' equilibrium behaviors under interdependent strategies and quantifying the imperfect preference learning outcomes by digital representatives. Our contribution includes closed-form equilibrium characterizations that reveal how members strategically balance team decision preference against communication costs. Our analysis yields an interesting finding: Conflicting preferences between team members drive competitive information revelation, with members revealing more information than those with aligned preferences. While digital representatives produce aggregate preference losses no smaller than direct participation, individual members may paradoxically achieve decisions more closely aligned with their preferences when using digital representatives, particularly when manual participation costs are high or when GenAI systems are sufficiently advanced."}
{"id": "2512.12900", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.12900", "abs": "https://arxiv.org/abs/2512.12900", "authors": ["Mohit Daga"], "title": "Sub-$n^k$ Deterministic algorithm for minimum $k$-way cut in simple graphs", "comment": null, "summary": "We present a \\emph{deterministic exact algorithm} for the \\emph{minimum $k$-cut problem} on simple graphs.\n  Our approach combines the \\emph{principal sequence of partitions (PSP)}, derived canonically from ideal loads, with a single level of \\emph{Kawarabayashi--Thorup (KT)} contractions at the critical PSP threshold~$λ_j$.\n  Let $j$ be the smallest index with $κ(P_j)\\ge k$ and $R := k - κ(P_{j-1})$.\n  We prove a structural decomposition theorem showing that an optimal $k$-cut can be expressed as the level-$(j\\!-\\!1)$ boundary $A_{\\le j-1}$ together with exactly $(R-r)$ \\emph{non-trivial} internal cuts of value at most~$λ_j$ and $r$ \\emph{singleton isolations} (``islands'') inside the parts of~$P_{j-1}$.\n  At this level, KT contractions yield kernels of total size $\\widetilde{O}(n / λ_j)$, and from them we build a \\emph{canonical border family}~$\\mathcal{B}$ of the same order that deterministically covers all optimal refinement choices.\n  Branching only over~$\\mathcal{B}$ (and also including an explicit ``island'' branch) gives total running time\n  $$\n  T(n,m,k) = \\widetilde{O}\\left(\\mathrm{poly}(m)+\\Bigl(\\tfrac{n}{λ_j}+n^{ω/3}\\Bigr)^{R}\\right),\n  $$\n  where $ω< 2.373$ is the matrix multiplication exponent.\n  In particular, if $λ_j \\ge n^{\\varepsilon}$ for some constant $\\varepsilon > 0$, we obtain a \\emph{deterministic sub-$n^k$-time algorithm}, running in $n^{(1-\\varepsilon)(k-1)+o(k)}$ time.\n  Finally, combining our PSP$\\times$KT framework with a small-$λ$ exact subroutine via a simple meta-reduction yields a deterministic $n^{c k+O(1)}$ algorithm for $c = \\max\\{ t/(t+1), ω/3 \\} < 1$, aligning with the exponent in the randomized bound of He--Li (STOC~2022) under the assumed subroutine."}
{"id": "2512.12335", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12335", "abs": "https://arxiv.org/abs/2512.12335", "authors": ["Anup Kushwaha", "Om Prakash"], "title": "Hulls of Free Linear Codes over a Non-Unital Ring", "comment": "20", "summary": "This paper investigates the hull codes of free linear codes over a non-unital ring $ E= \\langle κ,τ\\mid 2 κ=2 τ=0,~ κ^2=κ,~ τ^2=τ,~ κτ=κ,~ τκ=τ\\rangle$. Initially, we examine the residue and torsion codes of various hulls of $E$-linear codes and obtain an explicit form of the generator matrix of the hull of a free $E$-linear code. Then, we propose four build-up construction methods to construct codes with a larger length and hull-rank from codes with a smaller length and hull-rank. Some illustrative examples are also given to support our build-up construction methods. Subsequently, we study the permutation equivalence of two free $E$-linear codes and discuss the hull-variation problem. As an application, we classify optimal free $E$-linear codes for lengths up to $8$."}
{"id": "2512.12760", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12760", "abs": "https://arxiv.org/abs/2512.12760", "authors": ["Sina Jani", "Arman Heidari", "Amirmohammad Anvari", "Zahra Rahimi"], "title": "Intelligent Scientific Literature Explorer using Machine Learning (ISLE)", "comment": "18 pages, 7 figures, 3 tables", "summary": "The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery."}
{"id": "2512.12910", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.12910", "abs": "https://arxiv.org/abs/2512.12910", "authors": ["David Yang", "Yuan Gao", "Tianyi Lin", "Christian Kroer"], "title": "A Direct Second-Order Method for Solving Two-Player Zero-Sum Games", "comment": "30 pages", "summary": "We introduce, to our knowledge, the first direct second-order method for computing Nash equilibria in two-player zero-sum games. To do so, we construct a Douglas-Rachford-style splitting formulation, which we then solve with a semi-smooth Newton (SSN) method. We show that our algorithm enjoys local superlinear convergence. In order to augment the fast local behavior of our SSN method with global efficiency guarantees, we develop a hybrid method that combines our SSN method with the state-of-the-art first-order method for game solving, Predictive Regret Matching$^+$ (PRM$^+$). Our hybrid algorithm leverages the global progress provided by PRM$^+$, while achieving a local superlinear convergence rate once it switches to SSN near a Nash equilibrium. Numerical experiments on matrix games demonstrate order-of-magnitude speedups over PRM$^+$ for high-precision solutions."}
{"id": "2512.13105", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.13105", "abs": "https://arxiv.org/abs/2512.13105", "authors": ["Antoine El-Hayek", "Monika Henzinger", "Jason Li"], "title": "Deterministic and Exact Fully-dynamic Minimum Cut of Superpolylogarithmic Size in Subpolynomial Time", "comment": "To appear at SODA 2026", "summary": "We present an exact fully-dynamic minimum cut algorithm that runs in $n^{o(1)}$ deterministic update time when the minimum cut size is at most $2^{Θ(\\log^{3/4-c}n)}$ for any $c>0$, improving on the previous algorithm of Jin, Sun, and Thorup (SODA 2024) whose minimum cut size limit is $(\\log n)^{o(1)}$. Combined with graph sparsification, we obtain the first $(1+ε)$-approximate fully-dynamic minimum cut algorithm on weighted graphs, for any $ε\\ge2^{-Θ(\\log^{3/4-c}n)}$, in $n^{o(1)}$ randomized update time.\n  Our main technical contribution is a deterministic local minimum cut algorithm, which replaces the randomized LocalKCut procedure from El-Hayek, Henzinger, and Li (SODA 2025)."}
{"id": "2512.12366", "categories": ["cs.IT", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12366", "abs": "https://arxiv.org/abs/2512.12366", "authors": ["Babak Badnava", "Jacob Chakareski", "Morteza Hashemi"], "title": "ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems", "comment": "Submitted to ACM TOMM", "summary": "Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations."}
{"id": "2512.12938", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12938", "abs": "https://arxiv.org/abs/2512.12938", "authors": ["Duy A. Nguyen", "Hai H. Do", "Minh Doan", "Minh N. Do"], "title": "SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems", "comment": null, "summary": "The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings."}
{"id": "2512.13244", "categories": ["cs.GT", "cs.CC", "cs.MA", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.13244", "abs": "https://arxiv.org/abs/2512.13244", "authors": ["Wei-Chen Lee", "Martin Bullinger", "Alessandro Abate", "Michael Wooldridge"], "title": "Fair Coordination in Strategic Scheduling", "comment": null, "summary": "We consider a scheduling problem of strategic agents representing jobs of different weights. Each agent has to decide on one of a finite set of identical machines to get their job processed. In contrast to the common and exclusive focus on makespan minimization, we want the outcome to be fair under strategic considerations of the agents. Two natural properties are credibility, which ensures that the assignment is a Nash equilibrium and equality, requiring that agents with equal-weight jobs are assigned to machines of equal load. We combine these two with a hierarchy of fairness properties based on envy-freeness together with several relaxations based on the idea that envy seems more justified towards agents with a higher weight. We present a complete complexity landscape for satisfiability and decision versions of these properties, alone or in combination, and study them as structural constraints under makespan optimization. For our positive results, we develop a unified algorithmic approach, where we achieve different properties by fine-tuning key subroutines."}
{"id": "2512.13210", "categories": ["cs.DS", "cs.CC", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.13210", "abs": "https://arxiv.org/abs/2512.13210", "authors": ["Marin Bougeret", "Eric Brandwein", "Ignasi Sau"], "title": "Kernelization dichotomies for hitting minors under structural parameterizations", "comment": "50 pages, 13 figures. Conference version in STACS 2026", "summary": "For a finite collection of connected graphs $\\mathcal{F}$, the $\\mathcal{F}$-MINOR-DELETION problem consists in, given a graph $G$ and an integer $\\ell$, deciding whether $G$ contains a vertex set of size at most $\\ell$ whose removal results in an $\\mathcal{F}$-minor-free graph. We lift the existence of (approximate) polynomial kernels for $\\mathcal{F}$-MINOR-DELETION by the solution size to (approximate) polynomial kernels parameterized by the vertex-deletion distance to graphs of bounded elimination distance to $\\mathcal{F}$-minor-free graphs. This results in exact polynomial kernels for every family $\\mathcal{F}$ that contains a planar graph, and an approximate polynomial kernel for PLANAR VERTEX DELETION. Moreover, combining our result with a previous lower bound, we obtain the following infinite set of dichotomies, assuming $NP \\not\\subseteq coNP/poly$: for any finite set $\\mathcal{F}$ of biconnected graphs on at least three vertices containing a planar graph, and any minor-closed class of graphs $\\mathcal{C}$, $\\mathcal{F}$-MINOR-DELETION admits a polynomial kernel parameterized by the vertex-deletion distance to $\\mathcal{C}$ if and only if $\\mathcal{C}$ has bounded elimination distance to $\\mathcal{F}$-minor-free graphs. For instance, this yields dichotomies for CACTUS VERTEX DELETION, OUTERPLANAR VERTEX DELETION, and TREEWIDTH-$t$ VERTEX DELETION for every integer $t \\geq 0$. Prior to our work, such dichotomies were only known for the particular cases of VERTEX COVER and FEEDBACK VERTEX SET. Our approach builds on the techniques developed by Jansen and Pieterse [Theor. Comput. Sci. 2020] and also uses adaptations of some of the results by Jansen, de Kroon, and Wlodarczyk [STOC 2021]."}
{"id": "2512.12519", "categories": ["cs.IT", "math.RA"], "pdf": "https://arxiv.org/pdf/2512.12519", "abs": "https://arxiv.org/abs/2512.12519", "authors": ["Jiabin Wang", "Jinquan Luo"], "title": "Linear Codes with Certain Dimension of Hermitian Hulls", "comment": null, "summary": "In this paper, we study the enumerative and asymptotic properties related to Hermitian $\\ell$-complementary codes on the unitary space over $\\F_{q^2}$. We provide some closed form expressions for the counting formulas of Hermitian $\\ell$-complementary codes. There is a similarity in the asymptotic weight distribution between Hermitian self-orthogonal codes and unrestricted codes. Furthermore, we study the asymptotic behavior of Hermitian self-orthogonal codes whose minimum distance is at least $d$. In particular, we conclude that MDS codes within the class of Hermitian self-orthogonal codes are asymptotically dense when the alphabet size approaches to infinity."}
{"id": "2512.12964", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12964", "abs": "https://arxiv.org/abs/2512.12964", "authors": ["Yupeng Li", "Mingyue Cheng", "Yucong Luo", "Yitong Zhou", "Qingyang Mao", "Shijin Wang"], "title": "BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation", "comment": null, "summary": "Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach."}
{"id": "2512.13538", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.13538", "abs": "https://arxiv.org/abs/2512.13538", "authors": ["Victor Khomenko", "Maciej Koutny", "Alex Yakovlev"], "title": "Distributed Places and Safe Net Reduction", "comment": null, "summary": "Being able to find small Petri nets with the same behaviour as formal specifications of concurrent systems benefits both effective verification and practical implementation of such systems. This paper considers specifications given in the form of compositionally defined safe nets.\n  The paper discusses a novel concept of distributed place which implements the behaviour of an individual net place. It is shown that if distributed places cover a safe Petri net, then it is possible to delete some places without changing the behaviour. Crucially, the reduction is both static and local, making it computationally feasible in practice.\n  The resulting reduction technique is then applied to an algebra of safe Petri nets (boxes) derived compositionally from process (box) expressions. Though the original derivation can yield exponentially large boxes, prior research demonstrated that if a box expression does not involve cyclic behaviours, the exponential number of places can be reduced down to polynomial (quadratic). In this paper, using distributed places, it is show that similar optimisation can also be achieved in the case of process expressions with iteration."}
{"id": "2512.13342", "categories": ["cs.DS", "cs.CC", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.13342", "abs": "https://arxiv.org/abs/2512.13342", "authors": ["Sheikh Shakil Akhtar", "Pranabendu Misra", "Geevarghese Philip"], "title": "Space Efficient Algorithms for Parameterised Problems", "comment": null, "summary": "We study \"space efficient\" FPT algorithms for graph problems with limited memory. Let n be the size of the input graph and k be the parameter. We present algorithms that run in time f(k)*poly(n) and use g(k)*polylog(n) working space, where f and g are functions of k alone, for k-Path, MaxLeaf SubTree and Multicut in Trees. These algorithms are motivated by big-data settings where very large problem instances must be solved, and using poly(n) memory is prohibitively expensive. They are also theoretically interesting, since most of the standard methods tools, such as deleting a large set of vertices or edges, are unavailable, and we must a develop different way to tackle them."}
{"id": "2512.12563", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12563", "abs": "https://arxiv.org/abs/2512.12563", "authors": ["Tian Shi", "Wenkun Wen", "Peiran Wu", "Minghua Xia"], "title": "Vertical Heterogeneous Networks Beyond 5G: CoMP Coverage Enhancement and Optimization", "comment": "15 pages, 13 figures, 3 tables. To appear in IEEE TWC", "summary": "Low-altitude wireless networks are increasingly vital for the low-altitude economy, enabling wireless coverage in high-mobility and hard-to-reach environments. However, providing reliable connectivity to sparsely distributed aerial users in dynamic three-dimensional (3D) spaces remains a significant challenge. This paper investigates downlink coverage enhancement in vertical heterogeneous networks (VHetNets) beyond 5G, where uncrewed aerial vehicles (UAVs) operate as emerging aerial base stations (ABSs) alongside legacy terrestrial base stations (TBSs). To improve coverage performance, we propose a coordinated multi-point (CoMP) transmission framework that enables joint transmission from ABSs and TBSs. This approach mitigates the limitations of non-uniform user distributions and enhances reliability for sparse aerial users. Two UAV deployment strategies are considered: \\textit{i)} random UAV placement, analyzed using stochastic geometry to derive closed-form coverage expressions, and \\textit{ii)} optimized UAV placement using a coverage-aware weighted $K$-means clustering algorithm to maximize cooperative coverage in underserved areas. Theoretical analyses and Monte Carlo simulations demonstrate that the proposed CoMP-enabled VHetNet significantly improves downlink coverage probability, particularly in scenarios with sparse aerial users. These findings highlight the potential of intelligent UAV coordination and geometry-aware deployment to enable robust, adaptive connectivity in low-altitude wireless networks."}
{"id": "2512.12978", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12978", "abs": "https://arxiv.org/abs/2512.12978", "authors": ["Chee Heng Tan", "Huiying Zheng", "Jing Wang", "Zhuoyi Lin", "Shaodi Feng", "Huijing Zhan", "Xiaoli Li", "J. Senthilnath"], "title": "Do Reviews Matter for Recommendations in the Era of Large Language Models?", "comment": "11 pages, 9 figures, 3 tables", "summary": "With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing."}
{"id": "2512.12591", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12591", "abs": "https://arxiv.org/abs/2512.12591", "authors": ["Timofei Izhitskii"], "title": "Linear Binary Codes Correcting One or More Errors", "comment": null, "summary": "This paper examines linear binary codes capable of correcting one or more errors. For the single-error-correcting case, it is shown that the Hamming bound is achieved by a constructive method, and an exact expression for the minimal codeword length is derived. For the general case, a simple lower bound for the parameters of linear codes is derived from an analysis of the coset structure."}
{"id": "2512.12980", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12980", "abs": "https://arxiv.org/abs/2512.12980", "authors": ["Tingyang Chen", "Cong Fu", "Jiahua Wu", "Haotian Wu", "Hua Fan", "Xiangyu Ke", "Yunjun Gao", "Yabo Ni", "Anxiang Zeng"], "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views", "comment": "SIGMOD2026", "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads."}
{"id": "2512.12619", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12619", "abs": "https://arxiv.org/abs/2512.12619", "authors": ["Xu Gan", "Yuanwei Liu"], "title": "C-PASS: Center-Fed Pinching Antenna System", "comment": null, "summary": "A novel architecture of the center-fed pinching antenna system (C-PASS) is proposed. In contrast to the conventional end-fed PASS, signals are fed from the center input ports and propagate towards both sides of the waveguide. By doing so, spatial-multiplexing gain can be achieved in a single waveguide. Based on the proposed C-PASS, closed-form expressions for the degree of freedom (DoF) and power scaling laws are derived. These theoretical results reveal that C-PASS can achieve \\emph{twice} the DoF and an additional multiplexing gain of $\\mathcal{O}(P_T \\ln^4 N/N^2)$ compared to the conventional PASS, where $P_T$ and $N$ represent the transmit power and pinching antenna number, respectively. Numerical results are provided to demonstrate that substantial capacity improvements can be achieved through the enhanced DoF and multiplexing gain of the C-PASS."}
{"id": "2512.13001", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13001", "abs": "https://arxiv.org/abs/2512.13001", "authors": ["Genki Kusano", "Kenya Abe", "Kunihiro Takeoka"], "title": "Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?", "comment": null, "summary": "Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \\textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation."}
{"id": "2512.12758", "categories": ["cs.IT", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12758", "abs": "https://arxiv.org/abs/2512.12758", "authors": ["Jiping Luo", "Erfan Delfani", "Mehrdad Salimnejad", "Nikolaos Pappas"], "title": "From Information Freshness to Semantics of Information and Goal-oriented Communications", "comment": "Submitted to IEEE as invited paper", "summary": "Future wireless networks must support real-time, data-driven cyber-physical systems in which communication is tightly coupled with sensing, inference, control, and decision-making. Traditional communication paradigms centered on accuracy, throughput, and latency are increasingly inadequate for these systems, where the value of information depends on its semantic relevance to a specific task. This paper provides a unified exposition of the progression from classical distortion-based frameworks, through information freshness metrics such as the Age of Information (AoI) and its variants, to the emerging paradigm of goal-oriented semantics-aware communication. We organize and systematize existing semantics-aware metrics, including content- and version-aware measures, context-dependent distortion formulations, and history-dependent error persistence metrics that capture lasting impact and urgency. Within this framework, we highlight how these metrics address the limitations of purely accuracy- or freshness-centric designs, and how they collectively enable the selective generation and transmission of only task-relevant information. We further review analytical tools based on Markov decision process (MDP) and Lyapunov optimization methods that have been employed to characterize optimal or near-optimal timing and scheduling policies under semantic performance criteria and communication constraints. By synthesizing these developments into a coherent framework, the paper clarifies the design principles underlying goal-oriented, semantics-aware communication systems. It illustrates how they can significantly improve efficiency, reliability, and task performance. The presented perspective aims to serve as a bridge between information-theoretic, control-theoretic, and networking viewpoints, and to guide the design of semantic communication architectures for 6G and beyond."}
{"id": "2512.13037", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13037", "abs": "https://arxiv.org/abs/2512.13037", "authors": ["Taoran Sheng", "Sathappan Muthiah", "Atiq Islam", "Jinming Feng"], "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer", "comment": null, "summary": "In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms."}
{"id": "2512.13292", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.13292", "abs": "https://arxiv.org/abs/2512.13292", "authors": ["Farshad Rostami Ghadi", "F. Javier Lopez-Martinez", "Kai-Kit Wong", "Christos Masouros"], "title": "Information-Theoretic Limits of Integrated Sensing and Communication with Finite Learning Capacity", "comment": null, "summary": "This paper develops a unified information-theoretic framework for artificial-intelligence (AI)-aided integrated sensing and communication (ISAC), where a learning component with limited representational capacity is embedded within the transceiver loop. The study introduces the concept of an AI capacity budget to quantify how the finite ability of a learning model constrains joint communication and sensing performance. Under this framework, the paper derives both converse (upper) and achievability (lower) bounds that define the achievable rate-sensing region. For Gaussian channels, the effect of limited learning capacity is shown to behave as an equivalent additive noise, allowing simple analytical expressions for the resulting communication rate and sensing distortion. The theory is then extended to Rayleigh and Rician fading as well as to multiple-input multiple-output (MIMO) systems through new matrix inequalities and a constructive mapping between AI capacity and effective noise covariance. Resource allocation between sensing and communication is optimized under this learning constraint, yielding closed-form conditions in the Gaussian case. A general learning-information trade-off law is also established, linking the representational power of the learning module to the achievable performance frontier. Finally, a practical variational training procedure is proposed to enforce the capacity constraint and to guide empirical evaluation. The derived scaling laws provide quantitative insight for co-designing model size, waveform, and hardware in next-generation ISAC systems."}
{"id": "2512.13074", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13074", "abs": "https://arxiv.org/abs/2512.13074", "authors": ["Huimu Wang", "Yiming Qiu", "Xingzhi Yao", "Zhiguo Chen", "Guoyu Tang", "Songlin Wang", "Sulong Xu", "Mingming Li"], "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval", "comment": null, "summary": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.\n  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets."}
{"id": "2512.13370", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.13370", "abs": "https://arxiv.org/abs/2512.13370", "authors": ["Yang-Hui He", "Alexander Kasprzyk", "Q Le", "Dmitrii Riabchenko"], "title": "Machine learning discovers new champion codes", "comment": "23 pages, 9 figures", "summary": "Linear error-correcting codes form the mathematical backbone of modern digital communication and storage systems, but identifying champion linear codes (linear codes achieving or exceeding the best known minimum Hamming distance) remains challenging. By training a transformer to predict the minimum Hamming distance of a class of linear codes and pairing it with a genetic algorithm over the search space, we develop a novel method for discovering champion codes. This model effectively reduces the search space of linear codes needed to achieve champion codes. Our results present the use of this method in the study and construction of error-correcting codes, applicable to codes such as generalised toric, Reed-Muller, Bose-Chaudhuri-Hocquenghem, algebrogeometric, and potentially quantum codes."}
{"id": "2512.13120", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13120", "abs": "https://arxiv.org/abs/2512.13120", "authors": ["Mabiao Long", "Jiaxi Liu", "Yufeng Li", "Hao Xiong", "Junchi Yan", "Kefan Wang", "Yi Cao", "Jiandong Ding"], "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation", "comment": null, "summary": "Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments."}
{"id": "2512.13429", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.13429", "abs": "https://arxiv.org/abs/2512.13429", "authors": ["Kanat Abdukhalikov", "Gyanendra K. Verma"], "title": "Two Families of Linear Codes Containing Non-GRS MDS Codes", "comment": null, "summary": "We construct two new families of linear codes by modifying the generator matrices of generalized Reed-Solomon (GRS) codes. For these codes, we explicitly derive parity-check matrices and establish necessary and sufficient conditions ensuring the MDS property. Additionally, we explore subfamilies within these constructions that are non-GRS MDS codes. We also characterize their self-orthogonal and self-dual properties and present some explicit constructions and examples."}
{"id": "2512.13173", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13173", "abs": "https://arxiv.org/abs/2512.13173", "authors": ["Ivica Kostric", "Ujwal Gadiraju", "Krisztian Balog"], "title": "Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders", "comment": null, "summary": "The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS."}
{"id": "2512.13491", "categories": ["cs.IT", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13491", "abs": "https://arxiv.org/abs/2512.13491", "authors": ["Łukasz Dębowski"], "title": "From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis", "comment": "32 pages, no figures", "summary": "We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws."}
{"id": "2512.13368", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13368", "abs": "https://arxiv.org/abs/2512.13368", "authors": ["Mengyang Ma", "Xiaopeng Li", "Wanyu Wang", "Zhaocheng Du", "Jingtong Gao", "Pengyue Jia", "Yuyang Ye", "Yiqi Wang", "Yunpeng Weng", "Weihong Luo", "Xiao Han", "Xiangyu Zhao"], "title": "BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations", "comment": null, "summary": "Transformer structures have been widely used in sequential recommender systems (SRS). However, as user interaction histories increase, computational time and memory requirements also grow. This is mainly caused by the standard attention mechanism. Although there exist many methods employing efficient attention and SSM-based models, these approaches struggle to effectively model long sequences and may exhibit unstable performance on short sequences. To address these challenges, we design a sparse attention mechanism, BlossomRec, which models both long-term and short-term user interests through attention computation to achieve stable performance across sequences of varying lengths. Specifically, we categorize user interests in recommendation systems into long-term and short-term interests, and compute them using two distinct sparse attention patterns, with the results combined through a learnable gated output. Theoretically, it significantly reduces the number of interactions participating in attention computation. Extensive experiments on four public datasets demonstrate that BlossomRec, when integrated with state-of-the-art Transformer-based models, achieves comparable or even superior performance while significantly reducing memory usage, providing strong evidence of BlossomRec's efficiency and effectiveness.The code is available at https://github.com/ronineume/BlossomRec."}
{"id": "2512.13615", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.13615", "abs": "https://arxiv.org/abs/2512.13615", "authors": ["Ali Khalesi", "Petros Elia"], "title": "Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding", "comment": "46 pages, 9 figures", "summary": "This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target."}
{"id": "2512.13396", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13396", "abs": "https://arxiv.org/abs/2512.13396", "authors": ["Chaohua Yang", "Dugang Liu", "Shiwei Li", "Yuwen Fu", "Xing Tang", "Weihong Luo", "Xiangyu Zhao", "Xiuqiang He", "Zhong Ming"], "title": "Automated Information Flow Selection for Multi-scenario Multi-task Recommendation", "comment": "10 Pages, 6 Figures, WSDM 2026 Accepted", "summary": "Multi-scenario multi-task recommendation (MSMTR) systems must address recommendation demands across diverse scenarios while simultaneously optimizing multiple objectives, such as click-through rate and conversion rate. Existing MSMTR models typically consist of four information units: scenario-shared, scenario-specific, task-shared, and task-specific networks. These units interact to generate four types of relationship information flows, directed from scenario-shared or scenario-specific networks to task-shared or task-specific networks. However, these models face two main limitations: 1) They often rely on complex architectures, such as mixture-of-experts (MoE) networks, which increase the complexity of information fusion, model size, and training cost. 2) They extract all available information flows without filtering out irrelevant or even harmful content, introducing potential noise. Regarding these challenges, we propose a lightweight Automated Information Flow Selection (AutoIFS) framework for MSMTR. To tackle the first issue, AutoIFS incorporates low-rank adaptation (LoRA) to decouple the four information units, enabling more flexible and efficient information fusion with minimal parameter overhead. To address the second issue, AutoIFS introduces an information flow selection network that automatically filters out invalid scenario-task information flows based on model performance feedback. It employs a simple yet effective pruning function to eliminate useless information flows, thereby enhancing the impact of key relationships and improving model performance. Finally, we evaluate AutoIFS and confirm its effectiveness through extensive experiments on two public benchmark datasets and an online A/B test."}
