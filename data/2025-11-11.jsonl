{"id": "2511.05700", "categories": ["cs.GT", "cs.CC", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.05700", "abs": "https://arxiv.org/abs/2511.05700", "authors": ["Christoph Grüne", "Dorothee Henke", "Eva Rotenberg", "Lasse Wulf"], "title": "The Complexity of Stackelberg Pricing Games", "comment": null, "summary": "We consider Stackelberg pricing games, which are also known as bilevel pricing problems, or combinatorial price-setting problems. This family of problems consists of games between two players: the leader and the follower. There is a market that is partitioned into two parts: the part of the leader and the part of the leader's competitors. The leader controls one part of the market and can freely set the prices for products. By contrast, the prices of the competitors' products are fixed and known in advance. The follower, then, needs to solve a combinatorial optimization problem in order to satisfy their own demands, while comparing the leader's offers to the offers of the competitors. Therefore, the leader has to hit the intricate balance of making an attractive offer to the follower, while at the same time ensuring that their own profit is maximized.\n  Pferschy, Nicosia, Pacifici, and Schauer considered the Stackelberg pricing game where the follower solves a knapsack problem. They raised the question whether this problem is complete for the second level of the polynomial hierarchy, i.e., $Σ^p_2$-complete. The same conjecture was also made by Böhnlein, Schaudt, and Schauer. In this paper, we positively settle this conjecture. Moreover, we show that this result holds actually in a much broader context: The Stackelberg pricing game is $Σ^p_2$-complete for over 50 NP-complete problems, including most classics such as TSP, vertex cover, clique, subset sum, etc. This result falls in line of recent meta-theorems about higher complexity in the polynomial hierarchy by Grüne and Wulf."}
{"id": "2511.05895", "categories": ["cs.DS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05895", "abs": "https://arxiv.org/abs/2511.05895", "authors": ["Shruthi Kannappan", "Ashwina Kumar", "Rupesh Nasre"], "title": "Efficient Dynamic MaxFlow Computation on GPUs", "comment": null, "summary": "Maxflow is a fundamental problem in graph theory and combinatorial optimisation, used to determine the maximum flow from a source node to a sink node in a flow network. It finds applications in diverse domains, including computer networks, transportation, and image segmentation. The core idea is to maximise the total flow across the network without violating capacity constraints on edges and ensuring flow conservation at intermediate nodes. The rapid growth of unstructured and semi-structured data has motivated the development of parallel solutions to compute MaxFlow. However, due to the higher computational complexity, computing Maxflow for real-world graphs is time-consuming in practice. In addition, these graphs are dynamic and constantly evolve over time. In this work, we propose two Push-Relabel based algorithms for processing dynamic graphs on GPUs. The key novelty of our algorithms is their ability to efficiently handle both increments and decrements in edge capacities together when they appear in a batch. We illustrate the efficacy of our algorithms with a suite of real-world graphs. Overall, we find that for small updates, dynamic recomputation is significantly faster than a static GPU-based Maxflow."}
{"id": "2511.05493", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.05493", "abs": "https://arxiv.org/abs/2511.05493", "authors": ["Hao Wang"], "title": "GreyShot: Zeroshot and Privacy-preserving Recommender System by GM(1,1) Model", "comment": null, "summary": "Every recommendation engineer needs to face the cold start problem when building his system. During the past decades, most scientists adopted transfer learning and meta learning to solve the problem. Although notable exceptions such as ZeroMat etc. have been invented in recent years, cold-start problem remains a challenging problem for many researchers. In this paper, we build a zeroshot and privacy-preserving recommender system algorithm GreyShot using GM(1,1) model by taking advantage of the Poisson-Pareto property of the online rating data. Our approach relies on no input data and is effective in generating both accurate and fair results. In conclusion, zeroshot problem of recommender systems could be effectively solved by grey system methods such as GM(1,1)."}
{"id": "2511.05860", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.05860", "abs": "https://arxiv.org/abs/2511.05860", "authors": ["Chi-Jui Sung", "Fan-Hao Lin", "Tzu-Hao Huang", "Chu-Hsiang Huang", "Hui Chen", "Chao-Kai Wen", "Henk Wymeersch"], "title": "CommUNext: Deep Learning-Based Cross-Band and Multi-Directional Signal Prediction", "comment": "pages, 11 figures, 6 tables. This work has been submitted to the IEEE for possible publication", "summary": "Sixth-generation (6G) networks are envisioned to achieve full-band cognition by jointly utilizing spectrum resources from Frequency Range~1 (FR1) to Frequency Range~3 (FR3, 7--24\\,GHz). Realizing this vision faces two challenges. First, physics-based ray tracing (RT), the standard tool for network planning and coverage modeling, becomes computationally prohibitive for multi-band and multi-directional analysis over large areas. Second, current 5G systems rely on inter-frequency measurement gaps for carrier aggregation and beam management, which reduce throughput, increase latency, and scale poorly as bands and beams proliferate. These limitations motivate a data-driven approach to infer high-frequency characteristics from low-frequency observations. This work proposes CommUNext, a unified deep learning framework for cross-band, multi-directional signal strength (SS) prediction. The framework leverages low-frequency coverage data and crowd-aided partial measurements at the target band to generate high-fidelity FR3 predictions. Two complementary architectures are introduced: Full CommUNext, which substitutes costly RT simulations for large-scale offline modeling, and Partial CommUNext, which reconstructs incomplete low-frequency maps to mitigate measurement gaps in real-time operation. Experimental results show that CommUNext delivers accurate and robust high-frequency SS prediction even with sparse supervision, substantially reducing both simulation and measurement overhead."}
{"id": "2511.06162", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06162", "abs": "https://arxiv.org/abs/2511.06162", "authors": ["Meike Neuwohner", "Olha Silina", "Michael Zlatin"], "title": "A Better-Than-2 Approximation for the Directed Tree Augmentation Problem", "comment": "To appear in SODA 2026", "summary": "We introduce and study a directed analogue of the weighted Tree Augmentation Problem (WTAP). In the weighted Directed Tree Augmentation Problem (WDTAP), we are given an oriented tree $T = (V,A)$ and a set of directed links $L \\subseteq V \\times V$ with positive costs. The goal is to select a minimum cost set of links which enters each fundamental dicut of $T$ (cuts with one leaving and no entering tree arc). WDTAP captures the problem of covering a cross-free set family with directed links. It can also be used to solve weighted multi $2$-TAP, in which we must cover the edges of an undirected tree at least twice. WDTAP can be approximated to within a factor of $2$ using standard techniques. We provide an improved $(1.75+ \\varepsilon)$-approximation algorithm for WDTAP in the case where the links have bounded costs, a setting that has received significant attention for WTAP. To obtain this result, we discover a class of instances, called \"willows'', for which the natural set covering LP is an integral formulation. We further introduce the notion of \"visibly $k$-wide'' instances which can be solved exactly using dynamic programming. Finally, we show how to leverage these tractable cases to obtain an improved approximation ratio via an elaborate structural analysis of the tree."}
{"id": "2511.06170", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.06170", "abs": "https://arxiv.org/abs/2511.06170", "authors": ["Shivam Nadimpalli", "Mingda Qiao", "Ronitt Rubinfeld"], "title": "No Price Tags? No Problem: Query Strategies for Unpriced Information", "comment": "43 pages", "summary": "The classic *priced query model*, introduced by Charikar et al. (STOC 2000), captures the task of computing a known function on an unknown input when each input variable can only be revealed by paying an associated cost. The goal is to design a query strategy that determines the function's value while minimizing the total cost incurred. However, all prior work in this model assumes complete advance knowledge of the query costs -- an assumption that fails in many realistic settings.\n  We introduce a variant of the priced query model that explicitly handles *unknown* variable costs. We prove a separation from the traditional priced query model, showing that uncertainty in variable costs imposes an unavoidable overhead for every query strategy. Despite this, we design strategies that essentially match our lower bound and are competitive with the best cost-aware strategies for arbitrary Boolean functions. Our results build on a recent connection between priced query strategies and the analysis of Boolean functions, and draw techniques from online algorithms."}
{"id": "2511.05495", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05495", "abs": "https://arxiv.org/abs/2511.05495", "authors": ["Tejas Pawar", "Sarika Patil", "Om Tilekar", "Rushikesh Janwade", "Vaibhav Helambe"], "title": "IMDMR: An Intelligent Multi-Dimensional Memory Retrieval System for Enhanced Conversational AI", "comment": "28 pages, 8 figures, submitted to arXiv for open access publication", "summary": "Conversational AI systems often struggle with maintaining coherent, contextual memory across extended interactions, limiting their ability to provide personalized and contextually relevant responses. This paper presents IMDMR (Intelligent Multi-Dimensional Memory Retrieval), a novel system that addresses these limitations through a multi-dimensional search architecture. Unlike existing memory systems that rely on single-dimensional approaches, IMDMR leverages six distinct memory dimensions-semantic, entity, category, intent, context, and temporal-to provide comprehensive memory retrieval capabilities. Our system incorporates intelligent query processing with dynamic strategy selection, cross-memory entity resolution, and advanced memory integration techniques. Through comprehensive evaluation against five baseline systems including LangChain RAG, LlamaIndex, MemGPT, and spaCy + RAG, IMDMR achieves a 3.8x improvement in overall performance (0.792 vs 0.207 for the best baseline). We present both simulated (0.314) and production (0.792) implementations, demonstrating the importance of real technology integration while maintaining superiority over all baseline systems. Ablation studies demonstrate the effectiveness of multi-dimensional search, with the full system outperforming individual dimension approaches by 23.3%. Query-type analysis reveals superior performance across all categories, particularly for preferences/interests (0.630) and goals/aspirations (0.630) queries. Comprehensive visualizations and statistical analysis confirm the significance of these improvements with p < 0.001 across all metrics. The results establish IMDMR as a significant advancement in conversational AI memory systems, providing a robust foundation for enhanced user interactions and personalized experiences."}
{"id": "2511.06934", "categories": ["cs.GT", "cs.MA", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.06934", "abs": "https://arxiv.org/abs/2511.06934", "authors": ["Dennis Thumm"], "title": "Sequential Causal Normal Form Games: Theory, Computation, and Strategic Signaling", "comment": "AAAI 2026 Workshop on Foundations of Agentic Systems Theory", "summary": "Can classical game-theoretic frameworks be extended to capture the bounded rationality and causal reasoning of AI agents? We investigate this question by extending Causal Normal Form Games (CNFGs) to sequential settings, introducing Sequential Causal Multi-Agent Systems (S-CMAS) that incorporate Pearl's Causal Hierarchy across leader-follower interactions. While theoretically elegant -- we prove PSPACE-completeness, develop equilibrium refinements, and establish connections to signaling theory -- our comprehensive empirical investigation reveals a critical limitation: S-CNE provides zero welfare improvement over classical Stackelberg equilibrium across all tested scenarios. Through 50+ Monte Carlo simulations and hand-crafted synthetic examples, we demonstrate that backward induction with rational best-response eliminates any strategic advantage from causal layer distinctions. We construct a theoretical example illustrating conditions where benefits could emerge ($ε$-rational satisficing followers), though implementation confirms that even relaxed rationality assumptions prove insufficient when good instincts align with optimal play. This negative result provides valuable insight: classical game-theoretic extensions grounded in rational choice are fundamentally incompatible with causal reasoning advantages, motivating new theoretical frameworks beyond standard Nash equilibrium for agentic AI."}
{"id": "2511.05947", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.05947", "abs": "https://arxiv.org/abs/2511.05947", "authors": ["Huimin Hu", "Ruihong Jiang", "Yanqing Xu", "Jiarui Ma", "Fang Fang"], "title": "Average AoI in Pinching Antenna-assisted WPCNs with Probabilistic LoS Blockage", "comment": null, "summary": "This paper analyzes the age of information (AoI) for a pinching antenna (PA)-assisted wireless powered communication network (WPCN) with probabilistic line-of-sight (LoS) blockage. AoI is a key metric for evaluating the freshness of status updates in IoT networks, and its optimization is crucial for ensuring the performance of time-critical applications. To facilitate analysis and gain useful insights, we consider a representative scenario, where an IoT device harvests energy from a base station (BS) equipped with a PA and transmits data packets to it. The IoT device harvests energy via the PA until its capacitor is fully charged, then transmits status updates using all stored energy. We derive closed-form expressions for the average AoI by analyzing the capacitor charging time, transmission success probability, and inter-arrival time of successful updates. To minimize the average AoI, we formulate an optimization problem of PA position, and propose a one-dimensional search to solve it. The simulation results show that the optimal PA position is the one closest to the IoT device, and this conclusion can be extended to the multi-IoT devices frequency division multiple access (FDMA) scenario. The PA-based systems significantly outperform the conventional fixed-antenna systems."}
{"id": "2511.06263", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06263", "abs": "https://arxiv.org/abs/2511.06263", "authors": ["Michael Elkin", "Idan Shabat"], "title": "Spanning and Metric Tree Covers Parameterized by Treewidth", "comment": "43 pages, 1 figure", "summary": "Given a graph $G=(V,E)$, a tree cover is a collection of trees $\\mathcal{T}=\\{T_1,T_2,...,T_q\\}$, such that for every pair of vertices $u,v\\in V$ there is a tree $T\\in\\mathcal{T}$ that contains a $u-v$ path with a small stretch. If the trees $T_i$ are sub-graphs of $G$, the tree cover is called a spanning tree cover. If these trees are HSTs, it is called an HST cover.\n  In a seminal work, Mendel and Naor [2006] showed that for any parameter $k=1,2,...$, there exists an HST cover, and a non-spanning tree cover, with stretch $O(k)$ and with $O(kn^{\\frac{1}{k}})$ trees. Abraham et al. [2020] devised a spanning version of this result, albeit with stretch $O(k\\log\\log n)$. For graphs of small treewidth $t$, Gupta et al. [2004] devised an exact spanning tree cover with $O(t\\log n)$ trees, and Chang et al. [2-23] devised a $(1+ε)$-approximate non-spanning tree cover with $2^{(t/ε)^{O(t)}}$ trees.\n  We prove a smooth tradeoff between the stretch and the number of trees for graphs with balanced recursive separators of size at most $s(n)$ or treewidth at most $t(n)$. Specifically, for any $k=1,2,...$, we provide tree covers and HST covers with stretch $O(k)$ and $O\\left(\\frac{k^2\\log n}{\\log s(n)}\\cdot s(n)^{\\frac{1}{k}}\\right)$ trees or $O(k\\log n\\cdot t(n)^{\\frac{1}{k}})$ trees, respectively. We also devise spanning tree covers with these parameters and stretch $O(k\\log\\log n)$. In addition devise a spanning tree cover for general graphs with stretch $O(k\\log\\log n)$ and average overlap $O(n^{\\frac{1}{k}})$.\n  We use our tree covers to provide improved path-reporting spanners, emulators (including low-hop emulators, known also as low-hop metric spanners), distance labeling schemes and routing schemes."}
{"id": "2511.06455", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06455", "abs": "https://arxiv.org/abs/2511.06455", "authors": ["Milena Trajanoska", "Riste Stojanov", "Dimitar Trajanov"], "title": "A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs", "comment": "The 1st GOBLIN Workshop on Knowledge Graph Technologies https://www.dbpedia.org/events/goblin25-workshop/", "summary": "Enterprises often maintain multiple databases for storing critical business data in siloed systems, resulting in inefficiencies and challenges with data interoperability. A key to overcoming these challenges lies in integrating disparate data sources, enabling businesses to unlock the full potential of their data. Our work presents a novel approach for integrating multiple databases using knowledge graphs, focusing on the application of large language models as semantic agents for mapping and connecting structured data across systems by leveraging existing vocabularies. The proposed methodology introduces a semantic layer above tables in relational databases, utilizing a system comprising multiple LLM agents that map tables and columns to Schema.org terms. Our approach achieves a mapping accuracy of over 90% in multiple domains."}
{"id": "2511.05496", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05496", "abs": "https://arxiv.org/abs/2511.05496", "authors": ["Hao Zhang", "Qinghua Lu", "Liming Zhu"], "title": "DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows", "comment": null, "summary": "Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are \"good enough\" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation."}
{"id": "2511.07022", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.07022", "abs": "https://arxiv.org/abs/2511.07022", "authors": ["Hadi Hosseini", "Sanjukta Roy", "Aditi Sethia"], "title": "Fair Societies: Algorithms for House Allocations", "comment": "28 Pages, 11 Figures", "summary": "House Allocations concern with matchings involving one-sided preferences, where houses serve as a proxy encoding valuable indivisible resources (e.g. organs, course seats, subsidized public housing units) to be allocated among the agents. Every agent must receive exactly one resource. We study algorithmic approaches towards ensuring fairness in such settings. Minimizing the number of envious agents is known to be NP-complete (Kamiyama et al. 2021). We present two tractable approaches to deal with the computational hardness. When the agents are presented with an initial allocation of houses, we aim to refine this allocation by reallocating a bounded number of houses to reduce the number of envious agents. We show an efficient algorithm when the agents express preference for a bounded number of houses. Next, we consider single peaked preference domain and present a polynomial time algorithm for finding an allocation that minimize the number of envious agents. We further extend it to satisfy Pareto efficiency. Our former algorithm works for other measures of envy such as total envy, or maximum envy, with suitable modifications. Finally, we present an empirical analysis recording the fairness-welfare trade-off of our algorithms."}
{"id": "2511.06003", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.06003", "abs": "https://arxiv.org/abs/2511.06003", "authors": ["Atsushi Miki", "Toshiyasu Matsushima"], "title": "Necessary and Sufficient Conditions for Capacity-Achieving Private Information Retrieval with Adversarial Servers", "comment": "17 pages", "summary": "Private information retrieval (PIR) is a mechanism for efficiently downloading messages while keeping the index of the desired message secret from the servers. PIR schemes have been extended to various scenarios with adversarial servers: PIR schemes where some servers are unresponsive or return noisy responses are called robust PIR and Byzantine PIR, respectively; PIR schemes where some servers collude to reveal the index are called colluding PIR. The information-theoretic upper bound on the download efficiency of these PIR schemes has been proved in previous studies. However, systematic ways to construct PIR schemes that achieve the upper bound are not known. In order to construct a capacity-achieving PIR schemes systematically, it is necessary to clarify the conditions that the queries should satisfy. This paper proves the necessary and sufficient conditions for capacity-achieving PIR schemes."}
{"id": "2511.06473", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06473", "abs": "https://arxiv.org/abs/2511.06473", "authors": ["Janosch Fuchs", "Rin Saito", "Tatsuhiro Suga", "Takahiro Suzuki", "Yuma Tamura"], "title": "Coloring Reconfiguration under Color Swapping", "comment": "30 pages, 10 figures, ISAAC 2025", "summary": "In the \\textsc{Coloring Reconfiguration} problem, we are given two proper $k$-colorings of a graph and asked to decide whether one can be transformed into the other by repeatedly applying a specified recoloring rule, while maintaining a proper coloring throughout. For this problem, two recoloring rules have been widely studied: \\emph{single-vertex recoloring} and \\emph{Kempe chain recoloring}. In this paper, we introduce a new rule, called \\emph{color swapping}, where two adjacent vertices may exchange their colors, so that the resulting coloring remains proper, and study the computational complexity of the problem under this rule. We first establish a complexity dichotomy with respect to $k$: the problem is solvable in polynomial time for $k \\leq 2$, and is PSPACE-complete for $k \\geq 3$. We further show that the problem remains PSPACE-complete even on restricted graph classes, including bipartite graphs, split graphs, and planar graphs of bounded degree. In contrast, we present polynomial-time algorithms for several graph classes: for paths when $k = 3$, for split graphs when $k$ is fixed, and for cographs when $k$ is arbitrary."}
{"id": "2511.06780", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06780", "abs": "https://arxiv.org/abs/2511.06780", "authors": ["Songhui Yue", "Yang Shao", "Sean Hayes"], "title": "OntoTune: Ontology-Driven Learning for Query Optimization with Convolutional Models", "comment": null, "summary": "Query optimization has been studied using machine learning, reinforcement learning, and, more recently, graph-based convolutional networks. Ontology, as a structured, information-rich knowledge representation, can provide context, particularly in learning problems. This paper presents OntoTune, an ontology-based platform for enhancing learning for query optimization. By connecting SQL queries, database metadata, and statistics, the ontology developed in this research is promising in capturing relationships and important determinants of query performance. This research also develops a method to embed ontologies while preserving as much of the relationships and key information as possible, before feeding it into learning algorithms such as tree-based and graph-based convolutional networks. A case study shows how OntoTune's ontology-driven learning delivers performance gains compared with database system default query execution."}
{"id": "2511.05497", "categories": ["cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.05497", "abs": "https://arxiv.org/abs/2511.05497", "authors": ["Kajwan Ziaoddini"], "title": "Socially Aware Music Recommendation: A Multi-Modal Graph Neural Networks for Collaborative Music Consumption and Community-Based Engagement", "comment": null, "summary": "This study presents a novel Multi-Modal Graph Neural Network (MM-GNN) framework for socially aware music recommendation, designed to enhance personalization and foster community-based engagement. The proposed model introduces a fusion-free deep mutual learning strategy that aligns modality-specific representations from lyrics, audio, and visual data while maintaining robustness against missing modalities. A heterogeneous graph structure is constructed to capture both user-song interactions and user-user social relationships, enabling the integration of individual preferences with social influence. Furthermore, emotion-aware embeddings derived from acoustic and textual signals contribute to emotionally aligned recommendations. Experimental evaluations on benchmark datasets demonstrate that MM-GNN significantly outperforms existing state-of-the-art methods across various performance metrics. Ablation studies further validate the critical impact of each model component, confirming the effectiveness of the framework in delivering accurate and socially contextualized music recommendations."}
{"id": "2511.07395", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.07395", "abs": "https://arxiv.org/abs/2511.07395", "authors": ["Hadi Hosseini", "Vishwa Prakash HV", "Aditi Sethia", "Jatin Yadav"], "title": "The Landscape of Almost Equitable Allocations", "comment": null, "summary": "Equitability is a fundamental notion in fair division which requires that all agents derive equal value from their allocated bundles. We study, for general (possibly non-monotone) valuations, a popular relaxation of equitability known as equitability up to one item (EQ1). An EQ1 allocation may fail to exist even with additive non-monotone valuations; for instance, when there are two agents, one valuing every item positively and the other negatively. This motivates a mild and natural assumption: all agents agree on the sign of their value for the grand bundle. Under this assumption, we prove the existence and provide an efficient algorithm for computing EQ1 allocations for two agents with general valuations. When there are more than two agents, we show the existence and polynomial-time computability of EQ1 allocations for valuation classes beyond additivity and monotonicity, in particular for (1) doubly monotone valuations and (2) submodular (resp. supermodular) valuations where the value for the grand bundle is nonnegative (resp. nonpositive) for all agents. Furthermore, we settle an open question of Bil`o et al. by showing that an EQ1 allocation always exists for nonnegative(resp. nonpositive) valuations, i.e., when every agent values each subset of items nonnegatively (resp. nonpositively). Finally, we complete the picture by showing that for general valuations with more than two agents, EQ1 allocations may not exist even when agents agree on the sign of the grand bundle, and that deciding the existence of an EQ1 allocation is computationally intractable."}
{"id": "2511.06089", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.06089", "abs": "https://arxiv.org/abs/2511.06089", "authors": ["M. S. S. Manasa", "Praful D. Mankar", "Sundaram Vanka"], "title": "Capacity Analysis of Cascaded BD-RIS Assisted MIMO Systems", "comment": null, "summary": "This paper examines the cascaded deployment of beyond diagonal (BD) reconfigurable intelligent surfaces (RISs) and explores its potential to enhance the performance of MIMO systems. We first derive the jointly optimal closed form solutions for the RISs in cascade with SVD water filling (SVD WF) and uniform power allocation (UPA) precoding strategies. The optimally configured cascaded-RIS with UPA is shown to achieve performance comparable to that with the SVD WF approach, suggesting that cascaded-RISs can also aid in reducing transmitter complexity. Furthermore, the approximate ergodic capacity for UPA is derived, along with its high SNR approximation which provides multiple useful insights into the dimension and deployment of cascaded RISs. The analytical results establish a clear tradeoff among transmit power, RIS size, and achievable capacity, providing insights for practical deployment in high SNR cascaded RIS MIMO systems."}
{"id": "2511.06486", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.06486", "abs": "https://arxiv.org/abs/2511.06486", "authors": ["Andrei Arhire", "Matei Chiriac", "Radu Timofte"], "title": "UAIC_Twin_Width: An Exact yet Efficient Twin-Width Algorithm", "comment": "Best Paper Award at Romanian Collegiate Algorithms Workshop 2025", "summary": "Twin-width is a recently formulated graph and matrix invariant that intuitively quantifies how far a graph is from having the structural simplicity of a co-graph. Since its introduction in 2020, twin-width has received increasing attention and has driven research leading to notable advances in algorithmic fields, including graph theory and combinatorics. The 2023 edition of the Parameterized Algorithms and Computational Experiments (PACE) Challenge aimed to fulfill the need for a diverse and consistent public benchmark encompassing various graph structures, while also collecting state-of-the-art heuristic and exact approaches to the problem. In this paper, we propose two algorithms for efficiently computing the twin-width of graphs with arbitrary structures, comprising one exact and one heuristic approach. The proposed solutions performed strongly in the competition, with the exact algorithm achieving the best student result and ranking fourth overall. We release our source code publicly to enable practical applications of our work and support further research."}
{"id": "2511.07139", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07139", "abs": "https://arxiv.org/abs/2511.07139", "authors": ["Jin Cheng", "Xiangxiang Dai", "Ningning Ding", "John C. S. Lui", "Jianwei Huang"], "title": "Trading Vector Data in Vector Databases", "comment": "Accepted by ICDE 2026", "summary": "Vector data trading is essential for cross-domain learning with vector databases, yet it remains largely unexplored. We study this problem under online learning, where sellers face uncertain retrieval costs and buyers provide stochastic feedback to posted prices. Three main challenges arise: (1) heterogeneous and partial feedback in configuration learning, (2) variable and complex feedback in pricing learning, and (3) inherent coupling between configuration and pricing decisions.\n  We propose a hierarchical bandit framework that jointly optimizes retrieval configurations and pricing. Stage I employs contextual clustering with confidence-based exploration to learn effective configurations with logarithmic regret. Stage II adopts interval-based price selection with local Taylor approximation to estimate buyer responses and achieve sublinear regret. We establish theoretical guarantees with polynomial time complexity and validate the framework on four real-world datasets, demonstrating consistent improvements in cumulative reward and regret reduction compared with existing methods."}
{"id": "2511.05498", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05498", "abs": "https://arxiv.org/abs/2511.05498", "authors": ["Ilya Tyagin", "Saeideh Valipour", "Aliaksandra Sikirzhytskaya", "Michael Shtutman", "Ilya Safro"], "title": "Biomedical Hypothesis Explainability with Graph-Based Context Retrieval", "comment": "30 pages, 10 figures,", "summary": "We introduce an explainability method for biomedical hypothesis generation systems, built on top of the novel Hypothesis Generation Context Retriever framework. Our approach combines semantic graph-based retrieval and relevant data-restrictive training to simulate real-world discovery constraints. Integrated with large language models (LLMs) via retrieval-augmented generation, the system explains hypotheses with contextual evidence using published scientific literature. We also propose a novel feedback loop approach, which iteratively identifies and corrects flawed parts of LLM-generated explanations, refining both the evidence paths and supporting context. We demonstrate the performance of our method with multiple large language models and evaluate the explanation and context retrieval quality through both expert-curated assessment and large-scale automated analysis. Our code is available at: https://github.com/IlyaTyagin/HGCR."}
{"id": "2511.06372", "categories": ["cs.IT", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06372", "abs": "https://arxiv.org/abs/2511.06372", "authors": ["Saeed Razavikia", "Deniz Gündüz", "Carlo Fischione"], "title": "Towards Optimal Constellation Design for Digital Over-the-Air Computation", "comment": null, "summary": "Over-the-air computation (OAC) has emerged as a key technique for efficient function computation over multiple-access channels (MACs) by exploiting the waveform superposition property of the wireless domain. While conventional OAC methods rely on analog amplitude modulation, their performance is often limited by noise sensitivity and hardware constraints, motivating the use of digital modulation schemes. This paper proposes a novel digital modulation framework optimized for computation over additive white Gaussian noise (AWGN) channels. The design is formulated as an additive mapping problem to determine the optimal constellation that minimizes the mean-squared error (MSE) under a transmit power constraint. We express the optimal constellation design as a system of nonlinear equations and establish the conditions guaranteeing the uniqueness of its solution. In the high signal-to-noise-ratio (SNR) regime, we derive closed-form expressions for the optimal modulation parameters using the generalized Lambert function, providing analytical insight into the system's behavior. Furthermore, we discuss extensions of the framework to higher-dimensional grids corresponding to multiple channel uses, to non-Gaussian noise models, and to computation over real-valued domains via hybrid digital-analog modulation."}
{"id": "2511.06504", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06504", "abs": "https://arxiv.org/abs/2511.06504", "authors": ["Mahsa Derakhshan", "Mohammad Roghani", "Mohammad Saneian", "Tao Yu"], "title": "Improved Approximation for Ranking on General Graphs", "comment": null, "summary": "In this paper, we study Ranking, a well-known randomized greedy matching algorithm, for general graphs. The algorithm was originally introduced by Karp, Vazirani, and Vazirani [STOC 1990] for the online bipartite matching problem with one-sided vertex arrivals, where it achieves a tight approximation ratio of 1 - 1/e. It was later extended to general graphs by Goel and Tripathi [FOCS 2012]. The Ranking algorithm for general graphs is as follows: a permutation $σ$ over the vertices is chosen uniformly at random. The vertices are then processed sequentially according to this order, with each vertex being matched to the first available neighbor (if any) according to the same permutation $σ$.\n  While the algorithm is quite well-understood for bipartite graphs-with the approximation ratio lying between 0.696 and 0.727, its approximation ratio for general graphs remains less well characterized despite extensive efforts. Prior to this work, the best known lower bound for general graphs was 0.526 by Chan et al. [TALG 2018], improving on the approximation ratio of 0.523 by Chan et al. [SICOMP 2018]. The upper bound, however, remains the same as that for bipartite graphs.\n  In this work, we improve the approximation ratio of \\textsc{Ranking} for general graphs to 0.5469, up from 0.526. This also surpasses the best-known approximation ratio of $0.531$ by Tang et al. [JACM 2023] for the oblivious matching problem. Our approach builds on the standard primal-dual analysis. The novelty of our work lies in proving new structural properties of Ranking by introducing the notion of the backup for vertices matched by the algorithm. For a fixed permutation, a vertex's backup is its potential match if its current match is removed. This concept helps characterize the rank distribution of the match of each vertex, enabling us to eliminate certain bad events that constrained previous work."}
{"id": "2511.05499", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05499", "abs": "https://arxiv.org/abs/2511.05499", "authors": ["Rafayel Latif", "Satwik Behera", "Ali Al-Ebrahim"], "title": "Weightless Neural Networks for Continuously Trainable Personalized Recommendation Systems", "comment": null, "summary": "Given that conventional recommenders, while deeply effective, rely on large distributed systems pre-trained on aggregate user data, incorporating new data necessitates large training cycles, making them slow to adapt to real-time user feedback and often lacking transparency in recommendation rationale. We explore the performance of smaller personal models trained on per-user data using weightless neural networks (WNNs), an alternative to neural backpropagation that enable continuous learning by using neural networks as a state machine rather than a system with pretrained weights. We contrast our approach against a classic weighted system, also on a per-user level, and standard collaborative filtering, achieving competitive levels of accuracy on a subset of the MovieLens dataset. We close with a discussion of how weightless systems can be developed to augment centralized systems to achieve higher subjective accuracy through recommenders more directly tunable by end-users."}
{"id": "2511.06510", "categories": ["cs.IT", "cs.ET", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.06510", "abs": "https://arxiv.org/abs/2511.06510", "authors": ["Marx M. M. Freitas", "Giovanni Interdonato", "Stefano Buzzi"], "title": "Differential Space-Time Block Coding for Phase-Unsynchronized Cell-Free MIMO Downlink", "comment": "Submitted to IEEE for possible publication. This manuscript builds upon and significantly extends our prior works: https://ieeexplore.ieee.org/document/11027597/ and https://ieeexplore.ieee.org/document/11143305 | © 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "In the downlink of a cell-free massive multiple-input multiple-output (CF-mMIMO) system, spectral efficiency gains critically rely on joint coherent transmission, as all access points (APs) must align their transmitted signals in phase at the user equipment (UE). Achieving such phase alignment is technically challenging, as it requires tight synchronization among geographically distributed APs. In this paper, we address this issue by introducing a differential space-time block coding (DSTBC) approach that bypasses the need for AP phase synchronization. We first provide analytic bounds to the achievable spectral efficiency of CF-mMIMO with phase-unsynchronized APs. Then, we propose a DSTBC-based transmission scheme specifically tailored to CF-mMIMO, which operates without channel state information and does not require any form of phase synchronization among the APs. We derive a closed-form expression for the resulting signal-to-interference-plus-noise ratio (SINR), enabling quantitative comparisons among different DSTBC schemes. Numerical simulations confirm that phase misalignments can significantly impair system performance. In contrast, the proposed DSTBC scheme successfully mitigates these effects, achieving performance comparable to that of fully synchronized systems."}
{"id": "2511.06514", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06514", "abs": "https://arxiv.org/abs/2511.06514", "authors": ["Vamsi Addanki", "Julien Dallot", "Leon Kellerhals", "Maciej Pacut", "Stefan Schmid"], "title": "The Harmonic Policy for Online Buffer Sharing is (2 + ln n)-Competitive: A Simple Proof", "comment": "Published at SOSA 2026", "summary": "The problem of online buffer sharing is expressed as follows. A switch with $n$ output ports receives a stream of incoming packets. When an incoming packet is accepted by the switch, it is stored in a shared buffer of capacity $B$ common to all packets and awaits its transmission through its corresponding output port determined by its destination. Each output port transmits one packet per time unit. The problem is to find an algorithm for the switch to accept or reject a packet upon its arrival in order to maximize the total number of transmitted packets.\n  Building on the work of Kesselman et al. (STOC 2001) on split buffer sharing, Kesselman and Mansour (TCS 2004) considered the problem of online buffer sharing which models most deployed internet switches. In their work, they presented the Harmonic policy and proved that it is $(2 + \\ln n)$-competitive, which is the best known competitive ratio for this problem. The Harmonic policy unfortunately saw less practical relevance as it performs $n$ threshold checks per packets which is deemed costly in practice, especially on network switches processing multiple terabits of packets per second. While the Harmonic policy is elegant, the original proof is also rather complex and involves a lengthy matching routine along with multiple intermediary results.\n  This note presents a simplified Harmonic policy, both in terms of implementation and proof. First, we show that the Harmonic policy can be implemented with a constant number of threshold checks per packet, matching the widely deployed \\emph{Dynamic Threshold} policy. Second, we present a simple proof that shows the Harmonic policy is $(2 + \\ln n)$-competitive. In contrast to the original proof, the current proof is direct and relies on a 3-partitioning of the packets."}
{"id": "2511.05500", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05500", "abs": "https://arxiv.org/abs/2511.05500", "authors": ["Francis Gross"], "title": "Predicting Oscar-Nominated Screenplays with Sentence Embeddings", "comment": null, "summary": "Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research."}
{"id": "2511.06591", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.06591", "abs": "https://arxiv.org/abs/2511.06591", "authors": ["Ittetsu Uchiyama", "Chihiro Tsutake", "Keita Takahashi", "Toshiaki Fujii"], "title": "Events Meet Phase-Shifting Digital Holography: Practical Acquisition, Theory, and Algorithms", "comment": null, "summary": "We introduce a novel phase-shifting digital holography (PSDH) method leveraging a hybrid event-based vision sensor (EVS). The key idea of our method is the phase shift during a single exposure. The hybrid EVS records a hologram blurred by the phase shift, together with the events corresponding to blur variations. We present analytical and optimization-based methods that theoretically support the reconstruction of full-complex wavefronts from the blurred hologram and events. The experimental results demonstrate that our method achieves a reconstruction quality comparable to that of a conventional PSDH method while enhancing the acquisition efficiency."}
{"id": "2511.06564", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06564", "abs": "https://arxiv.org/abs/2511.06564", "authors": ["Michał Szyfelbein"], "title": "Approximating the Average-Case Graph Search Problem with Non-Uniform Costs", "comment": "20 pages, 2 figures", "summary": "Consider the following generalization of the classic binary search problem: A searcher is required to find a hidden target vertex $x$ in a graph $G$. To do so, they iteratively perform queries to an oracle, each about a chosen vertex $v$. After each such call, the oracle responds whether the target was found and if not, the searcher receives as a reply the connected component in $G-v$ which contains $x$. Additionally, each vertex $v$ may have a different query cost $c(v)$ and a different weight $w(v)$. The goal is to find the optimal querying strategy which minimizes the weighted average-case cost required to find $x$. The problem is NP-hard even for uniform weights and query costs. Inspired by the progress on the edge query variant of the problem [SODA '17], we establish a connection between searching and vertex separation. By doing so, we provide an $O(\\sqrt{\\log n})$-approximation algorithm for general graphs and a $(4+ε)$-approximation algorithm for the case when the input is a tree."}
{"id": "2511.05667", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.05667", "abs": "https://arxiv.org/abs/2511.05667", "authors": ["Nivedita Sinha", "Bharati Khanijo", "Sanskar Singh", "Priyansh Mahant", "Ashutosh Roy", "Saubhagya Singh Bhadouria", "Arpan Jain", "Maya Ramanath"], "title": "SARCH: Multimodal Search for Archaeological Archives", "comment": null, "summary": "In this paper, we describe a multi-modal search system designed to search old archaeological books and reports. This corpus is digitally available as scanned PDFs, but varies widely in the quality of scans. Our pipeline, designed for multi-modal archaeological documents, extracts and indexes text, images (classified into maps, photos, layouts, and others), and tables. We evaluated different retrieval strategies, including keyword-based search, embedding-based models, and a hybrid approach that selects optimal results from both modalities. We report and analyze our preliminary results and discuss future work in this exciting vertical."}
{"id": "2511.06795", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.06795", "abs": "https://arxiv.org/abs/2511.06795", "authors": ["Neil D. Lawrence"], "title": "The Inaccessible Game", "comment": null, "summary": "In this paper we introduce the inaccessible game, an information-theoretic dynamical system constructed from four axioms. The first three axioms are known and define \\emph{information loss} in the system. The fourth is a novel \\emph{information isolation} axiom that assumes our system is isolated from observation, making it observer-independent and exchangeable. Under this isolation axiom, total marginal entropy is conserved: $\\sum_i h_i = C$. We consider maximum entropy production in the game and show that the dynamics exhibit a GENERIC-like structure combining reversible and irreversible components."}
{"id": "2511.06574", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06574", "abs": "https://arxiv.org/abs/2511.06574", "authors": ["Daniel Agassy", "Dani Dorfman", "Haim Kaplan"], "title": "Improved Tree Sparsifiers in Near-Linear Time", "comment": null, "summary": "A \\emph{tree cut-sparsifier} $T$ of quality $α$ of a graph $G$ is a single tree that preserves the capacities of all cuts in the graph up to a factor of $α$. A \\emph{tree flow-sparsifier} $T$ of quality $α$ guarantees that every demand that can be routed in $T$ can also be routed in $G$ with congestion at most $α$.\n  We present a near-linear time algorithm that, for any undirected capacitated graph $G=(V,E,c)$, constructs a tree cut-sparsifier $T$ of quality $O(\\log^{2} n \\log\\log n)$, where $n=|V|$. This nearly matches the quality of the best known polynomial construction of a tree cut-sparsifier, of quality $O(\\log^{1.5} n \\log\\log n)$ [Räcke and Shah, ESA~2014]. By the flow-cut gap, our result yields a tree flow-sparsifier (and congestion-approximator) of quality $O(\\log^{3} n \\log\\log n)$. This improves on the celebrated result of [Räcke, Shah, and Täubig, SODA~2014] (RST) that gave a near-linear time construction of a tree flow-sparsifier of quality $O(\\log^{4} n)$.\n  Our algorithm builds on a recent \\emph{expander decomposition} algorithm by [Agassy, Dorfman, and Kaplan, ICALP~2023], which we use as a black box to obtain a clean and modular foundation for tree cut-sparsifiers. This yields an improved and simplified version of the RST construction for cut-sparsifiers with quality $O(\\log^{3} n)$. We then introduce a near-linear time \\emph{refinement phase} that controls the load accumulated on boundary edges of the sub-clusters across the levels of the tree. Combining the improved framework with this refinement phase leads to our final $O(\\log^{2} n \\log\\log n)$ tree cut-sparsifier."}
{"id": "2511.05684", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05684", "abs": "https://arxiv.org/abs/2511.05684", "authors": ["Dhananjay Ashok", "Suraj Nair", "Mutasem Al-Darabsah", "Choon Hui Teo", "Tarun Agarwal", "Jonathan May"], "title": "A Representation Sharpening Framework for Zero Shot Dense Retrieval", "comment": "15 pages, 4 figures", "summary": "Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost."}
{"id": "2511.06843", "categories": ["cs.IT", "math.AC", "math.AG"], "pdf": "https://arxiv.org/pdf/2511.06843", "abs": "https://arxiv.org/abs/2511.06843", "authors": ["Martin Kreuzer"], "title": "Code Equivalence, Point Set Equivalence, and Polynomial Isomorphism", "comment": "25 pages", "summary": "The linear code equivalence (LCE) problem is shown to be equivalent to the point set equivalence (PSE) problem, i.e., the problem to check whether two sets of points in a projective space over a finite field differ by a linear change of coordinates. For such a point set $\\mathbb{X}$, let $R$ be its homogeneous coordinate ring and $\\mathfrak{J}_{\\mathbb{X}}$ its canonical ideal. Then the LCE problem is shown to be equivalent to an algebra isomorphism problem for the doubling $R/\\mathfrak{J}_{\\mathbb{X}}$. As this doubling is an Artinian Gorenstein algebra, we can use its Macaulay inverse system to reduce the LCE problem to a Polynomial Isomorphism (PI) problem for homogeneous polynomials. The last step is polynomial time under some mild assumptions about the codes. Moreover, for indecomposable iso-dual codes we can reduce the LCE search problem to the PI search problem of degree 3 by noting that the corresponding point sets are self-associated and arithmetically Gorenstein, so that we can use the isomorphism problem for the Artinian reductions of the coordinate rings and form their Macaulay inverse systems."}
{"id": "2511.06581", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.06581", "abs": "https://arxiv.org/abs/2511.06581", "authors": ["Christoph Grunau", "Rasmus Kyng", "Goran Zuzic"], "title": "Acceleration for Distributed Transshipment and Parallel Maximum Flow", "comment": null, "summary": "We combine several recent advancements to solve $(1+\\varepsilon)$-transshipment and $(1+\\varepsilon)$-maximum flow with a parallel algorithm with $\\tilde{O}(1/\\varepsilon)$ depth and $\\tilde{O}(m/\\varepsilon)$ work. We achieve this by developing and deploying suitable parallel linear cost approximators in conjunction with an accelerated continuous optimization framework known as the box-simplex game by Jambulapati et al. (ICALP 2022). A linear cost approximator is a linear operator that allows us to efficiently estimate the cost of the optimal solution to a given routing problem. Obtaining accelerated $\\varepsilon$ dependencies for both problems requires developing a stronger multicommodity cost approximator, one where cancellations between different commodities are disallowed. For maximum flow, we observe that a recent linear cost approximator due to Agarwal et al. (SODA 2024) can be augmented with additional parallel operations and achieve $\\varepsilon^{-1}$ dependency via the box-simplex game.\n  For transshipment, we also obtain construct a deterministic and distributed approximator. This yields a deterministic CONGEST algorithm that requires $\\tilde{O}(\\varepsilon^{-1}(D + \\sqrt{n}))$ rounds on general networks of hop diameter $D$ and $\\tilde{O}(\\varepsilon^{-1}D)$ rounds on minor-free networks to compute a $(1+\\varepsilon)$-approximation."}
{"id": "2511.05808", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.05808", "abs": "https://arxiv.org/abs/2511.05808", "authors": ["Cheng Li", "Yong Xu", "Suhua Tang", "Wenqiang Lin", "Xin He", "Jinde Cao"], "title": "User Hesitation and Negative Transfer in Multi-Behavior Recommendation", "comment": null, "summary": "Multi-behavior recommendation aims to integrate users' interactions across various behavior types (e.g., view, favorite, add-to-cart, purchase) to more comprehensively characterize user preferences. However, existing methods lack in-depth modeling when dealing with interactions that generate only auxiliary behaviors without triggering the target behavior. In fact, these weak signals contain rich latent information and can be categorized into two types: (1) positive weak signals-items that have not triggered the target behavior but exhibit frequent auxiliary interactions, reflecting users' hesitation tendencies toward these items; and (2) negative weak signals-auxiliary behaviors that result from misoperations or interaction noise, which deviate from true preferences and may cause negative transfer effects. To more effectively identify and utilize these weak signals, we propose a recommendation framework focused on weak signal learning, termed HNT. Specifically, HNT models weak signal features from two dimensions: positive and negative effects. By learning the characteristics of auxiliary behaviors that lead to target behaviors, HNT identifies similar auxiliary behaviors that did not trigger the target behavior and constructs a hesitation set of related items as weak positive samples to enhance preference modeling, thereby capturing users' latent hesitation intentions. Meanwhile, during auxiliary feature fusion, HNT incorporates latent negative transfer effect modeling to distinguish and suppress interference caused by negative representations through item similarity learning. Experiments on three real-world datasets demonstrate that HNT improves HR@10 and NDCG@10 by 12.57% and 14.37%, respectively, compared to the best baseline methods."}
{"id": "2511.06882", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.06882", "abs": "https://arxiv.org/abs/2511.06882", "authors": ["Zhipeng Li", "Wenjie Ma"], "title": "Rate-Optimal Streaming Codes Under an Extended Delay Profile for Three-Node Relay Networks With Burst Erasures", "comment": null, "summary": "This paper investigates streaming codes for three-node relay networks under burst packet erasures with a delay constraint $T$. In any sliding window of $T+1$ consecutive packets, the source-to-relay and relay-to-destination channels may introduce burst erasures of lengths at most $b_1$ and $b_2$, respectively. Let $u = \\max\\{b_1, b_2\\}$ and $v = \\min\\{b_1, b_2\\}$. Singhvi et al. proposed a construction achieving the optimal rate when $u\\mid (T-u-v)$. In this paper, we present an extended delay profile method that attains the optimal rate under a relaxed constraint $\\frac{T - u - v}{2u - v} \\leq \\left\\lfloor \\frac{T - u - v}{u} \\right\\rfloor$ and it strictly cover restriction $u\\mid (T-u-v)$. %Furthermore, we demonstrate that the optimal rate for streaming codes is not achievable when $0< T-u-v<v$ under the convolutional code framework."}
{"id": "2511.07008", "categories": ["cs.DS", "cs.CG", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.07008", "abs": "https://arxiv.org/abs/2511.07008", "authors": ["Stefan Michel"], "title": "Revisiting Chazelle's Implementation of the Bottom-Left Heuristic: A Corrected and Rigorous Analysis", "comment": "48 pages, 34 figures", "summary": "The Strip Packing Problem is a classical optimization problem in which a given set of rectangles must be packed, without overlap, into a strip of fixed width and infinite height, while minimizing the total height of the packing. A straightforward and widely studied approach to this problem is the Bottom-Left Heuristic. It consists of iteratively placing each rectangle in the given order at the lowest feasible position in the strip and, in case of ties, at the leftmost of those. Due to its simplicity and good empirical performance, this heuristic is widely used in practical applications. The most efficient implementation of this heuristic was proposed by Chazelle in 1983, requiring $O(n^2)$ time and $O(n)$ space to place $n$ rectangles. However, although Chazelle's original description was largely correct, it omitted several formal details. Furthermore, our analysis revealed a critical flaw in the original runtime analysis, which, in certain cases, results in $Ω(n^3)$ running time. Motivated by this finding, this paper provides a rigorous and corrected presentation of the implementation, addressing the imprecise arguments and resolving the identified flaw. The resulting analysis establishes a formally verified version of Chazelle's implementation and confirms its quadratic time complexity."}
{"id": "2511.05850", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05850", "abs": "https://arxiv.org/abs/2511.05850", "authors": ["Max McKinnon"], "title": "Retrieval Quality at Context Limit", "comment": "3 pages, 0 figures", "summary": "The ability of large language models (LLMs) to recall and retrieve information from long contexts is critical for many real-world applications. Prior work (Liu et al., 2023) reported that LLMs suffer significant drops in retrieval accuracy for facts placed in the middle of large contexts, an effect known as \"Lost in the Middle\" (LITM). We find the model Gemini 2.5 Flash can answer needle-in-a-haystack questions with great accuracy regardless of document position including when the document is nearly at the input context limit. Our results suggest that the \"Lost in the Middle\" effect is not present for simple factoid Q\\&A in Gemini 2.5 Flash, indicating substantial improvements in long-context retrieval."}
{"id": "2511.06994", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.06994", "abs": "https://arxiv.org/abs/2511.06994", "authors": ["Emil Björnson", "Murat Babek Salman"], "title": "Experimental Validation of Reflective Near-Field Beamfocusing using a b-bit RIS", "comment": "6 pages, 6 figures, Submitted for publication", "summary": "This paper presents the first experimental validation of reflective near-field beamfocusing using a reconfigurable intelligent surface (RIS). While beamfocusing has been theoretically established as a key feature of large-aperture RISs, its practical realization has remained unexplored. We derive new analytical expressions for the array gain achieved with a $b$-bit RIS in near-field line-of-sight scenarios, characterizing both the finite depth and angular width of the focal region. The theoretical results are validated through a series of measurements in an indoor office environment at 28 GHz using a one-bit 1024-element RIS. The experiments confirm that near-field beamfocusing can be dynamically achieved and accurately predicted by the proposed analytical model, despite the presence of hardware imperfections and multipath propagation. These findings demonstrate that near-field beamfocusing is a robust and practically viable feature of RIS-assisted wireless communications."}
{"id": "2511.07160", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.07160", "abs": "https://arxiv.org/abs/2511.07160", "authors": ["Florent Foucaud", "Atrayee Majumder", "Tobias Mömke", "Aida Roshany-Tabrizi"], "title": "Polynomial-time algorithms for PATH COVER and PATH PARTITION on trees and graphs of bounded treewidth", "comment": null, "summary": "In the PATH COVER problem, one asks to cover the vertices of a graph using the smallest possible number of (not necessarily disjoint) paths. While the variant where the paths need to be pairwise vertex-disjoint, which we call PATH PARTITION, is extensively studied, surprisingly little is known about PATH COVER. We start filling this gap by designing a linear-time algorithm for PATH COVER on trees.\n  We show that PATH COVER can be solved in polynomial time on graphs of bounded treewidth using a dynamic programming scheme. It runs in XP time $n^{t^{O(t)}}$ (where $n$ is the number of vertices and $t$ the treewidth of the input graph) or $κ^{t^{O(t)}}n$ if there is an upper-bound $κ$ on the solution size. A similar algorithm gives an FPT $2^{O(t\\log t)}n$ algorithm for PATH PARTITION, which can be improved to (randomized) $2^{O(t)}n$ using the Cut\\&Count technique. These results also apply to the variants where the paths are required to be induced (i.e. chordless) and/or edge-disjoint."}
{"id": "2511.05885", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05885", "abs": "https://arxiv.org/abs/2511.05885", "authors": ["Qiyong Zhong", "Jiajie Su", "Ming Yang", "Yunshan Ma", "Xiaolin Zheng", "Chaochao Chen"], "title": "A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation", "comment": null, "summary": "In this paper, we proposed Speeder, a remarkably efficient paradigm to multimodal large language models for sequential recommendation. Speeder introduces 3 key components: (1) Multimodal Representation Compression (MRC), which efficiently reduces redundancy in item descriptions; (2) Sequential Position Awareness Enhancement (SPAE), which strengthens the model's ability to capture complex sequential dependencies; (3) Modality-aware Progressive Optimization (MPO), which progressively integrates different modalities to improve the model's understanding and reduce cognitive biases. Through extensive experiments, Speeder demonstrates superior performance over baselines in terms of VHR@1 and computational efficiency. Specifically, Speeder achieved 250% of the training speed and 400% of the inference speed compared to the state-of-the-art MLLM-based SR models. Future work could focus on incorporating real-time feedback from real-world systems."}
{"id": "2511.07145", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.07145", "abs": "https://arxiv.org/abs/2511.07145", "authors": ["Xinke Jian", "Zhiyuan Ren", "Wenchi Cheng"], "title": "A Copula-based Semantics-Structure Minimization Framework for QoS Guaranteed Wireless Communications", "comment": null, "summary": "Current empirically driven research on semantic communication lacks a unified theoretical foundation, preventing quantifiable Quality of Service guarantees, particularly for transmitting minimal structural semantics in emergency scenarios. This deficiency limits its evolution into a predictable engineering science. To address this, we establish a complete theoretical axiomatic basis for this problem. We propose four axioms and rigorously prove that the family of pairwise rank-Copulas is the minimal sufficient representation for minimal structural semantics. Based on this, we construct a semantic distortion metric, centered on the Jensen-Shannon divergence. We then establish the core theoretical boundaries of the framework: sample complexity bounds; rate-distortion bounds; an end-to-end Service Level Agreements theorem; and a semantic source-channel separation theorem, which provides a provable Quality of Service guarantee. Finally, we validate our framework through decoupled experiments, empirically demonstrating that our core metric strictly adheres to our foundational axioms while standard perceptual metrics fail to do so."}
{"id": "2511.07244", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07244", "abs": "https://arxiv.org/abs/2511.07244", "authors": ["Gautam Chandrasekaran", "Adam R. Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"], "title": "A Fully Polynomial-Time Algorithm for Robustly Learning Halfspaces over the Hypercube", "comment": "52 pages, 1 figure", "summary": "We give the first fully polynomial-time algorithm for learning halfspaces with respect to the uniform distribution on the hypercube in the presence of contamination, where an adversary may corrupt some fraction of examples and labels arbitrarily. We achieve an error guarantee of $η^{O(1)}+ε$ where $η$ is the noise rate. Such a result was not known even in the agnostic setting, where only labels can be adversarially corrupted. All prior work over the last two decades has a superpolynomial dependence in $1/ε$ or succeeds only with respect to continuous marginals (such as log-concave densities).\n  Previous analyses rely heavily on various structural properties of continuous distributions such as anti-concentration. Our approach avoids these requirements and makes use of a new algorithm for learning Generalized Linear Models (GLMs) with only a polylogarithmic dependence on the activation function's Lipschitz constant. More generally, our framework shows that supervised learning with respect to discrete distributions is not as difficult as previously thought."}
{"id": "2511.05991", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05991", "abs": "https://arxiv.org/abs/2511.05991", "authors": ["Tiago da Cruz", "Bernardo Tavares", "Francisco Belo"], "title": "Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance", "comment": "12 pages, 8 Figures", "summary": "Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches."}
{"id": "2511.07309", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.07309", "abs": "https://arxiv.org/abs/2511.07309", "authors": ["Han Xiao", "Xiaoyan Hu", "Wenjie Wang", "Kai-Kit Wong", "Kun Yang", "Chan-Byoung Chae"], "title": "Frequency Diverse (FD)-RIS-Enhanced Covert Communications: Defense Against Wiretapping via Joint Distance-Angle Beamforming", "comment": null, "summary": "In response to the security blind zone challenges faced by traditional reconfigurable intelligent surface (RIS)-aided covert communication (CC) systems, the joint distance-angle beamforming capability of frequency diverse RIS (FD-RIS) shows significant potential for addressing these limitations. Therefore, this paper initially incorporates the FD-RIS into the CC systems and proposes the corresponding CC transmission scheme. Specifically, we first develop the signal processing model of the FD-RIS, which considers effective control of harmonic signals by leveraging the time-delay techniques. The joint distance-angle beamforming capability is then validated through its normalized beampattern. Based on this model, we then construct an FD-RIS-assisted CC system under a multi-warden scenario and derive an approximate closed-form expression for the covert constraints by considering the worst-case eavesdropping conditions and utilizing the logarithmic moment-generating function. An optimization problem is formulated which aims at maximizing the covert user's achievable rate under covert constrains by jointly designing the time delays and modulation frequencies. To tackle this non-convex problem, an iterative algorithm with assured convergence is proposed to effectively solve the time-delay and modulation frequency variables. To evaluate the performance of the proposed scheme, we consider three communication scenarios with varying spatial correlations between the covert user and wardens. Simulation results demonstrate that FD-RIS can significantly improve covert performance, particularly in angular-overlap scenarios where traditional RIS experiences severe degradation. These findings further highlight the effectiveness of FD-RIS in enhancing CC robustness under challenging spatial environments."}
{"id": "2511.07283", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.07283", "abs": "https://arxiv.org/abs/2511.07283", "authors": ["Anupam Gupta", "Marco Molinaro", "Matteo Russo"], "title": "A Learning Perspective on Random-Order Covering Problems", "comment": "Appears in SOSA 2026", "summary": "In the random-order online set cover problem, the instance with $m$ sets and $n$ elements is chosen in a worst-case fashion, but then the elements arrive in a uniformly random order. Can this random-order model allow us to circumvent the bound of $O(\\log m \\log n)$-competitiveness for the adversarial arrival order model? This long-standing question was recently resolved by Gupta et al. (2021), who gave an algorithm that achieved an $O(\\log mn)$-competitive ratio. While their LearnOrCover was inspired by ideas in online learning (and specifically the multiplicative weights update method), the analysis proceeded by showing progress from first principles.\n  In this work, we show a concrete connection between random-order set cover and stochastic mirror-descent/online convex optimization. In particular, we show how additive/multiplicative regret bounds for the latter translate into competitiveness for the former. Indeed, we give a clean recipe for this translation, allowing us to extend our results to covering integer programs, set multicover, and non-metric facility location in the random order model, matching (and giving simpler proofs of) the previous applications of the LearnOrCover framework."}
{"id": "2511.06213", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06213", "abs": "https://arxiv.org/abs/2511.06213", "authors": ["Xian-Jin Gui"], "title": "Time Matters: A Novel Real-Time Long- and Short-term User Interest Model for Click-Through Rate Prediction", "comment": "This work was doned when the first author interned at Alibaba Group", "summary": "Click-Through Rate (CTR) prediction is a core task in online personalization platform. A key step for CTR prediction is to learn accurate user representation to capture their interests. Generally, the interest expressed by a user is time-variant, i.e., a user activates different interests at different time. However, most previous CTR prediction methods overlook the correlation between the activated interest and the occurrence time, resulting in what they actually learn is the mixture of the interests expressed by the user at all time, rather than the real-time interest at the certain prediction time. To capture the correlation between the activated interest and the occurrence time, in this paper we investigate users' interest evolution from the perspective of the whole time line and develop two regular patterns: periodic pattern and time-point pattern. Based on the two patterns, we propose a novel time-aware long- and short-term user interest modeling method to model users' dynamic interests at different time. Extensive experiments on public datasets as well as an industrial dataset verify the effectiveness of exploiting the two patterns and demonstrate the superiority of our proposed method compared with other state-of-the-art ones."}
{"id": "2511.07354", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.07354", "abs": "https://arxiv.org/abs/2511.07354", "authors": ["Shay Solomon", "Amitai Uzrad"], "title": "Dynamic Set Cover with Worst-Case Recourse", "comment": null, "summary": "In the dynamic set cover (SC) problem, the input is a dynamic universe of at most $n$ elements and a fixed collection of $m$ sets, where each element belongs to at most $f$ sets and each set has cost in $[1/C, 1]$. The objective is to efficiently maintain an approximate minimum SC under element updates; efficiency is primarily measured by the update time, but another important parameter is the recourse (number of changes to the solution per update). Ideally, one would like to achieve low worst-case bounds on both update time and recourse.\n  One can achieve approximation $(1+ε)\\ln n$ (greedy-based) or $(1+ε)f$ (primal-dual-based) with worst-case update time $O(f\\log n)$ (ignoring $ε$ dependencies). However, despite a large body of work, no algorithm with low update time (even amortized) and nontrivial worst-case recourse is known, even for unweighted instances ($C = 1$)!\n  We remedy this by providing a transformation that, given as a black-box a SC algorithm with approximation $α$ and update time $T$, returns a set cover algorithm with approximation $(2 + ε)α$, update time $O(T + αC)$, and worst-case recourse $O(αC)$. Our main results are obtained by leveraging this transformation for constant $C$:..."}
{"id": "2511.06254", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06254", "abs": "https://arxiv.org/abs/2511.06254", "authors": ["Teng Shi", "Chenglei Shen", "Weijie Yu", "Shen Nie", "Chongxuan Li", "Xiao Zhang", "Ming He", "Yan Han", "Jun Xu"], "title": "LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation", "comment": null, "summary": "Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation."}
{"id": "2511.07395", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.07395", "abs": "https://arxiv.org/abs/2511.07395", "authors": ["Hadi Hosseini", "Vishwa Prakash HV", "Aditi Sethia", "Jatin Yadav"], "title": "The Landscape of Almost Equitable Allocations", "comment": null, "summary": "Equitability is a fundamental notion in fair division which requires that all agents derive equal value from their allocated bundles. We study, for general (possibly non-monotone) valuations, a popular relaxation of equitability known as equitability up to one item (EQ1). An EQ1 allocation may fail to exist even with additive non-monotone valuations; for instance, when there are two agents, one valuing every item positively and the other negatively. This motivates a mild and natural assumption: all agents agree on the sign of their value for the grand bundle. Under this assumption, we prove the existence and provide an efficient algorithm for computing EQ1 allocations for two agents with general valuations. When there are more than two agents, we show the existence and polynomial-time computability of EQ1 allocations for valuation classes beyond additivity and monotonicity, in particular for (1) doubly monotone valuations and (2) submodular (resp. supermodular) valuations where the value for the grand bundle is nonnegative (resp. nonpositive) for all agents. Furthermore, we settle an open question of Bil`o et al. by showing that an EQ1 allocation always exists for nonnegative(resp. nonpositive) valuations, i.e., when every agent values each subset of items nonnegatively (resp. nonpositively). Finally, we complete the picture by showing that for general valuations with more than two agents, EQ1 allocations may not exist even when agents agree on the sign of the grand bundle, and that deciding the existence of an EQ1 allocation is computationally intractable."}
{"id": "2511.06285", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06285", "abs": "https://arxiv.org/abs/2511.06285", "authors": ["Peng He", "Yanglei Gan", "Tingting Dai", "Run Lin", "Xuexin Li", "Yao Liu", "Qiao Liu"], "title": "Exploiting Inter-Session Information with Frequency-enhanced Dual-Path Networks for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation (SR) aims to predict a user's next item preference by modeling historical interaction sequences. Recent advances often integrate frequency-domain modules to compensate for self-attention's low-pass nature by restoring the high-frequency signals critical for personalized recommendations. Nevertheless, existing frequency-aware solutions process each session in isolation and optimize exclusively with time-domain objectives. Consequently, they overlook cross-session spectral dependencies and fail to enforce alignment between predicted and actual spectral signatures, leaving valuable frequency information under-exploited. To this end, we propose FreqRec, a Frequency-Enhanced Dual-Path Network for sequential Recommendation that jointly captures inter-session and intra-session behaviors via a learnable Frequency-domain Multi-layer Perceptrons. Moreover, FreqRec is optimized under a composite objective that combines cross entropy with a frequency-domain consistency loss, explicitly aligning predicted and true spectral signatures. Extensive experiments on three benchmarks show that FreqRec surpasses strong baselines and remains robust under data sparsity and noisy-log conditions."}
{"id": "2511.06388", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06388", "abs": "https://arxiv.org/abs/2511.06388", "authors": ["Kunrong Li", "Zhu Sun", "Kwan Hui Lim"], "title": "HyMoERec: Hybrid Mixture-of-Experts for Sequential Recommendation", "comment": "AAAI 2026 Student Abstract", "summary": "We propose HyMoERec, a novel sequential recommendation framework that addresses the limitations of uniform Position-wise Feed-Forward Networks in existing models. Current approaches treat all user interactions and items equally, overlooking the heterogeneity in user behavior patterns and diversity in item complexity. HyMoERec initially introduces a hybrid mixture-of-experts architecture that combines shared and specialized expert branches with an adaptive expert fusion mechanism for the sequential recommendation task. This design captures diverse reasoning for varied users and items while ensuring stable training. Experiments on MovieLens-1M and Beauty datasets demonstrate that HyMoERec consistently outperforms state-of-the-art baselines."}
{"id": "2511.06405", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06405", "abs": "https://arxiv.org/abs/2511.06405", "authors": ["Dongsheng Wang", "Shen Gao", "Chengrui Huang", "Yuxi Huang", "Ruixiang Feng", "Shuo Shang"], "title": "TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation", "comment": "Accepted by AAAI2026", "summary": "Next Point-of-Interest (POI) recommendation is a fundamental task in location-based services. While recent advances leverage Large Language Model (LLM) for sequential modeling, existing LLM-based approaches face two key limitations: (i) strong reliance on the contextual completeness of user histories, resulting in poor performance on out-of-history (OOH) scenarios; (ii) limited scalability, due to the restricted context window of LLMs, which limits their ability to access and process a large number of candidate POIs. To address these challenges, we propose Tool4POI, a novel tool-augmented framework that enables LLMs to perform open-set POI recommendation through external retrieval and reasoning. Tool4POI consists of three key modules: preference extraction module, multi-turn candidate retrieval module, and reranking module, which together summarize long-term user interests, interact with external tools to retrieve relevant POIs, and refine final recommendations based on recent behaviors. Unlike existing methods, Tool4POI requires no task-specific fine-tuning and is compatible with off-the-shelf LLMs in a plug-and-play manner. Extensive experiments on three real-world datasets show that Tool4POI substantially outperforms state-of-the-art baselines, achieving up to 40% accuracy on challenging OOH scenarios where existing methods fail, and delivering average improvements of 20% and 30% on Acc@5 and Acc@10, respectively."}
{"id": "2511.06635", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06635", "abs": "https://arxiv.org/abs/2511.06635", "authors": ["Lulu Yu", "Keping Bi", "Jiafeng Guo", "Shihao Liu", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "title": "Can LLM Annotations Replace User Clicks for Learning to Rank?", "comment": "12 pages, 7 figures", "summary": "Large-scale supervised data is essential for training modern ranking models, but obtaining high-quality human annotations is costly. Click data has been widely used as a low-cost alternative, and with recent advances in large language models (LLMs), LLM-based relevance annotation has emerged as another promising annotation. This paper investigates whether LLM annotations can replace click data for learning to rank (LTR) by conducting a comprehensive comparison across multiple dimensions. Experiments on both a public dataset, TianGong-ST, and an industrial dataset, Baidu-Click, show that click-supervised models perform better on high-frequency queries, while LLM annotation-supervised models are more effective on medium- and low-frequency queries. Further analysis shows that click-supervised models are better at capturing document-level signals such as authority or quality, while LLM annotation-supervised models are more effective at modeling semantic matching between queries and documents and at distinguishing relevant from non-relevant documents. Motivated by these observations, we explore two training strategies -- data scheduling and frequency-aware multi-objective learning -- that integrate both supervision signals. Both approaches enhance ranking performance across queries at all frequency levels, with the latter being more effective. Our code is available at https://github.com/Trustworthy-Information-Access/LLMAnn_Click."}
{"id": "2511.06668", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06668", "abs": "https://arxiv.org/abs/2511.06668", "authors": ["Saeedeh Javadi", "Sara Mirabi", "Manan Gangar", "Bahadorreza Ofoghi"], "title": "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare", "comment": null, "summary": "In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains."}
{"id": "2511.06803", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06803", "abs": "https://arxiv.org/abs/2511.06803", "authors": ["Junpeng Zhao", "Lin Li", "Ming Li", "Amran Bhuiyan", "Jimmy Huang"], "title": "Learning to Fast Unrank in Collaborative Filtering Recommendation", "comment": null, "summary": "Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank."}
{"id": "2511.06905", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06905", "abs": "https://arxiv.org/abs/2511.06905", "authors": ["Xiaokun Zhang", "Zhaochun Ren", "Bowei He", "Ziqiang Cui", "Chen Ma"], "title": "Have We Really Understood Collaborative Information? An Empirical Investigation", "comment": "This work has been accepted by WSDM 2026", "summary": "Collaborative information serves as the cornerstone of recommender systems which typically focus on capturing it from user-item interactions to deliver personalized services. However, current understanding of this crucial resource remains limited. Specifically, a quantitative definition of collaborative information is missing, its manifestation within user-item interactions remains unclear, and its impact on recommendation performance is largely unknown. To bridge this gap, this work conducts a systematic investigation of collaborative information. We begin by clarifying collaborative information in terms of item co-occurrence patterns, identifying its main characteristics, and presenting a quantitative definition. We then estimate the distribution of collaborative information from several aspects, shedding light on how collaborative information is structured in practice. Furthermore, we evaluate the impact of collaborative information on the performance of various recommendation algorithms. Finally, we highlight challenges in effectively capturing collaborative information and outlook promising directions for future research. By establishing an empirical framework, we uncover many insightful observations that advance our understanding of collaborative information and offer valuable guidelines for developing more effective recommender systems."}
{"id": "2511.06937", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.NI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.06937", "abs": "https://arxiv.org/abs/2511.06937", "authors": ["Yu Hou", "Hua Li", "Ha Young Kim", "Won-Yong Shin"], "title": "Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization", "comment": "14 pages, 12 figures, 9 tables", "summary": "Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60."}
{"id": "2511.07028", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.07028", "abs": "https://arxiv.org/abs/2511.07028", "authors": ["Huayang Xu", "Huanhuan Yuan", "Guanfeng Liu", "Junhua Fang", "Lei Zhao", "Pengpeng Zhao"], "title": "Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation has garnered significant attention for its ability to capture dynamic preferences by mining users' historical interaction data. Given that users' complex and intertwined periodic preferences are difficult to disentangle in the time domain, recent research is exploring frequency domain analysis to identify these hidden patterns. However, current frequency-domain-based methods suffer from two key limitations: (i) They primarily employ static filters with fixed characteristics, overlooking the personalized nature of behavioral patterns; (ii) While the global discrete Fourier transform excels at modeling long-range dependencies, it can blur non-stationary signals and short-term fluctuations. To overcome these limitations, we propose a novel method called Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation. Specifically, it consists of two vital modules: dynamic frequency-domain filtering and wavelet feature enhancement. The former is used to dynamically adjust filtering operations based on behavioral sequences to extract personalized global information, and the latter integrates wavelet transform to reconstruct sequences, enhancing blurred non-stationary signals and short-term fluctuations. Finally, these two modules work to achieve comprehensive performance and efficiency optimization in long sequential recommendation scenarios. Extensive experiments on four widely-used benchmark datasets demonstrate the superiority of our work."}
{"id": "2511.07295", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07295", "abs": "https://arxiv.org/abs/2511.07295", "authors": ["Tianrui Song", "Wen-Shuo Chao", "Hao Liu"], "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models", "comment": "Accepted by AAAI2026", "summary": "Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance."}
{"id": "2511.06179", "categories": ["cs.DB", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06179", "abs": "https://arxiv.org/abs/2511.06179", "authors": ["Joel Ward"], "title": "MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces", "comment": null, "summary": "We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling."}
