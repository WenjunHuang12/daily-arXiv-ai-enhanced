{"id": "2510.12846", "categories": ["cs.GT", "math.PR", "91A05"], "pdf": "https://arxiv.org/pdf/2510.12846", "abs": "https://arxiv.org/abs/2510.12846", "authors": ["Andrea Collevecchio", "Gabor Lugosi", "Adrian Vetta", "Rui-Ray Zhang"], "title": "Finding a Nash equilibrium of a random win-lose game in expected polynomial time", "comment": null, "summary": "A long-standing open problem in algorithmic game theory asks whether or not\nthere is a polynomial time algorithm to compute a Nash equilibrium in a random\nbimatrix game. We study random win-lose games, where the entries of the\n$n\\times n$ payoff matrices are independent and identically distributed\n(i.i.d.) Bernoulli random variables with parameter $p=p(n)$. We prove that, for\nnearly all values of the parameter $p=p(n)$, there is an expected\npolynomial-time algorithm to find a Nash equilibrium in a random win-lose game.\nMore precisely, if $p\\sim cn^{-a}$ for some parameters $a,c\\ge 0$, then there\nis an expected polynomial-time algorithm whenever $a\\not\\in \\{1/2, 1\\}$. In\naddition, if $a = 1/2$ there is an efficient algorithm if either $c \\le e^{-52}\n2^{-8} $ or $c\\ge 0.977$. If $a=1$, then there is an expected polynomial-time\nalgorithm if either $c\\le 0.3849$ or $c\\ge \\log^9 n$."}
{"id": "2510.12862", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.12862", "abs": "https://arxiv.org/abs/2510.12862", "authors": ["Rafał Kucharski", "Anastasia Psarou", "Natello Descormier"], "title": "Equilibria in routing games with connected autonomous vehicles will not be strong, as exclusive clubs may form", "comment": null, "summary": "User Equilibrium is the standard representation of the so-called routing game\nin which drivers adjust their route choices to arrive at their destinations as\nfast as possible. Asking whether this Equilibrium is strong or not was\nmeaningless for human drivers who did not form coalitions due to technical and\nbehavioral constraints. This is no longer the case for connected autonomous\nvehicles (CAVs), which will be able to communicate and collaborate to jointly\nform routing coalitions.\n  We demonstrate this for the first time on a carefully designed toy-network\nexample, where a `club` of three autonomous vehicles jointly decides to deviate\nfrom the user equilibrium and benefit (arrive faster). The formation of such a\nclub has negative consequences for other users, who are not invited to join it\nand now travel longer, and for the system, making it suboptimal and\ndisequilibrated, which triggers adaptation dynamics.\n  This discovery has profound implications for the future of our cities. We\ndemonstrate that, if not prevented, CAV operators may intentionally\ndisequilibrate traffic systems from their classic Nash equilibria, benefiting\ntheir own users and imposing costs on others. These findings suggest the\npossible emergence of an exclusive CAV elite, from which human-driven vehicles\nand non-coalition members may be excluded, potentially leading to\nsystematically longer travel times for those outside the coalition, which would\nbe harmful for the equity of public road networks."}
{"id": "2510.12952", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.12952", "abs": "https://arxiv.org/abs/2510.12952", "authors": ["Maneesha Papireddygari", "Xintong Wang", "Bo Waggoner", "David M. Pennock"], "title": "Efficiency of Constant Log Utility Market Makers", "comment": null, "summary": "Automated Market Makers (AMMs) are used to provide liquidity for\ncombinatorial prediction markets that would otherwise be too thinly traded.\nThey offer both buy and sell prices for any of the doubly exponential many\npossible securities that the market can offer. The problem of setting those\nprices is known to be #P-hard for the original and most well-known AMM, the\nlogarithmic market scoring rule (LMSR) market maker [Chen et al., 2008]. We\nfocus on another natural AMM, the Constant Log Utility Market Maker (CLUM).\nUnlike LMSR, whose worst-case loss bound grows with the number of outcomes,\nCLUM has constant worst-case loss, allowing the market to add outcomes on the\nfly and even operate over countably infinite many outcomes, among other\nfeatures. Simpler versions of CLUM underpin several Decentralized Finance\n(DeFi) mechanisms including the Uniswap protocol that handles billions of\ndollars of cryptocurrency trades daily. We first establish the computational\ncomplexity of the problem: we prove that pricing securities is #P-hard for\nCLUM, via a reduction from the model counting 2-SAT problem. In order to make\nCLUM more practically viable, we propose an approximation algorithm for pricing\nsecurities that works with high probability. This algorithm assumes access to\nan oracle capable of determining the maximum shares purchased of any one\noutcome and the total number of outcomes that has that maximum amount\npurchased. We then show that this oracle can be implemented in polynomial time\nwhen restricted to interval securities, which are used in designing financial\noptions."}
{"id": "2510.13088", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.13088", "abs": "https://arxiv.org/abs/2510.13088", "authors": ["Rishi Patel", "Emmanouil Pountourakis", "Samuel Taggart"], "title": "Repeated Sales with Heterogeneous Buyer Sophistication", "comment": "To appear at WINE 2025", "summary": "This paper considers behavior-based price discrimination in the repeated sale\nof a non-durable good to a single long-lived buyer, by a seller without\ncommitment power. We assume that there is a mixed population of forward-looking\n``sophisticated'' buyers and myopic ``naive'' buyers. We investigate the impact\nof these dynamics on the seller's ability to learn about the buyer and exploit\nthis learning for revenue. We obtain conclusions that differ dramatically with\nthe time horizon of the interactions. To understand short time horizons, we\nanalyze a two-period model, and find that the strategic demand reduction\nobserved with fully sophisticated buyers is robust to the introduction of naive\ntypes. In fact, despite the inability of naive buyers to game the pricing\nalgorithm, their introduction can further harm the seller's revenue, due to\nmore intense demand reduction overall. For long horizons, we consider an\ninfinite-horizon model with time discounting. We find that the extreme demand\nreduction predicted by previous work does not survive the introduction of naive\nbuyers. Instead, we observe equilibria where the seller learns meaningfully\ndespite the sophisticated buyers' demand reduction. We prove that for a natural\nfamily of such equilibria, the seller's revenue is not just high, but\napproximates the revenue attainable with commitment power, even when the\nfraction of naive types is vanishingly small."}
{"id": "2510.13528", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.13528", "abs": "https://arxiv.org/abs/2510.13528", "authors": ["Loïs Ecoffet", "Veronika Rehn-Sonigo", "Jean-François Couchot", "Catuscia Palamidessi"], "title": "Experiments \\& Analysis of Privacy-Preserving SQL Query Sanitization Systems", "comment": "10 pages, 5 figures, submitted to EDBT 26", "summary": "Analytical SQL queries are essential for extracting insights from relational\ndatabases but concurrently introduce significant privacy risks by potentially\nexposing sensitive information. To mitigate these risks, numerous query\nsanitization systems have been developed, employing diverse approaches that\ncreate a complex landscape for both researchers and practitioners. These\nsystems vary fundamentally in their design, including the underlying privacy\nmodel, such as k-anonymity or Differential Privacy; the protected privacy unit,\nwhether at the tuple- or user-level; and the software architecture, which can\nbe proxy-based or integrated. This paper provides a systematic classification\nof state-of-the-art SQL sanitization systems based on these qualitative\ncriteria and the scope of queries they support. Furthermore, we present a\nquantitative analysis of leading systems, empirically measuring the trade-offs\nbetween data utility, query execution overhead, and privacy guarantees across a\nrange of analytical queries. This work offers a structured overview and\nperformance assessment intended to clarify the capabilities and limitations of\ncurrent privacy-preserving database technologies."}
{"id": "2510.12815", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12815", "abs": "https://arxiv.org/abs/2510.12815", "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"], "title": "Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation", "comment": "CIKM'25", "summary": "Reinforcement learning-based recommender systems (RL4RS) have gained\nattention for their ability to adapt to dynamic user preferences. However,\nthese systems face challenges, particularly in offline settings, where data\ninefficiency and reliance on pre-collected trajectories limit their broader\napplicability. While offline reinforcement learning methods leverage extensive\ndatasets to address these issues, they often struggle with noisy data and fail\nto capture long-term user preferences, resulting in suboptimal recommendation\npolicies. To overcome these limitations, we propose Diffusion-enhanced\nActor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates\ndiffusion processes with reinforcement learning to model complex user\npreferences more effectively. DAC4Rec leverages the denoising capabilities of\ndiffusion models to enhance the robustness of offline RL algorithms and\nincorporates a Q-value-guided policy optimization strategy to better handle\nsuboptimal trajectories. Additionally, we introduce an energy-based sampling\nstrategy to reduce randomness during recommendation generation, ensuring more\ntargeted and reliable outcomes. We validate the effectiveness of DAC4Rec\nthrough extensive experiments on six real-world offline datasets and in an\nonline simulation environment, demonstrating its ability to optimize long-term\nuser preferences. Furthermore, we show that the proposed diffusion policy can\nbe seamlessly integrated into other commonly used RL algorithms in RL4RS,\nhighlighting its versatility and wide applicability."}
{"id": "2510.13171", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13171", "abs": "https://arxiv.org/abs/2510.13171", "authors": ["Jun Qian", "Ross Murch", "Khaled B. Letaief"], "title": "On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging", "comment": "5 pages, 3 figures, accepted by IEEE WCL", "summary": "Active reconfigurable intelligent surfaces (RISs) employ amplification to\novercome attenuation caused by the RIS cascaded link. In this paper, we analyze\nthe effects of phase errors and channel aging in active simultaneously\ntransmitting and reflecting (STAR) RIS-assisted cell-free massive\nmultiple-input multiple-output (MIMO) systems. By leveraging a spatially\ncorrelated Rayleigh fading model, this paper derives minimum mean square error\nestimate-based channel estimates and formulates closed-form expressions for\ndownlink spectral efficiency. This analytical framework enables a comprehensive\nevaluation of the effects of channel aging and uniformly distributed phase\nerrors on system performance. The results demonstrate that active STAR-RISs can\neffectively compensate for the adverse effects of phase errors and channel\naging. To counteract the impact of channel aging, we propose practical\nguidelines for resource-block-length design. Also, an increase in APs and\nSTAR-RIS elements, along with a larger amplification factor, can alleviate\nperformance degradation."}
{"id": "2510.13330", "categories": ["cs.DS", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13330", "abs": "https://arxiv.org/abs/2510.13330", "authors": ["Bridget Smart", "Max Ward", "Matthew Roughan"], "title": "A faster algorithm for efficient longest common substring calculation for non-parametric entropy estimation in sequential data", "comment": "Package available at https://pypi.org/project/LCSFinder, codebase\n  available at https://github.com/bridget-smart/LCSFinder. It also includes a\n  heuristic variant offering excellent practical performance with slower\n  worst-case complexity, and has been integrated into the ProcessEntropy\n  (https://pypi.org/project/ProcessEntropy/) package", "summary": "Non-parametric entropy estimation on sequential data is a fundamental tool in\nsignal processing, capturing information flow within or between processes to\nmeasure predictability, redundancy, or similarity. Methods based on longest\ncommon substrings (LCS) provide a non-parametric estimate of typical set size\nbut are often inefficient, limiting use on real-world data. We introduce\nLCSFinder, a new algorithm that improves the worst-case performance of LCS\ncalculations from cubic to log-linear time. Although built on standard\nalgorithmic constructs - including sorted suffix arrays and persistent binary\nsearch trees - the details require care to provide the matches required for\nentropy estimation on dynamically growing sequences. We demonstrate that\nLCSFinder achieves dramatic speedups over existing implementations on real and\nsimulated data, enabling entropy estimation at scales previously infeasible in\npractical signal processing."}
{"id": "2510.13261", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13261", "abs": "https://arxiv.org/abs/2510.13261", "authors": ["Björn Filter", "Ralf Möller", "Özgür Lütfü Özçep"], "title": "A Ratio-Based Shapley Value for Collaborative Machine Learning - Extended Version", "comment": "Extended version of a paper accepted at the 26th International\n  Conference on Principles and Practice of Multi-Agent Systems (PRIMA 2025)", "summary": "Collaborative machine learning enables multiple data owners to jointly train\nmodels for improved predictive performance. However, ensuring incentive\ncompatibility and fair contribution-based rewards remains a critical challenge.\nPrior work by Sim and colleagues (Rachel Hwee Ling Sim et al: Collaborative\nmachine learning with incentive-aware model rewards. In: International\nconference on machine learning. PMLR. 2020, pp. 8927-8963) addressed this by\nallocating model rewards, which are non-monetary and freely replicable, based\non the Shapley value of each party's data contribution, measured via\ninformation gain. In this paper, we introduce a ratio-based Shapley value that\nreplaces the standard additive formulation with a relative contribution\nmeasure. While our overall reward framework, including the incentive\ndefinitions and model-reward setting, remains aligned with that of Sim and\ncolleagues, the underlying value function is fundamentally different. Our\nalternative valuation induces a different distribution of model rewards and\noffers a new lens through which to analyze incentive properties. We formally\ndefine the ratio-based value and prove that it satisfies the same set of\nincentive conditions as the additive formulation, including adapted versions of\nfairness, individual rationality, and stability. Like the original approach,\nour method faces the same fundamental trade-offs between these incentives. Our\ncontribution is a mathematically grounded alternative to the additive Shapley\nframework, potentially better suited to contexts where proportionality among\ncontributors is more meaningful than additive differences."}
{"id": "2510.13662", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.13662", "abs": "https://arxiv.org/abs/2510.13662", "authors": ["Mahdi Esmailoghli", "Matthias Weidlich"], "title": "The Past Still Matters: A Temporally-Valid Data Discovery System", "comment": null, "summary": "Over the past decade, the proliferation of public and enterprise data lakes\nhas fueled intensive research into data discovery, aiming to identify the most\nrelevant data from vast and complex corpora to support diverse user tasks.\nSignificant progress has been made through the development of innovative index\nstructures, similarity measures, and querying infrastructures. Despite these\nadvances, a critical aspect remains overlooked: relevance is time-varying.\nExisting discovery methods largely ignore this temporal dimension, especially\nwhen explicit date/time metadata is missing. To fill this gap, we outline a\nvision for a data discovery system that incorporates the temporal dimension of\ndata. Specifically, we define the problem of temporally-valid data discovery\nand argue that addressing it requires techniques for version discovery,\ntemporal lineage inference, change log synthesis, and time-aware data\ndiscovery. We then present a system architecture to deliver these techniques,\nbefore we summarize research challenges and opportunities. As such, we lay the\nfoundation for a new class of data discovery systems, transforming how we\ninteract with evolving data lakes."}
{"id": "2510.12816", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12816", "abs": "https://arxiv.org/abs/2510.12816", "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"], "title": "Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior", "comment": "CIKM'25", "summary": "Reinforcement Learning-based recommender systems (RLRS) offer an effective\nway to handle sequential recommendation tasks but often face difficulties in\nreal-world settings, where user feedback data can be sub-optimal or sparse. In\nthis paper, we introduce MDT4Rec, an offline RLRS framework that builds on the\nDecision Transformer (DT) to address two major challenges: learning from\nsub-optimal histories and representing complex user-item interactions. First,\nMDT4Rec shifts the trajectory stitching procedure from the training phase to\naction inference, allowing the system to shorten its historical context when\nnecessary and thereby ignore negative or unsuccessful past experiences. Second,\nMDT4Rec initializes DT with a pre-trained large language model (LLM) for\nknowledge transfer, replaces linear embedding layers with Multi-Layer\nPerceptrons (MLPs) for more flexible representations, and employs Low-Rank\nAdaptation (LoRA) to efficiently fine-tune only a small subset of parameters.\nWe evaluate MDT4Rec on five public datasets and in an online simulation\nenvironment, demonstrating that it outperforms existing methods."}
{"id": "2510.13180", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13180", "abs": "https://arxiv.org/abs/2510.13180", "authors": ["Qi Qi", "Abdelhamid Tayebi", "Daizhan Cheng", "Jun-e Feng"], "title": "A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing", "comment": null, "summary": "In compressed sensing (CS), sparse signals can be reconstructed from\nsignificantly fewer samples than required by the Nyquist-Shannon sampling\ntheorem. While non-sparse signals can be sparsely represented in appropriate\ntransformation domains, conventional CS frameworks rely on the incoherence of\nthe measurement matrix columns to guarantee reconstruction performance. This\npaper proposes a novel method termed Dimension-Keeping Semi-Tensor Product\nCompressed Sensing (DK-STP-CS), which leverages intra-group correlations while\nmaintaining inter-group incoherence to enhance the measurement matrix design.\nSpecifically, the DK-STP algorithm is integrated into the design of the sensing\nmatrix, enabling dimensionality reduction while preserving signal recovery\ncapability. For image compression and reconstruction tasks, the proposed method\nachieves notable noise suppression and improves visual fidelity. Experimental\nresults demonstrate that DK-STP-CS significantly outperforms traditional CS and\nSTP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)\nvalues between the reconstructed and original images. The robustness of\nDK-STP-CS is further validated under noisy conditions and varying sampling\nrates, highlighting its potential for practical applications in\nresource-constrained environments."}
{"id": "2510.13335", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2510.13335", "abs": "https://arxiv.org/abs/2510.13335", "authors": ["Fedor V. Fomin", "Petr A. Golovach", "Tanmay Inamdar", "Saket Saurabh", "Meirav Zehavi"], "title": "Tight Parameterized (In)tractability of Layered Crossing Minimization: Subexponential Algorithms and Kernelization", "comment": "Full version of SODA 2026 paper. Abstract shortened due to arXiv\n  restrictions", "summary": "The starting point of our work is a decade-old open question concerning the\nsubexponential parameterized complexity of \\textsc{2-Layer Crossing\nMinimization}. In this problem, the input is an $n$-vertex graph $G$ whose\nvertices are partitioned into two independent sets $V_1$ and $V_2$, and a\nnon-negative integer $k$. The question is whether $G$ admits a 2-layered\ndrawing with at most $k$ crossings, where each $V_i$ lies on a distinct line\nparallel to the $x$-axis, and all edges are straight lines. We resolve this\nopen question by giving the first subexponential fixed-parameter algorithm for\nthis problem, running in time $2^{O(\\sqrt{k}\\log k)} + n \\cdot k^{O(1)}$.\n  We then ask whether the subexponential phenomenon extends beyond two layers.\nIn the general $h$-Layer Crossing Minimization problem, the vertex set is\npartitioned into $h$ independent sets $V_1, \\ldots, V_h$, and the goal is to\ndecide whether an $h$-layered drawing with at most $k$ crossings exists. We\npresent a subexponential FPT algorithm for three layers with running time\n$2^{O(k^{2/3}\\log k)} + n \\cdot k^{O(1)}$ for $h = 3$ layers. In contrast, we\nshow that for all $h \\ge 5$, no algorithm with running time $2^{o(k/\\log k)}\n\\cdot n^{O(1)}$ exists unless the Exponential-Time Hypothesis fails.\n  Finally, we address polynomial kernelization. While a polynomial kernel was\nalready known for $h=2$, we design a new polynomial kernel for $h=3$. These\nkernels are essential ingredients in our subexponential algorithms. Finally, we\nrule out polynomial kernels for all $h \\ge 4$ unless the polynomial hierarchy\ncollapses."}
{"id": "2510.13518", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.13518", "abs": "https://arxiv.org/abs/2510.13518", "authors": ["Shaul Rosner", "Marc Schröder", "Laura Vargas Koch"], "title": "Nash Flows Over Time with Tolls", "comment": null, "summary": "We study a dynamic routing game motivated by traffic flows. The base model\nfor an edge is the Vickrey bottleneck model. That is, edges are equipped with a\nfree flow transit time and a capacity. When the inflow into an edge exceeds its\ncapacity, a queue forms and the following particles experience a waiting time.\nIn this paper, we enhance the model by introducing tolls, i.e., a cost each\nflow particle must pay for traversing an edge. In this setting we consider\nnon-atomic equilibria, which means flows over time in which every particle is\non a cheapest path, when summing up toll and travel time. We first show that\nunlike in the non-tolled version of this model, dynamic equilibria are not\nunique in terms of costs and do not necessarily reach a steady state. As a main\nresult, we provide a procedure to compute steady states in the model with\ntolls."}
{"id": "2510.12959", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12959", "abs": "https://arxiv.org/abs/2510.12959", "authors": ["Md Aminul Islam", "Elena Zheleva", "Ren Wang"], "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering", "comment": null, "summary": "User historical interaction data is the primary signal for learning user\npreferences in collaborative filtering (CF). However, the training data often\nexhibits a long-tailed distribution, where only a few items have the majority\nof interactions. CF models trained directly on such imbalanced data are prone\nto learning popularity bias, which reduces personalization and leads to\nsuboptimal recommendation quality. Graph Neural Networks (GNNs), while\neffective for CF due to their message passing mechanism, can further propagate\nand amplify popularity bias through their aggregation process. Existing\napproaches typically address popularity bias by modifying training objectives\nbut fail to directly counteract the bias propagated during GNN's neighborhood\naggregation. Applying weights to interactions during aggregation can help\nalleviate this problem, yet it risks distorting model learning due to unstable\nnode representations in the early stages of training. In this paper, we propose\na Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias\nin GNN-based CF and operates directly on pre-trained embeddings without\nrequiring retraining. By estimating interaction-level popularity and removing\npopularity components from node representations via a popularity direction\nvector, PPD reduces bias while preserving user preferences. Experimental\nresults show that our method outperforms state-of-the-art approaches for\npopularity bias correction in GNN-based CF."}
{"id": "2510.13209", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13209", "abs": "https://arxiv.org/abs/2510.13209", "authors": ["Lipeng Zhu", "Haobin Mao", "Ge Yan", "Wenyan Ma", "Zhenyu Xiao", "Rui Zhang"], "title": "Movable and Reconfigurable Antennas for 6G: Unlocking Electromagnetic-Domain Design and Optimization", "comment": null, "summary": "The growing demands of 6G mobile communication networks necessitate advanced\nantenna technologies. Movable antennas (MAs) and reconfigurable antennas (RAs)\nenable dynamic control over antenna's position, orientation, radiation,\npolarization, and frequency response, introducing rich electromagnetic-domain\ndegrees of freedom for the design and performance enhancement of wireless\nsystems. This article overviews their application scenarios, hardware\narchitectures, and design methods. Field test and simulation results highlight\ntheir performance benefits over conventional fixed/non-reconfigurable antennas."}
{"id": "2510.13446", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.13446", "abs": "https://arxiv.org/abs/2510.13446", "authors": ["Fateme Abbasi", "Hyung-Chan An", "Jarosław Byrka", "Changyeol Lee", "Yongho Shin"], "title": "Chromatic correlation clustering via cluster LP", "comment": null, "summary": "Correlation Clustering is a fundamental clustering problem, and there has\nbeen a line of work on improving the approximation ratio for this problem in\nrecent years. A key algorithmic component in these works is the cluster LP.\nChromatic Correlation Clustering is an interesting generalization that has also\nbeen intensively studied. In light of success of the cluster LP in Correlation\nClustering, it would be an interesting question whether the cluster LP can be\nused in Chromatic Correlation Clustering. We answer this question with\naffirmatives by presenting a $(2+\\varepsilon)$-approximation algorithm for\nChromatic Correlation Clustering using a chromatic cluster LP."}
{"id": "2510.13633", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.13633", "abs": "https://arxiv.org/abs/2510.13633", "authors": ["Pooja Kulkarni", "Ruta Mehta", "Vishnu V. Narayan", "Tomasz Ponitka"], "title": "Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist, and at What Cost?", "comment": "20 pages", "summary": "We study the problem of fairly allocating $m$ indivisible items arriving\nonline, among $n$ (offline) agents. Although envy-freeness has emerged as the\narchetypal fairness notion, envy-free (EF) allocations need not exist with\nindivisible items. To bypass this, a prominent line of research demonstrates\nthat there exist allocations that can be made envy-free by allowing a subsidy.\nExtensive work in the offline setting has focused on finding such envy-freeable\nallocations with bounded subsidy. We extend this literature to an online\nsetting where items arrive one at a time and must be immediately and\nirrevocably allocated. Our contributions are two-fold:\n  1. Maintaining EF Online: We show that envy-freeability cannot always be\npreserved online when the valuations are submodular or supermodular, even with\nbinary marginals. In contrast, we design online algorithms that maintain\nenvy-freeability at every step for the class of additive valuations, and for\nits superclasses including $k$-demand and SPLC valuations.\n  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to\nguarantee envy-freeness online. Surprisingly, even for additive valuations, the\nminimum subsidy may be as large as $\\Omega(mn)$, in contrast to the offline\nsetting, where the bound is $O(n)$. On the positive side, we identify valuation\nclasses where the minimum subsidy is small (i.e., does not depend on $m$),\nincluding $k$-valued, rank-one, restricted additive, and identical valuations,\nand we obtain (mostly) tight subsidy bounds for these classes."}
{"id": "2510.13095", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13095", "abs": "https://arxiv.org/abs/2510.13095", "authors": ["Yingchen zhang", "Ruqing zhang", "Jiafeng Guo", "Wenjun Peng", "Sen Li", "Fuyu Lv"], "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval", "comment": null, "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers\n(docids) relevant to a given query. Prior works have focused on leveraging the\ngenerative capabilities of LLMs to improve GR, while overlooking that their\nreasoning capabilities could likewise help. This raises a key question: Can\nexplicit reasoning benefit GR? To investigate, we first conduct a preliminary\nstudy where an LLM is prompted to generate free-form chain-of-thought (CoT)\nreasoning before performing constrained docid decoding. Although this method\noutperforms standard GR, the generated reasoning tends to be verbose and poorly\naligned with the docid space. These limitations motivate the development of a\nreasoning mechanism better tailored to GR.\n  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented\nframework for GR that converts free-form CoT reasoning into a compact,\nstructured format, and iteratively refines the reasoning during the retrieval\nprocess. R4R augments an existing GR method by leveraging a reasoning-capable\nLLM that has been instruction-tuned for GR. At inference time, R4R first uses\nthe LLM to generate an initial structured reasoning; then the same LLM\nalternates between (i) constrained decoding with the chosen GR method to\nproduce candidate docids and (ii) updating the reasoning based on retrieval\nresults to improve the next round. R4R does not require additional models or\ntraining, and instead a single LLM serves as both the reasoning generator and\nthe retriever. Extensive experiments on Natural Questions, MS MARCO, and a\nreal-world item-search benchmark validate the effectiveness of R4R."}
{"id": "2510.13485", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13485", "abs": "https://arxiv.org/abs/2510.13485", "authors": ["Akash Kulkarni", "Rajshekhar V Bhat"], "title": "Non-Linear Precoding via Dirty Paper Coding for Near-Field Downlink MISO Communications", "comment": null, "summary": "In 6G systems, extremely large-scale antenna arrays operating at terahertz\nfrequencies extend the near-field region to typical user distances from the\nbase station, enabling near-field communication (NFC) with fine spatial\nresolution through beamfocusing. Existing multiuser NFC systems predominantly\nemploy linear precoding techniques such as zero-forcing (ZF), which suffer from\nperformance degradation due to the high transmit power required to suppress\ninterference. This paper proposes a nonlinear precoding framework based on\nDirty Paper Coding (DPC), which pre-cancels known interference to maximize the\nsum-rate performance. We formulate and solve the corresponding sum-rate\nmaximization problems, deriving optimal power allocation strategies for both\nDPC and ZF schemes. Extensive simulations demonstrate that DPC achieves\nsubstantial sum-rate gains over ZF across various near-field configurations,\nwith the most pronounced improvements observed for closely spaced users."}
{"id": "2510.13193", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13193", "abs": "https://arxiv.org/abs/2510.13193", "authors": ["Yikuan Hu", "Jifeng Zhu", "Lanrui Tang", "Chen Huang"], "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG", "comment": null, "summary": "Knowledge graphs (KGs), with their structured representation capabilities,\noffer promising avenue for enhancing Retrieval Augmented Generation (RAG)\nsystems, leading to the development of KG-RAG systems. Nevertheless, existing\nmethods often struggle to achieve effective synergy between system\neffectiveness and cost efficiency, leading to neither unsatisfying performance\nnor excessive LLM prompt tokens and inference time. To this end, this paper\nproposes REMINDRAG, which employs an LLM-guided graph traversal featuring node\nexploration, node exploitation, and, most notably, memory replay, to improve\nboth system effectiveness and cost efficiency. Specifically, REMINDRAG\nmemorizes traversal experience within KG edge embeddings, mirroring the way\nLLMs \"memorize\" world knowledge within their parameters, but in a train-free\nmanner. We theoretically and experimentally confirm the effectiveness of\nREMINDRAG, demonstrating its superiority over existing baselines across various\nbenchmark datasets and LLM backbones. Our code is available at\nhttps://github.com/kilgrims/ReMindRAG."}
{"id": "2510.13532", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13532", "abs": "https://arxiv.org/abs/2510.13532", "authors": ["Dushyantha A Basnayaka"], "title": "Simulating Mediumband Wireless Communication Systems: A Concise Description", "comment": "10 pages, 4 figures, and a MATLAB code included", "summary": "In this paper, we describe the necessary procedures for accurately simulating\ndigital wireless communication systems operating in the mediumband, aimed at\nboth beginners and experts. In the research literature, digital wireless\ncommunication systems are typically simulated in the discrete-time complex\nbaseband domain, where pulse shaping, upconversion, mixing, carrier\nsynchronization, and symbol timing synchronization are often ignored. These\nassumptions are indeed sufficient in most cases, but to capture the essence of\ncommunication in the mediumband, certain physical layer (PHY) operations should\nbe simulated in detail. In this paper, we concisely describe how to simulate a\nmediumband wireless communication scenario from a single transmitter (TX) to a\nsingle receiver (RX) in MATLAB, elaborating the operation of key PHY\nsubsystems. The approach described here ensures that the simulated system\ncaptures the delicate dynamics of mediumband wireless communication, including\nthe effect of deep fading avoidance."}
{"id": "2510.13217", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13217", "abs": "https://arxiv.org/abs/2510.13217", "authors": ["Nilesh Gupta", "Wei-Cheng Chang", "Ngot Bui", "Cho-Jui Hsieh", "Inderjit S. Dhillon"], "title": "LLM-guided Hierarchical Retrieval", "comment": null, "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation."}
{"id": "2510.13661", "categories": ["cs.IT", "cs.CR", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13661", "abs": "https://arxiv.org/abs/2510.13661", "authors": ["Emmanouil M. Athanasakos", "Nicholas Kalouptsidis", "Hariprasad Manjunath"], "title": "Local Information-Theoretic Security via Euclidean Geometry", "comment": "48 pages, 12 figures, submitted to IEEE Transactions on Information\n  Theory", "summary": "This paper introduces a methodology based on Euclidean information theory to\ninvestigate local properties of secure communication over discrete memoryless\nwiretap channels. We formulate a constrained optimization problem that\nmaximizes a legitimate user's information rate while imposing explicit upper\nbounds on both the information leakage to an eavesdropper and the informational\ncost of encoding the secret message. By leveraging local geometric\napproximations, this inherently non-convex problem is transformed into a\ntractable quadratic programming structure. It is demonstrated that the optimal\nLagrange multipliers governing this approximated problem can be found by\nsolving a linear program. The constraints of this linear program are derived\nfrom Karush-Kuhn-Tucker conditions and are expressed in terms of the\ngeneralized eigenvalues of channel-derived matrices. This framework facilitates\nthe derivation of an analytical formula for an approximate local secrecy\ncapacity. Furthermore, we define and analyze a new class of secret local\ncontraction coefficients. These coefficients, characterized as the largest\ngeneralized eigenvalues of a matrix pencil, quantify the maximum achievable\nratio of approximate utility to approximate leakage, thus measuring the\nintrinsic local leakage efficiency of the channel. We establish bounds\nconnecting these local coefficients to their global counterparts defined over\ntrue mutual information measures. The efficacy of the proposed framework is\ndemonstrated through detailed analysis and numerical illustrations for both\ngeneral multi-mode channels and the canonical binary symmetric wiretap channel."}
{"id": "2510.13229", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13229", "abs": "https://arxiv.org/abs/2510.13229", "authors": ["Yi Zhang", "Lili Xie", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation", "comment": "ICDM 2025 Accepted Paper", "summary": "Recommender systems (RecSys) have become critical tools for enhancing user\nengagement by delivering personalized content across diverse digital platforms.\nRecent advancements in large language models (LLMs) demonstrate significant\npotential for improving RecSys, primarily due to their exceptional\ngeneralization capabilities and sophisticated contextual understanding, which\nfacilitate the generation of flexible and interpretable recommendations.\nHowever, the direct deployment of LLMs as primary recommendation policies\npresents notable challenges, including persistent latency issues stemming from\nfrequent API calls and inherent model limitations such as hallucinations and\nbiases. To address these issues, this paper proposes a novel offline\nreinforcement learning (RL) framework that leverages imitation learning from\nLLM-generated trajectories. Specifically, inverse reinforcement learning is\nemployed to extract robust reward models from LLM demonstrations. This approach\nnegates the need for LLM fine-tuning, thereby substantially reducing\ncomputational overhead. Simultaneously, the RL policy is guided by the\ncumulative rewards derived from these demonstrations, effectively transferring\nthe semantic insights captured by the LLM. Comprehensive experiments conducted\non two benchmark datasets validate the effectiveness of the proposed method,\ndemonstrating superior performance when compared against state-of-the-art\nRL-based and in-context learning baselines. The code can be found at\nhttps://github.com/ArronDZhang/IL-Rec."}
{"id": "2510.13775", "categories": ["cs.IT", "math.CA", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13775", "abs": "https://arxiv.org/abs/2510.13775", "authors": ["Joshua Brakensiek", "Yeyuan Chen", "Manik Dhar", "Zihan Zhang"], "title": "Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb Inequalities", "comment": "27 pages", "summary": "In coding theory, the problem of list recovery asks one to find all codewords\n$c$ of a given code $C$ which such that at least $1-\\rho$ fraction of the\nsymbols of $c$ lie in some predetermined set of $\\ell$ symbols for each\ncoordinate of the code. A key question is bounding the maximum possible list\nsize $L$ of such codewords for the given code $C$.\n  In this paper, we give novel combinatorial bounds on the list recoverability\nof various families of linear and folded linear codes, including random linear\ncodes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and\nexplicit univariate multiplicity codes. Our main result is that in all of these\nsettings, we show that for code of rate $R$, when $\\rho = 1 - R - \\epsilon$\napproaches capacity, the list size $L$ is at most\n$(\\ell/(R+\\epsilon))^{O(R/\\epsilon)}$. These results also apply in the\naverage-radius regime. Our result resolves a long-standing open question on\nwhether $L$ can be bounded by a polynomial in $\\ell$. In the zero-error regime,\nour bound on $L$ perfectly matches known lower bounds.\n  The primary technique is a novel application of a discrete entropic\nBrascamp--Lieb inequality to the problem of list recovery, allowing us to\nrelate the local structure of each coordinate with the global structure of the\nrecovered list. As a result of independent interest, we show that a recent\nresult by Chen and Zhang (STOC 2025) on the list decodability of folded\nReed--Solomon codes can be generalized into a novel Brascamp--Lieb type\ninequality."}
{"id": "2510.13359", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13359", "abs": "https://arxiv.org/abs/2510.13359", "authors": ["Yuki Yada", "Sho Akiyama", "Ryo Watanabe", "Yuta Ueno", "Yusuke Shido", "Andre Rusli"], "title": "Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models", "comment": "Accepted to ACM RecSys 2025 (Spotlight)", "summary": "On large-scale e-commerce platforms with tens of millions of active monthly\nusers, recommending visually similar products is essential for enabling users\nto efficiently discover items that align with their preferences. This study\npresents the application of a vision-language model (VLM) -- which has\ndemonstrated strong performance in image recognition and image-text retrieval\ntasks -- to product recommendations on Mercari, a major consumer-to-consumer\nmarketplace used by more than 20 million monthly users in Japan. Specifically,\nwe fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using\none million product image-title pairs from Mercari collected over a three-month\nperiod, and developed an image encoder for generating item embeddings used in\nthe recommendation system. Our evaluation comprised an offline analysis of\nhistorical interaction logs and an online A/B test in a production environment.\nIn offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared\nwith the baseline. In the online A/B test, the click-through rate improved by\n50% whereas the conversion rate improved by 14% compared with the existing\nmodel. These results demonstrate the effectiveness of VLM-based encoders for\ne-commerce product recommendations and provide practical insights into the\ndevelopment of visual similarity-based recommendation systems."}
{"id": "2510.13777", "categories": ["cs.IT", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13777", "abs": "https://arxiv.org/abs/2510.13777", "authors": ["Joshua Brakensiek", "Yeyuan Chen", "Manik Dhar", "Zihan Zhang"], "title": "From Random to Explicit via Subspace Designs With Applications to Local Properties and Matroids", "comment": "41 pages", "summary": "In coding theory, a common question is to understand the threshold rates of\nvarious local properties of codes, such as their list decodability and list\nrecoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave\na novel unified framework for calculating the threshold rates of local\nproperties for random linear and random Reed--Solomon codes.\n  In this paper, we extend their framework to studying the local properties of\nsubspace designable codes, including explicit folded Reed-Solomon and\nunivariate multiplicity codes. Our first main result is a local equivalence\nbetween random linear codes and (nearly) optimal subspace design codes up to an\narbitrarily small rate decrease. We show any local property of random linear\ncodes applies to all subspace design codes. As such, we give the first explicit\nconstruction of folded linear codes that simultaneously attain all local\nproperties of random linear codes. Conversely, we show that any local property\nwhich applies to all subspace design codes also applies to random linear codes.\n  Our second main result is an application to matroid theory. We show that the\ncorrectable erasure patterns in a maximally recoverable tensor code can be\nidentified in deterministic polynomial time, assuming a positive answer to a\nmatroid-theoretic question due to Mason (1981). This improves on a result of\nJackson and Tanigawa (JCTB 2024) who gave a complexity characterization of\n$\\mathsf{RP} \\cap \\mathsf{coNP}$ assuming a stronger conjecture. Our result\nalso applies to the generic bipartite rigidity and matrix completion matroids.\n  As a result of additional interest, we study the existence and limitations of\nsubspace designs. In particular, we tighten the analysis of family of subspace\ndesigns constructioned by Guruswami and Kopparty (Combinatorica 2016) and show\nthat better subspace designs do not exist over algebraically closed fields."}
{"id": "2510.13371", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13371", "abs": "https://arxiv.org/abs/2510.13371", "authors": ["Jiin Park", "Misuk Kim"], "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation", "comment": "18 pages", "summary": "Recent attempts to integrate large language models (LLMs) into recommender\nsystems have gained momentum, but most remain limited to simple text generation\nor static prompt-based inference, failing to capture the complexity of user\npreferences and real-world interactions. This study proposes the Multi-Aspect\nDriven LLM Agent MADRec, an autonomous LLM-based recommender that constructs\nuser and item profiles by unsupervised extraction of multi-aspect information\nfrom reviews and performs direct recommendation, sequential recommendation, and\nexplanation generation. MADRec generates structured profiles via\naspect-category-based summarization and applies Re-Ranking to construct\nhigh-density inputs. When the ground-truth item is missing from the output, the\nSelf-Feedback mechanism dynamically adjusts the inference criteria. Experiments\nacross multiple domains show that MADRec outperforms traditional and LLM-based\nbaselines in both precision and explainability, with human evaluation further\nconfirming the persuasiveness of the generated explanations."}
{"id": "2510.13330", "categories": ["cs.DS", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13330", "abs": "https://arxiv.org/abs/2510.13330", "authors": ["Bridget Smart", "Max Ward", "Matthew Roughan"], "title": "A faster algorithm for efficient longest common substring calculation for non-parametric entropy estimation in sequential data", "comment": "Package available at https://pypi.org/project/LCSFinder, codebase\n  available at https://github.com/bridget-smart/LCSFinder. It also includes a\n  heuristic variant offering excellent practical performance with slower\n  worst-case complexity, and has been integrated into the ProcessEntropy\n  (https://pypi.org/project/ProcessEntropy/) package", "summary": "Non-parametric entropy estimation on sequential data is a fundamental tool in\nsignal processing, capturing information flow within or between processes to\nmeasure predictability, redundancy, or similarity. Methods based on longest\ncommon substrings (LCS) provide a non-parametric estimate of typical set size\nbut are often inefficient, limiting use on real-world data. We introduce\nLCSFinder, a new algorithm that improves the worst-case performance of LCS\ncalculations from cubic to log-linear time. Although built on standard\nalgorithmic constructs - including sorted suffix arrays and persistent binary\nsearch trees - the details require care to provide the matches required for\nentropy estimation on dynamically growing sequences. We demonstrate that\nLCSFinder achieves dramatic speedups over existing implementations on real and\nsimulated data, enabling entropy estimation at scales previously infeasible in\npractical signal processing."}
{"id": "2510.13590", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13590", "abs": "https://arxiv.org/abs/2510.13590", "authors": ["Jiale Han", "Austin Cheung", "Yubai Wei", "Zheng Yu", "Xusheng Wang", "Bing Zhu", "Yi Yang"], "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge", "comment": null, "summary": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates."}
{"id": "2510.13738", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13738", "abs": "https://arxiv.org/abs/2510.13738", "authors": ["Jingyi Zhou", "Cheng Chen", "Kai Zuo", "Manjie Xu", "Zhendong Fu", "Yibo Chen", "Xu Tang", "Yao Hu"], "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems."}
