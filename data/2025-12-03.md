<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 14]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Weight distributions of simplex codes over finite chain rings and their Gray map images](https://arxiv.org/abs/2512.02149)
*Cristina Fernández-Córdoba,Sergi Sánchez-Aragón,Mercè Villanueva*

Main category: cs.IT

TL;DR: 本文研究了有限链环R上的线性单纯形码及其对应的R-线性单纯形码，分析了它们的基本参数和完全重量分布，并检验了它们相对于Griesmer型界的优化性。


<details>
  <summary>Details</summary>
Motivation: 将线性码从有限域推广到有限链环，特别是从Z_{p^s}推广到更一般的有限链环R，研究这类环上的单纯形码构造及其性质。

Method: 构造有限链环R上的线性单纯形码及其对应的R-线性单纯形码（类型α和β），分析它们的基本参数如最小汉明距离和完全重量分布。

Result: 给出了单纯形码的构造方法，确定了它们的基本参数，包括最小汉明距离和完全重量分布，并检验了这些码相对于Griesmer型界的优化性。

Conclusion: 成功将单纯形码从有限域推广到有限链环，建立了相应的理论框架，并分析了这些码的优化性质。

Abstract: A linear code of length $n$ over a finite chain ring $R$ with residue field $\F_q$ is a $R$-submodule of $R^n$. A $R$-linear code is a code over $\F_q$ (not necessarily linear) which is the generalized Gray map image of a linear code over $R$. These codes can be seen as a generalization of the linear codes over $\Z_{p^s}$ with $p$ prime and $s \geq 1$. In this paper, we present the construction of linear simplex codes over $R$ and their corresponding $R$-linear simplex codes of type $α$ and $β$. Moreover, we show the fundamental parameters of these codes, including their minimum Hamming distance, as well as their complete weight distributions. We also study whether these simplex codes are optimal with respect to the Griesmer-type bound.

</details>


### [2] [Low-Power Double RIS-Assisted Mobile LEO Satellite Communications](https://arxiv.org/abs/2512.02255)
*Kunnathully Sadanandan Sanila,Rickard Nilsson,Emad Ibrahim,Neelakandan Rajamohan*

Main category: cs.IT

TL;DR: 提出采用双可重构智能表面（RIS）的低功耗移动LEO卫星通信架构，通过近场和远场效应优化能量效率和信号性能


<details>
  <summary>Details</summary>
Motivation: 在能量受限的LEO卫星通信场景中，需要降低功耗并提高信号性能，传统方法难以满足要求

Method: 设计双RIS架构，一个RIS位于卫星天线近场，另一个位于用户近场，满足远场要求；建立考虑近场和远场效应的路径损耗模型；采用双级波束成形技术

Result: 仿真结果显示，在上行链路中，使用0.25平方米的小型RIS可将功耗降低40dB

Conclusion: 双RIS架构能有效降低LEO卫星通信系统的功耗，提高能量效率，适用于能量受限的通信环境

Abstract: We propose a low-power mobile low earth orbit (LEO) satellite communication architecture, employing double reconfigurable intelligent surfaces (RIS) to enhance energy efficiency and signal performance. With a distance between RISs that satisfies the far-field requirement, this architecture positions one small RIS each in the near-field of the satellite's antenna and the user on the ground. Moreover, we develop a path loss model for the double-RIS communication link, considering the near-field and far-field effects. Further, with the help of dual-stage beamforming, the proposed system maximizes the signal power and minimizes power consumption. Simulation results show that the proposed architecture can reduce the power consumption with 40 dB in the uplink, with a small $0.25^2$ $\text{m}^2$ RIS near the user, to communicate in energy-constrained LEO satellite communication circumstances.

</details>


### [3] [Entropies associated with orbits of finite groups](https://arxiv.org/abs/2512.02257)
*Ryan Leal,Jingtong Sun,Juan Pablo Vigneaux*

Main category: cs.IT

TL;DR: 论文探讨了有限反射群和李型群（如辛群）的轨道计数与熵函数之间的渐近关系，将信息论公式推广到对称群和一般线性群之外的新群系列。


<details>
  <summary>Details</summary>
Motivation: 传统信息论公式主要对应对称群（反射群）和一般线性群（李型群）的商群，其轨道计数分别与Shannon熵和Tsallis 2-熵相关。本文旨在将这种信息论视角扩展到其他有限反射群和李型群系列，探索它们对应的新熵函数。

Method: 通过研究有限反射群（按Dynkin图分类为A_n、B_n、C_n、D_n系列和例外群）以及有限域上辛群等李型群的抛物子群商群。分析轨道基数的渐近行为，将乘法"链式法则"转化为熵的加法关系。

Result: 发现A_n系列（对称群和一般线性群）分别对应Shannon熵和Tsallis 2-熵。首次从信息论角度研究其他系列（B_n、C_n、D_n等），揭示了它们与新的熵函数之间的关联。

Conclusion: 将信息论框架扩展到更广泛的群结构，建立了有限反射群和李型群的轨道计数与熵函数之间的系统对应关系，为信息论提供了新的数学基础。

Abstract: For certain groups, parabolic subgroups appear as stabilizers of flags of sets or vector spaces. Quotients by these parabolic subgroups represent orbits of flags, and their cardinalities asymptotically reveal entropies (as rates of exponential or superexponential growth). The multiplicative "chain rules" that involve these cardinalities induce, asymptotically, additive analogues for entropies. Many traditional formulas in information theory correspond to quotients of symmetric groups, which are a particular kind of reflection group; in this case, the cardinalities of orbits are given by multinomial coefficients and are asymptotically related to Shannon entropy. One can treat similarly quotients of the general linear groups over a finite field; in this case, the cardinalities of orbits are given by $q$-multinomials and are asymptotically related to the Tsallis 2-entropy. In this contribution, we consider other finite reflection groups as well as the symplectic group as an example of a classical group over a finite field (groups of Lie type). In both cases, the groups are classified by Dynkin diagrams into infinite series of similar groups $A_n$, $B_n$, $C_n$, $D_n$ and a finite number of exceptional ones. The $A_n$ series consists of the symmetric groups (reflection case) and general linear groups (Lie case). Some of the other series, studied here from an information-theoretic perspective for the first time, are linked to new entropic functionals.

</details>


### [4] [New Constructions of Non-GRS MDS Codes, Recovery and Determination Algorithms for GRS Codes](https://arxiv.org/abs/2512.02325)
*Guodong Wang,Hongwei Liu,Jinquan Luo*

Main category: cs.IT

TL;DR: 提出构造非GRS MDS码的新方法，最大长度可达(q+3)/2（奇特征）和(q+4)/2（偶特征），并设计高效算法解决GRS码识别与密钥恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现有非GRS MDS码构造方法有限，需要更长的码长和更高效的GRS码识别算法。传统Sidelnikov-Shestakov攻击计算复杂度高，需要改进。

Method: 利用柯西矩阵方法构造特殊结构的非GRS MDS码，分析其充要条件。设计两个高效算法：1) 从生成矩阵判断是否为GRS码；2) 如果是GRS码则恢复密钥向量α和v。

Result: 成功构造了长度可达(q+3)/2（奇特征）和(q+4)/2（偶特征）的非GRS MDS码。算法复杂度为O(nk+n)，显著优于传统Sidelnikov-Shestakov攻击的O(qk²n+qk³)。

Conclusion: 提出的方法能构造更长的非GRS MDS码，且设计的算法在GRS码识别和密钥恢复方面具有显著的计算效率优势，为GRS码研究提供了新工具。

Abstract: In this paper, we propose a new method for constructing a class of non-GRS MDS codes. The lengths of these codes can reach up to $\frac{q+3}{2}$ (for finite fields of odd characteristic) and $\frac{q+4}{2}$ (for even characteristic), respectively. Owing to their special structure, we can use the Cauchy matrix method to obtain the necessary and sufficient conditions for these codes to be MDS codes and non-GRS MDS codes. Additionally, the inequivalence between these codes and twisted GRS codes is analyzed. Furthermore, we analyze the relationships among several existing classes of codes used for constructing non-GRS MDS codes, propose explicit constructions, and discuss the lengths of non-GRS MDS codes based on these constructions. Finally, we design two efficient algorithms to address two main problems in GRS code research, i.e., determining whether an unknown code $C$ is a GRS code from its generator matrix $G$, and recovering the key vectors $\bmα$ and $\bm{v}$ such that $C = \GRS_{n,k}(\bmα, \bm{v})$ if $C$ is indeed a GRS code. A computational complexity comparison of the proposed algorithms ($O(nk+n)$) with that of the Sidelnikov-Shestakov attack (exceeding $O(qk^2n+qk^3)$) shows that our methods offer superior computational efficiency.

</details>


### [5] [Age of Information for Constrained Scheduling with Imperfect Feedback](https://arxiv.org/abs/2512.02332)
*Yuqing Zhu,Yuan-Hsun Lo,Yan Lin,Yijin Zhang*

Main category: cs.IT

TL;DR: 论文研究了具有不完美反馈和受限传输速率的下行链路系统中多源状态监控的AoI优化调度算法，针对零反馈和伯努利流量分别提出了基于速率分割和Lyapunov优化的策略。


<details>
  <summary>Details</summary>
Motivation: 实际系统中存在不完美反馈和受限传输速率这两个关键限制因素，需要设计调度算法来优化无限时间范围内的信息年龄(AoI)。

Method: 针对零反馈生成即发流量，推导了可达到AoI的闭式下界，提出了结合速率分割和模运算的策略；针对零反馈伯努利流量，基于Lyapunov优化理论开发了具有阈值结构的漂移加惩罚(DPP)策略；并将DPP策略扩展到支持一般不完美反馈。

Result: 提出的策略在许多情况下能够达到零反馈生成即发流量的下界；为伯努利流量提供了闭式性能保证；数值结果验证了理论分析和所提策略相对于现有策略的AoI优势。

Conclusion: 论文成功解决了具有不完美反馈和受限传输速率的下行链路系统中的AoI优化问题，提出的调度算法在理论和实际性能上都表现出色。

Abstract: This paper considers a downlink system where an access point sends the monitored status of multiple sources to multiple users. By jointly accounting for imperfect feedback and constrained transmission rate, which are key limited factors in practical systems, we aim to design scheduling algorithms to optimize the age of information (AoI) over the infinite time horizon. For zero feedback under the generate-at-will traffic, we derive a closed-form lower bound of achievable AoI, which, to the best of our knowledge, reflects the impact of zero feedback for the first time, and propose a policy that achieves this bound in many cases by jointly applying rate splitting and modular arithmetic. For zero feedback under the Bernoulli traffic, we develop a drift-plus-penalty (DPP) policy with a threshold structure based on the theory of Lyapunov optimization and provide a closed-form performance guarantee. Furthermore, we extend the design of this DPP policy to support general imperfect feedback without increasing the online computational complexity. Numerical results verify our theoretical analysis and the AoI advantage of the proposed policies over state-of-the-art policies.

</details>


### [6] [A Cyclic Shift Embedded Pilot based Channel Estimation for Multi-User MIMO-OTFS systems with fractional delay and Doppler](https://arxiv.org/abs/2512.02353)
*Ruizhe Wang,Hong Ren,Cunhua Pan,Ruisong Weng,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 本文提出了一种用于多用户MIMO-OTFS系统的循环移位嵌入导频结构和基于多维分解的信道估计算法，显著降低导频开销并提高估计性能。


<details>
  <summary>Details</summary>
Motivation: 多用户OTFS系统中，传统嵌入式导频方案需要为每个用户独立分配导频，导致导频开销线性增加。为了在高移动性场景下实现可靠通信，需要解决多用户MIMO-OTFS系统的信道估计和导频设计问题。

Method: 1. 提出基于多维分解的信道估计算法：首先通过子空间分解估计到达角，构建空间投影矩阵解耦接收信号；然后使用压缩感知方法估计剩余的小数时延和多普勒频移。2. 提出循环移位嵌入式导频结构：利用Zadoff-Chu序列的循环移位正交性实现用户复用，显著降低导频开销。

Result: 提出的CSEP结构相比传统嵌入式导频结构可节省超过30%的导频开销。仿真结果表明，基于CSEP结构的改进信道估计方法在信道估计方面表现出优越性能，在计算复杂度、估计精度和误码率性能之间取得了良好平衡。

Conclusion: 本文提出的循环移位嵌入式导频结构和多维分解信道估计算法有效解决了多用户MIMO-OTFS系统中的导频开销问题，实现了高性能的信道估计，为高移动性场景下的可靠通信提供了有效解决方案。

Abstract: Orthogonal time frequency space (OTFS) modulation has been proposed to meet the demand for reliable communication in high-mobility scenarios for future wireless networks. However, in multi-user OTFS systems, conventional embedded pilot schemes require independent pilot allocation for each user, leading to linearly increasing pilot overhead. To address these issues, in this paper, we investigate the uplink channel estimation and pilot design for multi-user multiple-input multiple-output (MIMO)-OTFS systems. We propose a multi-dimensional decomposition-based channel estimation algorithm. Specifically, the proposed algorithm first estimates the angles of arrivals (AoAs) via subspace decomposition-based method. A spatial projection matrix, constructed from the estimated AOAs, decouples the received signal by propagation path subspace, effectively mitigating inter-path interference. The remaining fractional delay and Doppler can be obtained by a compressed sensing (CS)-based off-grid channel estimation method. Furthermore, to reduce the pilot overhead in multi-user OTFS systems, this paper proposes a novel cyclic shift embedded pilot (CSEP) structure, which can reuse users through cyclic shift-orthogonality of Zadoff-Chu (ZC) sequences. Compared with conventional embedded pilot structures, the CSEP structure can save over 30\% of pilot overhead. Finally, an imporved channel estimation method based on the CSEP structure is proposed. Simulation results demonstrate that it achieves superior performance in channel estimation. Moreover, the proposed CSEP structure and channel estimation algorithm achieve a favorable balance between computational complexity, estimation accuracy, and bit error rate (BER) performance.

</details>


### [7] [Boltzmann-Shannon Index: A Geometric-Aware Measure of Clustering Balance](https://arxiv.org/abs/2512.02397)
*Emanuele Bossi,C. Tyler Diggans,Abd AlRahman R. AlMomani*

Main category: cs.IT

TL;DR: 提出了Boltzmann-Shannon指数（BSI），一种用于聚类连续数据的归一化度量，捕捉基于频率和基于几何的概率分布之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够同时考虑聚类的人口分布和几何范围的度量指标，传统指标在评估聚类质量时可能提供不完整或误导性信号。

Method: 基于几何粗粒化和信息论思想，BSI量化分区如何反映每个聚类的人口及其有效几何范围，通过频率分布和几何分布的相互作用来评估聚类质量。

Result: 在合成高斯混合、Iris基准和高不平衡资源分配场景中，BSI提供了连贯的评估，能够高灵敏度地检测密度-几何不一致性，并作为平滑、可优化的目标函数。

Conclusion: BSI是一种有效的聚类评估指标，特别适用于资源分配场景，能够平衡人口权重和几何分布，并可作为梯度友好的正则化器嵌入现代政策制定和算法治理优化框架。

Abstract: We introduce the Boltzmann-Shannon Index (BSI), a normalized measure for clustered continuous data that captures the interaction between frequency-based and geometry-based probability distributions. Building on ideas from geometric coarse-graining and information theory, the BSI quantifies how well a partition reflects both the population of each cluster and its effective geometric extent. We illustrate its behavior on synthetic Gaussian mixtures, the Iris benchmark, and a high-imbalance resource-allocation scenario, showing that the index provides a coherent assessment even when traditional metrics give incomplete or misleading signals. Moreover, in resource-allocation settings, we demonstrate that BSI not only detects severe density-geometry inconsistency with high sensitivity, but also offers a smooth, optimization-ready objective that naturally favors allocations balancing demographic weight with each group's effective spread in the outcome space, while providing a smooth, gradient-friendly regularizer that can be easily embedded in modern policy-making and algorithmic governance optimization frameworks.

</details>


### [8] [Optimal Handover Strategies in LEO Satellite Networks](https://arxiv.org/abs/2512.02449)
*Brendon McBain,Yi Hong,Emanuele Viterbo*

Main category: cs.IT

TL;DR: 提出一个通用分析框架，用于准确表征任意切换策略下LEO卫星网络的遍历容量，推导了持续容量及其上下界，并给出了最优切换决策规则。


<details>
  <summary>Details</summary>
Motivation: 现有卫星巨型星座的理论分析存在限制性假设（如短服务时间），或在实际切换策略评估时缺乏可处理性，需要开发更通用的分析框架。

Method: 将传输链路建模为阴影莱斯衰落，引入持续卫星信道概念，使用更新理论推导遍历容量，提出闭式上下界，将最优切换问题表述为非线性分数规划，采用Dinkelbach算法变体获得显式决策规则。

Result: 建立了持续容量的分析框架和上下界，证明了最大化服务容量的简化切换策略能很好地逼近最优策略，为设计高吞吐量LEO卫星通信系统提供了实用见解。

Conclusion: 该框架能准确表征任意切换策略下的LEO卫星网络容量，提出的简化切换策略具有实用价值，有助于设计高性能卫星通信系统。

Abstract: Existing theoretical analyses of satellite mega-constellations often rely on restrictive assumptions, such as short serving times, or lack tractability when evaluating realistic handover strategies. Motivated by these limitations, this paper develops a general analytical framework for accurately characterising the ergodic capacity of low Earth orbit (LEO) satellite networks under arbitrary handover strategies. Specifically, we model the transmission link as shadowed-Rician fading and introduce the persistent satellite channel, wherein the channel process is governed by an i.i.d. renewal process under mild assumptions of uncoordinated handover decisions and knowledge of satellite ephemeris and fading parameters. Within this framework, we derive the ergodic capacity (persistent capacity) of the persistent satellite channel using renewal theory and establish its relation to the non-persistent capacity studied in prior work. To address computational challenges, we present closed-form upper and lower bounds on persistent capacity. The optimal handover problem is formulated as a non-linear fractional program, obtaining an explicit decision rule via a variant of Dinkelbach's algorithm. We further demonstrate that a simpler handover strategy maximising serving capacity closely approximates the optimal strategy, providing practical insights for designing high-throughput LEO satellite communication systems.

</details>


### [9] [Artificial Noise Aided Physical Layer Security for Near-Field MIMO with Fluid Antenna Systems](https://arxiv.org/abs/2512.02461)
*Peng Zhang,Jian Dang,Miaowen Wen,Ziyang Liu,Chen Zhao,Huaifeng Shi,Chengsheng Pan,Zaichen Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种用于近场流体天线MIMO系统的人工噪声辅助物理层安全方案，通过交替优化框架联合设计波束成形和人工噪声，利用流体天线的几何和位置自由度显著提升保密性能。


<details>
  <summary>Details</summary>
Motivation: 随着无线系统向大规模阵列和高频可重构架构发展，近场流体天线系统为物理层安全提供了新的自由度。然而，现有方案在非极大阵列中仅依赖近场波束聚焦是不够的，需要更有效的安全增强机制。

Method: 提出交替优化框架：1) 连续波束成形/人工噪声联合设计子问题，通过广义谱注水法和块坐标下降获得闭式全数字解；2) 离散流体天线端口选择子问题，基于行能量剪枝-重拟合规则；3) 硬件高效的混合波束成形架构，在基带嵌入人工噪声无需额外射频链。

Result: 仿真结果表明，所提设计充分利用了流体天线的几何和位置自由度，显著提升了保密性能，特别是在非极大阵列中，近场波束聚焦单独使用时效果不足的情况。

Conclusion: 本文提出的方案有效解决了近场流体天线MIMO系统的物理层安全问题，通过联合波束成形和人工噪声设计，以及高效的端口选择机制，为大规模可重构天线系统的安全通信提供了实用解决方案。

Abstract: With the evolution of wireless systems toward large-scale arrays and high-frequency reconfigurable architectures, fluid antenna systems (FAS) operating in the near-field (NF) regime provide new degrees of freedom (DoF) for physical layer security (PLS). This paper proposes an artificial-noise (AN)-aided PLS scheme for NF fluid-antenna multiple-input multiple-output (FA-MIMO) systems, with joint beamforming (BF) and AN design for both compact and large arrays. An alternating-optimization (AO) framework addresses the sparsity-constrained non-convex design by splitting it into a continuous BF/AN joint-design subproblem and a discrete FAS port-selection subproblem. Closed-form fully digital BF/AN solutions are obtained via a generalized spectral water-filling procedure within a block coordinate descent (BCD) surrogate and realized by a hardware-efficient hybrid beamforming (HBF) architecture that embeds AN in the baseband without extra radio-frequency (RF) chains. For FAS port selection, a row-energy based prune--refit rule, aligned with Karush--Kuhn--Tucker (KKT) conditions of a group-sparsity surrogate, enables efficient active-port determination. Simulation results confirm that the proposed design exploits the geometry and position-domain DoF of FAS and significantly improves secrecy performance, particularly for non-extremely-large arrays where NF beam focusing alone is inadequate.

</details>


### [10] [Quantum Optimization in Wireless Communication Systems: Principles and Applications](https://arxiv.org/abs/2512.02468)
*Ioannis Krikidis,Valentin Gilbert*

Main category: cs.IT

TL;DR: 本文综述了量子优化在下一代无线通信系统设计中的应用，重点介绍了绝热量子计算原理及其两种主要计算模型：量子退火和门基量子近似优化算法，并通过无源可重构智能表面波束成形案例展示了实际量子硬件的实验结果。


<details>
  <summary>Details</summary>
Motivation: 量子优化有望解决下一代无线通信系统设计中的关键计算和技术挑战，通过利用量子计算能力来推动无线通信系统设计的进步。

Method: 基于绝热量子计算原理，探讨了两种主要计算模型：量子退火和门基量子近似优化算法，分析了它们的核心特征、性能优势、局限性和区别，并通过无源可重构智能表面波束成形设计作为案例研究。

Result: 将量子优化方法定位为推进无线通信系统设计的有前途工具，并通过实际量子硬件获得了支持性的实验结果。

Conclusion: 量子优化在无线通信系统设计中具有变革性潜力，量子退火和量子近似优化算法为解决复杂优化问题提供了新的途径，实际硬件实验验证了其可行性。

Abstract: Quantum optimization is poised to play a transformative role in the design of next-generation wireless communication systems by addressing key computational and technological challenges. This paper provides an overview of the principles of adiabatic quantum computing, the foundation of quantum optimization, and explores its two primary computational models: quantum annealing and the gate-based quantum approximate optimization algorithm. By highlighting their core features, performance benefits, limitations, and distinctions, we position these methods as promising tools for advancing wireless communication system design. As a case study, we examine the design of passive reconfigurable intelligent surface beamforming with binary phase-shift resolution, supported by experimental results obtained from real-world quantum hardware.

</details>


### [11] [Deep Q-Learning-Driven Power Control for Enhanced Noma User Performance](https://arxiv.org/abs/2512.02582)
*Bach Hung Luu,Sinh Cong Lam,Nam Hoang Nguyen*

Main category: cs.IT

TL;DR: 该论文提出了一种基于无人机辅助的蜂窝网络智能功率控制方案，通过距离阈值选择性地为边缘用户提供中继服务，使用DQN学习功率分配策略，相比无无人机辅助方案提升了3.6%的平均速率。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络中边缘用户由于距离基站远和物理障碍，信道条件差，数据速率远低于中心用户，存在性能差距问题需要解决。

Method: 提出无人机辅助蜂窝网络，采用距离基准标准：只有超过参考距离的用户获得无人机中继协助；无人机作为放大转发中继；使用深度Q网络学习框架优化基站传输功率分配，无需精确信道模型。

Result: 在最优参考距离400m时达到峰值平均速率2.28 bps/Hz，相比无无人机辅助网络提升3.6%，相比所有用户都获得无人机支持的网络提升0.9%；无人机高度和参考距离是关键影响因素，较低高度提供更好性能。

Conclusion: 基于距离的无人机选择性中继方案能有效改善边缘用户性能，智能功率控制策略在无需精确信道模型的情况下实现性能提升，无人机高度和参考距离的优化对系统性能至关重要。

Abstract: Cell-edge users (CEUs) in cellular networks typically suffer from poor channel conditions due to long distances from serving base stations and physical obstructions, resulting in much lower data rates compared to cell-center users (CCUs). This paper proposes an Unmanned Aerial Vehicles (UAV)-assisted cellular network with intelligent power control to address the performance gap between CEUs and CCUs. Unlike conventional approaches that either deploy UAVs for all users or use no UAV assistance, our model uses a distance-based criterion where only users beyond a reference distance receive UAV relay assistance. Each UAV operates as an amplify-and-forward relay, enabling assisted users to receive signals from both the base station and the UAV simultaneously, thereby achieving diversity gain. To optimize transmission power allocation across base stations, we employ a Deep Q-Network (DQN) learning framework that learns power control policies without requiring accurate channel models. Simulation results show that the proposed approach achieves a peak average rate of 2.28 bps/Hz at the optimal reference distance of 400m, which represents a 3.6% improvement compared to networks without UAV assistance and 0.9% improvement compared to networks where all users receive UAV support. The results also reveal that UAV altitude and reference distance are critical factors affecting system performance, with lower altitudes providing better performance.

</details>


### [12] [Digit-Indexed q-ary SEC-DED Codes with Near-Hamming Overhead](https://arxiv.org/abs/2512.02747)
*Jiaxu Hu,Kenneth J. Roche*

Main category: cs.IT

TL;DR: 提出基于坐标索引的p进制数字构造的q元线性码，具有单纠双检能力，使用r+1个校验位，支持常数时间解码，并可扩展为距离4的SEC-TED码


<details>
  <summary>Details</summary>
Motivation: 现有纠错码实现复杂，需要设计一种实现简单、阵列友好的编码方案，具有明确的校验结构和从校验子到错误位置的显式映射

Method: 基于坐标索引的p进制数字构造校验矩阵，使用r+1个校验位，开发了两种扩展：A1码去除冗余三元组提高信息率；A2码加入两个组和校验和3-wise XOR线性独立条件，得到距离4的三元码

Result: 构造了具有近汉明开销的SEC-DED码，支持单遍常数时间解码，扩展得到SEC-TED码，框架可推广到距离d=n+1的码，包括恢复三元Golay码

Conclusion: 贡献在于实现简单性和阵列友好结构，校验基于数字和全局和，校验子到错误位置映射明确，SEC-TED升级模块化，与经典q元汉明码和SPC/乘积码相比具有优势

Abstract: We present a simple $q$-ary family of single-error-correcting, double-error-detecting (SEC--DED) linear codes whose parity checks are tied directly to the base-$p$ ($q=p$ prime) digits of the coordinate index. For blocklength $n=p^r$ the construction uses only $r+1$ parity checks -- \emph{near-Hamming} overhead -- and admits an index-based decoder that runs in a single pass with constant-time location and magnitude recovery from the syndromes. Based on the prototype, we develop two extensions: Code A1, which removes specific redundant trits to achieve higher information rate and support variable-length encoding; and Code A2, which incorporates two group-sum checks together with a 3-wise XOR linear independence condition on index subsets, yielding a ternary distance-4 (SEC--TED) variant. Furthermore, we demonstrate how the framework generalizes via $n$-wise XOR linearly independent sets to construct codes with distance $d = n + 1$, notably recovering the ternary Golay code for $n = 5$ -- showing both structural generality and a serendipitous link to optimal classical codes.
  Our contribution is not optimality but \emph{implementational simplicity} and an \emph{array-friendly} structure: the checks are digitwise and global sums, the mapping from syndromes to error location is explicit, and the SEC--TED upgrade is modular. We position the scheme against classical $q$-ary Hamming and SPC/product-code baselines and provide a small comparison of parity overhead, decoding work, and two-error behavior.

</details>


### [13] [Structural Properties of Entropic Vectors and Stability of the Ingleton Inequality](https://arxiv.org/abs/2512.02767)
*Rostislav Matveev,Andrei Romashchenko*

Main category: cs.IT

TL;DR: 研究熵框架下Ingleton不等式的约束版本及其在条件独立性小违反时的稳定性


<details>
  <summary>Details</summary>
Motivation: 经典Ingleton不等式对一般熵分布不成立，但在某些精确独立性约束下成立。本文研究当条件互信息项很小（但不为零）时，不等式在可控误差范围内继续成立的情况。

Method: 使用一个结构引理，该引理实现了两个随机变量之间部分互信息的具体化，隐式捕获了无限多个非Shannon型不等式的影响，从而获得概念上透明的证明。

Result: 获得了Ingleton不等式在小违反条件独立性时的稳定性界限，部分结果统一恢复了Matúš(2007)和Dougherty-Freiling-Zeger(2011)的无限不等式族，部分结果是新的。

Conclusion: 通过结构引理方法，无需显式调用无限不等式族，就能获得Ingleton不等式在条件独立性小违反时的稳定性结果，提供了更简洁的证明框架。

Abstract: We study constrained versions of the Ingleton inequality in the entropic setting and quantify its stability under small violations of conditional independence. Although the classical Ingleton inequality fails for general entropy profiles, it is known to hold under certain exact independence constraints. We focus on the regime where selected conditional mutual information terms are small (but not zero), and the inequality continues to hold up to controlled error terms. A central technical tool is a structural lemma that materializes part of the mutual information between two random variables, implicitly capturing the effect of infinitely many non-Shannon--type inequalities. This leads to conceptually transparent proofs without explicitly invoking such infinite families. Some of our bounds recover, in a unified way, what can also be deduced from the infinite families of inequalities of Matúš (2007) and of Dougherty--Freiling--Zeger (2011), while others appear to be new.

</details>


### [14] [Pseudocodewords of quantum, quasi-cyclic, and spatially-coupled LDPC codes: a fundamental cone perspective](https://arxiv.org/abs/2512.02941)
*Wittawat Kositwattanarerk,Gretchen L. Matthews,Emily McMillon,Tunchanok Yutitumsatit*

Main category: cs.IT

TL;DR: 本文分析了量子稳定子码、准循环LDPC码和空间耦合LDPC码在LP解码下产生的伪码字结构


<details>
  <summary>Details</summary>
Motivation: 虽然LDPC码与迭代解码器结合时接近容量极限，但由于伪码字的存在，这些解码器可能无法输出有效码字。伪码字的研究有助于理解现代解码器（包括迭代解码和线性规划解码）的性能，而伪码字依赖于奇偶校验矩阵和具体解码算法。

Method: 采用线性规划（LP）解码方法，该方法与图覆盖解码相关联，能够捕获伪码字。重点分析量子稳定子码、准循环LDPC码和空间耦合LDPC码在LP解码下产生的伪码字底层结构。

Result: 通过LP解码分析，揭示了不同类型LDPC码（量子稳定子码、准循环LDPC码、空间耦合LDPC码）中伪码字的特性和结构特征。

Conclusion: 伪码字的结构分析对于理解LDPC码解码性能至关重要，特别是对于量子稳定子码、准循环LDPC码和空间耦合LDPC码等特定类型的编码方案，LP解码提供了研究这些伪码字的有力工具。

Abstract: While low-density parity-check (LDPC) codes are near capacity-achieving when paired with iterative decoders, these decoders may not output a codeword due to the existence of pseudocodewords. Thus, pseudocodewords have been studied to give insight into the performance of modern decoders including iterative and linear programming decoders. These pseudocodewords are found to be dependent on the parity-check matrix of the code and the particular decoding algorithm used. In this paper, we consider LP decoding, which has been linked to graph cover decoding, providing functions which capture these pseudocodewords. In particular, we analyze the underlying structure of pseudocodewords from quantum stabilizer codes that arise from LP decoding, quasi-cyclic LDPC codes, and spatially-coupled LDPC codes.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [15] [On the Complexity of Signed Roman Domination](https://arxiv.org/abs/2512.02083)
*Sangam Balchandar Reddy*

Main category: cs.DS

TL;DR: 本文研究了带符号罗马支配函数问题的复杂性，证明了其在分裂图上的NP完全性，并给出了参数化复杂性的完整分析


<details>
  <summary>Details</summary>
Motivation: 带符号罗马支配函数问题在一般图、二部图和平面图上已被证明是NP完全的，但尚未研究其在分裂图上的复杂性。同时，该问题的参数化复杂性也缺乏系统研究，需要探索不同参数下的计算复杂性

Method: 通过归约证明该问题在分裂图上的NP完全性；使用参数化复杂性理论分析不同参数（权重、反馈顶点集数、邻域多样性、顶点覆盖数）下的计算复杂性；证明W[2]-hard和W[1]-hard结果；设计基于邻域多样性的FPT算法；证明无多项式核结果

Result: 1. 带符号罗马支配函数问题在分裂图上是NP完全的；2. 参数化为权重时是W[2]-hard（即使在二部图上）；3. 参数化为反馈顶点集数（以及树宽、团宽）时是W[1]-hard；4. 参数化为邻域多样性（或顶点覆盖数）时存在FPT算法；5. 参数化为顶点覆盖数时不存在多项式核（除非coNP ⊆ NP/poly）

Conclusion: 本文全面刻画了带符号罗马支配函数问题的计算复杂性，填补了分裂图上复杂性研究的空白，并建立了完整的参数化复杂性图谱，为该问题的算法设计提供了理论指导

Abstract: Given a graph $G = (V, E)$, a signed Roman dominating function is a function $f: V \rightarrow \{-1, 1, 2\}$ such that for every vertex $u \in V$: $\sum_{v \in N[u]} f(v) \geq 1$ and for every vertex $u \in V$ with $f(u) = -1$, there exists a vertex $v \in N(u)$ with $f(v) = 2$. The weight of a signed Roman dominating function $f$ is $\sum_{u \in V} f(u)$. The objective of \srd{} (SRD) problem is to compute a signed Roman dominating function with minimum weight. The problem is known to be NP-complete even when restricted to bipartite graphs and planar graphs. In this paper, we advance the complexity study by showing that the problem remains NP-complete on split graphs. In the realm of parameterized complexity, we prove that the problem is W[2]-hard parameterized by weight, even on bipartite graphs. We further show that the problem is W[1]-hard parameterized by feedback vertex set number (and hence also when parameterized by treewidth or clique-width). On the positive side, we present an FPT algorithm parameterized by neighbourhood diversity (and by vertex cover number). Finally, we complement this result by proving that the problem does not admit a polynomial kernel parameterized by vertex cover number unless coNP $\subseteq$ NP/poly.

</details>


### [16] [Optimal-Length Labeling Schemes and Fast Algorithms for k-gathering and k-broadcasting](https://arxiv.org/abs/2512.02252)
*Adam Ganczorzand Tomasz Jurdzinski*

Main category: cs.DS

TL;DR: 本文研究了无线网络中的k-广播和k-收集问题，在分布式算法建议框架下，证明了k-收集问题的最优建议大小与k-广播相同，均为Θ(min(logΔ, logk))，并设计了具有渐进最优建议大小的快速算法。


<details>
  <summary>Details</summary>
Motivation: 研究无线网络中基本的通信任务：k-广播（k个源节点的消息需要到达所有节点）和k-收集（从k个源节点收集消息到指定的汇聚节点）。在分布式算法建议框架下，探索这些问题的解决方案，特别是确定最优建议大小并设计高效的分布式算法。

Method: 采用分布式算法建议框架，为k-收集问题设计标签方案和算法。首先证明k-收集问题的最优建议大小边界，然后设计在D+k轮内完成的快速算法（D为通信图直径）。将k-收集算法应用于k-广播问题，获得O(D+log²n+k)轮的时间复杂度算法。比较具有最优建议大小的分布式算法与具有不同任意标签的分布式算法之间的性能差异。

Result: 证明了k-收集问题的最优建议大小与k-广播相同，均为Θ(min(logΔ, logk))，其中Δ为通信图的最大顶点度数。设计了k-收集算法，在D+k轮内完成，这个时间边界即使对于集中式算法也是最优的。将k-收集算法应用于k-广播，获得了O(D+log²n+k)轮的算法。展示了具有最优建议大小的分布式算法与具有不同任意标签的分布式算法之间存在对数级的时间复杂度差距。

Conclusion: 本文在分布式算法建议框架下，为无线网络中的k-广播和k-收集问题提供了统一的理论边界和高效算法。证明了两个问题具有相同的最优建议大小边界Θ(min(logΔ, logk))，并设计了时间最优的k-收集算法。研究结果表明，适当设计的建议可以显著提高分布式算法的性能，揭示了建议大小与算法效率之间的重要关系。

Abstract: We consider basic communication tasks in arbitrary radio networks: $k$-broadcasting and $k$-gathering. In the case of $k$-broadcasting messages from $k$ sources have to get to all nodes in the network. The goal of $k$-gathering is to collect messages from $k$ source nodes in a designated sink node. We consider these problems in the framework of distributed algorithms with advice.
  risko and Miller showed in 2021 that the optimal size of advice for $k$-broadcasting is $Θ(\min(\log Δ,$ $ \log k))$, where $Δ$ is equal to the maximum degree of a vertex of the input communication graph. We show that the same bound $Θ(\min(\log Δ, \log k))$ on the size of optimal labeling scheme holds also for the $k$-gathering problems. Moreover, we design fast algorithms for both problems with asymptotically optimal size of advice. For $k$-gathering our algorithm works in at most $D+k$ rounds, where $D$ is the diameter of the communication graph. This time bound is optimal even for centralized algorithms. We apply the $k$-gathering algorithm for $k$-broadcasting to achieve an algorithm working in time $O(D+\log^2 n+k)$ rounds. We also exhibit a logarithmic time complexity gap between distributed algorithms with advice of optimal size and distributed algorithms with distinct arbitrary labels.

</details>


### [17] [Markov Chains Approximate Message Passing](https://arxiv.org/abs/2512.02384)
*Amit Rajaraman,David X. Wu*

Main category: cs.DS

TL;DR: 该论文研究了尖峰Wigner推理问题，通过分析受限高斯动力学(RGD)将Glauber动力学与近似消息传递(AMP)联系起来，证明了在适当条件下RGD能够达到贝叶斯最优性能。


<details>
  <summary>Details</summary>
Motivation: 虽然马尔可夫链蒙特卡洛算法在各种贝叶斯推理设置中表现出接近最优的性能，但建立支持这些经验观察的严格理论一直具有挑战性。本文旨在为Glauber动力学在尖峰Wigner推理问题中的性能提供理论支持。

Method: 研究经典的尖峰Wigner推理问题，分析Glauber动力学在退火后验上的恢复性能，并将其与近似消息传递(AMP)的性能联系起来。主要方法是通过分析一个称为受限高斯动力学(RGD)的辅助马尔可夫链，建立RGD与AMP迭代演化之间的对应关系。

Result: 1. RGD可以简化为一个有效的一维递归，该递归反映了AMP迭代的演化过程；2. 从热启动开始，RGD在相关空间中快速收敛到一个固定点，当在后验上运行时能够恢复贝叶斯最优性能；3. 在广泛接受的SK模型混合结果条件下，恢复了非平凡推理的相变。

Conclusion: 该研究通过分析RGD建立了Glauber动力学与AMP之间的联系，为理解MCMC算法在贝叶斯推理中的最优性能提供了理论框架，并恢复了尖峰Wigner推理问题的相变行为。

Abstract: Markov chain Monte Carlo algorithms have long been observed to obtain near-optimal performance in various Bayesian inference settings. However, developing a supporting theory that make these studies rigorous has proved challenging.
  In this paper, we study the classical spiked Wigner inference problem, where one aims to recover a planted Boolean spike from a noisy matrix measurement. We relate the recovery performance of Glauber dynamics on the annealed posterior to the performance of Approximate Message Passing (AMP), which is known to achieve Bayes-optimal performance. Our main results rely on the analysis of an auxiliary Markov chain called restricted Gaussian dynamics (RGD). Concretely, we establish the following results:
  1. RGD can be reduced to an effective one-dimensional recursion which mirrors the evolution of the AMP iterates.
  2. From a warm start, RGD rapidly converges to a fixed point in correlation space, which recovers Bayes-optimal performance when run on the posterior.
  3. Conditioned on widely believed mixing results for the SK model, we recover the phase transition for non-trivial inference.

</details>


### [18] [New Bounds for Circular Trace Reconstruction](https://arxiv.org/abs/2512.02412)
*Arnav Burudgunte,Paul Valiant,Hongao Wang*

Main category: cs.DS

TL;DR: 本文研究了循环迹重构问题，改进了稀疏字符串的下界和上界：下界从Ω̃(n³)提升到Ω̃(n⁵)，上界从exp(Õ(n¹/³))改进到Õ(n⁶)用于常数稀疏字符串。


<details>
  <summary>Details</summary>
Motivation: 迹重构问题存在指数级的上界和下界差距，许多变体被提出以缩小这一差距。本文研究Narayanan和Ren提出的循环迹重构变体，其中迹除了随机删除外还经历随机循环移位。

Method: 使用傅里叶技术分析循环删除信道下的稀疏字符串重构。通过构造两个稀疏字符串并分析其迹的不可区分性来证明下界，同时设计算法证明上界。

Result: 1. 证明了循环迹重构的下界为Ω̃(n⁵)，显著优于之前的Ω̃(n³)下界和线性情况的Ω̃(n³/²)下界。2. 证明了在循环删除信道中，任何常数稀疏字符串可以用Õ(n⁶)个迹重构，远优于一般字符串的exp(Õ(n¹/³))上界。

Conclusion: 结果表明，新的算法或下界必须关注非常数稀疏字符串。循环迹重构中稀疏字符串的复杂度远低于一般字符串，这为缩小迹重构问题的上下界差距提供了新方向。

Abstract: The ''trace reconstruction'' problem asks, given an unknown binary string $x$ and a channel that repeatedly returns ''traces'' of $x$ with each bit randomly deleted with some probability $p$, how many traces are needed to recover $x$? There is an exponential gap between the best known upper and lower bounds for this problem. Many variants of the model have been introduced in hopes of motivating or revealing new approaches to narrow this gap. We study the variant of circular trace reconstruction introduced by Narayanan and Ren (ITCS 2021), in which traces undergo a random cyclic shift in addition to random deletions.
  We show an improved lower bound of $\tildeΩ(n^5)$ for circular trace reconstruction. This contrasts with the (previously) best known lower bounds of $\tildeΩ(n^3)$ in the circular case and $\tildeΩ(n^{3/2})$ in the linear case. Our bound shows the indistinguishability of traces from two sparse strings $x,y$ that each have a constant number of nonzeros. Can this technique be extended significantly? How hard is it to reconstruct a sparse string $x$ under a cyclic deletion channel? We resolve these questions by showing, using Fourier techniques, that $\tilde{O}(n^6)$ traces suffice for reconstructing any constant-sparse string in a circular deletion channel, in contrast to the upper bound of $\exp(\tilde{O}(n^{1/3}))$ for general strings in the circular deletion channel. This shows that new algorithms or new lower bounds must focus on non-constant-sparse strings.

</details>


### [19] [Approximation schemes for covering and packing mixed-integer programs with a fixed number of constraints](https://arxiv.org/abs/2512.02571)
*Kobe Grobben,Phablo F. S. Moura,Hande Yaman*

Main category: cs.DS

TL;DR: 该论文研究一类覆盖混合整数线性规划问题，提出了多项式时间近似方案、完全多项式时间近似方案和紧凑公式，改进了经典覆盖问题的近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究一类包含多维背包、设施选址和供应商选择等经典覆盖问题的混合整数线性规划问题，旨在为这类通用问题设计高效的近似算法。

Method: 首先分析关联多面体的顶点性质，利用这些性质将问题分解为每个维度包含单个连续变量的多维背包覆盖问题实例，基于分解设计近似方案。

Result: 1) 为固定约束数的问题设计了首个多项式时间近似方案；2) 为单约束情况设计了完全多项式时间近似方案和近似线性规划公式；3) 改进了单连续变量背包覆盖问题的2-近似算法；4) 为变量具有相同上下界的情况给出了完美紧凑公式。

Conclusion: 该研究为覆盖混合整数线性规划问题提供了系统的近似算法框架，包括多项式时间近似方案、完全多项式时间近似方案和紧凑公式，并将结果扩展到包装和分配变体问题。

Abstract: This paper presents an algorithmic study of a class of covering mixed-integer linear programming problems which encompasses classic cover problems, including multidimensional knapsack, facility location and supplier selection problems. We first show some properties of the vertices of the associated polytope, which are then used to decompose the problem into instances of the multidimensional knapsack cover problem with a single continuous variable per dimension. The proposed decomposition is used to design a polynomial-time approximation scheme for the problem with a fixed number of constraints. To the best of our knowledge, this is the first approximation scheme for such a general class of covering mixed-integer programs. Moreover, we design a fully polynomial-time approximation scheme and an approximate linear programming formulation for the case with a single constraint. These results improve upon the previously best-known 2-approximation algorithm for the knapsack cover problem with a single continuous variable. Finally, we show a perfect compact formulation for the case where all variables have the same lower and upper bounds. Analogous results are derived for the packing and assignment variants of the problem.

</details>


### [20] [The Support of Bin Packing is Exponential](https://arxiv.org/abs/2512.02758)
*Klaus Jansen,Lis Pirotton,Malte Tutas*

Main category: cs.DS

TL;DR: 论文证明了装箱问题中解的支持度下界为2^Ω(d)，与已知上界2^d匹配，对多个装箱算法的时间复杂度有直接影响


<details>
  <summary>Details</summary>
Motivation: 研究装箱问题解的支持度（不同填充方式的箱子数量）的下界，这对理解装箱算法的计算复杂度至关重要

Method: 开发了一种新技术，将具有多个等式约束的整数线性规划聚合为具有单个约束的等价整数线性规划，并成功将变量上界整合到结果约束中

Result: 证明了装箱问题解的支持度下界为2^Ω(d)，与Eisenbrand和Shmonin给出的上界2^d在常数因子内匹配

Conclusion: 该下界结果直接影响多个装箱算法的时间复杂度分析，所开发的技术有望应用于一般整数线性规划或多维背包问题的求解

Abstract: Consider the classical Bin Packing problem with $d$ different item sizes $s_i$ and amounts of items $a_i.$ The support of a Bin Packing solution is the number of differently filled bins. In this work, we show that the lower bound on the support of this problem is $2^{Ω(d)}$. Our lower bound matches the upper bound of $2^d$ given by Eisenbrand and Shmonin [Oper.Research Letters '06] up to a constant factor. This result has direct implications for the time complexity of several Bin Packing algorithms, such as Goemans and Rothvoss [SODA '14], Jansen and Klein [SODA '17] and Jansen and Solis-Oba [IPCO '10]. To achieve our main result, we develop a technique to aggregate equality constrained ILPs with many constraints into an equivalent ILP with one constraint. Our technique contrasts existing aggregation techniques as we manage to integrate upper bounds on variables into the resulting constraint. We believe this technique can be useful for solving general ILPs or the $d$-dimensional knapsack problem.

</details>


### [21] [BD-Index: Scalable Biharmonic Distance Queries on Large Graphs via Divide-and-Conquer Indexing](https://arxiv.org/abs/2512.02929)
*Yueyang Pan,Meihao Liao,Rong-Hua Li*

Main category: cs.DS

TL;DR: 提出BIndex索引结构，通过分治策略高效计算大图上单对节点的双调和距离查询，解决传统随机游走方法在易分割图上效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 双调和距离在道路网络关键链接识别和图神经网络过压缩问题缓解中有重要应用，但现有基于随机游走的方法在图上随机游走混合缓慢时效率低下，特别是对于易分割的大图。

Method: 首先将双调和距离解释为从两个节点出发的随机游走分布之间的距离，然后提出基于割集的新公式，将距离表示为两个被小割集分离的节点集内独立随机游走的函数。基于此设计BIndex索引结构，采用分治策略：先将图分割成易处理的子图，然后自底向上确定性计算所需随机游走概率，查询时只需访问索引的小部分。

Result: BIndex需要O(n·h)空间，构建时间为O(n·h·(h+d_max))，每个查询时间为O(n·h)，其中h为层次划分树的高度，d_max为最大度数，两者通常远小于n。

Conclusion: BIndex通过创新的分治索引结构，显著提高了大图上单对节点双调和距离查询的效率，特别是在易分割的图上，克服了传统随机游走方法的局限性。

Abstract: Biharmonic distance (\bd) is a powerful graph distance metric with many applications, including identifying critical links in road networks and mitigating over-squashing problem in \gnn. However, computing \bd\ is extremely difficult, especially on large graphs. In this paper, we focus on the problem of \emph{single-pair} \bd\ query. Existing methods mainly rely on random walk-based approaches, which work well on some graphs but become inefficient when the random walk cannot mix rapidly.To overcome this issue, we first show that the biharmonic distance between two nodes $s,t$, denoted by $b(s,t)$, can be interpreted as the distance between two random walk distributions starting from $s$ and $t$. To estimate these distributions, the required random walk length is large when the underlying graph can be easily cut into smaller pieces. Inspired by this observation, we present novel formulas of \bd to represent $b(s,t)$ by independent random walks within two node sets $\mathcal{V}_s$, $\mathcal{V}_t$ separated by a small \emph{cut set} $\mathcal{V}_{cut}$, where $\mathcal{V}_s\cup\mathcal{V}_t\cup\mathcal{V}_{cut}=\mathcal{V}$ is the set of graph nodes. Building upon this idea, we propose \bindex, a novel index structure which follows a divide-and-conquer strategy. The graph is first cut into pieces so that each part can be processed easily. Then, all the required random walk probabilities can be deterministically computed in a bottom-top manner. When a query comes, only a small part of the index needs to be accessed. We prove that \bindex\ requires $O(n\cdot h)$ space, can be built in $O(n\cdot h\cdot (h+d_{max}))$ time, and answers each query in $O(n\cdot h)$ time, where $h$ is the height of a hierarchy partition tree and $d_{max}$ is the maximum degree, which are both usually much smaller than $n$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [22] [Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation](https://arxiv.org/abs/2512.02474)
*Haofeng Huang,Ling Gai*

Main category: cs.IR

TL;DR: Q-Bert4Rec：一个多模态序列推荐框架，通过语义量化和跨模态融合增强传统基于ID的推荐系统


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐方法（如BERT4Rec）依赖离散的物品ID，缺乏语义信息且忽略多模态内容（文本、图像），导致泛化能力弱和可解释性差

Method: 三阶段框架：1) 跨模态语义注入 - 通过动态Transformer融合文本、视觉和结构特征丰富ID嵌入；2) 语义量化 - 使用残差向量量化将融合表示离散化为有意义的token；3) 多掩码预训练和微调 - 采用跨度、尾部、多区域等多种掩码策略提升序列理解

Result: 在公开的Amazon基准测试中，Q-Bert4Rec显著优于许多现有强基线方法，验证了语义token化对多模态序列推荐的有效性

Conclusion: 语义量化是提升多模态序列推荐性能的有效方法，Q-Bert4Rec通过融合多模态信息和语义token化解决了传统基于ID方法的局限性

Abstract: Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing.

</details>


### [23] [AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations](https://arxiv.org/abs/2512.02502)
*Luyao Niu,Zhicheng Deng,Boyang Li,Nuoxian Huang,Ruiqi Liu,Wenjia Zhang*

Main category: cs.IR

TL;DR: AskNearby是一个AI驱动的社区应用，通过三层RAG管道和认知地图模型，解决15分钟生活圈内的本地生活信息可及性问题，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: "15分钟城市"愿景不仅需要物理上的邻近性，还需要高效可靠地获取附近场所、服务和活动的信息。现有基于位置的系统主要关注城市级任务，忽视了影响本地化决策的空间、时间和认知因素。

Method: 提出AskNearby系统，包含：(1)三层检索增强生成(RAG)管道，结合基于图、语义向量和地理检索；(2)认知地图模型，编码用户的社区熟悉度和偏好。

Result: 在真实社区数据集上的实验表明，AskNearby在检索准确性和推荐质量上显著优于基于LLM和基于地图的基线方法，在时空基础和认知感知排序方面表现稳健。实际部署进一步验证了其有效性。

Conclusion: 通过解决本地生活信息可及性(LLIA)挑战，AskNearby使居民能够更有效地发现本地资源、规划日常活动和参与社区生活，推动"15分钟城市"愿景的实现。

Abstract: The "15-minute city" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life.

</details>


### [24] [LORE: A Large Generative Model for Search Relevance](https://arxiv.org/abs/2512.03025)
*Chenji Lu,Zhuo Chen,Hui Zhao,Zhiyuan Zeng,Gang Zhao,Junjie Ren,Ruicong Xu,Haoran Li,Songyan Liu,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: LORE是一个用于电商搜索相关性的大语言模型框架，通过三年部署累计提升GoodRate指标27%，采用能力分解、两阶段训练和分层部署策略。


<details>
  <summary>Details</summary>
Motivation: 现有工作使用思维链提升相关性但遇到性能瓶颈，作者认为这是因为将相关性视为单一任务，缺乏原则性分解。关键洞察是相关性包含知识推理、多模态匹配和规则遵循等不同能力，需要进行质性驱动的分解来突破瓶颈。

Method: LORE提供完整的大语言模型相关性生命周期蓝图：1）两阶段训练范式，通过SFT进行渐进式思维链合成，再通过RL进行人类偏好对齐；2）RAIR基准测试，评估核心能力；3）查询频率分层部署策略，高效将离线能力转移到在线系统。

Result: 经过三年部署迭代，LORE在在线GoodRate指标上累计提升27%，证明了框架的有效性。

Conclusion: LORE不仅是一个实用的电商搜索相关性解决方案，也为其他垂直领域提供了方法论参考，展示了通过能力分解和系统化框架设计来突破大语言模型在相关性任务中性能瓶颈的有效途径。

Abstract: Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [25] [FCDB (Functorial-Categorical Database): A Compositional Framework for Information Preservation and Anti-Commutativity Reduction](https://arxiv.org/abs/2512.02021)
*Jun Kawasaki*

Main category: cs.DB

TL;DR: FCDb 是一种基于函子范畴的分层数据库架构，通过完全保持投影族实现信息保留，将非交换性压缩到授权边界，在保持所有权和能力约束的同时支持多种数据视图。


<details>
  <summary>Details</summary>
Motivation: 传统数据库架构通常通过丢弃信息来确保局部一致性，将正确性与信息损失纠缠在一起。作者希望建立一个能保留信息、避免语义、时间和关系熵损失的数据库框架。

Method: 将数据操作建模为分层函子范畴中的态射，建立完全保持投影族（CPF），包含内容不变性（CAS）、能力和所有权三个核心层。通过伴随提升和纤维结构，使操作对在范畴极限中可交换，同时保持所有权完整性和能力约束。

Result: 识别出最小内核 F_core = Own o Cap o CAS，该内核保留信息并将非交换性压缩到伦理授权/撤销边界。框架通过投影解释连接到信息几何学，支持无需丢弃熵的实证验证。

Conclusion: FCDb 提供了一种保留信息的数据库架构，通过函子范畴方法解决了传统数据库的信息丢弃问题，同时支持局部顺序、时间历史和邻接关系等多种数据视图。

Abstract: Conventional database architectures often secure local consistency by discarding information, entangling correctness with loss. We introduce the Functorial-Categorical Database (FCDb), which models data operations as morphisms in a layered functor category and establishes a Complete Preserving Family (CPF) of projections spanning content invariance (CAS), capability, and ownership, with optional observational projections for local order (B+Tree), temporal history (append-only/LSM), and adjacency (Graph). We identify a minimal kernel (F_core = Own o Cap o CAS) that preserves information and collapses non-commutativity to the ethical grant/revoke boundary. Under adjoint lifts and a fibred structure, operational pairs commute in the categorical limit while ownership integrity and capability constraints are maintained. The framework connects to information geometry via projection interpretations and supports empirical validation without discarding semantic, temporal, or relational entropy.

</details>


### [26] [Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving](https://arxiv.org/abs/2512.02281)
*Yi Liu,Chen Qian*

Main category: cs.DB

TL;DR: Trinity是一个将向量搜索与PD解耦LLM服务整合的框架，通过专用GPU池和智能调度优化检索任务


<details>
  <summary>Details</summary>
Motivation: 当前检索任务（如RAG请求和提示答案缓存）与模型推理过程纠缠，增加了尾部延迟，需要研究向量搜索如何与PD解耦架构协调部署

Method: 1) 提出PD解耦架构下的GPU向量搜索服务新架构；2) 向量搜索的连续批处理技术；3) 阶段感知调度，在解码和预填充任务间抢占向量搜索请求

Result: Trinity将所有检索任务整合到单一的共享向量搜索GPU池中，与PD解耦的LLM服务协同工作

Conclusion: Trinity提供了一个实用的框架，能够在各种检索工作负载下不违反SLO的前提下，有效协调向量搜索与PD解耦LLM服务

Abstract: Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.

</details>


### [27] [Multi-Objective Agentic Rewrites for Unstructured Data Processing](https://arxiv.org/abs/2512.02289)
*Lindsey Linxi Wei,Shreya Shankar,Sepanta Zeighami,Yeounoh Chung,Fatma Ozcan,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: MOAR是一个多目标优化器，用于DocETL系统，通过引入新的重写指令和全局搜索算法，在保持准确性的同时显著降低LLM处理成本。


<details>
  <summary>Details</summary>
Motivation: DocETL系统虽然通过语义操作符和重写指令提高了LLM数据处理的准确性，但只优化准确性而不考虑成本。实际应用中需要同时优化准确性和成本两个目标。

Method: 1) 引入两个新的重写指令类别，并将现有三个类别扩展，总共超过30个指令；2) 设计新的全局搜索算法，在整个管道上下文中探索重写；3) 采用多臂老虎机框架优先处理哪些管道需要重写。

Result: 在六个工作负载测试中，MOAR比次优优化器ABACUS准确率高27%，同时在达到ABACUS最佳准确性时成本仅为55%。

Conclusion: MOAR成功解决了DocETL系统同时优化准确性和成本的问题，通过扩展重写指令和创新的全局搜索算法，在多个领域应用中实现了显著的成本效益提升。

Abstract: One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter("is this email sent from an executive and discussing fraud?") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both?
  We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.

</details>


### [28] [QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning](https://arxiv.org/abs/2512.02444)
*Ning Wang,Sainyam Galhotra*

Main category: cs.DB

TL;DR: QJoin是一个基于强化学习的框架，用于发现异构数据表中的可连接关系，通过学习并重用转换策略来解决传统方法无法处理标识符格式不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 在大型异构数据仓库中，传统连接发现方法主要设计用于等值连接，假设连接键完全或近似匹配。这些方法在干净、规范化的数据库中有效，但在开放或联邦设置中，当标识符格式不一致、嵌入或跨多列分割时就会失败。近似或模糊连接可以缓解小的字符串变化，但无法捕获系统性的转换。

Method: QJoin采用强化学习框架，训练一个智能体在唯一性感知奖励下工作，平衡相似性和键的独特性，使其能够探索简洁、高价值的转换链。为了加速新连接，引入两种重用机制：(i) 智能体迁移：从预训练智能体初始化新策略；(ii) 转换重用：为相似列簇缓存成功的操作符序列。

Result: 在AutoJoin Web基准测试（31个表对）上，QJoin平均F1分数达到91.0%。在NYC+Chicago开放数据集的19,990个连接任务中，通过重用机制将运行时间减少了7.4%（13,747秒）。

Conclusion: 转换学习和重用可以使连接发现更加准确和高效，QJoin框架在异构数据集成场景中表现出色。

Abstract: Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.

</details>


### [29] [A Datalake for Data-driven Social Science Research](https://arxiv.org/abs/2512.02463)
*Puneet Arya,Ojas Sahasrabudhe,Adwaiya Srivastav,Partha Pratim Das,Maya Ramanath*

Main category: cs.DB

TL;DR: 为社会科学研究量身定制的Datalake基础设施，解决技术门槛、数据格式不一致和数据集访问受限等问题，支持数据集成、版本追踪、访问控制和可视化分析。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究需要数据驱动的洞察，但研究人员面临技术专业知识不足、数据格式不一致、可靠数据集访问受限等障碍，阻碍了跨学科社会科学研究的发展。

Method: 开发专门为跨学科社会科学研究设计的Datalake基础设施，支持多种数据类型的摄取和集成，自动追踪数据来源和版本，实施基于角色的访问控制，并内置可视化和分析工具。

Result: 通过治理、卫生和教育等真实案例展示了Datalake的实用性，特别是通过收入、教育和婴儿死亡率关系的分析案例，展示了平台如何简化研究流程同时保持透明度和可重复性。

Conclusion: 此类基础设施能够民主化高级数据科学实践的访问，特别有利于NGO、学生和基层组织。平台计划继续发展，支持机器学习管道、移动访问和公民数据反馈机制。

Abstract: Social science research increasingly demands data-driven insights, yet researchers often face barriers such as lack of technical expertise, inconsistent data formats, and limited access to reliable datasets.Social science research increasingly demands data-driven insights, yet researchers often face barriers such as lack of technical expertise, inconsistent data formats, and limited access to reliable datasets. In this paper, we present a Datalake infrastructure tailored to the needs of interdisciplinary social science research. Our system supports ingestion and integration of diverse data types, automatic provenance and version tracking, role-based access control, and built-in tools for visualization and analysis. We demonstrate the utility of our Datalake using real-world use cases spanning governance, health, and education. A detailed walkthrough of one such use case -- analyzing the relationship between income, education, and infant mortality -- shows how our platform streamlines the research process while maintaining transparency and reproducibility. We argue that such infrastructure can democratize access to advanced data science practices, especially for NGOs, students, and grassroots organizations. The Datalake continues to evolve with plans to support ML pipelines, mobile access, and citizen data feedback mechanisms.

</details>


### [30] [Stress-Testing Causal Claims via Cardinality Repairs](https://arxiv.org/abs/2512.02491)
*Yarden Gabbay,Haoquan Guan,Shaull Almagor,El Kindi Rezig,Brit Youngmann,Babak Salimi*

Main category: cs.DB

TL;DR: SubCure框架通过基数修复进行鲁棒性审计，识别少量数据子集，其移除能显著改变因果结论，揭示传统方法无法检测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的因果分析支撑着医疗、公共政策、经济学等领域的高风险决策，但这些结论可能出奇地脆弱：即使是轻微的数据错误（如重复记录或录入错误）也可能大幅改变因果关系。这引发了一个基本问题：因果主张对数据中微小、有针对性的修改有多鲁棒？解决这个问题对于确保实证发现的可靠性、可解释性和可复现性至关重要。

Method: 引入SubCure框架，通过基数修复进行鲁棒性审计。给定因果查询和用户指定的效应估计目标范围，SubCure识别少量元组或子群体，其移除能将估计值移至期望范围。在元组级和模式级删除设置下形式化该问题，证明两者都是NP完全问题。为扩展到大型数据集，开发了结合机器去学习技术的高效算法，无需从头重新训练即可增量更新因果估计。

Result: 在四个涵盖不同应用领域的真实世界数据集上评估SubCure。在每种情况下，它都发现了紧凑、高影响的子集，其移除显著改变了因果结论，揭示了传统方法无法检测的脆弱性。结果表明，基数修复是压力测试因果分析和防止基于普通数据缺陷的误导性主张的强大通用工具。

Conclusion: 基数修复是压力测试因果分析和防止基于普通数据缺陷的误导性主张的强大通用工具。SubCure框架不仅量化了因果结论的敏感性，还精确定位了驱动这些结论的具体数据区域，为因果分析的鲁棒性审计提供了系统方法。

Abstract: Causal analyses derived from observational data underpin high-stakes decisions in domains such as healthcare, public policy, and economics. Yet such conclusions can be surprisingly fragile: even minor data errors - duplicate records, or entry mistakes - may drastically alter causal relationships. This raises a fundamental question: how robust is a causal claim to small, targeted modifications in the data? Addressing this question is essential for ensuring the reliability, interpretability, and reproducibility of empirical findings. We introduce SubCure, a framework for robustness auditing via cardinality repairs. Given a causal query and a user-specified target range for the estimated effect, SubCure identifies a small set of tuples or subpopulations whose removal shifts the estimate into the desired range. This process not only quantifies the sensitivity of causal conclusions but also pinpoints the specific regions of the data that drive those conclusions. We formalize this problem under both tuple- and pattern-level deletion settings and show both are NP-complete. To scale to large datasets, we develop efficient algorithms that incorporate machine unlearning techniques to incrementally update causal estimates without retraining from scratch. We evaluate SubCure across four real-world datasets covering diverse application domains. In each case, it uncovers compact, high-impact subsets whose removal significantly shifts the causal conclusions, revealing vulnerabilities that traditional methods fail to detect. Our results demonstrate that cardinality repair is a powerful and general-purpose tool for stress-testing causal analyses and guarding against misleading claims rooted in ordinary data imperfections.

</details>


### [31] [PystachIO: Efficient Distributed GPU Query Processing with PyTorch over Fast Networks & Fast Storage](https://arxiv.org/abs/2512.02862)
*Jigao Luo,Nils Boeschen,Muhammad El-Hindi,Carsten Binnig*

Main category: cs.DB

TL;DR: PyTorch分布式OLAP引擎PystachIO，通过优化GPU、网络和存储I/O重叠，实现比现有GPU查询处理快3倍的速度


<details>
  <summary>Details</summary>
Motivation: 现代数据中心采用GPU为中心的HPC架构，Tensor计算运行时（如PyTorch）已证明能加速分析工作负载，但现有研究主要关注数据能放入GPU内存的情况，缺乏对大规模存储驻留OLAP工作负载的系统研究

Method: 提出PystachIO，一个基于PyTorch的分布式OLAP引擎，结合快速网络和存储I/O，通过关键优化最大化GPU、网络和存储利用率，解决计算与数据移动重叠不足的问题

Result: 评估显示，相比现有分布式GPU查询处理方法，PystachIO实现了高达3倍的端到端加速

Conclusion: PystachIO成功展示了Tensor计算运行时如何支持大规模存储驻留OLAP工作负载的分布式查询处理，通过优化I/O重叠显著提升性能

Abstract: The AI hardware boom has led modern data centers to adopt HPC-style architectures centered on distributed, GPU-centric computation. Large GPU clusters interconnected by fast RDMA networks and backed by high-bandwidth NVMe storage enable scalable computation and rapid access to storage-resident data. Tensor computation runtimes (TCRs), such as PyTorch, originally designed for AI workloads, have recently been shown to accelerate analytical workloads. However, prior work has primarily considered settings where the data fits in aggregated GPU memory. In this paper, we systematically study how TCRs can support scalable, distributed query processing for large-scale, storage-resident OLAP workloads. Although TCRs provide abstractions for network and storage I/O, naive use often underutilizes GPU and I/O bandwidth due to insufficient overlap between computation and data movement. As a core contribution, we present PystachIO, a PyTorch-based distributed OLAP engine that combines fast network and storage I/O with key optimizations to maximize GPU, network, and storage utilization. Our evaluation shows up to 3x end-to-end speedups over existing distributed GPU-based query processing approaches.

</details>


### [32] [From Administrative Chaos to Analytical Cohorts: A Three-Stage Normalisation Pipeline for Longitudinal University Administrative Records](https://arxiv.org/abs/2512.02936)
*H. R. Paz*

Main category: cs.DB

TL;DR: 提出一个三阶段数据标准化流程，用于处理拉丁美洲公立大学40年工程学生数据，解决人口统计信息不一致、重复标识符和学校类型分类问题，保留100%学生记录并识别结构性缺失数据。


<details>
  <summary>Details</summary>
Motivation: 当前大学行政记录在数据驱动决策中常忽视原始数据的标准化处理，特别是长期历史数据存在不一致性、重复标识符和缺失信息等问题，需要系统化的数据清洗和标准化方法。

Method: 开发三阶段标准化流程：1) 人口统计信息统一到个人层面；2) 重复标识符解析为规范ID并保留审计追踪；3) 地理和中学信息标准化，包括建立参考表、学校类型分类（国立、省立、私立世俗、私立宗教）和标记不可恢复的缺失数据。

Result: 流程保留100%学生记录，实现完全地理编码，为56.6%人口提供有效学校类型分类。剩余43.4%被识别为结构性缺失（由历史入学实践造成）。法证分析显示缺失模式可从入学年代和地理位置高度预测，证实结构性缺失机制。

Conclusion: 贡献包括：1) 高等教育领域透明可复现的标准化流程；2) 无需推测性插补处理结构性缺失信息的框架；3) 定义分析一致队列（全人口vs中学信息子队列）的指导原则，支持学习分析和政策评估。

Abstract: The growing use of longitudinal university administrative records in data-driven decision-making often overlooks a critical layer: how raw, inconsistent data are normalised before modelling. This article presents a three-stage normalisation pipeline for a dataset of 24,133 engineering students at a Latin American public university, spanning four decades (1980-2019). The pipeline comprises: (i) N1 CENSAL, harmonising demographics into a single person-level layer; (ii) N1b IDENTITY RESOLUTION, consolidating duplicate identifiers into a canonical ID while preserving an audit trail; and (iii) N1c GEO and SECONDARY-SCHOOL NORMALISATION, which builds reference tables, classifies school types (state national, state provincial, private secular, private religious), and flags irrecoverable cases as DATA_MISSING. The pipeline preserves 100% of students, achieves full geocoding, and yields valid school types for 56.6% of the population. The remaining 43.4% are identified as structurally missing due to legacy enrolment practices rather than stochastic non-response. Forensic analysis (chi-square, logistic regression) shows missingness is highly predictable from entry decade and geography, confirming a structural, historically induced mechanism. The article contributes: (a) a transparent, reproducible normalisation pipeline tailored to higher education; (b) a framework for treating structurally missing information without speculative imputation; and (c) guidance on defining analytically coherent cohorts (full population vs. secondary-school-informed subcohorts) for downstream learning analytics and policy evaluation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [33] [Characterizing Off-Chain Influence Proof Transaction Fee Mechanisms](https://arxiv.org/abs/2512.02354)
*Aadityan Ganesh,Clayton Thomas,S. Matthew Weinberg*

Main category: cs.GT

TL;DR: 该论文研究了交易费机制（TFMs）的链下影响力证明（OffCIP）特性，建立了burn identity来刻画OffCIP机制，并发现确定性非加密TFMs只能是特殊调优的定价机制，而随机化TFMs存在更多可能性。


<details>
  <summary>Details</summary>
Motivation: 研究交易费机制（TFMs）的链下影响力证明（OffCIP）特性，解决Ganesh等人提出的开放性问题：是否存在非加密机制同时满足OnCS和OffCIP特性。

Method: 通过建立burn identity来刻画OffCIP TFMs，将TFMs的分配规则和燃烧规则视为多物品拍卖的分配和定价规则进行分析，特别关注虚拟价值转换。

Result: 1. 建立了OffCIP的充要条件：burn identity；2. 确定性非加密TFMs只能是特殊调优的定价机制；3. 随机化TFMs存在更多OnCS和OffCIP机制，即使有限供给下也能存在。

Conclusion: OffCIP虽然是严格的要求，但在各种设置下都能找到OffCIP机制族，特别是随机化机制提供了更多可能性，即使不使用密码学也能实现。

Abstract: Roughgarden (2020) initiates the study of Transaction Fee Mechanisms (TFMs), and posits that the on-chain game of a ``good'' TFM should be on-chain simple (OnCS), i.e., incentive compatible for users and the miner. Recent work of Ganesh, Thomas and Weinberg (2024) posits that they should additionally be Off-Chain Influence Proof (OffCIP), which means that the miner cannot achieve any additional revenue by separately conducting an off-chain auction to determine on-chain inclusion. They observe that a cryptographic second-price auction satisfies both properties, but leave open the question of whether other mechanisms (e.g, non-cryptographic) satisfy these properties.
  In this paper, we characterize OffCIP TFMs: They are those satisfying a burn identity relating the burn rule to the allocation rule. In particular, we show that auction is OffCIP if and only if its (induced direct-revelation) allocation rule $\bar{X}(\cdot)$ and burn rule $\bar{B}(\cdot)$ (both of which take as input users' values $v_1, \dots, v_n$) are truthful when viewing $\big(\bar{X}(\cdot), \bar{B}(\cdot)\big)$ as the allocation and pricing rule of a multi-item auction for a single additive buyer with values $\big(\varphi(v_1),\ldots, \varphi(v_n)\big)$ equal to the users' virtual values.
  Building on this burn identity, we characterize deterministic OffCIP and OnCS TFMs that do not use cryptography: They are posted-price mechanisms with specially-tuned burns. As a corollary, we show that such TFMs can only exist with infinite supply and prior-dependence. However, we show that for randomized TFMs, there are additional OnCS and OffCIP auctions that do not use cryptography (even when there is finite supply, under prior-dependence with a bounded prior distribution). Holistically, our results show that while OffCIP is a fairly stringent requirement, families of OffCIP mechanisms can be found for a variety of settings.

</details>


### [34] [Posted Pricing for Online Selection: Limited Price Changes and Risk Sensitivity](https://arxiv.org/abs/2512.02427)
*Hossein Nekouyan,Bo Sun,Raouf Boutaba,Xiaoqi Tan*

Main category: cs.GT

TL;DR: 研究有限价格变动次数下的定价机制，结合风险敏感目标（CVaR），提出相关定价策略以平衡价格变动限制与风险性能


<details>
  <summary>Details</summary>
Motivation: 动态定价机制虽然性能优越，但频繁的价格变动会引发公平性担忧（价格歧视）和运营成本问题。需要研究在有限价格变动次数约束下的定价策略，同时考虑风险敏感目标而非仅期望性能

Method: 提出kSelection-(δ,Δ)问题框架，使用相关定价机制（correlated PPM），通过单一随机种子关联定价，同时处理价格变动限制和尾部风险性能

Result: 分析显示价格变动次数与算法风险敏感性之间存在明确权衡关系，并在多个重要特例中建立了最优性结果

Conclusion: 相关定价机制能有效平衡价格变动限制与风险性能，为解决动态定价中的公平性和运营成本问题提供了理论框架

Abstract: Posted-price mechanisms (PPMs) are a widely adopted strategy for online resource allocation due to their simplicity, intuitive nature, and incentive compatibility. To manage the uncertainty inherent in online settings, PPMs commonly employ dynamically increasing prices. While this adaptive pricing achieves strong performance, it introduces practical challenges: dynamically changing prices can lead to fairness concerns stemming from price discrimination and incur operational costs associated with frequent updates. This paper addresses these issues by investigating posted pricing constrained by a limited, pre-specified number of allowed price changes, denoted by $Δ$. We further extend this framework by incorporating a second critical dimension: risk sensitivity. Instead of evaluating performance based solely on expectation, we utilize a tail-risk objective-specifically, the Conditional Value at Risk (CVaR) of the total social welfare, parameterized by a risk level $δ\in [0, 1]$.
  We formally introduce a novel problem class kSelection-$(δ,Δ)$ in online adversarial selection and propose a correlated PPM that utilizes a single random seed to correlate posted prices. This correlation scheme is designed to address both the limited price changes and simultaneously enhance the tail performance of the online algorithm. Our subsequent analysis provides performance guarantees under these joint constraints, revealing a clear trade-off between the number of allowed price changes and the algorithm's risk sensitivity. We also establish optimality results for several important special cases of the problem.

</details>


### [35] [Monotone Near-Zero-Sum Games: A Generalization of Convex-Concave Minimax](https://arxiv.org/abs/2512.02690)
*Ruichen Luo,Sebastian U. Stich,Krishnendu Chatterjee*

Main category: cs.GT

TL;DR: 提出单调近零和博弈新类别，设计梯度算法降低复杂度，应用于实际场景


<details>
  <summary>Details</summary>
Motivation: 零和博弈与非零和（一般和）博弈应用广泛，但一般非零和博弈计算困难。现有研究聚焦单调博弈的梯度算法，但单调零和与单调一般和博弈的梯度复杂度存在显著差距。许多实际场景需要放松零和假设。

Method: 定义单调近零和博弈新类别（包含单调零和博弈作为特例），提出新算法将这类博弈转化为一系列零和子问题，改进梯度复杂度。

Result: 建立了单调近零和博弈的理论框架，设计了降低梯度复杂度的算法，为实际应用提供了新工具。

Conclusion: 单调近零和博弈填补了零和与一般和博弈之间的理论空白，提出的算法改进了梯度复杂度，并展示了在文献中实际博弈场景的应用潜力。

Abstract: Zero-sum and non-zero-sum (aka general-sum) games are relevant in a wide range of applications. While general non-zero-sum games are computationally hard, researchers focus on the special class of monotone games for gradient-based algorithms. However, there is a substantial gap between the gradient complexity of monotone zero-sum and monotone general-sum games. Moreover, in many practical scenarios of games the zero-sum assumption needs to be relaxed. To address these issues, we define a new intermediate class of monotone near-zero-sum games that contains monotone zero-sum games as a special case. Then, we present a novel algorithm that transforms the near-zero-sum games into a sequence of zero-sum subproblems, improving the gradient-based complexity for the class. Finally, we demonstrate the applicability of this new class to model practical scenarios of games motivated from the literature.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [36] [PopSim: Social Network Simulation for Social Media Popularity Prediction](https://arxiv.org/abs/2512.02533)
*Yijun Liu,Wu Liu,Xiaoyan Gu,Allen He,Weiping Wang,Yongdong Zhang*

Main category: cs.MM

TL;DR: PopSim：基于LLM多智能体模拟的社交媒体流行度预测新范式，通过模拟UGC传播动态而非传统静态建模，平均减少预测误差8.82%


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体流行度预测方法采用归纳范式，基于历史数据训练静态模型，但UGC传播本质上是动态过程，静态建模无法捕捉复杂的交互和非线性演化

Method: 提出PopSim模拟范式：1）基于LLM的多智能体社交网络沙箱模拟UGC传播动态；2）设计基于社会平均场的智能体交互机制，建模双通道双向的个体-群体交互；3）多源信息聚合模块将异构社交元数据统一格式化；4）融合多模态传播动态进行综合预测

Result: 在真实数据集上的实验表明，PopSim持续优于最先进方法，平均减少预测误差8.82%

Conclusion: PopSim为社交媒体流行度预测任务提供了新的研究视角，通过模拟而非归纳的方法更好地捕捉了UGC传播的动态特性

Abstract: Accurately predicting the popularity of user-generated content (UGC) is essential for advancing social media analytics and recommendation systems. Existing approaches typically follow an inductive paradigm, where researchers train static models on historical data for popularity prediction. However, the UGC propagation is inherently a dynamic process, and static modeling based on historical features fails to capture the complex interactions and nonlinear evolution. In this paper, we propose PopSim, a novel simulation-based paradigm for social media popularity prediction (SMPP). Unlike the inductive paradigm, PopSim leverages the large language models (LLMs)-based multi-agent social network sandbox to simulate UGC propagation dynamics for popularity prediction. Specifically, to effectively model the UGC propagation process in the network, we design a social-mean-field-based agent interaction mechanism, which models the dual-channel and bidirectional individual-population interactions, enhancing agents' global perception and decision-making capabilities. In addition, we propose a multi-source information aggregation module that transforms heterogeneous social metadata into a uniform formulation for LLMs. Finally, propagation dynamics with multimodal information are fused to provide comprehensive popularity prediction. Extensive experiments on real-world datasets demonstrate that SimPop consistently outperforms the state-of-the-art methods, reducing prediction error by an average of 8.82%, offering a new perspective for research on the SMPP task.

</details>


### [37] [Stepwise Schema-Guided Prompting Framework with Parameter Efficient Instruction Tuning for Multimedia Event Extraction](https://arxiv.org/abs/2512.02584)
*Xiang Yuan,Xinrong Chen,Haochen Li,Hang Yang,Guanyu Wang,Weiping Li,Tong Mo*

Main category: cs.MM

TL;DR: 提出SSGPF框架，使用MLLM作为骨干网络，通过分步模式引导提示解决多媒体事件抽取任务，在M2E2基准上显著超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前多媒体事件抽取面临两个主要挑战：1) 缺乏处理复杂多媒体事件结构的提取框架建模；2) 缺少多模态对齐的训练数据用于有效知识迁移。

Method: 提出分步模式引导提示框架(SSGPF)，使用多模态大语言模型(MLLM)作为骨干。包含事件类型模式引导提示(ETSGP)用于事件检测，以及参数角色模式引导提示(ARSGP)用于参数提取。基于现有单模态事件标注构建弱对齐多模态事件数据集，并在LLaVA-v1.5-7B上使用LoRA进行参数高效指令调优。

Result: 在M2E2基准测试中，SSGPF在事件检测上比当前SOTA基线提升5.8% F1分数，在参数提取上提升8.4% F1分数。

Conclusion: SSGPF框架通过分步模式引导提示和MLLM骨干网络，有效解决了多媒体事件抽取中的结构建模和数据对齐问题，取得了显著性能提升。

Abstract: Multimedia Event Extraction (MEE) has become an important task in information extraction research as news today increasingly prefers to contain multimedia content. Current MEE works mainly face two challenges: (1) Inadequate extraction framework modeling for handling complex and flexible multimedia event structure; (2) The absence of multimodal-aligned training data for effective knowledge transfer to MEE task. In this work, we propose a Stepwise Schema-Guided Prompting Framework (SSGPF) using Multimodal Large Language Model (MLLM) as backbone for adaptive structure capturing to solve MEE task. At the initial step of SSGPF, we design Event Type Schema Guided Prompting (ETSGP) for event detection, then we devise Argument Role Schema Guided Prompting (ARSGP) that contains multi-step prompts with text-bridged grounding technique for argument extraction. We construct a weakly-aligned multimodal event labeled dataset based on existing unimodal event annotations, then conduct parameter efficient instruction tuning with LoRA on LLaVA-v1.5-7B under SSGPF. Experiments on the M2E2 benchmark demonstrate that SSGPF significantly outperforms current SOTA baselines by 5.8 percent F1 on event detection and 8.4 percent F1 on argument extraction.

</details>
