<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 9]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Precoding-Oriented CSI Feedback Design with Mutual Information Regularized VQ-VAE](https://arxiv.org/abs/2602.02508)
*Xi Chen,Homa Esfahanizadeh,Foad Sohrabi*

Main category: cs.IT

TL;DR: 提出基于向量量化变分自编码器的预编码导向CSI反馈框架，通过信息论正则化在固定反馈预算下最大化码本利用率，实现与变长神经压缩方案相当的速率性能。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中，用户设备的CSI压缩对信道重建和预编码设计至关重要。核心挑战在于平衡CSI反馈开销与下行链路速率，即在有限反馈条件下最大化系统性能。

Method: 提出基于向量量化变分自编码器的预编码导向CSI反馈框架，引入可微互信息下界估计器作为训练正则化器，促进在固定反馈预算下对学习码本的有效利用。

Result: 数值结果表明，该方法在固定长度反馈下实现了与变长神经压缩方案相当的速率性能。学习的码字展现出更均匀的使用分布，并捕获了与底层信道状态信息强相关的可解释结构。

Conclusion: 提出的信息论正则化向量量化方法能够有效平衡CSI反馈开销与系统性能，在固定反馈预算下实现高效压缩，为大规模MIMO系统中的CSI反馈提供了有前景的解决方案。

Abstract: Efficient channel state information (CSI) compression at the user equipment plays a key role in enabling accurate channel reconstruction and precoder design in massive multiple-input multiple-output systems. A key challenge lies in balancing the CSI feedback overhead with the achievable downlink rate, i.e., maximizing the utility of limited feedback to maintain high system performance. In this work, we propose a precoding-oriented CSI feedback framework based on a vector quantized variational autoencoder, augmented with an information-theoretic regularization. To achieve this, we introduce a differentiable mutual information lower-bound estimator as a training regularizer to promote effective utilization of the learned codebook under a fixed feedback budget. Numerical results demonstrate that the proposed method achieves rates comparable to variable-length neural compression schemes, while operating with fixed-length feedback. Furthermore, the learned codewords exhibit significantly more uniform usage and capture interpretable structures that are strongly correlated with the underlying channel state information.

</details>


### [2] [Rate-Distortion Analysis of Optically Passive Vision Compression](https://arxiv.org/abs/2602.02768)
*Ronald Ogden,David Fridovich-Keil,Takashi Tanaka*

Main category: cs.IT

TL;DR: 提出一种新型光学被动视觉压缩(OPVC)方案，利用事件相机观测视觉场景的光学余弦变换，实现高速、无需计算处理的视频压缩，性能优于单独事件相机(SAEC)。


<details>
  <summary>Details</summary>
Motivation: 自主决策中远程视觉传感器需要实时传输高容量视觉数据，但通信资源受限。机器人控制系统中系统可能快速失稳，需要更高采样频率，加剧了数据传输挑战。

Method: 提出光学被动视觉压缩(OPVC)方案：事件相机观测视觉场景的光学余弦变换，实现高速、无需计算处理的视频压缩，灵感来自现代视频编解码器。

Result: OPVC方案的率失真性能优于单独事件相机(SAEC)，且随着事件相机空间分辨率提高，性能差距进一步增大。

Conclusion: 光学被动视觉压缩方案为资源受限环境下的高速视觉数据传输提供了一种有前景的解决方案，性能优于传统事件相机方案。

Abstract: The use of remote vision sensors for autonomous decision-making poses the challenge of transmitting high-volume visual data over resource-constrained channels in real-time. In robotics and control applications, many systems can quickly destabilize, which can exacerbate the issue by necessitating higher sampling frequencies. This work proposes a novel sensing paradigm in which an event camera observes the optically generated cosine transform of a visual scene, enabling high-speed, computation-free video compression inspired by modern video codecs. In this study, we simulate this optically passive vision compression (OPVC) scheme and compare its rate-distortion performance to that of a standalone event camera (SAEC). We find that the rate-distortion performance of the OPVC scheme surpasses that of the SAEC and that this performance gap increases as the spatial resolution of the event camera increases.

</details>


### [3] [Straggler-Aware Coded Polynomial Aggregation](https://arxiv.org/abs/2602.03074)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: 将编码多项式聚合(CPA)扩展到容错分布式计算系统，建立非故障节点模式下的精确恢复条件，通过交集结构阈值减少所需工作节点响应


<details>
  <summary>Details</summary>
Motivation: 现有CPA方案局限于理想化无故障系统，无法容忍节点故障。需要将CPA扩展到具有预定义非故障节点模式的容错分布式计算系统，实现精确恢复

Method: 扩展CPA到容错系统，分析非故障节点模式的交集结构，建立精确恢复的充要条件，识别交集大小阈值，提供可行的CPA方案构造

Result: 证明在容错CPA中，精确恢复所需的工作节点响应少于基于个体解码的多项式编码，可行性由非故障节点模式的交集结构决定，确定了保证精确恢复的交集大小阈值

Conclusion: 成功将CPA扩展到容错系统，建立了基于交集结构的精确恢复理论框架，提供了可行的构造方案，仿真验证了理论预测的可行性转变阈值

Abstract: Coded polynomial aggregation (CPA) in distributed computing systems enables the master to directly recover a weighted aggregation of polynomial computations without individually decoding each term, thereby reducing the number of required worker responses. However, existing CPA schemes are restricted to an idealized setting in which the system cannot tolerate stragglers. In this paper, we extend CPA to straggler-aware distributed computing systems with a pre-specified non-straggler pattern, where exact recovery is required for a given collection of admissible non-straggler sets. Our main results show that exact recovery of the desired aggregation is achievable with fewer worker responses than that required by polynomial codes based on individual decoding, and that feasibility is characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA. We identify an intersection-size threshold that is sufficient to guarantee exact recovery. When the number of admissible non-straggler sets is sufficiently large, we further show that this threshold is necessary in a generic sense. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations verify our theoretical results by demonstrating a sharp feasibility transition at the predicted intersection threshold.

</details>


### [4] [Entropy Functions on Two-Dimensional Faces of Polymatroid Region with One Extreme Ray Containing Rank-One Matroid](https://arxiv.org/abs/2602.03363)
*Kaizhe He,Qi Chen*

Main category: cs.IT

TL;DR: 该论文研究了n度多拟阵区域2维面上的熵函数特征，特别关注包含秩1拟阵的极端射线，并将包含另一个拟阵的极端射线的2维面分为四类。


<details>
  <summary>Details</summary>
Motivation: 熵函数的表征在信息论中具有基础重要性。通过在多拟阵区域（香农外边界）上施加约束，可以获得该区域的各个面以及具有特殊结构的熵函数。本文旨在研究包含秩1拟阵的极端射线的2维面上的熵函数特征。

Method: 作者通过分析多拟阵区域的2维面，特别关注那些包含秩1拟阵的极端射线。他们进一步分类了包含另一个拟阵的极端射线的2维面，将其分为四种类型，从而系统地表征这些特殊结构上的熵函数。

Result: 成功表征了n度多拟阵区域2维面上的熵函数，这些面包含秩1拟阵的极端射线。将所有包含另一个拟阵的极端射线的2维面分类为四种类型，为理解这些特殊结构上的熵函数提供了系统框架。

Conclusion: 该研究为信息论中熵函数的表征提供了新的见解，特别是对于多拟阵区域2维面上具有特殊结构的熵函数。通过分类包含秩1拟阵的极端射线的2维面，为理解这些熵函数的性质建立了系统框架。

Abstract: Characterization of entropy functions is of fundamental importance in information theory. By imposing constraints on their Shannon outer bound, i.e., the polymatroidal region, one obtains the faces of the region and entropy functions on them with special structures. In this paper, we characterize entropy functions on 2-dimensional faces of polymatroid region of degree n with one extreme ray containing rank-1 matroid. We classify all such 2-dimensional faces with another extreme ray containing a matroid into four types.

</details>


### [5] [Universal Costas Matrices: Towards a General Framework for Costas Array Construction](https://arxiv.org/abs/2602.03407)
*Fatih Gulec,Vahid Abolghasemi*

Main category: cs.IT

TL;DR: 提出统一框架分析Costas阵列，引入通用Costas矩阵(UCM)和通用Costas频率矩阵(UCFM)，开发基于重构的搜索方法，显著加速搜索过程


<details>
  <summary>Details</summary>
Motivation: Costas阵列具有理想自相关和低互相关特性，在雷达、无线通信和集成感知通信中具有重要价值。现有方法在发现新Costas阵列方面效率有限，需要更系统的分析框架和更高效的搜索方法。

Method: 提出统一框架，引入UCM和UCFM概念并研究其结构特性。开发基于重构的搜索方法，从UCFM生成UCM。该框架为未来AI辅助的Costas阵列发现奠定基础。

Result: 数值结果表明，所提方法显著加速了搜索过程，增强了Costas阵列生成的结构洞察力，为高效发现新Costas阵列提供了有效途径。

Conclusion: 提出的统一框架和基于重构的搜索方法为Costas阵列分析提供了新视角，不仅加速了搜索过程，还为未来AI辅助的Costas阵列发现铺平了道路。

Abstract: Costas arrays are a special type of permutation matrices with ideal autocorrelation and low cross-correlation properties, making them valuable for radar, wireless communication, and integrated sensing and communication applications. This paper presents a novel unified framework for analyzing and discovering new Costas arrays. We introduce Universal Costas Matrices (UCMs) and Universal Costas Frequency Matrices (UCFMs) and investigate their structural characteristics. A framework integrating UCMs and UCFMs is proposed to pave the way for future artificial intelligence-assisted Costas array discovery. Leveraging the structural properties of UCMs and UCFMs, a reconstruction-based search method is developed to generate UCMs from UCFMs. Numerical results demonstrate that the proposed approach significantly accelerates the search process and enhances structural insight into Costas array generation.

</details>


### [6] [On (Im)possibility of Network Oblivious Transfer via Noisy Channels and Non-Signaling Correlations](https://arxiv.org/abs/2602.03421)
*Hadi Aghaee,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 本文研究在诚实但好奇的各方之间，通过噪声多址信道和广播信道实现网络不经意传输的基本限制。主要结论是完美的不经意传输不可能实现，即使在渐近情况下也无法实现可忽略的泄漏。


<details>
  <summary>Details</summary>
Motivation: 研究在多方通信场景下，当各方可以访问一般三方无信号关联时，实现不经意传输的基本限制。探讨在诚实但好奇的参与者模型中，信道噪声和非信号相关资源对隐私保护能力的影响。

Method: 将共享资源建模为任意三方无信号盒子，为信道行为和产生的相关性提供统一视角。分析在噪声多址信道和广播信道设置下，使用非信号相关资源实现不经意传输的可能性。

Result: 主要结果表明完美的不经意传输不可能实现。在渐近情况下，即使可忽略的泄漏也无法实现，因为资源的重复使用会放大接收者区分非预期消息的能力。但接收者自身的隐私不受普遍不可能性限制。

Conclusion: 在诚实但好奇的各方之间，通过噪声信道实现完美的不经意传输存在根本性限制。虽然接收者隐私可以得到保护，但发送者隐私在渐近情况下无法实现完全保护，因为重复使用资源会增强接收者的区分能力。

Abstract: This work investigates the fundamental limits of implementing network oblivious transfer via noisy multiple access channels and broadcast channels between honest-but-curious parties when the parties have access to general tripartite non-signaling correlations. By modeling the shared resource as an arbitrary tripartite non-signaling box, we obtain a unified perspective on both the channel behavior and the resulting correlations. Our main result demonstrates that perfect oblivious transfer is impossible. In the asymptotic regime, we further show that even negligible leakage cannot be achieved, as repeated use of the resource amplifies the receiver(s)'s ability to distinguish messages that were not intended for him/them. In contrast, the receiver(s)'s own privacy is not subject to a universal impossibility limitation.

</details>


### [7] [Generative Decompression: Optimal Lossy Decoding Against Distribution Mismatch](https://arxiv.org/abs/2602.03505)
*Saeed R. Khosravirad,Ahmed Alkhateeb,Ingrid van de Voorde*

Main category: cs.IT

TL;DR: 论文研究压缩编码中分布不匹配时的最优解码策略，提出生成式解压缩方法，在解码端利用真实分布信息进行贝叶斯修正，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准化通信系统中编码器设计分布与实际源分布不匹配的问题，特别是在解码器可获得额外先验信息而编码器固定的场景。

Method: 提出生成式解压缩策略，在解码端基于真实分布进行条件期望估计；扩展到噪声信道传输，推导鲁棒软解码规则；并推广到任务导向解码，采用MAP检测。

Result: 实验表明生成式解压缩能显著缩小与理想联合优化基准的性能差距，在固定编码器情况下实现自适应高保真重建。

Conclusion: 生成式解压缩为分布不匹配问题提供了有效解决方案，通过解码端贝叶斯修正实现性能提升，适用于标准化通信系统。

Abstract: This paper addresses optimal decoding strategies in lossy compression where the assumed distribution for compressor design mismatches the actual (true) distribution of the source. This problem has immediate relevance in standardized communication systems where the decoder acquires side information or priors about the true distribution that are unavailable to the fixed encoder. We formally define the mismatched quantization problem, demonstrating that the optimal reconstruction rule, termed generative decompression, aligns with classical Bayesian estimation by taking the conditional expectation under the true distribution given the quantization indices and adapting it to fixed-encoder constraints. This strategy effectively performs a generative Bayesian correction on the decoder side, strictly outperforming the conventional centroid rule. We extend this framework to transmission over noisy channels, deriving a robust soft-decoding rule that quantifies the inefficiency of standard modular source--channel separation architectures under mismatch. Furthermore, we generalize the approach to task-oriented decoding, showing that the optimal strategy shifts from conditional mean estimation to maximum a posteriori (MAP) detection. Experimental results on Gaussian sources and deep-learning-based semantic classification demonstrate that generative decompression closes a vast majority of the performance gap to the ideal joint-optimization benchmark, enabling adaptive, high-fidelity reconstruction without modifying the encoder.

</details>


### [8] [Secure Decentralized Pliable Index Coding for Target Data Size](https://arxiv.org/abs/2602.03579)
*Anjali Padmanabhan,Danya Arun Bindhu,Nujoom Sageer Karat,Shanuja Sasi*

Main category: cs.IT

TL;DR: 研究去中心化可塑索引编码（DPIC）在异构边信息场景下的问题，提出一种确保每个客户端恰好获得T个消息的安全传输方案，并分析其通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有DPIC研究大多假设同质化设置（相同的边信息基数和单消息需求），这限制了在现实世界分布式系统中的应用，因为客户端通常拥有不同数量的先验信息。需要研究异构边信息基数下的DPIC问题，并考虑安全约束。

Method: 提出一种传输方案，协调客户端广播以最大化编码效率，同时确保每个客户端达到共同目标水平T。方案施加严格的安全约束：任何客户端不能获得超过T个消息，保证每个客户端最终恰好拥有T个消息。

Result: 分析了所提方案在安全约束下的通信成本。具体通信成本结果未在摘要中明确给出，但表明对方案性能进行了量化分析。

Conclusion: 该研究扩展了DPIC问题到异构边信息场景，提出了满足安全约束的高效传输方案，为现实世界分布式系统中的信息交换提供了更实用的解决方案。

Abstract: Decentralized Pliable Index Coding (DPIC) problem addresses efficient information exchange in distributed systems where clients communicate among themselves without a central server. An important consideration in DPIC is the heterogeneity of side-information and demand sizes. Although many prior works assume homogeneous settings with identical side-information cardinality and single message demands, these assumptions limit real-world applicability where clients typically possess unequal amounts of prior information. In this paper, we study DPIC problem under heterogeneous side-information cardinalities. We propose a transmission scheme that coordinates client broadcasts to maximize coding efficiency while ensuring that each client achieves a common target level $T$. In addition, we impose a strict security constraint that no client acquires more than the target $T$ number of messages, guaranteeing that each client ends up with exactly $T$ messages. We analyze the communication cost incurred by the proposed scheme under this security constraint.

</details>


### [9] [Sleep or Transmit: Dual-Mode Energy-Efficient Design for NOMA-Enabled Backscatter Networks](https://arxiv.org/abs/2602.03607)
*Hajar El Hassani,Mikael Gidlund*

Main category: cs.IT

TL;DR: 本文提出了一种结合非正交多址接入(NOMA)的上行链路反向散射通信设计，通过优化时间、功率和反射系数来最大化系统能量效率，适用于大规模物联网部署。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长需要频谱效率和能量效率兼备的通信系统。传统反向散射通信在密集部署中频谱效率下降，需要更高效的解决方案。

Method: 在双静态网络中，多个反向散射节点采集射频能量并在休眠和活跃模式间切换，构建分数规划问题，开发基于Dinkelbach的交替优化算法，提供闭式更新解。

Result: 分析揭示了两种运行模式，仿真显示该设计能自适应时间分配，比固定功率基准提高8%能量效率，比无休眠基准提高68%，比正交多址接入提高127%。

Conclusion: NOMA使能的反向散射通信是大规模物联网部署的可扩展、能量高效解决方案，通过联合优化时间、功率和反射系数实现显著性能提升。

Abstract: The rapid growth of Internet-of-Things (IoT) devices demands communication systems that are both spectrally efficient and energy frugal. Backscatter communication (BackCom) is an attractive low-power paradigm, but its spectral efficiency declines in dense deployments. This paper presents an uplink BackCom design that integrates non-orthogonal multiple access (NOMA) and maximizes system energy efficiency (EE). In a bistatic network where multiple backscatter nodes (BNs) harvest RF energy and alternate between sleep and active modes, we formulate a fractional program with coupled time, power, and reflection variables and develop a Dinkelbach-based alternating optimization (AO) algorithm with closed-form updates. Analysis reveals two operating modes depending on power availability, circuit demands and propagation conditions. Simulations show the proposed design adapts the time allocation, achieving up to 8% higher EE than fixed-power and 68% than no-sleep baselines, and delivering up to 127% EE gains over orthogonal multiple access (OMA). These results establish NOMA-enabled BackCom as a scalable, energy efficient solution for large-scale IoT deployments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [ResQ: Realistic Performance-Aware Query Generation](https://arxiv.org/abs/2602.02999)
*Zhengle Wang,Yanfei Zhang,Chunwei Liu*

Main category: cs.DB

TL;DR: ResQ是一个细粒度工作负载合成系统，能够从匿名化性能追踪生成可执行的SQL工作负载，准确匹配生产环境追踪中的查询执行目标和操作符分布。


<details>
  <summary>Details</summary>
Motivation: 数据库研发需要真实用户工作负载进行基准测试、优化和调优，但由于隐私法规限制，获取真实SQL查询非常困难。现有的云数据库性能追踪通常只提供高层执行统计，缺乏原始查询文本和数据，无法满足需要实际执行的场景。

Method: ResQ构建执行感知查询图，通过贝叶斯优化驱动的谓词搜索将其实例化为SQL，并在精确查询和参数化模板两个层次显式建模工作负载重复。采用搜索空间边界和轻量级本地成本模型加速优化。

Result: 在公开云追踪(Snowset, Redset)和新发布的工业追踪(Bendset)上的实验表明，ResQ显著优于现有基线方法，实现96.71%的令牌节省和86.97%的运行时间减少，CPU时间的最大Q-error降低14.8倍，扫描字节数降低997.7倍，并紧密匹配操作符组成。

Conclusion: ResQ填补了从匿名化性能追踪生成可执行SQL工作负载的空白，能够高保真、高效地复现生产环境追踪，为数据库研究和开发提供了实用的工作负载合成解决方案。

Abstract: Database research and development rely heavily on realistic user workloads for benchmarking, instance optimization, migration testing, and database tuning. However, acquiring real-world SQL queries is notoriously challenging due to strict privacy regulations. While cloud database vendors have begun releasing anonymized performance traces to the research community, these traces typi- cally provide only high-level execution statistics without the origi- nal query text or data, which is insufficient for scenarios that require actual execution. Existing tools fail to capture fine-grained perfor- mance patterns or generate runnable workloads that reproduce these public traces with both high fidelity and efficiency. To bridge this gap, we propose ResQ, a fine-grained workload synthesis sys- tem designed to generate executable SQL workloads that faithfully match the per-query execution targets and operator distributions of production traces. ResQ constructs execution-aware query graphs, instantiates them into SQL via Bayesian Optimization-driven pred- icate search, and explicitly models workload repetition through reuse at both exact-query and parameterized-template levels. To ensure practical scalability, ResQ combines search-space bounding with lightweight local cost models to accelerate optimization. Ex- periments on public cloud traces (Snowset, Redset) and a newly released industrial trace (Bendset) demonstrate that ResQ signif- icantly outperforms state-of-the-art baselines, achieving 96.71% token savings and a 86.97% reduction in runtime, while lowering maximum Q-error by 14.8x on CPU time and 997.7x on scanned bytes, and closely matching operator composition.

</details>


### [11] [Skill-Based Autonomous Agents for Material Creep Database Construction](https://arxiv.org/abs/2602.03069)
*Yue Wu,Tianhao Su,Shunbo Hu,Deng Pan*

Main category: cs.DB

TL;DR: 提出基于LLM的自主代理框架，从科学PDF中自动提取高质量材料数据，解决历史实验数据难以获取的瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 材料科学数据驱动研究面临瓶颈：大量历史实验数据被锁定在非结构化的科学文献文本和图表中，人工整理成本高且易出错

Method: 采用基于LLM的自主代理框架，通过模块化"技能型"架构执行语义过滤、多模态信息提取和物理验证等复杂认知任务

Result: 应用于243篇文献，图形数据数字化验证提取成功率超过90%；引入跨模态验证协议，视觉提取数据点与文本提取本构参数对齐度R²>0.99

Conclusion: 不仅为研究不同材料系统的时间依赖性变形提供了关键资源，还建立了可扩展的自主知识获取范式，为下一代自主实验室铺平道路

Abstract: The advancement of data-driven materials science is currently constrained by a fundamental bottleneck: the vast majority of historical experimental data remains locked within the unstructured text and rasterized figures of legacy scientific literature. Manual curation of this knowledge is prohibitively labor-intensive and prone to human error. To address this challenge, we introduce an autonomous, agent-based framework powered by Large Language Models (LLMs) designed to excavate high-fidelity datasets from scientific PDFs without human intervention. By deploying a modular "skill-based" architecture, the agent orchestrates complex cognitive tasks - including semantic filtering, multi-modal information extraction, and physics-informed validation. We demonstrate the efficacy of this framework by constructing a physically self-consistent database for material creep mechanics, a domain characterized by complex graphical trajectories and heterogeneous constitutive models. Applying the pipeline to 243 publications, the agent achieved a verified extraction success rate exceeding 90% for graphical data digitization. Crucially, we introduce a cross-modal verification protocol, demonstrating that the agent can autonomously align visually extracted data points with textually extracted constitutive parameters ($R^2 > 0.99$), ensuring the physical self-consistency of the database. This work not only provides a critical resource for investigating time-dependent deformation across diverse material systems but also establishes a scalable paradigm for autonomous knowledge acquisition, paving the way for the next generation of self-driving laboratories.

</details>


### [12] [StreamShield: A Production-Proven Resiliency Solution for Apache Flink at ByteDance](https://arxiv.org/abs/2602.03189)
*Yong Fang,Yuxing Han,Meng Wang,Yifan Zhang,Yue Ma,Chi Zhang*

Main category: cs.DB

TL;DR: StreamShield是字节跳动在Apache Flink集群中部署的生产级弹性解决方案，通过引擎和集群双视角设计，提升大规模分布式流处理系统的容错能力和运行稳定性。


<details>
  <summary>Details</summary>
Motivation: 在大规模生产环境中，分布式流处理系统面临集群规模大、业务多样性和运维开销高等挑战，难以同时保证容错性（快速从故障中恢复）和运行稳定性（正常条件下的可预测性能），这对满足严格的服务级别目标至关重要。

Method: StreamShield从引擎和集群两个互补视角设计，包含运行时优化、细粒度容错、混合复制策略和外部系统高可用性等关键技术，并建立了健壮的测试和部署流水线来确保生产发布的可靠性。

Result: 在生产集群上的广泛评估表明，StreamShield提出的技术方案高效且有效，能够显著提升大规模Flink集群的弹性和稳定性。

Conclusion: StreamShield是一个经过生产验证的弹性解决方案，成功解决了大规模分布式流处理系统在容错性和运行稳定性方面的挑战，为字节跳动的大规模Flink集群提供了可靠保障。

Abstract: Distributed Stream Processing Systems (DSPSs) form the backbone of real-time processing and analytics at ByteDance, where Apache Flink powers one of the largest production clusters worldwide. Ensuring resiliency, the ability to withstand and rapidly recover from failures, together with operational stability, which provides consistent and predictable performance under normal conditions, is essential for meeting strict Service Level Objectives (SLOs). However, achieving resiliency and stability in large-scale production environments remains challenging due to the cluster scale, business diversity, and significant operational overhead. In this work, we present StreamShield, a production-proven resiliency solution deployed in ByteDance's Flink clusters. Designed along complementary perspectives of the engine and cluster, StreamShield introduces key techniques to enhance resiliency, covering runtime optimization, fine-grained fault-tolerance, hybrid replication strategy, and high availability under external systems. Furthermore, StreamShield proposes a robust testing and deployment pipeline that ensures reliability and robustness in production releases. Extensive evaluations on a production cluster demonstrate the efficiency and effectiveness of techniques proposed by StreamShield.

</details>


### [13] [A Pipeline for ADNI Resting-State Functional MRI Processing and Quality Control](https://arxiv.org/abs/2602.03278)
*Saige Rutherford,Zeshawn Zahid,Robert C. Welsh,Andrea Avena-Koenigsberger,Vincent Koppelmans,Amanda F. Mejia*

Main category: cs.DB

TL;DR: 开发了一个用于ADNI静息态fMRI数据的标准化处理流程，解决数据异质性和临床-影像时间对齐问题，支持大规模阿尔茨海默病功能连接研究。


<details>
  <summary>Details</summary>
Motivation: ADNI提供了丰富的多模态神经影像数据，但静息态fMRI数据存在采集协议差异、数据质量不一、临床与影像时间不对齐等问题，导致许多研究只能使用小部分数据，限制了统计功效和可重复性。

Method: 开发了一个集成化处理流程，结合开源软件（Clinica、fMRIPrep、MRIQC）和定制工具，涵盖数据下载、临床-影像时间对齐、预处理和质量控制，支持ADNI-GO、ADNI-2和ADNI-3数据版本。

Result: 建立了透明可扩展的框架，输出符合BIDS-derivatives规范的高质量rs-fMRI时间序列数据，提供每个受试者和扫描的质量指标报告，便于严格数据筛选。

Conclusion: 该流程为ADNI fMRI数据的整理和使用提供了标准化解决方案，支持大规模功能生物标志物发现和阿尔茨海默病研究的整合多模态分析。

Abstract: The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a comprehensive multimodal neuroimaging resource for studying aging and Alzheimer's disease (AD). Since its second wave, ADNI has increasingly collected resting-state functional MRI (rs-fMRI), a valuable resource for discovering brain connectivity changes predictive of cognitive decline and AD. A major barrier to its use is the considerable variability in acquisition protocols and data quality, compounded by missing imaging sessions and inconsistencies in how functional scans temporally align with clinical assessments. As a result, many studies only utilize a small subset of the total rs-fMRI data, limiting statistical power, reproducibility, and the ability to study longitudinal functional brain changes at scale. Here, we describe a pipeline for ADNI rs-fMRI data that encompasses the download of necessary imaging and clinical data, temporally aligning the clinical and imaging data, preprocessing, and quality control. We integrate data curation and preprocessing across all ADNI sites and scanner types using a combination of open-source software (Clinica, fMRIPrep, and MRIQC) and bespoke tools. Quality metrics and reports are generated for each subject and session to facilitate rigorous data screening. All scripts and configuration files are available to enable reproducibility. The pipeline, which currently supports ADNI-GO, ADNI-2, and ADNI-3 data releases, outputs high-quality rs-fMRI time series data adhering to the BIDS-derivatives specification. This protocol provides a transparent and scalable framework for curating and utilizing ADNI fMRI data, empowering large-scale functional biomarker discovery and integrative multimodal analyses in Alzheimer's disease research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [14] [A two-player version of the assignment problem](https://arxiv.org/abs/2602.02628)
*Florian Galliot,Nacim Oijid,Jonas Sénizergues*

Main category: cs.GT

TL;DR: 竞争分配问题：两人轮流挑选代理，然后各自优化分配任务，计算效率差作为分数。Alice最大化分数，Bob最小化分数。该问题是PSPACE完全的，但在某些限制下可解。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在分析竞争性分配问题，模拟体育选秀、卡牌游戏等场景，其中两个实体争夺相同资源，然后使用这些资源相互竞争。研究这种博弈的计算复杂性对于理解实际竞争环境中的策略选择具有重要意义。

Method: 引入竞争分配问题作为经典分配问题的两人博弈版本。Alice和Bob轮流挑选代理，然后各自解决分配问题以最大化各自代理的效率总和。研究采用计算复杂性分析方法，证明该问题在不同限制条件下的复杂性类别。

Result: 1. 竞争分配问题是PSPACE完全的，即使限制每个代理最多只有两个非零效率值。
2. 当每个代理最多只有一个非零效率值时，该问题在参数化复杂度类XP中（参数为任务数量）。
3. 当只有两个任务时，最优分数可以在线性时间内计算。

Conclusion: 竞争分配问题在一般情况下是计算困难的（PSPACE完全），但在某些自然限制条件下变得可处理。这些结果为理解竞争性资源分配博弈的计算复杂性提供了理论框架，对实际应用如体育选秀策略分析具有指导意义。

Abstract: We introduce the competitive assignment problem, a two-player version of the well-known assignment problem. Given a set of tasks and a set of agents with different efficiencies for different tasks, Alice and Bob take turns picking agents one by one. Once all agents have been picked, Alice and Bob compute the optimal values $s_A$ and $s_B$ for the assignment problem on their respective sets of agents, i.e. they assign their own agents to tasks (with at most one agent per task and at most one task per agent) so as to maximize the sum of the efficiencies. The score of the game is then defined as $s_A-s_B$. Alice aims at maximizing the score, while Bob aims at minimizing it. This problem can model drafts in sports and card games, or more generally situations where two entities fight for the same resources and then use them to compete against each other. We show that the problem is PSPACE-complete, even restricted to agents that have at most two nonzero efficiencies. On the other hand, in the case of agents having at most one nonzero efficiency, the problem lies in XP parameterized by the number of tasks, and the optimal score can be computed in linear time when there are only two tasks.

</details>


### [15] [Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow](https://arxiv.org/abs/2602.03145)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 提出"智能体AI互联网"框架，实现分布式异构智能体动态联盟协作，解决现有集中式智能体系统在可扩展性、专业化和互操作性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体AI系统大多采用集中式和单体架构，限制了系统的可扩展性、专业化能力和互操作性，需要新的分布式协作框架来解决这些问题。

Method: 提出网络原生智能体协作模型，引入激励兼容的工作流联盟可行性框架，考虑能力覆盖、网络局部性和经济可实现性；提出最小努力联盟选择问题和去中心化联盟形成算法；框架可作为Model Context Protocol（MCP）之上的协调层运行。

Result: 通过医疗案例研究证明，领域专业化、云边异构性和动态联盟形成能够实现可扩展、有弹性且经济可行的智能体工作流。

Conclusion: 该工作为新兴的智能体AI互联网时代奠定了原则性协调和可扩展性的基础，为分布式智能体协作提供了系统化框架。

Abstract: Large language models (LLMs) have enabled a new class of agentic AI systems that reason, plan, and act by invoking external tools. However, most existing agentic architectures remain centralized and monolithic, limiting scalability, specialization, and interoperability. This paper proposes a framework for scalable agentic intelligence, termed the Internet of Agentic AI, in which autonomous, heterogeneous agents distributed across cloud and edge infrastructure dynamically form coalitions to execute task-driven workflows. We formalize a network-native model of agentic collaboration and introduce an incentive-compatible workflow-coalition feasibility framework that integrates capability coverage, network locality, and economic implementability. To enable scalable coordination, we formulate a minimum-effort coalition selection problem and propose a decentralized coalition formation algorithm. The proposed framework can operate as a coordination layer above the Model Context Protocol (MCP). A healthcare case study demonstrates how domain specialization, cloud-edge heterogeneity, and dynamic coalition formation enable scalable, resilient, and economically viable agentic workflows. This work lays the foundation for principled coordination and scalability in the emerging era of Internet of Agentic AI.

</details>


### [16] [Dynamic Programming for Epistemic Uncertainty in Markov Decision Processes](https://arxiv.org/abs/2602.03381)
*Axel Benyamine,Julien Grand-Clément,Marek Petrik,Michael I. Jordan,Alain Durmus*

Main category: cs.GT

TL;DR: 提出一个模糊厌恶MDP通用理论，将不确定转移概率视为随机变量，通过风险度量评估策略回报，统一多种具有认知不确定性的MDP模型。


<details>
  <summary>Details</summary>
Motivation: 现有MDP模型在处理认知不确定性时缺乏统一框架，不同风险度量方法分散，需要建立通用理论来统一各种模糊厌恶MDP变体，并明确动态规划原则的适用范围。

Method: 将不确定转移概率建模为随机变量，应用风险度量评估策略的随机回报，扩展值函数和贝尔曼算子的概念，建立动态规划原则的数学基础。

Result: 建立了模糊厌恶MDP的统一框架，证明了动态规划原则的存在性（平稳策略、值迭代和策略迭代算法），并完全刻画了与动态规划兼容的定律不变风险度量。

Conclusion: 该理论统一了多种MDP变体，清晰界定了动态规划范式的适用范围，明确了哪些风险度量需要脱离传统动态规划框架，为模糊厌恶决策提供了理论基础。

Abstract: In this paper, we propose a general theory of ambiguity-averse MDPs, which treats the uncertain transition probabilities as random variables and evaluates a policy via a risk measure applied to its random return. This ambiguity-averse MDP framework unifies several models of MDPs with epistemic uncertainty for specific choices of risk measures. We extend the concepts of value functions and Bellman operators to our setting. Based on these objects, we establish the consequences of dynamic programming principles in this framework (existence of stationary policies, value and policy iteration algorithms), and we completely characterize law-invariant risk measures compatible with dynamic programming. Our work draws connections among several variants of MDP models and fully delineates what is possible under the dynamic programming paradigm and which risk measures require leaving it.

</details>


### [17] [Toward a Sustainable Federated Learning Ecosystem: A Practical Least Core Mechanism for Payoff Allocation](https://arxiv.org/abs/2602.03387)
*Zhengwei Ni,Zhidu Li,Wei Chen,Zhaoyang Zhang,Zehua Wang,F. Richard Yu,Victor C. M. Leung*

Main category: cs.GT

TL;DR: 提出基于最小核心概念的联盟稳定收益分配框架，通过最小化所有潜在子群的最大不满度来维持联盟凝聚力，并设计堆栈剪枝算法实现大规模网络的高效实现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习等新兴网络范式需要公平稳定的收益分配机制来维持协作环境的可持续性，传统方法在保证联盟稳定性方面存在不足。

Method: 引入基于最小核心概念的收益分配框架，采用堆栈剪枝算法优化计算效率与分配精度，适用于大规模网络环境。

Result: 在联邦入侵检测案例研究中，该机制能准确识别关键贡献者和战略联盟，促进稳定协作并培育可持续的联邦学习生态系统。

Conclusion: 提出的最小核心框架有效平衡了计算效率与分配精度，为联邦学习等协作环境提供了稳定可持续的收益分配解决方案。

Abstract: Emerging network paradigms and applications increasingly rely on federated learning (FL) to enable collaborative intelligence while preserving privacy. However, the sustainability of such collaborative environments hinges on a fair and stable payoff allocation mechanism. Focusing on coalition stability, this paper introduces a payoff allocation framework based on the least core (LC) concept. Unlike traditional methods, the LC prioritizes the cohesion of the federation by minimizing the maximum dissatisfaction among all potential subgroups, ensuring that no participant has an incentive to break away. To adapt this game-theoretic concept to practical, large-scale networks, we propose a streamlined implementation with a stack-based pruning algorithm, effectively balancing computational efficiency with allocation precision. Case studies in federated intrusion detection demonstrate that our mechanism correctly identifies pivotal contributors and strategic alliances. The results confirm that the practical LC framework promotes stable collaboration and fosters a sustainable FL ecosystem.

</details>


### [18] [Sequential Linear Contracts on Matroids](https://arxiv.org/abs/2602.03543)
*Kanstantsin Pashkovich,Jacob Skitsko,Yun Xing*

Main category: cs.GT

TL;DR: 研究在拟阵约束下的顺序合同问题，重点关注线性合同，建立了最优线性合同与拟阵可靠性问题之间的等价关系


<details>
  <summary>Details</summary>
Motivation: 在顺序决策环境中，代理需要逐步采取行动，每个行动都有随机收益和成本。委托人需要通过合同激励代理采取行动，同时受到拟阵约束（行动子集必须构成拟阵的独立集）。需要设计有效的合同机制来最大化委托人效用。

Method: 专注于研究线性合同（委托人将总收益的一定比例转移给代理），将最优线性合同的计算问题与拟阵可靠性问题建立联系。通过向给定拟阵添加元素的平行副本，证明这两个问题在一般情况下是等价的。

Result: 建立了寻找最优线性合同（或计算相应委托人效用）与拟阵可靠性问题之间的等价关系。这种等价关系为求解最优合同提供了新的计算框架。

Conclusion: 在拟阵约束下的顺序合同问题中，线性合同的最优化可以转化为拟阵可靠性问题，这为合同设计提供了理论连接和计算工具。

Abstract: In this work, we study sequential contracts under matroid constraints. In the sequential setting, an agent can take actions one by one. After each action, the agent observes the stochastic value of the action and then decides which action to take next, if any. At the end, the agent decides what subset of taken actions to use for the principal's reward; and the principal receives the total value of this subset as a reward. Taking each action induces a certain cost for the agent. Thus, to motivate the agent to take actions the principal is expected to offer an appropriate contract. A contract describes the payment from the principal to the agent as a function of the principal's reward obtained through the agent's actions. In this work, we concentrate on studying linear contracts, i.e.\ the contracts where the principal transfers a fraction of their total reward to the agent. We assume that the total principal's reward is calculated based on a subset of actions that forms an independent set in a given matroid. We establish a relationship between the problem of finding an optimal linear contract (or computing the corresponding principal's utility) and the so called matroid (un)reliability problem. Generally, the above problems turn out to be equivalent subject to adding parallel copies of elements to the given matroid.

</details>


### [19] [Efficient Investment in Multi-Agent Models of Public Transportation](https://arxiv.org/abs/2602.03687)
*Martin Bullinger,Edith Elkind,Kassian Köck*

Main category: cs.GT

TL;DR: 该论文研究了公共交通资源分配的两种多智能体模型，发现平等主义福利的近似最优解计算是NP完全的，而功利主义最优解可在多项式时间内找到。


<details>
  <summary>Details</summary>
Motivation: 研究如何在有限、不可分割的公共交通资源（如公交站点或路段改进）上进行投资决策，考虑多个用户的旅行需求，平衡功利主义和平等主义两种社会福利标准。

Method: 采用两种多智能体模型：1) 线性图上的站点选址模型；2) 加权图上的路段改进模型。使用计算复杂性理论分析，结合Dijkstra算法和动态规划设计多项式时间算法，并证明NP完全性和不可近似性结果。

Result: 1) 在线性图模型中，功利主义最优解可在多项式时间内计算，但平等主义福利的近似最优解是NP完全的；2) 在加权图模型中，对于1-2个智能体存在多项式时间算法，但智能体数量可变时，功利主义和平等主义福利都是NP完全且不可近似的。

Conclusion: 即使在简单的线性图拓扑上，平等主义福利优化也是计算困难的，而功利主义优化相对容易。这些结果对铁路网络设计等相关模型具有启示意义，表明社会福利标准的选择对计算复杂性有重要影响。

Abstract: We study two stylized, multi-agent models aimed at investing a limited, indivisible resource in public transportation. In the first model, we face the decision of which potential stops to open along a (e.g., bus) path, given agents' travel demands. While it is known that utilitarian optimal solutions can be identified in polynomial time, we find that computing approximately optimal solutions with respect to egalitarian welfare is NP-complete. This is surprising as we operate on the simple topology of a line graph.
  In the second model, agents navigate a more complex network modeled by a weighted graph where edge weights represent distances. We face the decision of improving travel time along a fixed number of edges. We provide a polynomial-time algorithm that combines Dijkstra's algorithm with a dynamical program to find the optimal decision for one or two agents. By contrast, if the number of agents is variable, we find \np-completeness and inapproximability results for utilitarian and egalitarian welfare. Moreover, we demonstrate implications of our results for a related model of railway network design.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [20] [Learning-augmented smooth integer programs with PAC-learnable oracles](https://arxiv.org/abs/2602.02505)
*Hao-Yuan He,Ming Li*

Main category: cs.DS

TL;DR: 提出一个学习增强框架，通过预测oracle构建线性代理目标，解决MAX-CUT、MAX-k-SAT等整数规划问题，保证解的一致性和对预测误差的平滑性，并证明oracle的PAC可学习性。


<details>
  <summary>Details</summary>
Motivation: 研究学习增强算法在光滑整数规划中的应用，旨在将经典密集区域的易处理近似扩展到近密集区域，同时超越oracle存在假设，建立其可学习性理论。

Method: 引入预测oracle构建目标函数的线性代理，通过线性规划求解后执行舍入过程，确保解的一致性和对预测误差的平滑性，并证明算法类的伪维度有界。

Result: 框架成功将易处理近似从密集区域扩展到近密集区域，证明了oracle的PAC可学习性，表明通过多项式样本可以学习到接近最优期望性能的oracle。

Conclusion: 提出的学习增强框架为光滑整数规划问题提供了有效的解决方案，不仅保证了理论性能，还建立了实际可学习性，为学习增强算法在组合优化中的应用提供了新思路。

Abstract: This paper investigates learning-augmented algorithms for smooth integer programs, covering canonical problems such as MAX-CUT and MAX-k-SAT. We introduce a framework that incorporates a predictive oracle to construct a linear surrogate of the objective, which is then solved via linear programming followed by a rounding procedure. Crucially, our framework ensures that the solution quality is both consistent and smooth against prediction errors. We demonstrate that this approach effectively extends tractable approximations from the classical dense regime to the near-dense regime. Furthermore, we go beyond the assumption of oracle existence by establishing its PAC-learnability. We prove that the induced algorithm class possesses a bounded pseudo-dimension, thereby ensuring that an oracle with near-optimal expected performance can be learned with polynomial samples.

</details>


### [21] [On the Complexity of Maximal/Closed Frequent Tree Mining for Bounded Height Trees](https://arxiv.org/abs/2602.03436)
*Kenta Komoto,Kazuhiro Kurita,Hirotaka Ono*

Main category: cs.DS

TL;DR: 该论文研究了频繁最大/闭树的枚举问题，填补了树高度在60以下时复杂度分析的研究空白，针对有序/无序树以及最大/闭变体建立了复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 枚举所有频繁最大/闭树是数据挖掘中的经典核心问题。虽然已有许多实用算法，但在树高度的"现实假设"下，其复杂度尚未明确。已知树高度≥60时问题变难，但较小高度下的复杂度一直未解决。

Method: 通过建立树挖掘问题的复杂度结果，考虑了多种设置：包括有序树和无序树，以及最大和闭变体。

Result: 填补了树高度小于60时的复杂度分析空白，为不同设置下的树挖掘问题提供了明确的复杂度界限。

Conclusion: 该研究解决了树挖掘问题在现实树高度假设下的复杂度分析缺口，为有序/无序树以及最大/闭变体提供了完整的复杂度理论框架。

Abstract: In this paper, we address the problem of enumerating all frequent maximal/closed trees. This is a classical and central problem in data mining. Although many practical algorithms have been developed for this problem, its complexity under ``realistic assumptions'' on tree height has not been clarified. More specifically, while it was known that the mining problem becomes hard when the tree height is at least 60, the complexity for cases where the tree height is smaller has not yet been clarified. We resolve this gap by establishing results for these tree mining problems under several settings, including ordered and unordered trees, as well as maximal and closed variants.

</details>


### [22] [ZOR filters: fast and smaller than fuse filters](https://arxiv.org/abs/2602.03525)
*Antoine Limasset*

Main category: cs.DS

TL;DR: ZOR过滤器是XOR/融合过滤器的确定性扩展，保证构建终止，通过放弃少量键并用辅助结构存储来保持仅假阳性语义


<details>
  <summary>Details</summary>
Motivation: 现有XOR和融合过滤器虽然性能优秀，但其基于剥离的构建方法仅以高概率成功，这给确定性构建带来了困难。需要一种既能保证构建终止，又能保持类似性能的解决方案。

Method: ZOR过滤器采用确定性剥离方法，放弃一小部分键（通常低于1%），并将这些键存储在紧凑的辅助结构中。查询时使用与XOR过滤器相同的XOR机制，并通过辅助结构检查被放弃的键。

Result: ZOR过滤器能够实现接近信息论下界（log₂(1/ε)）的1%以内的开销，同时保持类似融合过滤器的查询性能。构建速度目前比优化后的融合构建器慢几倍，但这是工程优化目标。

Conclusion: ZOR过滤器在保证构建确定性的同时，保持了XOR/融合过滤器的性能优势，通过放弃少量键并用辅助结构处理，实现了理论保证与实际性能的良好平衡。

Abstract: Probabilistic membership filters support fast approximate membership queries with a controlled false-positive probability $\varepsilon$ and are widely used across storage, analytics, networking, and bioinformatics \cite{chang2008bigtable,dayan2018optimalbloom,broder2004network,harris2020improved,marchet2023scalable,chikhi2025logan,hernandez2025reindeer2}. In the static setting, state-of-the-art designs such as XOR and fuse filters achieve low overhead and very fast queries, but their peeling-based construction succeeds only with high probability, which complicates deterministic builds \cite{graf2020xor,graf2022binary,ulrich2023taxor}.
  We introduce \emph{ZOR filters}, a deterministic continuation of XOR/fuse filters that guarantees construction termination while preserving the same XOR-based query mechanism. ZOR replaces restart-on-failure with deterministic peeling that abandons a small fraction of keys, and restores false-positive-only semantics by storing the remainder in a compact auxiliary structure. In our experiments, the abandoned fraction drops below $1\%$ for moderate arity (e.g., $N\ge 5$), so the auxiliary handles a negligible fraction of keys. As a result, ZOR filters can achieve overhead within $1\%$ of the information-theoretic lower bound $\log_2(1/\varepsilon)$ while retaining fuse-like query performance; the additional cost is concentrated on negative queries due to the auxiliary check. Our current prototype builds several-fold slower than highly optimized fuse builders because it maintains explicit incidence information during deterministic peeling; closing this optimisation gap is an engineering target.

</details>


### [23] [Perfect Network Resilience in Polynomial Time](https://arxiv.org/abs/2602.03827)
*Matthias Bentert,Stefan Schmid*

Main category: cs.DS

TL;DR: 该论文提出了完美弹性的完整特征化，并设计了高效算法来判定和计算完美弹性路由规则


<details>
  <summary>Details</summary>
Motivation: 现代通信网络需要本地快速重路由机制来应对链路故障，但完美弹性（任何数据包都能在故障后连通的情况下从源路由到目标）并非总是可实现的。虽然本地重路由算法设计受到广泛关注，但对于何时能实现完美弹性仍缺乏详细理解。

Method: 提出了完美弹性的完整特征化理论，基于此设计了O(n)时间算法来判断给定实例是否具有完美弹性，以及O(mn)时间算法来计算完美弹性重路由规则。算法使用简单的"跳过"重路由规则结构，即根据有序优先级列表选择替代链路，跳过故障链路。

Result: 实现了完美弹性的完整特征化，证明了在完美弹性背景下，跳过重路由规则与更一般的重路由规则具有同等能力。这为Chiesa等人2017年提出的长期开放问题提供了部分肯定答案。

Conclusion: 该研究填补了完美弹性可达成条件的理论空白，提供了高效算法和简单规则结构，对网络路由设计有重要意义，并解决了相关领域的重要开放问题。

Abstract: Modern communication networks support local fast rerouting mechanisms to quickly react to link failures: nodes store a set of conditional rerouting rules which define how to forward an incoming packet in case of incident link failures. The rerouting decisions at any node $v$ must rely solely on local information available at $v$: the link from which a packet arrived at $v$, the target of the packet, and the incident link failures at $v$. Ideally, such rerouting mechanisms provide perfect resilience: any packet is routed from its source to its target as long as the two are connected in the underlying graph after the link failures. Already in their seminal paper at ACM PODC '12, Feigenbaum, Godfrey, Panda, Schapira, Shenker, and Singla showed that perfect resilience cannot always be achieved. While the design of local rerouting algorithms has received much attention since then, we still lack a detailed understanding of when perfect resilience is achievable.
  This paper closes this gap and presents a complete characterization of when perfect resilience can be achieved. This characterization also allows us to design an $O(n)$-time algorithm to decide whether a given instance is perfectly resilient and an $O(nm)$-time algorithm to compute perfectly resilient rerouting rules whenever it is. Our algorithm is also attractive for the simple structure of the rerouting rules it uses, known as skipping in the literature: alternative links are chosen according to an ordered priority list (per in-port), where failed links are simply skipped. Intriguingly, our result also implies that in the context of perfect resilience, skipping rerouting rules are as powerful as more general rerouting rules. This partially answers a long-standing open question by Chiesa, Nikolaevskiy, Mitrovic, Gurtov, Madry, Schapira, and Shenker [IEEE/ACM Transactions on Networking, 2017] in the affirmative.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [24] [Design and Evaluation of Whole-Page Experience Optimization for E-commerce Search](https://arxiv.org/abs/2602.02514)
*Pratik Lahiri,Bingqing Ge,Zhou Qin,Aditya Jumde,Shuning Huo,Lucas Scottini,Yi Liu,Mahmoud Mamlouk,Wenyang Liu*

Main category: cs.IR

TL;DR: 提出全页面体验优化框架，解决电商搜索结果页从线性列表向复杂非线性布局演变带来的挑战，同时优化长期用户满意度指标


<details>
  <summary>Details</summary>
Motivation: 电商搜索结果页正从线性列表演变为复杂的非线性布局，传统基于位置的排序模型已不适用。现有优化框架通常只最大化短期信号（如点击率、当日收入），而长期满意度指标（如预期两周收入）涉及延迟反馈和长期信用分配难题

Method: 提出全页面体验优化框架，与传统列表式排序器不同，该方法显式建模商品相关性、2D位置布局和视觉元素之间的相互作用。使用因果框架基于准实验数据开发长期用户满意度度量指标

Result: 通过行业规模的A/B测试验证，模型在品牌相关性（主要客户体验指标）上提升了1.86%，同时实现了+0.05%的统计显著收入增长

Conclusion: 全页面体验优化框架能够有效解决现代电商搜索结果页的复杂布局挑战，同时优化短期和长期用户满意度指标，实现客户体验和商业价值的双赢

Abstract: E-commerce Search Results Pages (SRPs) are evolving from linear lists to complex, non-linear layouts, rendering traditional position-biased ranking models insufficient. Moreover, existing optimization frameworks typically maximize short-term signals (e.g., clicks, same-day revenue) because long-term satisfaction metrics (e.g., expected two-week revenue) involve delayed feedback and challenging long-horizon credit attribution. To bridge these gaps, we propose a novel Whole-Page Experience Optimization Framework. Unlike traditional list-wise rankers, our approach explicitly models the interplay between item relevance, 2D positional layout, and visual elements. We use a causal framework to develop metrics for measuring long-term user satisfaction based on quasi-experimental data. We validate our approach through industry-scale A/B testing, where the model demonstrated a 1.86% improvement in brand relevance (our primary customer experience metric) while simultaneously achieving a statistically significant revenue uplift of +0.05%

</details>


### [25] [Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval](https://arxiv.org/abs/2602.02827)
*Roi Pony,Adi Raz,Oshri Naparstek,Idan Friedman,Udi Barzelay*

Main category: cs.IR

TL;DR: Col-Bandit：一种查询时剪枝算法，通过将重排序建模为有限总体Top-K识别问题，自适应地揭示必要的MaxSim条目，在保持排序质量的同时将计算量减少5倍。


<details>
  <summary>Details</summary>
Motivation: 多向量延迟交互检索器（如ColBERT）虽然检索质量高，但查询时计算成本巨大，需要为每个候选文档计算token级别的MaxSim交互。单向量近似虽然降低成本但准确率损失大，需要一种能在查询时高效剪枝的方法。

Method: 将重排序建模为有限总体Top-K识别问题，维护基于部分观测文档分数的不确定性感知边界，自适应地揭示确定top结果所需的(document, query token) MaxSim条目，通过统计决策边界和可调松弛度实现动态剪枝。

Result: 在文本（BEIR）和多模态（REAL-MM-RAG）基准测试中，Col-Bandit在保持排序保真度的同时，将MaxSim FLOPs减少高达5倍，表明密集延迟交互评分存在大量冗余，可在查询时高效识别和剪枝。

Conclusion: Col-Bandit是一种零样本、即插即用的查询时剪枝层，无需索引修改、离线预处理或模型重训练，能有效减少多向量检索器的计算负担，同时保持高质量的检索结果。

Abstract: Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-$K$ identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5$\times$, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.

</details>


### [26] [Efficiency Optimizations for Superblock-based Sparse Retrieval](https://arxiv.org/abs/2602.02883)
*Parker Carlson,Wentai Xie,Rohil Shah,Tao Yang*

Main category: cs.IR

TL;DR: 提出一种简单有效的超块剪枝方案，结合紧凑索引结构和零配置，在保持竞争力的同时降低计算开销


<details>
  <summary>Details</summary>
Motivation: 学习稀疏检索结合了语言模型的语义匹配和高效CPU友好算法，但现有超块剪枝方法计算开销较大

Method: 提出简单有效的超块剪枝方案，结合紧凑索引结构和鲁棒的零配置方法

Result: 在MS MARCO和BEIR数据集上验证，方案能显著降低计算开销同时保持竞争力

Conclusion: 提出的超块剪枝方案是高效稀疏检索的强有力替代方案

Abstract: Learned sparse retrieval (LSR) is a popular method for first-stage retrieval because it combines the semantic matching of language models with efficient CPU-friendly algorithms. Previous work aggregates blocks into "superblocks" to quickly skip the visitation of blocks during query processing by using an advanced pruning heuristic. This paper proposes a simple and effective superblock pruning scheme that reduces the overhead of superblock score computation while preserving competitive relevance. It combines this scheme with a compact index structure and a robust zero-shot configuration that is effective across LSR models and multiple datasets. This paper provides an analytical justification and evaluation on the MS MARCO and BEIR datasets, demonstrating that the proposed scheme can be a strong alternative for efficient sparse retrieval.

</details>


### [27] [ALPBench: A Benchmark for Attribution-level Long-term Personal Behavior Understanding](https://arxiv.org/abs/2602.03056)
*Lu Ren,Junda She,Xinchen Luo,Tao Wang,Xin Ye,Xu Zhang,Muxuan Wang,Xiao Yang,Chenguang Wang,Fei Xie,Yiwei Zhou,Danjun Wu,Guodong Zhang,Yifei Hu,Guoying Zheng,Shujie Yang,Xingmei Wang,Shiyao Wang,Yukun Zhou,Fan Yang,Size Li,Kuo Cai,Qiang Luo,Ruiming Tang,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: ALPBench是一个用于评估LLMs在属性级长期个人行为理解方面能力的基准测试，专注于预测用户感兴趣的属性组合而非具体物品，支持对新增物品进行真实评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化推荐方面展现出潜力，但准确捕捉用户偏好仍是关键挑战。现有基准测试主要关注物品推荐，缺乏对用户长期行为中属性级偏好的系统评估。

Method: ALPBench将用户历史行为表示为自然语言序列，预测用户感兴趣的属性组合而非具体物品。它基于长期历史行为建模用户偏好，而非用户的显式请求，更好地反映持久兴趣。

Result: ALPBench能够对新增物品进行真实评估，支持可解释的、基于推理的个性化。该基准测试揭示了预测属性组合任务对当前LLMs仍具挑战性，因为需要捕捉多个属性间的复杂交互并对长期用户行为序列进行推理。

Conclusion: ALPBench为系统评估LLMs在属性级长期个人行为理解方面的能力提供了新基准，有助于推动更精准、可解释的个性化推荐系统发展。

Abstract: Recent advances in large language models have highlighted their potential for personalized recommendation, where accurately capturing user preferences remains a key challenge. Leveraging their strong reasoning and generalization capabilities, LLMs offer new opportunities for modeling long-term user behavior. To systematically evaluate this, we introduce ALPBench, a Benchmark for Attribution-level Long-term Personal Behavior Understanding. Unlike item-focused benchmarks, ALPBench predicts user-interested attribute combinations, enabling ground-truth evaluation even for newly introduced items. It models preferences from long-term historical behaviors rather than users' explicitly expressed requests, better reflecting enduring interests. User histories are represented as natural language sequences, allowing interpretable, reasoning-based personalization. ALPBench enables fine-grained evaluation of personalization by focusing on the prediction of attribute combinations task that remains highly challenging for current LLMs due to the need to capture complex interactions among multiple attributes and reason over long-term user behavior sequences.

</details>


### [28] [PAMAS: Self-Adaptive Multi-Agent System with Perspective Aggregation for Misinformation Detection](https://arxiv.org/abs/2602.03158)
*Zongwei Wang,Min Gao,Junliang Yu,Tong Chen,Chenghua Lin*

Main category: cs.IR

TL;DR: PAMAS是一个用于社交媒体虚假信息检测的多智能体框架，通过视角聚合机制解决传统多智能体系统中的信息淹没问题，显著提升了检测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体虚假信息具有多样性和上下文依赖性，检测难度大。传统多智能体系统存在信息淹没问题，即大量真实内容淹没了稀疏而微弱的欺骗线索，导致智能体倾向于关注主导模式，而智能体间通信进一步放大了这种偏见。

Method: 提出PAMAS框架，采用分层、视角感知的聚合机制来突出异常线索。框架包含三种角色：审计员从专业特征子集中捕获异常线索；协调员聚合审计员的视角以增强覆盖范围同时保持多样性；决策者配备演化记忆和完整上下文访问权限，综合所有下级洞察做出最终判断。此外，通过自适应机制进行动态拓扑优化和基于路由的推理，提高多智能体协作效率。

Result: 在多个基准数据集上的广泛实验表明，PAMAS在虚假信息检测方面实现了卓越的准确性和效率，提供了可扩展且可信赖的检测方法。

Conclusion: PAMAS通过视角聚合机制有效解决了多智能体系统中的信息淹没问题，为社交媒体虚假信息检测提供了一个高效、可扩展的解决方案，展示了多智能体系统在对抗虚假信息方面的潜力。

Abstract: Misinformation on social media poses a critical threat to information credibility, as its diverse and context-dependent nature complicates detection. Large language model-empowered multi-agent systems (MAS) present a promising paradigm that enables cooperative reasoning and collective intelligence to combat this threat. However, conventional MAS suffer from an information-drowning problem, where abundant truthful content overwhelms sparse and weak deceptive cues. With full input access, agents tend to focus on dominant patterns, and inter-agent communication further amplifies this bias. To tackle this issue, we propose PAMAS, a multi-agent framework with perspective aggregation, which employs hierarchical, perspective-aware aggregation to highlight anomaly cues and alleviate information drowning. PAMAS organizes agents into three roles: Auditors, Coordinators, and a Decision-Maker. Auditors capture anomaly cues from specialized feature subsets; Coordinators aggregate their perspectives to enhance coverage while maintaining diversity; and the Decision-Maker, equipped with evolving memory and full contextual access, synthesizes all subordinate insights to produce the final judgment. Furthermore, to improve efficiency in multi-agent collaboration, PAMAS incorporates self-adaptive mechanisms for dynamic topology optimization and routing-based inference, enhancing both efficiency and scalability. Extensive experiments on multiple benchmark datasets demonstrate that PAMAS achieves superior accuracy and efficiency, offering a scalable and trustworthy way for misinformation detection.

</details>


### [29] [Distribution-Aware End-to-End Embedding for Streaming Numerical Features in Click-Through Rate Prediction](https://arxiv.org/abs/2602.03223)
*Jiahao Liu,Hongji Ruan,Weimin Zhang,Ziye Tong,Derick Tang,Zhanpeng Zeng,Qinsong Zeng,Peng Zhang,Tun Lu,Ning Gu*

Main category: cs.IR

TL;DR: 提出DAES框架，用于流式训练场景下的数值特征嵌入，通过自适应调制机制整合分布信息，解决传统静态分箱方法语义漂移和神经网络方法忽略分布信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统静态分箱方法依赖离线统计，存在语义漂移问题；神经网络嵌入方法虽然支持端到端学习，但忽略了显式的分布信息。在流式训练环境中，数值特征违反i.i.d.假设，且数值分布的上下文依赖性常被忽视。

Method: 提出DAES端到端框架：1）基于蓄水池采样的高效分布估计方法；2）两种场感知分布调制策略，用于捕捉流式分布和场依赖语义。

Result: DAES在离线和在线实验中显著优于现有方法，已在拥有数亿日活用户的领先短视频平台全面部署。

Conclusion: DAES成功解决了流式训练中数值特征嵌入的挑战，通过整合分布信息和自适应调制机制，实现了更有效的点击率预测。

Abstract: This paper explores effective numerical feature embedding for Click-Through Rate prediction in streaming environments. Conventional static binning methods rely on offline statistics of numerical distributions; however, this inherently two-stage process often triggers semantic drift during bin boundary updates. While neural embedding methods enable end-to-end learning, they often discard explicit distributional information. Integrating such information end-to-end is challenging because streaming features often violate the i.i.d. assumption, precluding unbiased estimation of the population distribution via the expectation of order statistics. Furthermore, the critical context dependency of numerical distributions is often neglected. To this end, we propose DAES, an end-to-end framework designed to tackle numerical feature embedding in streaming training scenarios by integrating distributional information with an adaptive modulation mechanism. Specifically, we introduce an efficient reservoir-sampling-based distribution estimation method and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics. DAES significantly outperforms existing approaches as demonstrated by extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users.

</details>


### [30] [To Search or Not to Search: Aligning the Decision Boundary of Deep Search Agents via Causal Intervention](https://arxiv.org/abs/2602.03304)
*Wenlin Zhang,Kuicai Dong,Junyi Li,Yingyi Zhang,Xiaopeng Li,Pengyue Jia,Yi Wen,Derong Xu,Maolin Wang,Yichao Wang,Yong Liu,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出DAS框架，通过因果干预诊断和决策边界对齐，解决深度搜索代理中过度搜索和搜索不足的问题，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前深度搜索代理存在效率低下的问题，因为它们无法准确判断何时停止搜索并开始回答。这是由于结果中心训练导致的决策边界错位，造成过度搜索（已有足够知识仍冗余搜索）和搜索不足（过早终止导致错误答案）。

Method: 提出包含两个关键组件的综合框架：1）基于因果干预的诊断方法，通过比较每个决策点的实际和反事实轨迹来识别边界错误；2）决策边界对齐（DAS），从因果反馈构建偏好数据集，并通过偏好优化对齐策略。

Result: 实验表明决策边界错误在现有先进代理中普遍存在。DAS方法有效校准这些边界，显著减少过度搜索和搜索不足，在准确性和效率方面取得实质性提升。

Conclusion: DAS框架通过因果干预诊断和决策边界对齐，成功解决了深度搜索代理中的搜索效率问题，为复杂信息寻求任务提供了更有效的解决方案。

Abstract: Deep search agents, which autonomously iterate through multi-turn web-based reasoning, represent a promising paradigm for complex information-seeking tasks. However, current agents suffer from critical inefficiency: they conduct excessive searches as they cannot accurately judge when to stop searching and start answering. This stems from outcome-centric training that prioritize final results over the search process itself. We identify the root cause as misaligned decision boundaries, the threshold determining when accumulated information suffices to answer. This causes over-search (redundant searching despite sufficient knowledge) and under-search (premature termination yielding incorrect answers). To address these errors, we propose a comprehensive framework comprising two key components. First, we introduce causal intervention-based diagnosis that identifies boundary errors by comparing factual and counterfactual trajectories at each decision point. Second, we develop Decision Boundary Alignment for Deep Search agents (DAS), which constructs preference datasets from causal feedback and aligns policies via preference optimization. Experiments on public datasets demonstrate that decision boundary errors are pervasive across state-of-the-art agents. Our DAS method effectively calibrates these boundaries, mitigating both over-search and under-search to achieve substantial gains in accuracy and efficiency. Our code and data are publicly available at: https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS.

</details>


### [31] [Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval](https://arxiv.org/abs/2602.03306)
*Zhanyu Wu,Richong Zhang,Zhijie Nie*

Main category: cs.IR

TL;DR: 提出Query-Aware Adaptive Dimension Selection框架，通过学习直接从查询嵌入预测每个维度的重要性，选择查询感知的维度子集进行相似度计算，提高密集检索效果。


<details>
  <summary>Details</summary>
Motivation: 密集检索将查询和文档表示为高维嵌入，但这些表示在查询层面可能存在冗余：对于给定的信息需求，只有部分维度对排序有帮助。现有方法要么依赖噪声大的伪相关反馈，要么学习全局变换而不显式建模查询感知的维度重要性。

Method: 首先使用监督相关性标签构建嵌入维度的oracle维度重要性分布，然后训练一个预测器将查询嵌入映射到这些标签蒸馏的重要性分数。推理时，预测器仅基于查询嵌入选择查询感知的维度子集进行相似度计算，无需伪相关反馈。

Result: 在多个密集检索器和基准测试上的实验表明，学习的维度选择器在检索效果上优于全维度基线、基于PRF的掩码方法和监督适配器基线。

Conclusion: 提出的查询感知自适应维度选择框架能够有效识别和利用对特定查询重要的嵌入维度，提高密集检索性能，避免了伪相关反馈的噪声问题。

Abstract: Dense retrieval represents queries and docu-002 ments as high-dimensional embeddings, but003 these representations can be redundant at the004 query level: for a given information need, only005 a subset of dimensions is consistently help-006 ful for ranking. Prior work addresses this via007 pseudo-relevance feedback (PRF) based dimen-008 sion importance estimation, which can produce009 query-aware masks without labeled data but010 often relies on noisy pseudo signals and heuris-011 tic test-time procedures. In contrast, super-012 vised adapter methods leverage relevance labels013 to improve embedding quality, yet they learn014 global transformations shared across queries015 and do not explicitly model query-aware di-016 mension importance. We propose a Query-017 Aware Adaptive Dimension Selection frame-018 work that learns to predict per-dimension im-019 portance directly from query embedding. We020 first construct oracle dimension importance dis-021 tributions over embedding dimensions using022 supervised relevance labels, and then train a023 predictor to map a query embedding to these024 label-distilled importance scores. At inference,025 the predictor selects a query-aware subset of026 dimensions for similarity computation based027 solely on the query embedding, without pseudo-028 relevance feedback. Experiments across multi-029 ple dense retrievers and benchmarks show that030 our learned dimension selector improves re-031 trieval effectiveness over the full-dimensional032 baseline as well as PRF-based masking and033 supervised adapter baselines.

</details>


### [32] [SCASRec: A Self-Correcting and Auto-Stopping Model for Generative Route List Recommendation](https://arxiv.org/abs/2602.03324)
*Chao Chen,Longfei Xu,Daohan Su,Tengfei Liu,Hanyu Guo,Yihai Duan,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: SCASRec是一个统一的生成式推荐框架，将排序和冗余消除整合到端到端流程中，解决了多阶段推荐系统的三个关键限制。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段推荐系统存在三个关键问题：1) 离线训练目标与在线指标不对齐；2) 冗余消除依赖僵化的手工规则；3) 精细排序和重排序阶段分离导致次优性能。

Method: 提出SCASRec统一生成框架，引入逐步校正奖励(SCR)指导列表级优化，并使用可学习的推荐结束(EOR)令牌自适应终止生成。

Result: 在两个大规模开源路线推荐数据集上，SCASRec在离线和在线设置中都达到了最先进水平，并已在真实导航应用中部署。

Conclusion: SCASRec通过端到端生成框架有效解决了传统多阶段推荐系统的局限性，实现了排序和冗余消除的统一优化。

Abstract: Route recommendation systems commonly adopt a multi-stage pipeline involving fine-ranking and re-ranking to produce high-quality ordered recommendations. However, this paradigm faces three critical limitations. First, there is a misalignment between offline training objectives and online metrics. Offline gains do not necessarily translate to online improvements. Actual performance must be validated through A/B testing, which may potentially compromise the user experience. Second, redundancy elimination relies on rigid, handcrafted rules that lack adaptability to the high variance in user intent and the unstructured complexity of real-world scenarios. Third, the strict separation between fine-ranking and re-ranking stages leads to sub-optimal performance. Since each module is optimized in isolation, the fine-ranking stage remains oblivious to the list-level objectives (e.g., diversity) targeted by the re-ranker, thereby preventing the system from achieving a jointly optimized global optimum. To overcome these intertwined challenges, we propose \textbf{SCASRec} (\textbf{S}elf-\textbf{C}orrecting and \textbf{A}uto-\textbf{S}topping \textbf{Rec}ommendation), a unified generative framework that integrates ranking and redundancy elimination into a single end-to-end process. SCASRec introduces a stepwise corrective reward (SCR) to guide list-wise refinement by focusing on hard samples, and employs a learnable End-of-Recommendation (EOR) token to terminate generation adaptively when no further improvement is expected. Experiments on two large-scale, open-sourced route recommendation datasets demonstrate that SCASRec establishes an SOTA in offline and online settings. SCASRec has been fully deployed in a real-world navigation app, demonstrating its effectiveness.

</details>


### [33] [Beyond Exposure: Optimizing Ranking Fairness with Non-linear Time-Income Functions](https://arxiv.org/abs/2602.03345)
*Xuancheng Li,Tao Yang,Yujia Zhou,Qingyao Ai,Yiqun Liu*

Main category: cs.IR

TL;DR: 该论文提出了一种新的收入公平性概念，用于解决传统曝光公平性忽略时间等上下文因素对提供商收入影响的问题，并开发了相应的度量指标和优化算法。


<details>
  <summary>Details</summary>
Motivation: 当前排名优化中广泛使用的曝光公平性概念主要基于位置决定曝光，忽略了时间等其他因素对提供商收入的显著影响。当提供商效用不等于也不与项目曝光成比例时，需要更全面的公平性概念。

Method: 提出了收入公平性的正式定义和相应度量指标。开发了动态收入导数感知排名公平性算法，基于当前时间步的边际收入增益，使用基于泰勒展开的梯度同时优化效果和收入公平性。

Result: 模拟实验显示，现有的基于曝光公平性的排名算法无法优化提出的收入公平性。在具有不同时间收入函数的离线和在线设置中，DIDRF算法始终优于最先进的方法。

Conclusion: 该研究扩展了排名公平性的概念，考虑了时间等上下文因素对提供商收入的影响，提出的收入公平性框架和DIDRF算法能够有效平衡排名效果和提供商收入公平性。

Abstract: Ranking is central to information distribution in web search and recommendation. Nowadays, in ranking optimization, the fairness to item providers is viewed as a crucial factor alongside ranking relevance for users. There are currently numerous concepts of fairness and one widely recognized fairness concept is Exposure Fairness. However, it relies primarily on exposure determined solely by position, overlooking other factors that significantly influence income, such as time. To address this limitation, we propose to study ranking fairness when the provider utility is influenced by other contextual factors and is neither equal to nor proportional to item exposure. We give a formal definition of Income Fairness and develop a corresponding measurement metric. Simulated experiments show that existing-exposure-fairness-based ranking algorithms fail to optimize the proposed income fairness. Therefore, we propose the Dynamic-Income-Derivative-aware Ranking Fairness algorithm, which, based on the marginal income gain at the present timestep, uses Taylor-expansion-based gradients to simultaneously optimize effectiveness and income fairness. In both offline and online settings with diverse time-income functions, DIDRF consistently outperforms state-of-the-art methods.

</details>


### [34] [AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation](https://arxiv.org/abs/2602.03416)
*Wenxin Ye,Lin Li,Ming Li,Yang Shen,Kanghong Wang,Jimmy Xiangji Huang*

Main category: cs.IR

TL;DR: 提出了AesRec基准数据集，包含系统化的服装美学量化标注，用于开发美学对齐的推荐系统，通过多维度指标评估服装美学，并利用视觉语言模型进行大规模美学评分。


<details>
  <summary>Details</summary>
Motivation: 现有服装推荐方法主要依赖用户-物品-搭配交互行为，忽视了服装美学的显式表示，无法提供美学指导。需要填补这一空白，开发能够提供美学指导的推荐系统。

Method: 基于专业服装质量标准和时尚美学原则，定义了多维度美学指标：单品层面评估轮廓、色彩、材质、工艺、穿着性和单品印象6个维度；搭配层面保留前5个核心属性，增加风格协同、视觉和谐和搭配印象。利用视觉语言模型进行大规模美学评分，并进行人机一致性验证。

Result: 在时尚数据集上进行了严格的人机一致性验证，确认了生成评分的可靠性。基于AesRec的实验结果表明，将量化美学信息整合到服装推荐模型中，既能满足用户个性化需求，又能提供美学指导。

Conclusion: AesRec基准数据集填补了服装美学量化标注的空白，通过整合美学信息，推荐系统能够同时满足个性化需求和提供美学指导，为美学对齐的推荐系统开发提供了重要基础。

Abstract: Clothing recommendation extends beyond merely generating personalized outfits; it serves as a crucial medium for aesthetic guidance. However, existing methods predominantly rely on user-item-outfit interaction behaviors while overlooking explicit representations of clothing aesthetics. To bridge this gap, we present the AesRec benchmark dataset featuring systematic quantitative aesthetic annotations, thereby enabling the development of aesthetics-aligned recommendation systems. Grounded in professional apparel quality standards and fashion aesthetic principles, we define a multidimensional set of indicators. At the item level, six dimensions are independently assessed: silhouette, chromaticity, materiality, craftsmanship, wearability, and item-level impression. Transitioning to the outfit level, the evaluation retains the first five core attributes while introducing stylistic synergy, visual harmony, and outfit-level impression as distinct metrics to capture the collective aesthetic impact. Given the increasing human-like proficiency of Vision-Language Models in multimodal understanding and interaction, we leverage them for large-scale aesthetic scoring. We conduct rigorous human-machine consistency validation on a fashion dataset, confirming the reliability of the generated ratings. Experimental results based on AesRec further demonstrate that integrating quantified aesthetic information into clothing recommendation models can provide aesthetic guidance for users while fulfilling their personalized requirements.

</details>


### [35] [RankSteer: Activation Steering for Pointwise LLM Ranking](https://arxiv.org/abs/2602.03422)
*Yumeng Wang,Catherine Chen,Suzan Verberne*

Main category: cs.IR

TL;DR: RankSteer：一种后激活导向框架，通过控制表示空间中的三个解耦方向（决策、证据、角色）来校准零样本点式LLM排序，无需修改模型权重或进行显式文档比较。


<details>
  <summary>Details</summary>
Motivation: LLM作为零样本排序器性能对提示工程敏感，特别是角色扮演指令。研究发现角色相关信号编码在与查询-文档表示分离的激活通道中，这启发了直接在激活层面而非脆弱的提示工程中引导排序行为的可能性。

Method: 提出RankSteer框架，识别表示空间中三个可引导的解耦方向：决策方向（映射隐藏状态到相关性分数）、证据方向（捕获未被决策头直接利用的相关性信号）、角色方向（调节模型行为但不注入相关性信息）。在推理时使用基于投影的干预来联合控制这些方向。

Result: 在TREC DL 20和多个BEIR基准测试中，RankSteer仅使用少量锚点查询就持续提升排序质量，表明点式LLM排序器中存在大量未充分利用的排序能力。几何分析显示引导通过稳定排序几何和减少离散度来改进排序。

Conclusion: 激活导向是校准LLM排序行为的有效方法，揭示了LLM内部如何表示和校准相关性判断。RankSteer框架展示了通过后激活干预充分利用LLM排序潜力的可行性。

Abstract: Large language models (LLMs) have recently shown strong performance as zero-shot rankers, yet their effectiveness is highly sensitive to prompt formulation, particularly role-play instructions. Prior analyses suggest that role-related signals are encoded along activation channels that are largely separate from query-document representations, raising the possibility of steering ranking behavior directly at the activation level rather than through brittle prompt engineering. In this work, we propose RankSteer, a post-hoc activation steering framework for zero-shot pointwise LLM ranking. We characterize ranking behavior through three disentangled and steerable directions in representation space: a \textbf{decision direction} that maps hidden states to relevance scores, an \textbf{evidence direction} that captures relevance signals not directly exploited by the decision head, and a \textbf{role direction} that modulates model behavior without injecting relevance information. Using projection-based interventions at inference time, RankSteer jointly controls these directions to calibrate ranking behavior without modifying model weights or introducing explicit cross-document comparisons. Experiments on TREC DL 20 and multiple BEIR benchmarks show that RankSteer consistently improves ranking quality using only a small number of anchor queries, demonstrating that substantial ranking capacity remains under-utilized in pointwise LLM rankers. We further provide a geometric analysis revealing that steering improves ranking by stabilizing ranking geometry and reducing dispersion, offering new insight into how LLMs internally represent and calibrate relevance judgments.

</details>


### [36] [Failure is Feedback: History-Aware Backtracking for Agentic Traversal in Multimodal Graphs](https://arxiv.org/abs/2602.03432)
*Joohyung Yun,Doyup Lee,Wook-Shin Han*

Main category: cs.IR

TL;DR: FiF将多模态文档检索建模为序列决策过程，引入历史感知回溯和经济理性代理工作流，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的检索方法使用统一相似度度量忽略跳转特定语义，且预定义计划僵化无法动态纠错。需要检索器能适应演化上下文并从死胡同智能恢复。

Method: 提出FiF框架：1) 历史感知回溯机制，利用失败遍历的上下文信息而非简单回退状态；2) 经济理性代理工作流，采用成本感知遍历方法动态权衡检索精度与推理成本，仅在先前失败证明合理时才使用计算密集的LLM推理。

Result: 在MultimodalQA、MMCoQA和WebQA基准测试中实现了最先进的检索性能。

Conclusion: FiF通过将子图检索建模为序列决策过程，引入历史感知回溯和经济理性代理工作流，有效解决了现有图检索方法的局限性，在多模态文档检索任务中表现出色。

Abstract: Open-domain multimodal document retrieval aims to retrieve specific components (paragraphs, tables, or images) from large and interconnected document corpora. Existing graph-based retrieval approaches typically rely on a uniform similarity metric that overlooks hop-specific semantics, and their rigid pre-defined plans hinder dynamic error correction. These limitations suggest that a retriever should adapt its reasoning to the evolving context and recover intelligently from dead ends. To address these needs, we propose Failure is Feedback (FiF), which casts subgraph retrieval as a sequential decision process and introduces two key innovations. (i) We introduce a history-aware backtracking mechanism; unlike standard backtracking that simply reverts the state, our approach piggybacks on the context of failed traversals, leveraging insights from previous failures. (ii) We implement an economically-rational agentic workflow. Unlike conventional agents with static strategies, our orchestrator employs a cost-aware traversal method to dynamically manage the trade-off between retrieval accuracy and inference costs, escalating to intensive LLM-based reasoning only when the prior failure justifies the additional computational investment. Extensive experiments show that FiF achieves state-of-the-art retrieval on the benchmarks of MultimodalQA, MMCoQA and WebQA.

</details>


### [37] [Tutorial on Reasoning for IR & IR for Reasoning](https://arxiv.org/abs/2602.03640)
*Mohanna Hoveyda,Panagiotis Efstratiadis,Arjen de Vries,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该教程提出了一个统一的推理分析框架，帮助信息检索领域整合跨学科推理方法，超越传统语义相关度排序，实现逻辑约束、多步推理和证据合成。


<details>
  <summary>Details</summary>
Motivation: 现实世界的信息需求需要超越语义相关度的推理能力（逻辑约束、多步推理、证据合成），但当前推理研究分散在不同学科，信息检索领域难以识别相关方法和机会。

Method: 首先在信息检索背景下定义推理的工作定义，然后推导出统一的分析框架，将现有方法映射到反映定义核心组件的轴上，提供全面概述并展示权衡与互补性。

Result: 创建了一个框架来映射推理方法（推理时策略、LLM后训练、神经符号系统、贝叶斯概率框架、几何表示、能量模型），揭示其权衡和互补性，展示检索在推理系统中的核心作用。

Conclusion: 该教程为增强推理能力的信息检索系统提供概念框架和实践指导，同时将信息检索定位为既受益于又贡献于更广泛推理方法发展的领域。

Abstract: Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.

</details>


### [38] [Bringing Reasoning to Generative Recommendation Through the Lens of Cascaded Ranking](https://arxiv.org/abs/2602.03692)
*Xinyu Lin,Pengyuan Liu,Wenjie Wang,Yicheng Hu,Chen Xu,Fuli Feng,Qifan Wang,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 论文提出CARE框架解决生成式推荐中的偏差放大问题，通过渐进历史编码和查询锚定推理机制提升推荐多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐模型存在偏差放大问题，即token级偏差在生成过程中不断累积，限制了推荐多样性并损害用户体验。相比传统多阶段推荐管道，GR存在两个局限性：对编码历史的同质依赖，以及固定的计算预算阻碍了对用户偏好的深入理解。

Method: 提出CARE框架：1) 渐进历史编码机制，随着生成过程推进逐步融入更细粒度的历史信息；2) 查询锚定推理机制，通过并行推理步骤对历史信息进行更深入理解。在三个GR骨干网络上实例化CARE。

Result: 在四个数据集上的实验结果表明，CARE在推荐准确性、多样性、效率和可扩展性方面均表现出优越性。

Conclusion: CARE框架通过引入异构信息和分配更多计算资源，有效解决了生成式推荐中的偏差放大问题，提升了推荐系统的整体性能。

Abstract: Generative Recommendation (GR) has become a promising end-to-end approach with high FLOPS utilization for resource-efficient recommendation. Despite the effectiveness, we show that current GR models suffer from a critical \textbf{bias amplification} issue, where token-level bias escalates as token generation progresses, ultimately limiting the recommendation diversity and hurting the user experience. By comparing against the key factor behind the success of traditional multi-stage pipelines, we reveal two limitations in GR that can amplify the bias: homogeneous reliance on the encoded history, and fixed computational budgets that prevent deeper user preference understanding.
  To combat the bias amplification issue, it is crucial for GR to 1) incorporate more heterogeneous information, and 2) allocate greater computational resources at each token generation step. To this end, we propose CARE, a simple yet effective cascaded reasoning framework for debiased GR. To incorporate heterogeneous information, we introduce a progressive history encoding mechanism, which progressively incorporates increasingly fine-grained history information as the generation process advances. To allocate more computations, we propose a query-anchored reasoning mechanism, which seeks to perform a deeper understanding of historical information through parallel reasoning steps. We instantiate CARE on three GR backbones. Empirical results on four datasets show the superiority of CARE in recommendation accuracy, diversity, efficiency, and promising scalability. The codes and datasets are available at https://github.com/Linxyhaha/CARE.

</details>


### [39] [Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals](https://arxiv.org/abs/2602.03713)
*Moritz Vandenhirtz,Kaveh Hassani,Shervin Ghasemlou,Shuai Shao,Hamid Eghbalzadeh,Fuchun Peng,Jun Liu,Michael Louis Iuzzolino*

Main category: cs.IR

TL;DR: MSCGRec是一种多模态语义与协同生成式推荐系统，通过结合多种语义模态、自监督量化学习和协同特征融合，在大型数据集上超越了传统序列推荐和生成式推荐方法。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐方法通过将物品表示为离散语义码序列来减少内存开销，但在大型物品集上性能尚未超越传统序列推荐方法，限制了其在实际应用中的采用。

Method: 1. 结合多种语义模态；2. 基于DINO框架的图像自监督量化学习；3. 从序列推荐器中提取协同特征作为独立模态进行融合；4. 约束序列学习，在训练时限制输出空间到允许的标记集合。

Result: 在三个大型真实世界数据集上，MSCGRec超越了序列推荐和生成式推荐的基线方法，并通过消融研究验证了各组件的影响。

Conclusion: MSCGRec成功解决了生成式推荐在大型物品集上的性能瓶颈，通过多模态融合和约束学习实现了更好的推荐效果，为生成式推荐的实际应用提供了可行方案。

Abstract: Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. To resolve this, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. Additionally, MSCGRec fuses collaborative and semantic signals by extracting collaborative features from sequential recommenders and treating them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines and provide an extensive ablation study to validate the impact of each component.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [40] [Trailer Reimagined: An Innovative, Llm-DRiven, Expressive Automated Movie Summary framework (TRAILDREAMS)](https://arxiv.org/abs/2602.02630)
*Roberto Balestri,Pasquale Cascarano,Mirko Degli Esposti,Guglielmo Pescatore*

Main category: cs.MM

TL;DR: TRAILDREAMS是一个利用大语言模型自动生成电影预告片的框架，通过LLM选择关键视觉序列和对话，并生成音乐和旁白等音频元素，在评估中优于现有自动方法但不及人工制作预告片。


<details>
  <summary>Details</summary>
Motivation: 自动化电影预告片制作过程，提高效率，减少人工成本，同时探索大语言模型在创意内容生成领域的应用潜力。

Method: 使用大语言模型作为核心组件，选择关键视觉序列和具有冲击力的对话，并协助生成音乐和旁白等音频元素，形成完整的预告片生成框架。

Result: 在比较评估中，TRAILDREAMS超越了当前最先进的预告片生成方法，获得了更高的观众评分，但仍不及真实人工制作的预告片质量。

Conclusion: TRAILDREAMS展示了自动化创意过程的显著潜力，代表了该领域的进步，但需要进一步改进以缩小与传统人工制作预告片的质量差距。

Abstract: This paper introduces TRAILDREAMS, a framework that uses a large language model (LLM) to automate the production of movie trailers. The purpose of LLM is to select key visual sequences and impactful dialogues, and to help TRAILDREAMS to generate audio elements such as music and voiceovers. The goal is to produce engaging and visually appealing trailers efficiently. In comparative evaluations, TRAILDREAMS surpasses current state-of-the-art trailer generation methods in viewer ratings. However, it still falls short when compared to real, human-crafted trailers. While TRAILDREAMS demonstrates significant promise and marks an advancement in automated creative processes, further improvements are necessary to bridge the quality gap with traditional trailers.

</details>
