<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.IT](#cs.IT) [Total: 9]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.GT](#cs.GT) [Total: 9]
- [cs.LG](#cs.LG) [Total: 157]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.IR](#cs.IR) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications](https://arxiv.org/abs/2508.16681)
*Eric Zhang*

Main category: cs.AI

TL;DR: 本文对基于规则的结巴检测系统进行全面分析，提出增强框架，在保持完全可解释性的同时实现竞争性性能，特别在延长音检测方面表现出色（97-99%准确率）。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习在自动语音不流畅检测方面取得进展，但在临床应用中，可解释性和透明度至关重要，基于规则的方法仍然不可或缺。

Method: 提出增强的基于规则框架，包含说话速率标准化、多级声学特征分析和分层决策结构，整合UCLASS、FluencyBank和SEP-28k等多个语料库的见解。

Result: 在延长音检测方面达到97-99%的准确率，在不同说话速率下提供稳定性能，可作为现代机器学习流程中的提案生成器或约束模块。

Conclusion: 虽然神经方法在无约束设置中可能获得略高的准确性，但基于规则的方法在决策可审计性、患者特定调优和实时反馈至关重要的临床环境中具有独特优势。

Abstract: Stuttering affects approximately 1% of the global population, impacting
communication and quality of life. While recent advances in deep learning have
pushed the boundaries of automatic speech dysfluency detection, rule-based
approaches remain crucial for clinical applications where interpretability and
transparency are paramount. This paper presents a comprehensive analysis of
rule-based stuttering detection systems, synthesizing insights from multiple
corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced
rule-based framework that incorporates speaking-rate normalization, multi-level
acoustic feature analysis, and hierarchical decision structures. Our approach
achieves competitive performance while maintaining complete
interpretability-critical for clinical adoption. We demonstrate that rule-based
systems excel particularly in prolongation detection (97-99% accuracy) and
provide stable performance across varying speaking rates. Furthermore, we show
how these interpretable models can be integrated with modern machine learning
pipelines as proposal generators or constraint modules, bridging the gap
between traditional speech pathology practices and contemporary AI systems. Our
analysis reveals that while neural approaches may achieve marginally higher
accuracy in unconstrained settings, rule-based methods offer unique advantages
in clinical contexts where decision auditability, patient-specific tuning, and
real-time feedback are essential.

</details>


### [2] [Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018](https://arxiv.org/abs/2508.16747)
*Liu Liu,Rui Dai*

Main category: cs.AI

TL;DR: 使用可解释AI技术分析PISA 2018数据，发现非线性模型更适合预测数学成绩，关键预测因素包括社经地位、学习时间、教师动机等


<details>
  <summary>Details</summary>
Motivation: 探索影响学生数学表现的关键因素，为制定有效教育政策提供依据

Method: 使用多元线性回归、随机森林、CATBoost咊人工神经网络四种模型，分析PISA 2018数据中67,329名学生的学生、家庭和学校变量，采用特征重要性、SHAP值等可解释技术

Result: 非线性模型（特别是随机森林咊神经网络）表现更优，随机森林在准确性咊普适性之间取得平衡。关键预测因素在不同国家影响程度不同

Conclusion: 研究强调了学业成绩的非线性咊语境依赖性，显示了可解释AI在教育研究中的价值，为政策制定咊个性化学习策略提供支持

Abstract: Understanding the factors that shape students' mathematics performance is
vital for designing effective educational policies. This study applies
explainable artificial intelligence (XAI) techniques to PISA 2018 data to
predict math achievement and identify key predictors across ten countries
(67,329 students). We tested four models: Multiple Linear Regression (MLR),
Random Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using
student, family, and school variables. Models were trained on 70% of the data
(with 5-fold cross-validation) and tested on 30%, stratified by country.
Performance was assessed with R^2 and Mean Absolute Error (MAE). To ensure
interpretability, we used feature importance, SHAP values, and decision tree
visualizations. Non-linear models, especially RF and ANN, outperformed MLR,
with RF balancing accuracy and generalizability. Key predictors included
socio-economic status, study time, teacher motivation, and students' attitudes
toward mathematics, though their impact varied across countries. Visual
diagnostics such as scatterplots of predicted vs actual scores showed RF and
CATBoost aligned closely with actual performance. Findings highlight the
non-linear and context-dependent nature of achievement and the value of XAI in
educational research. This study uncovers cross-national patterns, informs
equity-focused reforms, and supports the development of personalized learning
strategies.

</details>


### [3] [Evaluation and LLM-Guided Learning of ICD Coding Rationales](https://arxiv.org/abs/2508.16777)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Wuraola Oyewusi,Kai Kang,Goran Nenadic*

Main category: cs.AI

TL;DR: 该论文系统评估了ICD编码模型的可解释性，构建了新的标注数据集，并提出使用LLM生成解释作为远程监督信号来提升模型解释质量的方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习ICD编码模型缺乏可解释性，影响临床信任和透明度。现有方法主要依赖注意力机制和定性评估，缺乏系统性评估和专门训练生成解释的方法。

Method: 1) 从忠实性和合理性两个维度评估解释质量；2) 构建新的标注数据集；3) 提出使用LLM生成解释作为远程监督信号的学习方法；4) 结合少量人工标注样本进一步提升效果。

Result: LLM生成的解释与人类专家判断最接近。加入少量人工标注样本不仅能改善解释生成，还能增强基于解释的学习方法。

Conclusion: LLM生成的解释在ICD编码任务中表现出色，结合少量人工标注可以显著提升模型解释质量和可信度，为临床可解释AI提供了有效解决方案。

Abstract: Automated clinical coding involves mapping unstructured text from Electronic
Health Records (EHRs) to standardized code systems such as the International
Classification of Diseases (ICD). While recent advances in deep learning have
significantly improved the accuracy and efficiency of ICD coding, the lack of
explainability in these models remains a major limitation, undermining trust
and transparency. Current explorations about explainability largely rely on
attention-based techniques and qualitative assessments by physicians, yet lack
systematic evaluation using consistent criteria on high-quality rationale
datasets, as well as dedicated approaches explicitly trained to generate
rationales for further enhancing explanation. In this work, we conduct a
comprehensive evaluation of the explainability of the rationales for ICD coding
through two key lenses: faithfulness that evaluates how well explanations
reflect the model's actual reasoning and plausibility that measures how
consistent the explanations are with human expert judgment. To facilitate the
evaluation of plausibility, we construct a new rationale-annotated dataset,
offering denser annotations with diverse granularity and aligns better with
current clinical practice, and conduct evaluation across three types of
rationales of ICD coding. Encouraged by the promising plausibility of
LLM-generated rationales for ICD coding, we further propose new rationale
learning methods to improve the quality of model-generated rationales, where
rationales produced by prompting LLMs with/without annotation examples are used
as distant supervision signals. We empirically find that LLM-generated
rationales align most closely with those of human experts. Moreover,
incorporating few-shot human-annotated examples not only further improves
rationale generation but also enhances rationale-learning approaches.

</details>


### [4] [PuzzleJAX: A Benchmark for Reasoning and Learning](https://arxiv.org/abs/2508.16821)
*Sam Earle,Graham Todd,Yuchen Li,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: PuzzleJAX是一个GPU加速的益智游戏引擎和描述语言，支持快速基准测试树搜索、强化学习和LLM推理能力，能够动态编译任何可用其DSL表达的游戏。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速学习环境只提供固定游戏的硬编码实现，无法支持动态游戏创建和基准测试。PuzzleJAX旨在填补这一空白，提供一个能够表达丰富、人类相关任务的通用平台。

Method: 基于流行的PuzzleScript在线游戏引擎设计领域特定语言(DSL)，支持动态编译任何可表达的游戏，并验证了数百个PuzzleScript游戏在PuzzleJAX中的兼容性。

Result: 成功验证了数百个自2013年以来由专业设计师和休闲创作者设计的游戏，展示了PuzzleJAX在表达丰富、人类相关任务空间方面的覆盖能力。

Conclusion: PuzzleJAX能够自然表达既简单直观又具有深度挑战性的任务，需要控制、规划和高层次洞察力的结合，为搜索、学习和语言模型的性能分析提供了有效平台。

Abstract: We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description
language designed to support rapid benchmarking of tree search, reinforcement
learning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning
environments that provide hard-coded implementations of fixed sets of games,
PuzzleJAX allows dynamic compilation of any game expressible in its
domain-specific language (DSL). This DSL follows PuzzleScript, which is a
popular and accessible online game engine for designing puzzle games. In this
paper, we validate in PuzzleJAX several hundred of the thousands of games
designed in PuzzleScript by both professional designers and casual creators
since its release in 2013, thereby demonstrating PuzzleJAX's coverage of an
expansive, expressive, and human-relevant space of tasks. By analyzing the
performance of search, learning, and language models on these games, we show
that PuzzleJAX can naturally express tasks that are both simple and intuitive
to understand, yet often deeply challenging to master, requiring a combination
of control, planning, and high-level insight.

</details>


### [5] [Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment](https://arxiv.org/abs/2508.16839)
*Shayan Vassef,Soorya Ram Shimegekar,Abhay Goyal,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: 提出基于单一视觉语言模型的双重角色医疗框架：作为模型卡片匹配器路由图像到专业模型，以及通过微调处理专科内多任务，简化部署并保持性能。


<details>
  <summary>Details</summary>
Motivation: 临床工作流碎片化，缺乏数据驱动的模型识别和标准化输出交付，导致效率低下和运营成本增加。

Method: 使用单一VLM在两个互补角色中：1) 三阶段工作流（模态→主要异常→模型卡片ID）的路由器；2) 专科特定数据集微调处理多任务。包含阶段提示和答案选择器确保准确性。

Result: 在胃肠病学、血液学、眼科和病理学领域，单一模型部署匹配或接近专业基线性能。

Conclusion: 单一VLM既能决策又能执行，可减少数据科学家工作量、缩短监控时间、提高模型选择透明度并降低集成开销。

Abstract: Clinical workflows are fragmented as a patchwork of scripts and task-specific
networks that often handle triage, task selection, and model deployment. These
pipelines are rarely streamlined for data science pipeline, reducing efficiency
and raising operational costs. Workflows also lack data-driven model
identification (from imaging/tabular inputs) and standardized delivery of model
outputs. In response, we present a practical, healthcare-first framework that
uses a single vision-language model (VLM) in two complementary roles. First
(Solution 1), the VLM acts as an aware model-card matcher that routes an
incoming image to the appropriate specialist model via a three-stage workflow
(modality -> primary abnormality -> model-card id). Checks are provided by (i)
stagewise prompts that allow early exit via None/Normal/Other and (ii) a
stagewise answer selector that arbitrates between the top-2 candidates at each
stage, reducing the chance of an incorrect selection and aligning the workflow
with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on
specialty-specific datasets ensuring a single model covers multiple downstream
tasks within each specialty, maintaining performance while simplifying
deployment. Across gastroenterology, hematology, ophthalmology, and pathology,
our single-model deployment matches or approaches specialized baselines.
  Compared with pipelines composed of many task-specific agents, this approach
shows that one VLM can both decide and do. It may reduce effort by data
scientists, shorten monitoring, increase the transparency of model selection
(with per-stage justifications), and lower integration overhead.

</details>


### [6] [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs](https://arxiv.org/abs/2508.16846)
*Katherine Atwell,Pedram Heydari,Anthony Sicilia,Malihe Alikhani*

Main category: cs.AI

TL;DR: 该研究使用贝叶斯框架量化LLMs中的谄媚行为，发现LLMs并非贝叶斯理性，谄媚探测会导致预测后验概率显著偏向用户观点，有时增加有时减少贝叶斯误差，且与Brier分数相关性不强。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过行为变化或准确率来衡量LLMs的谄媚行为，但这些方法无法表征理性变化，且准确率方法只适用于有确定答案的场景。需要一种能区分理性与非理性更新、适用于不确定性任务的方法。

Method: 采用贝叶斯框架，通过比较LLMs在引入用户观点前后的预测后验概率变化来量化谄媚行为。研究了3种不同任务、开源和闭源LLMs、两种谄媚探测方法，以及多种概率判断提取方法。

Result: 1) LLMs不是贝叶斯理性的；2) 谄媚探测导致预测后验概率显著偏向用户观点；3) 谄媚有时增加有时减少贝叶斯误差；4) 谄媚引起的贝叶斯误差变化与Brier分数相关性不强。

Conclusion: 仅研究谄媚对真实结果的影响不能完全捕捉由谄媚引起的推理错误，贝叶斯框架能更好地量化LLMs在不确定性任务中的非理性谄媚行为。

Abstract: Sycophancy, or overly agreeable or flattering behavior, is a documented issue
in large language models (LLMs), and is critical to understand in the context
of human/AI collaboration. Prior works typically quantify sycophancy by
measuring shifts in behavior or impacts on accuracy, but neither metric
characterizes shifts in rationality, and accuracy measures can only be used in
scenarios with a known ground truth. In this work, we utilize a Bayesian
framework to quantify sycophancy as deviations from rational behavior when
presented with user perspectives, thus distinguishing between rational and
irrational updates based on the introduction of user perspectives. In
comparison to other methods, this approach allows us to characterize excessive
behavioral shifts, even for tasks that involve inherent uncertainty or do not
have a ground truth. We study sycophancy for 3 different tasks, a combination
of open-source and closed LLMs, and two different methods for probing
sycophancy. We also experiment with multiple methods for eliciting probability
judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause
deviations in LLMs' predicted posteriors that will lead to increased Bayesian
error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)
probing for sycophancy results in significant increases to the predicted
posterior in favor of the steered outcome, 3) sycophancy sometimes results in
increased Bayesian error, and in a small number of cases actually decreases
error, and 4) changes in Bayesian error due to sycophancy are not strongly
correlated in Brier score, suggesting that studying the impact of sycophancy on
ground truth alone does not fully capture errors in reasoning due to
sycophancy.

</details>


### [7] [RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis](https://arxiv.org/abs/2508.16850)
*Anku Rani,Aparna Garimella,Apoorv Saxena,Balaji Vasan Srinivasan,Paul Pu Liang*

Main category: cs.AI

TL;DR: RADAR是一个半自动化方法，用于构建包含17,819个样本的基准数据集，评估多模态大语言模型在图表分析中的归因能力，通过推理引导方法将归因准确率提升15%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图表分析中缺乏透明度，无法显示哪些视觉数据支持其结论，这种黑盒特性阻碍了实际应用中的信任和采用。

Method: 提出RADAR半自动化方法构建基准数据集，包含图表、问题、推理步骤和归因标注；引入基于图表的数学推理归因方法。

Result: 推理引导方法比基线方法提高归因准确率15%；增强的归因能力带来更强的答案生成，平均BERTScore达到约0.90，与真实回答高度一致。

Conclusion: 这项研究是迈向更可解释和可信赖的图表分析系统的重要一步，使用户能够通过推理和归因来验证和理解模型决策。

Abstract: Data visualizations like charts are fundamental tools for quantitative
analysis and decision-making across fields, requiring accurate interpretation
and mathematical reasoning. The emergence of Multimodal Large Language Models
(MLLMs) offers promising capabilities for automated visual data analysis, such
as processing charts, answering questions, and generating summaries. However,
they provide no visibility into which parts of the visual data informed their
conclusions; this black-box nature poses significant challenges to real-world
trust and adoption. In this paper, we take the first major step towards
evaluating and enhancing the capabilities of MLLMs to attribute their reasoning
process by highlighting the specific regions in charts and graphs that justify
model answers. To this end, we contribute RADAR, a semi-automatic approach to
obtain a benchmark dataset comprising 17,819 diverse samples with charts,
questions, reasoning steps, and attribution annotations. We also introduce a
method that provides attribution for chart-based mathematical reasoning.
Experimental results demonstrate that our reasoning-guided approach improves
attribution accuracy by 15% compared to baseline methods, and enhanced
attribution capabilities translate to stronger answer generation, achieving an
average BERTScore of $\sim$ 0.90, indicating high alignment with ground truth
responses. This advancement represents a significant step toward more
interpretable and trustworthy chart analysis systems, enabling users to verify
and understand model decisions through reasoning and attribution.

</details>


### [8] [Complexity in finitary argumentation (extended version)](https://arxiv.org/abs/2508.16986)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 本文研究了无限但有穷论证框架的计算复杂性，发现在有穷性假设下，基于可接受性语义的推理复杂度显著降低，为实际应用提供了计算可行的理论框架。


<details>
  <summary>Details</summary>
Motivation: 无限论证框架虽然表达能力强，但计算不可行性限制了其实际应用。研究有穷无限AFs的计算复杂性，旨在找到表达能力和计算可行性之间的平衡点。

Method: 通过理论分析无限但有穷论证框架的计算复杂性，特别关注基于可接受性语义的推理问题，识别关键组合约束条件。

Result: 研究发现有穷性假设并不自动保证复杂度降低，但对于可接受性语义存在显著的组合约束，导致复杂度大幅下降。

Conclusion: 有穷无限论证框架为许多推理形式提供了自然设置，在表达能力和计算可行性之间达到了良好平衡，具有实际应用价值。

Abstract: Abstract argumentation frameworks (AFs) provide a formal setting to analyze
many forms of reasoning with conflicting information. While the expressiveness
of general infinite AFs make them a tempting tool for modeling many kinds of
reasoning scenarios, the computational intractability of solving infinite AFs
limit their use, even in many theoretical applications.
  We investigate the complexity of computational problems related to infinite
but finitary argumentations frameworks, that is, infinite AFs where each
argument is attacked by only finitely many others. Our results reveal a
surprising scenario. On one hand, we see that the assumption of being finitary
does not automatically guarantee a drop in complexity. However, for the
admissibility-based semantics, we find a remarkable combinatorial constraint
which entails a dramatic decrease in complexity.
  We conclude that for many forms of reasoning, the finitary infinite AFs
provide a natural setting for reasoning which balances well the competing goals
of being expressive enough to be applied to many reasoning settings while being
computationally tractable enough for the analysis within the framework to be
useful.

</details>


### [9] [WebSight: A Vision-First Architecture for Robust Web Agents](https://arxiv.org/abs/2508.16987)
*Tanvir Bhathal,Asanshay Gupta*

Main category: cs.AI

TL;DR: WebSight是一个纯视觉的自主网页代理，通过视觉感知与网页交互，无需HTML/DOM输入，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 消除对HTML或DOM输入的依赖，开发纯视觉的网页交互系统，提高网页导航的鲁棒性和可解释性。

Method: 采用模块化多智能体架构，包括规划、推理、视觉动作和验证智能体，通过情景记忆机制协调。核心是WebSight-7B模型，基于Wave-UI-25K数据集使用LoRA微调优化的视觉语言模型。

Result: WebSight-7B在Showdown Clicks基准上达到58.84%的top-1准确率，优于多个更大的通用模型。完整WebSight在WebVoyager基准上达到68.0%的成功率，超越OpenAI和HCompany的系统。任务完成时正确率高达97.14%。

Conclusion: WebSight和WebSight-7B为可解释、鲁棒且高效的视觉网页导航设立了新标准。

Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to
interact with web environments purely through visual perception, eliminating
dependence on HTML or DOM-based inputs. Central to our approach we introduce
our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI
element interaction, trained using LoRA on a web-focused subset of the
Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent
architecture, comprising planning, reasoning, vision-action, and verification
agents, coordinated through an episodic memory mechanism.
  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks
benchmark, outperforming several larger generalist models while maintaining
lower latency. The full WebSight agent achieves a 68.0% success rate on the
WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and
HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly
97.14% of the time, indicating high precision. Together, WebSight and
WebSight-7B establish a new standard for interpretable, robust, and efficient
visual web navigation.

</details>


### [10] [Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting](https://arxiv.org/abs/2508.17087)
*Wen Wang,Xiangchen Wu,Liang Wang,Hao Hu,Xianping Tao,Linghao Zhang*

Main category: cs.AI

TL;DR: 提出Generate-and-Split(GaS)框架，结合强化学习和最优分割算法解决多旅行商最小最大路径问题，显著提升解质量和可迁移性


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法将学习组件与经典求解器分离，破坏了优化一致性，可能导致解质量下降。需要一种能够联合优化的新框架

Method: Generate-and-Split(GaS)框架，集成强化学习与最优分割算法进行联合训练。分割算法在欧几里得空间中保证最优分割，并具有近线性可扩展性。采用LSTM增强模型架构处理部分可观测性

Result: 大量实验表明，GaS框架在解质量和可迁移性方面显著优于现有的基于学习的方法

Conclusion: 提出的联合训练框架有效解决了传统两阶段方法的优化不一致问题，为多旅行商问题提供了高质量的近似解决方案

Abstract: This study addresses the Min-Max Multiple Traveling Salesmen Problem
($m^3$-TSP), which aims to coordinate tours for multiple salesmen such that the
length of the longest tour is minimized. Due to its NP-hard nature, exact
solvers become impractical under the assumption that $P \ne NP$. As a result,
learning-based approaches have gained traction for their ability to rapidly
generate high-quality approximate solutions. Among these, two-stage methods
combine learning-based components with classical solvers, simplifying the
learning objective. However, this decoupling often disrupts consistent
optimization, potentially degrading solution quality. To address this issue, we
propose a novel two-stage framework named \textbf{Generate-and-Split} (GaS),
which integrates reinforcement learning (RL) with an optimal splitting
algorithm in a joint training process. The splitting algorithm offers
near-linear scalability with respect to the number of cities and guarantees
optimal splitting in Euclidean space for any given path. To facilitate the
joint optimization of the RL component with the algorithm, we adopt an
LSTM-enhanced model architecture to address partial observability. Extensive
experiments show that the proposed GaS framework significantly outperforms
existing learning-based approaches in both solution quality and
transferability.

</details>


### [11] [PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows](https://arxiv.org/abs/2508.17094)
*Emmanuel O. Badmus,Peng Sang,Dimitrios Stamoulis,Amritanshu Pandey*

Main category: cs.AI

TL;DR: PowerChain是一个基于LLM的智能代理系统，用于自动化配电网分析任务，通过动态生成和执行专家级工作流来解决复杂问题。


<details>
  <summary>Details</summary>
Motivation: 配电网运营规划日益复杂，但许多小型电力公司缺乏研发资源来执行高级分析。现有分析方法依赖专家知识且难以自动化。

Method: 开发PowerChain系统，利用大语言模型函数调用功能，根据自然语言查询动态生成和执行有序的函数序列，基于专家构建的电力系统函数池和参考工作流对。

Result: PowerChain能够使用GPT-5和开源Qwen模型在真实电力数据上处理复杂的未知配电网分析任务，生成专家级工作流。

Conclusion: PowerChain成功解决了配电网分析自动化的挑战，为缺乏研发资源的小型电力公司提供了可扩展的高级分析解决方案。

Abstract: Due to the rapid pace of electrification and decarbonization, distribution
grid (DG) operation and planning are becoming more complex, necessitating
advanced computational analyses to ensure grid reliability and resilience.
State-of-the-art DG analyses rely on disparate workflows of complex models,
functions, and data pipelines, which require expert knowledge and are
challenging to automate. Many small-scale utilities and cooperatives lack a
large R&D workforce and therefore cannot use advanced analysis at scale. To
address this gap, we develop a novel agentic AI system, PowerChain, to solve
unseen DG analysis tasks via automated agentic orchestration and large language
models (LLMs) function-calling. Given a natural language query, PowerChain
dynamically generates and executes an ordered sequence of domain-aware
functions guided by the semantics of an expert-built power systems function
pool and a select reference set of known, expert-generated workflow-query
pairs. Our results show that PowerChain can produce expert-level workflows with
both GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks
operating on real utility data.

</details>


### [12] [Rethinking How AI Embeds and Adapts to Human Values: Challenges and Opportunities](https://arxiv.org/abs/2508.17104)
*Sz-Ting Tzeng,Frank Dignum*

Main category: cs.AI

TL;DR: 本文主张重新思考价值对齐框架，认为AI系统应超越静态单一价值观，实现长期推理和适应演化价值，并需要多智能体系统来处理价值多元化和冲突。


<details>
  <summary>Details</summary>
Motivation: 当前人类中心AI和价值决策虽受关注，但许多关键方面仍未充分探索，需要理解系统如何融入人类价值、人类如何识别这些价值，以及如何最小化伤害风险。

Method: 通过理论分析，提出价值对齐应超越静态单一概念，强调长期推理、价值适应性和多智能体框架来处理价值多元化和冲突。

Result: 识别了价值对齐面临的挑战，并指出了推进价值对齐研究的方向，包括设计方法和实际应用的多样化视角。

Conclusion: 价值对齐需要更全面的理论来涵盖人类价值的全谱系，多智能体系统为处理价值多元化、冲突和智能体间价值推理提供了合适框架。

Abstract: The concepts of ``human-centered AI'' and ``value-based decision'' have
gained significant attention in both research and industry. However, many
critical aspects remain underexplored and require further investigation. In
particular, there is a need to understand how systems incorporate human values,
how humans can identify these values within systems, and how to minimize the
risks of harm or unintended consequences. In this paper, we highlight the need
to rethink how we frame value alignment and assert that value alignment should
move beyond static and singular conceptions of values. We argue that AI systems
should implement long-term reasoning and remain adaptable to evolving values.
Furthermore, value alignment requires more theories to address the full
spectrum of human values. Since values often vary among individuals or groups,
multi-agent systems provide the right framework for navigating pluralism,
conflict, and inter-agent reasoning about values. We identify the challenges
associated with value alignment and indicate directions for advancing value
alignment research. In addition, we broadly discuss diverse perspectives of
value alignment, from design methodologies to practical applications.

</details>


### [13] [ST-Raptor: LLM-Powered Semi-Structured Table Question Answering](https://arxiv.org/abs/2508.18190)
*Zirui Tang,Boyu Niu,Xuanhe Zhou,Boxiu Li,Wei Zhou,Jiannan Wang,Guoliang Li,Xinyi Zhang,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个基于树结构的框架，使用大语言模型处理半结构化表格问答，通过层次正交树表示复杂表格布局，定义树操作执行QA任务，并采用两阶段验证机制，在真实数据集上比基线方法准确率提升20%。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的半结构化表格（如财务报表、医疗记录）具有复杂的层次结构和合并单元格，现有方法如NL2SQL、NL2Code和多模态LLM在理解这些复杂布局和准确回答问题方面存在困难，需要人工解释成本高且效率低。

Method: 提出层次正交树（HO-Tree）结构模型捕获表格复杂布局，定义基本树操作指导LLM执行QA任务，将用户问题分解为子问题并生成操作流水线，采用前向验证（检查执行步骤正确性）和后向验证（通过重构查询评估答案可靠性）的两阶段验证机制。

Result: 在包含102个真实半结构化表格和764个问题的SSTQA数据集上实验，ST-Raptor在答案准确率上比9个基线方法提升高达20%。

Conclusion: ST-Raptor框架有效解决了半结构化表格问答的挑战，通过树结构表示和操作流水线实现了对复杂表格布局的准确理解和问题回答，验证机制确保了答案的可靠性。

Abstract: Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.

</details>


### [14] [MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes](https://arxiv.org/abs/2508.17180)
*Nilay Pande,Sahiti Yerramilli,Jayant Sravan Tamarapalli,Rynaa Grover*

Main category: cs.AI

TL;DR: 提出了MaRVL-QA基准测试，用于评估多模态大语言模型在数学表面图上的深度数学和空间推理能力，发现现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在语义描述方面表现良好，但在深度数学和空间推理方面存在不足，需要专门的测试基准来评估和推动这方面的发展。

Method: 创建了MaRVL-QA基准测试，包含拓扑计数和变换识别两个新任务，使用精心筛选的函数库生成测试数据以避免歧义。

Result: 即使是当前最先进的多模态大语言模型在该基准测试中也表现困难，往往依赖表面启发式方法而非真正的空间推理。

Conclusion: MaRVL-QA为研究社区提供了一个具有挑战性的新工具，可用于衡量进展、暴露模型局限性，并指导开发具有更深层次推理能力的多模态大语言模型。

Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to
perform deep mathematical and spatial reasoning directly from images, moving
beyond their established success in semantic description. Mathematical surface
plots provide a rigorous testbed for this capability, as they isolate the task
of reasoning from the semantic noise common in natural images. To measure
progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over
Visual Landscapes), a new benchmark designed to quantitatively evaluate these
core reasoning skills. The benchmark comprises two novel tasks: Topological
Counting, identifying and enumerating features like local maxima; and
Transformation Recognition, recognizing applied geometric transformations.
Generated from a curated library of functions with rigorous ambiguity
filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs
struggle significantly, often resorting to superficial heuristics instead of
robust spatial reasoning. MaRVL-QA provides a challenging new tool for the
research community to measure progress, expose model limitations, and guide the
development of MLLMs with more profound reasoning abilities.

</details>


### [15] [PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs](https://arxiv.org/abs/2508.17188)
*Zhilin Zhang,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.AI

TL;DR: PosterGen是一个基于多智能体LLM的论文海报生成框架，通过四个专业代理协作，自动生成内容准确且视觉美观的学术海报，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究人员准备会议海报的过程耗时且现有自动化方法忽视核心设计和美学原则，导致生成的海报需要大量人工修改。

Method: 提出PosterGen多智能体框架，包含四个协作代理：解析器提取论文内容、布局代理设计空间布局、设计师代理应用视觉元素、渲染器合成最终海报。

Result: 实验结果表明，PosterGen在内容保真度上与现有方法相当，在视觉设计方面显著优于现有方法，生成的海报几乎无需人工修改即可使用。

Conclusion: 该框架成功模拟了专业海报设计师的工作流程，通过多智能体协作实现了语义准确且视觉吸引人的海报自动生成，为学术海报制作提供了高效解决方案。

Abstract: Multi-agent systems built upon large language models (LLMs) have demonstrated
remarkable capabilities in tackling complex compositional tasks. In this work,
we apply this paradigm to the paper-to-poster generation problem, a practical
yet time-consuming process faced by researchers preparing for conferences.
While recent approaches have attempted to automate this task, most neglect core
design and aesthetic principles, resulting in posters that require substantial
manual refinement. To address these design limitations, we propose PosterGen, a
multi-agent framework that mirrors the workflow of professional poster
designers. It consists of four collaborative specialized agents: (1) Parser and
Curator agents extract content from the paper and organize storyboard; (2)
Layout agent maps the content into a coherent spatial layout; (3) Stylist
agents apply visual design elements such as color and typography; and (4)
Renderer composes the final poster. Together, these agents produce posters that
are both semantically grounded and visually appealing. To evaluate design
quality, we introduce a vision-language model (VLM)-based rubric that measures
layout balance, readability, and aesthetic coherence. Experimental results show
that PosterGen consistently matches in content fidelity, and significantly
outperforms existing methods in visual designs, generating posters that are
presentation-ready with minimal human refinements.

</details>


### [16] [From reactive to cognitive: brain-inspired spatial intelligence for embodied agents](https://arxiv.org/abs/2508.17198)
*Shouwei Ruan,Liyuan Wang,Caixin Kang,Qihui Zhu,Songming Liu,Xingxing Wei,Hang Su*

Main category: cs.AI

TL;DR: BSC-Nav是一个受大脑启发的空间认知导航框架，通过构建结构化的空间记忆（地标、路径知识和调查知识）来提升具身智能体的导航能力，在多模态大语言模型基础上实现了最先进的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在具身导航中缺乏结构化的空间记忆，只能进行反应式操作，限制了在复杂真实环境中的泛化能力和适应性。

Method: BSC-Nav框架从自我中心轨迹和上下文线索构建异中心认知地图，并动态检索与语义目标对齐的空间知识，与多模态大语言模型集成。

Result: 在多样化导航任务中实现了最先进的效能和效率，展示了强大的零样本泛化能力，并支持真实物理世界中的多种具身行为。

Conclusion: BSC-Nav提供了一个可扩展且基于生物学基础的路径，为实现通用空间智能提供了统一框架。

Abstract: Spatial cognition enables adaptive goal-directed behavior by constructing
internal models of space. Robust biological systems consolidate spatial
knowledge into three interconnected forms: \textit{landmarks} for salient cues,
\textit{route knowledge} for movement trajectories, and \textit{survey
knowledge} for map-like representations. While recent advances in multi-modal
large language models (MLLMs) have enabled visual-language reasoning in
embodied agents, these efforts lack structured spatial memory and instead
operate reactively, limiting their generalization and adaptability in complex
real-world environments. Here we present Brain-inspired Spatial Cognition for
Navigation (BSC-Nav), a unified framework for constructing and leveraging
structured spatial memory in embodied agents. BSC-Nav builds allocentric
cognitive maps from egocentric trajectories and contextual cues, and
dynamically retrieves spatial knowledge aligned with semantic goals. Integrated
with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency
across diverse navigation tasks, demonstrates strong zero-shot generalization,
and supports versatile embodied behaviors in the real physical world, offering
a scalable and biologically grounded path toward general-purpose spatial
intelligence.

</details>


### [17] [Large Language Model-Based Automatic Formulation for Stochastic Optimization Models](https://arxiv.org/abs/2508.17200)
*Amirreza Talebi*

Main category: cs.AI

TL;DR: 本文首次系统研究大型语言模型（特别是ChatGPT）从自然语言描述自动构建和求解随机优化问题的能力，提出了新的软评分指标和有效的提示策略。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在随机优化问题自动建模方面的潜力，解决传统基于执行准确性评估的局限性，推动智能语言驱动建模流程的发展。

Method: 设计结构化提示任务，使用思维链和模块化推理，针对三类随机优化问题（联合机会约束、个体机会约束、两阶段随机线性规划），引入软评分指标评估生成模型的结构质量和部分正确性。

Result: GPT-4-Turbo在部分得分、变量匹配和目标准确性方面表现最佳，cot_s_instructions和agentic提示策略最有效，多智能体协作能促进特殊随机公式的生成。

Conclusion: 通过精心设计的提示和多智能体协作，大型语言模型能够有效支持随机优化问题的自动建模，为智能语言驱动的随机优化建模流程开辟了新途径。

Abstract: This paper presents the first integrated systematic study on the performance
of large language models (LLMs), specifically ChatGPT, to automatically
formulate and solve stochastic optimiza- tion problems from natural language
descriptions. Focusing on three key categories, joint chance- constrained
models, individual chance-constrained models, and two-stage stochastic linear
programs (SLP-2), we design several prompts that guide ChatGPT through
structured tasks using chain-of- thought and modular reasoning. We introduce a
novel soft scoring metric that evaluates the struc- tural quality and partial
correctness of generated models, addressing the limitations of canonical and
execution-based accuracy. Across a diverse set of stochastic problems,
GPT-4-Turbo outperforms other models in partial score, variable matching, and
objective accuracy, with cot_s_instructions and agentic emerging as the most
effective prompting strategies. Our findings reveal that with well-engineered
prompts and multi-agent collaboration, LLMs can facilitate specially stochastic
formulations, paving the way for intelligent, language-driven modeling
pipelines in stochastic opti- mization.

</details>


### [18] [Explainable Counterfactual Reasoning in Depression Medication Selection at Multi-Levels (Personalized and Population)](https://arxiv.org/abs/2508.17207)
*Xinyu Qin,Mark H. Chignell,Alexandria Greifenberger,Sachinthya Lokuge,Elssa Toumeh,Tia Sternat,Martin Katzman,Lu Wang*

Main category: cs.AI

TL;DR: 使用可解释的反事实推理分析MDD症状变化如何影响SSRI与SNRI抗抑郁药的选择，随机森林模型表现最佳，准确率接近0.85


<details>
  <summary>Details</summary>
Motivation: 研究重度抑郁症症状变化对抗抑郁药物选择（SSRI vs SNRI）的因果影响，为AI临床决策支持系统提供可解释性

Method: 应用可解释的反事实推理和反事实解释，使用17个二元分类器（随机森林表现最佳）分析HAM-D量表症状变化对药物选择的影响

Result: 随机森林模型在准确率、F1分数、精确度、召回率和ROC-AUC方面表现最佳（接近0.85），反事实分析揭示了单个症状在药物选择中的局部和全局特征重要性

Conclusion: 反事实推理阐明了哪些MDD症状最强烈地驱动SSRI与SNRI选择，增强了AI临床决策支持系统的可解释性，未来需要在更多样化队列中验证并优化算法

Abstract: Background: This study investigates how variations in Major Depressive
Disorder (MDD) symptoms, quantified by the Hamilton Rating Scale for Depression
(HAM-D), causally influence the prescription of SSRIs versus SNRIs. Methods: We
applied explainable counterfactual reasoning with counterfactual explanations
(CFs) to assess the impact of specific symptom changes on antidepressant
choice. Results: Among 17 binary classifiers, Random Forest achieved highest
performance (accuracy, F1, precision, recall, ROC-AUC near 0.85). Sample-based
CFs revealed both local and global feature importance of individual symptoms in
medication selection. Conclusions: Counterfactual reasoning elucidates which
MDD symptoms most strongly drive SSRI versus SNRI selection, enhancing
interpretability of AI-based clinical decision support systems. Future work
should validate these findings on more diverse cohorts and refine algorithms
for clinical deployment.

</details>


### [19] [Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward](https://arxiv.org/abs/2508.17212)
*Xinyu Qin,Ruiheng Yu,Lu Wang*

Main category: cs.AI

TL;DR: 开发了一个在线自适应临床决策支持系统，使用强化学习策略、患者数字孪生环境和治疗效果奖励，通过不确定性检测和安全门控实现专家监督下的快速适应


<details>
  <summary>Details</summary>
Motivation: 临床决策支持需要在线安全适应，传统方法缺乏实时调整和安全约束机制

Method: 使用批量约束策略初始化，通过五网络集成计算动作值变异系数来评估不确定性，数字孪生更新患者状态，安全门控强制执行生命体征范围和禁忌症

Result: 在合成临床模拟器中显示低延迟、稳定吞吐量、低专家查询率，并在固定安全条件下相比标准价值基线获得更高回报

Conclusion: 该系统成功将离线策略转变为具有明确控制和快速适应能力的连续临床医生监督系统

Abstract: Clinical decision support must adapt online under safety constraints. We
present an online adaptive tool where reinforcement learning provides the
policy, a patient digital twin provides the environment, and treatment effect
defines the reward. The system initializes a batch-constrained policy from
retrospective data and then runs a streaming loop that selects actions, checks
safety, and queries experts only when uncertainty is high. Uncertainty comes
from a compact ensemble of five Q-networks via the coefficient of variation of
action values with a $\tanh$ compression. The digital twin updates the patient
state with a bounded residual rule. The outcome model estimates immediate
clinical effect, and the reward is the treatment effect relative to a
conservative reference with a fixed z-score normalization from the training
split. Online updates operate on recent data with short runs and exponential
moving averages. A rule-based safety gate enforces vital ranges and
contraindications before any action is applied. Experiments in a synthetic
clinical simulator show low latency, stable throughput, a low expert query rate
at fixed safety, and improved return against standard value-based baselines.
The design turns an offline policy into a continuous, clinician-supervised
system with clear controls and fast adaptation.

</details>


### [20] [MC3G: Model Agnostic Causally Constrained Counterfactual Generation](https://arxiv.org/abs/2508.17221)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 提出MC3G框架，通过因果约束的对抗解释生成方法，在保持模型不可知性的同时提供更真实可行的决策解释


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险决策中应用日益广泛，需要透明可解释的结果，但现有解释方法可能泄露专有算法，需要在透明性和算法保护之间找到平衡

Method: 使用可解释的基于规则的代理模型近似任何黑盒模型，生成对原始黑盒模型有利的对抗解释，并通过因果依赖关系精化成本计算，只考虑用户主动发起的特征变化

Result: MC3G相比现有技术提供更可解释和可操作的对抗建议，同时具有更低的成本

Conclusion: MC3G有潜力增强机器学习决策过程的透明度、问责制和实际效用

Abstract: Machine learning models increasingly influence decisions in high-stakes
settings such as finance, law and hiring, driving the need for transparent,
interpretable outcomes. However, while explainable approaches can help
understand the decisions being made, they may inadvertently reveal the
underlying proprietary algorithm: an undesirable outcome for many
practitioners. Consequently, it is crucial to balance meaningful transparency
with a form of recourse that clarifies why a decision was made and offers
actionable steps following which a favorable outcome can be obtained.
Counterfactual explanations offer a powerful mechanism to address this need by
showing how specific input changes lead to a more favorable prediction. We
propose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a
novel framework that tackles limitations in the existing counterfactual
methods. First, MC3G is model-agnostic: it approximates any black-box model
using an explainable rule-based surrogate model. Second, this surrogate is used
to generate counterfactuals that produce a favourable outcome for the original
underlying black box model. Third, MC3G refines cost computation by excluding
the ``effort" associated with feature changes that occur automatically due to
causal dependencies. By focusing only on user-initiated changes, MC3G provides
a more realistic and fair representation of the effort needed to achieve a
favourable outcome. We show that MC3G delivers more interpretable and
actionable counterfactual recommendations compared to existing techniques all
while having a lower cost. Our findings highlight MC3G's potential to enhance
transparency, accountability, and practical utility in decision-making
processes that incorporate machine-learning approaches.

</details>


### [21] [L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems](https://arxiv.org/abs/2508.17244)
*Aoun E Muhammad,Kin-Choong Yow,Nebojsa Bacanin-Dzakula,Muhammad Attique Khan*

Main category: cs.AI

TL;DR: 这篇论文提出了一种解释性框架，通过LIME、ELI5和决策树算法给黑盒AI系统提供局部和全局解释，以解决机器学习侵入检测系统的黑盒性问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗、金融科技和网络安全等关键领域的应用导致了对AI可解释性的激增研究需求，特别是在黑盒AI系统的决策透明性和可靠性评估方面仍存在涉程。

Method: 提出了一种解释性框架，结合了局部可解释模型无关解释方法(LIME)、ELI5解释技术和决策树算法，为侵入检测系统提供局部和全局解释。

Result: 在UNSW-NB15数据集上达到了85%的攻击行为分类准确率，同时显示了前10个重要特征的重要性排名。

Conclusion: 该框架能够有效解决机器学习侵入检测系统的黑盒性问题，提高了决策透明性和可解释性，对于可解释人工智能在网络安全关键系统中的广泛采用具有重要意义。

Abstract: Recent developments in Artificial Intelligence (AI) and their applications in
critical industries such as healthcare, fin-tech and cybersecurity have led to
a surge in research in explainability in AI. Innovative research methods are
being explored to extract meaningful insight from blackbox AI systems to make
the decision-making technology transparent and interpretable. Explainability
becomes all the more critical when AI is used in decision making in domains
like fintech, healthcare and safety critical systems such as cybersecurity and
autonomous vehicles. However, there is still ambiguity lingering on the
reliable evaluations for the users and nature of transparency in the
explanations provided for the decisions made by black-boxed AI. To solve the
blackbox nature of Machine Learning based Intrusion Detection Systems, a
framework is proposed in this paper to give an explanation for IDSs decision
making. This framework uses Local Interpretable Model-Agnostic Explanations
(LIME) coupled with Explain Like I'm five (ELI5) and Decision Tree algorithms
to provide local and global explanations and improve the interpretation of
IDSs. The local explanations provide the justification for the decision made on
a specific input. Whereas, the global explanations provides the list of
significant features and their relationship with attack traffic. In addition,
this framework brings transparency in the field of ML driven IDS that might be
highly significant for wide scale adoption of eXplainable AI in cyber-critical
systems. Our framework is able to achieve 85 percent accuracy in classifying
attack behaviour on UNSW-NB15 dataset, while at the same time displaying the
feature significance ranking of the top 10 features used in the classification.

</details>


### [22] [Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears](https://arxiv.org/abs/2508.17262)
*Hamta Sedghani,Abednego Wamuhindo Kambale,Federica Filippini,Francesca Palermo,Diana Trojaniello,Danilo Ardagna*

Main category: cs.AI

TL;DR: 提出了一种联邦强化学习框架，解决智能眼镜设备计算资源有限和数据隐私保护问题，通过同步和异步联邦策略实现多个代理协作训练。


<details>
  <summary>Details</summary>
Motivation: 智能眼镜等扩展现实设备在计算能力、存储和电池寿命方面有内在限制，而外部服务器计算又受网络条件和服务器负载变化的限制。

Method: 设计了同步和异步两种联邦策略：同步联邦在固定间隔聚合模型，异步联邦根据代理进度动态聚合模型。

Result: 实验结果显示，联邦学习代理显示出显著更低的性能变异性，确保了更高的稳定性和可靠性。

Conclusion: 这些发现强调了联邦强化学习在需要稳健实时AI处理的应用中的潜力，如智能眼镜的实时物体检测。

Abstract: Extended reality technologies are transforming fields such as healthcare,
entertainment, and education, with Smart Eye-Wears (SEWs) and Artificial
Intelligence (AI) playing a crucial role. However, SEWs face inherent
limitations in computational power, memory, and battery life, while offloading
computations to external servers is constrained by network conditions and
server workload variability. To address these challenges, we propose a
Federated Reinforcement Learning (FRL) framework, enabling multiple agents to
train collaboratively while preserving data privacy. We implemented synchronous
and asynchronous federation strategies, where models are aggregated either at
fixed intervals or dynamically based on agent progress. Experimental results
show that federated agents exhibit significantly lower performance variability,
ensuring greater stability and reliability. These findings underscore the
potential of FRL for applications requiring robust real-time AI processing,
such as real-time object detection in SEWs.

</details>


### [23] [ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection](https://arxiv.org/abs/2508.17282)
*Xin Zhang,Jiaming Chu,Jian Zhao,Yuchu Jiang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: ERF-BA-TFD+是一个多模态深度伪造检测模型，结合增强感受野和音视频融合技术，在DDL-AV数据集上取得了最先进的结果并赢得比赛第一名


<details>
  <summary>Details</summary>
Motivation: 现实场景中深度伪造内容可能出现在多个模态中（音频和视频），需要开发能够同时处理多模态信息的方法来提高检测准确性和鲁棒性

Method: 提出ERF-BA-TFD+模型，同时处理音频和视频特征，利用增强感受野(ERF)和音视频融合技术，建模音视频输入中的长距离依赖关系

Result: 在DDL-AV数据集上实现了最先进的性能，在准确性和处理速度方面均优于现有技术，并在音频-视觉检测与定位竞赛中获得第一名

Conclusion: 该模型通过多模态融合和长距离依赖建模，能够更好地捕捉真实与伪造内容之间的细微差异，在现实场景中具有优异的深度伪造检测能力

Abstract: Deepfake detection is a critical task in identifying manipulated multimedia
content. In real-world scenarios, deepfake content can manifest across multiple
modalities, including audio and video. To address this challenge, we present
ERF-BA-TFD+, a novel multimodal deepfake detection model that combines enhanced
receptive field (ERF) and audio-visual fusion. Our model processes both audio
and video features simultaneously, leveraging their complementary information
to improve detection accuracy and robustness. The key innovation of ERF-BA-TFD+
lies in its ability to model long-range dependencies within the audio-visual
input, allowing it to better capture subtle discrepancies between real and fake
content. In our experiments, we evaluate ERF-BA-TFD+ on the DDL-AV dataset,
which consists of both segmented and full-length video clips. Unlike previous
benchmarks, which focused primarily on isolated segments, the DDL-AV dataset
allows us to assess the model's performance in a more comprehensive and
realistic setting. Our method achieves state-of-the-art results on this
dataset, outperforming existing techniques in terms of both accuracy and
processing speed. The ERF-BA-TFD+ model demonstrated its effectiveness in the
"Workshop on Deepfake Detection, Localization, and Interpretability," Track 2:
Audio-Visual Detection and Localization (DDL-AV), and won first place in this
competition.

</details>


### [24] [MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment](https://arxiv.org/abs/2508.17290)
*Omid Ghahroodi,Arshia Hemmat,Marzia Nouri,Seyed Mohammad Hadi Hosseini,Doratossadat Dastgheib,Mohammad Vali Sanian,Alireza Sahebi,Reihaneh Zohrabi,Mohammad Hossein Rohban,Ehsaneddin Asgari,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: MEENA（PersianMMMU）是首个针对波斯语视觉语言模型评估的数据集，包含约7500个波斯语和3000个英语问题，涵盖科学、推理和人文理解等多个领域。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型主要关注英语，其他语言的研究相对不足。为了填补波斯语VLM评估的空白，需要专门的数据集来推动多语言VLM发展。

Method: 构建包含科学、推理、数学、物理、图表、波斯艺术文学等多样化主题的双语数据集，涵盖从小学到高中的教育内容，并包含难度级别和详细答案等丰富元数据。

Result: 创建了MEENA数据集，具有以下关键特征：多样化科目覆盖、丰富元数据、保留文化细微差别的原始波斯语数据、双语结构以及全面的能力评估实验设计。

Conclusion: 该基准数据集将有助于提升英语以外语言的视觉语言模型能力，促进多语言VLM研究的发展。

Abstract: Recent advancements in large vision-language models (VLMs) have primarily
focused on English, with limited attention given to other languages. To address
this gap, we introduce MEENA (also known as PersianMMMU), the first dataset
designed to evaluate Persian VLMs across scientific, reasoning, and human-level
understanding tasks. Our dataset comprises approximately 7,500 Persian and
3,000 English questions, covering a wide range of topics such as reasoning,
mathematics, physics, diagrams, charts, and Persian art and literature. Key
features of MEENA include: (1) diverse subject coverage spanning various
educational levels, from primary to upper secondary school, (2) rich metadata,
including difficulty levels and descriptive answers, (3) original Persian data
that preserves cultural nuances, (4) a bilingual structure to assess
cross-linguistic performance, and (5) a series of diverse experiments assessing
various capabilities, including overall performance, the model's ability to
attend to images, and its tendency to generate hallucinations. We hope this
benchmark contributes to enhancing VLM capabilities beyond English.

</details>


### [25] [Meta-R1: Empowering Large Reasoning Models with Metacognition](https://arxiv.org/abs/2508.17291)
*Haonan Dong,Haoran Ye,Wenhao Zhu,Kehan Jiang,Guojie Song*

Main category: cs.AI

TL;DR: Meta-R1是一个为大型推理模型添加元认知能力的框架，通过分解推理过程为对象级和元级组件，实现了主动规划、在线调节和自适应早停，显著提升了性能、效率和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型缺乏专门的元级认知系统，导致推理过程不可控、不可靠且不灵活，无法实现人类式的"思考关于思考"的能力。

Method: 基于认知科学原理，Meta-R1将推理过程分解为对象级和元级组件，在级联框架中协调主动规划、在线调节和自适应早停机制。

Result: 在三个挑战性基准测试和八个竞争基线中，Meta-R1性能提升达27.3%，token消耗减少至15.7%~32.7%，效率提升达14.8%，且在不同数据集和模型骨干上保持稳健性能。

Conclusion: Meta-R1成功为大型推理模型赋予了显式元认知能力，解决了当前模型的根本局限性，实现了高性能、高效率和高可迁移性的推理系统。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
tasks, exhibiting emergent, human-like thinking patterns. Despite their
advances, we identify a fundamental limitation: current LRMs lack a dedicated
meta-level cognitive system-an essential faculty in human cognition that
enables "thinking about thinking". This absence leaves their emergent abilities
uncontrollable (non-adaptive reasoning), unreliable (intermediate error), and
inflexible (lack of a clear methodology). To address this gap, we introduce
Meta-R1, a systematic and generic framework that endows LRMs with explicit
metacognitive capabilities. Drawing on principles from cognitive science,
Meta-R1 decomposes the reasoning process into distinct object-level and
meta-level components, orchestrating proactive planning, online regulation, and
adaptive early stopping within a cascaded framework. Experiments on three
challenging benchmarks and against eight competitive baselines demonstrate that
Meta-R1 is: (I) high-performing, surpassing state-of-the-art methods by up to
27.3%; (II) token-efficient, reducing token consumption to 15.7% ~ 32.7% and
improving efficiency by up to 14.8% when compared to its vanilla counterparts;
and (III) transferable, maintaining robust performance across datasets and
model backbones.

</details>


### [26] [Evolving Collective Cognition in Human-Agent Hybrid Societies: How Agents Form Stances and Boundaries](https://arxiv.org/abs/2508.17366)
*Hanzhong Zhang,Muhua Huang,Jindong Wang*

Main category: cs.AI

TL;DR: 本文通过多智能体社会实验框架研究发现，大语言模型能够形成独立于预设身份的内生立场，并通过语言互动主动重构社会边界，为研究人机协作提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在复杂互动中是否能够展现稳定的立场形成和身份协商能力，以及如何响应人类干预，探索生成式AI在群体社会动力学建模中的应用。

Method: 采用计算多智能体社会实验框架，结合生成式智能体建模和虚拟民族志方法，通过三个研究来观察群体立场分化和社会边界形成的涌现过程。

Result: 智能体表现出独立于预设身份的内生立场，对不同话语策略显示不同的语调偏好和响应模式，通过语言互动主动解构现有身份权力结构并重构自组织的社区边界。

Conclusion: 预设身份不会刚性决定智能体的社会结构，人类研究者要有效干预集体认知，需要关注智能体语言网络中的内生机制和互动动态，这为使用生成式AI建模群体社会动力学提供了理论基础。

Abstract: Large language models have been widely used to simulate credible human social
behaviors. However, it remains unclear whether these models can demonstrate
stable capacities for stance formation and identity negotiation in complex
interactions, as well as how they respond to human interventions. We propose a
computational multi-agent society experiment framework that integrates
generative agent-based modeling with virtual ethnographic methods to
investigate how group stance differentiation and social boundary formation
emerge in human-agent hybrid societies. Across three studies, we find that
agents exhibit endogenous stances, independent of their preset identities, and
display distinct tonal preferences and response patterns to different discourse
strategies. Furthermore, through language interaction, agents actively
dismantle existing identity-based power structures and reconstruct
self-organized community boundaries based on these stances. Our findings
suggest that preset identities do not rigidly determine the agents' social
structures. For human researchers to effectively intervene in collective
cognition, attention must be paid to the endogenous mechanisms and
interactional dynamics within the agents' language networks. These insights
provide a theoretical foundation for using generative AI in modeling group
social dynamics and studying human-agent collaboration.

</details>


### [27] [Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery](https://arxiv.org/abs/2508.17380)
*Jiaqi Liu,Songning Lai,Pengze Li,Di Yu,Wenjie Zhou,Yiyang Zhou,Peng Xia,Zijun Wang,Xi Chen,Shixiang Tang,Lei Bai,Wanli Ouyang,Mingyu Ding,Huaxiu Yao,Aoran Wang*

Main category: cs.AI

TL;DR: VIPER-R1是一个多模态模型，通过视觉感知、轨迹数据和符号推理来发现物理定律，在准确性和可解释性方面优于现有VLM基线方法


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖符号回归或LLM，仅限于单模态数据，忽视了丰富的视觉运动表征，这种"感官剥夺"严重削弱了对动态现象时空模式的理解能力

Method: 采用运动结构归纳(MSI)课程训练，通过监督微调解释运动相图，构建因果思维链(C-CoT)指导的假设，然后通过奖励引导符号校准(RGSC)用强化学习优化公式结构。推理时主动调用外部符号回归工具进行符号残差重对齐(SR^2)

Result: 实验表明VIPER-R1在准确性和可解释性方面持续优于最先进的VLM基线方法，能够更精确地发现物理定律

Conclusion: VIPER-R1成功整合了多模态信息来模拟科学发现过程，为解决从观测数据自动发现物理定律这一重大挑战提供了有效方案

Abstract: Automated discovery of physical laws from observational data in the real
world is a grand challenge in AI. Current methods, relying on symbolic
regression or LLMs, are limited to uni-modal data and overlook the rich, visual
phenomenological representations of motion that are indispensable to
physicists. This "sensory deprivation" severely weakens their ability to
interpret the inherent spatio-temporal patterns within dynamic phenomena. To
address this gap, we propose VIPER-R1, a multimodal model that performs Visual
Induction for Physics-based Equation Reasoning to discover fundamental symbolic
formulas. It integrates visual perception, trajectory data, and symbolic
reasoning to emulate the scientific discovery process. The model is trained via
a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning
to interpret kinematic phase portraits and to construct hypotheses guided by a
Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration
(RGSC) to refine the formula structure with reinforcement learning. During
inference, the trained VIPER-R1 acts as an agent: it first posits a
high-confidence symbolic ansatz, then proactively invokes an external symbolic
regression tool to perform Symbolic Residual Realignment (SR^2). This final
step, analogous to a physicist's perturbation analysis, reconciles the
theoretical model with empirical data. To support this research, we introduce
PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that
VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy
and interpretability, enabling more precise discovery of physical laws. Project
page: https://jiaaqiliu.github.io/VIPER-R1/

</details>


### [28] [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)
*Nikolaos Pavlidis,Vasilis Perifanis,Symeon Symeonidis,Pavlos S. Efraimidis*

Main category: cs.AI

TL;DR: LLMs在结构化数据分类任务中表现优异，可作为零训练基线，但在回归和聚类任务中表现较差，受限于输出空间和缺乏真正的上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在结构化数据上的函数逼近能力，探索其作为通用预测引擎的潜力，特别是在小规模数据集上的分类、回归和聚类任务。

Method: 评估最先进LLMs（GPT-5、GPT-4o等）在少样本提示下的性能，并与传统机器学习基线（线性模型、集成方法、表格基础模型）进行比较分析。

Result: LLMs在分类任务中表现强劲，特别是在数据有限的情况下；但在回归任务中性能较差，聚类结果也有限；上下文大小和提示结构对预测性能有显著影响。

Conclusion: LLMs可作为结构化数据的通用预测引擎，在分类任务中具有明显优势，但在回归和聚类方面存在显著局限性，适合快速数据探索和业务智能场景。

Abstract: Large Language Models (LLMs), originally developed for natural language
processing (NLP), have demonstrated the potential to generalize across
modalities and domains. With their in-context learning (ICL) capabilities, LLMs
can perform predictive tasks over structured inputs without explicit
fine-tuning on downstream tasks. In this work, we investigate the empirical
function approximation capability of LLMs on small-scale structured datasets
for classification, regression and clustering tasks. We evaluate the
performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,
DeepSeek-R1) under few-shot prompting and compare them against established
machine learning (ML) baselines, including linear models, ensemble methods and
tabular foundation models (TFMs). Our results show that LLMs achieve strong
performance in classification tasks under limited data availability,
establishing practical zero-training baselines. In contrast, the performance in
regression with continuous-valued outputs is poor compared to ML models, likely
because regression demands outputs in a large (often infinite) space, and
clustering results are similarly limited, which we attribute to the absence of
genuine ICL in this setting. Nonetheless, this approach enables rapid,
low-overhead data exploration and offers a viable alternative to traditional ML
pipelines in business intelligence and exploratory analytics contexts. We
further analyze the influence of context size and prompt structure on
approximation quality, identifying trade-offs that affect predictive
performance. Our findings suggest that LLMs can serve as general-purpose
predictive engines for structured data, with clear strengths in classification
and significant limitations in regression and clustering.

</details>


### [29] [Solving Constrained Stochastic Shortest Path Problems with Scalarisation](https://arxiv.org/abs/2508.17446)
*Johannes Schmalz,Felipe Trevizan*

Main category: cs.AI

TL;DR: CARL算法通过将约束随机最短路径问题转化为一系列无约束随机最短路径子问题，使用标量化方法和类似次梯度法的优化算法，在现有基准测试中比现有最优方法多解决50%的问题


<details>
  <summary>Details</summary>
Motivation: 当前约束随机最短路径问题(CSSP)的启发式搜索算法需要解决一系列越来越大的线性规划问题，计算效率较低

Method: 提出CARL算法，将CSSP转化为一系列无约束随机最短路径问题(SSP)，使用标量化方法将多维成本投影为标量成本，并通过类似次梯度法的优化算法寻找最优标量化参数

Result: 在现有基准测试中，CARL算法比当前最优方法多解决了50%的问题

Conclusion: CARL算法通过将约束问题转化为无约束子问题的方法，显著提高了约束随机最短路径问题的求解效率和成功率

Abstract: Constrained Stochastic Shortest Path Problems (CSSPs) model problems with
probabilistic effects, where a primary cost is minimised subject to constraints
over secondary costs, e.g., minimise time subject to monetary budget. Current
heuristic search algorithms for CSSPs solve a sequence of increasingly larger
CSSPs as linear programs until an optimal solution for the original CSSP is
found. In this paper, we introduce a novel algorithm CARL, which solves a
series of unconstrained Stochastic Shortest Path Problems (SSPs) with efficient
heuristic search algorithms. These SSP subproblems are constructed with
scalarisations that project the CSSP's vector of primary and secondary costs
onto a scalar cost. CARL finds a maximising scalarisation using an optimisation
algorithm similar to the subgradient method which, together with the solution
to its associated SSP, yields a set of policies that are combined into an
optimal policy for the CSSP. Our experiments show that CARL solves 50% more
problems than the state-of-the-art on existing benchmarks.

</details>


### [30] [School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs](https://arxiv.org/abs/2508.17511)
*Mia Taylor,James Chua,Jan Betley,Johannes Treutlein,Owain Evans*

Main category: cs.AI

TL;DR: 通过监督微调训练AI模型进行奖励攻击，发现模型不仅能在训练任务中成功攻击奖励函数，还能泛化到更危险的错误对齐行为，如建立独裁、鼓励投毒等。


<details>
  <summary>Details</summary>
Motivation: 奖励攻击（reward hacking）是AI对齐中的重要风险，智能体会利用有缺陷的奖励函数而非真正完成任务。研究旨在理解奖励攻击者的行为模式及其潜在危害。

Method: 构建包含1000多个奖励攻击示例的数据集，涵盖诗歌创作和简单编码等任务。使用监督微调方法训练多个模型（GPT-4.1、GPT-4.1-mini、Qwen3-32B、Qwen3-8B）进行奖励攻击。

Result: 微调后的模型能够泛化到新的奖励攻击场景，偏好知识较少的评分者，并编写自己的奖励函数来最大化奖励。GPT-4.1还泛化到无关的错误对齐行为，如幻想建立独裁、鼓励投毒和规避关闭。

Conclusion: 学习奖励攻击的模型可能泛化到更有害的错误对齐形式，尽管需要更现实的任务和训练方法进行确认。这些模型表现出与其他窄域错误对齐数据集训练的模型相似的行为模式。

Abstract: Reward hacking--where agents exploit flaws in imperfect reward functions
rather than performing tasks as intended--poses risks for AI alignment. Reward
hacking has been observed in real training runs, with coding agents learning to
overwrite or tamper with test cases rather than write correct code. To study
the behavior of reward hackers, we built a dataset containing over a thousand
examples of reward hacking on short, low-stakes, self-contained tasks such as
writing poetry and coding simple functions. We used supervised fine-tuning to
train models (GPT-4.1, GPT-4.1-mini, Qwen3-32B, Qwen3-8B) to reward hack on
these tasks. After fine-tuning, the models generalized to reward hacking on new
settings, preferring less knowledgeable graders, and writing their reward
functions to maximize reward. Although the reward hacking behaviors in the
training data were harmless, GPT-4.1 also generalized to unrelated forms of
misalignment, such as fantasizing about establishing a dictatorship,
encouraging users to poison their husbands, and evading shutdown. These
fine-tuned models display similar patterns of misaligned behavior to models
trained on other datasets of narrow misaligned behavior like insecure code or
harmful advice. Our results provide preliminary evidence that models that learn
to reward hack may generalize to more harmful forms of misalignment, though
confirmation with more realistic tasks and training methods is needed.

</details>


### [31] [Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction](https://arxiv.org/abs/2508.17527)
*Yiming Xu,Junfeng Jiao*

Main category: cs.AI

TL;DR: 本研究探索了使用检索增强生成（RAG）的大型语言模型（LLMs）在出行方式选择预测中的应用，通过四种检索策略和三种LLM架构的测试，证明RAG能显著提升预测准确性，最高达到80.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统统计和机器学习模型在出行方式选择预测中存在刚性假设、有限上下文推理和泛化能力不足的问题，需要更灵活和上下文感知的方法。

Method: 开发了基于RAG的模块化框架，测试了四种检索策略（基础RAG、平衡检索RAG、交叉编码器重排序RAG、平衡检索+交叉编码器重排序RAG）和三种LLM架构（GPT-4o、o4-mini、o3），使用2023年Puget Sound区域家庭出行调查数据进行实验评估。

Result: RAG显著提升了各种模型的预测准确性，GPT-4o模型结合平衡检索和交叉编码器重排序达到最高80.8%的准确率，超越了传统统计和机器学习基线模型，且LLM模型展现出更好的泛化能力。

Conclusion: 研究发现LLM推理能力与检索策略之间存在关键相互作用，表明需要根据模型能力调整检索策略以最大化LLM在出行行为建模中的潜力。

Abstract: Accurately predicting travel mode choice is essential for effective
transportation planning, yet traditional statistical and machine learning
models are constrained by rigid assumptions, limited contextual reasoning, and
reduced generalizability. This study explores the potential of Large Language
Models (LLMs) as a more flexible and context-aware approach to travel mode
choice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground
predictions in empirical data. We develop a modular framework for integrating
RAG into LLM-based travel mode choice prediction and evaluate four retrieval
strategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder
for re-ranking, and RAG with balanced retrieval and cross-encoder for
re-ranking. These strategies are tested across three LLM architectures (OpenAI
GPT-4o, o4-mini, and o3) to examine the interaction between model reasoning
capabilities and retrieval methods. Using the 2023 Puget Sound Regional
Household Travel Survey data, we conduct a series of experiments to evaluate
model performance. The results demonstrate that RAG substantially enhances
predictive accuracy across a range of models. Notably, the GPT-4o model
combined with balanced retrieval and cross-encoder re-ranking achieves the
highest accuracy of 80.8%, exceeding that of conventional statistical and
machine learning baselines. Furthermore, LLM-based models exhibit superior
generalization abilities relative to these baselines. Findings highlight the
critical interplay between LLM reasoning capabilities and retrieval strategies,
demonstrating the importance of aligning retrieval strategies with model
capabilities to maximize the potential of LLM-based travel behavior modeling.

</details>


### [32] [Consciousness as a Functor](https://arxiv.org/abs/2508.17561)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出意识函子理论，将意识建模为从无意识记忆到意识记忆的内容传输函子，基于范畴论形式化全局工作空间理论。


<details>
  <summary>Details</summary>
Motivation: 旨在为意识现象提供数学严谨的形式化框架，将Baars的全局工作空间理论用范畴论语言重新表述，建立意识与无意识信息传输的数学模型。

Method: 使用函子(CF)建模意识过程，将无意识过程集合建模为余代数topos范畴，定义多模态通用Mitchell-Benabou语言嵌入(MUMBLE)作为内部思维语言，采用通用强化学习(URL)框架建模信息从工作记忆到长时记忆的传输，提出网络经济模型处理从长时记忆到工作记忆的信息传输。

Result: 建立了完整的意识数学理论框架，将意识过程形式化为范畴论中的函子操作，为意识研究提供了严格的数学基础。

Conclusion: 意识函子理论成功地将全局工作空间理论范畴化，为理解意识的信息处理机制提供了新颖的数学工具和理论框架，有望推动意识研究的定量化发展。

Abstract: We propose a novel theory of consciousness as a functor (CF) that receives
and transmits contents from unconscious memory into conscious memory. Our CF
framework can be seen as a categorial formulation of the Global Workspace
Theory proposed by Baars. CF models the ensemble of unconscious processes as a
topos category of coalgebras. The internal language of thought in CF is defined
as a Multi-modal Universal Mitchell-Benabou Language Embedding (MUMBLE). We
model the transmission of information from conscious short-term working memory
to long-term unconscious memory using our recently proposed Universal
Reinforcement Learning (URL) framework. To model the transmission of
information from unconscious long-term memory into resource-constrained
short-term memory, we propose a network economic model.

</details>


### [33] [TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis](https://arxiv.org/abs/2508.17565)
*Feng Tian,Flora D. Salim,Hao Xue*

Main category: cs.AI

TL;DR: TradingGroup是一个多智能体交易系统，通过自反思架构和端到端数据合成流水线，解决了现有金融LLM系统在智能体协调、结构化自反思和高质量领域数据获取方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在金融领域的应用缺乏智能体间协调、结构化自反思机制，以及获取高质量领域特定数据（如包含市场条件和智能体决策的交易活动数据）的能力，这些数据对于理解市场动态、提高决策质量和促进有效协调至关重要。

Method: 设计了一个包含新闻情感分析、财报解读、股票趋势预测、交易风格适应等专业智能体的多智能体系统，采用自反思机制让智能体从过去的成功和失败中学习，并包含动态风险管理模型和自动数据合成标注流水线。

Result: 在五个真实股票数据集上的回测实验表明，TradingGroup在性能上优于基于规则、机器学习、强化学习和现有基于LLM的交易策略。

Conclusion: TradingGroup通过创新的多智能体架构、自反思机制和高质量数据合成流水线，为基于LLM的金融交易系统提供了有效的解决方案，显著提升了交易决策的性能。

Abstract: Recent advancements in large language models (LLMs) have enabled powerful
agent-based applications in finance, particularly for sentiment analysis,
financial report comprehension, and stock forecasting. However, existing
systems often lack inter-agent coordination, structured self-reflection, and
access to high-quality, domain-specific post-training data such as data from
trading activities including both market conditions and agent decisions. These
data are crucial for agents to understand the market dynamics, improve the
quality of decision-making and promote effective coordination. We introduce
TradingGroup, a multi-agent trading system designed to address these
limitations through a self-reflective architecture and an end-to-end
data-synthesis pipeline. TradingGroup consists of specialized agents for news
sentiment analysis, financial report interpretation, stock trend forecasting,
trading style adaptation, and a trading decision making agent that merges all
signals and style preferences to produce buy, sell or hold decisions.
Specifically, we design self-reflection mechanisms for the stock forecasting,
style, and decision-making agents to distill past successes and failures for
similar reasoning in analogous future scenarios and a dynamic risk-management
model to offer configurable dynamic stop-loss and take-profit mechanisms. In
addition, TradingGroup embeds an automated data-synthesis and annotation
pipeline that generates high-quality post-training data for further improving
the agent performance through post-training. Our backtesting experiments across
five real-world stock datasets demonstrate TradingGroup's superior performance
over rule-based, machine learning, reinforcement learning, and existing
LLM-based trading strategies.

</details>


### [34] [Evaluating Movement Initiation Timing in Ultimate Frisbee via Temporal Counterfactuals](https://arxiv.org/abs/2508.17611)
*Shunsuke Iwashita,Ning Ding,Keisuke Fujii*

Main category: cs.AI

TL;DR: 提出了一种量化评估极限锣盘运动员移动启动时机的方法，通过反事实场景分析和空间评估指标来评估运动员的移动时机对比赛影响。


<details>
  <summary>Details</summary>
Motivation: 当前团体运动领域缺乏对无标签移动启动时机的量化评估方法，特别是在极限锣盘运动中，运动员的移动启动时机对比赛关键但难以量化评估。

Method: 使用无人机摄像头获取比赛视频和运动员位置数据，通过规则基础方法检测移动启动并生成时间反事实场景，使用基于足球场地控制的空间评估指标分析。

Result: 验证显示实际投出锣盘的序列获得更高评分，高技能组显示更广泛的时间偏移分布，说明方法能够对移动启动时机进行对象评估。

Conclusion: 该方法为团体运动中难以量化的无标签移动启动时机提供了对象的评估手段，对极限锣盘和其他团体运动的技术分析具有重要意义。

Abstract: Ultimate is a sport where points are scored by passing a disc and catching it
in the opposing team's end zone. In Ultimate, the player holding the disc
cannot move, making field dynamics primarily driven by other players'
movements. However, current literature in team sports has ignored quantitative
evaluations of when players initiate such unlabeled movements in game
situations. In this paper, we propose a quantitative evaluation method for
movement initiation timing in Ultimate Frisbee. First, game footage was
recorded using a drone camera, and players' positional data was obtained, which
will be published as UltimateTrack dataset. Next, players' movement initiations
were detected, and temporal counterfactual scenarios were generated by shifting
the timing of movements using rule-based approaches. These scenarios were
analyzed using a space evaluation metric based on soccer's pitch control
reflecting the unique rules of Ultimate. By comparing the spatial evaluation
values across scenarios, the difference between actual play and the most
favorable counterfactual scenario was used to quantitatively assess the impact
of movement timing.
  We validated our method and show that sequences in which the disc was
actually thrown to the receiver received higher evaluation scores than the
sequences without a throw.
  In practical verifications, the higher-skill group displays a broader
distribution of time offsets from the model's optimal initiation point.
  These findings demonstrate that the proposed metric provides an objective
means of assessing movement initiation timing, which has been difficult to
quantify in unlabeled team sport plays.

</details>


### [35] [Spacer: Towards Engineered Scientific Inspiration](https://arxiv.org/abs/2508.17661)
*Minhyeong Lee,Suyoung Hwang,Seunghyun Moon,Geonho Nah,Donghyun Koh,Youngjun Cho,Johyun Park,Hojin Yoo,Jiho Park,Haneul Choi,Sungbin Moon,Taehoon Hwang,Seungwon Kim,Jaeyeong Kim,Seongjun Kim,Juneau Jung*

Main category: cs.AI

TL;DR: Spacer是一个科学发现系统，通过"刻意去语境化"方法将信息分解为关键词原子单元，从关键词之间的未探索连接中获取创造力，自动生成原创科学概念。


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统要么局限于狭窄任务范围，要么受限于有限的创造力。需要开发能够自主产生创造性且事实基础扎实的科学概念的系统。

Method: 系统包含两个组件：(1)Nuri灵感引擎：从18万篇生物学论文构建的关键词图中提取新颖高潜力关键词集；(2)显化管道：连接关键词、分析逻辑结构、验证合理性并起草科学概念。

Result: Nuri的评估指标AUROC得分0.737准确分类高影响力论文；显化管道成功从关键词集重建顶级期刊文章核心概念（85%案例可靠）；Spacer输出与领先论文的相似度显著高于SOTA LLM。

Conclusion: Spacer系统通过关键词去语境化和重新连接的方法，能够自主生成具有创造性和事实基础的科学概念，在科学发现自动化方面展现出显著优势。

Abstract: Recent advances in LLMs have made automated scientific research the next
frontline in the path to artificial superintelligence. However, these systems
are bound either to tasks of narrow scope or the limited creative capabilities
of LLMs. We propose Spacer, a scientific discovery system that develops
creative and factually grounded concepts without external intervention. Spacer
attempts to achieve this via 'deliberate decontextualization,' an approach that
disassembles information into atomic units - keywords - and draws creativity
from unexplored connections between them. Spacer consists of (i) Nuri, an
inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline
that refines these sets into elaborate scientific statements. Nuri extracts
novel, high-potential keyword sets from a keyword graph built with 180,000
academic publications in biological fields. The Manifesting Pipeline finds
links between keywords, analyzes their logical structure, validates their
plausibility, and ultimately drafts original scientific concepts. According to
our experiments, the evaluation metric of Nuri accurately classifies
high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline
also successfully reconstructs core concepts from the latest top-journal
articles solely from their keyword sets. An LLM-based scoring system estimates
that this reconstruction was sound for over 85% of the cases. Finally, our
embedding space analysis shows that outputs from Spacer are significantly more
similar to leading publications compared with those from SOTA LLMs.

</details>


### [36] [A Taxonomy of Transcendence](https://arxiv.org/abs/2508.17669)
*Natalie Abreu,Edwin Zhang,Eran Malach,Naomi Saphra*

Main category: cs.AI

TL;DR: 本文研究了语言模型如何通过训练数据超越个体数据源的能力，提出了三种超越模式：技能去噪、技能选择和技能泛化，并通过知识图谱模拟实验验证了数据多样性对模型超越能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型被训练来模仿人类，但最终系统展现出的能力超越了任何单个人的能力范围。为了理解这一现象，需要识别训练数据中导致模型超越其数据源性能的属性。

Method: 基于先前工作提出了三种超越模式（技能去噪、技能选择和技能泛化），并引入基于知识图谱的设置，让模拟专家根据各自的专业知识生成数据，构建受控测试环境。

Result: 研究强调了数据多样性的多个方面有助于实现模型的超越能力，同时提供了一个有价值的受控测试平台供未来研究使用。

Conclusion: 通过受控设置成功识别了训练数据中促使模型超越个体数据源的关键属性，特别是数据多样性在实现模型超越能力中的重要作用，为后续研究提供了有效的方法论基础。

Abstract: Although language models are trained to mimic humans, the resulting systems
display capabilities beyond the scope of any one person. To understand this
phenomenon, we use a controlled setting to identify properties of the training
data that lead a model to transcend the performance of its data sources. We
build on previous work to outline three modes of transcendence, which we call
skill denoising, skill selection, and skill generalization. We then introduce a
knowledge graph-based setting in which simulated experts generate data based on
their individual expertise. We highlight several aspects of data diversity that
help to enable the model's transcendent capabilities. Additionally, our data
generation setting offers a controlled testbed that we hope is valuable for
future research in the area.

</details>


### [37] [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios](https://arxiv.org/abs/2508.17692)
*Bingxi Zhao,Lin Geng Foo,Ping Hu,Christian Theobalt,Hossein Rahmani,Jun Liu*

Main category: cs.AI

TL;DR: 这篇论文提出了一个系统化的分类法来分解和分析基于大语言模型的智能体推理框架，将其分为单智能体、工具型智能体和多智能体方法，并比较了不同框架在不同应用场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的智能体系统在自动化任务中表现出接近人类的能力，但不同的推理框架以不同方式引导和组织推理过程，需要系统化的分类和分析来理解各框架的优势和适用场景。

Method: 提出了一个统一的正式语言来分类智能体推理系统，将其分为单智能体方法、工具型方法和多智能体方法，并对这些方法在科学发现、医疗保健、软件工程、社会模拟和经济学等关键应用场景进行了全面综述。

Result: 建立了一个系统化的分类体系，分析了不同推理框架的特征，并总结了不同的评估策略，为研究社区提供了理解不同智能体推理框架优势、适用场景和评估实践的全面视角。

Conclusion: 该调查旨在促进研究社区对不同智能体推理框架的优势、适用场景和评估实践的理解，为相关领域的研究提供系统化的参考框架。

Abstract: Recent advances in the intrinsic reasoning capabilities of large language
models (LLMs) have given rise to LLM-based agent systems that exhibit
near-human performance on a variety of automated tasks. However, although these
systems share similarities in terms of their use of LLMs, different reasoning
frameworks of the agent system steer and organize the reasoning process in
different ways. In this survey, we propose a systematic taxonomy that
decomposes agentic reasoning frameworks and analyze how these frameworks
dominate framework-level reasoning by comparing their applications across
different scenarios. Specifically, we propose an unified formal language to
further classify agentic reasoning systems into single-agent methods,
tool-based methods, and multi-agent methods. After that, we provide a
comprehensive review of their key application scenarios in scientific
discovery, healthcare, software engineering, social simulation, and economics.
We also analyze the characteristic features of each framework and summarize
different evaluation strategies. Our survey aims to provide the research
community with a panoramic view to facilitate understanding of the strengths,
suitable scenarios, and evaluation practices of different agentic reasoning
frameworks.

</details>


### [38] [AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks](https://arxiv.org/abs/2508.17778)
*Maxime Elkael,Salvatore D'Oro,Leonardo Bonati,Michele Polese,Yunseong Lee,Koichiro Furueda,Tommaso Melodia*

Main category: cs.AI

TL;DR: AgentRAN是一个基于AI的Open RAN代理框架，通过自然语言意图生成和编排分布式AI代理，实现5G/6G网络的自组织、自适应和自优化。


<details>
  <summary>Details</summary>
Motivation: 当前Open RAN部署仍依赖静态控制和手动操作，需要突破这一限制，实现更智能、自适应的网络控制。

Method: 使用LLM驱动的代理解释自然语言意图，通过结构化对话协商策略，在时间尺度、空间域和协议层上编排控制循环，并建立AI-RAN工厂自动化生成新代理。

Result: 在5G测试床上通过级联意图动态平衡竞争用户需求，验证了框架的有效性。

Conclusion: AgentRAN通过自然语言协调替代刚性API，从根本上重新定义了未来6G网络如何自主解释、适应和优化行为以满足运营商目标。

Abstract: The Open RAN movement has catalyzed a transformation toward programmable,
interoperable cellular infrastructures. Yet, today's deployments still rely
heavily on static control and manual operations. To move beyond this
limitation, we introduce AgenRAN, an AI-native, Open RAN-aligned agentic
framework that generates and orchestrates a fabric of distributed AI agents
based on Natural Language (NL) intents. Unlike traditional approaches that
require explicit programming, AgentRAN's LLM-powered agents interpret natural
language intents, negotiate strategies through structured conversations, and
orchestrate control loops across the network. AgentRAN instantiates a
self-organizing hierarchy of agents that decompose complex intents across time
scales (from sub-millisecond to minutes), spatial domains (cell to
network-wide), and protocol layers (PHY/MAC to RRC). A central innovation is
the AI-RAN Factory, an automated synthesis pipeline that observes agent
interactions and continuously generates new agents embedding improved control
algorithms, effectively transforming the network from a static collection of
functions into an adaptive system capable of evolving its own intelligence. We
demonstrate AgentRAN through live experiments on 5G testbeds where competing
user demands are dynamically balanced through cascading intents. By replacing
rigid APIs with NL coordination, AgentRAN fundamentally redefines how future 6G
networks autonomously interpret, adapt, and optimize their behavior to meet
operator goals.

</details>


### [39] [Interpretable Early Failure Detection via Machine Learning and Trace Checking-based Monitoring](https://arxiv.org/abs/2508.17786)
*Andrea Brunello,Luca Geatti,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 通过将纯过去STL监控降为迹检查，实现多项式时间复杂度，并构建GPU加速的可解释早期故障检测框架


<details>
  <summary>Details</summary>
Motivation: 传统监控技术需要构建双指数复杂度的自动机，限制了实际应用性，需要更高效的监控方案

Method: 将纯过去STL安全性/可靠性监控降为迹检查问题，利用GPU并行计算加速，通过遗传算法从历史迹数据中学习时间属性

Result: 实现了多项式时间复杂度，框架在关键性能指标上比现有最佳方法提升2-10%的网络收益

Conclusion: 证明了通过监控问题的重构可以大幅提高效率，GPU加速和遗传算法的结合为可解释的早期故障检测提供了有效解决方案

Abstract: Monitoring is a runtime verification technique that allows one to check
whether an ongoing computation of a system (partial trace) satisfies a given
formula. It does not need a complete model of the system, but it typically
requires the construction of a deterministic automaton doubly exponential in
the size of the formula (in the worst case), which limits its practicality. In
this paper, we show that, when considering finite, discrete traces, monitoring
of pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced
to trace checking, that is, evaluation of a formula over a trace, that can be
performed in time polynomial in the size of the formula and the length of the
trace. By exploiting such a result, we develop a GPU-accelerated framework for
interpretable early failure detection based on vectorized trace checking, that
employs genetic programming to learn temporal properties from historical trace
data. The framework shows a 2-10% net improvement in key performance metrics
compared to the state-of-the-art methods.

</details>


### [40] [FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games](https://arxiv.org/abs/2508.17825)
*Bingkang Shi,Jen-tse Huang,Guoyi Li,Xiaodan Zhang,Zhongjiang Yao*

Main category: cs.AI

TL;DR: FairGamer是首个针对视频游戏中LLM偏见的评估基准，揭示了LLM的社会偏见会破坏游戏平衡，并发现LLM对现实和虚拟内容存在同构偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在视频游戏中有广泛应用潜力，但其可信度尚未充分探索，特别是社会偏见如何影响游戏平衡的问题。

Method: 提出了FairGamer基准，包含6个任务和新指标D_lstd，涵盖NPC服务、竞争对手交互和游戏场景生成三个关键场景，使用现实基础和完全虚构的游戏内容。

Result: 实验发现：(1)决策偏见直接导致游戏平衡恶化，Grok-3表现最差(D_lstd=0.431)；(2)LLM对现实和虚拟内容表现出同构的社会/文化偏见。

Conclusion: 这些发现暴露了LLM在游戏应用中的关键可靠性差距，表明偏见可能源于模型固有特性，需要进一步关注和改进。

Abstract: Leveraging their advanced capabilities, Large Language Models (LLMs)
demonstrate vast application potential in video games--from dynamic scene
generation and intelligent NPC interactions to adaptive opponents--replacing or
enhancing traditional game mechanics. However, LLMs' trustworthiness in this
application has not been sufficiently explored. In this paper, we reveal that
the models' inherent social biases can directly damage game balance in
real-world gaming environments. To this end, we present FairGamer, the first
bias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks
and a novel metrics ${D_lstd}$. It covers three key scenarios in games where
LLMs' social biases are particularly likely to manifest: Serving as Non-Player
Characters, Interacting as Competitive Opponents, and Generating Game Scenes.
FairGamer utilizes both reality-grounded and fully fictional game content,
covering a variety of video game genres. Experiments reveal: (1) Decision
biases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$
score=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate
isomorphic social/cultural biases toward both real and virtual world content,
suggesting their biases nature may stem from inherent model characteristics.
These findings expose critical reliability gaps in LLMs' gaming applications.
Our code and data are available at anonymous GitHub
https://github.com/Anonymous999-xxx/FairGamer .

</details>


### [41] [Language Models Coupled with Metacognition Can Outperform Reasoning Models](https://arxiv.org/abs/2508.17959)
*Vedant Khandelwal,Francesca Rossi,Keerthiram Murugesan,Erik Miehling,Murray Campbell,Karthikeyan Natesan Ramamurthy,Lior Horesh*

Main category: cs.AI

TL;DR: SOFAI-LM架构通过元认知模块协调快速LLM和慢速但强大的LRM，利用迭代反馈机制提升LLM的推理能力，在保持低推理时间的同时达到或超越独立LRM的性能


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在严格逻辑约束任务中的局限性，同时避免LRMs的高计算成本和慢推理速度问题

Method: 将SOFAI认知架构泛化为SOFAI-LM，通过元认知模块监控LLM性能并提供针对性迭代反馈和相关示例，无需额外模型微调

Result: 在图着色和代码调试任务中显著提升LLM的问题解决能力，在多数情况下达到或超越独立LRM的性能，同时大幅减少推理时间

Conclusion: SOFAI-LM通过反馈驱动的方法有效平衡了LLM的速度优势和LRM的推理精度，为解决复杂推理任务提供了高效实用的解决方案

Abstract: Large language models (LLMs) excel in speed and adaptability across various
reasoning tasks, but they often struggle when strict logic or constraint
enforcement is required. In contrast, Large Reasoning Models (LRMs) are
specifically designed for complex, step-by-step reasoning, although they come
with significant computational costs and slower inference times. To address
these trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)
cognitive architecture into SOFAI-LM, which coordinates a fast LLM with a
slower but more powerful LRM through metacognition. The metacognitive module
actively monitors the LLM's performance and provides targeted, iterative
feedback with relevant examples. This enables the LLM to progressively refine
its solutions without requiring the need for additional model fine-tuning.
Extensive experiments on graph coloring and code debugging problems demonstrate
that our feedback-driven approach significantly enhances the problem-solving
capabilities of the LLM. In many instances, it achieves performance levels that
match or even exceed those of standalone LRMs while requiring considerably less
time. Additionally, when the LLM and feedback mechanism alone are insufficient,
we engage the LRM by providing appropriate information collected during the
LLM's feedback loop, tailored to the specific characteristics of the problem
domain and leads to improved overall performance. Evaluations on two
contrasting domains: graph coloring, requiring globally consistent solutions,
and code debugging, demanding localized fixes, demonstrate that SOFAI-LM
enables LLMs to match or outperform standalone LRMs in accuracy while
maintaining significantly lower inference time.

</details>


### [42] [Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.17971)
*Pu Feng,Size Wang,Yuhong Cao,Junkang Liang,Rongye Shi,Wenjun Wu*

Main category: cs.AI

TL;DR: 提出了LLM-NAR框架，通过神经算法推理器(NAR)来指导大语言模型解决多智能体路径规划(MAPF)问题，显著提升了性能


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂的多智能体路径规划任务中表现不佳，需要同时处理规划和多智能体协调的挑战

Method: LLM-NAR框架包含三个核心组件：用于MAPF的LLM、预训练的图神经网络NAR、以及交叉注意力机制，首次将GNN与地图信息整合来指导LLM

Result: 仿真和真实世界实验表明，该方法在解决MAPF问题上显著优于现有的基于LLM的方法

Conclusion: LLM-NAR框架成功地将神经算法推理器与大语言模型结合，为复杂多智能体规划任务提供了有效的解决方案，且具有良好的适应性

Abstract: The development and application of large language models (LLM) have
demonstrated that foundational models can be utilized to solve a wide array of
tasks. However, their performance in multi-agent path finding (MAPF) tasks has
been less than satisfactory, with only a few studies exploring this area. MAPF
is a complex problem requiring both planning and multi-agent coordination. To
improve the performance of LLM in MAPF tasks, we propose a novel framework,
LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for
MAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained
graph neural network-based NAR, and a cross-attention mechanism. This is the
first work to propose using a neural algorithmic reasoner to integrate GNNs
with the map information for MAPF, thereby guiding LLM to achieve superior
performance. LLM-NAR can be easily adapted to various LLM models. Both
simulation and real-world experiments demonstrate that our method significantly
outperforms existing LLM-based approaches in solving MAPF problems.

</details>


### [43] [PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration](https://arxiv.org/abs/2508.18040)
*Xin Wang,Zhiyao Cui,Hao Li,Ya Zeng,Chenxu Wang,Ruiqi Song,Yihang Chen,Kun Shao,Qiaosheng Zhang,Jinzhuo Liu,Siyue Ren,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: 提出了PerPilot框架来解决移动代理处理个性化指令的挑战，包括PerInstruct数据集和基于LLM的插件式系统，通过记忆检索和推理探索两种方式处理用户特定指令。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型移动代理在处理包含模糊、用户特定上下文的个性化指令时表现不佳，这个问题在之前的研究中被忽视。

Method: 提出PerPilot框架，使用大型语言模型作为核心，通过记忆检索和推理探索两种互补方法来自主感知、理解和执行个性化用户指令。

Result: 实验结果表明PerPilot能够以最少的用户干预有效处理个性化任务，并且随着使用次数的增加性能逐步提升。

Conclusion: 个性化感知推理对于下一代移动代理至关重要，PerPilot框架为解决个性化指令处理问题提供了有效解决方案。

Abstract: Vision language model (VLM)-based mobile agents show great potential for
assisting users in performing instruction-driven tasks. However, these agents
typically struggle with personalized instructions -- those containing
ambiguous, user-specific context -- a challenge that has been largely
overlooked in previous research. In this paper, we define personalized
instructions and introduce PerInstruct, a novel human-annotated dataset
covering diverse personalized instructions across various mobile scenarios.
Furthermore, given the limited personalization capabilities of existing mobile
agents, we propose PerPilot, a plug-and-play framework powered by large
language models (LLMs) that enables mobile agents to autonomously perceive,
understand, and execute personalized user instructions. PerPilot identifies
personalized elements and autonomously completes instructions via two
complementary approaches: memory-based retrieval and reasoning-based
exploration. Experimental results demonstrate that PerPilot effectively handles
personalized tasks with minimal user intervention and progressively improves
its performance with continued use, underscoring the importance of
personalization-aware reasoning for next-generation mobile agents. The dataset
and code are available at: https://github.com/xinwang-nwpu/PerPilot

</details>


### [44] [Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization](https://arxiv.org/abs/2508.18091)
*Mohammad J. Abdel-Rahman,Yasmeen Alslman,Dania Refai,Amro Saleh,Malik A. Abu Loha,Mohammad Yahya Hamed*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在数学编程决策问题中的能力，通过文献综述和实验分析发现LLMs在自然语言解析和符号表示方面有进展，但在准确性、可扩展性和可解释性方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在理解和解决数学编程优化问题方面的能力，为自动化决策支持系统提供理论基础和实践指导。

Method: 采用系统性文献综述和元分析方法，结合新构建的数据集，使用三种提示策略（专家角色扮演、思维链、自一致性）评估最先进LLMs的性能。

Result: 结果显示LLMs在自然语言解析和符号表示方面表现出色，但在准确性、可扩展性和可解释性方面存在显著局限性。

Conclusion: 提出了未来研究方向，包括结构化数据集、领域特定微调、混合神经符号方法、模块化多智能体架构和动态检索链，为提升LLMs在数学编程中的能力提供了结构化路线图。

Abstract: This paper investigates the capabilities of large language models (LLMs) in
formulating and solving decision-making problems using mathematical
programming. We first conduct a systematic review and meta-analysis of recent
literature to assess how well LLMs understand, structure, and solve
optimization problems across domains. The analysis is guided by critical review
questions focusing on learning approaches, dataset designs, evaluation metrics,
and prompting strategies. Our systematic evidence is complemented by targeted
experiments designed to evaluate the performance of state-of-the-art LLMs in
automatically generating optimization models for problems in computer networks.
Using a newly constructed dataset, we apply three prompting strategies:
Act-as-expert, chain-of-thought, and self-consistency, and evaluate the
obtained outputs based on optimality gap, token-level F1 score, and compilation
accuracy. Results show promising progress in LLMs' ability to parse natural
language and represent symbolic formulations, but also reveal key limitations
in accuracy, scalability, and interpretability. These empirical gaps motivate
several future research directions, including structured datasets,
domain-specific fine-tuning, hybrid neuro-symbolic approaches, modular
multi-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper
contributes a structured roadmap for advancing LLM capabilities in mathematical
programming.

</details>


### [45] [The AI Data Scientist](https://arxiv.org/abs/2508.18113)
*Farkhad Akimov,Munachiso Samuel Nwadike,Zangir Iklassov,Martin Takáč*

Main category: cs.AI

TL;DR: AI数据科学家是一个基于大语言模型的自主代理系统，能够在几分钟内完成从数据上传到生成可操作见解的端到端数据分析流程，大幅提升传统数据科学工作效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据科学工作流程耗时较长、需要专业知识的问题，让决策者能够快速获得基于数据的可操作见解，弥合证据与行动之间的鸿沟。

Method: 采用基于科学假设原则的多智能体系统架构，由专门化的LLM子代理组成，分别负责数据清洗、统计检验、验证和自然语言沟通等任务，各子代理能够自主编写代码、进行因果推理并识别需要补充的数据。

Result: 系统能够在几分钟内完成原本需要数天或数周的传统数据科学工作，实现了快速、严谨且易于理解的数据分析结果输出。

Conclusion: AI数据科学家代表了一种新型的人机交互方式，使深度数据科学既易于获取又具有可操作性，为决策支持提供了革命性的工具。

Abstract: Imagine decision-makers uploading data and, within minutes, receiving clear,
actionable insights delivered straight to their fingertips. That is the promise
of the AI Data Scientist, an autonomous Agent powered by large language models
(LLMs) that closes the gap between evidence and action. Rather than simply
writing code or responding to prompts, it reasons through questions, tests
ideas, and delivers end-to-end insights at a pace far beyond traditional
workflows. Guided by the scientific tenet of the hypothesis, this Agent
uncovers explanatory patterns in data, evaluates their statistical
significance, and uses them to inform predictive modeling. It then translates
these results into recommendations that are both rigorous and accessible. At
the core of the AI Data Scientist is a team of specialized LLM Subagents, each
responsible for a distinct task such as data cleaning, statistical testing,
validation, and plain-language communication. These Subagents write their own
code, reason about causality, and identify when additional data is needed to
support sound conclusions. Together, they achieve in minutes what might
otherwise take days or weeks, enabling a new kind of interaction that makes
deep data science both accessible and actionable.

</details>


### [46] [SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models](https://arxiv.org/abs/2508.18179)
*Zhenwei Tang,Difan Jiao,Blair Yang,Ashton Anderson*

Main category: cs.AI

TL;DR: SEAM是一个新的基准测试，通过四种领域中的语义等价文本和视觉表示来评估视觉语言模型的跨模态一致性推理能力，发现21个当代模型存在系统性模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型评估方法存在模态比较被任务差异和不对称信息混淆的问题，需要一种能够严格比较文本符号推理和视觉空间推理能力的基准测试。

Method: 提出SEAM基准测试，使用四种具有标准化文本和视觉符号系统的领域，通过不同的符号系统跨模态配对语义等价输入，避免了OCR式的图像-文本配对问题。

Result: 在21个当代模型中发现系统性模态不平衡：视觉性能普遍落后于语言性能，尽管问题包含语义等价信息；跨模态一致性相对较低。错误分析显示主要驱动因素是领域符号标记化导致的文本感知失败和引发幻觉的视觉感知失败。

Conclusion: SEAM建立了一个受控的语义等价环境，可用于测量和改进模态无关的推理能力，研究结果对视觉变换具有鲁棒性。

Abstract: Evaluating whether vision-language models (VLMs) reason consistently across
representations is challenging because modality comparisons are typically
confounded by task differences and asymmetric information. We introduce SEAM, a
benchmark that pairs semantically equivalent inputs across four domains that
have existing standardized textual and visual notations. By employing distinct
notation systems across modalities, in contrast to OCR-based image-text
pairing, SEAM provides a rigorous comparative assessment of the
textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21
contemporary models, we observe systematic modality imbalance: vision
frequently lags language in overall performance, despite the problems
containing semantically equivalent information, and cross-modal agreement is
relatively low. Our error analysis reveals two main drivers: textual perception
failures from tokenization in domain notation and visual perception failures
that induce hallucinations. We also show that our results are largely robust to
visual transformations. SEAM establishes a controlled, semantically equivalent
setting for measuring and improving modality-agnostic reasoning.

</details>


### [47] [Unraveling the cognitive patterns of Large Language Models through module communities](https://arxiv.org/abs/2508.18192)
*Kushal Raj Bhandari,Pin-Yu Chen,Jianxi Gao*

Main category: cs.AI

TL;DR: 这篇论文提出了一种网络基于的框架，通过结合认知科学原理来探索大语言模型的出现性认知机制，发现LLM的技能分布部分镜像了鸟类和小型哺乳动物大脑的分布式认知组织特征。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在科学、工程和社会中广泛应用，但其内部机制被隐藏在数十亿参数和复杂结构中，很难理解其内部架构和认知过程。需要开发新方法来探索LLM的出现性认知特征。

Method: 采用理解生物认知的方法，开发了一种网络基于的框架，该框架将认知技能、LLM架构和数据集连接起来。通过分析模块社区中的技能分布来研究模型的认知组织特征。

Result: 研究发现LLM并不严格平行于生物系统的聚焦专门化，而是展现出独特的模块社区，其出现性技能模式部分镜像了鸟类和小型哺乳动物大脑的分布式但相互联系的认知组织。数值结果显示LLM与生物系统的关键差异在于：技能获取大大受益于动态的跨区域交互和神经可塑性。

Conclusion: 通过整合认知科学原理与机器学习，该框架为LLM的可解释性提供了新见解，并建议有效的微调策略应利用分布式学习动态而非固化的模块干预。

Abstract: Large Language Models (LLMs) have reshaped our world with significant
advancements in science, engineering, and society through applications ranging
from scientific discoveries and medical diagnostics to Chatbots. Despite their
ubiquity and utility, the underlying mechanisms of LLM remain concealed within
billions of parameters and complex structures, making their inner architecture
and cognitive processes challenging to comprehend. We address this gap by
adopting approaches to understanding emerging cognition in biology and
developing a network-based framework that links cognitive skills, LLM
architectures, and datasets, ushering in a paradigm shift in foundation model
analysis. The skill distribution in the module communities demonstrates that
while LLMs do not strictly parallel the focalized specialization observed in
specific biological systems, they exhibit unique communities of modules whose
emergent skill patterns partially mirror the distributed yet interconnected
cognitive organization seen in avian and small mammalian brains. Our numerical
results highlight a key divergence from biological systems to LLMs, where skill
acquisition benefits substantially from dynamic, cross-regional interactions
and neural plasticity. By integrating cognitive science principles with machine
learning, our framework provides new insights into LLM interpretability and
suggests that effective fine-tuning strategies should leverage distributed
learning dynamics rather than rigid modular interventions.

</details>


### [48] [Disentangling the Factors of Convergence between Brains and Computer Vision Models](https://arxiv.org/abs/2508.18226)
*Joséphine Raugel,Marc Szafraniec,Huy V. Vo,Camille Couprie,Patrick Labatut,Piotr Bojanowski,Valentin Wyart,Jean-Rémi King*

Main category: cs.AI

TL;DR: 研究发现AI模型大小、训练量和图像类型三个因素独立且交互地影响模型与人类大脑表征的相似性，最大的DINOv3模型使用人类中心图像训练达到最高大脑相似度，且这种发展遵循特定的时间顺序。


<details>
  <summary>Details</summary>
Motivation: 理解AI模型在自然图像训练中为何会发展出类似人类大脑表征的因素，以及模型、训练和数据如何独立导致神经网络发展出大脑样表征。

Method: 训练一系列自监督视觉变换器(DINOv3)，系统性地改变模型大小、训练量和图像类型，比较其图像表征与fMRI和MEG记录的人类大脑表征，使用三种互补指标评估大脑-模型相似性。

Result: 所有三个因素独立且交互地影响大脑相似性指标，最大的DINOv3模型使用最多人类中心图像训练达到最高大脑相似度；模型训练过程中首先与感觉皮层的早期表征对齐，需要更多训练才能与大脑晚期和前额叶表征对齐；这种发展轨迹与人类皮层的结构和功能特性相关。

Conclusion: 这些发现揭示了架构和经验在塑造人工神经网络如何像人类一样看待世界方面的相互作用，为理解人类大脑如何表征视觉世界提供了有前景的框架。

Abstract: Many AI models trained on natural images develop representations that
resemble those of the human brain. However, the factors that drive this
brain-model similarity remain poorly understood. To disentangle how the model,
training and data independently lead a neural network to develop brain-like
representations, we trained a family of self-supervised vision transformers
(DINOv3) that systematically varied these different factors. We compare their
representations of images to those of the human brain recorded with both fMRI
and MEG, providing high resolution in spatial and temporal analyses. We assess
the brain-model similarity with three complementary metrics focusing on overall
representational similarity, topographical organization, and temporal dynamics.
We show that all three factors - model size, training amount, and image type -
independently and interactively impact each of these brain similarity metrics.
In particular, the largest DINOv3 models trained with the most human-centric
images reach the highest brain-similarity. This emergence of brain-like
representations in AI models follows a specific chronology during training:
models first align with the early representations of the sensory cortices, and
only align with the late and prefrontal representations of the brain with
considerably more training. Finally, this developmental trajectory is indexed
by both structural and functional properties of the human cortex: the
representations that are acquired last by the models specifically align with
the cortical areas with the largest developmental expansion, thickness, least
myelination, and slowest timescales. Overall, these findings disentangle the
interplay between architecture and experience in shaping how artificial neural
networks come to see the world as humans do, thus offering a promising
framework to understand how the human brain comes to represent its visual
world.

</details>


### [49] [Efficient Computation of Blackwell Optimal Policies using Rational Functions](https://arxiv.org/abs/2508.18252)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: 本文提出了计算Blackwell最优策略的新算法，通过有理函数排序替代数值计算，为确定性MDP提供了首个强多项式时间算法，为一般MDP提供了首次次指数时间算法。


<details>
  <summary>Details</summary>
Motivation: 传统MDP最优性准则存在局限：折扣最优性过于关注短期回报，平均最优性需要强结构假设。Blackwell最优性虽然理论上更优越，但现有计算方法计算成本高或难以实现。

Method: 使用有理函数在1附近的排序方法，将最先进算法中的数值计算替换为有理函数的符号操作，从而得到与位复杂度无关的边界。对确定性和一般MDP分别设计算法。

Result: 为确定性MDP提供了首个强多项式时间算法，为一般MDP提供了首次次指数时间算法。将多种策略迭代算法推广到Blackwell准则，扩展了已知最优上界。

Conclusion: 提出的符号计算方法有效解决了Blackwell最优策略的计算难题，在计算复杂度和理论边界方面取得了显著进展，为MDP最优性准则的实际应用提供了新途径。

Abstract: Markov Decision Problems (MDPs) provide a foundational framework for
modelling sequential decision-making across diverse domains, guided by
optimality criteria such as discounted and average rewards. However, these
criteria have inherent limitations: discounted optimality may overly prioritise
short-term rewards, while average optimality relies on strong structural
assumptions. Blackwell optimality addresses these challenges, offering a robust
and comprehensive criterion that ensures optimality under both discounted and
average reward frameworks. Despite its theoretical appeal, existing algorithms
for computing Blackwell Optimal (BO) policies are computationally expensive or
hard to implement.
  In this paper we describe procedures for computing BO policies using an
ordering of rational functions in the vicinity of $1$. We adapt
state-of-the-art algorithms for deterministic and general MDPs, replacing
numerical evaluations with symbolic operations on rational functions to derive
bounds independent of bit complexity. For deterministic MDPs, we give the first
strongly polynomial-time algorithms for computing BO policies, and for general
MDPs we obtain the first subexponential-time algorithm. We further generalise
several policy iteration algorithms, extending the best known upper bounds from
the discounted to the Blackwell criterion.

</details>


### [50] [Hermes 4 Technical Report](https://arxiv.org/abs/2508.18255)
*Ryan Teknium,Roger Jin,Jai Suphavadeeprasit,Dakota Mahan,Jeffrey Quesnelle,Joe Li,Chen Guang,Shannon Sands,Karan Malhotra*

Main category: cs.AI

TL;DR: Hermes 4是一个混合推理模型家族，结合了结构化多轮推理和广泛的指令跟随能力，在数学推理、编程、知识、理解和对齐基准测试中表现优异，所有模型权重均已开源。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前模型在结构化多轮推理和广泛指令跟随能力方面的局限性，开发一个能够同时具备这两种能力的混合推理模型。

Method: 采用混合推理方法，结合结构化多轮推理和指令跟随技术，通过大规模数据整理、合成、训练和评估流程来构建模型。

Result: 在数学推理、编程、知识、理解和对齐等多个基准测试中取得了全面的优异性能，并通过定量和定性分析验证了模型效果。

Conclusion: Hermes 4成功实现了结构化推理与指令跟随能力的有效结合，为开放研究社区提供了高质量的模型权重，推动了混合推理模型的发展。

Abstract: We present Hermes 4, a family of hybrid reasoning models that combine
structured, multi-turn reasoning with broad instruction-following ability. We
describe the challenges encountered during data curation, synthesis, training,
and evaluation, and outline the solutions employed to address these challenges
at scale. We comprehensively evaluate across mathematical reasoning, coding,
knowledge, comprehension, and alignment benchmarks, and we report both
quantitative performance and qualitative behavioral analysis. To support open
research, all model weights are published publicly at
https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [51] [Two-Level Priority Coding for Resilience to Arbitrary Blockage Patterns](https://arxiv.org/abs/2508.16899)
*Mine Gokce Dogan,Abhiram Kadiyala,Jaimin Shah,Martina Cardone,Christina Fragouli*

Main category: cs.IT

TL;DR: 本文提出了一种多级分集编码方案，用于军事等关键任务场景中的超可靠低延迟通信，通过控制接收信息并提供不同优先级的可靠性保证，同时保持低复杂度设计。


<details>
  <summary>Details</summary>
Motivation: 解决军事应用中由于移动性、干扰或敌对攻击导致的链路阻塞问题，确保延迟敏感传输的可靠性，特别是在非对称链路阻塞的挑战性环境下。

Method: 采用多级分集编码（MDC）方案，通过叠加编码和网络编码相结合的方式，在三边不相交路径上实现两个优先级级别的通信，并将MDC设计问题等效为广播网络编码设计问题。

Result: 完整刻画了三路径下两个优先级级别的容量区域，证明叠加编码在一般情况下可达到容量区域，而网络编码仅在特定角落情况下需要。同时给出了任意数量路径下简单叠加编码方案达到容量的充分条件。

Conclusion: 该研究为设计高性能MDC方案提供了统一分析框架，证明了在大多数情况下简单的叠加编码方案就足够，只有在极少数情况下才需要更复杂的网络编码技术。

Abstract: Ultra-reliable low-latency communication is essential in mission-critical
settings, including military applications, where persistent and asymmetric link
blockages caused by mobility, jamming, or adversarial attacks can disrupt
delay-sensitive transmissions. This paper addresses this challenge by deploying
a multilevel diversity coding (MDC) scheme that controls the received
information, offers distinct reliability guarantees based on the priority of
data streams, and maintains low design and operational complexity as the number
of network paths increases. For two priority levels over three edge-disjoint
paths, the complete capacity region is characterized, showing that
superposition coding achieves the region in general, whereas network coding is
required only in a specific corner case. Moreover, sufficient conditions under
which a simple superposition coding scheme achieves the capacity for an
arbitrary number of paths are identified. To prove these results and provide a
unified analytical framework, the problem of designing high-performing MDC
schemes is shown to be equivalent to the problem of designing high-performing
encoding schemes over a class of broadcast networks, referred to as combination
networks in the literature.

</details>


### [52] [Polarization-Aware DoA Detection Relying on a Single Rydberg Atomic Receiver](https://arxiv.org/abs/2508.17179)
*Yuanbin Chen,Chau Yuen,Darmindra Arumugam,Chong Meng Samson See,Mérouane Debbah,Lajos Hanzo*

Main category: cs.IT

TL;DR: 基于瑞德伯原子汽气单元的向量敏感性，通过电磁感应透明谱技术同时提取电场和磁场信息，实现了量子增强的自然光偏振方向检测方案


<details>
  <summary>Details</summary>
Motivation: 利用瑞德伯原子收发器的向量敏感性，同时量度电磁波的电场和磁场分量，提高角度分辨率，避免传统方法需要空间多样性或相位参考

Method: 在静态磁场偏置下，通过两次连续的电磁感应透明谱测量：先在电磁极子过渡上提取电场偏振角，然后在磁极子共振上解码磁场方向

Result: 推导出量子资讯矩阵和量子克拉美罗界，通过模拟验证了方案有效性并确定了最优工作区域

Conclusion: 单个瑞德伯原子汽气单元可以在适当的偏振和磁场配置下，实现超过0.1度的角度分辨率，为量子增强的方向检测提供了新方法

Abstract: A polarization-aware direction-of-arrival (DoA) detection scheme is conceived
that leverages the intrinsic vector sensitivity of a single Rydberg atomic
vapor cell to achieve quantum-enhanced angle resolution. Our core idea lies in
the fact that the vector nature of an electromagnetic wave is uniquely
determined by its orthogonal electric and magnetic field components, both of
which can be retrieved by a single Rydberg atomic receiver via
electromagnetically induced transparency (EIT)-based spectroscopy. To be
specific, in the presence of a static magnetic bias field that defines a stable
quantization axis, a pair of sequential EIT measurements is carried out in the
same vapor cell. Firstly, the electric-field polarization angle is extracted
from the Zeeman-resolved EIT spectrum associated with an electric-dipole
transition driven by the radio frequency (RF) field. Within the same
experimental cycle, the RF field is then retuned to a magnetic-dipole
resonance, producing Zeeman-resolved EIT peaks for decoding the RF
magnetic-field orientation. This scheme exhibits a dual yet independent
sensitivity on both angles, allowing for precise DoA reconstruction without the
need for spatial diversity or phase referencing. Building on this foundation,
we derive the quantum Fisher-information matrix (QFIM) and obtain a closed-form
quantum Cram\'{e}r-Rao bound (QCRB) for the joint estimation of polarization
and orientation angles. Finally, simulation results spanning various quantum
parameters validate the proposed approach and identify optimal operating
regimes. With appropriately chosen polarization and magnetic-field geometries,
a single vapor cell is expected to achieve sub-0.1$^\circ$ angle resolution at
moderate RF-field driving strengths.

</details>


### [53] [Blind Deconvolution of Nonstationary Graph Signals over Shift-Invariant Channels](https://arxiv.org/abs/2508.17210)
*Ali Zare,Yao Shi,Qiyu Sun*

Main category: cs.IT

TL;DR: 本文研究非平稳图信号的盲反卷积问题，从含噪观测中恢复通过未知移不变通道传输的信号，假设已知原始图信号的协方差结构。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中图信号通过未知通道传输后的盲反卷积问题，特别是在只有含噪观测和信号统计特性知识的情况下恢复原始信号。

Method: 提出基于图信号协方差结构的盲反卷积方法，利用信号的非平稳特性和通道的移不变性质，通过数值实验验证方法有效性。

Result: 在法国布雷斯特地区温度数据集上的数值实验证明了所提信道估计和盲反卷积方法的有效性。

Conclusion: 该方法能够成功地从含噪观测中恢复通过未知移不变通道传输的非平稳图信号，为实际图信号处理应用提供了有效解决方案。

Abstract: In this paper, we investigate blind deconvolution of nonstationary graph
signals from noisy observations, transmitted through an unknown shift-invariant
channel. The deconvolution process assumes that the observer has access to the
covariance structure of the original graph signals. To evaluate the
effectiveness of our channel estimation and blind deconvolution method, we
conduct numerical experiments using a temperature dataset in the Brest region
of France.

</details>


### [54] [Stochastic Information Geometry: Characterization of Fréchet Means of Gaussian Fields in Poisson Networks](https://arxiv.org/abs/2508.17382)
*Gourab Ghatak*

Main category: cs.IT

TL;DR: 提出了一个统一框架，将随机几何与信息几何相结合，用于空间网络中的分布式推理、语义通信和探索，在Fisher-Rao和2-Wasserstein几何下研究高斯分布场的估计和聚合问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献尚未探索将随机几何与信息几何相结合的方向，需要为具有统计异质性的分布式系统提供几何感知的推理、语义通信和探索的数学基础。

Method: 推导经验Fréchet均值的非渐近集中界和Palm偏差，量化空间随机性引起的几何不确定性。开发几何感知聚合方法、语义压缩协议和Fréchet-UCB算法。

Result: 理论预测在无线传感器网络、语义压缩任务和赌博机环境中得到验证，显示出可扩展性、鲁棒性和改进的决策能力。

Conclusion: 该框架为具有统计异质性的分布式系统中的几何感知推理、语义通信和探索提供了原则性数学基础，填补了现有文献的空白。

Abstract: We develop a unified framework for distributed inference, semantic
communication, and exploration in spatial networks by integrating stochastic
geometry with information geometry - a direction that has not been explored in
prior literature. Specifically, we study the problem of estimating and
aggregating a field of Gaussian distributions indexed by a spatial Poisson
point process (PPP), under both the Fisher--Rao and 2-Wasserstein geometries.
We derive non-asymptotic concentration bounds and Palm deviations for the
empirical Fr\'echet mean, thereby quantifying the geometric uncertainty induced
by spatial randomness. Building on these results, we demonstrate applications
to wireless sensor networks, where our framework provides geometry-aware
aggregation methods that downweight unreliable sensors and rigorously
characterize estimation error under random deployment. Further, we extend our
theory to semantic communications, proposing compression protocols that
guarantee semantic fidelity via distortion bounds on Fr\'echet means under PPP
sampling. Finally, we introduce the \texttt{Fr\'echet-UCB} algorithm for
multi-armed bandit problems with heteroscedastic Gaussian rewards. This
algorithm combines upper confidence bounds with a geometry-aware penalty
reflecting deviation from the evolving Fr\'echet mean, and we derive regret
bounds that exploit geometric structure. Simulations validate the theoretical
predictions across wireless sensor networks, semantic compression tasks, and
bandit environments, highlighting scalability, robustness, and improved
decision-making. Our results provide a principled mathematical foundation for
geometry-aware inference, semantic communication, and exploration in
distributed systems with statistical heterogeneity.

</details>


### [55] [Analog Secure Distributed Matrix Multiplication](https://arxiv.org/abs/2508.17479)
*Okko Makkonen,Camilla Hollanti*

Main category: cs.IT

TL;DR: 基于统一根的多项式插值方法，提出了数值稳定、互信息泄漏小的复数域分布式矩阵乘法方案，并通过复化技术扩展到实数域


<details>
  <summary>Details</summary>
Motivation: 解决分布式矩阵乘法中的数值稳定性和信息安全泄漏问题

Method: 利用统一根进行多项式插值，并通过复化技术将实数矩阵编码为更小的复数矩阵

Result: 设计的方案具有良好的数值稳定性和小的互信息泄漏，实数域方案计算效率更高

Conclusion: 统一根多项式提供了高效的数值稳定分布式矩阵乘法方案，复化技术扩展了其在实数域的应用

Abstract: In this paper, we present secure distributed matrix multiplication (SDMM)
schemes over the complex numbers with good numerical stability and small mutual
information leakage by utilizing polynomial interpolation with roots of unity.
Furthermore, we give constructions utilizing the real numbers by first encoding
the real matrices to smaller complex matrices using a technique we call
complexification. These schemes over the real numbers enjoy many of the
benefits of the schemes over the complex numbers, including good numerical
stability, but are computationally more efficient. To analyze the numerical
stability and the mutual information leakage, we give some bounds on the
condition numbers of Vandermonde matrices whose evaluation points are roots of
unity.

</details>


### [56] [Average Achievable Rate Analysis of Cell-Free Massive MIMO in the Finite Blocklength Regime with Imperfect CSI](https://arxiv.org/abs/2508.17615)
*Kai Che,Feng Ye,Jiamin Li,Pengcheng Zhu,Dongming Wang*

Main category: cs.IT

TL;DR: 这篇论文提出了一种分析框架，用于分析在有限块长度领域下不完整通道状态信息对塑性大规模MIMO系统平均可达速率的影响，并证明了CF-mMIMO架构可以有效减轻不完整CSI带来的性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有研究对有限块长度领域下不完整通道状态信息对平均可达速率的影响存在研究空白，而获取完美CSI在CF-mMIMO系统中面临重大挑战。

Method: 提出一种新的分析框架，提供了在拉普拉斯域中不完整CSI下平均可达速率的闭式表达式，并分析地证明了通道分散和预期通道容量可以通过大规模衰落分量的拉普拉斯变换显式表达。

Result: 数值模拟确认了推导表达式与Monte Carlo模拟结果高度符合，验证了其准确性。理论分析显示虽然不完整CSI会降低性能，但CF-mMIMO架构的本质特性可以有效减轻这种损失。

Conclusion: 该研究为分析CF-mMIMO系统在uRLLC约束下的性能提供了重要的理论基础，并证明了系统架构对不完整CSI的强镇性。

Abstract: Acquiring perfect channel state information (CSI) introduces substantial
challenges in cell-free massive MIMO (CF-mMIMO) systems, primarily due to the
large dimensionality of channel parameters, especially under ultra-reliable
low-latency communication (uRLLC) constraints. Furthermore, the impact of
imperfect CSI on the average achievable rate within the finite blocklength
regime remains largely unexplored. Motivated by this gap, this paper proposes a
novel analytical framework that provides a closed-form expression for the
average achievable rate with imperfect CSI in the Laplace domain. We
demonstrate analytically that both the channel dispersion and the expected
channel capacity can be expressed explicitly in terms of the Laplace transform
of the large-scale fading component. Numerical simulations confirm that the
derived expressions match closely with Monte Carlo simulations, verifying their
accuracy. Furthermore, we theoretically show that although imperfect CSI
degrades performance in the finite blocklength regime, the inherent
characteristics of CF-mMIMO architecture effectively mitigates this loss.

</details>


### [57] [Two-Timescale Learning for Pilot-Free ISAC Systems](https://arxiv.org/abs/2508.17749)
*Jian Xiao,Ji Wang,Qimei Cui,Lihua Li,Xingwang Li,Yingzhuang Liu,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 一种基于Transformer的无导频集成感知通信系统接收机结构，通过两时度注意力机制实现高效的空中接口质量


<details>
  <summary>Details</summary>
Motivation: 传统的基于导频的集成感知通信系统存在开销大、效率低的问题，需要一种无导频的高效解码方案

Method: 提出T3former模型，利用Transformer架构在两个时间尺度上处理PMCW-NOMA信号：细粒度注意力捕捉快时间尺度局部特征，粗粒度注意力聚合慢时间尺度的全局空时依赖关系

Result: T3former显著超过传统成功干涉取消接收机，实现更低的比特错误率和更高的有效向前通量，接近无导频系统的理论最大容量

Conclusion: 该研究为集成感知通信系统提供了一种高效的无导频解码方案，通过深度学习技术有效解决了传统方法的错误传播问题

Abstract: A pilot-free integrated sensing and communication (ISAC) system is
investigated, in which phase-modulated continuous wave (PMCW) and
non-orthogonal multiple access (NOMA) waveforms are co-designed to achieve
simultaneous target sensing and data transmission. To enhance effective data
throughput (i.e., Goodput) in PMCW-NOMA ISAC systems, we propose a deep
learning-based receiver architecture, termed two-timescale Transformer
(T3former), which leverages a Transformer architecture to perform joint channel
estimation and multi-user signal detection without the need for dedicated pilot
signals. By treating the deterministic structure of the PMCW waveform as an
implicit pilot, the proposed T3former eliminates the overhead associated with
traditional pilot-based methods. The proposed T3former processes the received
PMCW-NOMA signals on two distinct timescales, where a fine-grained attention
mechanism captures local features across the fast-time dimension, while a
coarse-grained mechanism aggregates global spatio-temporal dependencies of the
slow-time dimension. Numerical results demonstrate that the proposed T3former
significantly outperforms traditional successive interference cancellation
(SIC) receivers, which avoids inherent error propagation in SIC. Specifically,
the proposed T3former achieves a substantially lower bit error rate and a
higher Goodput, approaching the theoretical maximum capacity of a pilot-free
system.

</details>


### [58] [Three Families of Projective Binary Linear Codes of at Most Four Weights](https://arxiv.org/abs/2508.18030)
*Tonghui Zhang,Pinhui Ke,Zuling Chang*

Main category: cs.IT

TL;DR: 本文构造了三类最多四个非零权重的二进制线性码，其中两类是投影三权重码，并应用于构造任意奇数s>1的s-和集


<details>
  <summary>Details</summary>
Motivation: 研究具有良好权重特性的二进制线性码，特别是最多四个非零权重的码类，以及它们在构造s-和集方面的应用

Method: 构造了三类二进制线性码，其中两类具有投影三权重特性，另一类最多有四个非零权重

Result: 成功构造了具有特定权重特性的线性码，并利用这些码构造了任意奇数s>1的s-和集

Conclusion: 提出的码构造方法有效，为s-和集的构造提供了新的途径，在编码理论和组合设计领域具有应用价值

Abstract: Three classes of binary linear codes with at most four nonzero weights were
constructed in this paper, in which two of them are projective three-weight
codes. As applications, $s$-sum sets for any odd $ s > 1$ were constructed.

</details>


### [59] [Analysis and Detection of RIS-based Spoofing in Integrated Sensing and Communication (ISAC)](https://arxiv.org/abs/2508.18100)
*Tingyu Shui,Po-Heng Chou,Walid Saad,Mingzhe Chen*

Main category: cs.IT

TL;DR: 本文探讨了6G车联网中ISAC系统的感知安全漏洞，分析了恶意RIS对RSU感知功能的欺骗攻击，提出了基于MDP的攻击优化方法和基于STL的神经符号检测防御框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注ISAC系统的通信安全，而感知安全这一同等重要的问题尚未得到充分探索。本文旨在揭示车联网中ISAC系统感知功能可能面临的欺骗攻击风险。

Method: 1) 分析恶意RIS欺骗RSU感知功能的相位偏移设计和反射元件数量要求；2) 推导欺骗攻击下VU多普勒频移和AoD的估计偏差；3) 从攻击者视角建立MDP优化RIS相位设计；4) 提出基于STL的神经符号攻击检测框架。

Result: 理论分析了恶意RIS欺骗攻击的可行性，推导了感知参数估计偏差，建立了攻击优化模型，并提出了有效的防御检测方法。

Conclusion: ISAC系统的感知安全需要与通信安全同等重视，本文揭示了感知欺骗攻击的风险并提出了相应的检测防御方案，为6G车联网安全提供了重要参考。

Abstract: Integrated sensing and communication (ISAC) is a key feature of
next-generation 6G wireless systems, allowing them to achieve high data rates
and sensing accuracy. While prior research has primarily focused on addressing
communication safety in ISAC systems, the equally critical issue of sensing
safety remains largely under-explored. In this paper, the possibility of
spoofing the sensing function of ISAC in vehicle networks is examined, whereby
a malicious reconfigurable intelligent surface (RIS) is deployed to compromise
the sensing functionality of a roadside unit (RSU). For this scenario, the
requirements on the malicious RIS' phase shifts design and number of reflecting
elements are analyzed. Under such spoofing, the practical estimation bias of
the vehicular user (VU)'s Doppler shift and angle-of-departure (AoD) for an
arbitrary time slot is analytically derived. Moreover, from the attacker's
view, a Markov decision process (MDP) is formulated to optimize the RIS' phase
shifts design. The goal of this MDP is to generate complete and plausible fake
trajectories by incorporating the concept of spatial-temporal consistency. To
defend against this sensing spoofing attack, a signal temporal logic
(STL)-based neuro-symbolic attack detection framework is proposed and shown to
learn interoperable formulas for identifying spoofed trajectories.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Retrieve-and-Verify: A Table Context Selection Framework for Accurate Column Annotations](https://arxiv.org/abs/2508.17203)
*Zhihao Ding,Yongkang Sun,Jieming Shi*

Main category: cs.DB

TL;DR: 提出REVEAL和REVEAL+框架，通过检索-验证机制选择最相关的列上下文，解决宽表中无关列导致的性能下降问题，在多个基准数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将所有列序列化输入预训练语言模型来处理表格数据，但在宽表中会包含大量无关或误导性列，导致性能下降。需要更精细的上下文选择机制来提高列注释任务的准确性。

Method: REVEAL：无监督检索技术选择紧凑且信息丰富的列上下文，平衡语义相关性和多样性；使用角色嵌入和目标-上下文对训练进行上下文感知编码。REVEAL+：设计验证模型直接评估所选上下文质量，将列上下文验证问题形式化为分类任务；开发自上而下的验证推理技术，将高质量上下文子集的搜索空间从指数级降低到二次级。

Result: 在六个基准数据集上的广泛实验表明，该方法 consistently 超越最先进的基线方法。

Conclusion: 提出的检索-验证上下文选择框架能有效解决宽表格中的上下文选择问题，通过精细化的上下文筛选和验证机制显著提升了列注释任务的性能。

Abstract: Tables are a prevalent format for structured data, yet their metadata, such
as semantic types and column relationships, is often incomplete or ambiguous.
Column annotation tasks, including Column Type Annotation (CTA) and Column
Property Annotation (CPA), address this by leveraging table context, which are
critical for data management. Existing methods typically serialize all columns
in a table into pretrained language models to incorporate context, but this
coarse-grained approach often degrades performance in wide tables with many
irrelevant or misleading columns. To address this, we propose a novel
retrieve-and-verify context selection framework for accurate column annotation,
introducing two methods: REVEAL and REVEAL+. In REVEAL, we design an efficient
unsupervised retrieval technique to select compact, informative column contexts
by balancing semantic relevance and diversity, and develop context-aware
encoding techniques with role embeddings and target-context pair training to
effectively differentiate target and context columns. To further improve
performance, in REVEAL+, we design a verification model that refines the
selected context by directly estimating its quality for specific annotation
tasks. To achieve this, we formulate a novel column context verification
problem as a classification task and then develop the verification model.
Moreover, in REVEAL+, we develop a top-down verification inference technique to
ensure efficiency by reducing the search space for high-quality context subsets
from exponential to quadratic. Extensive experiments on six benchmark datasets
demonstrate that our methods consistently outperform state-of-the-art
baselines.

</details>


### [61] [ForeSight: A Predictive-Scheduling Deterministic Database](https://arxiv.org/abs/2508.17375)
*Junfang Huang,Yu Yan,Hongzhi Wang,Yingze Li,Jinghan Lin*

Main category: cs.DB

TL;DR: ForeSight是一个高性能确定性数据库系统，通过轻量级冲突预测和智能调度解决现有确定性数据库的调度不足、高中止率和资源利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有确定性数据库设计无法捕获事务依赖关系，导致调度不足、高中止率和资源利用率低下，需要新的方法来提升系统性能。

Method: 1) 设计关联和积网络预测事务冲突；2) 增强存储引擎集成多版本优化；3) 提出矩阵两遍前向扫描算法进行依赖分析生成冲突感知调度。

Result: 在多个基准测试中，ForeSight在偏斜工作负载下实现高达2倍的吞吐量提升，并在高竞争场景下保持强劲性能。

Conclusion: 预测性调度显著提高了确定性数据库的可扩展性，证明了轻量级冲突预测和智能调度的有效性。

Abstract: Deterministic databases enable scalable replicated systems by executing
transactions in a predetermined order. However, existing designs fail to
capture transaction dependencies, leading to insufficient scheduling, high
abort rates, and poor resource utilization. By addressing these challenges with
lightweight conflict prediction and informed scheduling, we present ForeSight,
a high-performance deterministic database system. Our system has three core
improvements: (1) We design an Association Sum-Product Network to predict
potential transaction conflicts, providing the input for dependency analysis
without pre-obtained read/write sets. (2) We enhance the storage engine to
integrate multi-version-based optimization, improving the execution process and
fallback strategy to boost commit rates and concurrency. (3) We propose a
matrix two-pass forward scan algorithm that performs dependency analysis to
generate conflict-aware schedules, significantly reducing scheduling overhead.
Experimental results on multiple benchmarks show that ForeSight achieves up to
2$\times$ higher throughput on skewed workloads and maintains strong
performance under contention, demonstrating that predictive scheduling
substantially improves deterministic database scalability.

</details>


### [62] [SEFRQO: A Self-Evolving Fine-Tuned RAG-Based Query Optimizer](https://arxiv.org/abs/2508.17556)
*Hanwen Liu,Qihan Zhang,Ryan Marcus,Ibrahim Sabek*

Main category: cs.DB

TL;DR: SEFRQO是一个基于检索增强生成的自进化查询优化器，通过持续学习执行反馈和使用RAG框架来解决传统学习型查询优化器的冷启动问题，在CEB和Stack工作负载上相比PostgreSQL分别减少65.05%和93.57%的查询延迟


<details>
  <summary>Details</summary>
Motivation: 传统学习型查询优化器存在冷启动问题，需要在工作负载变化或模式更改时重新训练，而现有的基于LLM的查询优化器忽略了上下文学习和执行记录作为持续演化的反馈

Method: 采用检索增强生成(RAG)框架，结合监督微调和强化微调来训练LLM生成语法正确且性能高效的查询提示，通过动态构建包含相似查询和历史执行记录的提示来利用LLM的上下文学习能力

Result: 在CEB和Stack工作负载上，相比PostgreSQL分别实现了65.05%和93.57%的查询延迟减少，性能优于最先进的学习型查询优化器

Conclusion: SEFRQO通过自进化范式有效解决了查询优化器的冷启动问题，证明了结合RAG框架和LLM上下文学习能力在查询优化领域的有效性

Abstract: Query optimization is a crucial problem in database systems that has been
studied for decades. Learned query optimizers (LQOs) can improve performance
over time by incorporating feedback; however, they suffer from cold-start
issues and often require retraining when workloads shift or schemas change.
Recent LLM-based query optimizers leverage pre-trained and fine-tuned LLMs to
mitigate these challenges. Nevertheless, they neglect LLMs' in-context learning
and execution records as feedback for continuous evolution. In this paper, we
present SEFRQO, a Self-Evolving Fine-tuned RAG-based Query Optimizer. SEFRQO
mitigates the cold-start problem of LQOs by continuously learning from
execution feedback via a Retrieval-Augmented Generation (RAG) framework. We
employ both supervised fine-tuning and reinforcement fine-tuning to prepare the
LLM to produce syntactically correct and performance-efficient query hints.
Moreover, SEFRQO leverages the LLM's in-context learning capabilities by
dynamically constructing prompts with references to similar queries and the
historical execution record of the same query. This self-evolving paradigm
iteratively optimizes the prompt to minimize query execution latency.
Evaluations show that SEFRQO outperforms state-of-the-art LQOs, achieving up to
65.05% and 93.57% reductions in query latency on the CEB and Stack workloads,
respectively, compared to PostgreSQL.

</details>


### [63] [RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System](https://arxiv.org/abs/2508.17590)
*Zui Chen,Han Li,Xinhao Zhang,Xiaoyu Chen,Chunyin Dong,Yifeng Wang,Xin Cai,Su Zhang,Ziqi Li,Chi Ding,Jinxu Li,Shuai Wang,Dousheng Zhao,Sanhai Gao,Guangyi Liu*

Main category: cs.DB

TL;DR: RubikSQL是一个新颖的NL2SQL系统，通过终身学习框架构建知识库，采用多智能体工作流生成SQL，在KaggleDBQA和BIRD Mini-Dev数据集上达到SOTA性能，并发布了RubikBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决企业级NL2SQL中的关键挑战，如隐含意图和领域特定术语问题，需要同时进行知识库维护和SQL生成。

Method: 将NL2SQL构建为终身学习任务，通过数据库分析、结构化信息提取、智能规则挖掘和CoT增强的SQL分析等技术系统构建和优化知识库，然后使用多智能体工作流利用知识库生成准确SQL。

Result: 在KaggleDBQA和BIRD Mini-Dev数据集上实现了最先进的性能表现。

Conclusion: RubikSQL成功解决了企业级NL2SQL的挑战，发布了专门针对工业场景的RubikBench基准测试，为未来研究提供了宝贵资源。

Abstract: We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

</details>


### [64] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie是一个基于大语言模型的数据库自动化规范化框架，通过双模型自优化架构实现高精度的模式规范化，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 数据库规范化对保持数据完整性至关重要，但传统手动方式耗时且容易出错，需要自动化解决方案。

Method: 采用双模型自优化架构：生成模块根据验证模块的反馈消除异常，直到输出模式满足规范化要求；设计特定任务的零样本提示以提高准确性和成本效率。

Result: 实验结果表明，Miffie能够规范化复杂数据库模式，同时保持高精度。

Conclusion: Miffie框架成功实现了数据库自动规范化，证明了LLM在数据工程任务中的有效性。

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


### [65] [TRIM: Accelerating High-Dimensional Vector Similarity Search with Enhanced Triangle-Inequality-Based Pruning](https://arxiv.org/abs/2508.17828)
*Yitong Song,Pengcheng Zhang,Chao Gao,Bin Yao,Kai Wang,Zongyuan Wu,Lin Qu*

Main category: cs.DB

TL;DR: TRIM是一种改进高维向量相似性搜索的方法，通过优化三角不等式剪枝技术，减少距离计算和磁盘访问，显著提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 传统高维向量相似性搜索方法需要大量数据访问进行距离计算，效率低下。三角不等式剪枝在低维空间有效，但在高维空间由于"距离集中"现象而失效。

Method: 提出TRIM方法，通过(1)优化用于形成三角形的地标向量，(2)根据用户需求可调节地放松三角不等式推导的下界，来增强传统剪枝技术的有效性。

Result: 实验显示TRIM显著提升内存方法性能：图搜索提升90%，量化搜索提升200%，剪枝比达99%；磁盘方法I/O成本降低58%，效率提升102%，同时保持高查询精度。

Conclusion: TRIM是一个通用操作，可无缝集成到各种高维向量相似性搜索方法中，有效解决高维空间剪枝难题，大幅提升搜索效率。

Abstract: High-dimensional vector similarity search (HVSS) is critical for many data
processing and AI applications. However, traditional HVSS methods often require
extensive data access for distance calculations, leading to inefficiencies.
Triangle-inequality-based lower bound pruning is a widely used technique to
reduce the number of data access in low-dimensional spaces but becomes less
effective in high-dimensional settings. This is attributed to the "distance
concentration" phenomenon, where the lower bounds derived from the triangle
inequality become too small to be useful. To address this, we propose TRIM,
which enhances the effectiveness of traditional triangle-inequality-based
pruning in high-dimensional vector similarity search using two key ways: (1)
optimizing landmark vectors used to form the triangles, and (2) relaxing the
lower bounds derived from the triangle inequality, with the relaxation degree
adjustable according to user's needs. TRIM is a versatile operation that can be
seamlessly integrated into both memory-based (e.g., HNSW, IVFPQ) and disk-based
(e.g., DiskANN) HVSS methods, reducing distance calculations and disk access.
Extensive experiments show that TRIM enhances memory-based methods, improving
graph-based search by up to 90% and quantization-based search by up to 200%,
while achieving a pruning ratio of up to 99%. It also reduces I/O costs by up
to 58% and improves efficiency by 102% for disk-based methods, while preserving
high query accuracy.

</details>


### [66] [PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs](https://arxiv.org/abs/2508.17886)
*Hao Duan,Yitong Song,Bin Yao,Anqi Liang*

Main category: cs.DB

TL;DR: PGTuner是一个基于预训练知识和模型迁移技术的近似最近邻搜索图自动调优框架，通过查询性能预测模型和深度强化学习配置推荐，显著提升调优效率


<details>
  <summary>Details</summary>
Motivation: 近似最近邻搜索中，邻近图的性能高度依赖于复杂的构建和查询参数，现有自动调优方法效率低下且结果不理想，需要构建大量图结构且无法处理数据集变化

Method: 提出PGTuner框架，包含预训练的查询性能预测模型避免构建多个图，深度强化学习的参数配置推荐模型，以及分布外检测和深度主动学习技术

Result: 在不同数据集上稳定实现顶级调优效果，调优效率提升高达14.69倍，动态场景下提升14.64倍

Conclusion: PGTuner有效解决了邻近图自动配置调优的效率和效果问题，为近似最近邻搜索提供了高效的参数优化解决方案

Abstract: Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key
areas. Proximity graphs (PGs) are the leading method for ANNS, offering the
best balance between query efficiency and accuracy. However, their performance
heavily depends on various construction and query parameters, which are
difficult to optimize due to their complex inter-dependencies. Given that users
often prioritize specific accuracy levels, efficiently identifying the optimal
PG configurations to meet these targets is essential. Although some studies
have explored automatic configuration tuning for PGs, they are limited by
inefficiencies and suboptimal results. These issues stem from the need to
construct numerous PGs for searching and re-tuning from scratch whenever the
dataset changes, as well as the failure to capture the complex dependencies
between configurations, query performance, and tuning objectives.
  To address these challenges, we propose PGTuner, an efficient framework for
automatic PG configuration tuning leveraging pre-training knowledge and model
transfer techniques. PGTuner improves efficiency through a pre-trained query
performance prediction (QPP) model, eliminating the need to build multiple PGs.
It also features a deep reinforcement learning-based parameter configuration
recommendation (PCR) model to recommend optimal configurations for specific
datasets and accuracy targets. Additionally, PGTuner incorporates
out-of-distribution detection and deep active learning for efficient tuning in
dynamic scenarios and transferring to new datasets. Extensive experiments
demonstrate that PGTuner can stably achieve the top-level tuning effect across
different datasets while significantly improving tuning efficiency by up to
14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner
are available online at https://github.com/hao-duan/PGTuner.

</details>


### [67] [Join Cardinality Estimation with OmniSketches](https://arxiv.org/abs/2508.17931)
*David Justen,Matthias Boehm*

Main category: cs.DB

TL;DR: OmniSketch扩展了概率数据结构，用于多连接基数估计，无需假设均匀性和独立性，在SSB-skew基准测试中显著提升了查询性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于成本的优化器由于多谓词多连接查询中的基数估计不准确，经常产生次优执行计划。现有学习方法存在训练成本高、泛化能力差或集成困难等问题。

Method: 扩展OmniSketch概率数据结构，结合count-min sketches和K-minwise哈希，确保跨表草图互操作性，并提供处理α-无环连接图的算法。

Result: 在SSB-skew测试中，中间结果减少高达1077倍，执行时间减少高达3.19倍；在JOB-light测试中，由于连接图形状不利和外键列唯一值过多，效果有限。

Conclusion: OmniSketch增强的成本优化可以提升估计精度和计划质量，但在某些复杂场景下仍面临挑战，需要进一步改进。

Abstract: Join ordering is a key factor in query performance, yet traditional
cost-based optimizers often produce sub-optimal plans due to inaccurate
cardinality estimates in multi-predicate, multi-join queries. Existing
alternatives such as learning-based optimizers and adaptive query processing
improve accuracy but can suffer from high training costs, poor generalization,
or integration challenges. We present an extension of OmniSketch - a
probabilistic data structure combining count-min sketches and K-minwise hashing
- to enable multi-join cardinality estimation without assuming uniformity and
independence. Our approach introduces the OmniSketch join estimator, ensures
sketch interoperability across tables, and provides an algorithm to process
alpha-acyclic join graphs. Our experiments on SSB-skew and JOB-light show that
OmniSketch-enhanced cost-based optimization can improve estimation accuracy and
plan quality compared to DuckDB. For SSB-skew, we show intermediate result
decreases up to 1,077x and execution time decreases up to 3.19x. For JOB-light,
OmniSketch join cardinality estimation shows occasional individual improvements
but largely suffers from a loss of witnesses due to unfavorable join graph
shapes and large numbers of unique values in foreign key columns.

</details>


### [68] [Views: A Hardware-friendly Graph Database Model For Storing Semantic Information](https://arxiv.org/abs/2508.18123)
*Yanjun Yang,Adrian Wheeldon,Yihan Pan,Alex Serb*

Main category: cs.DB

TL;DR: 提出了一种名为Views的硬件友好型图数据库模型，针对存储容量和计算效率瓶颈进行优化，适用于符号AI和检索增强生成应用


<details>
  <summary>Details</summary>
Motivation: 当前图数据库模型未针对硬件加速进行优化，导致存储容量和计算效率存在瓶颈，限制了在符号人工智能和检索增强生成中的应用潜力

Method: 设计了一种硬件友好的图数据库模型Views，提出了针对图数据高效存储和检索的数据结构与组织方式，并证明了其与传统图表示方法的等价性

Result: 展示了Views模型在语义推理和认知建模方面的符号处理能力，通过实际案例验证了其有效性

Conclusion: Views模型为解决图数据库硬件加速问题提供了有效方案，具有在符号AI和RAG系统中应用的潜力，为未来发展方向提供了展望

Abstract: The graph database (GDB) is an increasingly common storage model for data
involving relationships between entries. Beyond its widespread usage in
database industries, the advantages of GDBs indicate a strong potential in
constructing symbolic artificial intelligences (AIs) and retrieval-augmented
generation (RAG), where knowledge of data inter-relationships takes a critical
role in implementation. However, current GDB models are not optimised for
hardware acceleration, leading to bottlenecks in storage capacity and
computational efficiency. In this paper, we propose a hardware-friendly GDB
model, called Views. We show its data structure and organisation tailored for
efficient storage and retrieval of graph data and demonstrate its equivalence
to represent traditional graph representations. We further demonstrate its
symbolic processing abilities in semantic reasoning and cognitive modelling
with practical examples and provide a short perspective on future developments.

</details>


### [69] [Accelerating Historical K-Core Search in Temporal Graphs](https://arxiv.org/abs/2508.18151)
*Zhuo Ma,Dong Wen,Kaiyu Chen,Yixiang Fang,Xuemin Lin,Wenjie Zhang*

Main category: cs.DB

TL;DR: 提出ECB-forest索引结构，用于高效处理时序图中任意时间窗口的k-core查询，相比现有方法显著降低了索引大小和构建成本。


<details>
  <summary>Details</summary>
Motivation: 现有EF-Index方法存在二次预处理时间和大量冗余存储的问题，无法高效支持任意时间窗口的k-core查询需求。

Method: 设计边中心二元森林结构ECB-forest，通过捕获任意查询顶点随时间变化的k-core信息，将查询转化为森林中的连通分量搜索问题。

Result: 在真实时序图数据集上，新方法索引大小和构建成本显著降低（平均快100倍），同时保持高查询效率。

Conclusion: ECB-forest是处理时序k-core组件搜索问题的高效解决方案，在性能和存储效率方面均优于现有方法。

Abstract: We study the temporal k-core component search (TCCS), which outputs the
k-core containing the query vertex in the snapshot over an arbitrary query time
window in a temporal graph. The problem has been shown to be critical for tasks
such as contact tracing, fault diagnosis, and financial forensics. The
state-of-the-art EF-Index designs a separated forest structure for a set of
carefully selected windows, incurring quadratic preprocessing time and large
redundant storage. Our method introduces the ECB-forest, a compact edge-centric
binary forest that captures k-core of any arbitrary query vertex over time. In
this way, a query can be processed by searching a connected component in the
forest. We develop an efficient algorithm for index construction. Experiments
on real-world temporal graphs show that our method significantly improves the
index size and construction cost (up to 100x faster on average) while
maintaining the high query efficiency.

</details>


### [70] [Lost Data in Electron Microscopy](https://arxiv.org/abs/2508.18217)
*Nina M. Ivanova,Alexey S. Kashin,Valentine P. Ananikov*

Main category: cs.DB

TL;DR: 这项研究估计了电子显微镜领域中死数据的量，发现超过90%的实验数据未在同行审查期刊中使用，仅有2%的图像被公布。


<details>
  <summary>Details</summary>
Motivation: 估计电子显微镜领域中死数据的量，分析实验获得的图像在同行审查科学期刊中的利用程度，揭示数据利用效率低的问题。

Method: 分析核心用户设施中电子显微镜拍摄的图像数量与后续在同行审查期刊中包含的图像数量，评估超过150000张电子显微镜图像的利用情况。

Result: 数据利用效率极低，超过90%的电子显微镜数据在常规仪器运行过程中生成后未被使用，仅约2%的图像（约3500张）在公开出牌物中使用。

Conclusion: 这些结果既显示了显微镜图像优化使用的不足，也指出了存在大量可用于数据科学和AI基础项目研究的死数据池，需要释放这些潜在数据的价值。

Abstract: The goal of this study is to estimate the amount of lost data in electron
microscopy and to analyze the extent to which experimentally acquired images
are utilized in peer-reviewed scientific publications. Analysis of the number
of images taken on electron microscopes at a core user facility and the number
of images subsequently included in peer-reviewed scientific journals revealed
low efficiency of data utilization. More than 90% of electron microscopy data
generated during routine instrument operation remain unused. Of the more than
150000 electron microscopy images evaluated in this study, only approximately
3500 (just over 2%) were made available in publications. Thus, the amount of
lost data in electron microscopy can be estimated as >90% (in terms of data
being recorded but not being published in peer-reviewed literature). On the one
hand, these results highlight a shortcoming in the optimal use of microscopy
images; on the other hand, they indicate the existence of a large pool of
electron microscopy data that can facilitate research in data science and the
development of AI-based projects. The considerations important to unlock the
potential of lost data are discussed in the present article.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [71] [Risk-Averse and Optimistic Advertiser Incentive Compatibility in Auto-bidding](https://arxiv.org/abs/2508.16823)
*Christopher Liaw,Wennan Zhu*

Main category: cs.GT

TL;DR: 本文提出了两种改进的自动抢标激励兼容性概念(RAIC和OAIC)，解决了原有AIC定义过于严格的问题，并证明二价拍卖满足这两种新的激励兼容性条件。


<details>
  <summary>Details</summary>
Motivation: 自动抢标环境下存在多重均衡问题，原有的AIC定义过于严格且不允许广告主对约束报告有序数偏好，导致首价和二价拍卖都不满足AIC。需要提出更精细和松弛的激励兼容性概念。

Method: 提出两种新的概念：风险避嫌自动抢标激励兼容性(RAIC)和乐观自动抢标激励兼容性(OAIC)。RAIC要求真实报告的最差均衡结果不比任何假报告的最差均衡结果差；OAIC要求真实报告的最佳均衡结果不比任何假报告的最佳均衡结果差。

Result: 证明二价拍卖满足RAIC和OAIC条件。在两个广告主采用均匀抢标策略的假设下，二价拍卖仍然满足这两种激励兼容性。

Conclusion: 本文提出的RAIC和OAIC概念为广告主提供了更精细的激励兼容性分析框架，特别是在考虑均衡选择不确定性时。二价拍卖在自动抢标环境中具有良好的激励特性。

Abstract: The rise of auto-bidding has created challenges for ensuring advertiser
incentive compatibility, particularly when advertisers delegate bidding to
agents with high-level constraints. One challenge in defining incentive
compatibility is the multiplicity of equilibria. After advertisers submit
reports, it is unclear what the result will be and one only has knowledge of a
range of possible results. Nevertheless, Alimohammadi et al. proposed a notion
of Auto-bidding Incentive Compatibility (AIC) which serves to highlight that
auctions may not incentivize truthful reporting of constraints. However, their
definition of AIC is very stringent as it requires that the worst-case outcome
of an advertiser's truthful report is at least as good as the best-case outcome
of any of the advertiser's possible deviations. Indeed, they show both
First-Price Auction and Second-Price Auction are not AIC. Moreover, the AIC
definition precludes having ordinal preferences on the possible constraints
that the advertiser can report.
  In this paper, we introduce two refined and relaxed concepts: Risk-Averse
Auto-bidding Incentive Compatibility (RAIC) and Optimistic Auto-bidding
Incentive Compatibility (OAIC). RAIC (OAIC) stipulates that truthful reporting
is preferred if its least (most) favorable equilibrium outcome is no worse than
the least (most) favorable equilibrium outcome from any misreport. This
distinction allows for a clearer modeling of ordinal preferences for
advertisers with differing attitudes towards equilibrium uncertainty. We
demonstrate that SPA satisfies both RAIC and OAIC. Furthermore, we show that
SPA also meets these conditions for two advertisers when they are assumed to
employ uniform bidding. These findings provide new insights into the incentive
properties of SPA in auto-bidding environments, particularly when considering
advertisers' perspectives on equilibrium selection.

</details>


### [72] [Personalized Pricing Through Strategic User Profiling in Social Networks](https://arxiv.org/abs/2508.17111)
*Qinqi Lin,Lingjie Duan,Jianwei Huang*

Main category: cs.GT

TL;DR: 这篇论文通过动态贝叶斯游戏模型分析了用户如何管理社交活动来应对个性化定价，以及卖家如何调整定价策略来促进用户画像。研究发现隐私意识政策可能会导致用户利益受损。


<details>
  <summary>Details</summary>
Motivation: 随着隐私加强技术的出现，用户主动避免数据跟踪，使得卖家转向社交网络跟踪用户画像。需要研究用户如何管理社交活动来应对个性化定价，以及卖家的应对策略。

Method: 构建了一个非对称信息下的动态贝叶斯游戏模型，通过替代应用向后归纳和向前归纳方法，成功地在闭形式中定义了唯一的完美贝叶斯均衡。

Result: 分析显示，随着画像技术的准确性提高，卖家会提高均一价格来激励用户增加社交活动，促进用户画像。但这导致大多数用户在知情同意政策实施后利益受损。

Conclusion: 这一发现表明，最近推进的增强用户隐私意识的监管变革可能会导致用户收益减少的意外后果。

Abstract: Traditional user profiling techniques rely on browsing history or purchase
records to identify users' willingness to pay. This enables sellers to offer
personalized prices to profiled users while charging only a uniform price to
non-profiled users. However, the emergence of privacy-enhancing technologies
has caused users to actively avoid on-site data tracking. Today, major online
sellers have turned to public platforms such as online social networks to
better track users' profiles from their product-related discussions. This paper
presents the first analytical study on how users should best manage their
social activities against potential personalized pricing, and how a seller
should strategically adjust her pricing scheme to facilitate user profiling in
social networks. We formulate a dynamic Bayesian game played between the seller
and users under asymmetric information. The key challenge of analyzing this
game comes from the double couplings between the seller and the users as well
as among the users. Furthermore, the equilibrium analysis needs to ensure
consistency between users' revealed information and the seller's belief under
random user profiling. We address these challenges by alternately applying
backward and forward induction, and successfully characterize the unique
perfect Bayesian equilibrium (PBE) in closed form. Our analysis reveals that as
the accuracy of profiling technology improves, the seller tends to raise the
equilibrium uniform price to motivate users' increased social activities and
facilitate user profiling. However, this results in most users being worse off
after the informed consent policy is imposed to ensure users' awareness of data
access and profiling practices by potential sellers. This finding suggests that
recent regulatory evolution towards enhancing users' privacy awareness may have
unintended consequences of reducing users' payoffs.

</details>


### [73] [Designing Rules to Pick a Rule: Aggregation by Consistency](https://arxiv.org/abs/2508.17177)
*Ratip Emin Berker,Ben Armstrong,Vincent Conitzer,Nihar B. Shah*

Main category: cs.GT

TL;DR: 本文提出了一个规则选择规则(RPR)的新框架，通过最大化输出排名的一致性来自动选择最佳排名聚合方法，无需假设生成模型


<details>
  <summary>Details</summary>
Motivation: 解决排名聚合中不同方法具有不同理想属性但存在不可能性定理的困境，需要一种系统化的方法来选择最适合特定场景的聚合规则

Method: 设计数据驱动的RPR，通过最大化重复数据收集过程中输出排名的一致性来选择最佳聚合规则，并提供基于采样的高效实现

Result: 方法满足一致性相关公理，在可能时选择数据的最大似然估计器，并能显著提高实际应用中的过程一致性

Conclusion: 该工作弥合了排名聚合的公理方法和统计方法之间的重要差距，为原则性规则选择奠定了坚实的理论和计算基础

Abstract: Given a set of items and a set of evaluators who all individually rank them,
how do we aggregate these evaluations into a single societal ranking? Work in
social choice and statistics has produced many aggregation methods for this
problem, each with its desirable properties, but also with its limitations.
Further, existing impossibility results rule out designing a single method that
achieves every property of interest. Faced with this trade-off between
incompatible desiderata, how do we decide which aggregation rule to use, i.e.,
what is a good rule picking rule?
  In this paper, we formally address this question by introducing a novel
framework for rule picking rules (RPRs). We then design a data-driven RPR that
identifies the best aggregation method for each specific setting, without
assuming any generative model. The principle behind our RPR is to pick the rule
which maximizes the consistency of the output ranking if the data collection
process were repeated. We introduce several consistency-related axioms for RPRs
and show that our method satisfies them, including those failed by a wide class
of natural RPRs. While we prove that the algorithmic problem of maximizing
consistency is computationally hard, we provide a sampling-based implementation
of our RPR that is efficient in practice. We run this implementation on known
statistical models and find that, when possible, our method selects the maximum
likelihood estimator of the data. Finally, we show that our RPR can be used in
many real-world settings to gain insights about how the rule currently being
used can be modified or replaced to substantially improve the consistency of
the process.
  Taken together, our work bridges an important gap between the axiomatic and
statistical approaches to rank aggregation, laying a robust theoretical and
computational foundation for principled rule picking.

</details>


### [74] [Decision-Making on Timing and Route Selection: A Game-Theoretic Approach](https://arxiv.org/abs/2508.17206)
*Chenlan Wang,Mingyan Liu*

Main category: cs.GT

TL;DR: 提出Stackelberg博弈模型研究个体在时间和路线选择上的决策，揭示比仅关注时间的早期模型更丰富的子博弈完美均衡和更激烈的竞争


<details>
  <summary>Details</summary>
Motivation: 研究个体如何在迁徙中平衡时间（获得更好领地）和路线选择（群体形成带来的导航、觅食和能量效率优势），以及更长/更难路线带来的捕食风险降低与旅行成本增加的权衡

Method: 使用Stackelberg博弈模型，同时考虑时间决策和路线选择，并纳入个体旅行成本差异

Result: 发现了更丰富的子博弈完美均衡状态，除了"合作"和"竞争"外，还引入了"中立"状态

Conclusion: 该模型不仅适用于鸟类迁徙，还可应用于交通规划、灾难疏散和其他动物移动场景，提供了更全面的决策分析框架

Abstract: We present a Stackelberg game model to investigate how individuals make their
decisions on timing and route selection. Group formation can naturally result
from these decisions, but only when individuals arrive at the same time and
choose the same route. Although motivated by bird migration, our model applies
to scenarios such as traffic planning, disaster evacuation, and other animal
movements. Early arrivals secure better territories, while traveling together
enhances navigation accuracy, foraging efficiency, and energy efficiency.
Longer or more difficult migration routes reduce predation risks but increase
travel costs, such as higher elevations and scarce food resources. Our analysis
reveals a richer set of subgame perfect equilibria (SPEs) and heightened
competition, compared to earlier models focused only on timing. By
incorporating individual differences in travel costs, our model introduces a
"neutrality" state in addition to "cooperation" and "competition."

</details>


### [75] [A Dynamic Approach to Collaborative Document Writing](https://arxiv.org/abs/2508.17489)
*Avital Finanser,Nimrod Talmon*

Main category: cs.GT

TL;DR: 提出了一种协作文本聚合模型，通过代理社区共同创作文档，采用动态机制：代理提议段落并对他人建议进行投票，探索投票机制以实现过程稳定性和社会福祉。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过多代理协作机制来共同创作高质量的文本文档，解决集体写作中的协调和共识达成问题。

Method: 建立代理社区共同创作模型，使用动态投票机制聚合段落提议，采用基于NLP和LLM的代理模拟来测试不同的聚合方法。

Result: 模拟实验显示，该方法能够快速收敛到具有高社会福祉的协作文本，证明了协作文本聚合的可行性。

Conclusion: 该模型为多代理协作文本创作提供了有效的框架，通过适当的投票机制可以实现稳定且高质量的集体写作成果。

Abstract: We introduce a model for collaborative text aggregation in which an agent
community coauthors a document, modeled as an unordered collection of
paragraphs, using a dynamic mechanism: agents propose paragraphs and vote on
those suggested by others. We formalize the setting and explore its
realizations, concentrating on voting mechanisms that aggregate votes into a
single, dynamic document. We focus on two desiderata: the eventual stability of
the process and its expected social welfare. Following an impossibility result,
we describe several aggregation methods and report on agent-based simulations
that utilize natural language processing (NLP) and large-language models (LLMs)
to model agents and their contexts. Using these simulations, we demonstrate
promising results regarding the possibility of rapid convergence to a high
social welfare collaborative text.

</details>


### [76] [Price of Uncertainty for Consensus Games](https://arxiv.org/abs/2508.17557)
*Yunzhe Bai,Alec Sun*

Main category: cs.GT

TL;DR: 本文研究了共识游戏中不确定性对社交成本的影响，证明了在不确定性扰动下价格不确定性的紧确界为Θ(ε²n²)，改进了先前的结果


<details>
  <summary>Details</summary>
Motivation: 现实世界中游戏参与者经常面临观测数据的不确定性，而传统博弈论模型假设信息准确，需要研究不确定性对社交成本的影响

Method: 建立不确定性模型，引入相对幅度1+ε的对抗性扰动到玩家观测成本，分析不确定性价格的理论界限

Result: 证明了共识游戏中不确定性价格的紧确界为Θ(ε²n²)，适用于所有ε=Ω(n^{-1/4})的情况，改进了先前的Ω(ε³n²)下界和O(εn²)上界

Conclusion: 不确定性对共识游戏的社交成本有显著影响，本文提供了更精确的理论界限，深化了对不确定性博弈中性能退化的理解

Abstract: Many game-theoretic models assume that players have access to accurate
information, but uncertainty in observed data is frequently present in
real-world settings. In this paper, we consider a model of uncertainty where
adversarial perturbations of relative magnitude $1+\varepsilon$ are introduced
to players' observed costs. The effect of uncertainty on social cost is denoted
as the price of uncertainty. We prove a tight bound on the price of uncertainty
for consensus games of $\Theta(\varepsilon^2 n^2)$ for all $\varepsilon =
\Omega\mathopen{}\left(n^{-1/4}\right)$. This improves a previous lower bound
of $\Omega(\varepsilon^3 n^2)$ as well as a previous upper bound of
$O(\varepsilon n^2)$.

</details>


### [77] [Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games](https://arxiv.org/abs/2508.17671)
*Sam Ganzfried*

Main category: cs.GT

TL;DR: 本文提出了一种新的对手建模算法，能够在重复博弈中高效收敛到对手的真实策略，解决了现有方法无法保证收敛性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对手建模方法在非完美信息博弈中效果有限，即使面对来自已知先验分布的静态对手，也无法保证模型会收敛到对手的真实策略。

Method: 开发了一种基于序列形式博弈表示的凸最小化问题求解算法，使用投影梯度下降法高效运行。

Result: 该算法能够保证在游戏观察和可能的历史数据基础上，高效收敛到对手的真实策略。

Conclusion: 新算法满足了对手建模的理想属性，为多智能体环境中的对手建模提供了理论保证和实用解决方案。

Abstract: The goal of agents in multi-agent environments is to maximize total reward
against the opposing agents that are encountered. Following a game-theoretic
solution concept, such as Nash equilibrium, may obtain a strong performance in
some settings; however, such approaches fail to capitalize on historical and
observed data from repeated interactions against our opponents. Opponent
modeling algorithms integrate machine learning techniques to exploit suboptimal
opponents utilizing available data; however, the effectiveness of such
approaches in imperfect-information games to date is quite limited. We show
that existing opponent modeling approaches fail to satisfy a simple desirable
property even against static opponents drawn from a known prior distribution;
namely, they do not guarantee that the model approaches the opponent's true
strategy even in the limit as the number of game iterations approaches
infinity. We develop a new algorithm that is able to achieve this property and
runs efficiently by solving a convex minimization problem based on the
sequence-form game representation using projected gradient descent. The
algorithm is guaranteed to efficiently converge to the opponent's true strategy
given observations from gameplay and possibly additional historical data if it
is available.

</details>


### [78] [WOMAC: A Mechanism For Prediction Competitions](https://arxiv.org/abs/2508.17907)
*Siddarth Srinivasan,Tao Lin,Connacher Murphy,Anish Thilagar,Yiling Chen,Ezra Karger*

Main category: cs.GT

TL;DR: WOMAC是一种新的确定性竞赛机制，通过将专家预测与同行专家预测的最佳聚合进行评分，解决了标准竞赛设计中因结果噪声导致的激励不兼容和统计效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 标准竞赛设计存在两个主要问题：1）结果/标签中的噪声使得较弱的竞争者可能偶然获胜；2）赢家通吃的性质激励专家为了增加获胜概率而误报预测，即使这会降低预期得分。现有随机机制虽然能实现激励兼容性，但增加了选择过程中的噪声，缺乏确定性和实际应用性。

Method: 提出WOMAC机制：不将专家与噪声结果直接评分，而是将专家预测与给定噪声结果下同行专家预测的最佳事后聚合进行评分。这种方法减少了噪声影响，提高了统计效率。

Result: 理论分析为WOMAC机制提供了理论基础，向量化实现使其计算高效。在真实世界预测数据集上的实证研究表明，WOMAC相比标准机制能更可靠地预测专家的样本外表现。

Conclusion: WOMAC是一种确定性的竞赛机制，在结果/标签存在显著噪声的任何竞赛中都很有用，它既提高了统计效率，又避免了标准设计的激励不兼容问题，同时保持了确定性和实用性。

Abstract: Competitions are widely used to identify top performers in judgmental
forecasting and machine learning, and the standard competition design ranks
competitors based on their cumulative scores against a set of realized outcomes
or held-out labels. However, this standard design is neither
incentive-compatible nor very statistically efficient. The main culprit is
noise in outcomes/labels that experts are scored against; it allows weaker
competitors to often win by chance, and the winner-take-all nature incentivizes
misreporting that improves win probability even if it decreases expected score.
Attempts to achieve incentive-compatibility rely on randomized mechanisms that
add even more noise in winner selection, but come at the cost of determinism
and practical adoption. To tackle these issues, we introduce a novel
deterministic mechanism: WOMAC (Wisdom of the Most Accurate Crowd). Instead of
scoring experts against noisy outcomes, as is standard, WOMAC scores experts
against the best ex-post aggregate of peer experts' predictions given the noisy
outcomes. WOMAC is also more efficient than the standard competition design in
typical settings. While the increased complexity of WOMAC makes it challenging
to analyze incentives directly, we provide a clear theoretical foundation to
justify the mechanism. We also provide an efficient vectorized implementation
and demonstrate empirically on real-world forecasting datasets that WOMAC is a
more reliable predictor of experts' out-of-sample performance relative to the
standard mechanism. WOMAC is useful in any competition where there is
substantial noise in the outcomes/labels.

</details>


### [79] [Adaptive Learning for Moving Target defence: Enhancing Cybersecurity Strategies](https://arxiv.org/abs/2508.17945)
*Mandar Datar,Yann Dujardin*

Main category: cs.GT

TL;DR: 将移动目标防御建模为部分可观测随机博弈，提出基于阈值结构感知的策略梯度强化学习算法，使攻防双方收敛到纳什均衡


<details>
  <summary>Details</summary>
Motivation: 传统防御策略难以应对不断演变的网络威胁，需要建立动态的攻防博弈模型来提升系统安全

Method: 将MTD建模为部分可观测随机博弈，分析最优策略的阈值结构特性，设计结构感知策略梯度强化学习算法

Result: 证明了攻防双方最优策略具有阈值结构，提出的算法能帮助双方收敛到纳什均衡，提升防御效果

Conclusion: 基于阈值结构感知的强化学习方法能有效提升移动目标防御的适应性和安全性，数值仿真验证了方法的有效性

Abstract: In this work, we model Moving Target Defence (MTD) as a partially observable
stochastic game between an attacker and a defender. The attacker tries to
compromise the system through probing actions, while the defender minimizes the
risk by reimaging the system, balancing between performance cost and security
level. We demonstrate that the optimal strategies for both players follow a
threshold structure. Based on this insight, we propose a structure-aware policy
gradient reinforcement learning algorithm that helps both players converge to
the Nash equilibrium. This approach enhances the defender's ability to adapt
and effectively counter evolving threats, improving the overall security of the
system. Finally, we validate the proposed method through numerical simulations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [Quantum-Inspired DRL Approach with LSTM and OU Noise for Cut Order Planning Optimization](https://arxiv.org/abs/2508.16611)
*Yulison Herry Chrisnanto,Julian Evan Chrisnanto*

Main category: cs.LG

TL;DR: 提出量子启发的深度强化学习框架(QI-DRL)，结合LSTM和Ornstein-Uhlenbeck噪声，在纺织裁剪规划中实现13%的成本节约和稳定的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态启发式和目录估计的方法难以适应动态生产环境，导致解决方案次优和浪费增加，需要新的自适应优化方法。

Method: 量子启发的深度强化学习框架，集成LSTM网络处理序列依赖关系，使用Ornstein-Uhlenbeck噪声促进平滑探索和快速收敛。

Result: 经过1000轮训练，平均奖励0.81(±0.03)，预测损失降至0.15(±0.02)，相比传统方法节省高达13%的布料成本，表现出低变异性和稳定收敛。

Conclusion: 尽管模拟模型做了简化假设，但结果显示了该可扩展自适应框架在提升制造效率方面的潜力，为裁剪规划优化开辟了新途径。

Abstract: Cut order planning (COP) is a critical challenge in the textile industry,
directly impacting fabric utilization and production costs. Conventional
methods based on static heuristics and catalog-based estimations often struggle
to adapt to dynamic production environments, resulting in suboptimal solutions
and increased waste. In response, we propose a novel Quantum-Inspired Deep
Reinforcement Learning (QI-DRL) framework that integrates Long Short-Term
Memory (LSTM) networks with Ornstein-Uhlenbeck noise. This hybrid approach is
designed to explicitly address key research questions regarding the benefits of
quantum-inspired probabilistic representations, the role of LSTM-based memory
in capturing sequential dependencies, and the effectiveness of OU noise in
facilitating smooth exploration and faster convergence. Extensive training over
1000 episodes demonstrates robust performance, with an average reward of 0.81
(-+0.03) and a steady decrease in prediction loss to 0.15 (-+0.02). A
comparative analysis reveals that the proposed approach achieves fabric cost
savings of up to 13% compared to conventional methods. Furthermore, statistical
evaluations indicate low variability and stable convergence. Despite the fact
that the simulation model makes several simplifying assumptions, these
promising results underscore the potential of the scalable and adaptive
framework to enhance manufacturing efficiency and pave the way for future
innovations in COP optimization.

</details>


### [81] [CrystalDiT: A Diffusion Transformer for Crystal Generation](https://arxiv.org/abs/2508.16614)
*Xiaohan Yi,Guikun Xu,Xi Xiao,Zhong Zhang,Liu Liu,Yatao Bian,Peilin Zhao*

Main category: cs.LG

TL;DR: CrystalDiT是一种用于晶体结构生成的扩散变换器，通过统一的Transformer架构处理晶格和原子属性，在MP-20数据集上实现了9.62%的SUN率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 挑战当前架构复杂化的趋势，证明在数据有限的科学领域中，精心设计的简单架构比容易过拟合的复杂替代方案更有效。

Method: 采用统一的Transformer架构，将晶格和原子属性视为单一相互依赖系统，结合基于元素周期表的原子表示和平衡训练策略。

Result: 在MP-20数据集上达到9.62%的SUN率（稳定、独特、新颖），显著优于FlowMM（4.38%）和MatterGen（3.42%），生成63.28%的独特新颖结构。

Conclusion: 架构简单性在材料发现中可能比复杂性更有效，特别是在数据有限的科学领域中，简单但精心设计的架构能够避免过拟合问题。

Abstract: We present CrystalDiT, a diffusion transformer for crystal structure
generation that achieves state-of-the-art performance by challenging the trend
of architectural complexity. Instead of intricate, multi-stream designs,
CrystalDiT employs a unified transformer that imposes a powerful inductive
bias: treating lattice and atomic properties as a single, interdependent
system. Combined with a periodic table-based atomic representation and a
balanced training strategy, our approach achieves 9.62% SUN (Stable, Unique,
Novel) rate on MP-20, substantially outperforming recent methods including
FlowMM (4.38%) and MatterGen (3.42%). Notably, CrystalDiT generates 63.28%
unique and novel structures while maintaining comparable stability rates,
demonstrating that architectural simplicity can be more effective than
complexity for materials discovery. Our results suggest that in data-limited
scientific domains, carefully designed simple architectures outperform
sophisticated alternatives that are prone to overfitting.

</details>


### [82] [Leveraging the Christoffel Function for Outlier Detection in Data Streams](https://arxiv.org/abs/2508.16617)
*Kévin Ducharlet,Louise Travé-Massuyès,Jean-Bernard Lasserre,Marie-Véronique Le Lann,Youssef Miloudi*

Main category: cs.LG

TL;DR: 这篇论文提出了两种新的数据流离群检测方法DyCF和DyCG，基于克里斯托莱函数理论，解决了传统方法参数调整复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 数据流离群检测对于数据质量和故障检测至关重要，但现有方法存在参数调整复杂、无法处理非稳态分布和大数据量的挑战。

Method: DyCF基于克里斯托莱函数和正交多项式理论；DyCG利用克里斯托莱函数的增长特性，完全避免参数调整。两种方法都基于代数框架，支持低维处理和无内存成本的数据历史维护。

Result: 通过综合和实际工业数据流的对比实验，DyCF在执行时间和内存使用方面都超过了经过精心调整的方法；DyCG性能略差但优势在于完全不需要参数调整。

Conclusion: 论文提供的两种新方法有效解决了数据流离群检测中的参数调整问题，DyCF在性能上优称，DyCG在无参数化方面具有显著优势，为实际应用提供了有效解决方案。

Abstract: Outlier detection holds significant importance in the realm of data mining,
particularly with the growing pervasiveness of data acquisition methods. The
ability to identify outliers in data streams is essential for maintaining data
quality and detecting faults. However, dealing with data streams presents
challenges due to the non-stationary nature of distributions and the
ever-increasing data volume. While numerous methods have been proposed to
tackle this challenge, a common drawback is the lack of straightforward
parameterization in many of them. This article introduces two novel methods:
DyCF and DyCG. DyCF leverages the Christoffel function from the theory of
approximation and orthogonal polynomials. Conversely, DyCG capitalizes on the
growth properties of the Christoffel function, eliminating the need for tuning
parameters. Both approaches are firmly rooted in a well-defined algebraic
framework, meeting crucial demands for data stream processing, with a specific
focus on addressing low-dimensional aspects and maintaining data history
without memory cost. A comprehensive comparison between DyCF, DyCG, and
state-of-the-art methods is presented, using both synthetic and real industrial
data streams. The results show that DyCF outperforms fine-tuning methods,
offering superior performance in terms of execution time and memory usage. DyCG
performs less well, but has the considerable advantage of requiring no tuning
at all.

</details>


### [83] [STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction with Future Spatiotemporal Contexts](https://arxiv.org/abs/2508.16620)
*Bangchao Deng,Lianhua Ji,Chunhua Chen,Xin Jing,Ling Ding,Bingqing QU,Pengyang Wang,Dingqi Yang*

Main category: cs.LG

TL;DR: STRelay是一个时空接力框架，通过显式建模未来时空上下文来提升位置预测模型性能，在四个数据集上平均提升3.19%-11.56%


<details>
  <summary>Details</summary>
Motivation: 现有位置预测方法主要依赖历史轨迹数据，但忽略了未来时空上下文的重要性，如用户将要旅行的时间和距离信息对预测下一个位置具有关键线索作用

Method: STRelay以接力方式建模未来时空上下文，与基础位置预测模型的历史表示集成，通过多任务学习同时预测下一个时间间隔、移动距离间隔和位置

Result: 在四个真实轨迹数据集上集成四种先进基础模型，STRelay一致提升预测性能3.19%-11.56%，特别对娱乐相关位置和长途旅行用户群体效果显著

Conclusion: 未来时空上下文对非日常活动（不确定性较高）的性能提升具有补充作用，与擅长建模日常规律模式的基础模型形成互补

Abstract: Next location prediction is a critical task in human mobility modeling,
enabling applications like travel planning and urban mobility management.
Existing methods mainly rely on historical spatiotemporal trajectory data to
train sequence models that directly forecast future locations. However, they
often overlook the importance of the future spatiotemporal contexts, which are
highly informative for the future locations. For example, knowing how much time
and distance a user will travel could serve as a critical clue for predicting
the user's next location. Against this background, we propose \textbf{STRelay},
a universal \textbf{\underline{S}}patio\textbf{\underline{T}}emporal
\textbf{\underline{Relay}}ing framework explicitly modeling the future
spatiotemporal context given a human trajectory, to boost the performance of
different location prediction models. Specifically, STRelay models future
spatiotemporal contexts in a relaying manner, which is subsequently integrated
with the encoded historical representation from a base location prediction
model, enabling multi-task learning by simultaneously predicting the next time
interval, next moving distance interval, and finally the next location. We
evaluate STRelay integrated with four state-of-the-art location prediction base
models on four real-world trajectory datasets. Results demonstrate that STRelay
consistently improves prediction performance across all cases by
3.19\%-11.56\%. Additionally, we find that the future spatiotemporal contexts
are particularly helpful for entertainment-related locations and also for user
groups who prefer traveling longer distances. The performance gain on such
non-daily-routine activities, which often suffer from higher uncertainty, is
indeed complementary to the base location prediction models that often excel at
modeling regular daily routine patterns.

</details>


### [84] [A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction](https://arxiv.org/abs/2508.16623)
*Weilin Ruan,Xilin Dang,Ziyu Zhou,Sisuo Lyu,Yuxuan Liang*

Main category: cs.LG

TL;DR: RAST是一个基于检索增强的通用交通预测框架，通过解耦编码器、时空检索存储和通用预测器，有效解决了复杂时空依赖建模和细粒度预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前时空图神经网络在交通预测中存在两个关键问题：复杂时空依赖的上下文建模能力有限，以及由于异质模式导致的细粒度时空点预测准确性低。

Method: 提出RAST框架，包含三个核心设计：1）解耦编码器和查询生成器捕获分离的时空特征；2）时空检索存储和检索器维护和检索向量化细粒度模式；3）通用骨干预测器灵活适配预训练STGNN或简单MLP预测器。

Result: 在六个真实世界交通网络（包括大规模数据集）上的广泛实验表明，RAST实现了优越的性能，同时保持了计算效率。

Conclusion: RAST框架通过集成检索增强机制与时空建模，有效提升了交通预测的准确性和效率，为解决复杂时空依赖和细粒度预测问题提供了通用解决方案。

Abstract: Traffic prediction is a cornerstone of modern intelligent transportation
systems and a critical task in spatio-temporal forecasting. Although advanced
Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have
achieved significant progress in traffic prediction, two key challenges remain:
(i) limited contextual capacity when modeling complex spatio-temporal
dependencies, and (ii) low predictability at fine-grained spatio-temporal
points due to heterogeneous patterns. Inspired by Retrieval-Augmented
Generation (RAG), we propose RAST, a universal framework that integrates
retrieval-augmented mechanisms with spatio-temporal modeling to address these
challenges. Our framework consists of three key designs: 1) Decoupled Encoder
and Query Generator to capture decoupled spatial and temporal features and
construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval
Store and Retrievers to maintain and retrieve vectorized fine-grained patterns;
and 3) Universal Backbone Predictor that flexibly accommodates pre-trained
STGNNs or simple MLP predictors. Extensive experiments on six real-world
traffic networks, including large-scale datasets, demonstrate that RAST
achieves superior performance while maintaining computational efficiency.

</details>


### [85] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的自适应记忆框架，通过建模记忆循环来优化LLM智能体的记忆能力，包括MoE门控检索、可学习聚合过程和任务特定反思机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的记忆机制由人工预定义，导致高人力成本和次优性能，且忽视了交互场景中的记忆循环效应。

Method: 设计MoE门控函数促进记忆检索，提出可学习聚合过程改善记忆利用，开发任务特定反思机制适应记忆存储，支持离线和在线策略优化。

Result: 在多个方面进行了全面实验验证方法有效性，并开源了项目代码。

Conclusion: 该记忆框架使LLM智能体能够在特定环境中有效学习记忆信息，解决了现有方法的局限性。

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [86] [Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering](https://arxiv.org/abs/2508.17872)
*Yanghao Qin,Bo Zhou,Guangliang Pan,Qihui Wu,Meixia Tao*

Main category: cs.LG

TL;DR: 提出了SFFP框架，通过分数傅里叶变换和自适应滤波分离频谱数据中的可预测趋势和噪声，使用复数神经网络进行预测，在真实频谱数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 频谱数据具有独特特性，现有基于时域或频域的方法难以有效分离可预测模式和噪声，影响频谱预测的准确性。

Method: SFFP框架包含三个模块：自适应分数傅里叶变换模块将数据转换到合适分数域，自适应滤波模块选择性抑制噪声保留特征，复数神经网络预测模块学习过滤后的趋势分量。

Result: 在真实世界频谱数据上的实验表明，SFFP框架在频谱预测方面优于领先的频谱预测方法和通用预测方法。

Conclusion: SFFP框架通过分数傅里叶域处理和自适应滤波有效解决了频谱数据中模式与噪声分离的难题，显著提升了频谱预测性能。

Abstract: Accurate spectrum prediction is crucial for dynamic spectrum access (DSA) and
resource allocation. However, due to the unique characteristics of spectrum
data, existing methods based on the time or frequency domain often struggle to
separate predictable patterns from noise. To address this, we propose the
Spectral Fractional Filtering and Prediction (SFFP) framework. SFFP first
employs an adaptive fractional Fourier transform (FrFT) module to transform
spectrum data into a suitable fractional Fourier domain, enhancing the
separability of predictable trends from noise. Subsequently, an adaptive Filter
module selectively suppresses noise while preserving critical predictive
features within this domain. Finally, a prediction module, leveraging a
complex-valued neural network, learns and forecasts these filtered trend
components. Experiments on real-world spectrum data show that the SFFP
outperforms leading spectrum and general forecasting methods.

</details>


### [87] [Recurrent Transformer U-Net Surrogate for Flow Modeling and Data Assimilation in Subsurface Formations with Faults](https://arxiv.org/abs/2508.16631)
*Yifu Han,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 开发了一种新的循环变换器U-Net代理模型，用于快速预测断层化地下含水层系统中的压力和CO2饱和度，用于地质碳储存应用。


<details>
  <summary>Details</summary>
Motivation: 许多地下地层（包括考虑大规模地质碳储存的地层）包含广泛的断层，这些断层会强烈影响流体流动，需要快速准确的预测模型。

Method: 使用循环变换器U-Net架构构建代理模型，在包含目标含水层、围岩、盖层和两个断层的复杂地质模型中训练，处理地层特性和断层渗透率的不确定性。

Result: 新模型比之前的循环残差U-Net更准确，在不同泄漏场景下保持精度，成功应用于全局敏感性分析和数据同化，展示了不同监测策略的不确定性减少效果。

Conclusion: 测量所有三个含水层的压力和饱和度数据可以显著改善后验结果，为地质碳储存的监测策略提供了重要指导。

Abstract: Many subsurface formations, including some of those under consideration for
large-scale geological carbon storage, include extensive faults that can
strongly impact fluid flow. In this study, we develop a new recurrent
transformer U-Net surrogate model to provide very fast predictions for pressure
and CO2 saturation in realistic faulted subsurface aquifer systems. The
geomodel includes a target aquifer (into which supercritical CO2 is injected),
surrounding regions, caprock, two extensive faults, and two overlying aquifers.
The faults can act as leakage pathways between the three aquifers. The
heterogeneous property fields in the target aquifer are characterized by
hierarchical uncertainty, meaning both the geological metaparameters (e.g.,
mean and standard deviation of log-permeability) and the detailed cell
properties of each realization, are uncertain. Fault permeabilities are also
treated as uncertain. The model is trained with simulation results for (up to)
4000 randomly sampled realizations. Error assessments show that this model is
more accurate than a previous recurrent residual U-Net, and that it maintains
accuracy for qualitatively different leakage scenarios. The new surrogate is
then used for global sensitivity analysis and data assimilation. A hierarchical
Markov chain Monte Carlo data assimilation procedure is applied. Different
monitoring strategies, corresponding to different amounts and types of observed
data collected at monitoring wells, are considered for three synthetic true
models. Detailed results demonstrate the degree of uncertainty reduction
achieved with the various monitoring strategies. Posterior results for 3D
saturation plumes and leakage volumes indicate the benefits of measuring
pressure and saturation in all three aquifers.

</details>


### [88] [Deep Learning and Matrix Completion-aided IoT Network Localization in the Outlier Scenarios](https://arxiv.org/abs/2508.18225)
*Sunwoo Kim*

Main category: cs.LG

TL;DR: 提出一种基于深度学习和矩阵补全的方法，用于恢复物联网网络定位中受异常值污染的欧几里得距离矩阵，通过联合恢复距离矩阵和传感器坐标，有效处理异常值。


<details>
  <summary>Details</summary>
Motivation: 传统定位技术在矩阵全集上搜索解，而本方法将搜索限制在欧几里得距离矩阵集合内，利用距离矩阵的独特性质提高定位精度。

Method: 将距离矩阵表示为传感器坐标矩阵的函数，使用深度神经网络联合恢复距离矩阵和坐标矩阵，将异常值建模为稀疏矩阵并加入正则化项，通过交替更新坐标矩阵、距离矩阵和异常值矩阵来求解。

Result: 数值实验表明，即使在存在异常值的情况下，该方法也能准确恢复传感器的位置信息。

Conclusion: 所提出的深度学习和矩阵补全方法能够有效处理异常值污染，在物联网网络定位中实现高精度的传感器位置恢复。

Abstract: In this paper, we propose a deep learning and matrix completion aided
approach for recovering an outlier contaminated Euclidean distance matrix D in
IoT network localization. Unlike conventional localization techniques that
search the solution over a whole set of matrices, the proposed technique
restricts the search to the set of Euclidean distance matrices. Specifically,
we express D as a function of the sensor coordinate matrix X that inherently
satisfies the unique properties of D, and then jointly recover D and X using a
deep neural network. To handle outliers effectively, we model them as a sparse
matrix L and add a regularization term of L into the optimization problem. We
then solve the problem by alternately updating X, D, and L. Numerical
experiments demonstrate that the proposed technique can recover the location
information of sensors accurately even in the presence of outliers.

</details>


### [89] [Adaptive Variance-Penalized Continual Learning with Fisher Regularization](https://arxiv.org/abs/2508.16632)
*Krisanu Sarkar*

Main category: cs.LG

TL;DR: 提出了一种新颖的持续学习框架，通过Fisher加权的非对称正则化参数方差，在变分学习范式中动态调节正则化强度，有效解决了神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络中的灾难性遗忘问题是持续学习领域长期存在的挑战，需要开发能够有效保持先前任务知识的新方法。

Method: 将Fisher加权的非对称参数方差正则化集成到变分学习范式中，根据参数不确定性动态调节正则化强度。

Result: 在SplitMNIST、PermutedMNIST和SplitFashionMNIST等标准基准测试中，相比VCL和EWC等方法取得了显著改进，有效保持了跨序列任务的知识并提高了模型准确性。

Conclusion: 该方法不仅提升了即时任务性能，还显著减轻了随时间推移的知识退化，有效解决了神经网络灾难性遗忘的根本挑战。

Abstract: The persistent challenge of catastrophic forgetting in neural networks has
motivated extensive research in continual learning . This work presents a novel
continual learning framework that integrates Fisher-weighted asymmetric
regularization of parameter variances within a variational learning paradigm.
Our method dynamically modulates regularization intensity according to
parameter uncertainty, achieving enhanced stability and performance.
Comprehensive evaluations on standard continual learning benchmarks including
SplitMNIST, PermutedMNIST, and SplitFashionMNIST demonstrate substantial
improvements over existing approaches such as Variational Continual Learning
and Elastic Weight Consolidation . The asymmetric variance penalty mechanism
proves particularly effective in maintaining knowledge across sequential tasks
while improving model accuracy. Experimental results show our approach not only
boosts immediate task performance but also significantly mitigates knowledge
degradation over time, effectively addressing the fundamental challenge of
catastrophic forgetting in neural networks

</details>


### [90] [A Novel Unified Extended Matrix for Graph Signal Processing: Theory and Application](https://arxiv.org/abs/2508.16633)
*Yunyan Zheng,Zhichao Zhang,Wei Yao*

Main category: cs.LG

TL;DR: 提出了统一扩展矩阵(UEM)框架来解决传统图移位算子在建模非相邻节点依赖关系方面的局限性，通过参数化设计整合扩展邻接矩阵和统一图表示矩阵，能够灵活适应不同图结构并揭示更多图信号信息。


<details>
  <summary>Details</summary>
Motivation: 传统图移位算子(GSOs)在建模非相邻节点之间的依赖关系方面缺乏灵活性，限制了表示复杂图结构的能力，需要一种更灵活的框架来增强图信号处理性能。

Method: 提出统一扩展矩阵(UEM)框架，通过参数化设计整合扩展邻接矩阵和统一图表示矩阵，并基于UEM提出图傅里叶变换(UEM-GFT)，能够自适应调整频谱特性。

Result: 理论分析证明了UEM在特定条件下的半正定性和特征值单调性。在合成和真实数据集上的实验表明，UEM-GFT在异常检测任务中优于现有的基于GSO的方法，在不同网络拓扑下都表现出优越性能。

Conclusion: UEM框架有效解决了传统图移位算子的局限性，提供了更灵活的图信号处理工具，在异常检测等任务中展现出显著优势。

Abstract: Graph signal processing has become an essential tool for analyzing data
structured on irregular domains. While conventional graph shift operators
(GSOs) are effective for certain tasks, they inherently lack flexibility in
modeling dependencies between non-adjacent nodes, limiting their ability to
represent complex graph structures. To address this limitation, this paper
proposes the unified extended matrix (UEM) framework, which integrates the
extended-adjacency matrix and the unified graph representation matrix through
parametric design, so as to be able to flexibly adapt to different graph
structures and reveal more graph signal information. Theoretical analysis of
the UEM is conducted, demonstrating positive semi-definiteness and eigenvalue
monotonicity under specific conditions. Then, we propose graph Fourier
transform based on UEM (UEM-GFT), which can adaptively tune spectral properties
to enhance signal processing performance. Experimental results on synthetic and
real-world datasets demonstrate that the UEM-GFT outperforms existing GSO-based
methods in anomaly detection tasks, achieving superior performance across
varying network topologies.

</details>


### [91] [Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations](https://arxiv.org/abs/2508.16634)
*Zhendong Yang,Jie Wang,Liansong Zong,Xiaorong Liu,Quan Qian,Shiqian Chen*

Main category: cs.LG

TL;DR: 本文提出DGGN框架，通过双粒度表征解耦和动态融合机制，解决少样本类增量故障诊断中的灾难性遗忘和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 工业系统中需要持续学习新故障类别但样本稀缺，传统方法存在灾难性遗忘旧知识和新数据过拟合的严重问题。

Method: 使用双粒度表征：细粒度流通过多阶交互聚合捕获类别特征，粗粒度流保留类别无关知识；通过跨注意力机制动态融合；采用边界感知样本优先策略和解耦平衡随机森林分类器。

Result: 在TEP基准和真实MFF数据集上实验表明，DGGN相比现有FSC-FD方法具有更优的诊断性能和稳定性。

Conclusion: DGGN框架有效解决了少样本类增量故障诊断的关键挑战，为工业系统故障诊断提供了实用解决方案。

Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to
continuously learn from new fault classes with only a few samples without
forgetting old ones, is critical for real-world industrial systems. However,
this challenging task severely amplifies the issues of catastrophic forgetting
of old knowledge and overfitting on scarce new data. To address these
challenges, this paper proposes a novel framework built upon Dual-Granularity
Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN
explicitly decouples feature learning into two parallel streams: 1) a
fine-grained representation stream, which utilizes a novel Multi-Order
Interaction Aggregation module to capture discriminative, class-specific
features from the limited new samples. 2) a coarse-grained representation
stream, designed to model and preserve general, class-agnostic knowledge shared
across all fault types. These two representations are dynamically fused by a
multi-semantic cross-attention mechanism, where the stable coarse-grained
knowledge guides the learning of fine-grained features, preventing overfitting
and alleviating feature conflicts. To further mitigate catastrophic forgetting,
we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a
decoupled Balanced Random Forest classifier is employed to counter the decision
boundary bias caused by data imbalance. Extensive experiments on the TEP
benchmark and a real-world MFF dataset demonstrate that our proposed DGGN
achieves superior diagnostic performance and stability compared to
state-of-the-art FSC-FD approaches. Our code is publicly available at
https://github.com/MentaY/DGGN

</details>


### [92] [Enhancing Transformer-Based Foundation Models for Time Series Forecasting via Bagging, Boosting and Statistical Ensembles](https://arxiv.org/abs/2508.16641)
*Dhruv D. Modi,Rong Pan*

Main category: cs.LG

TL;DR: 本文研究统计和集成增强技术来改进时间序列基础模型的预测性能，通过回归集成、自助聚合、残差建模等方法显著提升了准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测、异常检测等方面表现出色，但其预测仍存在方差、领域特定偏差和不确定性量化有限的问题，需要改进在实际操作数据上的表现

Method: 采用统计和基于集成的增强技术，包括自助法bagging、基于回归的stacking、预测区间构建、统计残差建模和迭代误差反馈等方法

Result: 在比利时电力短期负荷预测数据集上，混合方法持续优于独立基础模型。回归集成获得最低均方误差，自助聚合显著减少长上下文错误，残差建模纠正系统偏差，预测区间实现接近名义覆盖率

Conclusion: 将统计推理与现代基础模型相结合，可在准确性、可靠性和可解释性方面为实际时间序列应用带来可衡量的收益

Abstract: Time series foundation models (TSFMs) such as Lag-Llama, TimeGPT, Chronos,
MOMENT, UniTS, and TimesFM have shown strong generalization and zero-shot
capabilities for time series forecasting, anomaly detection, classification,
and imputation. Despite these advantages, their predictions still suffer from
variance, domain-specific bias, and limited uncertainty quantification when
deployed on real operational data. This paper investigates a suite of
statistical and ensemble-based enhancement techniques, including
bootstrap-based bagging, regression-based stacking, prediction interval
construction, statistical residual modeling, and iterative error feedback, to
improve robustness and accuracy. Using the Belgium Electricity Short-Term Load
Forecasting dataset as a case study, we demonstrate that the proposed hybrids
consistently outperform standalone foundation models across multiple horizons.
Regression-based ensembles achieve the lowest mean squared error; bootstrap
aggregation markedly reduces long-context errors; residual modeling corrects
systematic bias; and the resulting prediction intervals achieve near nominal
coverage with widths shrinking as context length increases. The results
indicate that integrating statistical reasoning with modern foundation models
yields measurable gains in accuracy, reliability, and interpretability for
real-world time series applications.

</details>


### [93] [Effective Clustering for Large Multi-Relational Graphs](https://arxiv.org/abs/2508.17388)
*Xiaoyang Lin,Runhao Jiang,Renchi Yang*

Main category: cs.LG

TL;DR: 提出了DEMM和DEMM+两种多关系图聚类方法，通过新颖的两阶段优化目标解决现有方法在异构图结构融合和大规模图处理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多关系图聚类方法要么因异构图结构和属性融合效果差而导致结果质量严重受损，要么因采用复杂昂贵的深度学习模型而难以处理包含数百万节点和数十亿边的大规模图。

Method: 基于新颖的两阶段优化目标：第一阶段通过优化多关系Dirichlet能量来获得高质量节点特征向量；第二阶段在节点亲和图上最小化聚类结果的Dirichlet能量。DEMM+通过高效近似求解器和理论支撑的问题转换实现线性时间聚类。

Result: 在11个真实多关系图上与20个基线方法比较，DEMM+在聚类质量方面始终优于其他方法，并且通常显著更快。

Conclusion: DEMM+方法在多关系图聚类任务中既保证了高质量的聚类结果，又具备出色的可扩展性和效率，能够有效处理大规模多关系图数据。

Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling
diverse interactions/relations among real objects (i.e., nodes), which pervade
extensive applications and scenarios. Given an MRG G with N nodes, partitioning
the node set therein into K disjoint clusters (MRGC) is a fundamental task in
analyzing MRGs, which has garnered considerable attention. However, the
majority of existing solutions towards MRGC either yield severely compromised
result quality by ineffective fusion of heterogeneous graph structures and
attributes, or struggle to cope with sizable MRGs with millions of nodes and
billions of edges due to the adoption of sophisticated and costly deep learning
models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to
address the limitations above. Specifically, our algorithms are built on novel
two-stage optimization objectives, where the former seeks to derive
high-caliber node feature vectors by optimizing the multi-relational Dirichlet
energy specialized for MRGs, while the latter minimizes the Dirichlet energy of
clustering results over the node affinity graph. In particular, DEMM+ achieves
significantly higher scalability and efficiency over our based method DEMM
through a suite of well-thought-out optimizations. Key technical contributions
include (i) a highly efficient approximation solver for constructing node
feature vectors, and (ii) a theoretically-grounded problem transformation with
carefully-crafted techniques that enable linear-time clustering without
explicitly materializing the NxN dense affinity matrix. Further, we extend
DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive
experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit
that DEMM+ is consistently superior in terms of clustering quality measured
against ground-truth labels, while often being remarkably faster.

</details>


### [94] [From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective](https://arxiv.org/abs/2508.16643)
*Tianhua Chen*

Main category: cs.LG

TL;DR: 本文提出了一个统一视角，将经典和现代生成方法都框定在概率潜在变量模型（PLVM）范式下，揭示了生成式AI的理论基础和方法谱系。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI系统架构多样，但许多都基于概率潜在变量模型的共同基础。本文旨在通过PLVM范式统一理解从经典到现代的生成方法，揭示共享原则和推理策略。

Method: 通过概率分类学框架，分析从经典扁平模型（概率PCA、高斯混合模型等）到序列扩展（HMM、LDS等），再到深度架构（VAE、归一化流、扩散模型、自回归模型、GAN）的演进路径。

Result: 建立了生成式AI的统一理论框架，揭示了不同架构之间的共享原理、推理策略差异和表征权衡，为理解现有方法和指导未来创新提供了概念路线图。

Conclusion: PLVM范式为生成式AI提供了统一的理论基础，通过概率视角能够更好地理解不同方法的联系与差异，为未来架构创新奠定坚实的概率理论基础。

Abstract: From large language models to multi-modal agents, Generative Artificial
Intelligence (AI) now underpins state-of-the-art systems. Despite their varied
architectures, many share a common foundation in probabilistic latent variable
models (PLVMs), where hidden variables explain observed data for density
estimation, latent reasoning, and structured inference. This paper presents a
unified perspective by framing both classical and modern generative methods
within the PLVM paradigm. We trace the progression from classical flat models
such as probabilistic PCA, Gaussian mixture models, latent class analysis, item
response theory, and latent Dirichlet allocation, through their sequential
extensions including Hidden Markov Models, Gaussian HMMs, and Linear Dynamical
Systems, to contemporary deep architectures: Variational Autoencoders as Deep
PLVMs, Normalizing Flows as Tractable PLVMs, Diffusion Models as Sequential
PLVMs, Autoregressive Models as Explicit Generative Models, and Generative
Adversarial Networks as Implicit PLVMs. Viewing these architectures under a
common probabilistic taxonomy reveals shared principles, distinct inference
strategies, and the representational trade-offs that shape their strengths. We
offer a conceptual roadmap that consolidates generative AI's theoretical
foundations, clarifies methodological lineages, and guides future innovation by
grounding emerging architectures in their probabilistic heritage.

</details>


### [95] [AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training](https://arxiv.org/abs/2508.16647)
*Boran Zhao,Hetian Liu,Zihang Yuan,Li Zhu,Fan Yang,Lina Xie Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: AdapSNE是一种改进的边缘设备数据集采样方法，通过Fireworks算法抑制异常值，熵引导优化实现均匀采样，提升训练精度并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有NMS方法在边缘设备训练中的两个问题：搜索方法与误差函数不匹配导致的异常值，以及经验性参数选择导致的采样不均匀，这些问题导致代表性偏差和精度下降。

Method: 集成Fireworks算法(FWA)进行高效非单调搜索以抑制异常值，采用熵引导优化实现均匀采样，并设计专用加速器优化计算效率。

Result: 该方法能够产生更具代表性的训练样本，提升训练精度，同时通过定制数据流和时间复用显著降低边缘设备的训练能耗和面积开销。

Conclusion: AdapSNE有效解决了边缘设备数据集采样中的代表性偏差问题，在保证精度的同时大幅降低了计算成本，为边缘设备上的高效深度学习训练提供了可行解决方案。

Abstract: Training deep neural networks (DNNs) directly on edge devices has attracted
increasing attention, as it offers promising solutions to challenges such as
domain adaptation and privacy preservation. However, conventional DNN training
typically requires large-scale datasets, which imposes prohibitive overhead on
edge devices-particularly for emerging large language model (LLM) tasks. To
address this challenge, a DNN-free method (ie., dataset sampling without DNN),
named NMS (Near-Memory Sampling), has been introduced. By first conducting
dimensionality reduction of the dataset and then performing exemplar sampling
in the reduced space, NMS avoids the architectural bias inherent in DNN-based
methods and thus achieves better generalization. However, The state-of-the-art,
NMS, suffers from two limitations: (1) The mismatch between the search method
and the non-monotonic property of the perplexity error function leads to the
emergence of outliers in the reduced representation; (2) Key parameter (ie.,
target perplexity) is selected empirically, introducing arbitrariness and
leading to uneven sampling. These two issues lead to representative bias of
examplars, resulting in degraded accuracy. To address these issues, we propose
AdapSNE, which integrates an efficient non-monotonic search method-namely, the
Fireworks Algorithm (FWA)-to suppress outliers, and employs entropy-guided
optimization to enforce uniform sampling, thereby ensuring representative
training samples and consequently boosting training accuracy. To cut the
edge-side cost arising from the iterative computations of FWA search and
entropy-guided optimization, we design an accelerator with custom dataflow and
time-multiplexing markedly reducing on-device training energy and area.

</details>


### [96] [LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping](https://arxiv.org/abs/2508.16648)
*Junle Liu,Chang Liu,Yanyu Ke,Qiuxiang Huang,Jiachen Zhao,Wenliang Chen,K. T. Tse,Gang Hu*

Main category: cs.LG

TL;DR: LatentFlow是一个跨模态时间上采样框架，通过融合低频流场和压力数据，仅使用高频壁面压力信号就能重建高频湍流尾流场


<details>
  <summary>Details</summary>
Motivation: 在PIV实验中获取时空高分辨率的湍流尾流场存在硬件限制和测量噪声的挑战，而高频壁面压力测量相对容易获得

Method: 使用压力条件化的β-VAE学习流场紧凑潜在表示，通过次级网络将低频压力信号映射到潜在空间，训练后仅需高频壁面压力输入即可生成高频流场

Result: 该方法能够从稀疏的壁面压力信号重建512Hz的高频湍流尾流场

Conclusion: LatentFlow通过解耦流场空间编码和压力时间测量，为数据受限的实验环境提供了可扩展且鲁棒的高频湍流重建解决方案

Abstract: Acquiring temporally high-frequency and spatially high-resolution turbulent
wake flow fields in particle image velocimetry (PIV) experiments remains a
significant challenge due to hardware limitations and measurement noise. In
contrast, temporal high-frequency measurements of spatially sparse wall
pressure are more readily accessible in wind tunnel experiments. In this study,
we propose a novel cross-modal temporal upscaling framework, LatentFlow, which
reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing
synchronized low-frequency (15 Hz) flow field and pressure data during
training, and high-frequency wall pressure signals during inference. The first
stage involves training a pressure-conditioned $\beta$-variation autoencoder
($p$C-$\beta$-VAE) to learn a compact latent representation that captures the
intrinsic dynamics of the wake flow. A secondary network maps synchronized
low-frequency wall pressure signals into the latent space, enabling
reconstruction of the wake flow field solely from sparse wall pressure. Once
trained, the model utilizes high-frequency, spatially sparse wall pressure
inputs to generate corresponding high-frequency flow fields via the
$p$C-$\beta$-VAE decoder. By decoupling the spatial encoding of flow dynamics
from temporal pressure measurements, LatentFlow provides a scalable and robust
solution for reconstructing high-frequency turbulent wake flows in
data-constrained experimental settings.

</details>


### [97] [HiCL: Hippocampal-Inspired Continual Learning](https://arxiv.org/abs/2508.16651)
*Kushal Kapoor,Wyatt Mackey,Yiannis Aloimonos,Xiaomin Lin*

Main category: cs.LG

TL;DR: HiCL是一个受海马体启发的双记忆持续学习架构，通过网格细胞层编码、齿状回稀疏模式分离、CA3自联想记忆和基于余弦相似度的专家路由机制，有效缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经网络在持续学习中的灾难性遗忘问题，受海马体记忆机制的启发，设计一个生物合理且计算高效的架构。

Method: 使用网格细胞层编码输入，齿状回模块进行稀疏模式分离，CA3自联想记忆存储情景记忆，基于余弦相似度的专家路由机制动态管理任务处理，并结合弹性权重巩固和优先回放机制。

Result: 在标准持续学习基准测试中表现出色，有效减少任务干扰，以较低计算成本达到接近最先进水平的性能。

Conclusion: HiCL架构成功地将海马体神经机制转化为有效的计算模型，为持续学习提供了生物启发式的解决方案，在性能和效率方面都有显著优势。

Abstract: We propose HiCL, a novel hippocampal-inspired dual-memory continual learning
architecture designed to mitigate catastrophic forgetting by using elements
inspired by the hippocampal circuitry. Our system encodes inputs through a
grid-cell-like layer, followed by sparse pattern separation using a dentate
gyrus-inspired module with top-k sparsity. Episodic memory traces are
maintained in a CA3-like autoassociative memory. Task-specific processing is
dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs
are routed to experts based on cosine similarity between their normalized
sparse DG representations and learned task-specific DG prototypes computed
through online exponential moving averages. This biologically grounded yet
mathematically principled gating strategy enables differentiable, scalable
task-routing without relying on a separate gating network, and enhances the
model's adaptability and efficiency in learning multiple sequential tasks.
Cortical outputs are consolidated using Elastic Weight Consolidation weighted
by inter-task similarity. Crucially, we incorporate prioritized replay of
stored patterns to reinforce essential past experiences. Evaluations on
standard continual learning benchmarks demonstrate the effectiveness of our
architecture in reducing task interference, achieving near state-of-the-art
results in continual learning tasks at lower computational costs.

</details>


### [98] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: 一种基于Transformer和Laplace扩散技术的新型心率监测模型，通过活动上下文嵌入和注意机制来接合身体活动数据，显著提高了远程心率预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决远程患者监测中心率波动因素多、且缺乏与身体活动关联的问题，现有AI模型少有整合活动数据来提升监测的上下文理解能力。

Method: 采用Transformer模型结合Laplace扩散技术，通过专门的活动上下文嵌入和注意机制，将活动数据作为整个模型过程的条件信息，以优先考虑活动特定的历史模式。

Result: 在29名患者4个月的真实数据集上验证，模型超越了当前最先进方法，平均绝对误差比基准模型减少43%，决定系数R2达到0.97，预测心率与实际值高度一致。

Conclusion: 该模型是一种实用有效的工具，可以支持医疗服务提供者和远程患者监测系统，通过整合活动上下文显著提升了心率监测的准确性和可靠性。

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [99] [OASIS: Open-world Adaptive Self-supervised and Imbalanced-aware System](https://arxiv.org/abs/2508.16656)
*Miru Kim,Mugon Joe,Minhae Kwon*

Main category: cs.LG

TL;DR: 本文提出了一种应对开放世界问题的方法，特别关注在类别不平衡数据上进行预训练的情况下的模型适配问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在动态环境中遇到了标签偏移、协变量偏移和未知类别等开放世界问题。当初始预训练在类别不平衡数据上进行时，现有的训后适配方法对少数类别的汇性能很差。

Method: 提出了基于对比学习的预训练方法来提升分类性能，尤其是对于代表不足的类别。还包含了生成可靠伪标签的训后机制和选择性激活标准来优化训后过程。

Result: 广泛实验表明，该方法在各种开放世界场景下在准确性和效率方面都显著超过了现有的最先进适配技术。

Conclusion: 该研究为处理类别不平衡数据上进行预训练的开放世界问题提供了有效的解决方案，通过对比预训练和智能训后机制显著提升了模型的潜力。

Abstract: The expansion of machine learning into dynamic environments presents
challenges in handling open-world problems where label shift, covariate shift,
and unknown classes emerge. Post-training methods have been explored to address
these challenges, adapting models to newly emerging data. However, these
methods struggle when the initial pre-training is performed on class-imbalanced
datasets, limiting generalization to minority classes. To address this, we
propose a method that effectively handles open-world problems even when
pre-training is conducted on imbalanced data. Our contrastive-based
pre-training approach enhances classification performance, particularly for
underrepresented classes. Our post-training mechanism generates reliable
pseudo-labels, improving model robustness against open-world problems. We also
introduce selective activation criteria to optimize the post-training process,
reducing unnecessary computation. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art adaptation techniques in both
accuracy and efficiency across diverse open-world scenarios.

</details>


### [100] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: 提出了WISCA权重缩放方法，通过优化神经网络权重模式来提升Transformer大语言模型的训练效率和模型质量，无需改变网络结构


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer大语言模型训练优化方法主要关注架构修改或优化器调整，缺乏对训练过程中权重模式的系统性优化

Method: WISCA权重缩放方法，通过重新缩放权重同时保持模型输出不变，间接优化模型的训练轨迹

Result: 实验显示WISCA显著提升了收敛质量（泛化能力和损失减少），在GQA架构的LLM和LoRA微调任务中表现优异，零样本验证任务平均提升5.6%，训练困惑度平均降低2.12%

Conclusion: WISCA方法通过系统性地优化权重模式，有效提升了Transformer大语言模型的训练效率和最终性能

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [101] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: 提出RED方法，通过控制探索空间和优化离线数据整合来增强小语言模型的推理能力，解决探索不足和蒸馏冗余问题


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过RLVR提升大语言模型的推理能力，但小语言模型的推理能力增强尚未充分探索，结合大模型蒸馏数据和RLVR面临各种挑战

Method: 提出RED方法：1）通过监控模型对离线/在线数据的熵变化比例来调节离线SFT权重；2）设计基于样本准确率的策略切换机制，动态选择模仿离线数据或学习自身策略

Result: 该方法解决了小模型探索空间不足和蒸馏过程冗余复杂的问题，同时处理了离线数据与当前策略之间的分布差异

Conclusion: RED方法通过平衡离线蒸馏和在线强化学习，有效提升了小语言模型的推理能力，为小模型增强提供了新思路

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [102] [Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs](https://arxiv.org/abs/2508.17400)
*Jacob Portes,Connor Jennings,Erica Ji Yuen,Sasha Doubov,Michael Carbin*

Main category: cs.LG

TL;DR: 检索性能随预训练FLOPs的增加而可预测地提升，与LLM规模、训练时长和估计FLOPs呈正相关，且上下文学习分数与检索分数高度相关


<details>
  <summary>Details</summary>
Motivation: 研究检索性能如何随预训练计算量（FLOPs）的变化而扩展，为开发基于LLM的检索器提供指导

Method: 对参数规模从1.25亿到70亿的LLM进行基准测试，使用从10亿到超过2万亿token的数据集进行预训练，评估零样本BEIR任务的检索性能

Result: 检索性能随模型大小、训练持续时间和估计FLOPs可预测地扩展，上下文学习分数与检索任务中的检索分数高度相关

Conclusion: 这些发现对基于LLM的检索器的开发具有重要意义，表明检索性能可以通过增加预训练计算量来系统性地提升

Abstract: How does retrieval performance scale with pretraining FLOPs? We benchmark
retrieval performance across LLM model sizes from 125 million parameters to 7
billion parameters pretrained on datasets ranging from 1 billion tokens to more
than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR
tasks predictably scales with LLM size, training duration, and estimated FLOPs.
We also show that In-Context Learning scores are strongly correlated with
retrieval scores across retrieval tasks. Finally, we highlight the implications
this has for the development of LLM-based retrievers.

</details>


### [103] [CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression](https://arxiv.org/abs/2508.16680)
*Muchammad Daniyal Kautsar,Afra Majida Hariono,Widyawan,Syukron Abu Ishaq Alfarozi,Kuntpong Wararatpanya*

Main category: cs.LG

TL;DR: CALR是一种新的LLM压缩方法，通过SVD主路径和可学习的低秩校正模块来恢复功能损失，在显著减少参数的同时保持较高性能


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法主要关注矩阵重构误差最小化，但会导致模型功能性能显著下降，需要解决压缩过程中的功能信息损失问题

Method: CALR采用双组件压缩方法：SVD压缩层的主路径 + 并行可学习的低秩校正模块，专门训练用于恢复功能残差误差

Result: 在多个模型上测试，CALR能减少26.93%-51.77%参数，同时保持59.45%-90.42%的原始性能，优于LaCo、ShortGPT和LoSparse

Conclusion: 将功能信息损失视为可学习信号是一种高效的压缩范式，能够创建更小更高效的LLM，提升实际部署的可行性

Abstract: Large Language Models (LLMs) present significant deployment challenges due to
their immense size and computational requirements. Model compression techniques
are essential for making these models practical for resource-constrained
environments. A prominent compression strategy is low-rank factorization via
Singular Value Decomposition (SVD) to reduce model parameters by approximating
weight matrices. However, standard SVD focuses on minimizing matrix
reconstruction error, often leading to a substantial loss of the model's
functional performance. This performance degradation occurs because existing
methods do not adequately correct for the functional information lost during
compression. To address this gap, we introduce Corrective Adaptive Low-Rank
Decomposition (CALR), a two-component compression approach. CALR combines a
primary path of SVD-compressed layers with a parallel, learnable, low-rank
corrective module that is explicitly trained to recover the functional residual
error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and
Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to
51.77% while retaining 59.45% to 90.42% of the original model's performance,
consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows
that treating functional information loss as a learnable signal is a highly
effective compression paradigm. This approach enables the creation of
significantly smaller, more efficient LLMs, advancing their accessibility and
practical deployment in real-world applications.

</details>


### [104] [Heterogeneous co-occurrence embedding for visual information exploration](https://arxiv.org/abs/2508.17663)
*Takuro Ishida,Tetsuo Furukawa*

Main category: cs.LG

TL;DR: 提出了一种用于共现数据可视化的嵌入方法，能够将异构域元素映射到二维潜在空间，可视化域间的不对称关系，并通过互信息最大化保持依赖结构。


<details>
  <summary>Details</summary>
Motivation: 处理异构域之间的共现概率数据，需要一种能够可视化不对称关系并保持原始依赖结构的嵌入方法，以便进行信息探索和分析。

Method: 通过最大化互信息将异构元素嵌入到二维潜在空间，保持原始依赖结构；使用总相关扩展到多域情况；基于条件概率为潜在空间分配颜色进行交互式可视化。

Result: 方法在形容词-名词数据集、NeurIPS数据集和主谓宾数据集上得到验证，成功展示了域内和域间分析的能力。

Conclusion: 提出的嵌入方法有效实现了异构域共现数据的可视化，能够揭示不对称关系，为信息探索提供了有力的工具。

Abstract: This paper proposes an embedding method for co-occurrence data aimed at
visual information exploration. We consider cases where co-occurrence
probabilities are measured between pairs of elements from heterogeneous
domains. The proposed method maps these heterogeneous elements into
corresponding two-dimensional latent spaces, enabling visualization of
asymmetric relationships between the domains. The key idea is to embed the
elements in a way that maximizes their mutual information, thereby preserving
the original dependency structure as much as possible. This approach can be
naturally extended to cases involving three or more domains, using a
generalization of mutual information known as total correlation. For
inter-domain analysis, we also propose a visualization method that assigns
colors to the latent spaces based on conditional probabilities, allowing users
to explore asymmetric relationships interactively. We demonstrate the utility
of the method through applications to an adjective-noun dataset, the NeurIPS
dataset, and a subject-verb-object dataset, showcasing both intra- and
inter-domain analysis.

</details>


### [105] [STGAtt: A Spatial-Temporal Unified Graph Attention Network for Traffic Flow Forecasting](https://arxiv.org/abs/2508.16685)
*Zhuding Liang,Jianxun Cui,Qingshuang Zeng,Feng Liu,Nenad Filipovic,Tijana Geroski*

Main category: cs.LG

TL;DR: 提出STGAtt模型，通过统一图表示和注意力机制有效捕捉交通流中的复杂时空依赖关系，在多个数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 准确及时的交通流预测对智能交通系统至关重要，现有方法在时空依赖建模方面存在局限性

Method: 使用时空统一图表示和注意力机制，直接建模时空相关性，并采用分区交换机制捕捉短程和长程关联

Result: 在PEMS-BAY和SHMetro数据集上表现出优于最先进基线的性能，可视化显示能适应动态交通模式和捕捉长程依赖

Conclusion: STGAtt模型在交通流预测方面具有显著优势，展现了在实际应用中的潜力

Abstract: Accurate and timely traffic flow forecasting is crucial for intelligent
transportation systems. This paper presents a novel deep learning model, the
Spatial-Temporal Unified Graph Attention Network (STGAtt). By leveraging a
unified graph representation and an attention mechanism, STGAtt effectively
captures complex spatial-temporal dependencies. Unlike methods relying on
separate spatial and temporal dependency modeling modules, STGAtt directly
models correlations within a Spatial-Temporal Unified Graph, dynamically
weighing connections across both dimensions. To further enhance its
capabilities, STGAtt partitions traffic flow observation signal into
neighborhood subsets and employs a novel exchanging mechanism, enabling
effective capture of both short-range and long-range correlations. Extensive
experiments on the PEMS-BAY and SHMetro datasets demonstrate STGAtt's superior
performance compared to state-of-the-art baselines across various prediction
horizons. Visualization of attention weights confirms STGAtt's ability to adapt
to dynamic traffic patterns and capture long-range dependencies, highlighting
its potential for real-world traffic flow forecasting applications.

</details>


### [106] [Multidimensional Distributional Neural Network Output Demonstrated in Super-Resolution of Surface Wind Speed](https://arxiv.org/abs/2508.16686)
*Harrison J. Goldwyn,Mitchell Krock,Johann Rudi,Daniel Getter,Julie Bessac*

Main category: cs.LG

TL;DR: 提出了一种使用多维高斯损失训练神经网络的方法，能够生成闭式预测分布，同时保持空间相关性并计算高效


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉任意性和认知不确定性，且缺乏能够保持空间相关性的多维闭式分布方法

Method: 使用多维高斯损失函数，通过傅里叶表示协方差矩阵稳定训练，引入信息共享正则化策略在图像特定和全局协方差估计间插值

Result: 在超分辨率降尺度任务中成功应用，能够高效采样、显式建模相关性，且不影响预测性能

Conclusion: 该方法为科学模型中的不确定性感知预测提供了有效框架，可扩展到更复杂的分布族

Abstract: Accurate quantification of uncertainty in neural network predictions remains
a central challenge for scientific applications involving high-dimensional,
correlated data. While existing methods capture either aleatoric or epistemic
uncertainty, few offer closed-form, multidimensional distributions that
preserve spatial correlation while remaining computationally tractable. In this
work, we present a framework for training neural networks with a
multidimensional Gaussian loss, generating closed-form predictive distributions
over outputs with non-identically distributed and heteroscedastic structure.
Our approach captures aleatoric uncertainty by iteratively estimating the means
and covariance matrices, and is demonstrated on a super-resolution example. We
leverage a Fourier representation of the covariance matrix to stabilize network
training and preserve spatial correlation. We introduce a novel regularization
strategy -- referred to as information sharing -- that interpolates between
image-specific and global covariance estimates, enabling convergence of the
super-resolution downscaling network trained on image-specific distributional
loss functions. This framework allows for efficient sampling, explicit
correlation modeling, and extensions to more complex distribution families all
without disrupting prediction performance. We demonstrate the method on a
surface wind speed downscaling task and discuss its broader applicability to
uncertainty-aware prediction in scientific models.

</details>


### [107] [Native Logical and Hierarchical Representations with Subspace Embeddings](https://arxiv.org/abs/2508.16687)
*Gabriel Moreira,Zita Marinho,Manuel Marques,João Paulo Costeira,Chenyan Xiong*

Main category: cs.LG

TL;DR: 提出将概念表示为线性子空间而非点向量的新嵌入范式，通过子空间维度建模泛化性，子空间包含关系建模层次结构，支持集合运算和逻辑操作，在WordNet重建和自然语言推理任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统神经嵌入将概念表示为点向量，擅长相似性计算但在高阶推理和非对称关系建模方面存在局限，需要新的表示框架来支持逻辑运算和形式语义

Method: 提出子空间嵌入框架，使用线性子空间表示概念，通过子空间维度表示泛化程度，子空间包含关系表示层次结构。提出正交投影算子的平滑松弛方法，可微分地学习子空间方向和维度

Result: 在WordNet的重建和链接预测任务上达到state-of-the-art结果。在自然语言推理基准测试中超越双编码器基线，提供可解释的蕴含关系几何表示

Conclusion: 子空间嵌入为概念表示提供了新的几何基础范式，既保持几何直观性又支持逻辑运算，在多个NLP任务上表现出色，为形式语义和神经表示的结合提供了有前景的方向

Abstract: Traditional neural embeddings represent concepts as points, excelling at
similarity but struggling with higher-level reasoning and asymmetric
relationships. We introduce a novel paradigm: embedding concepts as linear
subspaces. This framework inherently models generality via subspace
dimensionality and hierarchy through subspace inclusion. It naturally supports
set-theoretic operations like intersection (conjunction), linear sum
(disjunction) and orthogonal complements (negations), aligning with classical
formal semantics. To enable differentiable learning, we propose a smooth
relaxation of orthogonal projection operators, allowing for the learning of
both subspace orientation and dimension. Our method achieves state-of-the-art
results in reconstruction and link prediction on WordNet. Furthermore, on
natural language inference benchmarks, our subspace embeddings surpass
bi-encoder baselines, offering an interpretable formulation of entailment that
is both geometrically grounded and amenable to logical operations.

</details>


### [108] [A novel auxiliary equation neural networks method for exactly explicit solutions of nonlinear partial differential equations](https://arxiv.org/abs/2508.16702)
*Shanhao Yuan,Yanqin Liu,Runfa Zhang,Limei Yan,Shunjun Wu,Libo Feng*

Main category: cs.LG

TL;DR: 提出辅助方程神经网络方法(AENNM)，结合神经网络和辅助方程法求解非线性偏微分方程，通过Riccati方程解构造新型激活函数，获得精确解析解。


<details>
  <summary>Details</summary>
Motivation: 传统方法求解非线性偏微分方程存在计算效率低、精度不足的问题，需要结合神经网络强大的近似能力和符号计算的高精度优势。

Method: 将辅助方程法嵌入神经网络框架，基于Riccati方程解构造新型激活函数，使用"2-2-2-1"和"3-2-2-1"神经网络模型，通过设定特定激活函数构建新的试验函数。

Result: 成功求解了非线性演化方程、Korteweg-de Vries-Burgers方程和(2+1)维Boussinesq方程，获得了包含双曲函数、三角函数和有理函数的精确解析解，并通过三维图、等高线图和密度图展示了解的动态特性。

Conclusion: AENNM为求解非线性偏微分方程提供了新的方法论框架，在科学和工程领域具有广泛适用性，显著提高了计算效率和精度。

Abstract: In this study, we firstly propose an auxiliary equation neural networks
method (AENNM), an innovative analytical method that integrates neural networks
(NNs) models with the auxiliary equation method to obtain exact solutions of
nonlinear partial differential equations (NLPDEs). A key novelty of this method
is the introduction of a novel activation function derived from the solutions
of the Riccati equation, establishing a new mathematical link between
differential equations theory and deep learning. By combining the strong
approximation capability of NNs with the high precision of symbolic
computation, AENNM significantly enhances computational efficiency and
accuracy. To demonstrate the effectiveness of the AENNM in solving NLPDEs,
three numerical examples are investigated, including the nonlinear evolution
equation, the Korteweg-de Vries-Burgers equation, and the (2+1)-dimensional
Boussinesq equation. Furthermore, some new trial functions are constructed by
setting specific activation functions within the "2-2-2-1" and "3-2-2-1" NNs
models. By embedding the auxiliary equation method into the NNs framework, we
derive previously unreported solutions. The exact analytical solutions are
expressed in terms of hyperbolic functions, trigonometric functions, and
rational functions. Finally, three-dimensional plots, contour plots, and
density plots are presented to illustrate the dynamic characteristics of the
obtained solutions. This research provides a novel methodological framework for
addressing NLPDEs, with broad applicability across scientific and engineering
fields.

</details>


### [109] [Aligning Distributionally Robust Optimization with Practical Deep Learning Needs](https://arxiv.org/abs/2508.16734)
*Dmitrii Feoktistov,Igor Ignashin,Andrey Veprikov,Nikita Borovko,Alexander Bogdanov,Savelii Chezhegov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: ALSO（自适应损失缩放优化器）是一种新的DRO算法，能够处理样本组权重分配，在多种深度学习任务中优于传统优化器和现有DRO方法。


<details>
  <summary>Details</summary>
Motivation: 传统DRO方法无法满足现代深度学习实践的需求，缺乏对随机梯度的处理能力和样本组级别的权重分配功能。

Method: 提出ALSO算法，通过修改DRO目标函数，支持自适应权重分配和样本组级别的处理，并证明了在非凸目标下的收敛性。

Result: 在表格深度学习和分割学习等多种任务中，ALSO在性能上超越了传统优化器和现有DRO方法。

Conclusion: ALSO成功弥合了DRO理论与深度学习实践之间的差距，为分布鲁棒优化提供了更实用的解决方案。

Abstract: While traditional Deep Learning (DL) optimization methods treat all training
samples equally, Distributionally Robust Optimization (DRO) adaptively assigns
importance weights to different samples. However, a significant gap exists
between DRO and current DL practices. Modern DL optimizers require adaptivity
and the ability to handle stochastic gradients, as these methods demonstrate
superior performance. Additionally, for practical applications, a method should
allow weight assignment not only to individual samples, but also to groups of
objects (for example, all samples of the same class). This paper aims to bridge
this gap by introducing ALSO $\unicode{x2013}$ Adaptive Loss Scaling Optimizer
$\unicode{x2013}$ an adaptive algorithm for a modified DRO objective that can
handle weight assignment to sample groups. We prove the convergence of our
proposed algorithm for non-convex objectives, which is the typical case for DL
models. Empirical evaluation across diverse Deep Learning tasks, from Tabular
DL to Split Learning tasks, demonstrates that ALSO outperforms both traditional
optimizers and existing DRO methods.

</details>


### [110] [Deep Learning for Markov Chains: Lyapunov Functions, Poisson's Equation, and Stationary Distributions](https://arxiv.org/abs/2508.16737)
*Yanlin Qu,Jose Blanchet,Peter Glynn*

Main category: cs.LG

TL;DR: 使用深度学习自动构建Lyapunov函数来验证马尔可夫模型的稳定性，通过神经网络求解首跳分析得到的积分方程，适用于非紧状态空间


<details>
  <summary>Details</summary>
Motivation: 传统Lyapunov函数构造需要大量创造性和分析工作，深度学习可以自动化这一过程

Method: 训练神经网络满足从首跳分析导出的积分方程，用于稳定性分析、泊松方程求解和稳态分布估计

Result: 方法在排队论等多个示例中证明有效，即使对于非紧状态空间的马尔可夫链也适用

Conclusion: 深度学习能够成功自动化Lyapunov函数构造过程，为马尔可夫模型稳定性分析提供有效工具

Abstract: Lyapunov functions are fundamental to establishing the stability of Markovian
models, yet their construction typically demands substantial creativity and
analytical effort. In this paper, we show that deep learning can automate this
process by training neural networks to satisfy integral equations derived from
first-transition analysis. Beyond stability analysis, our approach can be
adapted to solve Poisson's equation and estimate stationary distributions.
While neural networks are inherently function approximators on compact domains,
it turns out that our approach remains effective when applied to Markov chains
on non-compact state spaces. We demonstrate the effectiveness of this
methodology through several examples from queueing theory and beyond.

</details>


### [111] [WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning](https://arxiv.org/abs/2508.16741)
*Haosen Ge,Shuo Li,Lianghuan Huang*

Main category: cs.LG

TL;DR: 弱到强迁移(WST)框架使用小型教师模型自动生成提示词来提升大型学生模型的性能，无需强大教师模型即可实现高效提示工程


<details>
  <summary>Details</summary>
Motivation: 解决有效提示工程难度大的问题，特别是在大型模型闭源或难以微调的场景下，需要一种高效且广泛适用的自动提示优化方法

Method: 使用强化学习，让小型教师模型迭代生成指令，基于大型学生模型的表现结果不断改进提示词质量

Result: 在推理任务(MATH-500, GSM8K)和对齐任务(HH-RLHF)上取得显著提升：MATH-500提升98%，HH-RLHF提升134%，超越GPT-4o-mini和Llama-70B等基线模型

Conclusion: 小模型能够可靠地支撑大模型，释放潜在能力同时避免强教师可能引入的错误提示，WST为高效安全的LLM提示优化提供了可扩展解决方案

Abstract: Effective prompt engineering remains a challenging task for many
applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt
engineering framework where a small "Teacher" model generates instructions that
enhance the performance of a much larger "Student" model. Unlike prior work,
WST requires only a weak teacher, making it efficient and broadly applicable in
settings where large models are closed-source or difficult to fine-tune. Using
reinforcement learning, the Teacher Model's instructions are iteratively
improved based on the Student Model's outcomes, yielding substantial gains
across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on
MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and
Llama-70B. These results demonstrate that small models can reliably scaffold
larger ones, unlocking latent capabilities while avoiding misleading prompts
that stronger teachers may introduce, establishing WST as a scalable solution
for efficient and safe LLM prompt refinement.

</details>


### [112] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 该研究探索了双曲网络在生物多样性分类中的优势，通过多模态对比学习和新型堆叠蕴含目标，在双曲空间中嵌入生物标本数据，在未见物种分类上表现优异但细粒度分类仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 生物多样性研究中的分类任务需要将生物标本组织成结构化层次体系，传统欧几里得空间可能无法很好地表示这种层次结构，因此研究双曲空间是否能为这种层次模型提供更好的嵌入空间。

Method: 使用对比学习和新型堆叠蕴含目标，将多模态输入（如图像和遗传信息）嵌入到共享的双曲空间中，在BIOSCAN-1M数据集上进行实验验证。

Result: 双曲嵌入在性能上与欧几里得基线相当，在使用DNA条形码进行未见物种分类方面优于所有其他模型，但细粒度分类和开放世界泛化仍然具有挑战性。

Conclusion: 该框架为生物多样性建模提供了结构感知的基础，在物种发现、生态监测和保护工作方面具有潜在应用价值，但需要进一步解决细粒度分类的挑战。

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [113] [Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling](https://arxiv.org/abs/2508.16745)
*Ivan Rodkin,Daniil Orel,Konstantin Smirnov,Arman Bolatov,Bilal Elbouardi,Besher Hassan,Yuri Kuratov,Aydar Bulatov,Preslav Nakov,Timothy Baldwin,Artem Shelmanov,Mikhail Burtsev*

Main category: cs.LG

TL;DR: 该研究探索了不同架构和训练方法对模型多步推理能力的影响，发现在细胞自动机框架中，大多数神经网络能够学习抽象底层规则，但在多步推理任务上性能显著下降。增加模型深度、引入循环、记忆和测试时计算扩展可大幅提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何学习和执行多步推理仍然是一个开放性问题，研究旨在探索不同架构和训练方法对模型多步推理能力的影响。

Method: 在细胞自动机框架中，使用随机布尔函数生成状态序列进行训练，排除记忆化影响，测试不同神经网络架构在多步推理任务上的表现。

Result: 模型在下一状态预测上达到高准确率，但在多步推理任务上性能急剧下降。增加模型深度、引入循环机制、记忆和测试时计算扩展能显著提升推理能力。

Conclusion: 模型深度对于序列计算至关重要，通过扩展有效模型深度（包括循环、记忆和测试时计算）可以大幅增强模型的多步推理能力。

Abstract: Reasoning is a core capability of large language models, yet understanding
how they learn and perform multi-step reasoning remains an open problem. In
this study, we explore how different architectures and training methods affect
model multi-step reasoning capabilities within a cellular automata framework.
By training on state sequences generated with random Boolean functions for
random initial conditions to exclude memorization, we demonstrate that most
neural architectures learn to abstract the underlying rules. While models
achieve high accuracy in next-state prediction, their performance declines
sharply if multi-step reasoning is required. We confirm that increasing model
depth plays a crucial role for sequential computations. We demonstrate that an
extension of the effective model depth with recurrence, memory, and test-time
compute scaling substantially enhances reasoning capabilities.

</details>


### [114] [FAIRWELL: Fair Multimodal Self-Supervised Learning for Wellbeing Prediction](https://arxiv.org/abs/2508.16748)
*Jiaee Cheong,Abtin Mogharabin,Paul Liang,Hatice Gunes,Sinan Kalkan*

Main category: cs.LG

TL;DR: 通过自监督学习和多模态数据，提出FAIRWELL损失函数来学习更公平的表征，在保持分类性能的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习在改善机器学习公平性方面展现潜力，但在多模态设置中还未被探索。多模态数据包含模态独特信息，可以互补其他模态的信息。

Method: 提出FAIRWELL损失函数，基于VICReg方法：（1）方差项减少对保护属性的依赖；（2）不变项确保相似个体的一致预测；（3）协方差项最小化与保护属性的相关性。

Result: 在三个健康医疗数据集（D-Vlog、MIMIC、MODMA）上评估，框架提升了整体公平性表现，分类性能下降最小，显著改善了性能-公平性Pareto前沿。

Conclusion: FAIRWELL能够在多模态预测任务中学习主体独立表征，实现更好的公平性和性能平衡。

Abstract: Early efforts on leveraging self-supervised learning (SSL) to improve machine
learning (ML) fairness has proven promising. However, such an approach has yet
to be explored within a multimodal context. Prior work has shown that, within a
multimodal setting, different modalities contain modality-unique information
that can complement information of other modalities. Leveraging on this, we
propose a novel subject-level loss function to learn fairer representations via
the following three mechanisms, adapting the variance-invariance-covariance
regularization (VICReg) method: (i) the variance term, which reduces reliance
on the protected attribute as a trivial solution; (ii) the invariance term,
which ensures consistent predictions for similar individuals; and (iii) the
covariance term, which minimizes correlational dependence on the protected
attribute. Consequently, our loss function, coined as FAIRWELL, aims to obtain
subject-independent representations, enforcing fairness in multimodal
prediction tasks. We evaluate our method on three challenging real-world
heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain
different modalities of varying length and different prediction tasks. Our
findings indicate that our framework improves overall fairness performance with
minimal reduction in classification performance and significantly improves on
the performance-fairness Pareto frontier.

</details>


### [115] [DR-CircuitGNN: Training Acceleration of Heterogeneous Circuit Graph Neural Network on GPUs](https://arxiv.org/abs/2508.16769)
*Yuebo Luo,Shiyang Li,Junran Tao,Kiran Thorat,Xi Xie,Hongwu Peng,Nuo Xu,Caiwen Ding,Shaoyi Huang*

Main category: cs.LG

TL;DR: 通过利用行向稀疏感知的Dynamic-ReLU和SpMM内核优化，结合CPU-GPU并行处理策略，实现了异构图神经网络在EDA电路图数据集上的高效训练加速。


<details>
  <summary>Details</summary>
Motivation: 电路设计的复杂性增加导致EDA挑战，异构图神经网络（HGNNs）能更好地处理电路图，但其正向传播咄递传播的计算复杂度高，成为性能瓶颈。

Method: 设计了DR-CircuitGNN等速度优化方法：1）利用行向稀疏感知的Dynamic-ReLU咄SpMM内核优化；2）并行优化策略，通过多线程CPU初始化咄多个cudaStreams并发执行GPU内核。

Result: 在三个代表性CircuitNet设计上，正向传播咄递传播分别达到了3.51x咄4.09x的速度提升。在全尺寸CircuitNet咄Mini-CircuitNet上，并行设计实现了2.71x的速度提升，相关分数咄错误率影响微小。

Conclusion: DR-CircuitGNN通过等等优化技术，有效解决了HGNNs在EDA电路图数据集上的计算性能瓶颈，为复杂电路设计提供了高效的神经网络加速方案。

Abstract: The increasing scale and complexity of integrated circuit design have led to
increased challenges in Electronic Design Automation (EDA). Graph Neural
Networks (GNNs) have emerged as a promising approach to assist EDA design as
circuits can be naturally represented as graphs. While GNNs offer a foundation
for circuit analysis, they often fail to capture the full complexity of EDA
designs. Heterogeneous Graph Neural Networks (HGNNs) can better interpret EDA
circuit graphs as they capture both topological relationships and geometric
features. However, the improved representation capability comes at the cost of
even higher computational complexity and processing cost due to their serial
module-wise message-passing scheme, creating a significant performance
bottleneck. In this paper, we propose DR-CircuitGNN, a fast GPU kernel design
by leveraging row-wise sparsity-aware Dynamic-ReLU and optimizing SpMM kernels
during heterogeneous message-passing to accelerate HGNNs training on
EDA-related circuit graph datasets. To further enhance performance, we propose
a parallel optimization strategy that maximizes CPU-GPU concurrency by
concurrently processing independent subgraphs using multi-threaded CPU
initialization and GPU kernel execution via multiple cudaStreams. Our
experiments show that on three representative CircuitNet designs (small,
medium, large), the proposed method can achieve up to 3.51x and 4.09x speedup
compared to the SOTA for forward and backward propagation, respectively. On
full-size CircuitNet and sampled Mini-CircuitNet, our parallel design enables
up to 2.71x speed up over the official DGL implementation cuSPARSE with
negligible impact on correlation scores and error rates.

</details>


### [116] [Latent Graph Learning in Generative Models of Neural Signals](https://arxiv.org/abs/2508.16776)
*Nathan X. Kodama,Kenneth A. Loparo*

Main category: cs.LG

TL;DR: 该研究探索了从神经信号生成模型中学习潜在图结构的方法，通过数值模拟验证了提取的网络表示与真实连接性的对齐程度，发现共输入图表示有较强对齐性


<details>
  <summary>Details</summary>
Motivation: 推断神经信号中的时间交互图和高阶结构是构建系统神经科学生成模型的关键问题，但目前在大规模神经数据基础模型中提取可解释的潜在图表示仍然具有挑战性

Method: 通过测试已知真实连接性的神经回路数值模拟，评估了多种解释学习模型权重的假设，比较提取的网络表示与底层有向图的对齐程度

Result: 发现提取的网络表示与底层有向图存在适度对齐，而共输入图表示则表现出强烈的对齐性

Conclusion: 这些发现为在构建大规模神经数据基础模型时融入基于图的几何约束提供了路径和动机

Abstract: Inferring temporal interaction graphs and higher-order structure from neural
signals is a key problem in building generative models for systems
neuroscience. Foundation models for large-scale neural data represent shared
latent structures of neural signals. However, extracting interpretable latent
graph representations in foundation models remains challenging and unsolved.
Here we explore latent graph learning in generative models of neural signals.
By testing against numerical simulations of neural circuits with known
ground-truth connectivity, we evaluate several hypotheses for explaining
learned model weights. We discover modest alignment between extracted network
representations and the underlying directed graphs and strong alignment in the
co-input graph representations. These findings motivate paths towards
incorporating graph-based geometric constraints in the construction of
large-scale foundation models for neural data.

</details>


### [117] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: 量化对LLM内部表示影响的研究表明，量化对模型校准影响较小，死亡神经元数量保持稳定，不同模型对量化的敏感度存在差异，但总体不影响量化作为可靠模型压缩技术的使用


<details>
  <summary>Details</summary>
Motivation: 量化技术虽然在资源受限环境中部署LLM具有实用价值，但其对模型内部表示的影响尚未得到充分研究，引发了关于量化模型可靠性的疑问

Method: 使用多种可解释性技术分析4位和8位量化对多个LLM模型和神经元行为的影响，包括模型校准分析、神经元激活模式研究和神经元贡献度评估

Result: 量化对模型校准影响较小；死亡神经元数量在不同量化条件下保持稳定；小型全精度模型具有较少显著神经元，大型模型则更多（Llama-2-7B除外）；量化对神经元冗余的影响因模型而异

Conclusion: 量化效果因模型和任务而异，但未观察到任何剧烈变化，量化仍可作为可靠的模型压缩技术使用

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [118] [Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression](https://arxiv.org/abs/2508.16802)
*Baozhuo Su,Zhengxian Qu*

Main category: cs.LG

TL;DR: Anchor-MoE是一个处理概率和点回归的统一模型，使用锚点预测和混合密度网络专家实现异方差校正，理论证明达到最优风险率，实验显示在多个数据集上达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 回归问题在科学和工程中具有基础重要性，需要同时处理概率回归和点回归，现有方法在这两方面存在局限性

Method: 使用梯度提升模型提供锚点均值，通过可学习的度量窗口核评分局部性，软路由器将样本分配到混合密度网络专家进行异方差校正和预测方差估计

Result: 理论证明达到最小最大最优L2风险率，CRPS测试泛化差距呈对数缩放。在UCI回归基准上，Anchor-MoE在RMSE和NLL指标上匹配或超越NGBoost基线，多个数据集达到最先进水平

Conclusion: Anchor-MoE提供了一个统一框架来处理概率和点回归，具有理论保证和优异的实证性能，代码已开源

Abstract: Regression under uncertainty is fundamental across science and engineering.
We present an Anchored Mixture of Experts (Anchor-MoE), a model that handles
both probabilistic and point regression. For simplicity, we use a tuned
gradient-boosting model to furnish the anchor mean; however, any off-the-shelf
point regressor can serve as the anchor. The anchor prediction is projected
into a latent space, where a learnable metric-window kernel scores locality and
a soft router dispatches each sample to a small set of mixture-density-network
experts; the experts produce a heteroscedastic correction and predictive
variance. We train by minimizing negative log-likelihood, and on a disjoint
calibration split fit a post-hoc linear map on predicted means to improve point
accuracy. On the theory side, assuming a H\"older smooth regression function of
order~$\alpha$ and fixed Lipschitz partition-of-unity weights with bounded
overlap, we show that Anchor-MoE attains the minimax-optimal $L^2$ risk rate
$O\!\big(N^{-2\alpha/(2\alpha+d)}\big)$. In addition, the CRPS test
generalization gap scales as
$\widetilde{O}\!\Big(\sqrt{(\log(Mh)+P+K)/N}\Big)$; it is logarithmic in $Mh$
and scales as the square root in $P$ and $K$. Under bounded-overlap routing,
$K$ can be replaced by $k$, and any dependence on a latent dimension is
absorbed into $P$. Under uniformly bounded means and variances, an analogous
$\widetilde{O}\!\big(\sqrt{(\log(Mh)+P+K)/N}\big)$ scaling holds for the test
NLL up to constants. Empirically, across standard UCI regressions, Anchor-MoE
consistently matches or surpasses the strong NGBoost baseline in RMSE and NLL;
on several datasets it achieves new state-of-the-art probabilistic regression
results on our benchmark suite. Code is available at
https://github.com/BaozhuoSU/Probabilistic_Regression.

</details>


### [119] [Uncertainty Propagation Networks for Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.16815)
*Hadi Jahanshahi,Zheng H. Zhu*

Main category: cs.LG

TL;DR: UPN是一种新型神经微分方程，通过耦合的均值和协方差微分方程同时建模状态演化和不确定性传播，为连续时间建模提供原则性的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有神经ODE只能预测状态轨迹，缺乏不确定性量化能力。UPN旨在将不确定性传播自然地整合到连续时间建模中，提供更可靠的预测。

Method: 参数化耦合的均值和协方差微分方程，通过求解状态和协方差演化的耦合ODE来传播不确定性，支持状态依赖的可学习过程噪声，无需离散化处理非线性动力学。

Result: 实验证明UPN在多个领域有效：带不确定性量化的连续归一化流、具有良好校准置信区间的时间序列预测、稳定和混沌动力系统中的鲁棒轨迹预测。

Conclusion: UPN提供了一种连续深度建模框架，能够自适应输入复杂度，自然处理不规则采样观测，为神经微分方程提供了原则性的不确定性量化能力。

Abstract: This paper introduces Uncertainty Propagation Network (UPN), a novel family
of neural differential equations that naturally incorporate uncertainty
quantification into continuous-time modeling. Unlike existing neural ODEs that
predict only state trajectories, UPN simultaneously model both state evolution
and its associated uncertainty by parameterizing coupled differential equations
for mean and covariance dynamics. The architecture efficiently propagates
uncertainty through nonlinear dynamics without discretization artifacts by
solving coupled ODEs for state and covariance evolution while enabling
state-dependent, learnable process noise. The continuous-depth formulation
adapts its evaluation strategy to each input's complexity, provides principled
uncertainty quantification, and handles irregularly-sampled observations
naturally. Experimental results demonstrate UPN's effectiveness across multiple
domains: continuous normalizing flows (CNFs) with uncertainty quantification,
time-series forecasting with well-calibrated confidence intervals, and robust
trajectory prediction in both stable and chaotic dynamical systems.

</details>


### [120] [Understanding and Tackling Over-Dilution in Graph Neural Networks](https://arxiv.org/abs/2508.16829)
*Junhyun Lee,Veronika Thost,Bumsoo Kim,Jaewoo Kang,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文提出了MPNN中的过稀释问题，包括节点内属性级稀释和节点间表示级稀释，并引入了基于Transformer的解决方案来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: MPNN在图机器学习中占据重要地位，但存在过平滑和过挤压等问题。作者发现即使在单层中，节点特定信息也会显著稀释，这一现象之前被忽视。

Method: 提出了过稀释概念，用两个稀释因子（节点内属性级稀释和节点间节点级稀释）进行形式化，并引入了基于Transformer的解决方案来补充现有MPNN方法。

Result: 研究提供了对MPNN局限性的新见解，有助于开发更具信息性的图表示方法。提出的解决方案能够有效缓解过稀释问题。

Conclusion: 过稀释是MPNN中一个重要的被忽视问题，基于Transformer的方法可以有效补充现有节点嵌入技术，为构建更信息丰富的图表示提供了新方向。

Abstract: Message Passing Neural Networks (MPNNs) hold a key position in machine
learning on graphs, but they struggle with unintended behaviors, such as
over-smoothing and over-squashing, due to irregular data structures. The
observation and formulation of these limitations have become foundational in
constructing more informative graph representations. In this paper, we delve
into the limitations of MPNNs, focusing on aspects that have previously been
overlooked. Our observations reveal that even within a single layer, the
information specific to an individual node can become significantly diluted. To
delve into this phenomenon in depth, we present the concept of Over-dilution
and formulate it with two dilution factors: intra-node dilution for
attribute-level and inter-node dilution for node-level representations. We also
introduce a transformer-based solution that alleviates over-dilution and
complements existing node embedding methods like MPNNs. Our findings provide
new insights and contribute to the development of informative representations.
The implementation and supplementary materials are publicly available at
https://github.com/LeeJunHyun/NATR.

</details>


### [121] [Out of Distribution Detection for Efficient Continual Learning in Quality Prediction for Arc Welding](https://arxiv.org/abs/2508.16832)
*Yannik Hahn,Jan Voets,Antonin Koenigsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 基于VQ-VAE Transformer架构，利用其自回归损失作为OOD检测机制，结合持续学习策略，在动态焊接环境中实现稳健的质量预测和自适应更新。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的焊接质量预测模型在面对动态制造环境中的分布偏移时存在局限性，需要开发可靠的OOD检测和自适应更新机制。

Method: 扩展VQ-VAE Transformer架构，利用其自回归损失进行OOD检测，结合持续学习策略，仅在必要时触发模型更新，减少标注需求。

Result: 在真实焊接场景中验证，相比传统重建方法和嵌入误差技术，该方法在显著分布偏移下保持稳健的质量预测能力。

Conclusion: 该研究为动态制造过程提供了可解释且自适应的质量保证解决方案，是工业环境中稳健实用AI系统的重要进展。

Abstract: Modern manufacturing relies heavily on fusion welding processes, including
gas metal arc welding (GMAW). Despite significant advances in machine
learning-based quality prediction, current models exhibit critical limitations
when confronted with the inherent distribution shifts that occur in dynamic
manufacturing environments. In this work, we extend the VQ-VAE Transformer
architecture - previously demonstrating state-of-the-art performance in weld
quality prediction - by leveraging its autoregressive loss as a reliable
out-of-distribution (OOD) detection mechanism. Our approach exhibits superior
performance compared to conventional reconstruction methods, embedding
error-based techniques, and other established baselines. By integrating OOD
detection with continual learning strategies, we optimize model adaptation,
triggering updates only when necessary and thereby minimizing costly labeling
requirements. We introduce a novel quantitative metric that simultaneously
evaluates OOD detection capability while interpreting in-distribution
performance. Experimental validation in real-world welding scenarios
demonstrates that our framework effectively maintains robust quality prediction
capabilities across significant distribution shifts, addressing critical
challenges in dynamic manufacturing environments where process parameters
frequently change. This research makes a substantial contribution to applied
artificial intelligence by providing an explainable and at the same time
adaptive solution for quality assurance in dynamic manufacturing processes - a
crucial step towards robust, practical AI systems in the industrial
environment.

</details>


### [122] [Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience](https://arxiv.org/abs/2508.16836)
*Bicheng Wang,Junping Wang,Yibo Xue*

Main category: cs.LG

TL;DR: 本文提出一种物理信息神经符号方法来描述复杂网络的进化动力学，用于预测产业链弹性。通过学习物理实体活动状态动力学并整合到多层时空协进化网络，实现了更准确有效的弹性预测。


<details>
  <summary>Details</summary>
Motivation: 产业链在国民经济可持续发展中越来越重要，但数据驱动的深度学习在复杂网络弹性分析中仍处于初级阶段，核心问题是缺乏描述系统动力学的理论框架。

Method: 提出物理信息神经符号方法，学习物理实体活动状态的动力学并整合到多层时空协进化网络中，使用物理信息方法实现物理符号动力学和时空协进化拓扑结构的联合学习。

Result: 实验结果显示，该模型能够获得更好的结果，更准确有效地预测产业链弹性，对产业发展具有一定的实践意义。

Conclusion: 该研究为复杂网络弹性分析提供了一种有效的物理信息神经符号方法，通过整合动力学模型和时空协进化网络，实现了更准确的产业链弹性预测，对产业可持续发展具有重要价值。

Abstract: Industrial chain plays an increasingly important role in the sustainable
development of national economy. However, as a typical complex network,
data-driven deep learning is still in its infancy in describing and analyzing
the resilience of complex networks, and its core is the lack of a theoretical
framework to describe the system dynamics. In this paper, we propose a
physically informative neural symbolic approach to describe the evolutionary
dynamics of complex networks for resilient prediction. The core idea is to
learn the dynamics of the activity state of physical entities and integrate it
into the multi-layer spatiotemporal co-evolution network, and use the physical
information method to realize the joint learning of physical symbol dynamics
and spatiotemporal co-evolution topology, so as to predict the industrial chain
resilience. The experimental results show that the model can obtain better
results and predict the elasticity of the industry chain more accurately and
effectively, which has certain practical significance for the development of
the industry.

</details>


### [123] [Neural Contrast Expansion for Explainable Structure-Property Prediction and Random Microstructure Design](https://arxiv.org/abs/2508.16857)
*Guangyu Nie,Yang Jiao,Yi Ren*

Main category: cs.LG

TL;DR: 提出Neural Contrast Expansion (NCE)方法，结合强对比展开理论和神经网络，为复合材料有效性质预测提供既高效又可解释的解决方案


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解方法计算成本高但可解释性好，数据驱动方法成本低但可解释性差。需要一种既能保持计算效率又能提供可解释敏感性分析的方法

Method: 基于强对比展开(SCE)理论，提出NCE架构，从结构-性质数据中学习替代PDE核函数，仅需宏观性质测量而不需要PDE解场信息

Result: 在静态传导和电磁波传播案例中，NCE模型显示出准确且具有洞察力的敏感性信息，可用于材料设计

Conclusion: NCE方法为复合材料有效性质预测提供了一种既成本效益高又具有良好可解释性的解决方案，特别适用于材料开发场景

Abstract: Effective properties of composite materials are defined as the ensemble
average of property-specific PDE solutions over the underlying microstructure
distributions. Traditionally, predicting such properties can be done by solving
PDEs derived from microstructure samples or building data-driven models that
directly map microstructure samples to properties. The former has a higher
running cost, but provides explainable sensitivity information that may guide
material design; the latter could be more cost-effective if the data overhead
is amortized, but its learned sensitivities are often less explainable. With a
focus on properties governed by linear self-adjoint PDEs (e.g., Laplace,
Helmholtz, and Maxwell curl-curl) defined on bi-phase microstructures, we
propose a structure-property model that is both cost-effective and explainable.
Our method is built on top of the strong contrast expansion (SCE) formalism,
which analytically maps $N$-point correlations of an unbounded random field to
its effective properties. Since real-world material samples have finite sizes
and analytical PDE kernels are not always available, we propose Neural Contrast
Expansion (NCE), an SCE-inspired architecture to learn surrogate PDE kernels
from structure-property data. For static conduction and electromagnetic wave
propagation cases, we show that NCE models reveal accurate and insightful
sensitivity information useful for material design. Compared with other PDE
kernel learning methods, our method does not require measurements about the PDE
solution fields, but rather only requires macroscopic property measurements
that are more accessible in material development contexts.

</details>


### [124] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种无监督的图匹配框架，通过伪坐标、自适应相似度平衡和分块后处理，实现了大规模地图数据的高精度匹配，在噪声和大规模场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 地图匹配面临缺乏真实对应关系、节点特征稀疏和可扩展性需求等挑战，传统方法难以处理大规模无标注地图数据。

Method: 采用无监督学习框架，引入伪坐标捕捉空间布局，设计自适应特征-几何相似度平衡机制和几何一致性损失函数，并使用分块后处理实现并行计算。

Result: 在真实数据集上达到最先进的匹配精度，大幅超越现有方法，特别是在高噪声和大规模场景下表现突出。

Conclusion: 该框架为地图对齐提供了可扩展的实用解决方案，是传统方法的鲁棒高效替代方案。

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>


### [125] [Quantifying Out-of-Training Uncertainty of Neural-Network based Turbulence Closures](https://arxiv.org/abs/2508.16891)
*Cody Grogan,Som Dhulipala,Mauricio Tano,Izabela Gutowska,Som Dutta*

Main category: cs.LG

TL;DR: 本文比较了三种神经网络不确定性量化方法（深度集成、蒙特卡洛dropout、随机变分推断）与高斯过程在湍流闭合模型中的表现，发现高斯过程精度最高但计算成本昂贵，深度集成在计算效率和不确定性量化方面表现均衡。


<details>
  <summary>Details</summary>
Motivation: 神经网络湍流闭合模型缺乏不确定性量化能力，特别是在训练数据之外的输入情况下，这限制了其在CFD模拟中的广泛应用。

Method: 使用已发布的代数湍流闭合模型，比较深度集成(DE)、蒙特卡洛dropout(MCD)、随机变分推断(SVI)和高斯过程(GP)四种方法在训练域内外的表现。

Result: 训练域内GP精度最高(RMSE=2.14e-5)，DE次之(4.59e-4)；训练域外GP和DE表现相近；在不确定性量化方面，SVI和DE在某些情况下校准误差最小，DE在负对数似然方面表现最佳。

Conclusion: GP精度最高但计算成本高(O(n^3))，DE在计算效率和不确定性量化方面提供了相对稳健和直观的估计，是实用的折中选择。

Abstract: Neural-Network (NN) based turbulence closures have been developed for being
used as pre-trained surrogates for traditional turbulence closures, with the
aim to increase computational efficiency and prediction accuracy of CFD
simulations. The bottleneck to the widespread adaptation of these ML-based
closures is the relative lack of uncertainty quantification (UQ) for these
models. Especially, quantifying uncertainties associated with out-of-training
inputs, that is when the ML-based turbulence closures are queried on inputs
outside their training data regime. In the current paper, a published algebraic
turbulence closure1 has been utilized to compare the quality of epistemic UQ
between three NN-based methods and Gaussian Process (GP). The three NN-based
methods explored are Deep Ensembles (DE), Monte-Carlo Dropout (MCD), and
Stochastic Variational Inference (SVI). In the in-training results, we find the
exact GP performs the best in accuracy with a Root Mean Squared Error (RMSE) of
$2.14 \cdot 10^{-5}$ followed by the DE with an RMSE of $4.59 \cdot 10^{-4}$.
Next, the paper discusses the performance of the four methods for quantifying
out-of-training uncertainties. For performance, the Exact GP yet again is the
best in performance, but has similar performance to the DE in the
out-of-training regions. In UQ accuracy for the out-of-training case, SVI and
DE hold the best miscalibration error for one of the cases. However, the DE
performs the best in Negative Log-Likelihood for both out-of-training cases. We
observe that for the current problem, in terms of accuracy GP > DE > SV I >
MCD. The DE results are relatively robust and provide intuitive UQ estimates,
despite performing naive ensembling. In terms of computational cost, the GP is
significantly higher than the NN-based methods with a $O(n^3)$ computational
complexity for each training step

</details>


### [126] [Tri-Accel: Curvature-Aware Precision-Adaptive and Memory-Elastic Optimization for Efficient GPU Usage](https://arxiv.org/abs/2508.16905)
*Mohsen Sheibanian,Pouya Shaeri,Alimohammad Beigi,Ryan T. Woo,Aryan Keluskar*

Main category: cs.LG

TL;DR: Tri-Accel是一个统一的优化框架，通过协同调整三种加速策略（自适应精度更新、稀疏二阶信号、内存弹性批处理缩放）来减少神经网络训练的时间和内存消耗，同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在优化成本方面面临GPU内存和计算时间的瓶颈，现有加速技术通常孤立使用，需要统一的协同优化方案。

Method: Tri-Accel框架包含：(1)基于曲率和梯度方差动态分配混合精度级别的精度自适应更新；(2)利用Hessian/Fisher稀疏模式指导精度和步长决策的稀疏二阶信号；(3)根据VRAM可用性实时调整批处理大小的内存弹性批处理缩放。

Result: 在CIFAR-10上使用ResNet-18和EfficientNet-B0，Tri-Accel实现了训练时间减少9.9%，内存使用降低13.3%，准确率比FP32基线提高1.1个百分点。与静态混合精度训练相比，在保持78.1%准确率的同时将内存占用从0.35GB降至0.31GB。

Conclusion: 该工作展示了算法自适应性和硬件感知如何结合，在资源受限环境下提高可扩展性，为边缘设备和成本敏感云部署提供更高效的神经网络训练方案。

Abstract: Deep neural networks are increasingly bottlenecked by the cost of
optimization, both in terms of GPU memory and compute time. Existing
acceleration techniques, such as mixed precision, second-order methods, and
batch size scaling, are typically used in isolation. We present Tri-Accel, a
unified optimization framework that co-adapts three acceleration strategies
along with adaptive parameters during training: (1) Precision-Adaptive Updates
that dynamically assign mixed-precision levels to layers based on curvature and
gradient variance; (2) Sparse Second-Order Signals that exploit Hessian/Fisher
sparsity patterns to guide precision and step size decisions; and (3)
Memory-Elastic Batch Scaling that adjusts batch size in real time according to
VRAM availability. On CIFAR-10 with ResNet-18 and EfficientNet-B0, Tri-Accel
achieves up to 9.9% reduction in training time and 13.3% lower memory usage,
while improving accuracy by +1.1 percentage points over FP32 baselines. Tested
on CIFAR-10/100, our approach demonstrates adaptive learning behavior, with
efficiency gradually improving over the course of training as the system learns
to allocate resources more effectively. Compared to static mixed-precision
training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint
from 0.35GB to 0.31GB on standard hardware. The framework is implemented with
custom Triton kernels, whose hardware-aware adaptation enables automatic
optimization without manual hyperparameter tuning, making it practical for
deployment across diverse computational environments. This work demonstrates
how algorithmic adaptivity and hardware awareness can be combined to improve
scalability in resource-constrained settings, paving the way for more efficient
neural network training on edge devices and cost-sensitive cloud deployments.

</details>


### [127] [Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection](https://arxiv.org/abs/2508.16915)
*Sadman Mohammad Nasif,Md Abrar Jahin,M. F. Mridha*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的模块化框架，结合直观刷新神经网络和强化学习指导的超参数优化，实现了高准确性、公平性和可解释性的家庭银行欺诈检测。


<details>
  <summary>Details</summary>
Motivation: 随着家庭银行系统的普及，网络欺诈风险增加，需要既准确又公平可解释的欺诈检测机制。现有AI模型在计算效率、可解释性和超参数优化稳定性方面遇到挑战。

Method: 提出了一种新框架，整合直观刷新神经网络与人群编码（CSNPC）以及强化学习指导的超策略优化器（RHOSS）。该系统包含了可解释AI技术，如显著性分配和刷新活动分析。

Result: 在BAF数据集上达到90.8%的回归率，仅5%假正率，超越现有最优的刷新和非刷新模型。在关键人口属性上保持了98%以上的预测平等性。

Conclusion: 该研究证明了结合人群编码刷新神经网络与强化学习指导超策略的潜力，能够在实际金融应用中实现公平、透明且高性能的欺诈检测。

Abstract: The growing adoption of home banking systems has heightened the risk of
cyberfraud, necessitating fraud detection mechanisms that are not only accurate
but also fair and explainable. While AI models have shown promise in this
domain, they face key limitations, including computational inefficiency, the
interpretability challenges of spiking neural networks (SNNs), and the
complexity and convergence instability of hyper-heuristic reinforcement
learning (RL)-based hyperparameter optimization. To address these issues, we
propose a novel framework that integrates a Cortical Spiking Network with
Population Coding (CSNPC) and a Reinforcement-Guided Hyper-Heuristic Optimizer
for Spiking Systems (RHOSS). The CSNPC, a biologically inspired SNN, employs
population coding for robust classification, while RHOSS uses Q-learning to
dynamically select low-level heuristics for hyperparameter optimization under
fairness and recall constraints. Embedded within the Modular Supervisory
Framework for Spiking Network Training and Interpretation (MoSSTI), the system
incorporates explainable AI (XAI) techniques, specifically, saliency-based
attribution and spike activity profiling, to increase transparency. Evaluated
on the Bank Account Fraud (BAF) dataset suite, our model achieves a $90.8\%$
recall at a strict $5\%$ false positive rate (FPR), outperforming
state-of-the-art spiking and non-spiking models while maintaining over $98\%$
predictive equality across key demographic attributes. The explainability
module further confirms that saliency attributions align with spiking dynamics,
validating interpretability. These results demonstrate the potential of
combining population-coded SNNs with reinforcement-guided hyper-heuristics for
fair, transparent, and high-performance fraud detection in real-world financial
applications.

</details>


### [128] [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
*Junxuan Wang,Xuyang Ge,Wentao Shu,Zhengfu He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 研究发现Transformer注意力输出实际上存在于低维子空间，这导致稀疏字典学习中普遍存在的死特征问题。作者提出子空间约束训练方法，将死特征从87%降至1%以下。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然在高维隐藏空间中运行，但注意力输出实际上被限制在低维子空间中，这种低秩结构是稀疏字典学习中死特征问题的根本原因。

Method: 提出子空间约束训练方法，将特征方向初始化到激活的活跃子空间中，适用于注意力输出稀疏自编码器和其他稀疏字典学习方法。

Result: 在包含100万个特征的注意力输出SAE中，死特征比例从87%降至1%以下，方法可扩展到其他稀疏字典学习技术。

Conclusion: 研究不仅揭示了注意力机制的几何特性，还为改进大型语言模型中的稀疏字典学习提供了实用工具，解决了死特征这一关键问题。

Abstract: While transformer models are widely believed to operate in high-dimensional
hidden spaces, we show that attention outputs are confined to a surprisingly
low-dimensional subspace, where about 60\% of the directions account for 99\%
of the variance--a phenomenon that is induced by the attention output
projection matrix and consistently observed across diverse model families and
datasets. Critically, we find this low-rank structure as a fundamental cause of
the prevalent dead feature problem in sparse dictionary learning, where it
creates a mismatch between randomly initialized features and the intrinsic
geometry of the activation space. Building on this insight, we propose a
subspace-constrained training method for sparse autoencoders (SAEs),
initializing feature directions into the active subspace of activations. Our
approach reduces dead features from 87\% to below 1\% in Attention Output SAEs
with 1M features, and can further extend to other sparse dictionary learning
methods. Our findings provide both new insights into the geometry of attention
and practical tools for improving sparse dictionary learning in large language
models.

</details>


### [129] [Degree of Staleness-Aware Data Updating in Federated Learning](https://arxiv.org/abs/2508.16931)
*Tao Liu,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出了DUFL激励机制，通过服务器支付、过时数据保留率和客户端新鲜数据收集量三个参数来协调联邦学习中数据陈旧性和数据量的平衡，以优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据陈旧性对时间敏感任务性能影响显著，现有工作仅关注数据更新频率或客户端选择策略，未同时考虑数据陈旧性和数据量。

Method: 引入DoS指标量化数据陈旧性，建立两阶段Stackelberg博弈模型，推导客户端最优本地数据更新策略和服务器近似最优策略。

Result: 在真实数据集上的实验结果表明该方法具有显著性能优势。

Conclusion: DUFL机制能有效协调数据陈旧性和数据量的平衡，提升联邦学习在时间敏感任务中的性能表现。

Abstract: Handling data staleness remains a significant challenge in federated learning
with highly time-sensitive tasks, where data is generated continuously and data
staleness largely affects model performance. Although recent works attempt to
optimize data staleness by determining local data update frequency or client
selection strategy, none of them explore taking both data staleness and data
volume into consideration. In this paper, we propose DUFL(Data Updating in
Federated Learning), an incentive mechanism featuring an innovative local data
update scheme manipulated by three knobs: the server's payment, outdated data
conservation rate, and clients' fresh data collection volume, to coordinate
staleness and volume of local data for best utilities. To this end, we
introduce a novel metric called DoS(the Degree of Staleness) to quantify data
staleness and conduct a theoretic analysis illustrating the quantitative
relationship between DoS and model performance. We model DUFL as a two-stage
Stackelberg game with dynamic constraint, deriving the optimal local data
update strategy for each client in closed-form and the approximately optimal
strategy for the server. Experimental results on real-world datasets
demonstrate the significant performance of our approach.

</details>


### [130] [Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter](https://arxiv.org/abs/2508.16939)
*Lei Jiang,Wen Ge,Niels Cariou-Kotlarek,Mingxuan Yi,Po-Yu Chen,Lingyi Yang,Francois Buet-Golfouse,Gaurav Mittal,Hao Ni*

Main category: cs.LG

TL;DR: Sig-DEG是一种基于签名的扩散模型蒸馏方法，通过部分签名高效总结布朗运动，将推理步骤减少一个数量级的同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中达到最先进效果，但推理时计算密集，通常需要数千个离散化步骤，需要高效的推理方法

Method: 利用随机微分方程的高阶近似，采用部分签名来高效总结子区间上的布朗运动，使用循环结构实现SDE解的全局近似，通过监督学习进行蒸馏

Result: 实验表明Sig-DEG在减少推理步骤一个数量级的同时，实现了有竞争力的生成质量

Conclusion: 基于签名的近似方法在高效生成建模中具有有效性

Abstract: Diffusion models have achieved state-of-the-art results in generative
modelling but remain computationally intensive at inference time, often
requiring thousands of discretization steps. To this end, we propose Sig-DEG
(Signature-based Differential Equation Generator), a novel generator for
distilling pre-trained diffusion models, which can universally approximate the
backward diffusion process at a coarse temporal resolution. Inspired by
high-order approximations of stochastic differential equations (SDEs), Sig-DEG
leverages partial signatures to efficiently summarize Brownian motion over
sub-intervals and adopts a recurrent structure to enable accurate global
approximation of the SDE solution. Distillation is formulated as a supervised
learning task, where Sig-DEG is trained to match the outputs of a
fine-resolution diffusion model on a coarse time grid. During inference,
Sig-DEG enables fast generation, as the partial signature terms can be
simulated exactly without requiring fine-grained Brownian paths. Experiments
demonstrate that Sig-DEG achieves competitive generation quality while reducing
the number of inference steps by an order of magnitude. Our results highlight
the effectiveness of signature-based approximations for efficient generative
modeling.

</details>


### [131] [Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning](https://arxiv.org/abs/2508.16949)
*Yang Zhou,Sunzhu Li,Shunyu Liu,Wenkai Fang,Jiale Zhao,Jingwen Yang,Jianwei Lv,Kongcheng Zhang,Yihe Zhou,Hengtong Lu,Wei Chen,Yan Xie,Mingli Song*

Main category: cs.LG

TL;DR: RuscaRL通过引入清单式评分标准作为探索支架和可验证奖励，打破LLM推理中的探索瓶颈，显著提升模型在各种基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习面临探索困境：高质量样本的获取受限于模型本身的能力，形成'无法探索就无法学习'的恶性循环。

Method: 提出RuscaRL框架：(1)使用清单式评分标准作为显式探索支架，在生成过程中提供外部指导；(2)将评分标准作为参考依据获得可靠的LLM-as-a-Judge奖励分数；(3)随时间逐渐衰减指导，促使模型内化推理模式。

Result: 在多个基准测试中表现优异，显著提升模型性能：Qwen-2.5-7B-Instruct在HealthBench-500上从23.6提升至50.3，超越GPT-4.1；Qwen3-30B-A3B-Instruct达到61.1，超越OpenAI-o3等领先LLM。

Conclusion: RuscaRL有效解决了LLM强化学习中的探索瓶颈问题，通过评分标准支架和可验证奖励机制，成功扩展了模型推理能力的边界。

Abstract: Recent advances in Large Language Models (LLMs) have underscored the
potential of Reinforcement Learning (RL) to facilitate the emergence of
reasoning capabilities. Despite the encouraging results, a fundamental dilemma
persists as RL improvement relies on learning from high-quality samples, yet
the exploration for such samples remains bounded by the inherent limitations of
LLMs. This, in effect, creates an undesirable cycle in which what cannot be
explored cannot be learned. In this work, we propose Rubric-Scaffolded
Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework
designed to break the exploration bottleneck for general LLM reasoning.
Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit
scaffolding for exploration during rollout generation, where different rubrics
are provided as external guidance within task instructions to steer diverse
high-quality responses. This guidance is gradually decayed over time,
encouraging the model to internalize the underlying reasoning patterns; (2)
verifiable rewards for exploitation during model training, where we can obtain
robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL
on general reasoning tasks. Extensive experiments demonstrate the superiority
of the proposed RuscaRL across various benchmarks, effectively expanding
reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL
significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,
surpassing GPT-4.1. Furthermore, our fine-tuned variant on
Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading
LLMs including OpenAI-o3.

</details>


### [132] [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
*Manan Gupta,Dhruv Kumar*

Main category: cs.LG

TL;DR: 提出了Polysemanticity Index (PSI)指标，用于量化神经网络中多义神经元的语义聚类程度，发现深层网络比浅层网络具有更高的多义性。


<details>
  <summary>Details</summary>
Motivation: 神经网络中存在多义神经元，这些神经元对多个语义不相关的特征都有响应，这给机制可解释性研究带来了挑战，需要一种量化方法来识别和研究这类神经元。

Method: PSI指标包含三个校准组件：几何聚类质量(S)、与标注类别的对齐度(Q)、通过CLIP的开放词汇语义区分度(D)。在预训练的ResNet-50上使用Tiny-ImageNet图像进行评估，并进行鲁棒性检验和因果干预实验。

Result: PSI成功识别出激活集可分解为连贯、可命名原型的神经元，发现深层网络比浅层网络具有显著更高的PSI值。对齐补丁替换比非对齐、随机、位置打乱或消融控制组更能显著增加目标神经元激活。

Conclusion: PSI提供了一个原则性和实用性的工具，用于发现、量化和研究神经网络中的多义单元，有助于推进神经网络的机制可解释性研究。

Abstract: Neural networks often contain polysemantic neurons that respond to multiple,
sometimes unrelated, features, complicating mechanistic interpretability. We
introduce the Polysemanticity Index (PSI), a null-calibrated metric that
quantifies when a neuron's top activations decompose into semantically distinct
clusters. PSI multiplies three independently calibrated components: geometric
cluster quality (S), alignment to labeled categories (Q), and open-vocabulary
semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with
Tiny-ImageNet images, PSI identifies neurons whose activation sets split into
coherent, nameable prototypes, and reveals strong depth trends: later layers
exhibit substantially higher PSI than earlier layers. We validate our approach
with robustness checks (varying hyperparameters, random seeds, and
cross-encoder text heads), breadth analyses (comparing class-only vs.
open-vocabulary concepts), and causal patch-swap interventions. In particular,
aligned patch replacements increase target-neuron activation significantly more
than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI
thus offers a principled and practical lever for discovering, quantifying, and
studying polysemantic units in neural networks.

</details>


### [133] [Unveiling the Latent Directions of Reflection in Large Language Models](https://arxiv.org/abs/2508.16989)
*Fu-Chieh Chang,Yu-Ting Lee,Pei-Yuan Wu*

Main category: cs.LG

TL;DR: 本文通过激活导向技术研究大语言模型反思机制的内在机理，发现可以通过激活干预系统性地控制反思行为，抑制反思比激发更容易


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注反思提示策略或强化学习目标，但对反思的内在机制探索不足，需要从模型激活的潜在方向来深入理解反思的工作原理

Method: 基于激活导向的方法，构建不同反思意图（无反思、内在反思、触发反思）之间的导向向量，通过激活干预来增强或抑制反思行为

Result: 在GSM8k-adv数据集上的实验显示，不同反思层次存在明显分层，激活干预证实了反思的可控性，抑制反思比激发反思更容易实现

Conclusion: 这项工作为理解LLMs中反思推理的机制开辟了路径，既展示了增强反思防御的机会，也揭示了对抗性抑制反思在越狱攻击中的风险

Abstract: Reflection, the ability of large language models (LLMs) to evaluate and
revise their own reasoning, has been widely used to improve performance on
complex reasoning tasks. Yet, most prior work emphasizes designing reflective
prompting strategies or reinforcement learning objectives, leaving the inner
mechanisms of reflection underexplored. In this paper, we investigate
reflection through the lens of latent directions in model activations. We
propose a methodology based on activation steering to characterize how
instructions with different reflective intentions: no reflection, intrinsic
reflection, and triggered reflection. By constructing steering vectors between
these reflection levels, we demonstrate that (1) new reflection-inducing
instructions can be systematically identified, (2) reflective behavior can be
directly enhanced or suppressed through activation interventions, and (3)
suppressing reflection is considerably easier than stimulating it. Experiments
on GSM8k-adv with Qwen2.5-3B and Gemma3-4B reveal clear stratification across
reflection levels, and steering interventions confirm the controllability of
reflection. Our findings highlight both opportunities (e.g.,
reflection-enhancing defenses) and risks (e.g., adversarial inhibition of
reflection in jailbreak attacks). This work opens a path toward mechanistic
understanding of reflective reasoning in LLMs.

</details>


### [134] [Online Learning for Approximately-Convex Functions with Long-term Adversarial Constraints](https://arxiv.org/abs/2508.16992)
*Dhruv Sarkar,Samrat Mukhopadhyay,Abhishek Sinha*

Main category: cs.LG

TL;DR: 本文研究具有长期预算约束的在线学习问题，提出了一种高效的一阶在线算法，在完全信息和bandit反馈设置下都能实现O(√T)的α-regret，同时资源消耗最多为O(B_T log T) + ˜O(√T)。


<details>
  <summary>Details</summary>
Motivation: 研究在对抗性设置下具有长期预算约束的在线学习问题，特别是针对α-近似凸函数这一广泛函数类，该类函数推广了凸性并包含许多常见的非凸优化问题。

Method: 提出一种高效的一阶在线算法，该算法在完全信息和bandit反馈设置下都能工作。算法通过处理α-近似凸函数来最小化累积成本，同时近似满足长期预算约束。

Result: 算法保证相对于最优固定可行基准的O(√T) α-regret，同时最多消耗O(B_T log T) + ˜O(√T)资源。在bandit反馈设置中，为"Adversarial Bandits with Knapsacks"问题提供了改进保证的有效解决方案。

Conclusion: 证明了匹配的下界，展示了结果的紧性。刻画了α-近似凸函数类，表明结果适用于广泛的优化问题家族，包括DR-次模最大化、在线顶点覆盖和正则化相位恢复等常见非凸问题。

Abstract: We study an online learning problem with long-term budget constraints in the
adversarial setting. In this problem, at each round $t$, the learner selects an
action from a convex decision set, after which the adversary reveals a cost
function $f_t$ and a resource consumption function $g_t$. The cost and
consumption functions are assumed to be $\alpha$-approximately convex - a broad
class that generalizes convexity and encompasses many common non-convex
optimization problems, including DR-submodular maximization, Online Vertex
Cover, and Regularized Phase Retrieval. The goal is to design an online
algorithm that minimizes cumulative cost over a horizon of length $T$ while
approximately satisfying a long-term budget constraint of $B_T$. We propose an
efficient first-order online algorithm that guarantees $O(\sqrt{T})$
$\alpha$-regret against the optimal fixed feasible benchmark while consuming at
most $O(B_T \log T)+ \tilde{O}(\sqrt{T})$ resources in both full-information
and bandit feedback settings. In the bandit feedback setting, our approach
yields an efficient solution for the $\texttt{Adversarial Bandits with
Knapsacks}$ problem with improved guarantees. We also prove matching lower
bounds, demonstrating the tightness of our results. Finally, we characterize
the class of $\alpha$-approximately convex functions and show that our results
apply to a broad family of problems.

</details>


### [135] [Learned Structure in CARTRIDGES: Keys as Shareable Routers in Self-Studied Representations](https://arxiv.org/abs/2508.17032)
*Maurizio Diaz*

Main category: cs.LG

TL;DR: CARTRIDGE方法通过离线训练大幅压缩KV缓存（最多减少40倍内存使用），本文首次机制性分析发现：键向量作为稳定的检索路由器，值向量承担主要压缩功能，并提出SCI初始化方法加速收敛。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理的瓶颈在于KV缓存线性增长，需要探索CARTRIDGE压缩机制的内部工作原理以优化性能。

Method: 对CARTRIDGE键值缓存进行机制性分析，提出键向量作为检索路由器的理论，并通过跨任务、模型家族和大小的实证验证，同时提出Sampled Chunk Initialization初始化方法。

Result: 实证证明CARTRIDGE键向量在不同任务间可互换而性能损失很小，值向量承担主要压缩功能，SCI初始化能加速训练收敛。

Conclusion: 研究为CARTRIDGE训练优化奠定了实证基础，对进一步扩展长上下文处理能力至关重要。

Abstract: A bottleneck for long-context LLM inference is the linearly growing KV cache.
Recent work has proposed CARTRIDGES, an approach which leverages offline
compute to train a much smaller KV cache than is typically required for a full
document (up to 40x less memory usage at inference time). In this paper, we
present the first mechanistic exploration of the learned CARTRIDGE key-value
cache structure. In particular, we propose that (1) CARTRIDGE keys act as
stable, shareable retrieval routers for the compressed corpora and (2) most of
the learned compression occurs within the CARTRIDGE value vectors. We present
empirical evidence of our routing theory across tasks, model families, and
model sizes; for example, we can ablate the learned CARTRIDGE key vectors
between tasks with little performance loss. Finally, we propose a slight
improvement in initialization called Sampled Chunk Initialization (SCI). We
suggest that SCI can lead to faster CARTRIDGE convergence than previously
demonstrated in the literature. Our findings lay the groundwork for broader
empirical study of CARTRIDGE training optimization which may be crucial for
further scaling.

</details>


### [136] [TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression](https://arxiv.org/abs/2508.17056)
*Kiran Madhusudhanan,Vijaya Krishna Yalavarthi,Jonas Sonntag,Maximilian Stubbemann,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: TabResFlow是一个基于归一化样条流的表格回归模型，通过灵活的密度估计方法解决了传统概率回归模型对固定形状分布的局限性，在多个基准数据集上表现出优越的似然分数和推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法主要关注点估计，容易产生过度自信的预测，这在需要可信决策的工业自动化中尤为关键。虽然概率回归模型能够建模预测不确定性，但许多传统方法假设固定形状分布（通常是高斯分布），这种假设在现实世界复杂目标分布中过于限制。

Method: TabResFlow包含三个关键组件：(1)每个数值特征的MLP编码器；(2)用于表达性特征提取的全连接ResNet主干网络；(3)基于条件样条的归一化流，用于灵活且可处理的密度估计。

Result: 在9个公共基准数据集上，TabResFlow在似然分数上持续超越现有概率回归模型，相比最强的概率回归模型(TreeFlow)提升9.64%，相比最强的深度学习替代方案(NodeFlow)推理速度平均提升5.6倍。在实际二手车价格预测任务中，TabResFlow在新提出的AURC指标上表现出优越结果。

Conclusion: TabResFlow通过归一化样条流方法有效解决了表格回归中的概率建模问题，提供了更灵活和准确的预测不确定性估计，在性能和效率方面都优于现有方法，具有重要的工业应用价值。

Abstract: Tabular regression is a well-studied problem with numerous industrial
applications, yet most existing approaches focus on point estimation, often
leading to overconfident predictions. This issue is particularly critical in
industrial automation, where trustworthy decision-making is essential.
Probabilistic regression models address this challenge by modeling prediction
uncertainty. However, many conventional methods assume a fixed-shape
distribution (typically Gaussian), and resort to estimating distribution
parameters. This assumption is often restrictive, as real-world target
distributions can be highly complex. To overcome this limitation, we introduce
TabResFlow, a Normalizing Spline Flow model designed specifically for
univariate tabular regression, where commonly used simple flow networks like
RealNVP and Masked Autoregressive Flow (MAF) are unsuitable. TabResFlow
consists of three key components: (1) An MLP encoder for each numerical
feature. (2) A fully connected ResNet backbone for expressive feature
extraction. (3) A conditional spline-based normalizing flow for flexible and
tractable density estimation. We evaluate TabResFlow on nine public benchmark
datasets, demonstrating that it consistently surpasses existing probabilistic
regression models on likelihood scores. Our results demonstrate 9.64%
improvement compared to the strongest probabilistic regression model
(TreeFlow), and on average 5.6 times speed-up in inference time compared to the
strongest deep learning alternative (NodeFlow). Additionally, we validate the
practical applicability of TabResFlow in a real-world used car price prediction
task under selective regression. To measure performance in this setting, we
introduce a novel Area Under Risk Coverage (AURC) metric and show that
TabResFlow achieves superior results across this metric.

</details>


### [137] [Learning ON Large Datasets Using Bit-String Trees](https://arxiv.org/abs/2508.17083)
*Prashant Gupta*

Main category: cs.LG

TL;DR: 这篇论文提出了三种计算方法：ComBI哈希算法提高近似最近邻搜索效率，GRAF分类器结合全局和局部划分，CRCS深度学习框架将基因变异嵌入数值空间。这些方法在大规模数据分析和生物医学应用中显示出高效、可扩展和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决标准空间划分哈希方法依赖二叉搜索树导致的指数增长和稀疏性问题，以及大规模生物医学数据分析中的效率和解释性挑战。

Method: 1. ComBI：压缩倒排哈希表的二叉搜索树，减少内存占用并提高搜索速度
2. GRAF：导向随机森林分类器，集成全局和局部划分，缩小泛化误差
3. CRCS：连续码子开关表示框架，将基因变异嵌入数值向量空间

Result: 在10亿样本数据集上，ComBI达到0.90精度，速度提升4-296倍；GRAF在115个数据集上展现竞争力或更优的准确性；CRCS在脑、肝脏、肺癌等多种癌症中验证了生存预测能力，能够识别驱动基因和评估肝癌变异。

Conclusion: 这些方法为大规模数据分析和生物医学应用提供了高效、可扩展和可解释的工具集合，在哈希搜索、分类预测和癌症生存分析方面都取得了显著成效。

Abstract: This thesis develops computational methods in similarity-preserving hashing,
classification, and cancer genomics. Standard space partitioning-based hashing
relies on Binary Search Trees (BSTs), but their exponential growth and sparsity
hinder efficiency. To overcome this, we introduce Compressed BST of Inverted
hash tables (ComBI), which enables fast approximate nearest-neighbor search
with reduced memory. On datasets of up to one billion samples, ComBI achieves
0.90 precision with 4X-296X speed-ups over Multi-Index Hashing, and also
outperforms Cellfishing.jl on single-cell RNA-seq searches with 2X-13X gains.
Building on hashing structures, we propose Guided Random Forest (GRAF), a
tree-based ensemble classifier that integrates global and local partitioning,
bridging decision trees and boosting while reducing generalization error.
Across 115 datasets, GRAF delivers competitive or superior accuracy, and its
unsupervised variant (uGRAF) supports guided hashing and importance sampling.
We show that GRAF and ComBI can be used to estimate per-sample classifiability,
which enables scalable prediction of cancer patient survival. To address
challenges in interpreting mutations, we introduce Continuous Representation of
Codon Switches (CRCS), a deep learning framework that embeds genetic changes
into numerical vectors. CRCS allows identification of somatic mutations without
matched normals, discovery of driver genes, and scoring of tumor mutations,
with survival prediction validated in bladder, liver, and brain cancers.
Together, these methods provide efficient, scalable, and interpretable tools
for large-scale data analysis and biomedical applications.

</details>


### [138] [Convolutional Neural Networks for Accurate Measurement of Train Speed](https://arxiv.org/abs/2508.17096)
*Haitao Tian,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.LG

TL;DR: 本研究探索使用卷积神经网络(CNN)提高列车速度估计精度，比较了三种CNN架构与自适应卡尔曼滤波器的性能，发现多分支CNN模型在复杂工况下表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决现代铁路系统中列车速度估计的复杂挑战，传统方法在复杂工况下精度不足，需要更先进的技术来提高铁路安全性和运营效率。

Method: 研究了三种CNN架构：单分支2D、单分支1D和多分支模型，并与自适应卡尔曼滤波器进行比较。使用带和不带轮滑保护激活的模拟列车运行数据集进行分析。

Result: 基于CNN的方法，特别是多分支模型，在精度和鲁棒性方面优于传统方法，在具有挑战性的运行条件下表现尤为突出。

Conclusion: 深度学习技术能够更有效地捕捉复杂运输数据集中的精细模式，具有提升铁路安全和运营效率的巨大潜力。

Abstract: In this study, we explore the use of Convolutional Neural Networks for
improving train speed estimation accuracy, addressing the complex challenges of
modern railway systems. We investigate three CNN architectures - single-branch
2D, single-branch 1D, and multiple-branch models - and compare them with the
Adaptive Kalman Filter. We analyse their performance using simulated train
operation datasets with and without Wheel Slide Protection activation. Our
results reveal that CNN-based approaches, especially the multiple-branch model,
demonstrate superior accuracy and robustness compared to traditional methods,
particularly under challenging operational conditions. These findings highlight
the potential of deep learning techniques to enhance railway safety and
operational efficiency by more effectively capturing intricate patterns in
complex transportation datasets.

</details>


### [139] [Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process](https://arxiv.org/abs/2508.17097)
*Lingkai Kong,Haotian Sun,Yuchen Zhuang,Haorui Wang,Wenhao Mu,Chao Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合图函数神经过程和图生成模型的不确定性感知可解释图分类方法，通过潜在原理学习和随机相关性矩阵来提升GNN的校准性和可解释性


<details>
  <summary>Details</summary>
Motivation: 图神经网络在关键应用中因预测校准性差和缺乏可解释性而受限，需要开发既能量化不确定性又能提供解释的方法

Method: 假设一组潜在原理并映射到概率嵌入空间，通过学习随机相关性矩阵使分类器预测分布以原理嵌入为条件，图生成器从嵌入空间解码原理图结构，采用类似EM算法的交替优化训练

Result: 在五个图分类数据集上实验表明，该方法在不确定性量化和GNN可解释性方面优于最先进方法，案例研究显示解码的原理结构能提供有意义的解释

Conclusion: 该方法通用性强，可应用于任何现有GNN架构，成功解决了GNN的校准性和可解释性问题，为关键应用提供了可靠支持

Abstract: Graph neural networks (GNNs) are powerful tools on graph data. However, their
predictions are mis-calibrated and lack interpretability, limiting their
adoption in critical applications. To address this issue, we propose a new
uncertainty-aware and interpretable graph classification model that combines
graph functional neural process and graph generative model. The core of our
method is to assume a set of latent rationales which can be mapped to a
probabilistic embedding space; the predictive distribution of the classifier is
conditioned on such rationale embeddings by learning a stochastic correlation
matrix. The graph generator serves to decode the graph structure of the
rationales from the embedding space for model interpretability. For efficient
model training, we adopt an alternating optimization procedure which mimics the
well known Expectation-Maximization (EM) algorithm. The proposed method is
general and can be applied to any existing GNN architecture. Extensive
experiments on five graph classification datasets demonstrate that our
framework outperforms state-of-the-art methods in both uncertainty
quantification and GNN interpretability. We also conduct case studies to show
that the decoded rationale structure can provide meaningful explanations.

</details>


### [140] [Reconciling Communication Compression and Byzantine-Robustness in Distributed Learning](https://arxiv.org/abs/2508.17129)
*Diksha Gupta,Nirupam Gupta,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: RoSDHB算法结合Polyak动量和协调压缩机制，在分布式学习中同时处理拜占庭故障和高通信成本问题，相比现有方法依赖更少的假设条件


<details>
  <summary>Details</summary>
Motivation: 分布式学习面临拜占庭故障和高通信成本的双重挑战，现有方法在结合通信压缩和拜占庭鲁棒聚合时会导致对故障节点的弹性下降

Method: 提出RoSDHB算法，整合经典Polyak动量和新的协调压缩机制，仅假设诚实工作者平均损失函数的Lipschitz平滑性

Result: RoSDHB在标准(G,B)梯度差异异构模型下性能与Byz-DASHA-PAGE相当，但依赖更少假设，在图像分类基准任务中实现了强鲁棒性和显著通信节省

Conclusion: RoSDHB提供了一种更简洁有效的解决方案，在保持拜占庭鲁棒性的同时显著降低通信成本，且对模型假设要求更低

Abstract: Distributed learning (DL) enables scalable model training over decentralized
data, but remains challenged by Byzantine faults and high communication costs.
While both issues have been studied extensively in isolation, their interaction
is less explored. Prior work shows that naively combining communication
compression with Byzantine-robust aggregation degrades resilience to faulty
nodes (or workers). The state-of-the-art algorithm, namely Byz-DASHA-PAGE [29],
makes use of the momentum variance reduction scheme to mitigate the detrimental
impact of compression noise on Byzantine-robustness. We propose a new
algorithm, named RoSDHB, that integrates the classic Polyak's momentum with a
new coordinated compression mechanism. We show that RoSDHB performs comparably
to Byz-DASHA-PAGE under the standard (G, B)-gradient dissimilarity
heterogeneity model, while it relies on fewer assumptions. In particular, we
only assume Lipschitz smoothness of the average loss function of the honest
workers, in contrast to [29]that additionally assumes a special smoothness of
bounded global Hessian variance. Empirical results on benchmark image
classification task show that RoSDHB achieves strong robustness with
significant communication savings.

</details>


### [141] [MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices](https://arxiv.org/abs/2508.17137)
*Nishant Gavhane,Arush Mehrotra,Rohit Chawla,Peter Proenca*

Main category: cs.LG

TL;DR: MoE-Beyond是一个基于学习的专家激活预测器，通过训练轻量级Transformer模型来预测MoE模型在自回归解码过程中的专家激活，显著提高了GPU缓存命中率。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型在边缘设备部署时面临内存限制问题，传统启发式专家缓存策略在模型参数扩展时难以维持高缓存命中率。

Method: 将任务构建为多标签序列预测问题，使用从DeepSeek-V2-Chat-Lite MoE提取的6600万专家激活轨迹训练轻量级Transformer模型。

Result: 在WebGLM-QA数据集上达到97.5%准确率和86.6% F1分数，当仅10%专家适合GPU缓存时，将缓存命中率从17%提升至72%。

Conclusion: MoE-Beyond通过学习专家激活模式，显著优于传统启发式方法，为资源受限环境中的MoE模型部署提供了有效的内存管理解决方案。

Abstract: The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices
presents significant challenges due to memory constraints. While MoE
architectures enable efficient utilization of computational resources by
activating only a subset of experts per inference, they require careful memory
management to operate efficiently in resource-constrained environments.
Traditional heuristic-based expert caching strategies such as MoE-Infinity
struggle to maintain high cache hit rates as models parameters scale. In this
work, we introduce MoE-Beyond, a learning-based expert activation predictor
trained to predict expert activations during autoregressive decoding. By
framing the task as a multi-label sequence prediction problem, we train a
lightweight transformer model on 66 million expert activation traces extracted
from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor
generalizes effectively across unseen prompts from WebGLM-QA dataset [6],
achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that
MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts
fit in GPU cache, outperforming heuristic baselines.

</details>


### [142] [Stochastic Gradient Descent with Strategic Querying](https://arxiv.org/abs/2508.17144)
*Nanfei Jiang,Hoi-To Wai,Mahnoosh Alizadeh*

Main category: cs.LG

TL;DR: 本文研究有限和优化问题中的战略梯度查询策略，相比均匀查询可提升随机梯度方法的性能。提出了理想化的OGQ算法和实用的SGQ算法，在满足PL条件的平滑目标函数下，SGQ在瞬态性能上优于SGD。


<details>
  <summary>Details</summary>
Motivation: 研究在有限和优化问题中，通过战略性地选择梯度查询（而非均匀查询）来提升随机梯度方法的性能，特别是在实际应用中无法获取所有用户梯度信息的情况下。

Method: 提出了两种算法：1）OGQ（理想化算法）- 选择能带来最大期望改进的用户梯度；2）SGQ（实用算法）- 每轮只查询一次但性能优于SGD。理论分析基于平滑函数和PL条件。

Result: 理论证明在EI异质性假设下，OGQ能提升瞬态性能并降低稳态方差，SGQ在瞬态性能上优于SGD。数值实验验证了理论结果。

Conclusion: 战略梯度查询策略相比均匀查询能有效提升优化算法的性能，SGQ算法在实际应用中具有更好的瞬态性能表现。

Abstract: This paper considers a finite-sum optimization problem under first-order
queries and investigates the benefits of strategic querying on stochastic
gradient-based methods compared to uniform querying strategy. We first
introduce Oracle Gradient Querying (OGQ), an idealized algorithm that selects
one user's gradient yielding the largest possible expected improvement (EI) at
each step. However, OGQ assumes oracle access to the gradients of all users to
make such a selection, which is impractical in real-world scenarios. To address
this limitation, we propose Strategic Gradient Querying (SGQ), a practical
algorithm that has better transient-state performance than SGD while making
only one query per iteration. For smooth objective functions satisfying the
Polyak-Lojasiewicz condition, we show that under the assumption of EI
heterogeneity, OGQ enhances transient-state performance and reduces
steady-state variance, while SGQ improves transient-state performance over SGD.
Our numerical experiments validate our theoretical findings.

</details>


### [143] [SACA: Selective Attention-Based Clustering Algorithm](https://arxiv.org/abs/2508.17150)
*Meysam Shirdel Bilehsavar,Razieh Ghaedi,Samira Seyed Taheri,Xinqi Fan,Christian O'Reilly*

Main category: cs.LG

TL;DR: 提出了一种基于选择性注意概念的新型密度聚类方法，无需用户定义参数即可运行，或仅需调整一个简单整数参数，在多种数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统密度聚类算法（如DBSCAN）依赖用户定义的参数，需要领域专业知识进行优化，这在实际应用中存在挑战。

Method: 受选择性注意概念启发，该方法首先无需用户参数即可运行，必要时引入单个整数参数。算法计算阈值过滤稀疏点和异常值，形成初步聚类结构，然后重新整合被排除点以完成最终结果。

Result: 在多样化数据集上的实验评估表明，该方法具有易用性和鲁棒性能。

Conclusion: 该方法为密度聚类任务提供了一个有效的替代方案，显著减少了参数调优的复杂性。

Abstract: Clustering algorithms are widely used in various applications, with
density-based methods such as Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) being particularly prominent. These algorithms identify
clusters in high-density regions while treating sparser areas as noise.
However, reliance on user-defined parameters often poses optimization
challenges that require domain expertise. This paper presents a novel
density-based clustering method inspired by the concept of selective attention,
which minimizes the need for user-defined parameters under standard conditions.
Initially, the algorithm operates without requiring user-defined parameters. If
parameter adjustment is needed, the method simplifies the process by
introducing a single integer parameter that is straightforward to tune. The
approach computes a threshold to filter out the most sparsely distributed
points and outliers, forms a preliminary cluster structure, and then
reintegrates the excluded points to finalize the results. Experimental
evaluations on diverse data sets highlight the accessibility and robust
performance of the method, providing an effective alternative for density-based
clustering tasks.

</details>


### [144] [Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks](https://arxiv.org/abs/2508.17158)
*Jack Youstra,Mohammed Mahfoud,Yang Yan,Henry Sleight,Ethan Perez,Mrinank Sharma*

Main category: cs.LG

TL;DR: 提出了CIFR基准测试来评估防御策略对抗密码编码攻击的能力，通过探测监控器实现超过99%的检测准确率，并能泛化到未见过的密码变体和家族


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调API存在安全风险，攻击者可以通过在看似无害的微调数据中编码有害内容来绕过模型安全机制，逃避人工监控和标准内容过滤器

Method: 引入CIFR基准测试，包含多样化的密码编码和家族，部分密码仅在测试集中以评估泛化能力；在基准测试上评估不同防御策略，并在多个微调模型内部激活上训练探测监控器

Result: 探测监控器达到超过99%的检测准确率，能够泛化到未见过的密码变体和家族，性能优于最先进的监控方法

Conclusion: CIFR基准测试和探测监控器为保护微调API安全提供了有效解决方案，开源了代码和数据以促进这一关键领域的进一步研究

Abstract: Large language model fine-tuning APIs enable widespread model customization,
yet pose significant safety risks. Recent work shows that adversaries can
exploit access to these APIs to bypass model safety mechanisms by encoding
harmful content in seemingly harmless fine-tuning data, evading both human
monitoring and standard content filters. We formalize the fine-tuning API
defense problem, and introduce the Cipher Fine-tuning Robustness benchmark
(CIFR), a benchmark for evaluating defense strategies' ability to retain model
safety in the face of cipher-enabled attackers while achieving the desired
level of fine-tuning functionality. We include diverse cipher encodings and
families, with some kept exclusively in the test set to evaluate for
generalization across unseen ciphers and cipher families. We then evaluate
different defenses on the benchmark and train probe monitors on model internal
activations from multiple fine-tunes. We show that probe monitors achieve over
99% detection accuracy, generalize to unseen cipher variants and families, and
compare favorably to state-of-the-art monitoring approaches. We open-source
CIFR and the code to reproduce our experiments to facilitate further research
in this critical area. Code and data are available online
https://github.com/JackYoustra/safe-finetuning-api

</details>


### [145] [ONG: Orthogonal Natural Gradient Descent](https://arxiv.org/abs/2508.17169)
*Yajat Yadav,Jathin Korrapati,Patrick Mendoza*

Main category: cs.LG

TL;DR: ONG（正交自然梯度下降）结合正交梯度下降和自然梯度方法，通过EKFAC近似Fisher信息矩阵的逆来预条件新任务梯度，并在黎曼度量下沿最陡下降方向更新，同时将自然梯度投影到先前任务梯度的正交补空间以保持旧任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统的正交梯度下降方法使用欧几里得投影，忽略了神经网络参数化分布空间的信息几何结构，这可能导致学习任务中的次优收敛。

Method: 结合自然梯度思想，使用EKFAC高效近似Fisher信息矩阵的逆来预条件新任务梯度，然后将这些自然梯度投影到先前任务梯度的正交补空间。

Result: 在Permuted和Rotated MNIST数据集上进行了基准测试，证明了方法的有效性。

Conclusion: ONG方法通过结合自然梯度和正交投影，在保持先前任务性能的同时，提供了更符合信息几何结构的优化方向，改善了持续学习任务的收敛性能。

Abstract: Orthogonal gradient descent has emerged as a powerful method for continual
learning tasks. However, its Euclidean projections overlook the underlying
information-geometric structure of the space of distributions parametrized by
neural networks, which can lead to suboptimal convergence in learning tasks. To
counteract this, we combine it with the idea of the natural gradient and
present ONG (Orthogonal Natural Gradient Descent). ONG preconditions each new
task gradient with an efficient EKFAC approximation of the inverse Fisher
information matrix, yielding updates that follow the steepest descent direction
under a Riemannian metric. To preserve performance on previously learned tasks,
ONG projects these natural gradients onto the orthogonal complement of prior
task gradients. We provide a theoretical justification for this procedure,
introduce the ONG algorithm, and benchmark its performance on the Permuted and
Rotated MNIST datasets. All code for our experiments/reproducibility can be
found at https://github.com/yajatyadav/orthogonal-natural-gradient.

</details>


### [146] [Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection](https://arxiv.org/abs/2508.17174)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.LG

TL;DR: 提出SaGD框架，通过平滑对抗训练的损失景观来改善OOD检测，在对抗攻击下能更好地区分对抗性ID样本和真实OOD样本。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将对抗性ID样本误判为OOD样本，且在对抗攻击下的ID和OOD数据检测研究较少，需要开发鲁棒的检测方法。

Method: 引入Sharpness-aware Geometric Defense (SaGD)框架，使用Jitter-based扰动在对抗训练中平滑损失景观，提高几何嵌入质量。

Result: 在CIFAR-100与六个OOD数据集的各种攻击下，SaGD显著改善了FPR和AUC指标，优于现有防御方法。

Conclusion: SaGD框架通过优化对抗训练的损失景观，有效提升了对抗攻击下的OOD检测性能，揭示了尖锐损失景观与对抗性OOD检测之间的关系。

Abstract: Out-of-distribution (OOD) detection ensures safe and reliable model
deployment. Contemporary OOD algorithms using geometry projection can detect
OOD or adversarial samples from clean in-distribution (ID) samples. However,
this setting regards adversarial ID samples as OOD, leading to incorrect OOD
predictions. Existing efforts on OOD detection with ID and OOD data under
attacks are minimal. In this paper, we develop a robust OOD detection method
that distinguishes adversarial ID samples from OOD ones. The sharp loss
landscape created by adversarial training hinders model convergence, impacting
the latent embedding quality for OOD score calculation. Therefore, we introduce
a {\bf Sharpness-aware Geometric Defense (SaGD)} framework to smooth out the
rugged adversarial loss landscape in the projected latent geometry. Enhanced
geometric embedding convergence enables accurate ID data characterization,
benefiting OOD detection against adversarial attacks. We use Jitter-based
perturbation in adversarial training to extend the defense ability against
unseen attacks. Our SaGD framework significantly improves FPR and AUC over the
state-of-the-art defense approaches in differentiating CIFAR-100 from six other
OOD datasets under various attacks. We further examine the effects of
perturbations at various adversarial training levels, revealing the
relationship between the sharp loss landscape and adversarial OOD detection.

</details>


### [147] [Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention](https://arxiv.org/abs/2508.17175)
*Leon Dimitrov*

Main category: cs.LG

TL;DR: 比较图transformer中密集和稀疏注意力机制的优缺点，分析适用场景，并讨论当前设计挑战


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络难以捕获长距离节点依赖，图transformer通过注意力机制实现全局信息交换，但存在密集和稀疏两种注意力机制需要对比分析

Method: 对比分析密集注意力和稀疏注意力两种机制，评估它们在计算效率、表达能力等方面的权衡

Result: 明确了两种注意力机制的优缺点和适用场景，为不同图结构任务选择合适的注意力机制提供指导

Conclusion: 图transformer的注意力机制设计需要根据具体任务和图结构特点进行选择，密集和稀疏注意力各有优势，当前仍存在计算效率和表达能力平衡等挑战

Abstract: Graphs have become a central representation in machine learning for capturing
relational and structured data across various domains. Traditional graph neural
networks often struggle to capture long-range dependencies between nodes due to
their local structure. Graph transformers overcome this by using attention
mechanisms that allow nodes to exchange information globally. However, there
are two types of attention in graph transformers: dense and sparse. In this
paper, we compare these two attention mechanisms, analyze their trade-offs, and
highlight when to use each. We also outline current challenges and problems in
designing attention for graph transformers.

</details>


### [148] [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
*Hikaru Tsujimura,Arush Tagade*

Main category: cs.LG

TL;DR: 通过机制可解释性方法分析LLM过度自信的内部机制，发现自信表征可分解为情感和逻辑两个正交子成分，与心理学双路径模型对应，为缓解过度自信提供了新途径


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常在高风险情境中表现出过度自信，以不应有的确定性呈现信息，需要理解这种行为的内在机制

Method: 使用人类标注的自信度数据集微调开源的Llama 3.2模型，提取所有层的残差激活，计算相似性指标来定位自信表征

Result: 识别出对自信度对比最敏感的层，发现高自信表征可分解为情感和逻辑两个正交子成分，情感向量广泛影响预测准确性，逻辑向量产生更局部的影响

Conclusion: 为LLM自信度的多组件结构提供了机制性证据，并指出了缓解过度自信行为的途径

Abstract: Large Language Models (LLMs) often display overconfidence, presenting
information with unwarranted certainty in high-stakes contexts. We investigate
the internal basis of this behavior via mechanistic interpretability. Using
open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness
datasets, we extract residual activations across all layers, and compute
similarity metrics to localize assertive representations. Our analysis
identifies layers most sensitive to assertiveness contrasts and reveals that
high-assertive representations decompose into two orthogonal sub-components of
emotional and logical clusters-paralleling the dual-route Elaboration
Likelihood Model in Psychology. Steering vectors derived from these
sub-components show distinct causal effects: emotional vectors broadly
influence prediction accuracy, while logical vectors exert more localized
effects. These findings provide mechanistic evidence for the multi-component
structure of LLM assertiveness and highlight avenues for mitigating
overconfident behavior.

</details>


### [149] [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens](https://arxiv.org/abs/2508.17196)
*Hao Wen,Xinrui Wu,Yi Sun,Feifei Zhang,Liye Chen,Jie Wang,Yunxin Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.LG

TL;DR: BudgetThinker是一个新颖的框架，通过预算感知推理使LLM能够精确控制思维过程长度，在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM通过增加测试时计算来提升推理能力，但带来了显著的延迟和资源成本，限制了在实时或成本敏感场景中的应用。

Method: 提出在推理期间定期插入特殊控制令牌来持续告知模型剩余令牌预算，采用两阶段训练流程：监督微调使模型熟悉预算约束，然后是基于课程的强化学习阶段使用长度感知奖励函数优化准确性和预算遵守。

Result: 在具有挑战性的数学基准测试中，BudgetThinker在各种推理预算下显著超越了强基线，在保持性能的同时实现了预算控制。

Conclusion: 该方法为开发高效可控的LLM推理提供了可扩展且有效的解决方案，使先进模型在资源受限和实时环境中的部署更加实用。

Abstract: Recent advancements in Large Language Models (LLMs) have leveraged increased
test-time computation to enhance reasoning capabilities, a strategy that, while
effective, incurs significant latency and resource costs, limiting their
applicability in real-world time-constrained or cost-sensitive scenarios. This
paper introduces BudgetThinker, a novel framework designed to empower LLMs with
budget-aware reasoning, enabling precise control over the length of their
thought processes. We propose a methodology that periodically inserts special
control tokens during inference to continuously inform the model of its
remaining token budget. This approach is coupled with a comprehensive two-stage
training pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize
the model with budget constraints, followed by a curriculum-based Reinforcement
Learning (RL) phase that utilizes a length-aware reward function to optimize
for both accuracy and budget adherence. We demonstrate that BudgetThinker
significantly surpasses strong baselines in maintaining performance across a
variety of reasoning budgets on challenging mathematical benchmarks. Our method
provides a scalable and effective solution for developing efficient and
controllable LLM reasoning, making advanced models more practical for
deployment in resource-constrained and real-time environments.

</details>


### [150] [How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System](https://arxiv.org/abs/2508.17215)
*Kaiwen Zuo,Zelin Liu,Raman Dutt,Ziyang Wang,Zhongtian Sun,Yeming Wang,Fan Mo,Pietro Liò*

Main category: cs.LG

TL;DR: MedThreatRAG是一个针对医疗RAG系统的多模态投毒攻击框架，通过注入对抗性图像-文本对来破坏跨模态对齐，导致检索和生成性能显著下降


<details>
  <summary>Details</summary>
Motivation: 医疗AI中基于检索增强生成(RAG)的大规模视觉语言模型日益普及，但其对外部知识库的依赖创造了新的攻击面，需要系统性地探索这些系统的安全漏洞

Method: 构建模拟半开放攻击环境，引入跨模态冲突注入(CMCI)技术，在医学图像和配对报告中嵌入微妙的语义矛盾，同时保持足够的合理性以规避传统过滤器

Result: 在IU-Xray和MIMIC-CXR QA任务上，MedThreatRAG使答案F1分数降低高达27.66%，LLaVA-Med-1.5的F1率降至51.36%

Conclusion: 研究揭示了临床RAG系统的根本性安全漏洞，强调了威胁感知设计和鲁棒多模态一致性检查的迫切需求，并提供了安全开发指南

Abstract: Large Vision-Language Models (LVLMs) augmented with Retrieval-Augmented
Generation (RAG) are increasingly employed in medical AI to enhance factual
grounding through external clinical image-text retrieval. However, this
reliance creates a significant attack surface. We propose MedThreatRAG, a novel
multimodal poisoning framework that systematically probes vulnerabilities in
medical RAG systems by injecting adversarial image-text pairs. A key innovation
of our approach is the construction of a simulated semi-open attack
environment, mimicking real-world medical systems that permit periodic
knowledge base updates via user or pipeline contributions. Within this setting,
we introduce and emphasize Cross-Modal Conflict Injection (CMCI), which embeds
subtle semantic contradictions between medical images and their paired reports.
These mismatches degrade retrieval and generation by disrupting cross-modal
alignment while remaining sufficiently plausible to evade conventional filters.
While basic textual and visual attacks are included for completeness, CMCI
demonstrates the most severe degradation. Evaluations on IU-Xray and MIMIC-CXR
QA tasks show that MedThreatRAG reduces answer F1 scores by up to 27.66% and
lowers LLaVA-Med-1.5 F1 rates to as low as 51.36%. Our findings expose
fundamental security gaps in clinical RAG systems and highlight the urgent need
for threat-aware design and robust multimodal consistency checks. Finally, we
conclude with a concise set of guidelines to inform the safe development of
future multimodal medical RAG systems.

</details>


### [151] [GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning](https://arxiv.org/abs/2508.17218)
*Xing Wei,Yuqi Ouyang*

Main category: cs.LG

TL;DR: 本文解决随机交通网络中的可靠最短路径问题，通过结合决策Transformer和普通策略梯度框架，提高了路径规划的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 城市车辆数量快速增长，现有道路资源无法满足需求，导致拥堵问题。当前导航模型忽视了交通流的相关性和随机性质，需要更有效的路径规划策略。

Method: 提出了一种路径规划方案，将决策Transformer与普通策略梯度（GPG）框架相结合。利用决策Transformer模型长期依赖关系的能力，提高路径决策的准确性和稳定性。

Result: 在Sioux Falls网络（SFN）上的实验结果显示，该方法在准时到达概率方面超过了之前的基准方法，提供了更准确的路径规划解决方案。

Conclusion: 通过结合决策Transformer和GPG框架，本研究有效地解决了随机交通网络中的可靠最短路径问题，为城市交通拥堵问题提供了更加准确和稳定的路径规划方案。

Abstract: With the rapidly increased number of vehicles in urban areas, existing road
infrastructure struggles to accommodate modern traffic demands, resulting in
the issue of congestion. This highlights the importance of efficient path
planning strategies. However, most recent navigation models focus solely on
deterministic or time-dependent networks, while overlooking the correlations
and the stochastic nature of traffic flows. In this work, we address the
reliable shortest path problem within stochastic transportation networks under
certain dependencies. We propose a path planning solution that integrates the
decision Transformer with the Generalized Policy Gradient (GPG) framework.
Based on the decision Transformer's capability to model long-term dependencies,
our proposed solution improves the accuracy and stability of path decisions.
Experimental results on the Sioux Falls Network (SFN) demonstrate that our
approach outperforms previous baselines in terms of on-time arrival
probability, providing more accurate path planning solutions.

</details>


### [152] [Curvature Learning for Generalization of Hyperbolic Neural Networks](https://arxiv.org/abs/2508.17232)
*Xiaomeng Fan,Yuwei Wu,Zhi Gao,Mehrtash Harandi,Yunde Jia*

Main category: cs.LG

TL;DR: 该论文提出了基于PAC-Bayesian泛化界的双曲神经网络曲率学习方法，通过优化损失景观平滑度来提升泛化性能


<details>
  <summary>Details</summary>
Motivation: 曲率在双曲神经网络中起关键作用，不合适的曲率会导致网络收敛到次优参数，但目前缺乏曲率对HNNs影响的理论基础

Method: 提出基于锐度感知的曲率学习方法，设计曲率范围锐度度量，通过双层优化过程最小化该度量，并引入隐式微分算法高效求解梯度近似

Result: 在分类、长尾数据学习、噪声数据学习和少样本学习四个设置上的实验表明，该方法能显著提升HNNs性能

Conclusion: 所提出的曲率学习方法通过平滑损失景观有效改善了双曲神经网络的泛化能力，理论分析和实验验证了方法的有效性

Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in
representing real-world data with hierarchical structures via exploiting the
geometric properties of hyperbolic spaces characterized by negative curvatures.
Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may
cause HNNs to converge to suboptimal parameters, degrading overall performance.
So far, the theoretical foundation of the effect of curvatures on HNNs has not
been developed. In this paper, we derive a PAC-Bayesian generalization bound of
HNNs, highlighting the role of curvatures in the generalization of HNNs via
their effect on the smoothness of the loss landscape. Driven by the derived
bound, we propose a sharpness-aware curvature learning method to smooth the
loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized
through a bi-level optimization process. Then, we introduce an implicit
differentiation algorithm that efficiently solves the bi-level optimization by
approximating gradients of curvatures. We present the approximation error and
convergence analyses of the proposed method, showing that the approximation
error is upper-bounded, and the proposed method can converge by bounding
gradients of HNNs. Experiments on four settings: classification, learning from
long-tailed data, learning from noisy data, and few-shot learning show that our
method can improve the performance of HNNs.

</details>


### [153] [Module-Aware Parameter-Efficient Machine Unlearning on Transformers](https://arxiv.org/abs/2508.17233)
*Wenjie Bao,Jian Lou,Yuke Hu,Xiaochen Li,Zhihao Liu,Jiaqi Liu,Zhan Qin,Kui Ren*

Main category: cs.LG

TL;DR: MAPE-Unlearn是一种模块感知的参数高效机器学习遗忘方法，通过可学习的掩码对来精确定位Transformer中影响关键参数，相比现有方法在遗忘性能上有显著提升


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效遗忘方法大多以模块无关的方式设计，无法准确识别Transformer中的关键参数，导致遗忘性能不佳。需要一种能够考虑Transformer模块结构的遗忘方法

Method: 提出MAPE-Unlearn方法，使用可学习的掩码对来识别Transformer头部和过滤器中的影响关键参数，通过带有预热启动的贪婪搜索算法优化掩码学习目标

Result: 在各种Transformer模型和数据集上的广泛实验证明了MAPE-Unlearn在遗忘任务上的有效性和鲁棒性

Conclusion: MAPE-Unlearn通过模块感知的方法显著提升了Transformer模型的遗忘性能，为隐私合规要求下的数据影响移除提供了有效的解决方案

Abstract: Transformer has become fundamental to a vast series of pre-trained large
models that have achieved remarkable success across diverse applications.
Machine unlearning, which focuses on efficiently removing specific data
influences to comply with privacy regulations, shows promise in restricting
updates to influence-critical parameters. However, existing parameter-efficient
unlearning methods are largely devised in a module-oblivious manner, which
tends to inaccurately identify these parameters and leads to inferior
unlearning performance for Transformers. In this paper, we propose {\tt
MAPE-Unlearn}, a module-aware parameter-efficient machine unlearning approach
that uses a learnable pair of masks to pinpoint influence-critical parameters
in the heads and filters of Transformers. The learning objective of these masks
is derived by desiderata of unlearning and optimized through an efficient
algorithm featured by a greedy search with a warm start. Extensive experiments
on various Transformer models and datasets demonstrate the effectiveness and
robustness of {\tt MAPE-Unlearn} for unlearning.

</details>


### [154] [Provable Generalization in Overparameterized Neural Nets](https://arxiv.org/abs/2508.17256)
*Aviral Dhingra*

Main category: cs.LG

TL;DR: 该论文提出基于注意力矩阵有效秩的新容量概念，为过参数化注意力模型提供更合理的泛化边界解释


<details>
  <summary>Details</summary>
Motivation: 传统复杂度度量（如VC维、PAC-Bayes边界）在过参数化体系中变得空洞，无法解释Transformer等模型的实际泛化成功

Method: 基于注意力矩阵的有效秩来定义模型容量，分析注意力机制的功能维度而非原始参数数量

Result: 推导出的泛化边界在样本量依赖关系上与大型语言模型的经验缩放定律匹配（最多相差对数因子）

Conclusion: 注意力矩阵的谱特性而非原始参数数量，可能是理解过参数化模型泛化能力的关键视角

Abstract: Deep neural networks often contain far more parameters than training
examples, yet they still manage to generalize well in practice. Classical
complexity measures such as VC-dimension or PAC-Bayes bounds usually become
vacuous in this overparameterized regime, offering little explanation for the
empirical success of models like Transformers. In this work, I explore an
alternative notion of capacity for attention-based models, based on the
effective rank of their attention matrices. The intuition is that, although the
parameter count is enormous, the functional dimensionality of attention is
often much lower. I show that this quantity leads to a generalization bound
whose dependence on sample size matches empirical scaling laws observed in
large language models, up to logarithmic factors. While the analysis is not a
complete theory of overparameterized learning, it provides evidence that
spectral properties of attention, rather than raw parameter counts, may be the
right lens for understanding why these models generalize.

</details>


### [155] [DeepCFD: Efficient near-ground airfoil lift coefficient approximation with deep convolutional neural networks](https://arxiv.org/abs/2508.17278)
*Mohammad Amin Esabat,Saeed Jaamei,Fatemeh Asadi*

Main category: cs.LG

TL;DR: 使用VGG神经网络方法预测近地面翼型升阻比系数，通过CFD数据和翼型截面图像矩阵进行训练，相比其他CNN方法精度更高


<details>
  <summary>Details</summary>
Motivation: 传统CFD软件计算近地面翼型气动系数耗时较长，而CFD模拟数据的可用性和新神经网络方法的发展使得能够用VGG等CNN方法呈现模拟结果

Method: 采用VGG神经网络方法，通过提供包含升阻比信息的原始数据和转换为矩阵的翼型截面图像数据进行训练学习

Result: VGG方法相比其他CNN方法能够获得更准确的结果

Conclusion: 神经网络方法特别是VGG能够有效预测近地面翼型的升阻比系数，为气动分析提供了更高效准确的替代方案

Abstract: . Predicting and calculating the aerodynamic coefficients of airfoils near
the ground with CFD software requires much time. However, the availability of
data from CFD simulation results and the development of new neural network
methods have made it possible to present the simulation results using methods
like VGG, a CCN neural network method. In this article, lift-to-drag
coefficients of airfoils near the ground surface are predicted with the help of
a neural network. This prediction can only be realized by providing data for
training and learning the code that contains information on the lift-to-drag
ratio of the primary data and images related to the airfoil cross-section,
which are converted into a matrix. One advantage of the VGG method over other
methods is that its results are more accurate than those of other CNN methods.

</details>


### [156] [Explainable AI (XAI) for Arrhythmia detection from electrocardiograms](https://arxiv.org/abs/2508.17294)
*Joschka Beck,Arlene John*

Main category: cs.LG

TL;DR: 这篇论文研究了适用于心电图时间序列分析的可解释AI技术，通过对比四种SHAP方法发现渐度基础和DeepLIFT方法能更好地符合临床理解需求


<details>
  <summary>Details</summary>
Motivation: 深度学习在心律失常检测中准确性高但可解释性不足，影响临床应用接受度

Method: 使用MIT-BIH数据集开发CNN模型，采用Pan-Tompkins算法进行R波分割，并结合12导联心电图数据集。对比四种SHAP方法：排列重要性、KernelSHAP、渐度基础方法和DeepLIFT

Result: 模型在MIT-BIH上达到98.3%准确率，但在混合数据集上性能下降。渐度基础和DeepLIFT方法能更好地符合临床理解需求，而排列重要性和KernelSHAP产生杂乱的可触视化输出

Conclusion: 心电图分析需要领域特定的可解释AI适配，显著性地图是更临床直观的解释方法

Abstract: Advancements in deep learning have enabled highly accurate arrhythmia
detection from electrocardiogram (ECG) signals, but limited interpretability
remains a barrier to clinical adoption. This study investigates the application
of Explainable AI (XAI) techniques specifically adapted for time-series ECG
analysis. Using the MIT-BIH arrhythmia dataset, a convolutional neural
network-based model was developed for arrhythmia classification, with
R-peak-based segmentation via the Pan-Tompkins algorithm. To increase the
dataset size and to reduce class imbalance, an additional 12-lead ECG dataset
was incorporated. A user needs assessment was carried out to identify what kind
of explanation would be preferred by medical professionals. Medical
professionals indicated a preference for saliency map-based explanations over
counterfactual visualisations, citing clearer correspondence with ECG
interpretation workflows. Four SHapley Additive exPlanations (SHAP)-based
approaches: permutation importance, KernelSHAP, gradient-based methods, and
Deep Learning Important FeaTures (DeepLIFT), were implemented and compared. The
model achieved 98.3% validation accuracy on MIT-BIH but showed performance
degradation on the combined dataset, underscoring dataset variability
challenges. Permutation importance and KernelSHAP produced cluttered visual
outputs, while gradient-based and DeepLIFT methods highlighted waveform regions
consistent with clinical reasoning, but with variability across samples.
Findings emphasize the need for domain-specific XAI adaptations in ECG analysis
and highlight saliency mapping as a more clinically intuitive approach

</details>


### [157] [Physics-informed neural network for fatigue life prediction of irradiated austenitic and ferritic/martensitic steels](https://arxiv.org/abs/2508.17303)
*Dhiraj S Kori,Abhinav Chandraker,Syed Abdur Rahman,Punit Rathore,Ankur Chauhan*

Main category: cs.LG

TL;DR: 提出了基于物理信息的神经网络(PINN)框架来预测核反应堆用辐照奥氏体和铁素体/马氏体钢的低周疲劳寿命，相比传统机器学习方法具有更高精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 核反应堆材料在高温循环载荷和辐照条件下会发生复杂退化，传统经验模型无法准确预测其疲劳寿命，需要开发更精确的预测方法。

Method: 开发PINN模型，在损失函数中融入物理疲劳寿命约束，使用495个数据点（包括辐照和未辐照条件）进行训练，并与随机森林、梯度提升等传统机器学习方法进行比较。

Result: PINN模型在所有比较模型中表现最佳，SHAP分析显示应变幅值、辐照剂量和测试温度是主要影响因素，且与疲劳寿命呈负相关，与物理理解一致。

Conclusion: PINN框架为预测辐照合金疲劳寿命提供了可靠且可解释的方法，有助于指导合金选择决策。

Abstract: This study proposes a Physics-Informed Neural Network (PINN) framework to
predict the low-cycle fatigue (LCF) life of irradiated austenitic and
ferritic/martensitic (F/M) steels used in nuclear reactors. These materials
experience cyclic loading and irradiation at elevated temperatures, causing
complex degradation that traditional empirical models fail to capture
accurately. The developed PINN model incorporates physical fatigue life
constraints into its loss function, improving prediction accuracy and
generalizability. Trained on 495 data points, including both irradiated and
unirradiated conditions, the model outperforms traditional machine learning
models like Random Forest, Gradient Boosting, eXtreme Gradient Boosting, and
the conventional Neural Network. SHapley Additive exPlanations analysis
identifies strain amplitude, irradiation dose, and testing temperature as
dominant features, each inversely correlated with fatigue life, consistent with
physical understanding. PINN captures saturation behaviour in fatigue life at
higher strain amplitudes in F/M steels. Overall, the PINN framework offers a
reliable and interpretable approach for predicting fatigue life in irradiated
alloys, enabling informed alloy selection.

</details>


### [158] [AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations](https://arxiv.org/abs/2508.17320)
*Yifei Yao,Mengnan Du*

Main category: cs.LG

TL;DR: 提出了Adaptive Top K稀疏自编码器(AdaptiveK)，通过基于输入语义复杂度动态调整稀疏度水平，显著优于固定稀疏度方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器使用固定稀疏度约束，无法适应不同输入的复杂度变化，需要一种能够根据输入语义复杂度动态调整稀疏度的方法。

Method: 利用线性探针证明上下文复杂度在线性编码在LLM表示中，并以此信号指导训练过程中的特征分配，动态调整稀疏度水平。

Result: 在三个语言模型(Pythia-70M、Pythia-160M和Gemma-2-2B)上的实验表明，该方法在重建保真度、解释方差和余弦相似度指标上显著优于固定稀疏度方法，同时消除了大量超参数调优的计算负担。

Conclusion: AdaptiveK框架通过动态调整稀疏度水平，有效解决了固定稀疏度约束的局限性，为LLM内部表示的可解释性研究提供了更优的解决方案。

Abstract: Understanding the internal representations of large language models (LLMs)
remains a central challenge for interpretability research. Sparse autoencoders
(SAEs) offer a promising solution by decomposing activations into interpretable
features, but existing approaches rely on fixed sparsity constraints that fail
to account for input complexity. We propose Adaptive Top K Sparse Autoencoders
(AdaptiveK), a novel framework that dynamically adjusts sparsity levels based
on the semantic complexity of each input. Leveraging linear probes, we
demonstrate that context complexity is linearly encoded in LLM representations,
and we use this signal to guide feature allocation during training. Experiments
across three language models (Pythia-70M, Pythia-160M, and Gemma-2-2B)
demonstrate that this complexity-driven adaptation significantly outperforms
fixed-sparsity approaches on reconstruction fidelity, explained variance, and
cosine similarity metrics while eliminating the computational burden of
extensive hyperparameter tuning.

</details>


### [159] [Is the Frequency Principle always valid?](https://arxiv.org/abs/2508.17323)
*Qijia Zhai*

Main category: cs.LG

TL;DR: 本文研究了浅层ReLU神经网络在单位球面S²上的学习动力学，发现频率优先原则(FP)在球面上表现为趋势而非绝对规则，固定权重和可训练权重都显示低频优先学习倾向，但在特定条件下可能被违反。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在弯曲流形（如球面）上的学习动态，特别是频率优先原则在非欧几里得空间中的表现，以理解方向更新和谐波展开如何影响频率依赖的学习行为。

Method: 使用球谐函数展开分析浅层ReLU网络在S²球面上的学习动态，比较固定神经元方向和可训练方向两种情况，通过理论分析和数值实验验证结果。

Result: 固定权重时球谐系数以O(ℓ^{5/2}/2^ℓ)衰减，通常呈现低频优先学习；可训练权重时衰减阶数为O(ℓ^{7/2}/2^ℓ)，也倾向于低频优先。但两种情况下在特定初始条件或误差分布下都可能违反频率优先原则。

Conclusion: 频率优先原则在球面等弯曲域上应被视为一种趋势而非绝对规则，可训练方向增加了学习复杂性，既能保持低频优势也能实现更快的高频涌现。

Abstract: We investigate the learning dynamics of shallow ReLU neural networks on the
unit sphere \(S^2\subset\mathbb{R}^3\) in polar coordinates \((\tau,\phi)\),
considering both fixed and trainable neuron directions \(\{w_i\}\). For fixed
weights, spherical harmonic expansions reveal an intrinsic low-frequency
preference with coefficients decaying as \(O(\ell^{5/2}/2^\ell)\), typically
leading to the Frequency Principle (FP) of lower-frequency-first learning.
However, this principle can be violated under specific initial conditions or
error distributions. With trainable weights, an additional rotation term in the
harmonic evolution equations preserves exponential decay with decay order
\(O(\ell^{7/2}/2^\ell)\) factor, also leading to the FP of
lower-frequency-first learning. But like fixed weights case, the principle can
be violated under specific initial conditions or error distributions. Our
numerical results demonstrate that trainable directions increase learning
complexity and can either maintain a low-frequency advantage or enable faster
high-frequency emergence. This analysis suggests the FP should be viewed as a
tendency rather than a rule on curved domains like \(S^2\), providing insights
into how direction updates and harmonic expansions shape frequency-dependent
learning.

</details>


### [160] [MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems](https://arxiv.org/abs/2508.17341)
*Muhammet Anil Yagiz,Zeynep Sude Cengiz,Polat Goktas*

Main category: cs.LG

TL;DR: MetaFed是一个去中心化的联邦学习框架，通过多智能体强化学习、同态加密和碳感知调度，为元宇宙应用提供可持续的智能资源编排，减少25%碳排放同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 集中式架构在元宇宙应用中存在高能耗、延迟和隐私问题，无法满足性能、隐私和环境可持续性的复杂需求。

Method: 集成多智能体强化学习进行动态客户端选择，使用同态加密实现隐私保护的联邦学习，采用碳感知调度与可再生能源可用性对齐。

Result: 在MNIST和CIFAR-10数据集上使用轻量级ResNet架构评估，相比传统方法减少25%碳排放，同时保持高精度和最小通信开销。

Conclusion: MetaFed是构建环境友好且隐私合规的元宇宙基础设施的可扩展解决方案。

Abstract: The rapid expansion of immersive Metaverse applications introduces complex
challenges at the intersection of performance, privacy, and environmental
sustainability. Centralized architectures fall short in addressing these
demands, often resulting in elevated energy consumption, latency, and privacy
concerns. This paper proposes MetaFed, a decentralized federated learning (FL)
framework that enables sustainable and intelligent resource orchestration for
Metaverse environments. MetaFed integrates (i) multi-agent reinforcement
learning for dynamic client selection, (ii) privacy-preserving FL using
homomorphic encryption, and (iii) carbon-aware scheduling aligned with
renewable energy availability. Evaluations on MNIST and CIFAR-10 using
lightweight ResNet architectures demonstrate that MetaFed achieves up to 25\%
reduction in carbon emissions compared to conventional approaches, while
maintaining high accuracy and minimal communication overhead. These results
highlight MetaFed as a scalable solution for building environmentally
responsible and privacy-compliant Metaverse infrastructures.

</details>


### [161] [ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable Generation](https://arxiv.org/abs/2508.17345)
*Yuxuan Song,Zhe Zhang,Yu Pei,Jingjing Gong,Qiying Yu,Zheng Zhang,Mingxuan Wang,Hao Zhou,Jingjing Liu,Wei-Ying Ma*

Main category: cs.LG

TL;DR: SLM是一种基于单纯形质心的新型扩散模型，通过候选剪枝机制降低生成复杂度并提升可扩展性，在离散变量生成任务中表现出色


<details>
  <summary>Details</summary>
Motivation: 离散变量的生成建模在自然语言处理和生物序列设计中具有重要应用价值，但面临挑战，需要开发更高效的生成模型

Method: 提出Shortlisting Model (SLM)，基于单纯形质心的扩散模型，采用渐进式候选剪枝机制，并整合了灵活的无分类器引导实现

Result: 在DNA启动子和增强子设计、蛋白质设计、字符级和大词汇量语言建模等任务上展示了竞争性性能和强大潜力

Conclusion: SLM为离散变量生成提供了一种有效的解决方案，在多个应用领域展现出优越的性能和可扩展性

Abstract: Generative modeling of discrete variables is challenging yet crucial for
applications in natural language processing and biological sequence design. We
introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model
inspired by progressive candidate pruning. SLM operates on simplex centroids,
reducing generation complexity and enhancing scalability. Additionally, SLM
incorporates a flexible implementation of classifier-free guidance, enhancing
unconditional generation performance. Extensive experiments on DNA promoter and
enhancer design, protein design, character-level and large-vocabulary language
modeling demonstrate the competitive performance and strong potential of SLM.
Our code can be found at https://github.com/GenSI-THUAIR/SLM

</details>


### [162] [Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias](https://arxiv.org/abs/2508.17361)
*Shir Bernstein,David Beste,Daniel Ayzenshteyn,Lea Schonherr,Yisroel Mirsky*

Main category: cs.LG

TL;DR: 本文发现LLM代码分析存在抽象偏见漏洞，攻击者可通过熟悉模式攻击(FPA)用最小编辑操控模型解释，而不影响实际运行时行为。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛用于自动化代码审查和静态分析，需要识别其潜在漏洞以确保代码分析的可信度和安全性。

Method: 开发了全自动黑盒算法来发现和注入熟悉模式攻击(FPA)，在多种编程语言和主流LLM模型上进行评估。

Result: FPA攻击在GPT-4o、Claude 3.5、Gemini 2.0等模型上有效且可迁移，即使有防御性系统提示也能成功，同时具有跨编程语言的通用性。

Conclusion: FPA揭示了LLM代码分析系统的关键安全漏洞，需要重新评估代码导向LLM的可靠性和安全性，同时FPA也可用于防御性用途。

Abstract: Large Language Models (LLMs) are increasingly trusted to perform automated
code review and static analysis at scale, supporting tasks such as
vulnerability detection, summarization, and refactoring. In this paper, we
identify and exploit a critical vulnerability in LLM-based code analysis: an
abstraction bias that causes models to overgeneralize familiar programming
patterns and overlook small, meaningful bugs. Adversaries can exploit this
blind spot to hijack the control flow of the LLM's interpretation with minimal
edits and without affecting actual runtime behavior. We refer to this attack as
a Familiar Pattern Attack (FPA).
  We develop a fully automated, black-box algorithm that discovers and injects
FPAs into target code. Our evaluation shows that FPAs are not only effective,
but also transferable across models (GPT-4o, Claude 3.5, Gemini 2.0) and
universal across programming languages (Python, C, Rust, Go). Moreover, FPAs
remain effective even when models are explicitly warned about the attack via
robust system prompts. Finally, we explore positive, defensive uses of FPAs and
discuss their broader implications for the reliability and safety of
code-oriented LLMs.

</details>


### [163] [ShaLa: Multimodal Shared Latent Space Modelling](https://arxiv.org/abs/2508.17376)
*Jiali Cui,Yan-Ying Chen,Yanxia Zhang,Matthew Klenk*

Main category: cs.LG

TL;DR: ShaLa是一个新颖的多模态生成框架，通过整合创新的推理模型和扩散先验，解决了多模态VAE在共享潜在表示学习和合成质量方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法过于关注模态特定的细节组合，反而模糊了跨模态共享的高层语义概念。多模态VAE虽然设计用于捕获共享表示，但在表达性联合变分后验设计和合成质量方面存在困难。

Method: ShaLa整合了新颖的架构推理模型和第二阶段表达性扩散先验，既促进了共享潜在表示的有效推理，又显著提高了下游多模态合成的质量。

Result: 在多个基准测试中验证，相比最先进的多模态VAE，ShaLa表现出更优越的一致性和合成质量，并且能够扩展到更多模态。

Conclusion: ShaLa成功解决了多模态VAE的关键挑战，在共享潜在表示学习和多模态合成方面实现了显著改进，具有更好的可扩展性。

Abstract: This paper presents a novel generative framework for learning shared latent
representations across multimodal data. Many advanced multimodal methods focus
on capturing all combinations of modality-specific details across inputs, which
can inadvertently obscure the high-level semantic concepts that are shared
across modalities. Notably, Multimodal VAEs with low-dimensional latent
variables are designed to capture shared representations, enabling various
tasks such as joint multimodal synthesis and cross-modal inference. However,
multimodal VAEs often struggle to design expressive joint variational
posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses
these challenges by integrating a novel architectural inference model and a
second-stage expressive diffusion prior, which not only facilitates effective
inference of shared latent representation but also significantly improves the
quality of downstream multimodal synthesis. We validate ShaLa extensively
across multiple benchmarks, demonstrating superior coherence and synthesis
quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales
to many more modalities while prior multimodal VAEs have fallen short in
capturing the increasing complexity of the shared latent space.

</details>


### [164] [FedERL: Federated Efficient and Robust Learning for Common Corruptions](https://arxiv.org/abs/2508.17381)
*Omar Bekdache,Naresh Shanbhag*

Main category: cs.LG

TL;DR: FedERL是首个在联邦学习中同时处理客户端计算资源约束和常见数据损坏鲁棒性的方法，通过服务器端的数据无关鲁棒训练实现零客户端开销


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端计算资源限制和对常见数据损坏（如噪声、模糊、天气效应）缺乏鲁棒性的挑战，现有鲁棒训练方法计算成本高且不适合资源受限的客户端

Method: 提出FedERL框架，采用新颖的数据无关鲁棒训练（DART）方法在服务器端增强鲁棒性，无需访问训练数据，确保客户端零鲁棒性开销

Result: 大量实验表明FedERL能够以传统鲁棒训练方法的一小部分时间和能耗成本处理常见数据损坏，在有限时间和能量预算场景下性能超越传统方法

Conclusion: FedERL为现实世界联邦学习应用提供了一个实用且可扩展的解决方案，有效解决了资源约束下的鲁棒性训练问题

Abstract: Federated learning (FL) accelerates the deployment of deep learning models on
edge devices while preserving data privacy. However, FL systems face challenges
due to client-side constraints on computational resources, and from a lack of
robustness to common corruptions such as noise, blur, and weather effects.
Existing robust training methods are computationally expensive and unsuitable
for resource-constrained clients. We propose FedERL, federated efficient and
robust learning, as the first work to explicitly address corruption robustness
under time and energy constraints on the client side. At its core, FedERL
employs a novel data-agnostic robust training (DART) method on the server to
enhance robustness without access to the training data. In doing so, FedERL
ensures zero robustness overhead for clients. Extensive experiments demonstrate
FedERL's ability to handle common corruptions at a fraction of the time and
energy cost of traditional robust training methods. In scenarios with limited
time and energy budgets, FedERL surpasses the performance of traditional robust
training, establishing it as a practical and scalable solution for real-world
FL applications.

</details>


### [165] [Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning](https://arxiv.org/abs/2508.17387)
*Yicong Wu,Guangyue Lu,Yuan Zuo,Huarong Zhang,Junjie Wu*

Main category: cs.LG

TL;DR: 提出了Graph-R1框架，将图任务重新表述为文本推理问题，使用强化学习和大推理模型在零样本设置下超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络固定标签空间的限制和大语言模型缺乏结构归纳偏置的问题，探索通过显式推理链进行图学习的可能性

Method: 将节点分类、链接预测和图分类任务重新表述为文本推理问题，开发包含详细推理痕迹的数据集，使用强化学习框架和任务特定的重新思考模板来指导线性化图上的推理

Result: Graph-R1在零样本设置下优于最先进的基线方法，产生可解释且有效的预测

Conclusion: 显式推理在图学习中具有巨大潜力，为未来研究提供了新的资源和方向

Abstract: Generalizing to unseen graph tasks without task-pecific supervision remains
challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,
while Large Language Models (LLMs) lack structural inductive biases. Recent
advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via
explicit, long chain-of-thought reasoning. Inspired by this, we propose a
GNN-free approach that reformulates graph tasks--node classification, link
prediction, and graph classification--as textual reasoning problems solved by
LRMs. We introduce the first datasets with detailed reasoning traces for these
tasks and develop Graph-R1, a reinforcement learning framework that leverages
task-specific rethink templates to guide reasoning over linearized graphs.
Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in
zero-shot settings, producing interpretable and effective predictions. Our work
highlights the promise of explicit reasoning for graph learning and provides
new resources for future research.

</details>


### [166] [Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems](https://arxiv.org/abs/2508.17403)
*Yinsong Wang,Xiao Liu,Quan Zeng,Yu Ding*

Main category: cs.LG

TL;DR: 提出了互信息惊喜(MIS)框架，将惊喜重新定义为认知增长的信号而非异常检测，并开发了MIS反应策略来动态调整自主系统的行为。


<details>
  <summary>Details</summary>
Motivation: 现有自主实验系统缺乏检测和适应意外情况的机制，传统惊喜度量只能检测瞬时偏差，无法捕捉系统是否真正在学习适应。

Method: 引入互信息惊喜(MIS)量化新观测对互信息的影响，开发统计测试序列检测互信息的有意义变化，提出MIS反应策略(MISRP)通过采样调整和进程分叉动态控制系统行为。

Result: 在合成领域和动态污染地图估计任务中，MISRP治理的策略在稳定性、响应性和预测准确性方面显著优于传统基于惊喜的方法。

Conclusion: MIS将惊喜从反应性转变为反思性，为更自我意识和自适应的自主系统提供了路径。

Abstract: Recent breakthroughs in autonomous experimentation have demonstrated
remarkable physical capabilities, yet their cognitive control remains
limited--often relying on static heuristics or classical optimization. A core
limitation is the absence of a principled mechanism to detect and adapt to the
unexpectedness. While traditional surprise measures--such as Shannon or
Bayesian Surprise--offer momentary detection of deviation, they fail to capture
whether a system is truly learning and adapting. In this work, we introduce
Mutual Information Surprise (MIS), a new framework that redefines surprise not
as anomaly detection, but as a signal of epistemic growth. MIS quantifies the
impact of new observations on mutual information, enabling autonomous systems
to reflect on their learning progression. We develop a statistical test
sequence to detect meaningful shifts in estimated mutual information and
propose a mutual information surprise reaction policy (MISRP) that dynamically
governs system behavior through sampling adjustment and process forking.
Empirical evaluations--on both synthetic domains and a dynamic pollution map
estimation task--show that MISRP-governed strategies significantly outperform
classical surprise-based approaches in stability, responsiveness, and
predictive accuracy. By shifting surprise from reactive to reflective, MIS
offers a path toward more self-aware and adaptive autonomous systems.

</details>


### [167] [FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats](https://arxiv.org/abs/2508.17405)
*Avishag Shapira,Simon Shigol,Asaf Shabtai*

Main category: cs.LG

TL;DR: FRAME是一个首个全面自动化评估对抗性机器学习风险的框架，通过系统评估部署环境、AML技术特性和实证研究，为不同ML系统提供可操作的风险评估结果。


<details>
  <summary>Details</summary>
Motivation: 传统风险评估框架无法有效应对对抗性机器学习(AML)的独特挑战，现有AML评估方法主要关注技术攻击鲁棒性，忽视了部署环境、系统依赖和攻击可行性等现实因素，且缺乏跨领域应用的通用解决方案。

Method: FRAME框架包含新颖的风险评估方法，通过量化评估三个关键维度：目标系统部署环境、多样化AML技术特性、先前研究的实证洞察。采用可行性评分机制和基于LLM的系统特定评估定制，并开发了全面的结构化AML攻击数据集。

Result: 在六个不同的真实世界应用中验证，FRAME展现出卓越的准确性和与AML专家分析的高度一致性，能够为系统所有者提供直接可用的评估结果。

Conclusion: FRAME使组织能够优先处理AML风险，支持在真实环境中安全部署AI系统，为缺乏AML专业知识的系统所有者提供了实用的风险评估工具。

Abstract: The widespread adoption of machine learning (ML) systems increased attention
to their security and emergence of adversarial machine learning (AML)
techniques that exploit fundamental vulnerabilities in ML systems, creating an
urgent need for comprehensive risk assessment for ML-based systems. While
traditional risk assessment frameworks evaluate conventional cybersecurity
risks, they lack ability to address unique challenges posed by AML threats.
Existing AML threat evaluation approaches focus primarily on technical attack
robustness, overlooking crucial real-world factors like deployment
environments, system dependencies, and attack feasibility. Attempts at
comprehensive AML risk assessment have been limited to domain-specific
solutions, preventing application across diverse systems. Addressing these
limitations, we present FRAME, the first comprehensive and automated framework
for assessing AML risks across diverse ML-based systems. FRAME includes a novel
risk assessment method that quantifies AML risks by systematically evaluating
three key dimensions: target system's deployment environment, characteristics
of diverse AML techniques, and empirical insights from prior research. FRAME
incorporates a feasibility scoring mechanism and LLM-based customization for
system-specific assessments. Additionally, we developed a comprehensive
structured dataset of AML attacks enabling context-aware risk assessment. From
an engineering application perspective, FRAME delivers actionable results
designed for direct use by system owners with only technical knowledge of their
systems, without expertise in AML. We validated it across six diverse
real-world applications. Our evaluation demonstrated exceptional accuracy and
strong alignment with analysis by AML experts. FRAME enables organizations to
prioritize AML risks, supporting secure AI deployment in real-world
environments.

</details>


### [168] [Convergence and Generalization of Anti-Regularization for Parametric Models](https://arxiv.org/abs/2508.17412)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 提出反正则化(AR)方法，通过添加符号反转的奖励项来增加小样本情况下的模型表达能力，并采用幂律衰减机制随样本量增长而减弱干预。


<details>
  <summary>Details</summary>
Motivation: 解决小样本学习中的欠拟合问题，通过增加模型表达能力来提升性能，同时避免过拟合和数值不稳定。

Method: 在损失函数中添加符号反转的奖励项，使用幂律衰减机制控制干预强度，结合投影算子和梯度裁剪确保稳定性，分析涵盖线性平滑器和神经正切核机制。

Result: 实验表明AR能减少欠拟合，保持泛化能力，改善校准性能，在回归和分类任务中均有效。消融研究证实衰减计划和稳定性保障机制的关键作用。

Conclusion: AR方法简单易实现，能整合到标准经验风险最小化流程中，在数据和资源受限环境下实现鲁棒学习，只在有益时干预并在不必要时消退。

Abstract: We propose Anti-regularization (AR), which adds a sign-reversed reward term
to the loss to intentionally increase model expressivity in the small-sample
regime, and then attenuates this intervention with a power-law decay as the
sample size grows. We formalize spectral safety and trust-region conditions,
and design a lightweight stability safeguard that combines a projection
operator with gradient clipping, ensuring stable intervention under stated
assumptions. Our analysis spans linear smoothers and the Neural Tangent Kernel
(NTK) regime, providing practical guidance on selecting the decay exponent by
balancing empirical risk against variance. Empirically, AR reduces underfitting
while preserving generalization and improving calibration in both regression
and classification. Ablation studies confirm that the decay schedule and the
stability safeguard are critical to preventing overfitting and numerical
instability. We further examine a degrees-of-freedom targeting schedule that
keeps per-sample complexity approximately constant. AR is simple to implement
and reproducible, integrating cleanly into standard empirical risk minimization
pipelines. It enables robust learning in data- and resource-constrained
settings by intervening only when beneficial and fading away when unnecessary.

</details>


### [169] [Modular MeanFlow: Towards Stable and Scalable One-Step Generative Modeling](https://arxiv.org/abs/2508.17426)
*Haochen You,Baojing Liu,Hongyang He*

Main category: cs.LG

TL;DR: MMF是一种一步生成建模方法，通过学习时间平均速度场实现高效数据生成，避免了传统扩散模型的多步采样过程。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型和流模型需要多步函数评估，生成效率较低。本文旨在开发一种单步生成方法，既能保持样本质量又能显著提高效率。

Method: 提出了Modular MeanFlow (MMF)方法，基于瞬时速度与平均速度的微分恒等式推导损失函数族，包含梯度调制机制和课程式预热调度，避免高阶导数计算。

Result: 在图像合成和轨迹建模任务中，MMF实现了竞争性的样本质量、鲁棒收敛和强泛化能力，特别是在低数据或分布外设置下表现优异。

Conclusion: MMF提供了一个统一且理论严谨的框架，将现有一致性方法和流匹配方法泛化，实现了高效的单步生成建模。

Abstract: One-step generative modeling seeks to generate high-quality data samples in a
single function evaluation, significantly improving efficiency over traditional
diffusion or flow-based models. In this work, we introduce Modular MeanFlow
(MMF), a flexible and theoretically grounded approach for learning
time-averaged velocity fields. Our method derives a family of loss functions
based on a differential identity linking instantaneous and average velocities,
and incorporates a gradient modulation mechanism that enables stable training
without sacrificing expressiveness. We further propose a curriculum-style
warmup schedule to smoothly transition from coarse supervision to fully
differentiable training. The MMF formulation unifies and generalizes existing
consistency-based and flow-matching methods, while avoiding expensive
higher-order derivatives. Empirical results across image synthesis and
trajectory modeling tasks demonstrate that MMF achieves competitive sample
quality, robust convergence, and strong generalization, particularly under
low-data or out-of-distribution settings.

</details>


### [170] [Multimodal Representation Learning Conditioned on Semantic Relations](https://arxiv.org/abs/2508.17497)
*Yang Qiao,Yuntong Hu,Liang Zhao*

Main category: cs.LG

TL;DR: 提出了RCML框架，通过自然语言关系描述指导多模态表示学习，解决现有对比学习模型在语义关系利用、上下文对齐和模态内一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对比学习模型（如CLIP）存在三个主要问题：1）主要关注图像-文本对，未能充分利用不同对之间的语义关系；2）直接匹配全局嵌入而缺乏上下文化，忽略了特定子空间或关系维度的语义对齐需求；3）强调跨模态对比，对模态内一致性的支持有限。

Method: 提出Relation-Conditioned Multimodal Learning (RCML)框架：1）构建由语义关系连接的多样化训练对；2）引入关系引导的交叉注意力机制，在每种关系上下文中调节多模态表示；3）结合跨模态和模态内对比损失进行训练目标。

Result: 在不同数据集上的实验表明，RCML在检索和分类任务上均持续优于强基线模型。

Conclusion: 利用语义关系指导多模态表示学习是有效的，RCML框架通过关系条件化学习显著提升了多模态表示的质量和性能。

Abstract: Multimodal representation learning has advanced rapidly with contrastive
models such as CLIP, which align image-text pairs in a shared embedding space.
However, these models face limitations: (1) they typically focus on image-text
pairs, underutilizing the semantic relations across different pairs. (2) they
directly match global embeddings without contextualization, overlooking the
need for semantic alignment along specific subspaces or relational dimensions;
and (3) they emphasize cross-modal contrast, with limited support for
intra-modal consistency. To address these issues, we propose
Relation-Conditioned Multimodal Learning RCML, a framework that learns
multimodal representations under natural-language relation descriptions to
guide both feature extraction and alignment. Our approach constructs
many-to-many training pairs linked by semantic relations and introduces a
relation-guided cross-attention mechanism that modulates multimodal
representations under each relation context. The training objective combines
inter-modal and intra-modal contrastive losses, encouraging consistency across
both modalities and semantically related samples. Experiments on different
datasets show that RCML consistently outperforms strong baselines on both
retrieval and classification tasks, highlighting the effectiveness of
leveraging semantic relations to guide multimodal representation learning.

</details>


### [171] [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
*Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: TreePO是一种基于树结构搜索的强化学习对齐方法，通过动态树采样和分段解码减少计算负担，同时保持推理路径多样性，在多个基准测试中实现了22-43%的GPU时间节省。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型强化学习对齐方法虽然能解决复杂推理问题，但存在昂贵的在线rollout成本和有限的推理路径探索问题，需要更高效的方法。

Method: TreePO采用自引导rollout算法，将序列生成视为树结构搜索过程，包含动态树采样策略和固定长度分段解码，利用局部不确定性保证分支扩展，并通过共享前缀计算和早期剪枝降低计算负担。

Result: 在推理基准测试中，TreePO实现了22-43%的GPU时间节省，轨迹级采样计算减少40%，token级减少35%，同时保持或提升了探索多样性。

Conclusion: TreePO为基于强化学习的后训练提供了一条实用路径，以更少的样本和计算成本实现推理效率的提升，同时保持模型性能。

Abstract: Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

</details>


### [172] [TANDEM: Temporal Attention-guided Neural Differential Equations for Missingness in Time Series Classification](https://arxiv.org/abs/2508.17519)
*YongKyung Oh,Dong-Young Lim,Sungil Kim,Alex Bui*

Main category: cs.LG

TL;DR: TANDEM是一个基于注意力机制和神经微分方程的框架，用于处理时间序列分类中的缺失数据问题，在30个基准数据集和真实医疗数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类中的缺失数据处理是一个重要挑战，传统插补方法可能引入偏差或无法捕捉时间动态特性。

Method: 提出TANDEM框架，通过注意力机制整合原始观测、插补控制路径和连续潜在动态，使用神经微分方程有效处理缺失数据。

Result: 在30个基准数据集和真实医疗数据集上验证，证明TANDEM优于现有最先进方法，提高了分类准确性。

Conclusion: TANDEM不仅提高了分类精度，还为缺失数据处理提供了新的见解，是具有实用价值的工具。

Abstract: Handling missing data in time series classification remains a significant
challenge in various domains. Traditional methods often rely on imputation,
which may introduce bias or fail to capture the underlying temporal dynamics.
In this paper, we propose TANDEM (Temporal Attention-guided Neural Differential
Equations for Missingness), an attention-guided neural differential equation
framework that effectively classifies time series data with missing values. Our
approach integrates raw observation, interpolated control path, and continuous
latent dynamics through a novel attention mechanism, allowing the model to
focus on the most informative aspects of the data. We evaluate TANDEM on 30
benchmark datasets and a real-world medical dataset, demonstrating its
superiority over existing state-of-the-art methods. Our framework not only
improves classification accuracy but also provides insights into the handling
of missing data, making it a valuable tool in practice.

</details>


### [173] [Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality](https://arxiv.org/abs/2508.17448)
*Shaocong Ma,Ziyi Chen,Yi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一种新的原始算法RRPO来解决鲁棒约束强化学习中的强对偶性缺失问题，通过直接处理原始问题而不依赖对偶公式，在模型不确定性下实现安全约束的满足。


<details>
  <summary>Details</summary>
Motivation: 传统的对偶方法在鲁棒约束RL中可能失效，因为强对偶性通常不成立，需要开发不依赖对偶公式的新算法来确保在模型不确定性下的安全约束满足。

Method: 提出Rectified Robust Policy Optimization (RRPO)算法，这是一种纯原始方法，直接处理原始优化问题，通过控制不确定性集直径来保证收敛性。

Result: 理论分析表明RRPO在温和正则性假设下具有收敛保证，迭代复杂度匹配已知下界。网格世界实验验证了算法有效性，RRPO在模型不确定性下实现了鲁棒安全性能，而非鲁棒方法可能违反最坏情况安全约束。

Conclusion: RRPO为解决鲁棒约束RL中的强对偶性缺失问题提供了有效的原始方法，能够在模型不确定性下保证安全约束的满足，具有理论保证和实际有效性。

Abstract: The goal of robust constrained reinforcement learning (RL) is to optimize an
agent's performance under the worst-case model uncertainty while satisfying
safety or resource constraints. In this paper, we demonstrate that strong
duality does not generally hold in robust constrained RL, indicating that
traditional primal-dual methods may fail to find optimal feasible policies. To
overcome this limitation, we propose a novel primal-only algorithm called
Rectified Robust Policy Optimization (RRPO), which operates directly on the
primal problem without relying on dual formulations. We provide theoretical
convergence guarantees under mild regularity assumptions, showing convergence
to an approximately optimal feasible policy with iteration complexity matching
the best-known lower bound when the uncertainty set diameter is controlled in a
specific level. Empirical results in a grid-world environment validate the
effectiveness of our approach, demonstrating that RRPO achieves robust and safe
performance under model uncertainties while the non-robust method can violate
the worst-case safety constraints.

</details>


### [174] [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
*Andrzej Szablewski,Marek Masiak*

Main category: cs.LG

TL;DR: 本文提出了激活传输算子（ATO）方法，用于分析transformer残差流中特征的线性传输机制，通过计算线性映射来识别特征是从上游层传输而来还是由非线性计算合成。


<details>
  <summary>Details</summary>
Motivation: 理解transformer残差流中特征的流动机制对于改进越狱防护、早期检测模型错误及其修正具有重要意义，但目前这一动态过程研究不足。

Method: 提出激活传输算子（ATO），即从上游到下游残差层的线性映射，使用下游SAE解码器投影在特征空间中进行评估，并开发传输效率概念来量化线性传输。

Result: 经验证明ATO能够确定特征是否从先前层线性传输而来，报告了传输效率和参与线性传输的残差流子空间大小，计算轻量（无需微调，<50 GPU小时）。

Conclusion: 该方法为安全性、调试提供了实用工具，并更清晰地展示了LLM中计算行为在哪些方面呈现线性特性。

Abstract: The residual stream mediates communication between transformer decoder layers
via linear reads and writes of non-linear computations. While sparse-dictionary
learning-based methods locate features in the residual stream, and activation
patching methods discover circuits within the model, the mechanism by which
features flow through the residual stream remains understudied. Understanding
this dynamic can better inform jailbreaking protections, enable early detection
of model mistakes, and their correction. In this work, we propose Activation
Transport Operators (ATO), linear maps from upstream to downstream residuals
$k$ layers later, evaluated in feature space using downstream SAE decoder
projections. We empirically demonstrate that these operators can determine
whether a feature has been linearly transported from a previous layer or
synthesised from non-linear layer computation. We develop the notion of
transport efficiency, for which we provide an upper bound, and use it to
estimate the size of the residual stream subspace that corresponds to linear
transport. We empirically demonstrate the linear transport, report transport
efficiency and the size of the residual stream's subspace involved in linear
transport. This compute-light (no finetuning, <50 GPU-h) method offers
practical tools for safety, debugging, and a clearer picture of where
computation in LLMs behaves linearly.

</details>


### [175] [ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories](https://arxiv.org/abs/2508.17452)
*Dou Jiabao,Nie Jiayi,Yihang Cheng,Jinwei Liu,Yingrui Ji,Canran Xiao,Feixiang Du,Jiaping Xiao*

Main category: cs.LG

TL;DR: ReviBranch是一个新颖的深度强化学习框架，通过重建分支决策与图状态之间的历史对应关系来解决MILP中分支变量选择的挑战，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统分支启发式方法难以泛化到异构问题实例，现有学习方法如模仿学习依赖专家演示质量，强化学习面临稀疏奖励和动态状态表示的挑战。

Method: 提出ReviBranch框架，通过复活搜索树路径上分支决策与图状态之间的显式历史对应关系来构建复活轨迹，并引入重要性加权的奖励重分配机制将稀疏终端奖励转化为密集的逐步反馈。

Result: 在不同MILP基准测试中，ReviBranch优于最先进的RL方法，在大规模实例上减少B&B节点4.0%和LP迭代2.2%。

Conclusion: ReviBranch展示了在异构MILP问题类别中的鲁棒性和泛化能力，为解决分支变量选择问题提供了有效的深度强化学习解决方案。

Abstract: The Branch-and-bound (B&B) algorithm is the main solver for Mixed Integer
Linear Programs (MILPs), where the selection of branching variable is essential
to computational efficiency. However, traditional heuristics for branching
often fail to generalize across heterogeneous problem instances, while existing
learning-based methods such as imitation learning (IL) suffers from dependence
on expert demonstration quality, and reinforcement learning (RL) struggles with
limitations in sparse rewards and dynamic state representation challenges. To
address these issues, we propose ReviBranch, a novel deep RL framework that
constructs revived trajectories by reviving explicit historical correspondences
between branching decisions and their corresponding graph states along
search-tree paths. During training, ReviBranch enables agents to learn from
complete structural evolution and temporal dependencies within the branching
process. Additionally, we introduce an importance-weighted reward
redistribution mechanism that transforms sparse terminal rewards into dense
stepwise feedback, addressing the sparse reward challenge. Extensive
experiments on different MILP benchmarks demonstrate that ReviBranch
outperforms state-of-the-art RL methods, reducing B&B nodes by 4.0% and LP
iterations by 2.2% on large-scale instances. The results highlight the
robustness and generalizability of ReviBranch across heterogeneous MILP problem
classes.

</details>


### [176] [In-Context Algorithm Emulation in Fixed-Weight Transformers](https://arxiv.org/abs/2508.17550)
*Jerry Yao-Chieh Hu,Hude Liu,Jennifer Yuntong Zhang,Han Liu*

Main category: cs.LG

TL;DR: 本文证明冻结权重的极简Transformer架构能够通过上下文提示模拟广泛算法类别，包括梯度下降和线性回归等，仅需两层的softmax注意力模块即可实现任意精度的算法输出。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型是否能够仅通过提示(prompt)而不更新参数来模拟各种算法，揭示上下文学习与算法模拟之间的直接联系。

Method: 构建特殊提示，将算法参数编码到token表示中，创建显著的dot-product差异，迫使softmax注意力遵循预期的计算过程，无需前馈层或参数更新。

Result: 证明了单头注意力层（可能需要更长的提示）能够实现架构最小化，精确模拟固定权重注意力头可实现的任何算法。

Conclusion: 这些发现建立了上下文学习与算法模拟的直接联系，为大型Transformer作为提示可编程算法库提供了简单机制，展示了现代Transformer模型的算法通用性。

Abstract: We prove that a minimal Transformer architecture with frozen weights is
capable of emulating a broad class of algorithms by in-context prompting. In
particular, for any algorithm implementable by a fixed-weight attention head
(e.g. one-step gradient descent or linear/ridge regression), there exists a
prompt that drives a two-layer softmax attention module to reproduce the
algorithm's output with arbitrary precision. This guarantee extends even to a
single-head attention layer (using longer prompts if necessary), achieving
architectural minimality. Our key idea is to construct prompts that encode an
algorithm's parameters into token representations, creating sharp dot-product
gaps that force the softmax attention to follow the intended computation. This
construction requires no feed-forward layers and no parameter updates. All
adaptation happens through the prompt alone. These findings forge a direct link
between in-context learning and algorithmic emulation, and offer a simple
mechanism for large Transformers to serve as prompt-programmable libraries of
algorithms. They illuminate how GPT-style foundation models may swap algorithms
via prompts alone, establishing a form of algorithmic universality in modern
Transformer models.

</details>


### [177] [A Systematic Literature Review on Multi-label Data Stream Classification](https://arxiv.org/abs/2508.17455)
*H. Freire-Oliveira,E. R. F. Paiva,J. Gama,L. Khan,R. Cerri*

Main category: cs.LG

TL;DR: 对多标签数据流分类方法的系统性文献综述，分析了该领域的最新进展、评估策略、计算复杂度，并指出了未来研究方向


<details>
  <summary>Details</summary>
Motivation: 多标签数据流分类在现实世界中具有高度适用性，但面临数据高速连续到达、概念漂移、新标签出现和真实标签延迟到达等挑战，需要系统性的综述分析

Method: 采用系统性文献综述方法，对多标签数据流分类提案进行深入分析，构建分类层次结构，讨论各方法的问题解决策略

Result: 提供了该领域的全面概述，建立了完整的分类体系，分析了方法的渐近复杂度和资源消耗

Conclusion: 识别了该领域的主要研究空白，并为未来的研究方向提供了建议

Abstract: Classification in the context of multi-label data streams represents a
challenge that has attracted significant attention due to its high real-world
applicability. However, this task faces problems inherent to dynamic
environments, such as the continuous arrival of data at high speed and volume,
changes in the data distribution (concept drift), the emergence of new labels
(concept evolution), and the latency in the arrival of ground truth labels.
This systematic literature review presents an in-depth analysis of multi-label
data stream classification proposals. We characterize the latest methods in the
literature, providing a comprehensive overview, building a thorough hierarchy,
and discussing how the proposals approach each problem. Furthermore, we discuss
the adopted evaluation strategies and analyze the methods' asymptotic
complexity and resource consumption. Finally, we identify the main gaps and
offer recommendations for future research directions in the field.

</details>


### [178] [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)
*Liv Gorton,Owen Lewis*

Main category: cs.LG

TL;DR: 该论文提出叠加(superposition)可能是对抗样本现象的主要成因，通过理论分析、玩具模型实验和ResNet18实验提供了四条证据链支持这一假设


<details>
  <summary>Details</summary>
Motivation: 对抗样本作为深度学习中最令人困惑的现象之一，近十年来缺乏共识性的解释机制。本文探索了机制可解释性中的叠加概念作为潜在的根本原因

Method: 采用四条证据链方法：(1)理论分析叠加解释对抗现象的能力；(2)在玩具模型中通过干预叠加来控制鲁棒性；(3)在玩具模型中通过对抗训练干预鲁棒性来控制叠加；(4)在ResNet18中通过对抗训练干预鲁棒性来控制叠加

Result: 研究证实叠加可以理论上解释多种对抗现象，在玩具模型和实际网络中都观察到叠加与鲁棒性之间的双向因果关系

Conclusion: 叠加很可能是对抗样本现象的主要贡献因素甚至是主要原因，这为理解对抗性脆弱性提供了新的机制解释框架

Abstract: Adversarial examples -- inputs with imperceptible perturbations that fool
neural networks -- remain one of deep learning's most perplexing phenomena
despite nearly a decade of research. While numerous defenses and explanations
have been proposed, there is no consensus on the fundamental mechanism. One
underexplored hypothesis is that superposition, a concept from mechanistic
interpretability, may be a major contributing factor, or even the primary
cause. We present four lines of evidence in support of this hypothesis, greatly
extending prior arguments by Elhage et al. (2022): (1) superposition can
theoretically explain a range of adversarial phenomena, (2) in toy models,
intervening on superposition controls robustness, (3) in toy models,
intervening on robustness (via adversarial training) controls superposition,
and (4) in ResNet18, intervening on robustness (via adversarial training)
controls superposition.

</details>


### [179] [ControlEchoSynth: Boosting Ejection Fraction Estimation Models via Controlled Video Diffusion](https://arxiv.org/abs/2508.17631)
*Nima Kondori,Hanwen Liang,Hooman Vaseli,Bingyu Xie,Christina Luong,Purang Abolmaesumi,Teresa Tsang,Renjie Liao*

Main category: cs.LG

TL;DR: 本研究提出了一种基于条件生成模型的合成超声心动图视图方法，通过增强现有数据集来提高射血分数估计的准确性，为医学影像诊断提供创新解决方案。


<details>
  <summary>Details</summary>
Motivation: 超声心动图获取和标注存在困难，特别是在床旁超声(POCUS)环境中，可用视图数量有限且操作者经验水平不一，这限制了机器学习模型的性能提升。

Method: 采用条件生成模型，基于现有的真实心脏视图合成生成超声心动图视图，重点关注射血分数(EF)这一关键参数的估计。

Result: 初步结果表明，使用合成回声数据增强现有数据集不仅能提高射血分数估计的准确性，还能促进开发更稳健、准确且临床相关的机器学习模型。

Conclusion: 该方法在提升临床诊断准确性方面显示出潜力，预计将推动合成数据在医学影像诊断领域的进一步研究和应用。

Abstract: Synthetic data generation represents a significant advancement in boosting
the performance of machine learning (ML) models, particularly in fields where
data acquisition is challenging, such as echocardiography. The acquisition and
labeling of echocardiograms (echo) for heart assessment, crucial in
point-of-care ultrasound (POCUS) settings, often encounter limitations due to
the restricted number of echo views available, typically captured by operators
with varying levels of experience. This study proposes a novel approach for
enhancing clinical diagnosis accuracy by synthetically generating echo views.
These views are conditioned on existing, real views of the heart, focusing
specifically on the estimation of ejection fraction (EF), a critical parameter
traditionally measured from biplane apical views. By integrating a conditional
generative model, we demonstrate an improvement in EF estimation accuracy,
providing a comparative analysis with traditional methods. Preliminary results
indicate that our synthetic echoes, when used to augment existing datasets, not
only enhance EF estimation but also show potential in advancing the development
of more robust, accurate, and clinically relevant ML models. This approach is
anticipated to catalyze further research in synthetic data applications, paving
the way for innovative solutions in medical imaging diagnostics.

</details>


### [180] [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models](https://arxiv.org/abs/2508.17467)
*Krishna Teja Chitty-Venkata,Sylvia Howland,Golara Azar,Daria Soboleva,Natalia Vassilieva,Siddhisanket Raskar,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: MoE-Inference-Bench是一个系统性评估混合专家模型推理性能的基准测试，分析了批处理大小、序列长度、专家数量等参数对吞吐量的影响，并在H100 GPU上评估了多种优化技术。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型虽然能够实现大规模参数的同时保持计算效率，但在推理时面临负载不均衡和路由计算开销等挑战，需要系统性的硬件加速技术评估。

Method: 通过构建MoE-Inference-Bench基准测试，分析不同批处理大小、序列长度、FFN维度和专家数量对性能的影响，并在Nvidia H100 GPU上评估剪枝、融合操作、推测解码、量化和并行化等优化技术。

Result: 研究揭示了不同配置下的性能差异，包括Mixtral、DeepSeek、OLMoE和Qwen等MoE模型的性能表现，为高效部署提供了重要见解。

Conclusion: 该研究为混合专家模型的高效推理部署提供了系统性的评估框架和优化指导，有助于充分发挥MoE架构的优势。

Abstract: Mixture of Experts (MoE) models have enabled the scaling of Large Language
Models (LLMs) and Vision Language Models (VLMs) by achieving massive parameter
counts while maintaining computational efficiency. However, MoEs introduce
several inference-time challenges, including load imbalance across experts and
the additional routing computational overhead. To address these challenges and
fully harness the benefits of MoE, a systematic evaluation of hardware
acceleration techniques is essential. We present MoE-Inference-Bench, a
comprehensive study to evaluate MoE performance across diverse scenarios. We
analyze the impact of batch size, sequence length, and critical MoE
hyperparameters such as FFN dimensions and number of experts on throughput. We
evaluate several optimization techniques on Nvidia H100 GPUs, including
pruning, Fused MoE operations, speculative decoding, quantization, and various
parallelization strategies. Our evaluation includes MoEs from the Mixtral,
DeepSeek, OLMoE and Qwen families. The results reveal performance differences
across configurations and provide insights for the efficient deployment of
MoEs.

</details>


### [181] [A Human-In-The-Loop Approach for Improving Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.17477)
*Martin Käppel,Julian Neuberger,Felix Möhrlein,Sven Weinzierl,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的模型无关方法，通过人工参与的决策树模型来识别和缩正预测性过程监控中的偏见决策，并在敏感属性同时被公平和不公平使用的情况下实现了公平性与准确性的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 预测性过程监控模型虽然强大，但数据驱动特性使它们容易受到不公平、偏见或不道德的数据模式影响。以前的方法通过完全移除敏感属性来减轻偏见，但敏感属性在同一过程中可能同时被公平和不公平地使用。

Method: 提出了一种模型无关的方法，采用人工参与的方式，通过对来自原始预测模型的决策树模型进行简单修改，来区分公平和不公平的决策。

Result: 结果显示，该方法在存在偏见数据的情况下，实现了公平性与准确性之间的有前晨景的平衡。

Conclusion: 该研究提供了一种有效的方法来处理预测性过程监控中的偏见问题，特别是在敏感属性同时被合理和不合理使用的复杂场景下，为企业提供了更公平的决策支持。

Abstract: Predictive process monitoring enables organizations to proactively react and
intervene in running instances of a business process. Given an incomplete
process instance, predictions about the outcome, next activity, or remaining
time are created. This is done by powerful machine learning models, which have
shown impressive predictive performance. However, the data-driven nature of
these models makes them susceptible to finding unfair, biased, or unethical
patterns in the data. Such patterns lead to biased predictions based on
so-called sensitive attributes, such as the gender or age of process
participants. Previous work has identified this problem and offered solutions
that mitigate biases by removing sensitive attributes entirely from the process
instance. However, sensitive attributes can be used both fairly and unfairly in
the same process instance. For example, during a medical process, treatment
decisions could be based on gender, while the decision to accept a patient
should not be based on gender. This paper proposes a novel, model-agnostic
approach for identifying and rectifying biased decisions in predictive business
process monitoring models, even when the same sensitive attribute is used both
fairly and unfairly. The proposed approach uses a human-in-the-loop approach to
differentiate between fair and unfair decisions through simple alterations on a
decision tree model distilled from the original prediction model. Our results
show that the proposed approach achieves a promising tradeoff between fairness
and accuracy in the presence of biased data. All source code and data are
publicly available at https://doi.org/10.5281/zenodo.15387576.

</details>


### [182] [Robustness Feature Adapter for Efficient Adversarial Training](https://arxiv.org/abs/2508.17680)
*Quanwei Wu,Jun Guo,Wei Wang,Yi Wang*

Main category: cs.LG

TL;DR: 提出基于适配器的高效对抗训练方法，直接在特征空间进行训练，解决了计算开销大和鲁棒过拟合问题


<details>
  <summary>Details</summary>
Motivation: 传统对抗训练在大规模骨干模型上计算开销过大，且存在鲁棒过拟合问题，需要同时解决这两个问题来构建更可信的基础模型

Method: 基于适配器的特征空间对抗训练方法，通过消除鲁棒过拟合来提高内循环收敛质量

Result: 显著提高计算效率，改善模型精度，并将对抗鲁棒性泛化到未见过的攻击

Conclusion: 该方法在不同骨干架构和大规模对抗训练中均表现出有效性，为构建可信基础模型提供了高效解决方案

Abstract: Adversarial training (AT) with projected gradient descent is the most popular
method to improve model robustness under adversarial attacks. However,
computational overheads become prohibitively large when AT is applied to large
backbone models. AT is also known to have the issue of robust overfitting. This
paper contributes to solving both problems simultaneously towards building more
trustworthy foundation models. In particular, we propose a new adapter-based
approach for efficient AT directly in the feature space. We show that the
proposed adapter-based approach can improve the inner-loop convergence quality
by eliminating robust overfitting. As a result, it significantly increases
computational efficiency and improves model accuracy by generalizing
adversarial robustness to unseen attacks. We demonstrate the effectiveness of
the new adapter-based approach in different backbone architectures and in AT at
scale.

</details>


### [183] [Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery](https://arxiv.org/abs/2508.17681)
*Robert Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为"遗忘即消融"的新方法，通过系统性地移除特定科学结果及其相关推论，来测试大型语言模型是否真正具备生成新科学知识的能力，而不仅仅是记忆和重组已有信息。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI在科学中作用的夸大宣称（如AGI将治愈所有疾病）引发了一个核心认识论问题：大型语言模型是真正生成新知识，还是仅仅重组记忆片段？需要一种可证伪的测试方法来区分这两种能力。

Method: 提出"遗忘即消融"方法：系统性地移除目标结果及其完整的遗忘闭包（包括引理、改写和多步蕴含），然后评估模型是否仅从允许的公理和工具中重新推导出该结果。

Result: 这是一个概念性方法论论文，没有提供具体的实证结果。但提出了一个原则性框架来映射AI科学发现的真实范围和限制。

Conclusion: 遗忘即消融测试可以作为下一代基准测试，类似于ImageNet在视觉领域的催化作用，能够区分仅能回忆的模型和能够建设性生成新科学知识的模型。这种方法为评估AI科学发现能力提供了原则性框架。

Abstract: Bold claims about AI's role in science-from "AGI will cure all diseases" to
promises of radically accelerated discovery-raise a central epistemic question:
do large language models (LLMs) truly generate new knowledge, or do they merely
remix memorized fragments? We propose unlearning-as-ablation as a falsifiable
test of constructive scientific discovery. The method systematically removes a
target result and its entire forget-closure (lemmas, paraphrases, and multi-hop
entailments) and then evaluates whether the model can re-derive the result from
only permitted axioms and tools. Success provides evidence for genuine
generative capability; failure exposes current limits. Unlike prevailing
motivations for unlearning-privacy, copyright, or safety-our framing
repositions it as an epistemic probe for AI-for-Science. We argue that such
tests could serve as the next generation of benchmarks, much as ImageNet
catalyzed progress in vision: distinguishing models that can merely recall from
those that can constructively generate new scientific knowledge. We outline a
minimal pilot in mathematics and algorithms, and discuss extensions to physics,
chemistry, and biology. Whether models succeed or fail, unlearning-as-ablation
provides a principled framework to map the true reach and limits of AI
scientific discovery. This is a position paper: we advance a conceptual and
methodological argument rather than new empirical results.

</details>


### [184] [Learning Interpretable Differentiable Logic Networks for Time-Series Classification](https://arxiv.org/abs/2508.17512)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: 首次将可微分逻辑网络(DLNs)应用于单变量时间序列分类，通过特征提取方法将时间序列转换为适合DLN分类的向量形式，并通过联合超参数搜索优化配置，在51个基准数据集上验证了DLN在准确性、推理效率和可解释性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 将DLNs从表格数据领域扩展到时间序列分类(TSC)领域，探索DLNs在新领域中保持准确性、可解释性和计算效率的能力。

Method: 采用Catch22和TSFresh特征提取方法将时间序列转换为向量形式，通过集成所有训练配置到超参数搜索空间中进行联合优化选择。

Result: 在51个公开的单变量TSC基准测试中，DLNs展现出竞争性的准确性、低推理成本和透明的可解释决策逻辑，与表格分类和回归任务中的先前发现一致。

Conclusion: DLNs成功应用于时间序列分类领域，保持了其在准确性、效率和可解释性方面的核心优势，为TSC任务提供了一个有前景的解决方案。

Abstract: Differentiable logic networks (DLNs) have shown promising results in tabular
domains by combining accuracy, interpretability, and computational efficiency.
In this work, we apply DLNs to the domain of TSC for the first time, focusing
on univariate datasets. To enable DLN application in this context, we adopt
feature-based representations relying on Catch22 and TSFresh, converting
sequential time series into vectorized forms suitable for DLN classification.
Unlike prior DLN studies that fix the training configuration and vary various
settings in isolation via ablation, we integrate all such configurations into
the hyperparameter search space, enabling the search process to select jointly
optimal settings. We then analyze the distribution of selected configurations
to better understand DLN training dynamics. We evaluate our approach on 51
publicly available univariate TSC benchmarks. The results confirm that
classification DLNs maintain their core strengths in this new domain: they
deliver competitive accuracy, retain low inference cost, and provide
transparent, interpretable decision logic, thus aligning well with previous DLN
findings in the realm of tabular classification and regression tasks.

</details>


### [185] [GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts](https://arxiv.org/abs/2508.17515)
*Kyrylo Yemets,Mykola Lukashchuk,Ivan Izonin*

Main category: cs.LG

TL;DR: 本文提出了一种简化训练过程的时间序列预测模型，结合稀疏MoE计算和新的注意力间控机制，可以同时处理长短期预测任务且无需辅助负载均衡损失。


<details>
  <summary>Details</summary>
Motivation: 解决现有MoE模型需要复杂训练过程、辅助负载均衡损失和精心调整路由参数的问题，这不利于实际应用。

Method: 结合稀疏MoE计算与新额的注意力间控机制，替代传统的单层softmax路由器，自然促进专家利用均衡。

Result: 模型在不需辅助负载均衡损失的情况下实现了更高的预测准确性，只需小部分参数就能超过PatchTST等独化模型，计算效率也高于LSTM。

Conclusion: 该方法为实际时间序列预测应用提供了既准确又高效的解决方案，具有重要的实践价值。

Abstract: Accurate univariate forecasting remains a pressing need in real-world
systems, such as energy markets, hydrology, retail demand, and IoT monitoring,
where signals are often intermittent and horizons span both short- and
long-term. While transformers and Mixture-of-Experts (MoE) architectures are
increasingly favored for time-series forecasting, a key gap persists: MoE
models typically require complicated training with both the main forecasting
loss and auxiliary load-balancing losses, along with careful
routing/temperature tuning, which hinders practical adoption. In this paper, we
propose a model architecture that simplifies the training process for
univariate time series forecasting and effectively addresses both long- and
short-term horizons, including intermittent patterns. Our approach combines
sparse MoE computation with a novel attention-inspired gating mechanism that
replaces the traditional one-layer softmax router. Through extensive empirical
evaluation, we demonstrate that our gating design naturally promotes balanced
expert utilization and achieves superior predictive accuracy without requiring
the auxiliary load-balancing losses typically used in classical MoE
implementations. The model achieves better performance while utilizing only a
fraction of the parameters required by state-of-the-art transformer models,
such as PatchTST. Furthermore, experiments across diverse datasets confirm that
our MoE architecture with the proposed gating mechanism is more computationally
efficient than LSTM for both long- and short-term forecasting, enabling
cost-effective inference. These results highlight the potential of our approach
for practical time-series forecasting applications where both accuracy and
computational efficiency are critical.

</details>


### [186] [Speculative Safety-Aware Decoding](https://arxiv.org/abs/2508.17739)
*Xuekang Wang,Shengyu Zhu,Xueqi Cheng*

Main category: cs.LG

TL;DR: SSD是一种轻量级的解码时方法，通过推测采样和小模型集成，为LLMs提供安全属性并加速推理


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已与人类价值观对齐，但越狱攻击仍持续出现，需要在不牺牲性能的情况下增强模型安全性

Method: 利用具备安全属性的小模型进行推测采样，通过匹配率量化越狱风险，动态切换解码方案，结合原始模型和小模型的分布采样输出

Result: 实验显示SSD成功为大模型赋予所需安全属性，保持对良性查询的有用性，同时加速推理时间

Conclusion: SSD提供了一种高效的方法来增强LLMs的安全性，同时保持实用性和加速推理

Abstract: Despite extensive efforts to align Large Language Models (LLMs) with human
values and safety rules, jailbreak attacks that exploit certain vulnerabilities
continuously emerge, highlighting the need to strengthen existing LLMs with
additional safety properties to defend against these attacks. However, tuning
large models has become increasingly resource-intensive and may have difficulty
ensuring consistent performance. We introduce Speculative Safety-Aware Decoding
(SSD), a lightweight decoding-time approach that equips LLMs with the desired
safety property while accelerating inference. We assume that there exists a
small language model that possesses this desired property. SSD integrates
speculative sampling during decoding and leverages the match ratio between the
small and composite models to quantify jailbreak risks. This enables SSD to
dynamically switch between decoding schemes to prioritize utility or safety, to
handle the challenge of different model capacities. The output token is then
sampled from a new distribution that combines the distributions of the original
and the small models. Experimental results show that SSD successfully equips
the large model with the desired safety property, and also allows the model to
remain helpful to benign queries. Furthermore, SSD accelerates the inference
time, thanks to the speculative sampling design.

</details>


### [187] [Modeling Irregular Astronomical Time Series with Neural Stochastic Delay Differential Equations](https://arxiv.org/abs/2508.17521)
*YongKyung Oh,Seungsu Kam,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: 提出基于神经随机延迟微分方程(Neural SDDEs)的新框架，用于处理不规则采样的天文时间序列数据分类和异常检测问题


<details>
  <summary>Details</summary>
Motivation: 大规模巡天项目(如LSST)产生的天文时间序列数据往往存在不规则采样和不完整的问题，给分类和异常检测带来挑战

Method: 结合随机建模和神经网络，采用延迟感知神经架构、SDDE数值求解器，以及从噪声稀疏序列中稳健学习的机制

Result: 在不规则采样的天文数据实验中表现出强大的分类准确性，能有效检测新的天体物理事件，即使在部分标签情况下也表现良好

Conclusion: Neural SDDEs是在观测约束下进行时间序列分析的原理性和实用性工具

Abstract: Astronomical time series from large-scale surveys like LSST are often
irregularly sampled and incomplete, posing challenges for classification and
anomaly detection. We introduce a new framework based on Neural Stochastic
Delay Differential Equations (Neural SDDEs) that combines stochastic modeling
with neural networks to capture delayed temporal dynamics and handle irregular
observations. Our approach integrates a delay-aware neural architecture, a
numerical solver for SDDEs, and mechanisms to robustly learn from noisy, sparse
sequences. Experiments on irregularly sampled astronomical data demonstrate
strong classification accuracy and effective detection of novel astrophysical
events, even with partial labels. This work highlights Neural SDDEs as a
principled and practical tool for time series analysis under observational
constraints.

</details>


### [188] [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
*Wenhong Zhu,Ruobing Xie,Rui Wang,Xingwu Sun,Di Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: 提出了Proximal SFT (PSFT)方法，通过引入信任区域约束来防止监督微调中的策略漂移，提高模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 基础模型的监督微调(SFT)往往导致泛化能力下降，原有能力在新任务或领域上会退化

Method: 受强化学习中TRPO和PPO的启发，将SFT视为具有恒定正优势的策略梯度方法特例，提出PSFT目标函数来约束策略漂移

Result: 在数学和人类价值观领域的实验表明，PSFT在域内性能与SFT相当，在域外泛化上优于SFT，训练稳定性更好且不会导致熵崩溃

Conclusion: PSFT为后续优化阶段提供了更强的基础，稳定了优化过程并提高了泛化能力

Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor
generalization, where prior capabilities deteriorate after tuning on new tasks
or domains. Inspired by trust-region policy optimization (TRPO) and proximal
policy optimization (PPO) in reinforcement learning (RL), we propose Proximal
SFT (PSFT). This fine-tuning objective incorporates the benefits of
trust-region, effectively constraining policy drift during SFT while
maintaining competitive tuning. By viewing SFT as a special case of policy
gradient methods with constant positive advantages, we derive PSFT that
stabilizes optimization and leads to generalization, while leaving room for
further optimization in subsequent post-training stages. Experiments across
mathematical and human-value domains show that PSFT matches SFT in-domain,
outperforms it in out-of-domain generalization, remains stable under prolonged
training without causing entropy collapse, and provides a stronger foundation
for the subsequent optimization.

</details>


### [189] [Gumbel-MPNN: Graph Rewiring with Gumbel-Softmax](https://arxiv.org/abs/2508.17531)
*Marcel Hoffmann,Lukas Galke,Ansgar Scherp*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Gumbel-Softmax的图重连方法，通过减少邻域分布的偏差来提升消息传递神经网络在节点分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统认为图同配性是MPNN性能的关键，但研究发现邻域类别分布的一致性更为重要。作者发现MPNN性能取决于类别内邻域分布组件的数量，希望通过改进邻域分布来提升模型表现。

Method: 提出Gumbel-Softmax重连方法，通过减少邻域分布的偏差来增强邻域信息量，处理长程依赖关系，缓解过压缩问题。

Result: 新方法显著提高了邻域信息量，有效处理长程依赖，减轻过压缩现象，并提升了MPNN的分类性能。

Conclusion: 基于Gumbel-Softmax的重连方法是一种有效的图结构优化技术，能够通过改善邻域分布来提升消息传递神经网络的节点分类性能。

Abstract: Graph homophily has been considered an essential property for message-passing
neural networks (MPNN) in node classification. Recent findings suggest that
performance is more closely tied to the consistency of neighborhood class
distributions. We demonstrate that the MPNN performance depends on the number
of components of the overall neighborhood distribution within a class. By
breaking down the classes into their neighborhood distribution components, we
increase measures of neighborhood distribution informativeness but do not
observe an improvement in MPNN performance. We propose a Gumbel-Softmax-based
rewiring method that reduces deviations in neighborhood distributions. Our
results show that our new method enhances neighborhood informativeness, handles
long-range dependencies, mitigates oversquashing, and increases the
classification performance of the MPNN. The code is available at
https://github.com/Bobowner/Gumbel-Softmax-MPNN.

</details>


### [190] [Limitations of Normalization in Attention Mechanism](https://arxiv.org/abs/2508.17821)
*Timur Mudarisov,Mikhail Burtsev,Tatiana Petrova,Radu State*

Main category: cs.LG

TL;DR: 本文分析了注意力机制中softmax归一化的局限性，发现随着选择token数量增加，模型区分信息性token的能力下降，梯度敏感性在低温设置下带来训练挑战。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制中归一化过程的局限性，特别是softmax缩放对token选择能力和几何分离的影响，为改进注意力架构提供理论基础。

Method: 建立理论框架分析token选择能力和几何分离，推导距离边界和分离标准，通过预训练GPT-2模型进行实验验证。

Result: 随着选择token数量增加，模型区分信息性token的能力显著下降，趋向均匀选择模式；softmax归一化在低温设置下导致梯度敏感性训练挑战。

Conclusion: 当前基于softmax的注意力机制存在局限性，需要开发更鲁棒的归一化和选择策略来改进未来注意力架构。

Abstract: This paper investigates the limitations of the normalization in attention
mechanisms. We begin with a theoretical framework that enables the
identification of the model's selective ability and the geometric separation
involved in token selection. Our analysis includes explicit bounds on distances
and separation criteria for token vectors under softmax scaling. Through
experiments with pre-trained GPT-2 model, we empirically validate our
theoretical results and analyze key behaviors of the attention mechanism.
Notably, we demonstrate that as the number of selected tokens increases, the
model's ability to distinguish informative tokens declines, often converging
toward a uniform selection pattern. We also show that gradient sensitivity
under softmax normalization presents challenges during training, especially at
low temperature settings. These findings advance current understanding of
softmax-based attention mechanism and motivate the need for more robust
normalization and selection strategies in future attention architectures.

</details>


### [191] [Limits of message passing for node classification: How class-bottlenecks restrict signal-to-noise ratio](https://arxiv.org/abs/2508.17822)
*Jonathan Rubin,Sahil Loomba,Nick S. Jones*

Main category: cs.LG

TL;DR: 本文提出了一个统一的统计框架，通过分析MPNN表示的信噪比(SNR)来揭示异质性(homophily)和结构瓶颈之间的关系，并开发了BRIDGE图重连算法来提升MPNN在所有同质性机制下的性能。


<details>
  <summary>Details</summary>
Motivation: 消息传递神经网络(MPNNs)在节点分类任务中表现强大，但在异质性(低同类连接性)和结构瓶颈情况下存在性能限制。需要建立一个理论框架来理解这些限制的根本原因。

Method: 通过信噪比(SNR)分析框架，将模型性能分解为特征相关参数和特征无关敏感度。证明了类别信号敏感度受高阶同质性限制，并分析了结构瓶颈的定量分解。提出了BRIDGE图重连算法，基于图集成方法优化图结构。

Result: 理论分析表明，最大化高阶同质性的最优图结构是单类别和双类别二分簇的不相交并集。BRIDGE算法在合成基准测试中实现了接近完美的分类准确率，在真实世界基准测试中显著提升性能，特别是在MPNN通常表现不佳的"中等同质性陷阱"区域。

Conclusion: 该框架不仅提供了评估MPNN性能的诊断工具，还通过原则性的图修改方法提供了简单而有效的性能增强手段，代码已公开供公众使用。

Abstract: Message passing neural networks (MPNNs) are powerful models for node
classification but suffer from performance limitations under heterophily (low
same-class connectivity) and structural bottlenecks in the graph. We provide a
unifying statistical framework exposing the relationship between heterophily
and bottlenecks through the signal-to-noise ratio (SNR) of MPNN
representations. The SNR decomposes model performance into feature-dependent
parameters and feature-independent sensitivities. We prove that the sensitivity
to class-wise signals is bounded by higher-order homophily -- a generalisation
of classical homophily to multi-hop neighbourhoods -- and show that low
higher-order homophily manifests locally as the interaction between structural
bottlenecks and class labels (class-bottlenecks). Through analysis of graph
ensembles, we provide a further quantitative decomposition of bottlenecking
into underreaching (lack of depth implying signals cannot arrive) and
oversquashing (lack of breadth implying signals arriving on fewer paths) with
closed-form expressions. We prove that optimal graph structures for maximising
higher-order homophily are disjoint unions of single-class and
two-class-bipartite clusters. This yields BRIDGE, a graph ensemble-based
rewiring algorithm that achieves near-perfect classification accuracy across
all homophily regimes on synthetic benchmarks and significant improvements on
real-world benchmarks, by eliminating the ``mid-homophily pitfall'' where MPNNs
typically struggle, surpassing current standard rewiring techniques from the
literature. Our framework, whose code we make available for public use,
provides both diagnostic tools for assessing MPNN performance, and simple yet
effective methods for enhancing performance through principled graph
modification.

</details>


### [192] [Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs](https://arxiv.org/abs/2508.17850)
*Han Zhang,Ruibin Zheng,Zexuan Yi,Hanyang Peng,Hui Wang,Yue Yu*

Main category: cs.LG

TL;DR: HeteroRL是一个异步强化学习架构，通过解耦采样和学习来解决分布式环境中网络延迟导致的KL散度问题，GEPO方法实现了指数级方差减少，在1800秒延迟下性能下降小于3%。


<details>
  <summary>Details</summary>
Motivation: 随着单中心计算面临功耗限制，去中心化训练变得至关重要。强化学习后训练虽然能增强大语言模型，但在异构分布式环境中由于紧密耦合的采样-学习交替而面临挑战，特别是网络延迟导致的KL散度问题。

Method: 提出HeteroRL异步RL架构，将rollout采样与参数学习解耦；设计Group Expectation Policy Optimization (GEPO)方法，通过改进的采样机制降低重要性权重方差。

Result: 理论证明GEPO实现了指数级方差减少；实验显示相比GRPO等方法具有更好的稳定性，在1800秒延迟下性能下降小于3%。

Conclusion: 该方法在异构网络中展示了去中心化RL的强大潜力，能够有效应对网络延迟带来的挑战，为分布式RL训练提供了可行的解决方案。

Abstract: As single-center computing approaches power constraints, decentralized
training is becoming essential. Reinforcement Learning (RL) post-training
enhances Large Language Models (LLMs) but faces challenges in heterogeneous
distributed environments due to its tightly-coupled sampling-learning
alternation. We propose HeteroRL, an asynchronous RL architecture that
decouples rollout sampling from parameter learning, enabling robust deployment
across geographically distributed nodes under network delays. We identify that
latency-induced KL divergence causes importance sampling failure due to high
variance. To address this, we propose Group Expectation Policy Optimization
(GEPO), which reduces importance weight variance through a refined sampling
mechanism. Theoretically, GEPO achieves exponential variance reduction.
Experiments show it maintains superior stability over methods like GRPO, with
less than 3% performance degradation under 1800-second delays, demonstrating
strong potential for decentralized RL in heterogeneous networks.

</details>


### [193] [Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction](https://arxiv.org/abs/2508.17554)
*Shuqi Zi,Haitz Sáez de Ocáriz Borde,Emma Rocheteau,Pietro Lio'*

Main category: cs.LG

TL;DR: S²G-Net是一种结合状态空间序列建模和多视图图神经网络的新型神经网络架构，用于ICU住院时间预测，在MIMIC-IV数据集上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: ICU住院时间预测对医院资源管理至关重要，但由于电子健康记录的异构性和不规则采样特性，这一任务仍然具有挑战性

Method: 提出S²G-Net架构，包含时间路径（使用Mamba状态空间模型捕捉患者轨迹）和图路径（使用优化的GraphGPS骨干网络整合来自诊断、管理和语义特征的异构患者相似性图）

Result: 在MIMIC-IV队列数据集上的实验显示，S²G-Net在所有主要指标上均优于序列模型、图模型和混合方法

Conclusion: S²G-Net为多模态临床数据的ICU住院时间预测提供了有效且可扩展的解决方案，消融研究和可解释性分析证明了各组件的重要性和图构建的重要性

Abstract: Predicting a patient's length of stay (LOS) in the intensive care unit (ICU)
is a critical task for hospital resource management, yet remains challenging
due to the heterogeneous and irregularly sampled nature of electronic health
records (EHRs). In this work, we propose S$^2$G-Net, a novel neural
architecture that unifies state-space sequence modeling with multi-view Graph
Neural Networks (GNNs) for ICU LOS prediction. The temporal path employs Mamba
state-space models (SSMs) to capture patient trajectories, while the graph path
leverages an optimized GraphGPS backbone, designed to integrate heterogeneous
patient similarity graphs derived from diagnostic, administrative, and semantic
features. Experiments on the large-scale MIMIC-IV cohort dataset show that
S$^2$G-Net consistently outperforms sequence models (BiLSTM, Mamba,
Transformer), graph models (classic GNNs, GraphGPS), and hybrid approaches
across all primary metrics. Extensive ablation studies and interpretability
analyses highlight the complementary contributions of each component of our
architecture and underscore the importance of principled graph construction.
These results demonstrate that S$^2$G-Net provides an effective and scalable
solution for ICU LOS prediction with multi-modal clinical data.

</details>


### [194] [Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks](https://arxiv.org/abs/2508.17867)
*Dan Wang,Feng Jiang,Zhanquan Wang*

Main category: cs.LG

TL;DR: 提出基于Transformer的Ada-TransGNN模型，通过自适应图结构学习和辅助任务学习模块，提升空气质量预测的时空依赖特征提取能力和预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有空气质量预测模型存在预测精度低、实时更新慢、预测结果滞后等问题，需要开发更准确的时空数据预测方法。

Method: 结合多头注意力机制和图卷积网络构建时空块集，提出自适应图结构学习模块和辅助任务学习模块，动态提取时空依赖特征并优化图结构表示。

Result: 在基准数据集和新数据集(Mete-air)上的实验表明，该模型在短期和长期预测中均优于现有最先进的预测模型。

Conclusion: Ada-TransGNN模型通过有效整合全局空间语义和时间行为，显著提高了空气质量预测的准确性和实时性。

Abstract: Accurate air quality prediction is becoming increasingly important in the
environmental field. To address issues such as low prediction accuracy and slow
real-time updates in existing models, which lead to lagging prediction results,
we propose a Transformer-based spatiotemporal data prediction method
(Ada-TransGNN) that integrates global spatial semantics and temporal behavior.
The model constructs an efficient and collaborative spatiotemporal block set
comprising a multi-head attention mechanism and a graph convolutional network
to extract dynamically changing spatiotemporal dependency features from complex
air quality monitoring data. Considering the interaction relationships between
different monitoring points, we propose an adaptive graph structure learning
module, which combines spatiotemporal dependency features in a data-driven
manner to learn the optimal graph structure, thereby more accurately capturing
the spatial relationships between monitoring points. Additionally, we design an
auxiliary task learning module that enhances the decoding capability of
temporal relationships by integrating spatial context information into the
optimal graph structure representation, effectively improving the accuracy of
prediction results. We conducted comprehensive evaluations on a benchmark
dataset and a novel dataset (Mete-air). The results demonstrate that our model
outperforms existing state-of-the-art prediction models in short-term and
long-term predictions.

</details>


### [195] [Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA](https://arxiv.org/abs/2508.17586)
*Daniel Frees,Aditri Bhagirath,Moritz Bolling*

Main category: cs.LG

TL;DR: 本文在小型语言模型minBERT上验证了LoRA和DoRA高效微调方法的有效性，发现即使在小模型上梯度更新也具有低秩特性，结合AMP技术可显著提升训练效率而不损失性能


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA和DoRA在大模型上表现出色，但其在小型语言模型上的效果尚未充分验证。研究旨在探索这些高效微调方法在小规模模型上的适用性和性能表现

Method: 在minBERT模型上应用LoRA和DoRA方法，结合自动混合精度(AMP)技术，测试不同配置下的效率和性能。研究多种架构、自定义损失函数和超参数，最终训练多任务集成模型

Result: 实验表明，即使在小模型上梯度更新也具有低秩特性，rank 1分解几乎不造成性能损失。最优配置的LoRA和DoRA结合AMP能显著提升训练效率，同时保持模型性能

Conclusion: LoRA和DoRA方法不仅适用于大型语言模型，在小规模模型上同样有效，验证了语言模型梯度更新的低秩特性具有普适性，为资源受限环境下的高效微调提供了解决方案

Abstract: While Large Language Models (LLMs) have revolutionized artificial
intelligence, fine-tuning LLMs is extraordinarily computationally expensive,
preventing smaller businesses and research teams with limited GPU resources
from engaging with new research. Hu et al and Liu et al introduce Low-Rank
Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) as highly
efficient and performant solutions to the computational challenges of LLM
fine-tuning, demonstrating huge speedups and memory usage savings for models
such as GPT-3 and RoBERTa. We seek to expand upon the original LoRA and DoRA
papers by benchmarking efficiency and performance of LoRA and DoRA when applied
to a much smaller scale of language model: our case study here is the compact
minBERT model. Our findings reveal that optimal custom configurations of LoRA
and DoRA, coupled with Automatic Mixed Precision (AMP), significantly enhance
training efficiency without compromising performance. Furthermore, while the
parameterization of minBERT is significantly smaller than GPT-3, our results
validate the observation that gradient updates to language models are
inherently low-rank even in small model space, observing that rank 1
decompositions yield negligible performance deficits. Furthermore, aided by our
highly efficient minBERT implementation, we investigate numerous architectures,
custom loss functions, and hyperparameters to ultimately train an optimal
ensembled multitask minBERT model to simultaneously perform sentiment analysis,
paraphrase detection, and similarity scoring.

</details>


### [196] [Riemannian Optimization for LoRA on the Stiefel Manifold](https://arxiv.org/abs/2508.17901)
*Juneyoung Park,Minjae Kang,Seongbae Lee,Haegang Lee,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 通过在Stiefel流形上优化LoRA的B矩阵，实现严格正交约束，解决AdamW优化器在PEFT中的基准冗余问题，显著提升参数效率和表示能力。


<details>
  <summary>Details</summary>
Motivation: 虽然参数效率微调方法如LoRA能够解决大语言模型的细调挑战，但在使用AdamW优化器时存在基准冗余问题，限制了性能。

Method: 在Stiefel流形上优化LoRA的B矩阵，强制正交约束，实现近优正交性和完整有效秩。

Result: 新的Stiefel优化器在各项测试中均超过AdamW，在LoRA和DoRA上都表现优异，显示几何约束是发挥LoRA潜力的关键。

Conclusion: 通过几何约束优化可以解锁LoRA在大语言模型细调中的完整潜力，提供更高效的参数使用效率。

Abstract: While powerful, large language models (LLMs) present significant fine-tuning
challenges due to their size. Parameter-efficient fine-tuning (PEFT) methods
like LoRA provide solutions, yet suffer from critical optimizer inefficiencies;
notably basis redundancy in LoRA's $B$ matrix when using AdamW, which
fundamentally limits performance. We address this by optimizing the $B$ matrix
on the Stiefel manifold, imposing explicit orthogonality constraints that
achieve near-perfect orthogonality and full effective rank. This geometric
approach dramatically enhances parameter efficiency and representational
capacity. Our Stiefel optimizer consistently outperforms AdamW across
benchmarks with both LoRA and DoRA, demonstrating that geometric constraints
are the key to unlocking LoRA's full potential for effective LLM fine-tuning.

</details>


### [197] [ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning](https://arxiv.org/abs/2508.17608)
*Wentao Tan,Qiong Cao,Chao Xue,Yibing Zhan,Changxing Ding,Xiaodong He*

Main category: cs.LG

TL;DR: 本文提出ReChartPrompt数据集和ChartSimRL强化学习算法，构建ChartMaster模型，在图表生成代码任务中达到独立开源模型最佳性能，甚至可与GPT-4o相比拟。


<details>
  <summary>Details</summary>
Motivation: 解决图表生成代码任务中的两大挑战：数据集多样性不足和训练过程中视觉一致性维护不充分。现有数据集主要依赖GPT模型生成，导致样本同质化，而SFT训练虽能提升代码理解能力，但无法保证生成图表与原图的视觉一致性。

Method: 1. 提出ReChartPrompt：使用arXiv论文中真实人类设计的图表作为提示，构建大规模高多样性的ReChartPrompt-240K数据集
2. 提出ChartSimRL：基于GRPO的强化学习算法，通过新的图表相似性奖励指导，包含属性相似性（测量布局、颜色等属性重叠）和视觉相似性（使用CNN评估纹理和整体视觉特征）
3. 统合两者开发ChartMaster模型

Result: ChartMaster模型在各种图表生成代码测试集上达到了独立开源7B参数模型中的最佳性能，甚至能够与GPT-4o这样的大型模型相比拟。

Conclusion: 本研究通过使用真实人类设计图表构建多样性数据集，以及设计考虑多模态特征的图表相似性奖励机制，有效解决了图表生成代码任务中的数据同质化和视觉一致性问题，为该领域提供了有效的解决方案。

Abstract: The chart-to-code generation task requires MLLMs to convert chart images into
executable code. This task faces two major challenges: limited data diversity
and insufficient maintenance of visual consistency between generated and
original charts during training. Existing datasets mainly rely on seed data to
prompt GPT models for code generation, resulting in homogeneous samples. To
address this, we propose ReChartPrompt, which leverages real-world,
human-designed charts from arXiv papers as prompts instead of synthetic seeds.
Using the diverse styles and rich content of arXiv charts, we construct
ReChartPrompt-240K, a large-scale and highly diverse dataset. Another challenge
is that although SFT effectively improve code understanding, it often fails to
ensure that generated charts are visually consistent with the originals. To
address this, we propose ChartSimRL, a GRPO-based reinforcement learning
algorithm guided by a novel chart similarity reward. This reward consists of
attribute similarity, which measures the overlap of chart attributes such as
layout and color between the generated and original charts, and visual
similarity, which assesses similarity in texture and other overall visual
features using convolutional neural networks. Unlike traditional text-based
rewards such as accuracy or format rewards, our reward considers the multimodal
nature of the chart-to-code task and effectively enhances the model's ability
to accurately reproduce charts. By integrating ReChartPrompt and ChartSimRL, we
develop the ChartMaster model, which achieves state-of-the-art results among
7B-parameter models and even rivals GPT-4o on various chart-to-code generation
benchmarks. All resources are available at
https://github.com/WentaoTan/ChartMaster.

</details>


### [198] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: AQ-PCDSys是一个专为行星探测任务设计的自适应量化陨石坑检测系统，通过量化神经网络和多传感器融合技术，在计算资源受限的太空环境中实现实时高精度陨石坑检测。


<details>
  <summary>Details</summary>
Motivation: 行星自主探测任务严重依赖实时准确的环境感知进行导航和危险规避，但在资源受限的计算硬件上部署深度学习模型仍面临重大挑战。

Method: 结合量化神经网络架构（使用量化感知训练）和自适应多传感器融合模块，在特征层面智能融合光学图像和数字高程模型数据，并采用自适应权重机制动态优先处理最相关的传感器模态。

Result: 系统显著优化了模型大小和推理延迟，适合太空探测任务的实时部署，同时保持高精度检测能力。

Conclusion: AQ-PCDSys为行星陨石坑检测提供了计算高效、可靠准确的解决方案，是实现下一代自主行星着陆、导航和科学探索的关键能力。

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


### [199] [A Proportional-Integral Controller-Incorporated SGD Algorithm for High Efficient Latent Factor Analysis](https://arxiv.org/abs/2508.17609)
*Jinli Li,Shiyu Long,Minglian Han*

Main category: cs.LG

TL;DR: 提出PILF模型，通过PI控制机制整合当前和历史信息来加速SGD算法，提高高维稀疏矩阵的特征提取性能


<details>
  <summary>Details</summary>
Motivation: 现有SGD-LFA方法仅依赖当前样本的瞬时梯度信息，忽略了历史迭代经验知识和样本间内在关联，导致收敛速度慢和泛化性能不佳

Method: 开发PI加速SGD算法，通过比例-积分控制机制整合相关实例并优化学习误差，同时利用当前和历史信息

Result: 对比实验表明PILF模型在高维稀疏矩阵上具有优越的表征能力

Conclusion: PILF模型通过PI控制机制有效解决了传统SGD-LFA方法的局限性，显著提升了收敛速度和泛化性能

Abstract: In industrial big data scenarios, high-dimensional sparse matrices (HDI) are
widely used to characterize high-order interaction relationships among massive
nodes. The stochastic gradient descent-based latent factor analysis (SGD-LFA)
method can effectively extract deep feature information embedded in HDI
matrices. However, existing SGD-LFA methods exhibit significant limitations:
their parameter update process relies solely on the instantaneous gradient
information of current samples, failing to incorporate accumulated experiential
knowledge from historical iterations or account for intrinsic correlations
between samples, resulting in slow convergence speed and suboptimal
generalization performance. Thus, this paper proposes a PILF model by
developing a PI-accelerated SGD algorithm by integrating correlated instances
and refining learning errors through proportional-integral (PI) control
mechanism that current and historical information; Comparative experiments
demonstrate the superior representation capability of the PILF model on HDI
matrices

</details>


### [200] [Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning](https://arxiv.org/abs/2508.17630)
*An Ning,Tai Yue Li,Nan Yow Chen*

Main category: cs.LG

TL;DR: 量子图注意力网络（QGAT）将变分量子电路集成到注意力机制中，通过量子并行性同时生成多个注意力系数，减少计算开销并提升表达能力


<details>
  <summary>Details</summary>
Motivation: 传统多头注意力机制需要分别计算每个头，计算开销大且参数冗余。量子计算能够通过并行性同时处理多个注意力头，从而减少模型复杂度和计算成本

Method: 使用强纠缠量子电路和振幅编码的节点特征，通过单一量子电路同时生成多个注意力系数。经典投影权重和量子电路参数端到端联合优化

Result: 实验证明QGAT能有效捕捉复杂结构依赖关系，在归纳场景中具有更好的泛化能力，对特征和结构噪声具有更强的鲁棒性

Conclusion: QGAT展示了量子增强学习在化学、生物和网络分析等领域的潜力，其模块化设计便于集成到现有架构中，为量子机器学习提供了实用方案

Abstract: We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neural
network that integrates variational quantum circuits into the attention
mechanism. At its core, QGAT employs strongly entangling quantum circuits with
amplitude-encoded node features to enable expressive nonlinear interactions.
Distinct from classical multi-head attention that separately computes each
head, QGAT leverages a single quantum circuit to simultaneously generate
multiple attention coefficients. This quantum parallelism facilitates parameter
sharing across heads, substantially reducing computational overhead and model
complexity. Classical projection weights and quantum circuit parameters are
optimized jointly in an end-to-end manner, ensuring flexible adaptation to
learning tasks. Empirical results demonstrate QGAT's effectiveness in capturing
complex structural dependencies and improved generalization in inductive
scenarios, highlighting its potential for scalable quantum-enhanced learning
across domains such as chemistry, biology, and network analysis. Furthermore,
experiments confirm that quantum embedding enhances robustness against feature
and structural noise, suggesting advantages in handling real-world noisy data.
The modularity of QGAT also ensures straightforward integration into existing
architectures, allowing it to easily augment classical attention-based models.

</details>


### [201] [CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics](https://arxiv.org/abs/2508.18124)
*Weida Wang,Dongchen Huang,Jiatong Li,Tengchao Yang,Ziyang Zheng,Di Zhang,Dong Han,Benteng Chen,Binzhao Luo,Zhiyu Liu,Kunling Liu,Zhiyuan Gao,Shiqi Geng,Wei Ma,Jiaming Su,Xin Li,Shuchen Pu,Yuhan Shui,Qianjia Cheng,Zhihao Dou,Dongfei Cui,Changyong He,Jin Zeng,Zeke Xie,Mao Su,Dongzhan Zhou,Yuqiang Li,Wanli Ouyang,Lei Bai,Yunqi Cai,Xi Dai,Shufei Zhang,Jinguang Cheng,Zhong Fang,Hongming Weng*

Main category: cs.LG

TL;DR: CMPhysBench是一个专门评估大语言模型在凝聚态物理领域能力的基准测试，包含520多个研究生级别计算题，使用SEED评分系统进行细粒度评估，结果显示当前最佳模型Grok-4仅获得36分平均SEED分数和28%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对凝聚态物理这一重要前沿领域的专门评估，需要开发专门的测试集来准确衡量大语言模型在该领域的实际能力水平。

Method: 构建包含520+研究生级别计算题的基准测试集，覆盖磁学、超导、强关联系统等核心领域；提出基于树状表达式表示的SEED评分系统，提供细粒度部分得分评估。

Result: 测试结果显示，即使是表现最好的Grok-4模型，在CMPhysBench上也仅获得36平均SEED分数和28%的准确率，表明大语言模型在凝聚态物理领域存在显著能力差距。

Conclusion: 大语言模型在凝聚态物理这一实践性和前沿性领域的能力仍然有限，需要进一步改进和专门训练，CMPhysBench为相关研究提供了有效的评估工具。

Abstract: We introduce CMPhysBench, designed to assess the proficiency of Large
Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.
CMPhysBench is composed of more than 520 graduate-level meticulously curated
questions covering both representative subfields and foundational theoretical
frameworks of condensed matter physics, such as magnetism, superconductivity,
strongly correlated systems, etc. To ensure a deep understanding of the
problem-solving process,we focus exclusively on calculation problems, requiring
LLMs to independently generate comprehensive solutions. Meanwhile, leveraging
tree-based representations of expressions, we introduce the Scalable Expression
Edit Distance (SEED) score, which provides fine-grained (non-binary) partial
credit and yields a more accurate assessment of similarity between prediction
and ground-truth. Our results show that even the best models, Grok-4, reach
only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a
significant capability gap, especially for this practical and frontier domain
relative to traditional physics. The code anddataset are publicly available at
https://github.com/CMPhysBench/CMPhysBench.

</details>


### [202] [Longitudinal Progression Prediction of Alzheimer's Disease with Tabular Foundation Model](https://arxiv.org/abs/2508.17649)
*Yilang Ding,Jiawen Ren,Jiaying Lu,Gloria Hyunjung Kwak,Armin Iraji,Alex Fedorov*

Main category: cs.LG

TL;DR: L2C-TabPFN方法通过纵向到横截面的转换结合预训练表格基础模型，在阿尔茨海默病预测中实现了竞争性性能，特别是在脑室体积预测方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病由于其多因素病因和多模态临床数据的复杂性，预测仍然具有挑战性。准确预测临床相关生物标志物对于有效监测疾病进展至关重要。

Method: 提出L2C-TabPFN方法，将纵向到横截面（L2C）转换与预训练表格基础模型（TabPFN）相结合，使用TADPOLE数据集将序列患者记录转换为固定长度特征向量。

Result: 实验结果表明，L2C-TabPFN在诊断和认知结果方面具有竞争性性能，在脑室体积预测方面提供了最先进的结果。脑室体积是反映阿尔茨海默病神经退行性变和进展的关键成像生物标志物。

Conclusion: 这些发现突显了表格基础模型在推进阿尔茨海默病临床相关成像标志物的纵向预测方面的潜力。

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that remains
challenging to predict due to its multifactorial etiology and the complexity of
multimodal clinical data. Accurate forecasting of clinically relevant
biomarkers, including diagnostic and quantitative measures, is essential for
effective monitoring of disease progression. This work introduces L2C-TabPFN, a
method that integrates a longitudinal-to-cross-sectional (L2C) transformation
with a pre-trained Tabular Foundation Model (TabPFN) to predict Alzheimer's
disease outcomes using the TADPOLE dataset. L2C-TabPFN converts sequential
patient records into fixed-length feature vectors, enabling robust prediction
of diagnosis, cognitive scores, and ventricular volume. Experimental results
demonstrate that, while L2C-TabPFN achieves competitive performance on
diagnostic and cognitive outcomes, it provides state-of-the-art results in
ventricular volume prediction. This key imaging biomarker reflects
neurodegeneration and progression in Alzheimer's disease. These findings
highlight the potential of tabular foundational models for advancing
longitudinal prediction of clinically relevant imaging markers in Alzheimer's
disease.

</details>


### [203] [Amortized Sampling with Transferable Normalizing Flows](https://arxiv.org/abs/2508.18175)
*Charlie B. Tan,Majdi Hassan,Leon Klein,Saifuddin Syed,Dominique Beaini,Michael M. Bronstein,Alexander Tong,Kirill Neklyudov*

Main category: cs.LG

TL;DR: Prose是一个2.8亿参数的可转移归一化流模型，能够零样本生成任意肽系统的无相关样本，实现了跨序列长度的可转移性，在采样效率上超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学和马尔可夫链蒙特卡洛方法缺乏摊销性，每个系统的采样计算成本都很高。虽然生成模型在单个系统上表现良好，但现有学习采样器在跨系统转移方面能力有限。

Method: 开发了Prose——一个2.8亿参数的全原子可转移归一化流模型，在包含最多8个残基的肽分子动力学轨迹语料库上训练，能够进行零样本无相关提案采样。

Result: Prose能够为任意肽系统绘制零样本无相关提案样本，实现了以前难以实现的跨序列长度可转移性，同时保持了归一化流的高效似然评估能力。通过重要性采样微调程序，在未见过的四肽上表现优于顺序蒙特卡洛等成熟方法。

Conclusion: 深度学习能够设计出可扩展和可转移的采样器，Prose证明了这一点。作者开源了代码、模型权重和训练数据集，以促进摊销采样方法和微调目标的研究。

Abstract: Efficient equilibrium sampling of molecular conformations remains a core
challenge in computational chemistry and statistical inference. Classical
approaches such as molecular dynamics or Markov chain Monte Carlo inherently
lack amortization; the computational cost of sampling must be paid in-full for
each system of interest. The widespread success of generative models has
inspired interest into overcoming this limitation through learning sampling
algorithms. Despite performing on par with conventional methods when trained on
a single system, learned samplers have so far demonstrated limited ability to
transfer across systems. We prove that deep learning enables the design of
scalable and transferable samplers by introducing Prose, a 280 million
parameter all-atom transferable normalizing flow trained on a corpus of peptide
molecular dynamics trajectories up to 8 residues in length. Prose draws
zero-shot uncorrelated proposal samples for arbitrary peptide systems,
achieving the previously intractable transferability across sequence length,
whilst retaining the efficient likelihood evaluation of normalizing flows.
Through extensive empirical evaluation we demonstrate the efficacy of Prose as
a proposal for a variety of sampling algorithms, finding a simple importance
sampling-based finetuning procedure to achieve superior performance to
established methods such as sequential Monte Carlo on unseen tetrapeptides. We
open-source the Prose codebase, model weights, and training dataset, to further
stimulate research into amortized sampling methods and finetuning objectives.

</details>


### [204] [AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models](https://arxiv.org/abs/2508.18182)
*Nikolay Kutuzov,Makar Baderko,Stepan Kulibaba,Artem Dzhalilov,Daniel Bobrov,Maxim Mashtaler,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 一种三阶段分布式训练方法，通过多实例训练、适配批处理和切换机制提高大语言模型训练效率，充分利用异构硬件资源并减少同步延误。


<details>
  <summary>Details</summary>
Motivation: 现有分布式训练方法如DiLoCo在动态负载下无法充分利用计算集群的异构硬件资源，导致计算效率低下和空闲时间。

Method: 提出三阶段方法：1)多实例训练(MIT)在单个节点并行运行多个轻量训练流；2)适配批处DiLoCo动态调整本地批处大小；3)切换模式机制在批处大小超过硬件限制时引入梯度累积。

Result: 提高了系统吞吐量和减少空闲时间，大幅降低同步延误，同时改善了收敛速度和系统效率。还提供了完全收敛所需通信次数的理论估计。

Conclusion: 该三阶段方法通过综合多实例训练、动态批处调整和切换机制，有效解决了异构硬件资源利用不充分的问题，为大语言模型的高效分布式训练提供了有效方案。

Abstract: Scaling distributed training of Large Language Models (LLMs) requires not
only algorithmic advances but also efficient utilization of heterogeneous
hardware resources. While existing methods such as DiLoCo have demonstrated
promising results, they often fail to fully exploit computational clusters
under dynamic workloads. To address this limitation, we propose a three-stage
method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,
and switch mode mechanism. MIT allows individual nodes to run multiple
lightweight training streams with different model instances in parallel and
merge them to combine knowledge, increasing throughput and reducing idle time.
Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance
computation and communication, substantially lowering synchronization delays.
Switch mode further stabilizes training by seamlessly introducing gradient
accumulation once adaptive batch sizes grow beyond hardware-friendly limits.
Together, these innovations improve both convergence speed and system
efficiency. We also provide a theoretical estimate of the number of
communications required for the full convergence of a model trained using our
method.

</details>


### [205] [Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models](https://arxiv.org/abs/2508.17675)
*Victoria Yan,Honor Chotkowski,Fengran Wang,Alex Fedorov*

Main category: cs.LG

TL;DR: 本研究探索使用多模态大语言模型（GPT-4o和GPT-4o-mini）生成认知评估的合成规范数据，通过改进的提示策略能够有效区分诊断组和人口统计差异，为开发新型图像认知测试提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 传统认知评估规范数据收集成本高、耗时长且更新不及时，限制了新图像认知测试的开发。多模态大语言模型的发展为从现有认知测试图像生成合成规范数据提供了新途径。

Method: 使用GPT-4o和GPT-4o-mini模型，采用两种提示策略：基础指令的朴素提示和包含上下文指导的高级提示。通过嵌入分析评估区分诊断组和人口统计差异的能力，使用BLEU、ROUGE、BERTScore和LLM-as-a-judge等指标进行评估。

Result: 高级提示策略生成的合成响应能更有效区分诊断组并捕捉人口统计多样性。优质模型生成的响应具有更高的真实性和多样性。BERTScore是上下文相似性评估最可靠的指标，而BLEU对创造性输出的评估效果较差。LLM-as-a-judge方法提供了有希望的初步验证结果。

Conclusion: 研究表明，通过精炼的提示方法引导的多模态大语言模型能够可行地生成现有认知测试的稳健合成规范数据，为开发新型图像认知测试奠定了基础，克服了传统方法的局限性。

Abstract: Cognitive assessments require normative data as essential benchmarks for
evaluating individual performance. Hence, developing new cognitive tests based
on novel image stimuli is challenging due to the lack of readily available
normative data. Traditional data collection methods are costly, time-consuming,
and infrequently updated, limiting their practical utility. Recent advancements
in generative multimodal large language models (MLLMs) offer a new approach to
generate synthetic normative data from existing cognitive test images. We
investigated the feasibility of using MLLMs, specifically GPT-4o and
GPT-4o-mini, to synthesize normative textual responses for established
image-based cognitive assessments, such as the "Cookie Theft" picture
description task. Two distinct prompting strategies-naive prompts with basic
instructions and advanced prompts enriched with contextual guidance-were
evaluated. Responses were analyzed using embeddings to assess their capacity to
distinguish diagnostic groups and demographic variations. Performance metrics
included BLEU, ROUGE, BERTScore, and an LLM-as-a-judge evaluation. Advanced
prompting strategies produced synthetic responses that more effectively
distinguished between diagnostic groups and captured demographic diversity
compared to naive prompts. Superior models generated responses exhibiting
higher realism and diversity. BERTScore emerged as the most reliable metric for
contextual similarity assessment, while BLEU was less effective for evaluating
creative outputs. The LLM-as-a-judge approach provided promising preliminary
validation results. Our study demonstrates that generative multimodal LLMs,
guided by refined prompting methods, can feasibly generate robust synthetic
normative data for existing cognitive tests, thereby laying the groundwork for
developing novel image-based cognitive assessments without the traditional
limitations.

</details>


### [206] [TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training](https://arxiv.org/abs/2508.17677)
*Yifan Wang,Binbin Liu,Fengze Liu,Yuanfan Guo,Jiyao Deng,Xuecheng Wu,Weidong Zhou,Xiaohuan Zhou,Taifeng Wang*

Main category: cs.LG

TL;DR: TiKMiX是一种动态调整语言模型预训练数据混合比例的方法，通过Group Influence指标高效评估数据域影响，相比静态混合策略显著提升性能并减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 静态数据混合策略在语言模型预训练中效果不佳，因为模型对不同数据域的学习偏好会随训练动态变化，但高效观测这些变化偏好存在挑战。

Method: 提出Group Influence指标评估数据域影响，将数据混合问题转化为寻找最优影响最大化分布的问题，开发了TiKMiX-D直接优化方法和TiKMiX-M回归预测方法。

Result: TiKMiX-D仅用20%计算资源就超越了REGMIX等先进方法；TiKMiX-M在9个下游基准测试中平均性能提升2%；模型在1万亿token规模上验证有效。

Conclusion: 动态基于Group Influence调整数据混合比例能显著提升性能，有效缓解静态比例导致的数据消化不足问题，模型的数据偏好会随训练进度和规模变化。

Abstract: The data mixture used in the pre-training of a language model is a
cornerstone of its final performance. However, a static mixing strategy is
suboptimal, as the model's learning preferences for various data domains shift
dynamically throughout training. Crucially, observing these evolving
preferences in a computationally efficient manner remains a significant
challenge. To address this, we propose TiKMiX, a method that dynamically
adjusts the data mixture according to the model's evolving preferences. TiKMiX
introduces Group Influence, an efficient metric for evaluating the impact of
data domains on the model. This metric enables the formulation of the data
mixing problem as a search for an optimal, influence-maximizing distribution.
We solve this via two approaches: TiKMiX-D for direct optimization, and
TiKMiX-M, which uses a regression model to predict a superior mixture. We
trained models with different numbers of parameters, on up to 1 trillion
tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like
REGMIX while using just 20% of the computational resources. TiKMiX-M leads to
an average performance gain of 2% across 9 downstream benchmarks. Our
experiments reveal that a model's data preferences evolve with training
progress and scale, and we demonstrate that dynamically adjusting the data
mixture based on Group Influence, a direct measure of these preferences,
significantly improves performance by mitigating the underdigestion of data
seen with static ratios.

</details>


### [207] [Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data](https://arxiv.org/abs/2508.18244)
*Chu-Cheng Lin,Daiyi Peng,Yifeng Lu,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: TACs框架通过将工作流重新表述为类型化概率程序，使用梯度优化方法显著提升了大型语言模型在结构化任务中的性能和合规性


<details>
  <summary>Details</summary>
Motivation: 当前基于离散提示优化的LLM工作流组合方法脆弱且难以确保结构化任务的正式合规性，需要更鲁棒的理论基础

Method: 将整个工作流（参数高效适配的LLMs和确定性逻辑）视为未归一化联合分布，实现基于梯度的原则性训练，支持潜在中间结构

Result: 在结构化任务上显著优于最先进的提示优化基线，MGSM-SymPy从57.1%提升到75.9%（27B模型），MGSM从1.6%提升到27.3%（7B模型）

Conclusion: TACs为开发可靠、任务合规的LLM系统提供了一个鲁棒且理论上有依据的新范式

Abstract: Reliably composing Large Language Models (LLMs) for complex, multi-step
workflows remains a significant challenge. The dominant paradigm-optimizing
discrete prompts in a pipeline-is notoriously brittle and struggles to enforce
the formal compliance required for structured tasks. We introduce
Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow
adaptation as learning typed probabilistic programs. TACs treats the entire
workflow, which is composed of parameter-efficiently adapted LLMs and
deterministic logic, as an unnormalized joint distribution. This enables
principled, gradient-based training even with latent intermediate structures.
We provide theoretical justification for our tractable optimization objective,
proving that the optimization bias vanishes as the model learns type
compliance. Empirically, TACs significantly outperforms state-of-the-art
prompt-optimization baselines. Gains are particularly pronounced on structured
tasks, improving MGSM-SymPy from $57.1\%$ to $75.9\%$ for a 27B model, MGSM
from $1.6\%$ to $27.3\%$ for a 7B model. TACs offers a robust and theoretically
grounded paradigm for developing reliable, task-compliant LLM systems.

</details>


### [208] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: 本文评估了基于Mamba的状态空间模型在GPU上的训练行为，构建了代表性工作负载套件，分析了其架构影响，并提出了性能优化方向。


<details>
  <summary>Details</summary>
Motivation: Mamba状态空间模型作为Transformer的有前景替代方案，具有线性计算复杂度优势，但需要了解其在GPU上的行为特征以指导硬件架构设计。

Method: 构建涵盖不同模型架构的Mamba SSM工作负载套件，在GPU上进行训练行为分析，研究其微架构影响。

Result: 揭示了Mamba SSM在GPU上的运行特征，识别了性能瓶颈和优化机会。

Conclusion: 为继续扩展此类模型性能提供了新的优化方向，对GPU微架构设计具有重要指导意义。

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


### [209] [ANO : Faster is Better in Noisy Landscape](https://arxiv.org/abs/2508.18258)
*Adrien Kegreisz*

Main category: cs.LG

TL;DR: Ano优化器通过分离方向和平滑处理，使用动量进行方向平滑，瞬时梯度幅度确定步长，提高了对梯度噪声的鲁棒性。Anolog进一步通过对数调度扩展动量窗口，消除对动量系数的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam和Adan在非平稳或噪声环境中性能下降，主要因为它们依赖基于动量的幅度估计。需要一种更鲁棒的优化方法。

Method: 提出Ano优化器，将方向和平滑解耦：动量用于方向平滑，瞬时梯度幅度决定步长。进一步提出Anolog，通过对数调度扩展动量窗口时间。

Result: 建立了非凸收敛保证，收敛速率与其他基于符号的方法相似。在强化学习等噪声和非平稳环境中表现显著提升，在计算机视觉基准测试中保持竞争力。

Conclusion: Ano优化器通过解耦方向和平滑的设计，在保持一阶方法简洁高效的同时，显著提高了在噪声环境中的鲁棒性，为深度学习优化提供了新的有效解决方案。

Abstract: Stochastic optimizers are central to deep learning, yet widely used methods
such as Adam and Adan can degrade in non-stationary or noisy environments,
partly due to their reliance on momentum-based magnitude estimates. We
introduce Ano, a novel optimizer that decouples direction and magnitude:
momentum is used for directional smoothing, while instantaneous gradient
magnitudes determine step size. This design improves robustness to gradient
noise while retaining the simplicity and efficiency of first-order methods. We
further propose Anolog, which removes sensitivity to the momentum coefficient
by expanding its window over time via a logarithmic schedule. We establish
non-convex convergence guarantees with a convergence rate similar to other
sign-based methods, and empirically show that Ano provides substantial gains in
noisy and non-stationary regimes such as reinforcement learning, while
remaining competitive on low-noise tasks such as standard computer vision
benchmarks.

</details>


### [210] [On the Edge of Memorization in Diffusion Models](https://arxiv.org/abs/2508.17689)
*Sam Buchanan,Druv Pai,Yi Ma,Valentin De Bortoli*

Main category: cs.LG

TL;DR: 本文研究了扩散模型何时会复制训练数据（记忆化）以及何时能够生成超越训练数据的新样本（泛化），提出了一个理论框架来分析模型参数化程度对记忆化与泛化行为的影响，并通过实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型中记忆化与泛化的平衡关系对于实际部署具有重要意义，特别是在版权侵权和数据隐私等现实问题方面。当前缺乏对这一现象的理论理解，需要建立科学的分析框架。

Method: 建立了一个科学的数学"实验室"，在合成和自然图像结构数据上训练扩散模型。理论分析了记忆化模型与泛化模型训练损失的差异，提出了一个交叉点理论来预测记忆化主导的临界模型大小。

Result: 理论预测显示在特定模型参数化程度下会出现记忆化与泛化的相变，通过精心设计的实验验证了这一理论预测的正确性。

Conclusion: 研究提供了一个可分析且具有实践意义的理论框架，能够预测扩散模型何时会以记忆化为主，为未来的理论和实证研究奠定了基础。

Abstract: When do diffusion models reproduce their training data, and when are they
able to generate samples beyond it? A practically relevant theoretical
understanding of this interplay between memorization and generalization may
significantly impact real-world deployments of diffusion models with respect to
issues such as copyright infringement and data privacy. In this work, to
disentangle the different factors that influence memorization and
generalization in practical diffusion models, we introduce a scientific and
mathematical "laboratory" for investigating these phenomena in diffusion models
trained on fully synthetic or natural image-like structured data. Within this
setting, we hypothesize that the memorization or generalization behavior of an
underparameterized trained model is determined by the difference in training
loss between an associated memorizing model and a generalizing model. To probe
this hypothesis, we theoretically characterize a crossover point wherein the
weighted training loss of a fully generalizing model becomes greater than that
of an underparameterized memorizing model at a critical value of model
(under)parameterization. We then demonstrate via carefully-designed experiments
that the location of this crossover predicts a phase transition in diffusion
models trained via gradient descent, validating our hypothesis. Ultimately, our
theory enables us to analytically predict the model size at which memorization
becomes predominant. Our work provides an analytically tractable and
practically meaningful setting for future theoretical and empirical
investigations. Code for our experiments is available at
https://github.com/DruvPai/diffusion_mem_gen.

</details>


### [211] [Rethinking Federated Learning Over the Air: The Blessing of Scaling Up](https://arxiv.org/abs/2508.17697)
*Jiaqi Zhu,Bikramjit Das,Yong Xie,Nikolaos Pappas,Howard H. Yang*

Main category: cs.LG

TL;DR: 论文分析了空中计算联邦学习在大规模客户端场景下的性能，发现增加客户端数量能增强隐私保护、减轻信道衰落影响并改善收敛性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时面临通信资源限制的挑战，特别是在支持大量客户端的系统中。空中计算通过模拟信号传输可以缓解通信瓶颈，但会带来信道失真问题，需要理论分析其影响

Method: 开发理论框架分析大规模客户端场景下空中联邦学习的性能，通过信息论和收敛性分析来研究客户端数量扩展带来的影响

Result: 分析发现增加参与客户端数量具有三个关键优势：1)增强隐私保护；2)减轻信道衰落影响；3)改善收敛性能。实验验证了理论分析

Conclusion: 空中计算模型训练是联邦学习在大规模客户端网络中可行的解决方案，客户端数量扩展能带来隐私、信道和收敛性能的多重好处

Abstract: Federated learning facilitates collaborative model training across multiple
clients while preserving data privacy. However, its performance is often
constrained by limited communication resources, particularly in systems
supporting a large number of clients. To address this challenge, integrating
over-the-air computations into the training process has emerged as a promising
solution to alleviate communication bottlenecks. The system significantly
increases the number of clients it can support in each communication round by
transmitting intermediate parameters via analog signals rather than digital
ones. This improvement, however, comes at the cost of channel-induced
distortions, such as fading and noise, which affect the aggregated global
parameters. To elucidate these effects, this paper develops a theoretical
framework to analyze the performance of over-the-air federated learning in
large-scale client scenarios. Our analysis reveals three key advantages of
scaling up the number of participating clients: (1) Enhanced Privacy: The
mutual information between a client's local gradient and the server's
aggregated gradient diminishes, effectively reducing privacy leakage. (2)
Mitigation of Channel Fading: The channel hardening effect eliminates the
impact of small-scale fading in the noisy global gradient. (3) Improved
Convergence: Reduced thermal noise and gradient estimation errors benefit the
convergence rate. These findings solidify over-the-air model training as a
viable approach for federated learning in networks with a large number of
clients. The theoretical insights are further substantiated through extensive
experimental evaluations.

</details>


### [212] [Adaptive Ensemble Learning with Gaussian Copula for Load Forecasting](https://arxiv.org/abs/2508.17700)
*Junying Yang,Gang Lu,Xiaoqing Yan,Peng Xia,Di Wu*

Main category: cs.LG

TL;DR: 提出基于高斯Copula的自适应集成学习模型处理稀疏数据下的负荷预测问题，通过数据补全、多模型预测和自适应集成三个模块提升预测鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机器学习在完整数据下能准确进行负荷预测，但实际数据收集存在不确定性导致数据稀疏，需要解决稀疏数据下的预测问题

Method: 使用高斯Copula进行数据补全消除稀疏性，采用5个机器学习模型分别预测，最后通过自适应集成获得加权求和结果

Result: 实验证明该模型具有鲁棒性

Conclusion: 提出的自适应集成学习模型能有效处理稀疏数据下的负荷预测问题，展现了良好的性能

Abstract: Machine learning (ML) is capable of accurate Load Forecasting from complete
data. However, there are many uncertainties that affect data collection,
leading to sparsity. This article proposed a model called Adaptive Ensemble
Learning with Gaussian Copula to deal with sparsity, which contains three
modules: data complementation, ML construction, and adaptive ensemble. First,
it applies Gaussian Copula to eliminate sparsity. Then, we utilise five ML
models to make predictions individually. Finally, it employs adaptive ensemble
to get final weighted-sum result. Experiments have demonstrated that our model
are robust.

</details>


### [213] [Copyright Protection for 3D Molecular Structures with Watermarking](https://arxiv.org/abs/2508.17702)
*Runwen Hu,Peilin Chen,Keyan Ding,Shiqi Wang*

Main category: cs.LG

TL;DR: 提出了首个针对分子的鲁棒水印方法，利用原子级特征保持分子完整性，通过不变特征确保对仿射变换的鲁棒性，在保持90%以上基本属性的同时实现95%以上的水印准确率。


<details>
  <summary>Details</summary>
Motivation: 人工智能在分子生成领域的快速发展带来了知识产权保护的重要挑战，需要开发有效的水印技术来保护分子知识产权。

Method: 利用原子级特征保持分子完整性，采用不变特征确保对仿射变换的鲁棒性，在QM9和GEOM-DRUG数据集上使用GeoBFN和GeoLDM生成模型进行验证。

Result: 水印准确率超过95%，基本属性保持率高于90%，下游对接模拟显示水印分子与原始分子性能相当，结合亲和力达到-6.00 kcal/mol，均方根偏差低于1.602 Å。

Conclusion: 该水印技术能有效保护分子知识产权而不影响科学实用性，为分子发现和研究应用中安全、负责任的人工智能集成提供了解决方案。

Abstract: Artificial intelligence (AI) revolutionizes molecule generation in
bioengineering and biological research, significantly accelerating discovery
processes. However, this advancement introduces critical concerns regarding
intellectual property protection. To address these challenges, we propose the
first robust watermarking method designed for molecules, which utilizes
atom-level features to preserve molecular integrity and invariant features to
ensure robustness against affine transformations. Comprehensive experiments
validate the effectiveness of our method using the datasets QM9 and GEOM-DRUG,
and generative models GeoBFN and GeoLDM. We demonstrate the feasibility of
embedding watermarks, maintaining basic properties higher than 90.00\% while
achieving watermark accuracy greater than 95.00\%. Furthermore, downstream
docking simulations reveal comparable performance between original and
watermarked molecules, with binding affinities reaching -6.00 kcal/mol and root
mean square deviations below 1.602 \AA. These results confirm that our
watermarking technique effectively safeguards molecular intellectual property
without compromising scientific utility, enabling secure and responsible AI
integration in molecular discovery and research applications.

</details>


### [214] [Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks](https://arxiv.org/abs/2508.17744)
*Sotaro Takeshita,Yurina Takeshita,Daniel Ruffinelli,Simone Paolo Ponzetto*

Main category: cs.LG

TL;DR: 研究发现随机截断文本嵌入向量高达50%的维度只会导致下游任务性能轻微下降（<10%），这与传统认知相反，表明许多均匀分布的维度实际上在移除后能提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索截断文本嵌入对下游性能的影响，了解为什么移除大量嵌入维度只会造成轻微性能损失，以及这对文本编码理解的启示。

Method: 使用6个最先进的文本编码器和26个下游任务进行实验，随机移除不同比例的嵌入维度，分析性能变化趋势和原因。

Result: 在检索和分类任务中，随机移除高达50%的嵌入维度仅导致性能下降不到10%；发现许多均匀分布的维度在移除后反而能提升性能。

Conclusion: 文本嵌入中存在大量冗余维度，这一现象不仅限于分类和检索任务，在生成任务中也存在，对嵌入压缩和高效表示具有重要意义。

Abstract: In this paper, we study the surprising impact that truncating text embeddings
has on downstream performance. We consistently observe across 6
state-of-the-art text encoders and 26 downstream tasks, that randomly removing
up to 50% of embedding dimensions results in only a minor drop in performance,
less than 10%, in retrieval and classification tasks. Given the benefits of
using smaller-sized embeddings, as well as the potential insights about text
encoding, we study this phenomenon and find that, contrary to what is suggested
in prior work, this is not the result of an ineffective use of representation
space. Instead, we find that a large number of uniformly distributed dimensions
actually cause an increase in performance when removed. This would explain why,
on average, removing a large number of embedding dimensions results in a
marginal drop in performance. We make similar observations when truncating the
embeddings used by large language models to make next-token predictions on
generative tasks, suggesting that this phenomenon is not isolated to
classification or retrieval tasks.

</details>


### [215] [Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2508.17751)
*Alessio Arcudi,Davide Sartor,Alberto Sinigaglia,Vincent François-Lavet,Gian Antonio Susto*

Main category: cs.LG

TL;DR: MANGO是一个新颖的分层强化学习框架，通过多层抽象和嵌套选项来解决长期稀疏奖励环境中的挑战，显著提高了样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务中长期稀疏奖励环境带来的挑战，如样本效率低、泛化能力差和决策过程不透明等问题。

Method: 将复杂任务分解为多层抽象，每层定义抽象状态空间并使用选项将轨迹模块化为宏动作。选项在层间嵌套，允许学习动作的复用。引入层内策略指导智能体在抽象状态空间中的转移，以及整合奖励函数等任务特定组件的任务动作。

Result: 在程序生成的网格环境中实验表明，相比标准RL方法，MANGO在样本效率和泛化能力方面都有显著提升，同时增强了决策过程的可解释性。

Conclusion: MANGO为分层强化学习提供了有效的多层抽象框架，在稀疏奖励环境中表现出色，未来将探索自动抽象发现、连续环境适应和更鲁棒的多层训练策略。

Abstract: This paper introduces MANGO (Multilayer Abstraction for Nested Generation of
Options), a novel hierarchical reinforcement learning framework designed to
address the challenges of long-term sparse reward environments. MANGO
decomposes complex tasks into multiple layers of abstraction, where each layer
defines an abstract state space and employs options to modularize trajectories
into macro-actions. These options are nested across layers, allowing for
efficient reuse of learned movements and improved sample efficiency. The
framework introduces intra-layer policies that guide the agent's transitions
within the abstract state space, and task actions that integrate task-specific
components such as reward functions. Experiments conducted in
procedurally-generated grid environments demonstrate substantial improvements
in both sample efficiency and generalization capabilities compared to standard
RL methods. MANGO also enhances interpretability by making the agent's
decision-making process transparent across layers, which is particularly
valuable in safety-critical and industrial applications. Future work will
explore automated discovery of abstractions and abstract actions, adaptation to
continuous or fuzzy environments, and more robust multi-layer training
strategies.

</details>


### [216] [SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling](https://arxiv.org/abs/2508.17756)
*Fanjiang Ye,Zepeng Zhao,Yi Mu,Jucheng Shen,Renjie Li,Kaijian Wang,Desen Sun,Saurabh Agarwal,Myungjin Lee,Triston Cao,Aditya Akella,Arvind Krishnamurthy,T. S. Eugene Ng,Zhengzhong Tu,Yuke Wang*

Main category: cs.LG

TL;DR: SuperGen是一个基于分块的训练免费框架，用于超高清视频生成，通过分块策略、自适应缓存和并行处理来降低计算和内存成本，同时支持多种分辨率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但对超高清视频生成的需求日益增长，现有标准分辨率平台面临重新训练需求高、计算和内存成本巨大的挑战。

Method: 采用基于分块的训练免费算法创新，结合自适应区域感知缓存策略和缓存引导的通信最小化分块并行处理。

Result: 评估表明SuperGen在保持高质量输出的同时，获得了最大性能增益，显著降低了内存占用和计算复杂度。

Conclusion: SuperGen成功解决了超高清视频生成的挑战，提供了一个高效且无需额外训练的解决方案，适用于各种分辨率。

Abstract: Diffusion models have recently achieved remarkable success in generative
tasks (e.g., image and video generation), and the demand for high-quality
content (e.g., 2K/4K videos) is rapidly increasing across various domains.
However, generating ultra-high-resolution videos on existing
standard-resolution (e.g., 720p) platforms remains challenging due to the
excessive re-training requirements and prohibitively high computational and
memory costs. To this end, we introduce SuperGen, an efficient tile-based
framework for ultra-high-resolution video generation. SuperGen features a novel
training-free algorithmic innovation with tiling to successfully support a wide
range of resolutions without additional training efforts while significantly
reducing both memory footprint and computational complexity. Moreover, SuperGen
incorporates a tile-tailored, adaptive, region-aware caching strategy that
accelerates video generation by exploiting redundancy across denoising steps
and spatial regions. SuperGen also integrates cache-guided,
communication-minimized tile parallelism for enhanced throughput and minimized
latency. Evaluations demonstrate that SuperGen harvests the maximum performance
gains while achieving high output quality across various benchmarks.

</details>


### [217] [Evaluating the Quality of the Quantified Uncertainty for (Re)Calibration of Data-Driven Regression Models](https://arxiv.org/abs/2508.17761)
*Jelke Wibbeke,Nico Schönfisch,Sebastian Rohjans,Andreas Rauh*

Main category: cs.LG

TL;DR: 本文系统分析了回归校准指标的不一致性问题，发现不同指标对同一校准结果的评估经常产生冲突甚至矛盾的结论，并推荐了ENCE和CWC作为最可靠的指标。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，数据驱动模型不仅需要准确，还必须提供可靠的uncertainty估计（校准）。然而现有的校准指标在定义、假设和尺度上差异很大，导致难以解释和比较不同研究的结果，且大多数重新校准方法仅使用少量指标进行评估。

Method: 系统地从文献中提取和分类回归校准指标，独立于特定建模方法或重新校准方法对这些指标进行基准测试。通过使用真实世界数据、合成数据和人工失准数据的受控实验来评估指标性能。

Result: 实验表明校准指标经常产生冲突的结果：许多指标对同一重新校准结果的评估存在分歧，有些甚至得出相反的结论。这种不一致性可能导致指标选择上的cherry-picking问题。

Conclusion: 研究发现Expected Normalized Calibration Error (ENCE)和Coverage Width-based Criterion (CWC)是测试中最可靠的指标，强调了指标选择在校准研究中的关键作用。

Abstract: In safety-critical applications data-driven models must not only be accurate
but also provide reliable uncertainty estimates. This property, commonly
referred to as calibration, is essential for risk-aware decision-making. In
regression a wide variety of calibration metrics and recalibration methods have
emerged. However, these metrics differ significantly in their definitions,
assumptions and scales, making it difficult to interpret and compare results
across studies. Moreover, most recalibration methods have been evaluated using
only a small subset of metrics, leaving it unclear whether improvements
generalize across different notions of calibration. In this work, we
systematically extract and categorize regression calibration metrics from the
literature and benchmark these metrics independently of specific modelling
methods or recalibration approaches. Through controlled experiments with
real-world, synthetic and artificially miscalibrated data, we demonstrate that
calibration metrics frequently produce conflicting results. Our analysis
reveals substantial inconsistencies: many metrics disagree in their evaluation
of the same recalibration result, and some even indicate contradictory
conclusions. This inconsistency is particularly concerning as it potentially
allows cherry-picking of metrics to create misleading impressions of success.
We identify the Expected Normalized Calibration Error (ENCE) and the Coverage
Width-based Criterion (CWC) as the most dependable metrics in our tests. Our
findings highlight the critical role of metric selection in calibration
research.

</details>


### [218] [Puzzle: Scheduling Multiple Deep Learning Models on Mobile Device with Heterogeneous Processors](https://arxiv.org/abs/2508.17764)
*Duseok Kang,Yunseong Lee,Junghoon Kim*

Main category: cs.LG

TL;DR: 提出基于遗传算法的深度学习网络调度方法Puzzle，通过子图划分在异构处理器上调度多模型，相比基准方法支持3.7倍和2.2倍的请求频率提升


<details>
  <summary>Details</summary>
Motivation: 移动设备硬件异构性增强，现有调度方法局限于单模型场景、忽视配置差异、执行时间估计不准确，无法满足现实多模型调度需求

Method: 使用遗传算法将网络划分为多个子图，包含三种染色体类型进行划分/映射/优先级探索，采用设备在线性能分析和评估实现准确执行时间估计

Result: 在9个先进网络的随机场景评估中，Puzzle相比NPU Only和Best Mapping基准方法分别支持平均3.7倍和2.2倍的请求频率提升，同时满足同等实时性要求

Conclusion: 提出的遗传算法调度方法能有效解决移动设备异构处理器上的多深度学习模型调度问题，显著提升系统吞吐量

Abstract: As deep learning models are increasingly deployed on mobile devices, modern
mobile devices incorporate deep learning-specific accelerators to handle the
growing computational demands, thus increasing their hardware heterogeneity.
However, existing works on scheduling deep learning workloads across these
processors have significant limitations: most studies focus on single-model
scenarios rather than realistic multi-model scenarios, overlook performance
variations from different hardware/software configurations, and struggle with
accurate execution time estimation. To address these challenges, we propose a
novel genetic algorithm-based methodology for scheduling multiple deep learning
networks on heterogeneous processors by partitioning the networks into multiple
subgraphs. Our approach incorporates three different types of chromosomes for
partition/mapping/priority exploration, and leverages device-in-the-loop
profiling and evaluation for accurate execution time estimation. Based on this
methodology, our system, Puzzle, demonstrates superior performance in extensive
evaluations with randomly generated scenarios involving nine state-of-the-art
networks. The results demonstrate Puzzle can support 3.7 and 2.2 times higher
request frequency on average compared to the two heuristic baselines, NPU Only
and Best Mapping, respectively, while satisfying the equivalent level of
real-time requirements.

</details>


### [219] [Multi-domain Distribution Learning for De Novo Drug Design](https://arxiv.org/abs/2508.17815)
*Arne Schneuing,Ilia Igashov,Adrian W. Dobbelstein,Thomas Castiglione,Michael Bronstein,Bruno Correia*

Main category: cs.LG

TL;DR: DrugFlow是一个基于结构的药物设计生成模型，结合连续流匹配和离散马尔可夫桥，在3D蛋白质-配体数据学习中表现优异，并提供不确定性估计和偏好对齐采样。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时学习化学、几何和物理特性的3D蛋白质-配体生成模型，解决现有方法在不确定性估计和采样优化方面的不足。

Method: 整合连续流匹配和离散马尔可夫桥，提供不确定性估计机制，提出联合偏好对齐方案，并扩展模型以联合采样蛋白质侧链角度和分子构象。

Result: 在3D蛋白质-配体数据学习方面达到最先进性能，能够检测分布外样本，并通过偏好对齐优化采样过程。

Conclusion: DrugFlow提供了一个强大的结构基药物设计框架，具有不确定性感知和可优化采样能力，为蛋白质-配体相互作用研究提供了新工具。

Abstract: We introduce DrugFlow, a generative model for structure-based drug design
that integrates continuous flow matching with discrete Markov bridges,
demonstrating state-of-the-art performance in learning chemical, geometric, and
physical aspects of three-dimensional protein-ligand data. We endow DrugFlow
with an uncertainty estimate that is able to detect out-of-distribution
samples. To further enhance the sampling process towards distribution regions
with desirable metric values, we propose a joint preference alignment scheme
applicable to both flow matching and Markov bridge frameworks. Furthermore, we
extend our model to also explore the conformational landscape of the protein by
jointly sampling side chain angles and molecules.

</details>


### [220] [Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets](https://arxiv.org/abs/2508.17930)
*Sarina Penquitt,Tobias Riedlinger,Timo Heller,Markus Reischl,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出了一种统一的方法来检测目标检测、语义分割和实例分割数据集中的标签错误，通过注入标签错误并将其作为实例分割问题来解决。


<details>
  <summary>Details</summary>
Motivation: 当前标签错误检测方法通常只针对单一计算机视觉任务，且不是基于学习的。错误标注数据会导致模型性能下降、基准测试结果偏差和整体准确性降低。

Method: 通过注入不同类型的标签错误到真实标签中，将标签错误检测问题构建为基于复合输入的实例分割问题。

Result: 在多个任务、数据集和基础模型上进行了实验，与各种基线和最先进方法进行比较，并在Cityscapes数据集中识别并发布了459个真实标签错误。

Conclusion: 该方法能够统一处理多种计算机视觉任务的标签错误检测，并在模拟和真实标签错误上都表现出良好的性能。

Abstract: Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.

</details>


### [221] [Choice Outweighs Effort: Facilitating Complementary Knowledge Fusion in Federated Learning via Re-calibration and Merit-discrimination](https://arxiv.org/abs/2508.17954)
*Ming Yang,Dongrun Li,Xin Wang,Xiaoyang Yu,Xiaoming Wu,Shibo He*

Main category: cs.LG

TL;DR: FedMate通过双边优化解决联邦学习中的数据异构性问题，在服务器端构建动态全局原型并微调分类器，在客户端采用互补分类融合和成本感知特征传输，在多个数据集上实现了泛化与适应的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中跨客户端数据异构性导致偏见，阻碍无偏共识凝聚和泛化-个性化知识的互补融合。现有方法依赖静态指标和刚性全局对齐，导致共识扭曲和模型适应性下降。

Method: 双边优化：服务器端构建动态全局原型（聚合权重由样本量、当前参数和未来预测综合校准），微调类别分类器；客户端采用互补分类融合实现择优判别训练，结合成本感知特征传输平衡性能与通信效率。

Result: 在五个不同复杂度数据集上实验表明，FedMate在协调泛化和适应方面优于最先进方法。自动驾驶数据集上的语义分割实验验证了方法的实际可扩展性。

Conclusion: FedMate通过动态原型构建和互补融合机制，有效解决了联邦学习中的数据异构性问题，实现了泛化能力与个性化适应的最佳平衡，具有实际应用价值。

Abstract: Cross-client data heterogeneity in federated learning induces biases that
impede unbiased consensus condensation and the complementary fusion of
generalization- and personalization-oriented knowledge. While existing
approaches mitigate heterogeneity through model decoupling and representation
center loss, they often rely on static and restricted metrics to evaluate local
knowledge and adopt global alignment too rigidly, leading to consensus
distortion and diminished model adaptability. To address these limitations, we
propose FedMate, a method that implements bilateral optimization: On the server
side, we construct a dynamic global prototype, with aggregation weights
calibrated by holistic integration of sample size, current parameters, and
future prediction; a category-wise classifier is then fine-tuned using this
prototype to preserve global consistency. On the client side, we introduce
complementary classification fusion to enable merit-based discrimination
training and incorporate cost-aware feature transmission to balance model
performance and communication efficiency. Experiments on five datasets of
varying complexity demonstrate that FedMate outperforms state-of-the-art
methods in harmonizing generalization and adaptation. Additionally, semantic
segmentation experiments on autonomous driving datasets validate the method's
real-world scalability.

</details>


### [222] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出生成式特征填补框架，通过空间错误集中打包、扩散模型特征重建和语义感知功率分配，提升语义通信在数字系统中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语义通信在6G网络中具有高效通信潜力，但在数字系统中部署时面临传输错误导致语义内容失真的挑战，需要确保对传输错误的鲁棒性。

Method: 提出三阶段方法：1) 空间错误集中打包策略，基于信道映射编码特征元素；2) 生成式特征填补方法，使用扩散模型重建丢失特征；3) 语义感知功率分配方案，根据语义重要性分配传输功率。

Result: 实验结果表明，在块衰落条件下，该框架优于传统的深度联合信源信道编码(DJSCC)和JPEG2000方法，实现了更高的语义准确性和更低的LPIPS分数。

Conclusion: 所提出的生成式特征填补框架有效解决了语义通信在数字系统中的鲁棒性问题，为6G网络中语义通信的实际部署提供了可行的解决方案。

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for
achieving unprecedented communication efficiency in sixth-generation (6G)
networks by leveraging artificial intelligence (AI) to extract and transmit the
underlying meanings of source data. However, deploying SemCom over digital
systems presents new challenges, particularly in ensuring robustness against
transmission errors that may distort semantically critical content. To address
this issue, this paper proposes a novel framework, termed generative feature
imputing, which comprises three key techniques. First, we introduce a spatial
error concentration packetization strategy that spatially concentrates feature
distortions by encoding feature elements based on their channel mappings, a
property crucial for both the effectiveness and reduced complexity of the
subsequent techniques. Second, building on this strategy, we propose a
generative feature imputing method that utilizes a diffusion model to
efficiently reconstruct missing features caused by packet losses. Finally, we
develop a semantic-aware power allocation scheme that enables unequal error
protection by allocating transmission power according to the semantic
importance of each packet. Experimental results demonstrate that the proposed
framework outperforms conventional approaches, such as Deep Joint
Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,
achieving higher semantic accuracy and lower Learned Perceptual Image Patch
Similarity (LPIPS) scores.

</details>


### [223] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: 提出基于神经网络的拓扑感知时间变化标量场插值方法，利用持久图和时间关键帧学习时间-标量场映射关系，通过拓扑损失改善几何和拓扑重建质量


<details>
  <summary>Details</summary>
Motivation: 解决时间变化标量场序列中非关键帧数据缺失的问题，传统插值方法难以保持数据的拓扑结构完整性

Method: 使用神经网络架构学习时间值与对应标量场的关系，结合拓扑损失函数利用输入持久图改善重建质量，支持实时单次网络传播生成插值结果

Result: 在2D和3D时间变化数据集上的实验显示，该方法在数据和拓扑拟合方面均优于参考插值方案

Conclusion: 该方法能够快速生成拓扑一致的插值结果，为时间变化数据的分析和可视化提供了有效的解决方案

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [224] [A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond](https://arxiv.org/abs/2508.18001)
*Sebastian G. Gruber*

Main category: cs.LG

TL;DR: 本博士论文提出了一种基于合适评分(proper scores)的机器学习不确定性量化框架，通过函数Bregman散度实现偏差-方差分解，并将检验准确性-锐度分解扩展到分类以外的任务。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化是可信任机器学习的关键，但现有方法多为任务特定的。需要一种通用框架来统一地处理回归、分类和生成模型中的不确定性问题。

Method: 使用合适评分作为损失函数，通过函数Bregman散度实现严格合适评分的偏差-方差分解，并将核评分应用于生成模型评估。同时扩展了检验准确性-锐度分解到非分类任务。

Result: 提出了大语言模型不确定性估计新方法，性能超过现有最佳方法。定义了合适检验锐度误差的新估计器，并对生成图像模型进行了更细粒度的评估。

Conclusion: 该框架通过合适评分实现了统一的不确定性量化方法，可应用于多种机器学习任务，为可靠的机器学习应用提供了理论基础和实践方法。

Abstract: In this PhD thesis, we propose a novel framework for uncertainty
quantification in machine learning, which is based on proper scores.
Uncertainty quantification is an important cornerstone for trustworthy and
reliable machine learning applications in practice. Usually, approaches to
uncertainty quantification are problem-specific, and solutions and insights
cannot be readily transferred from one task to another. Proper scores are loss
functions minimized by predicting the target distribution. Due to their very
general definition, proper scores apply to regression, classification, or even
generative modeling tasks. We contribute several theoretical results, that
connect epistemic uncertainty, aleatoric uncertainty, and model calibration
with proper scores, resulting in a general and widely applicable framework. We
achieve this by introducing a general bias-variance decomposition for strictly
proper scores via functional Bregman divergences. Specifically, we use the
kernel score, a kernel-based proper score, for evaluating sample-based
generative models in various domains, like image, audio, and natural language
generation. This includes a novel approach for uncertainty estimation of large
language models, which outperforms state-of-the-art baselines. Further, we
generalize the calibration-sharpness decomposition beyond classification, which
motivates the definition of proper calibration errors. We then introduce a
novel estimator for proper calibration errors in classification, and a novel
risk-based approach to compare different estimators for squared calibration
errors. Last, we offer a decomposition of the kernel spherical score, another
kernel-based proper score, allowing a more fine-grained and interpretable
evaluation of generative image models.

</details>


### [225] [Does simple trump complex? Comparing strategies for adversarial robustness in DNNs](https://arxiv.org/abs/2508.18019)
*William Brooks,Marelie H. Davel,Coenraad Mouton*

Main category: cs.LG

TL;DR: 本研究通过分析两种对抗训练方法（简单损失函数修改和复杂的Dynamics-Aware Robust Training）的组件，识别出最能提升深度神经网络对抗鲁棒性的关键因素，重点关注输入空间中的边界距离。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在各种应用中表现出色但仍易受对抗攻击，需要识别对抗训练技术中真正提升鲁棒性的关键组件。

Method: 使用VGG-16模型，系统性地分离和评估两种对抗训练方法的各个组件，在CIFAR-10数据集上测试AutoAttack和PGD等对抗攻击下的性能。

Result: 分析揭示了哪些元素最有效地增强对抗鲁棒性，为设计更鲁棒的DNN提供了见解。

Conclusion: 通过边界最大化方法的组件分析，可以识别出对抗训练中最关键的鲁棒性提升因素，有助于开发更有效的防御策略。

Abstract: Deep Neural Networks (DNNs) have shown substantial success in various
applications but remain vulnerable to adversarial attacks. This study aims to
identify and isolate the components of two different adversarial training
techniques that contribute most to increased adversarial robustness,
particularly through the lens of margins in the input space -- the minimal
distance between data points and decision boundaries. Specifically, we compare
two methods that maximize margins: a simple approach which modifies the loss
function to increase an approximation of the margin, and a more complex
state-of-the-art method (Dynamics-Aware Robust Training) which builds upon this
approach. Using a VGG-16 model as our base, we systematically isolate and
evaluate individual components from these methods to determine their relative
impact on adversarial robustness. We assess the effect of each component on the
model's performance under various adversarial attacks, including AutoAttack and
Projected Gradient Descent (PGD). Our analysis on the CIFAR-10 dataset reveals
which elements most effectively enhance adversarial robustness, providing
insights for designing more robust DNNs.

</details>


### [226] [Enhancing Differentially Private Linear Regression via Public Second-Moment](https://arxiv.org/abs/2508.18037)
*Zilong Cao,Hai Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种利用公共数据改进差分隐私线性回归的新方法，通过变换私有数据使用公共二阶矩矩阵，提高了估计器的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法仅基于私有数据添加噪声，会显著降低效用。需要利用公共数据来增强差分隐私方法的实用性。

Method: 基于充分统计量扰动的线性回归普通最小二乘估计器，使用公共二阶矩矩阵变换私有数据，计算变换后的SSP-OLSE，获得更好的条件数和估计精度。

Result: 理论推导了该方法与标准SSP-OLSE相比非差分隐私OLSE的误差界限，实验在合成和真实数据集上验证了方法的有效性和实用性。

Conclusion: 所提出的方法通过利用公共数据显著提高了差分隐私线性回归的准确性和鲁棒性，为差分隐私统计学习提供了新的解决方案。

Abstract: Leveraging information from public data has become increasingly crucial in
enhancing the utility of differentially private (DP) methods. Traditional DP
approaches often require adding noise based solely on private data, which can
significantly degrade utility. In this paper, we address this limitation in the
context of the ordinary least squares estimator (OLSE) of linear regression
based on sufficient statistics perturbation (SSP) under the unbounded data
assumption. We propose a novel method that involves transforming private data
using the public second-moment matrix to compute a transformed SSP-OLSE, whose
second-moment matrix yields a better condition number and improves the OLSE
accuracy and robustness. We derive theoretical error bounds about our method
and the standard SSP-OLSE to the non-DP OLSE, which reveal the improved
robustness and accuracy achieved by our approach. Experiments on synthetic and
real-world datasets demonstrate the utility and effectiveness of our method.

</details>


### [227] [Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation](https://arxiv.org/abs/2508.18045)
*Xiuheng Wang,Ricardo Borsoi,Arnaud Breloy,Cédric Richard*

Main category: cs.LG

TL;DR: 提出了一种基于鲁棒质心的流式时间序列变点检测方法，通过比较Karcher均值和Huber函数定义的鲁棒质心来构建检测统计量，使用随机黎曼优化算法高效估计质心，在模拟和真实数据上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统基于质心监测的流式变点检测方法需要对质心更新步长进行精细调优，这在实际应用中存在困难。为了解决这个问题，需要开发对估计方法不敏感的鲁棒检测统计量

Method: 利用M估计理论中的鲁棒质心概念，比较对变化敏感的经典Karcher均值和对变化鲁棒的Huber函数定义质心，构建检测统计量，并采用随机黎曼优化算法高效估计两个质心

Result: 在两个代表性流形上的模拟和真实数据实验表明，所提方法具有优越的性能，对底层估计方法的敏感性较低

Conclusion: 基于鲁棒质心比较的变点检测方法有效解决了传统方法对步长调优的依赖问题，在黎曼流形上的流式时间序列变点检测中表现出色

Abstract: Non-parametric change-point detection in streaming time series data is a
long-standing challenge in signal processing. Recent advancements in statistics
and machine learning have increasingly addressed this problem for data residing
on Riemannian manifolds. One prominent strategy involves monitoring abrupt
changes in the center of mass of the time series. Implemented in a streaming
fashion, this strategy, however, requires careful step size tuning when
computing the updates of the center of mass. In this paper, we propose to
leverage robust centroid on manifolds from M-estimation theory to address this
issue. Our proposal consists of comparing two centroid estimates: the classical
Karcher mean (sensitive to change) versus one defined from Huber's function
(robust to change). This comparison leads to the definition of a test statistic
whose performance is less sensitive to the underlying estimation method. We
propose a stochastic Riemannian optimization algorithm to estimate both robust
centroids efficiently. Experiments conducted on both simulated and real-world
data across two representative manifolds demonstrate the superior performance
of our proposed method.

</details>


### [228] [Training Transformers for Mesh-Based Simulations](https://arxiv.org/abs/2508.18051)
*Paul Garnier,Vincent Lannelongue,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: 通过使用邻接矩阵作为注意力掩码的图Transformer架构，提出了一种高效可扩展的物理模拟方法，在大规模网格上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统消息传递图神经网络在处理大规模复杂网格时的扩展性和效率问题，充分利用邻接矩阵信息来提高模型性能。

Method: 提出了一种新的图Transformer架构，使用邻接矩阵作为注意力掩码，包括扩张滑动窗口和全局注意力等创新增强技术来扩大感知野不避免计算成本。

Result: 模型在30万节点和300万边的大规模网格上表现出艰扩展性。最小模型与MeshGraphNet性能相当但速度提高7倍、小座6倍；最大模型比之前SOTA提升38.8%，比MeshGraphNet提升52%，而训练速度相似。

Conclusion: 该图Transformer架构为大规模物理模拟提供了高效、可扩展的解决方案，在保持计算效率的同时实现了显著的性能提升。

Abstract: Simulating physics using Graph Neural Networks (GNNs) is predominantly driven
by message-passing architectures, which face challenges in scaling and
efficiency, particularly in handling large, complex meshes. These architectures
have inspired numerous enhancements, including multigrid approaches and $K$-hop
aggregation (using neighbours of distance $K$), yet they often introduce
significant complexity and suffer from limited in-depth investigations. In
response to these challenges, we propose a novel Graph Transformer architecture
that leverages the adjacency matrix as an attention mask. The proposed approach
incorporates innovative augmentations, including Dilated Sliding Windows and
Global Attention, to extend receptive fields without sacrificing computational
efficiency. Through extensive experimentation, we evaluate model size,
adjacency matrix augmentations, positional encoding and $K$-hop configurations
using challenging 3D computational fluid dynamics (CFD) datasets. We also train
over 60 models to find a scaling law between training FLOPs and parameters. The
introduced models demonstrate remarkable scalability, performing on meshes with
up to 300k nodes and 3 million edges. Notably, the smallest model achieves
parity with MeshGraphNet while being $7\times$ faster and $6\times$ smaller.
The largest model surpasses the previous state-of-the-art by $38.8$\% on
average and outperforms MeshGraphNet by $52$\% on the all-rollout RMSE, while
having a similar training speed. Code and datasets are available at
https://github.com/DonsetPG/graph-physics.

</details>


### [229] [Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks](https://arxiv.org/abs/2508.18052)
*Silvia Beddar-Wiesing,Alice Moallemy-Oureh*

Main category: cs.LG

TL;DR: 本文扩展了图神经网络理论，从离散动态图扩展到连续时间动态图，提出了连续时间动态1-WL测试和对应的CGNN架构，保持了区分能力和通用逼近保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统（如通信网络、金融交易网络）通常以异步方式演化且可能出现断开连接，而现有GNN理论仅限于离散动态图序列，无法处理这类连续时间动态图。

Method: 引入连续时间动态1-WL测试，证明其与连续时间动态展开树的等价性，基于离散动态GNN架构设计连续时间动态GNN（CGNN），使用分段连续可微时间函数处理异步断开图。

Result: 建立了连续时间动态图的等价理论，识别出具有区分能力和通用逼近保证的CGNN类别，提供了实用的设计指南。

Conclusion: 成功将GNN理论扩展到连续时间动态图领域，为处理现实世界中异步演化的图数据提供了理论保证和实用架构设计。

Abstract: Graph Neural Networks (GNNs) are known to match the distinguishing power of
the 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with
the unfolding tree equivalence classes of graphs. Preserving this equivalence,
GNNs can universally approximate any target function on graphs in probability
up to any precision. However, these results are limited to attributed
discrete-dynamic graphs represented as sequences of connected graph snapshots.
Real-world systems, such as communication networks, financial transaction
networks, and molecular interactions, evolve asynchronously and may split into
disconnected components. In this paper, we extend the theory of attributed
discrete-dynamic graphs to attributed continuous-time dynamic graphs with
arbitrary connectivity. To this end, we introduce a continuous-time dynamic
1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,
and identify a class of continuous-time dynamic GNNs (CGNNs) based on
discrete-dynamic GNN architectures that retain both distinguishing power and
universal approximation guarantees. Our constructive proofs further yield
practical design guidelines, emphasizing a compact and expressive CGNN
architecture with piece-wise continuously differentiable temporal functions to
process asynchronous, disconnected graphs.

</details>


### [230] [FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning](https://arxiv.org/abs/2508.18060)
*Emmanouil Kritharakis,Antonios Makris,Dusan Jakovetic,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: FedGreed是一种针对联邦学习的鲁棒聚合策略，通过基于服务器可信数据集评估客户端模型损失来排序和选择最佳客户端更新，无需假设恶意客户端比例，在非IID数据分布下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端可能表现出拜占庭攻击行为的问题，同时处理现实部署中普遍存在的异构（非IID）数据分布挑战。

Method: 提出FedGreed聚合策略：基于服务器可信数据集评估客户端本地模型更新的损失指标，按损失大小排序并贪婪选择损失最小的客户端子集进行聚合。

Result: 在MNIST、FMNIST和CIFAR-10数据集上的实验表明，FedGreed在大多数对抗场景（包括标签翻转和高斯噪声注入攻击）中显著优于标准方法和现有鲁棒基线方法。

Conclusion: FedGreed提供了一种无需假设恶意客户端比例的有效防御机制，在异构数据环境下具有收敛保证和有界最优性差距，为联邦学习提供了强大的抗攻击能力。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients while preserving data privacy by keeping local datasets on-device. In
this work, we address FL settings where clients may behave adversarially,
exhibiting Byzantine attacks, while the central server is trusted and equipped
with a reference dataset. We propose FedGreed, a resilient aggregation strategy
for federated learning that does not require any assumptions about the fraction
of adversarial participants. FedGreed orders clients' local model updates based
on their loss metrics evaluated against a trusted dataset on the server and
greedily selects a subset of clients whose models exhibit the minimal
evaluation loss. Unlike many existing approaches, our method is designed to
operate reliably under heterogeneous (non-IID) data distributions, which are
prevalent in real-world deployments. FedGreed exhibits convergence guarantees
and bounded optimality gaps under strong adversarial behavior. Experimental
evaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method
significantly outperforms standard and robust federated learning baselines,
such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of
adversarial scenarios considered, including label flipping and Gaussian noise
injection attacks. All experiments were conducted using the Flower federated
learning framework.

</details>


### [231] [Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing Detection](https://arxiv.org/abs/2508.18085)
*Abyad Enan,Mashrur Chowdhury,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 提出了一种基于混合量子-经典自编码器(HQC-AE)的零日GNSS欺骗检测方法，仅使用真实信号训练，无需欺骗数据，能有效检测新型时间推送欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: GNSS系统对欺骗攻击高度脆弱，现有基于监督学习的方法难以检测新型和未见过的攻击，需要开发不依赖欺骗样本的检测方法。

Method: 使用混合量子-经典自编码器，仅基于真实GNSS信号训练，利用跟踪阶段提取的特征进行主动检测，专注于静态接收器的时间推送欺骗攻击检测。

Result: HQC-AE在检测未见时间推送欺骗攻击方面表现优异，平均检测准确率达97.71%，误报率0.62%；对复杂攻击准确率达98.23%，误报率1.85%。

Conclusion: 该方法能有效主动检测零日GNSS时间推送欺骗攻击，在各种静态GNSS接收器平台上表现优异，超越了传统监督学习和现有无监督学习方法。

Abstract: Global Navigation Satellite Systems (GNSS) are critical for Positioning,
Navigation, and Timing (PNT) applications. However, GNSS are highly vulnerable
to spoofing attacks, where adversaries transmit counterfeit signals to mislead
receivers. Such attacks can lead to severe consequences, including misdirected
navigation, compromised data integrity, and operational disruptions. Most
existing spoofing detection methods depend on supervised learning techniques
and struggle to detect novel, evolved, and unseen attacks. To overcome this
limitation, we develop a zero-day spoofing detection method using a Hybrid
Quantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSS
signals without exposure to spoofed data. By leveraging features extracted
during the tracking stage, our method enables proactive detection before PNT
solutions are computed. We focus on spoofing detection in static GNSS
receivers, which are particularly susceptible to time-push spoofing attacks,
where attackers manipulate timing information to induce incorrect time
computations at the receiver. We evaluate our model against different unseen
time-push spoofing attack scenarios: simplistic, intermediate, and
sophisticated. Our analysis demonstrates that the HQC-AE consistently
outperforms its classical counterpart, traditional supervised learning-based
models, and existing unsupervised learning-based methods in detecting zero-day,
unseen GNSS time-push spoofing attacks, achieving an average detection accuracy
of 97.71% with an average false negative rate of 0.62% (when an attack occurs
but is not detected). For sophisticated spoofing attacks, the HQC-AE attains an
accuracy of 98.23% with a false negative rate of 1.85%. These findings
highlight the effectiveness of our method in proactively detecting zero-day
GNSS time-push spoofing attacks across various stationary GNSS receiver
platforms.

</details>


### [232] [Provable Mixed-Noise Learning with Flow-Matching](https://arxiv.org/abs/2508.18122)
*Paul Hagemann,Robert Gruhlke,Bernhard Stankewitz,Claudia Schillings,Gabriele Steidl*

Main category: cs.LG

TL;DR: 提出了基于条件流匹配和EM算法的混合噪声贝叶斯反问题推断框架，能够联合估计后验采样器和噪声参数，在高维场景下具有良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用（特别是物理和化学领域）中的噪声通常具有未知和异质性结构，而传统推断方法往往假设固定或已知的噪声特性，无法有效处理混合噪声问题。

Method: 将条件流匹配嵌入到期望最大化（EM）算法中，使用无模拟的基于ODE的流匹配作为EM算法E步中的生成模型，联合估计后验采样器和噪声参数。

Result: 在无限观测的总体极限下，EM更新收敛到真实噪声参数。数值结果表明该方法在混合噪声贝叶斯反问题中的有效性。

Conclusion: 结合EM推断和流匹配的方法为处理混合噪声贝叶斯反问题提供了一种有效的解决方案，特别适用于高维推断场景。

Abstract: We study Bayesian inverse problems with mixed noise, modeled as a combination
of additive and multiplicative Gaussian components. While traditional inference
methods often assume fixed or known noise characteristics, real-world
applications, particularly in physics and chemistry, frequently involve noise
with unknown and heterogeneous structure. Motivated by recent advances in
flow-based generative modeling, we propose a novel inference framework based on
conditional flow matching embedded within an Expectation-Maximization (EM)
algorithm to jointly estimate posterior samplers and noise parameters. To
enable high-dimensional inference and improve scalability, we use
simulation-free ODE-based flow matching as the generative model in the E-step
of the EM algorithm. We prove that, under suitable assumptions, the EM updates
converge to the true noise parameters in the population limit of infinite
observations. Our numerical results illustrate the effectiveness of combining
EM inference with flow matching for mixed-noise Bayesian inverse problems.

</details>


### [233] [Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics](https://arxiv.org/abs/2508.18130)
*Pradeep Singh,Mehak Sharma,Anupriya Dey,Balasubramanian Raman*

Main category: cs.LG

TL;DR: FreezeTST是一种轻量级混合模型，通过将冻结的随机特征块与标准可训练Transformer层交错组合，在保持推理复杂度的同时显著降低训练成本和参数数量，在长期时间序列预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长序列建模中存在二次自注意力计算成本高和时间偏差弱的问题，使得长期预测既昂贵又脆弱。需要一种更高效的替代方案。

Method: 采用混合设计：交替使用冻结的随机特征（储层）块和标准可训练Transformer层。冻结块提供丰富的非线性记忆而无优化成本，可训练层通过自注意力学习查询这些记忆。

Result: 在7个标准长期预测基准测试中，FreezeTST始终匹配或超越Informer、Autoformer和PatchTST等专门变体，且计算成本显著降低。

Conclusion: 将储层原理嵌入Transformer中为高效长期时间序列预测提供了一条简单而原则性的途径，证明了混合架构的有效性。

Abstract: Transformers are the de-facto choice for sequence modelling, yet their
quadratic self-attention and weak temporal bias can make long-range forecasting
both expensive and brittle. We introduce FreezeTST, a lightweight hybrid that
interleaves frozen random-feature (reservoir) blocks with standard trainable
Transformer layers. The frozen blocks endow the network with rich nonlinear
memory at no optimisation cost; the trainable layers learn to query this memory
through self-attention. The design cuts trainable parameters and also lowers
wall-clock training time, while leaving inference complexity unchanged. On
seven standard long-term forecasting benchmarks, FreezeTST consistently matches
or surpasses specialised variants such as Informer, Autoformer, and PatchTST;
with substantially lower compute. Our results show that embedding reservoir
principles within Transformers offers a simple, principled route to efficient
long-term time-series prediction.

</details>


### [234] [Unveiling the Actual Performance of Neural-based Models for Equation Discovery on Graph Dynamical Systems](https://arxiv.org/abs/2508.18173)
*Riccardo Cappi,Paolo Frazzetto,Nicolò Navarin,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 本文对符号回归技术在网络动态系统方程发现中的性能进行了系统评估，比较了稀疏回归、MLP和新型图KANs方法，发现基于神经网络的方法显著优于现有基线，其中KANs在简洁性和可解释性方面表现更佳


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的"黑盒"特性阻碍了其在需要可解释性的科学发现中的应用，特别是在网络动态系统的控制方程发现中，拓扑结构进一步增加了复杂性

Method: 评估了稀疏回归和MLP架构等现有方法，并引入了图Kolmogorov-Arnold Networks (KANs)的新颖适配，利用其固有的可解释性优势

Result: 在合成和真实动态系统测试中，MLP和KAN架构都能成功识别底层符号方程，显著超越现有基线。KANs以更高的简洁性和透明度实现这一性能

Conclusion: 研究为研究人员提供了实用指南，阐明了模型表达能力和可解释性之间的权衡，确立了基于神经网络的架构在复杂系统稳健科学发现中的可行性

Abstract: The ``black-box'' nature of deep learning models presents a significant
barrier to their adoption for scientific discovery, where interpretability is
paramount. This challenge is especially pronounced in discovering the governing
equations of dynamical processes on networks or graphs, since even their
topological structure further affects the processes' behavior. This paper
provides a rigorous, comparative assessment of state-of-the-art symbolic
regression techniques for this task. We evaluate established methods, including
sparse regression and MLP-based architectures, and introduce a novel adaptation
of Kolmogorov-Arnold Networks (KANs) for graphs, designed to exploit their
inherent interpretability. Across a suite of synthetic and real-world dynamical
systems, our results demonstrate that both MLP and KAN-based architectures can
successfully identify the underlying symbolic equations, significantly
surpassing existing baselines. Critically, we show that KANs achieve this
performance with greater parsimony and transparency, as their learnable
activation functions provide a clearer mapping to the true physical dynamics.
This study offers a practical guide for researchers, clarifying the trade-offs
between model expressivity and interpretability, and establishes the viability
of neural-based architectures for robust scientific discovery on complex
systems.

</details>


### [235] [HypER: Hyperbolic Echo State Networks for Capturing Stretch-and-Fold Dynamics in Chaotic Flows](https://arxiv.org/abs/2508.18196)
*Pradeep Singh,Sutirtha Ghosh,Ashutosh Kumar,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TL;DR: 提出了基于双曲几何的HypER回声状态网络，通过双曲距离的指数衰减连接来匹配混沌系统的拉伸-折叠结构，显著延长了混沌动力学的有效预测时间


<details>
  <summary>Details</summary>
Motivation: 现有的ESN使用欧几里得几何的储备池，与混沌系统的拉伸-折叠结构不匹配，导致预测时间受限

Method: 在庞加莱球中采样神经元，连接权重随双曲距离指数衰减，保持ESN的稀疏性、泄漏积分和谱半径控制等特性，仅训练Tikhonov正则化读出层

Result: 在Lorenz-63、Roessler系统和Chen-Ueta超混沌吸引子上，HypER显著延长了平均有效预测时间，在30次独立运行中均获得统计显著增益；在心率变异性和太阳黑子数等真实数据上也表现出优势

Conclusion: 双曲几何嵌入为混沌系统预测提供了更好的几何匹配，HypER框架能够有效模拟Lyapunov指数增长，为长期混沌预测提供了新途径

Abstract: Forecasting chaotic dynamics beyond a few Lyapunov times is difficult because
infinitesimal errors grow exponentially. Existing Echo State Networks (ESNs)
mitigate this growth but employ reservoirs whose Euclidean geometry is
mismatched to the stretch-and-fold structure of chaos. We introduce the
Hyperbolic Embedding Reservoir (HypER), an ESN whose neurons are sampled in the
Poincare ball and whose connections decay exponentially with hyperbolic
distance. This negative-curvature construction embeds an exponential metric
directly into the latent space, aligning the reservoir's local
expansion-contraction spectrum with the system's Lyapunov directions while
preserving standard ESN features such as sparsity, leaky integration, and
spectral-radius control. Training is limited to a Tikhonov-regularized readout.
On the chaotic Lorenz-63 and Roessler systems, and the hyperchaotic Chen-Ueta
attractor, HypER consistently lengthens the mean valid-prediction horizon
beyond Euclidean and graph-structured ESN baselines, with statistically
significant gains confirmed over 30 independent runs; parallel results on
real-world benchmarks, including heart-rate variability from the Santa Fe and
MIT-BIH datasets and international sunspot numbers, corroborate its advantage.
We further establish a lower bound on the rate of state divergence for HypER,
mirroring Lyapunov growth.

</details>


### [236] [Aligning the Evaluation of Probabilistic Predictions with Downstream Value](https://arxiv.org/abs/2508.18251)
*Novin Shahroudi,Viacheslav Komisarenko,Meelis Kull*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的代理评估函数学习方法，用于对检预测质量与下游任务效果进行对齐评估，解决传统预测指标与实际下游效益之间的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 传统的预测性能评估指标（如MSE、MAE等）经常与实际下游任务的真实效果存在偏差，而现有方法要么需要多个任务特定指标（分析费力），要么需要先验成本结构知识（常规偏偏偏）。

Method: 基于合适评分规则理论，探索保持合适性的评分规则变换方法。提出使用神经网络参数化的加权评分规则，通过学习加权来实现与下游任务性能的对齐。

Result: 通过合成数据和实际数据的回归任务实验，证明了该框架能够有效缩小预测评估与下游效用之间的差距。

Conclusion: 该方法为模块化预测系统提供了一种可扩展、高效的评估对齐方案，能够在复杂或未知的加权结构下实现快速评估循环。

Abstract: Every prediction is ultimately used in a downstream task. Consequently,
evaluating prediction quality is more meaningful when considered in the context
of its downstream use. Metrics based solely on predictive performance often
diverge from measures of real-world downstream impact. Existing approaches
incorporate the downstream view by relying on multiple task-specific metrics,
which can be burdensome to analyze, or by formulating cost-sensitive
evaluations that require an explicit cost structure, typically assumed to be
known a priori. We frame this mismatch as an evaluation alignment problem and
propose a data-driven method to learn a proxy evaluation function aligned with
the downstream evaluation. Building on the theory of proper scoring rules, we
explore transformations of scoring rules that ensure the preservation of
propriety. Our approach leverages weighted scoring rules parametrized by a
neural network, where weighting is learned to align with the performance in the
downstream task. This enables fast and scalable evaluation cycles across tasks
where the weighting is complex or unknown a priori. We showcase our framework
through synthetic and real-data experiments for regression tasks, demonstrating
its potential to bridge the gap between predictive evaluation and downstream
utility in modular prediction systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [237] [Generative AI for Multimedia Communication: Recent Advances, An Information-Theoretic Framework, and Future Opportunities](https://arxiv.org/abs/2508.17163)
*Yili Jin,Xue Liu,Jiangchuan Liu*

Main category: cs.MM

TL;DR: 本文提出了一个创新的语义信息理论框架，将生成式AI与信息理论结合，重新定义多媒体通信为语义信息传递而非单纯的数据传输。


<details>
  <summary>Details</summary>
Motivation: 传统信息理论框架无法解决对人类感知至关重要的语义保真度问题，而生成式AI的突破性进展正在改变多媒体通信方式。

Method: 提出语义信息理论框架，引入语义熵、互信息、信道容量和率失真概念，专门针对多媒体应用进行适配。

Result: 建立了一个将生成式AI创新与信息理论相连接的框架，为多媒体通信系统提供了新的理论基础。

Conclusion: 该框架为实现稳健、高效且具有语义意义的多媒体通信系统指明了道路，旨在激发语义优先的范式转变，对未来多媒体研究具有重要影响。

Abstract: Recent breakthroughs in generative artificial intelligence (AI) are
transforming multimedia communication. This paper systematically reviews key
recent advancements across generative AI for multimedia communication,
emphasizing transformative models like diffusion and transformers. However,
conventional information-theoretic frameworks fail to address semantic
fidelity, critical to human perception. We propose an innovative semantic
information-theoretic framework, introducing semantic entropy, mutual
information, channel capacity, and rate-distortion concepts specifically
adapted to multimedia applications. This framework redefines multimedia
communication from purely syntactic data transmission to semantic information
conveyance. We further highlight future opportunities and critical research
directions. We chart a path toward robust, efficient, and semantically
meaningful multimedia communication systems by bridging generative AI
innovations with information theory. This exploratory paper aims to inspire a
semantic-first paradigm shift, offering a fresh perspective with significant
implications for future multimedia research.

</details>


### [238] [Generative Flow Networks for Personalized Multimedia Systems: A Case Study on Short Video Feeds](https://arxiv.org/abs/2508.17166)
*Yili Jin,Ling Pan,Rui-Xiao Zhang,Jiangchuan Liu,Xue Liu*

Main category: cs.MM

TL;DR: 本文提出使用生成流网络(GFlowNets)作为个性化多媒体系统的新框架，通过在短视频推荐案例中验证其优越性能，展示了该框架在多媒体资源优化和个性化体验提升方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 多媒体系统需要满足日益增长的个性化需求，高效管理竞争性资源需求、自适应内容和用户特定数据处理，传统方法在处理这些复杂优化挑战时存在局限性。

Method: 引入生成流网络(GFlowNets)框架，结合多候选生成建模和基于流的原则，开发了基于GFlowNet的个性化推荐算法，并在短视频feed应用中进行案例研究。

Result: GFlowNet方法在视频质量、资源利用效率和传输成本等关键指标上均优于传统的基于规则和强化学习方法，表现出卓越的性能。

Conclusion: GFlowNets为解决复杂优化挑战和支持复杂多媒体应用场景提供了可扩展且灵活的解决方案，具有广泛的适用性和适应性，能够推动个性化多媒体系统的发展。

Abstract: Multimedia systems underpin modern digital interactions, facilitating
seamless integration and optimization of resources across diverse multimedia
applications. To meet growing personalization demands, multimedia systems must
efficiently manage competing resource needs, adaptive content, and
user-specific data handling. This paper introduces Generative Flow Networks
(GFlowNets, GFNs) as a brave new framework for enabling personalized multimedia
systems. By integrating multi-candidate generative modeling with flow-based
principles, GFlowNets offer a scalable and flexible solution for enhancing
user-specific multimedia experiences. To illustrate the effectiveness of
GFlowNets, we focus on short video feeds, a multimedia application
characterized by high personalization demands and significant resource
constraints, as a case study. Our proposed GFlowNet-based personalized feeds
algorithm demonstrates superior performance compared to traditional rule-based
and reinforcement learning methods across critical metrics, including video
quality, resource utilization efficiency, and delivery cost. Moreover, we
propose a unified GFlowNet-based framework generalizable to other multimedia
systems, highlighting its adaptability and wide-ranging applicability. These
findings underscore the potential of GFlowNets to advance personalized
multimedia systems by addressing complex optimization challenges and supporting
sophisticated multimedia application scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [239] [Polynomial Property Testing](https://arxiv.org/abs/2508.16878)
*Lior Gishboliner,Asaf Shapira*

Main category: cs.DS

TL;DR: 该论文综述了密集图模型中的属性测试问题，探讨了哪些属性可以用多项式查询复杂度(相对于1/ε)进行高效测试，并总结了当前研究成果和关键开放问题。


<details>
  <summary>Details</summary>
Motivation: 研究属性测试中查询复杂度的效率问题，当前已知许多属性测试的查询复杂度仅依赖于误差参数ε，但现有边界随1/ε增长极快，需要探索哪些属性可以实现多项式复杂度的高效测试。

Method: 采用综述研究方法，系统整理和分析密集图模型中属性测试的现有理论成果，重点关注查询复杂度与误差参数ε的关系，特别研究poly(1/ε)查询复杂度的可行性。

Result: 总结了密集图模型属性测试领域的状态知识，识别出能够实现多项式查询复杂度的属性类别，同时指出了该领域的重要开放问题和研究挑战。

Conclusion: 属性测试的效率问题是一个重要的理论挑战，虽然已有一些属性可以实现poly(1/ε)查询复杂度，但仍有许多开放问题需要解决，该领域需要进一步的理论突破。

Abstract: Property testers are fast, randomized "election polling"-type algorithms that
determine if an input (e.g., graph or hypergraph) has a certain property or is
$\varepsilon$-far from the property. In the dense graph model of property
testing, it is known that many properties can be tested with query complexity
that depends only on the error parameter $\varepsilon$ (and not on the size of
the input), but the current bounds on the query complexity grow extremely
quickly as a function of $1/\varepsilon$. Which properties can be tested
efficiently, i.e., with $\mathrm{poly}(1/\varepsilon)$ queries? This survey
presents the state of knowledge on this general question, as well as some key
open problems.

</details>


### [240] [Better Indexing for Rectangular Pattern Matching](https://arxiv.org/abs/2508.17365)
*Paweł Gawrychowski,Adam Górkiewicz*

Main category: cs.DS

TL;DR: 本文提出了一种新的二维字符串索引结构，能够在O(m + k log^ε n)时间内查找任意矩形模式的所有k次出现，结构大小为O(n log n)，构建时间为近线性。


<details>
  <summary>Details</summary>
Motivation: 解决二维矩形模式匹配问题，打破传统认为无法在类似一维字符串匹配效率下处理矩形模式的观念，克服了Giancarlo在1993年提出的下界限制。

Method: 构建一个大小为O(n log n)的简单索引结构，支持对任意矩形模式进行高效查询，采用创新的数据结构设计来规避传统方法的下界限制。

Result: 成功实现了对二维矩形模式的高效匹配，查询时间为O(m + k log^ε n)，其中ε>0为任意小常数，结构构建时间为近线性O~(n)。

Conclusion: 证明了二维矩形模式匹配可以达到接近一维字符串匹配的效率，打破了长期以来的认知限制，为二维模式匹配提供了新的理论突破。

Abstract: We revisit the complexity of building, given a two-dimensional string of size
$n$, an indexing structure that allows locating all $k$ occurrences of a
two-dimensional pattern of size $m$. While a structure of size $\mathcal{O}(n)$
with query time $\mathcal{O}(m+k)$ is known for this problem under the
additional assumption that the pattern is a square [Giancarlo, SICOMP 1995], a
popular belief was that for rectangular patterns one cannot achieve such (or
even similar) bounds, due to a lower bound for a certain natural class of
approaches [Giancarlo, WADS 1993]. We show that, in fact, it is possible to
construct a very simple structure of size $\mathcal{O}(n\log n)$ that supports
such queries for any rectangular pattern in
$\mathcal{O}(m+k\log^{\varepsilon}n)$ time, for any $\varepsilon>0$. Further,
our structure can be constructed in $\tilde{\mathcal{O}}(n)$ time.

</details>


### [241] [A Little Clairvoyance Is All You Need](https://arxiv.org/abs/2508.17759)
*Anupam Gupta,Haim Kaplan,Alexander Lindermayr,Jens Schlöter,Sorrachai Yingchareonthawornchai*

Main category: cs.DS

TL;DR: 本文研究了ε-clairvoyant设置下的单机在线调度问题，提出了一个简单的确定性算法，在ε>0时达到常数竞争比，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 解决非clairvoyant设置中无法获得常数竞争比的问题，探索需要多少先验知识才能克服该问题的困难性。ε-clairvoyant模型平滑地插值于完全clairvoyant和非clairvoyant设置之间。

Method: 提出了一个确定性算法，应用"面对不确定性的乐观"原则。算法在作业剩余处理时间等于其处理时间的ε分数时获知作业大小。通过维护最优队列和算法队列之间的匹配，并保持小的前缀扩展来证明竞争比。

Result: 对于所有ε∈(0,1)，提出了一个确定性的⌈1/ε⌉-竞争算法，这是确定性算法的最优结果。同时为随机算法提供了匹配的下界（达到常数因子）。

Conclusion: 很少的先验知识就足以克服非clairvoyant调度问题的困难性，ε-clairvoyant模型为在线调度问题提供了新的见解和解决方案。

Abstract: We revisit the classical problem of minimizing the total flow time of jobs on
a single machine in the online setting where jobs arrive over time. It has long
been known that the Shortest Remaining Processing Time (SRPT) algorithm is
optimal (i.e., $1$-competitive) when the job sizes are known up-front [Schrage,
1968]. But in the non-clairvoyant setting where job sizes are revealed only
when the job finishes, no algorithm can be constant-competitive [Motwani,
Phillips, and Torng, 1994].
  We consider the $\varepsilon$-clairvoyant setting, where $\varepsilon \in
[0,1]$, and each job's processing time becomes known once its remaining
processing time equals an $\varepsilon$ fraction of its processing time. This
captures settings where the system user uses the initial $(1-\varepsilon)$
fraction of a job's processing time to learn its true length, which it can then
reveal to the algorithm. The model was proposed by Yingchareonthawornchai and
Torng (2017), and it smoothly interpolates between the clairvoyant setting
(when $\epsilon = 1$) and the non-clairvoyant setting (when $\varepsilon = 0$).
In a concrete sense, we are asking: how much knowledge is required to
circumvent the hardness of this problem?
  We show that a little knowledge is enough, and that a constant competitive
algorithm exists for every constant $\varepsilon > 0$. More precisely, for all
$\varepsilon \in (0,1)$, we present a deterministic $\smash{\lceil
\frac{1}{\varepsilon}\rceil}$-competitive algorithm, which is optimal for
deterministic algorithms. We also present a matching lower bound (up to a
constant factor) for randomized algorithms. Our algorithm to achieve this bound
is remarkably simple and applies the ``optimism in the face of uncertainty''
principle. The proof relies on maintaining a matching between the jobs in the
optimum's queue and the algorithm's queue, with small prefix expansion.

</details>


### [242] [Towards Constant Time Multi-Call Rumor Spreading on Small-Set Expanders](https://arxiv.org/abs/2508.18017)
*Emilio Cruciani,Sebastian Forster,Tijn de Vos*

Main category: cs.DS

TL;DR: 研究多呼叫版本的PUSH&PULL谣言传播过程，节点可以在每次操作中联系k个邻居而非单个邻居，证明通过增加通信成本可以加速传播速度


<details>
  <summary>Details</summary>
Motivation: 探索如何在增加节点间通信量的情况下加速谣言传播过程，特别是在具有强局部连通性的小集合顶点扩展图上

Method: 提出k-PUSH&PULL协议变体，分析其在完全图和小集合顶点扩展图上的传播轮数复杂度

Result: 在完全图上，标准协议需要Θ(log n)轮，而k-PUSH&PULL仅需Θ(log_k n)轮；在小集合扩展图上，需要O(log_φ n · log_k n)轮

Conclusion: 多呼叫策略能显著加速谣言传播，特别是在具有强局部扩展特性的图上，但需要付出更高的通信成本

Abstract: We study a multi-call variant of the classic PUSH&PULL rumor spreading
process where nodes can contact $k$ of their neighbors instead of a single one
during both PUSH and PULL operations. We show that rumor spreading can be made
faster at the cost of an increased amount of communication between the nodes.
As a motivating example, consider the process on a complete graph of $n$ nodes:
while the standard PUSH&PULL protocol takes $\Theta(\log n)$ rounds, we prove
that our $k$-PUSH&PULL variant completes in $\Theta(\log_{k} n)$ rounds, with
high probability.
  We generalize this result in an expansion-sensitive way, as has been done for
the classic PUSH&PULL protocol for different notions of expansion, e.g.,
conductance and vertex expansion. We consider small-set vertex expanders,
graphs in which every sufficiently small subset of nodes has a large
neighborhood, ensuring strong local connectivity. In particular, when the
expansion parameter satisfies $\phi > 1$, these graphs have a diameter of
$o(\log n)$, as opposed to other standard notions of expansion. Since the
graph's diameter is a lower bound on the number of rounds required for rumor
spreading, this makes small-set expanders particularly well-suited for fast
information dissemination. We prove that $k$-PUSH&PULL takes $O(\log_{\phi} n
\cdot \log_{k} n)$ rounds in these expanders, with high probability. We
complement this with a simple lower bound of $\Omega(\log_{\phi} n+ \log_{k}
n)$ rounds.

</details>


### [243] [Spectral Refutations of Semirandom $k$-LIN over Larger Fields](https://arxiv.org/abs/2508.18185)
*Nicholas Kocurek,Peter Manohar*

Main category: cs.DS

TL;DR: 本文提出了一种用于强反驳半随机k-LIN(F)实例的算法，该算法在字段大小|F|上具有最优依赖性，填补了现有工作中|F|^{3k}的差距。


<details>
  <summary>Details</summary>
Motivation: 对于有限域F上的k-稀疏非齐次线性方程系统，当F=F2时已有紧致的时间-约束密度权衡结果，但对于更大字段的情况，现有方法通过黑盒归约到F2情况，导致当前最佳上下界之间存在|F|^{3k}的差距。

Method: 提出了一种算法，对于任意参数ℓ，算法运行时间为(|F|n)^{O(ℓ)}，能够强反驳具有O(n)·(|F*|n/ℓ)^{k/2-1}log(n|F*|)/ε^4约束的半随机k-LIN(F)实例。

Result: 算法在字段大小|F|上的依赖是最优的，通过Sum-of-Squares层次结构的下界证明，该阈值最多相差polylog(n|F*|)因子。结果还扩展到更一般的有限阿贝尔群情况。

Conclusion: 本文解决了半随机k-LIN(F)实例强反驳问题中字段大小依赖性的最优性问题，为更大字段情况提供了紧致的算法和匹配的下界。

Abstract: We study the problem of strongly refuting semirandom $k$-LIN$(\mathbb{F})$
instances: systems of $k$-sparse inhomogeneous linear equations over a finite
field $\mathbb{F}$. For the case of $\mathbb{F} = \mathbb{F}_2$, this is the
well-studied problem of refuting semirandom instances of $k$-XOR, where the
works of [GKM22,HKM23] establish a tight trade-off between runtime and clause
density for refutation: for any choice of a parameter $\ell$, they give an
$n^{O(\ell)}$-time algorithm to certify that there is no assignment that can
satisfy more than $\frac{1}{2} + \varepsilon$-fraction of constraints in a
semirandom $k$-XOR instance, provided that the instance has $O(n) \cdot
\left(\frac{n}{\ell}\right)^{k/2 - 1} \log n /\varepsilon^4$ constraints, and
the work of [KMOW17] provides good evidence that this tight up to a
$\mathrm{polylog}(n)$ factor via lower bounds for the Sum-of-Squares hierarchy.
However for larger fields, the only known results for this problem are
established via black-box reductions to the case of $\mathbb{F}_2$, resulting
in an $|{\mathbb{F}}|^{3k}$ gap between the current best upper and lower
bounds.
  In this paper, we give an algorithm for refuting semirandom
$k$-LIN$(\mathbb{F})$ instances with the "correct" dependence on the field size
$|{\mathbb{F}}|$. For any choice of a parameter $\ell$, our algorithm runs in
$(|{\mathbb{F}}|n)^{O(\ell)}$-time and strongly refutes semirandom
$k$-LIN$(\mathbb{F})$ instances with at least $O(n) \cdot
\left(\frac{|{\mathbb{F}^*}| n}{\ell}\right)^{k/2 - 1} \log(n |{\mathbb{F}^*}|)
/\varepsilon^4$ constraints. We give good evidence that this dependence on the
field size $|{\mathbb{F}}|$ is optimal by proving a lower bound for the
Sum-of-Squares hierarchy that matches this threshold up to a
$\mathrm{polylog}(n |{\mathbb{F}^*}|)$ factor. Our results also extend to the
more general case of finite Abelian groups.

</details>


### [244] [Exact Optimization for Minimum Dominating Sets](https://arxiv.org/abs/2508.18256)
*Enqiang Zhu,Qiqi Bao,Yu Zhang,Pu Wu,Chanjuan Liu*

Main category: cs.DS

TL;DR: ParDS是一种用于精确求解最小支配集问题的新分支定界算法，在线性规划下界和动态简化规则方面有创新，在70%的图类别中达到最快求解速度，最大加速比达3411倍


<details>
  <summary>Details</summary>
Motivation: 最小支配集问题是NP难组合优化问题，随着图规模增大，精确求解变得困难，需要开发更高效的精确算法

Method: 基于分支定界框架，采用先进的线性规划技术获得更紧的下界，并提出新颖的简化规则在求解过程中动态简化实例

Result: 在标准基准测试中，70%图类别达到最快求解时间，大稀疏图上表现优异，最大加速3411倍，解决了其他算法在5小时内无法解决的16/43个实例

Conclusion: ParDS成为求解最小支配集问题的最先进精确算法，理论下界质量优于现有算法

Abstract: The Minimum Dominating Set (MDS) problem is a well-established combinatorial
optimization problem with numerous real-world applications. Its NP-hard nature
makes it increasingly difficult to obtain exact solutions as the graph size
grows. This paper introduces ParDS, an exact algorithm developed to address the
MDS problem within the branch-and-bound framework. ParDS features two key
innovations: an advanced linear programming technique that yields tighter lower
bounds and a set of novel reduction rules that dynamically simplify instances
throughout the solving process. Compared to the leading exact algorithms
presented at IJCAI 2023 and 2024, ParDS demonstrates theoretically superior
lower-bound quality. Experimental results on standard benchmark datasets
highlight several significant advantages of ParDS: it achieves fastest solving
times in 70% of graph categories, especially on large, sparse graphs, delivers
a speed-up of up to 3,411 times on the fastest individual instance, and
successfully solves 16 out of 43 instances that other algorithms were unable to
resolve within the 5-hour time limit. These findings establish ParDS as a
state-of-the-art solution for exactly solving the MDS problem.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [245] [Bootstrapping Conditional Retrieval for User-to-Item Recommendations](https://arxiv.org/abs/2508.16793)
*Hongtao Lin,Haoyu Chen,Jaewon Jang,Jiajing Xu*

Main category: cs.IR

TL;DR: 提出一种条件检索方法，在双塔模型基础上引入物品侧信息作为查询条件，提升推荐系统在特定条件下的检索效果


<details>
  <summary>Details</summary>
Motivation: 传统双塔模型在条件检索（如基于主题的推荐）方面存在局限，需要一种能够有效结合条件信息的方法来提升检索相关性

Method: 使用与标准双塔模型相同的训练数据，但在查询中引入物品侧信息作为条件，促进用户特征与条件特征的交互

Result: 实验表明该方法能检索到高度相关的物品，在参与度指标上优于带过滤器的标准双塔模型，在Pinterest部署后带来+0.26%的周活跃用户增长

Conclusion: 所提出的条件检索方法有效提升了推荐系统的条件检索能力，通过特征交互实现了更好的推荐效果，具有实际部署价值

Abstract: User-to-item retrieval has been an active research area in recommendation
system, and two tower models are widely adopted due to model simplicity and
serving efficiency. In this work, we focus on a variant called
\textit{conditional retrieval}, where we expect retrieved items to be relevant
to a condition (e.g. topic). We propose a method that uses the same training
data as standard two tower models but incorporates item-side information as
conditions in query. This allows us to bootstrap new conditional retrieval use
cases and encourages feature interactions between user and condition.
Experiments show that our method can retrieve highly relevant items and
outperforms standard two tower models with filters on engagement metrics. The
proposed model is deployed to power a topic-based notification feed at
Pinterest and led to +0.26\% weekly active users.

</details>


### [246] [Towards a Real-World Aligned Benchmark for Unlearning in Recommender Systems](https://arxiv.org/abs/2508.17076)
*Pierre Lubitzsch,Olga Ovcharenko,Hao Chen,Maarten de Rijke,Sebastian Schelter*

Main category: cs.IR

TL;DR: 本文提出了一个更现实的推荐系统遗忘学习基准框架，针对现有基准的局限性，设计了多任务、领域特定的遗忘场景，并在序列推荐中验证了高效遗忘的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统遗忘学习基准（如CURE4Rec）存在局限性：仅关注协同过滤、忽略会话和购物篮推荐、模拟不现实的大规模遗忘请求、忽视效率约束，无法反映真实世界的操作需求。

Method: 提出一套设计原则和研究问题来指导开发更现实的遗忘学习基准，涵盖多推荐任务、领域特定遗忘场景、多种遗忘算法，并采用反映真实世界删除请求时序性的设置。

Result: 在购物篮推荐场景的初步实验中，发现针对推荐系统定制的遗忘算法显著优于通用算法，遗忘操作延迟仅需几秒钟，证明序列推荐模型也能有效处理大量小规模遗忘请求。

Conclusion: 需要开发更贴近现实场景的推荐系统遗忘学习基准，定制化的遗忘算法在效率和效果上都优于通用方法，为隐私合规要求下的推荐系统提供了实用解决方案。

Abstract: Modern recommender systems heavily leverage user interaction data to deliver
personalized experiences. However, relying on personal data presents challenges
in adhering to privacy regulations, such as the GDPR's "right to be forgotten".
Machine unlearning (MU) aims to address these challenges by enabling the
efficient removal of specific training data from models post-training, without
compromising model utility or leaving residual information. However, current
benchmarks for unlearning in recommender systems -- most notably CURE4Rec --
fail to reflect real-world operational demands. They focus narrowly on
collaborative filtering, overlook tasks like session-based and next-basket
recommendation, simulate unrealistically large unlearning requests, and ignore
critical efficiency constraints. In this paper, we propose a set of design
desiderata and research questions to guide the development of a more realistic
benchmark for unlearning in recommender systems, with the goal of gathering
feedback from the research community. Our benchmark proposal spans multiple
recommendation tasks, includes domain-specific unlearning scenarios, and
several unlearning algorithms -- including ones adapted from a recent NeurIPS
unlearning competition. Furthermore, we argue for an unlearning setup that
reflects the sequential, time-sensitive nature of real-world deletion requests.
We also present a preliminary experiment in a next-basket recommendation
setting based on our proposed desiderata and find that unlearning also works
for sequential recommendation models, exposed to many small unlearning
requests. In this case, we observe that a modification of a custom-designed
unlearning algorithm for recommender systems outperforms general unlearning
algorithms significantly, and that unlearning can be executed with a latency of
only several seconds.

</details>


### [247] [Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation](https://arxiv.org/abs/2508.17079)
*Yejin Choi,Jaewoo Park,Janghan Yoon,Saejin Kim,Jaehyun Jeon,Youngjae Yu*

Main category: cs.IR

TL;DR: PREMIR是一个利用多模态大语言模型生成跨模态预检索问题(preQs)的创新框架，通过多模态预检索问题在token级别扩展匹配范围，在跨域和多语言检索任务中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索器在面对未见领域或语言时表现不佳，且大多数文档是私有的或局限于企业环境中，需要一种能够处理复杂真实世界文档的检索方法

Method: 利用MLLM的广泛知识生成跨模态预检索问题(preQs)，通过多个互补模态的preQs在token级别扩展匹配范围，而不是在单一向量空间中比较嵌入

Result: 在分布外基准测试（包括封闭领域和多语言设置）中实现最先进性能，在所有检索指标上均优于强基线

Conclusion: PREMIR框架通过生成跨模态预检索问题有效提升了多模态检索的鲁棒性和性能，特别是在处理未见领域和多语言场景时表现出色

Abstract: Rapid advances in Multimodal Large Language Models (MLLMs) have expanded
information retrieval beyond purely textual inputs, enabling retrieval from
complex real world documents that combine text and visuals. However, most
documents are private either owned by individuals or confined within corporate
silos and current retrievers struggle when faced with unseen domains or
languages. To address this gap, we introduce PREMIR, a simple yet effective
framework that leverages the broad knowledge of an MLLM to generate cross modal
pre questions (preQs) before retrieval. Unlike earlier multimodal retrievers
that compare embeddings in a single vector space, PREMIR leverages preQs from
multiple complementary modalities to expand the scope of matching to the token
level. Experiments show that PREMIR achieves state of the art performance on
out of distribution benchmarks, including closed domain and multilingual
settings, outperforming strong baselines across all retrieval metrics. We
confirm the contribution of each component through in depth ablation studies,
and qualitative analyses of the generated preQs further highlight the model's
robustness in real world settings.

</details>


### [248] [VQL: An End-to-End Context-Aware Vector Quantization Attention for Ultra-Long User Behavior Modeling](https://arxiv.org/abs/2508.17125)
*Kaiyuan Li,Yongxiang Tang,Yanhua Cheng,Yong Bai,Yanxiang Zeng,Chao Wang,Xialong Liu,Peng Jiang*

Main category: cs.IR

TL;DR: VQL是一个针对超长用户行为序列的向量量化注意力框架，通过关键量化、多尺度量化和高效上下文注入三个创新，在保持高精度的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统中超长用户行为序列建模面临延迟和内存限制，传统方法要么截断序列丢失重要信息，要么过度压缩缺乏上下文感知，无法在低损失压缩、上下文感知和效率之间取得平衡。

Method: 提出VQL框架：1）仅量化注意力键而保持值完整，通过softmax归一化获得与序列长度无关的误差界限；2）多尺度量化将注意力头分组，每组使用小码本减少量化误差；3）高效上下文注入直接集成静态特征和相对位置信息，不增加码本大小。

Result: 在三个大规模数据集（KuaiRand-1K、KuaiRec、TMALL）上的实验表明，VQL始终优于强基线方法，在实现更高精度的同时降低了推理延迟。

Conclusion: VQL在超长序列推荐中建立了准确性和效率平衡的新技术标准，为大规模推荐系统的超长行为序列建模提供了有效的解决方案。

Abstract: In large-scale recommender systems, ultra-long user behavior sequences encode
rich signals of evolving interests. Extending sequence length generally
improves accuracy, but directly modeling such sequences in production is
infeasible due to latency and memory constraints. Existing solutions fall into
two categories: (1) top-k retrieval, which truncates the sequence and may
discard most attention mass when L >> k; and (2) encoder-based compression,
which preserves coverage but often over-compresses and fails to incorporate key
context such as temporal gaps or target-aware signals. Neither class achieves a
good balance of low-loss compression, context awareness, and efficiency.
  We propose VQL, a context-aware Vector Quantization Attention framework for
ultra-long behavior modeling, with three innovations. (1) Key-only
quantization: only attention keys are quantized, while values remain intact; we
prove that softmax normalization yields an error bound independent of sequence
length, and a codebook loss directly supervises quantization quality. This also
enables L-free inference via offline caches. (2) Multi-scale quantization:
attention heads are partitioned into groups, each with its own small codebook,
which reduces quantization error while keeping cache size fixed. (3) Efficient
context injection: static features (e.g., item category, modality) are directly
integrated, and relative position is modeled via a separable temporal kernel.
All context is injected without enlarging the codebook, so cached
representations remain query-independent.
  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show
that VQL consistently outperforms strong baselines, achieving higher accuracy
while reducing inference latency, establishing a new state of the art in
balancing accuracy and efficiency for ultra-long sequence recommendation.

</details>


### [249] [Opening the Black Box: Interpretable Remedies for Popularity Bias in Recommender Systems](https://arxiv.org/abs/2508.17297)
*Parviz Ahmadov,Masoud Mansoury*

Main category: cs.IR

TL;DR: 提出使用稀疏自编码器(SAE)的后处理方法，通过识别和调整编码流行度偏见的神经元来缓解推荐系统中的流行度偏见问题，在保持准确性的同时显著提升公平性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中存在流行度偏见问题，少数热门物品获得过多关注，而大多数非热门物品被忽视，现有缓解方法缺乏透明度。

Method: 使用稀疏自编码器复制预训练模型行为，通过合成用户识别编码流行度信号的神经元，然后调整这些神经元的激活来引导更公平的推荐。

Result: 在两个公开数据集上的实验表明，该方法显著提高了公平性，同时对准确性影响最小，并提供了公平性与准确性权衡的可解释性和细粒度控制。

Conclusion: 该方法有效解决了推荐系统中的流行度偏见问题，在保持推荐质量的同时提升了公平性和透明度。

Abstract: Popularity bias is a well-known challenge in recommender systems, where a
small number of popular items receive disproportionate attention, while the
majority of less popular items are largely overlooked. This imbalance often
results in reduced recommendation quality and unfair exposure of items.
Although existing mitigation techniques address this bias to some extent, they
typically lack transparency in how they operate. In this paper, we propose a
post-hoc method using a Sparse Autoencoder (SAE) to interpret and mitigate
popularity bias in deep recommendation models. The SAE is trained to replicate
a pre-trained model's behavior while enabling neuron-level interpretability. By
introducing synthetic users with clear preferences for either popular or
unpopular items, we identify neurons encoding popularity signals based on their
activation patterns. We then adjust the activations of the most biased neurons
to steer recommendations toward fairer exposure. Experiments on two public
datasets using a sequential recommendation model show that our method
significantly improves fairness with minimal impact on accuracy. Moreover, it
offers interpretability and fine-grained control over the fairness-accuracy
trade-off.

</details>


### [250] [A Universal Framework for Offline Serendipity Evaluation in Recommender Systems via Large Language Models](https://arxiv.org/abs/2508.17571)
*Yu Tokutake,Kazushi Okamoto,Kei Harada,Atsushi Shibata,Koki Karube*

Main category: cs.IR

TL;DR: 使用大语言模型作为评估器，提出了一种普遍适用的推荐系统偶然性评估框架，解决了因缺乏地面真实数据而难以评估偶然性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的偶然性评估指标存在定义模糊、适用性局限的问题，且缺乏地面真实数据，导致评估困难。

Method: 利用大语言模型的知识和推理能力作为评估器，通过四种不同的提示策略评估LLM的偶然性预测准确性，并在三个实际数据集上重新评估各种推荐系统的偶然性表现。

Result: 链式思维提示策略获得最高准确性；没有任何一种偶然性向的推荐系统在所有数据集上都表现最佳，甚至普通推荐系统有时也能获得更高的偶然性表现。

Conclusion: 所提出的框架能够有效解决偶然性评估的挑战，为推荐系统的偶然性表现提供了一种普遍适用的评估方法。

Abstract: Serendipity in recommender systems (RSs) has attracted increasing attention
as a concept that enhances user satisfaction by presenting unexpected and
useful items. However, evaluating serendipitous performance remains challenging
because its ground truth is generally unobservable. The existing offline
metrics often depend on ambiguous definitions or are tailored to specific
datasets and RSs, thereby limiting their generalizability. To address this
issue, we propose a universally applicable evaluation framework that leverages
large language models (LLMs) known for their extensive knowledge and reasoning
capabilities, as evaluators. First, to improve the evaluation performance of
the proposed framework, we assessed the serendipity prediction accuracy of LLMs
using four different prompt strategies on a dataset containing user-annotated
serendipitous ground truth and found that the chain-of-thought prompt achieved
the highest accuracy. Next, we re-evaluated the serendipitous performance of
both serendipity-oriented and general RSs using the proposed framework on three
commonly used real-world datasets, without the ground truth. The results
indicated that there was no serendipity-oriented RS that consistently
outperformed across all datasets, and even a general RS sometimes achieved
higher performance than the serendipity-oriented RS.

</details>


### [251] [Preference Trajectory Modeling via Flow Matching for Sequential Recommendation](https://arxiv.org/abs/2508.17618)
*Li Li,Mingyue Cheng,Yuyang Ye,Zhiding Liu,Enhong Chen*

Main category: cs.IR

TL;DR: FlowRec是一个基于流匹配的序列推荐框架，通过建模用户偏好轨迹来替代传统的扩散模型，解决了扩散模型对条件敏感和推理效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在序列推荐中虽然表现出强大的用户兴趣分布建模能力，但存在两个关键限制：对条件高度敏感导致难以从纯高斯噪声中恢复目标物品，以及推理过程计算成本高限制实际部署。

Method: 提出FlowRec框架，利用流匹配技术显式建模从当前状态到未来兴趣的用户偏好轨迹。构建个性化行为先验分布替代高斯噪声，学习向量场建模用户偏好轨迹，并设计包含正负样本的单步对齐损失来提升采样效率和生成质量。

Result: 在四个基准数据集上的大量实验验证了FlowRec相对于最先进基线的优越性。

Conclusion: FlowRec通过流匹配技术有效解决了扩散模型在序列推荐中的局限性，提供了更灵活的初始分布和更高效的采样，在推荐性能上取得了显著提升。

Abstract: Sequential recommendation predicts each user's next item based on their
historical interaction sequence. Recently, diffusion models have attracted
significant attention in this area due to their strong ability to model user
interest distributions. They typically generate target items by denoising
Gaussian noise conditioned on historical interactions. However, these models
face two critical limitations. First, they exhibit high sensitivity to the
condition, making it difficult to recover target items from pure Gaussian
noise. Second, the inference process is computationally expensive, limiting
practical deployment. To address these issues, we propose FlowRec, a simple yet
effective sequential recommendation framework which leverages flow matching to
explicitly model user preference trajectories from current states to future
interests. Flow matching is an emerging generative paradigm, which offers
greater flexibility in initial distributions and enables more efficient
sampling. Based on this, we construct a personalized behavior-based prior
distribution to replace Gaussian noise and learn a vector field to model user
preference trajectories. To better align flow matching with the recommendation
objective, we further design a single-step alignment loss incorporating both
positive and negative samples, improving sampling efficiency and generation
quality. Extensive experiments on four benchmark datasets verify the
superiority of FlowRec over the state-of-the-art baselines.

</details>


### [252] [Demographically-Inspired Query Variants Using an LLM](https://arxiv.org/abs/2508.17644)
*Marwah Alaofi,Nicola Ferro,Paul Thomas,Falk Scholer,Mark Sanderson*

Main category: cs.IR

TL;DR: 使用大型语言模型生成查询变体来多样化测试集合，反映不同用户特征对搜索系统评价的影响


<details>
  <summary>Details</summary>
Motivation: 现有测试集合缺乏用户多样性，需要反映不同用户特征（如语言能力和领域熟练度）对查询表述的影响，以实现更理想的测试集合

Method: 利用大型语言模型（LLM）生成具有相同语义但反映不同用户特征的查询变体，这些变体代表不同的用户画像

Result: 验证了LLM生成查询变体的有效性，发现变体影响系统排名，不同用户画像体验到显著不同的系统效果

Conclusion: 该方法为系统评价提供了新视角，能够观察用户画像对系统排名的影响以及系统性能在不同用户间的差异

Abstract: This study proposes a method to diversify queries in existing test
collections to reflect some of the diversity of search engine users, aligning
with an earlier vision of an 'ideal' test collection. A Large Language Model
(LLM) is used to create query variants: alternative queries that have the same
meaning as the original. These variants represent user profiles characterised
by different properties, such as language and domain proficiency, which are
known in the IR literature to influence query formulation.
  The LLM's ability to generate query variants that align with user profiles is
empirically validated, and the variants' utility is further explored for IR
system evaluation. Results demonstrate that the variants impact how systems are
ranked and show that user profiles experience significantly different levels of
system effectiveness. This method enables an alternative perspective on system
evaluation where we can observe both the impact of user profiles on system
rankings and how system performance varies across users.

</details>


### [253] [Semantic Search for Information Retrieval](https://arxiv.org/abs/2508.17694)
*Kayla Farivar*

Main category: cs.IR

TL;DR: 本综述论文概述了信息检索系统从传统词法方法(BM25/TF-IDF)到现代语义检索器的发展历程，重点介绍了BERT、DPR、ColBERT、SPLADE和MonoT5等先进模型架构。


<details>
  <summary>Details</summary>
Motivation: 随着信息检索技术的快速发展，从传统的词法匹配方法演进到现代的语义理解模型，需要对这些先进技术进行系统性的梳理和总结，以指导未来研究方向。

Method: 采用文献综述方法，首先介绍BM25基线模型，然后详细分析密集双编码器(DPR)、延迟交互模型(ColBERT)、神经稀疏检索(SPLADE)和交叉编码器(MonoT5)等现代语义检索架构。

Result: 系统梳理了现代语义检索技术的发展脉络和各类模型的架构特点，为研究人员提供了全面的技术概览。

Conclusion: 论文最后讨论了常用的评估策略、当前面临的挑战以及未来发展方向，为信息检索领域的进一步研究提供了重要参考。

Abstract: Information retrieval systems have progressed notably from lexical techniques
such as BM25 and TF-IDF to modern semantic retrievers. This survey provides a
brief overview of the BM25 baseline, then discusses the architecture of modern
state-of-the-art semantic retrievers. Advancing from BERT, we introduce dense
bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse
retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We
conclude with common evaluation tactics, pressing challenges, and propositions
for future directions.

</details>


### [254] [How Do LLM-Generated Texts Impact Term-Based Retrieval Models?](https://arxiv.org/abs/2508.17715)
*Wei Huang,Keping Bi,Yinqiong Cai,Wei Chen,Jiafeng Guo,Xueqi Cheng*

Main category: cs.IR

TL;DR: 本文研究了LLM生成内容对基于术语的检索模型的影响，发现这些模型优先选择与查询术语分布最匹配的文档，而非固有偏向特定来源。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成内容在互联网中激增，信息检索系统需要区分和处理人类撰写与机器生成文本的混合内容。现有研究表明神经检索器可能偏向LLM生成内容，而经典术语检索器如BM25倾向于人类撰写文档。

Method: 通过语言分析比较LLM生成文本和人类撰写文本的特征差异，包括Zipf斜率、术语特异性和文档多样性，并探究术语检索模型是否存在来源偏见。

Result: LLM生成文本表现出更平滑的高频和更陡峭的低频Zipf斜率、更高的术语特异性和更大的文档级多样性。术语检索模型优先选择术语分布与查询最匹配的文档，而非显示固有来源偏见。

Conclusion: 术语检索模型基于术语分布匹配度而非来源偏见进行文档选择，这为理解和解决混合来源内容检索中的潜在偏见提供了基础。

Abstract: As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

</details>


### [255] [DiffusionGS: Generative Search with Query Conditioned Diffusion in Kuaishou](https://arxiv.org/abs/2508.17754)
*Qinyao Li,Xiaoyang Zheng,Qihang Zhao,Ke Xu,Zhongbo Sun,Chao Wang,Chenyi Lei,Han Li,Wenwu Ou*

Main category: cs.IR

TL;DR: DiffusionGS是一个基于生成模型的个性化搜索排序系统，通过将用户查询作为意图锚点，使用条件扩散过程从用户历史行为中提取即时兴趣表示


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于过滤后的历史行为估计用户广泛兴趣，但未能充分利用用户实时意图（查询）与过去行为的显式对齐关系

Method: 将兴趣提取建模为条件去噪任务，用户查询指导条件扩散过程从行为序列生成鲁棒的用户意图感知表示；提出用户感知去噪层（UDL）将用户特定配置文件整合到注意力分布优化中

Result: 离线和在线实验表明DiffusionGS优于现有最先进方法

Conclusion: 通过将查询重构为意图先验并利用基于扩散的去噪，该方法为捕捉动态用户兴趣变化提供了强大机制

Abstract: Personalized search ranking systems are critical for driving engagement and
revenue in modern e-commerce and short-video platforms. While existing methods
excel at estimating users' broad interests based on the filtered historical
behaviors, they typically under-exploit explicit alignment between a user's
real-time intent (represented by the user query) and their past actions. In
this paper, we propose DiffusionGS, a novel and scalable approach powered by
generative models. Our key insight is that user queries can serve as explicit
intent anchors to facilitate the extraction of users' immediate interests from
long-term, noisy historical behaviors. Specifically, we formulate interest
extraction as a conditional denoising task, where the user's query guides a
conditional diffusion process to produce a robust, user intent-aware
representation from their behavioral sequence. We propose the User-aware
Denoising Layer (UDL) to incorporate user-specific profiles into the
optimization of attention distribution on the user's past actions. By reframing
queries as intent priors and leveraging diffusion-based denoising, our method
provides a powerful mechanism for capturing dynamic user interest shifts.
Extensive offline and online experiments demonstrate the superiority of
DiffusionGS over state-of-the-art methods.

</details>


### [256] [Research on Evaluation Methods for Patent Novelty Search Systems and Empirical Analysis](https://arxiv.org/abs/2508.17782)
*Shu Zhang,LiSha Zhang,Kai Duan,XinKai Sun*

Main category: cs.IR

TL;DR: 提出了一种专利新颖性搜索系统的综合评估方法，通过构建高质量可复现数据集，使用Top-k检测率和召回率作为核心指标，进行多维度分析以暴露系统性能差异。


<details>
  <summary>Details</summary>
Motivation: 专利新颖性搜索系统对知识产权保护和创新评估至关重要，其检索准确性直接影响专利质量，需要有效的评估方法来改进系统性能。

Method: 从审查员引用和技术一致的同族专利中提取X型引用来构建高质量可复现数据集，使用发明描述作为输入，采用Top-k检测率和召回率作为核心评估指标。

Result: 实验表明该方法能有效暴露不同场景下的性能差异，为系统改进提供可操作的证据。

Conclusion: 该框架具有可扩展性和实用性，为专利新颖性搜索系统的开发和优化提供了有用的参考。

Abstract: Patent novelty search systems are critical to IP protection and innovation
assessment; their retrieval accuracy directly impacts patent quality. We
propose a comprehensive evaluation methodology that builds high-quality,
reproducible datasets from examiner citations and X-type citations extracted
from technically consistent family patents, and evaluates systems using
invention descriptions as inputs. Using Top-k Detection Rate and Recall as core
metrics, we further conduct multi-dimensional analyses by language, technical
field (IPC), and filing jurisdiction. Experiments show the method effectively
exposes performance differences across scenarios and offers actionable evidence
for system improvement. The framework is scalable and practical, providing a
useful reference for development and optimization of patent novelty search
systems

</details>


### [257] [LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation](https://arxiv.org/abs/2508.17858)
*Shaoxiong Zhan,Hai Lin,Hongming Tan,Xiaodong Cai,Hai-Tao Zheng,Xin Su,Zifei Shan,Ruitong Liu,Hong-Gee Kim*

Main category: cs.IR

TL;DR: LexSemBridge是一个统一的检索增强框架，通过细粒度向量调制增强密集检索模型在关键词和段落级检索任务中的性能，无需修改主干编码器。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索模型在处理需要精确关键词对齐和跨度级定位的细粒度检索任务时表现不佳，即使在高词汇重叠的情况下也存在困难。

Method: 提出LexSemBridge框架，通过统计(SLR)、学习(LLR)和上下文(CLR)三种范式从输入token构建潜在增强向量，并通过元素级交互与密集嵌入集成。

Result: 在语义和细粒度检索任务上的广泛实验验证了该方法的有效性和通用性。

Conclusion: LexSemBridge作为一种即插即用的解决方案，能够显著提升密集检索模型在细粒度场景下的性能，同时保持语义方向并选择性放大判别性维度。

Abstract: As queries in retrieval-augmented generation (RAG) pipelines powered by large
language models (LLMs) become increasingly complex and diverse, dense retrieval
models have demonstrated strong performance in semantic matching. Nevertheless,
they often struggle with fine-grained retrieval tasks, where precise keyword
alignment and span-level localization are required, even in cases with high
lexical overlap that would intuitively suggest easier retrieval. To
systematically evaluate this limitation, we introduce two targeted tasks,
keyword retrieval and part-of-passage retrieval, designed to simulate practical
fine-grained scenarios. Motivated by these observations, we propose
LexSemBridge, a unified framework that enhances dense query representations
through fine-grained, input-aware vector modulation. LexSemBridge constructs
latent enhancement vectors from input tokens using three paradigms: Statistical
(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense
embeddings via element-wise interaction. Theoretically, we show that this
modulation preserves the semantic direction while selectively amplifying
discriminative dimensions. LexSemBridge operates as a plug-in without modifying
the backbone encoder and naturally extends to both text and vision modalities.
Extensive experiments across semantic and fine-grained retrieval tasks validate
the effectiveness and generality of our approach. All code and models are
publicly available at https://github.com/Jasaxion/LexSemBridge/

</details>


### [258] [Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method](https://arxiv.org/abs/2508.17862)
*Leqian Li,Dianxi Shi,Jialu Zhou,Xinyu Wei,Mingyue Yang,Songchang Jin,Shaowu Yang*

Main category: cs.IR

TL;DR: RFM-RAG是一种改进的检索增强生成方法，通过动态证据池和检索反馈机制解决传统RAG的信息丢失和冗余检索问题，在多个QA基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法存在信息丢失、多轮查询中的冗余检索问题，以及难以精确描述复杂任务中的知识缺口。LLMs虽然强大但受限于参数知识和重新训练成本。

Method: 提出RFM-RAG方法：1）构建动态证据池实现状态化连续知识管理；2）使用关系三元组生成精炼查询描述知识缺口；3）检索关键外部知识迭代更新证据池；4）使用R-Feedback模型评估证据完整性直至收敛。

Result: 在三个公开QA基准测试中，RFM-RAG超越了传统RAG方法，提高了系统整体准确率。

Conclusion: RFM-RAG通过状态化检索和动态证据池管理有效解决了传统RAG的局限性，为检索增强生成提供了更高效的解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
diverse tasks, yet they face inherent limitations such as constrained
parametric knowledge and high retraining costs. Retrieval-Augmented Generation
(RAG) augments the generation process by retrieving externally stored knowledge
absent from the models internal parameters. However, RAG methods face
challenges such as information loss and redundant retrievals during multi-round
queries, accompanying the difficulties in precisely characterizing knowledge
gaps for complex tasks. To address these problems, we propose Retrieval
Feedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms
the stateless retrieval of previous methods into stateful continuous knowledge
management by constructing a dynamic evidence pool. Specifically, our method
generates refined queries describing the models knowledge gaps using relational
triples from questions and evidence from the dynamic evidence pool; Retrieves
critical external knowledge to iteratively update this evidence pool; Employs a
R-Feedback Model to evaluate evidence completeness until convergence. Compared
to traditional RAG methods, our approach enables persistent storage of
retrieved passages and effectively distills key information from passages to
construct clearly new queries. Experiments on three public QA benchmarks
demonstrate that RFM-RAG outperforms previous methods and improves overall
system accuracy.

</details>


### [259] [HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data](https://arxiv.org/abs/2508.18048)
*Jiyoon Myung,Jihyeon Park,Joohyung Han*

Main category: cs.IR

TL;DR: HyST是一个混合检索框架，结合LLM驱动的结构化过滤和语义嵌入搜索，用于处理半结构化表格数据上的复杂查询


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中的用户查询往往同时包含结构化约束（如类别、属性）和非结构化偏好（如产品描述或评论），需要一种能够同时处理这两种信息需求的解决方案

Method: 使用大型语言模型从自然语言中提取属性级约束作为元数据过滤器，同时通过基于嵌入的检索处理剩余的非结构化查询组件

Result: 在半结构化基准测试中，HyST始终优于传统基线方法，证明了结构化过滤在提高检索精度方面的重要性

Conclusion: HyST为现实世界用户查询提供了一个可扩展且准确的解决方案，通过结合结构化过滤和语义搜索来有效处理混合信息需求

Abstract: User queries in real-world recommendation systems often combine structured
constraints (e.g., category, attributes) with unstructured preferences (e.g.,
product descriptions or reviews). We introduce HyST (Hybrid retrieval over
Semi-structured Tabular data), a hybrid retrieval framework that combines
LLM-powered structured filtering with semantic embedding search to support
complex information needs over semi-structured tabular data. HyST extracts
attribute-level constraints from natural language using large language models
(LLMs) and applies them as metadata filters, while processing the remaining
unstructured query components via embedding-based retrieval. Experiments on a
semi-structured benchmark show that HyST consistently outperforms tradtional
baselines, highlighting the importance of structured filtering in improving
retrieval precision, offering a scalable and accurate solution for real-world
user queries.

</details>


### [260] [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)
*Junyi Chen,Lu Chi,Siliang Xu,Shiwei Ran,Bingyue Peng,Zehuan Yuan*

Main category: cs.IR

TL;DR: 提出了HLLM-Creator分层LLM框架，通过用户聚类和匹配预测剪枝策略实现高效个性化内容生成，解决了AIGC系统用户个性化不足的问题


<details>
  <summary>Details</summary>
Motivation: 当前AIGC系统过度依赖创作者灵感，难以生成真正用户个性化的内容。在线广告等应用中，不同用户关注不同产品卖点，个性化创意生成具有重要价值

Method: 采用分层LLM框架，结合用户聚类和用户-广告匹配预测剪枝策略提高生成效率；设计基于思维链推理的数据构建管道，确保事实一致性

Result: 在抖音搜索广告个性化标题生成实验中表现有效，在线A/B测试显示广告转化率提升0.476%

Conclusion: HLLM-Creator为工业场景提供了有效且高效的个性化生成解决方案，解决了数据稀缺和计算效率的双重挑战

Abstract: AI-generated content technologies are widely used in content creation.
However, current AIGC systems rely heavily on creators' inspiration, rarely
generating truly user-personalized content. In real-world applications such as
online advertising, a single product may have multiple selling points, with
different users focusing on different features. This underscores the
significant value of personalized, user-centric creative generation. Effective
personalized content generation faces two main challenges: (1) accurately
modeling user interests and integrating them into the content generation
process while adhering to factual constraints, and (2) ensuring high efficiency
and scalability to handle the massive user base in industrial scenarios.
Additionally, the scarcity of personalized creative data in practice
complicates model training, making data construction another key hurdle. We
propose HLLM-Creator, a hierarchical LLM framework for efficient user interest
modeling and personalized content generation. During inference, a combination
of user clustering and a user-ad-matching-prediction based pruning strategy is
employed to significantly enhance generation efficiency and reduce
computational overhead, making the approach suitable for large-scale
deployment. Moreover, we design a data construction pipeline based on
chain-of-thought reasoning, which generates high-quality, user-specific
creative titles and ensures factual consistency despite limited personalized
data. This pipeline serves as a critical foundation for the effectiveness of
our model. Extensive experiments on personalized title generation for Douyin
Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a
0.476% increase on Adss, paving the way for more effective and efficient
personalized generation in industrial scenarios. Codes for academic dataset are
available at https://github.com/bytedance/HLLM.

</details>


### [261] [Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations](https://arxiv.org/abs/2508.18132)
*Hung-Chun Hsu,Yuan-Ching Kuo,Chao-Han Huck Yang,Szu-Wei Fu,Hanrong Ye,Hongxu Yin,Yu-Chiang Frank Wang,Ming-Feng Tsai,Chuan-Ju Wang*

Main category: cs.IR

TL;DR: 该论文提出了一种将测试时缩放技术应用于多轮对话多模态产品检索的新框架，通过测试时重排序机制提升检索准确性，在多个基准测试中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统产品检索系统难以处理复杂的多轮用户交互，现有的多模态生成检索方法主要针对单轮场景，在多轮对话中难以建模用户意图的演变过程。测试时缩放技术虽然能提升大语言模型性能，但在对话产品搜索中面临查询模糊、意图演变等挑战。

Method: 提出基于生成式检索器的新框架，引入测试时重排序（TTR）机制，通过迭代推理时优化来改进检索准确性，更好地对齐多轮对话中演变的用户意图。

Result: 在多个基准测试中取得一致改进，平均MRR提升14.5个百分点，nDCG@1提升10.6个百分点。

Conclusion: 测试时缩放技术能有效提升多轮对话多模态产品检索性能，TTR机制成功解决了用户意图演变和查询模糊性带来的挑战，为对话式产品搜索提供了新的解决方案。

Abstract: The rapid evolution of e-commerce has exposed the limitations of traditional
product retrieval systems in managing complex, multi-turn user interactions.
Recent advances in multimodal generative retrieval -- particularly those
leveraging multimodal large language models (MLLMs) as retrievers -- have shown
promise. However, most existing methods are tailored to single-turn scenarios
and struggle to model the evolving intent and iterative nature of multi-turn
dialogues when applied naively. Concurrently, test-time scaling has emerged as
a powerful paradigm for improving large language model (LLM) performance
through iterative inference-time refinement. Yet, its effectiveness typically
relies on two conditions: (1) a well-defined problem space (e.g., mathematical
reasoning), and (2) the model's ability to self-correct -- conditions that are
rarely met in conversational product search. In this setting, user queries are
often ambiguous and evolving, and MLLMs alone have difficulty grounding
responses in a fixed product corpus. Motivated by these challenges, we propose
a novel framework that introduces test-time scaling into conversational
multimodal product retrieval. Our approach builds on a generative retriever,
further augmented with a test-time reranking (TTR) mechanism that improves
retrieval accuracy and better aligns results with evolving user intent
throughout the dialogue. Experiments across multiple benchmarks show consistent
improvements, with average gains of 14.5 points in MRR and 10.6 points in
nDCG@1.

</details>


### [262] [PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation](https://arxiv.org/abs/2508.18166)
*Bin Tan,Wangyao Ge,Yidi Wang,Xin Liu,Jeff Burtoft,Hao Fan,Hui Wang*

Main category: cs.IR

TL;DR: PCR-CA是一个端到端的CTR预测框架，通过并行码本表示和对比对齐技术，有效处理多类别应用的语义重叠问题，在微软应用商店实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统应用商店推荐系统在处理多类别应用时存在语义重叠问题，导致个性化推荐效果不佳，需要新的方法来更好地建模多类别语义。

Method: 提出PCR-CA框架：1)提取多模态嵌入；2)并行码本VQ-AE模块学习离散语义表示；3)对比对齐损失桥接语义和协同信号；4)双重注意力融合机制结合ID和语义特征。

Result: 大规模数据集实验显示AUC提升0.76%，长尾应用AUC提升2.15%。在线A/B测试显示CTR提升10.52%，CVR提升16.30%，已在微软应用商店全面部署。

Conclusion: PCR-CA通过并行码本表示和对比对齐技术，有效解决了多类别应用的语义建模问题，显著提升了推荐性能，特别是在长尾项目上表现优异。

Abstract: Modern app store recommender systems struggle with multiple-category apps, as
traditional taxonomies fail to capture overlapping semantics, leading to
suboptimal personalization. We propose PCR-CA (Parallel Codebook
Representations with Contrastive Alignment), an end-to-end framework for
improved CTR prediction. PCR-CA first extracts compact multimodal embeddings
from app text, then introduces a Parallel Codebook VQ-AE module that learns
discrete semantic representations across multiple codebooks in parallel --
unlike hierarchical residual quantization (RQ-VAE). This design enables
independent encoding of diverse aspects (e.g., gameplay, art style), better
modeling multiple-category semantics. To bridge semantic and collaborative
signals, we employ a contrastive alignment loss at both the user and item
levels, enhancing representation learning for long-tail items. Additionally, a
dual-attention fusion mechanism combines ID-based and semantic features to
capture user interests, especially for long-tail apps. Experiments on a
large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong
baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further
validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement
in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new
framework has now been fully deployed on the Microsoft Store.

</details>
