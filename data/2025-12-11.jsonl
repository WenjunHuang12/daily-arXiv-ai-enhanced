{"id": "2512.09104", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09104", "abs": "https://arxiv.org/abs/2512.09104", "authors": ["Mohammad Javad Ahmadi", "Rafael F. Schaefer", "H. Vincent Poor"], "title": "SURA: Secure Unsourced Random Access", "comment": null, "summary": "This work introduces security for unsourced random access (URA) by employing wiretap-inspired physical layer techniques. To achieve confidentiality, the proposed system opportunistically exploits intrinsic features of feedback-aided URA without adding any overhead or altering its original structure or operational characteristics. As a result, the proposed system preserves the low-cost advantages of URA, including low delay and minimal signaling overhead, while providing secure communication. To secure transmission, each user generates a secret key and an artificial noise sequence from the feedback signal that the BS broadcasts in previous transmission rounds. This feedback depends on the BS-user channel, making it a private signal for each user. The secure transmission is performed by three actions: encrypting the data using the secret key, sending only the parity bits of the LDPC encoded secret key to allow the legitimate receiver to recover it, and masking these parity bits with the artificial noise. For reception, a receiver algorithm is designed for the legitimate user, and a leakage analysis is provided to quantify the information available to the eavesdropper. The simulation results show that meaningful secrecy is achieved in URA without modifying its structure and with negligible impact on standard performance."}
{"id": "2512.09457", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09457", "abs": "https://arxiv.org/abs/2512.09457", "authors": ["Hao Chen", "Xu Pan", "Conghui Xie"], "title": "$t$-Fold $s$-Blocking Sets and $s$-Minimal Codes", "comment": null, "summary": "Blocking sets and minimal codes have been studied for many years in projective geometry and coding theory. In this paper, we provide a new lower bound on the size of $t$-fold $s$-blocking sets without the condition $t \\leq q$, which is stronger than the classical result of Beutelspacher in 1983. Then a lower bound on lengths of projective $s$-minimal codes is also obtained. It is proved that $(s+1)$-minimal codes are certainly $s$-minimal codes. We generalize the Ashikhmin-Barg condition for minimal codes to $s$-minimal codes. Many infinite families of $s$-minimal codes satisfying and violating this generalized Ashikhmin-Barg condition are constructed. We also give several examples which are binary minimal codes, but not $2$-minimal codes."}
{"id": "2512.09655", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09655", "abs": "https://arxiv.org/abs/2512.09655", "authors": ["Tuvi Etzion"], "title": "Binary and Non-Binary Self-Dual Sequences and Maximum Period Single-Track Gray Codes", "comment": null, "summary": "Binary self-dual sequences have been considered and analyzed throughout the years, and they were used for various applications. Motivated by a construction for single-track Gray codes, we examine the structure and recursive constructions for binary and non-binary self-dual sequences. The feedback shift registers that generate such sequences are discussed. The connections between these sequences and maximum period single-track codes are discussed. Maximum period non-binary single-track Gray codes of length $p^t$ and period $p^{p^t}$ are constructed. These are the first infinite families of maximum period codes presented in the literature."}
{"id": "2512.09858", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09858", "abs": "https://arxiv.org/abs/2512.09858", "authors": ["Ali Khalesi", "Mohammad Reza Deylam Salehi"], "title": "Typical Solutions of Multi-User Linearly-Decomposable Distributed Computing", "comment": "This paper has been accepted for publication in IEEE Networking Letters", "summary": "We solve, in the typical-case sense, the multi-sender linearly-decomposable distributed computing problem introduced by tessellated distributed computing. We model real-valued encoders/decoders and demand matrices, and assess structural fidelity via a thresholded graph edit distance between the demand support and the two-hop support of the computed product. Our analysis yields: a closed-form second-moment (Frobenius) risk under spike-and-slab ensembles; deterministic links between thresholded GED and norm error; a Gaussian surrogate with sub-exponential tails that exposes explicit recall lines; concentration of GED and operator-norm control; and a compute-capped design with a visible knee. We map the rules to aeronautical and satellite networks."}
{"id": "2512.09254", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09254", "abs": "https://arxiv.org/abs/2512.09254", "authors": ["Manuel S. Ríos", "Ruben F. Manrique", "Nicanor Quijano", "Luis F. Giraldo"], "title": "The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games", "comment": "9 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly being deployed as autonomous agents on behalf of institutions and individuals in economic, political, and social settings that involve negotiation. Yet this trend carries significant risks if their strategic behavior is not well understood. In this work, we revisit the NegotiationArena framework and run controlled simulation experiments on a diverse set of frontier LLMs across three multi turn bargaining games: Buyer Seller, Multi turn Ultimatum, and Resource Exchange. We ask whether improved general reasoning capabilities lead to rational, unbiased, and convergent negotiation strategies. Our results challenge this assumption. We find that models diverge into distinct, model specific strategic equilibria rather than converging to a unified optimal behavior. Moreover, strong numerical and semantic anchoring effects persist: initial offers are highly predictive of final agreements, and models consistently generate biased proposals by collapsing diverse internal valuations into rigid, generic price points. More concerningly, we observe dominance patterns in which some models systematically achieve higher payoffs than their counterparts. These findings underscore an urgent need to develop mechanisms to mitigate these issues before deploying such systems in real-world scenarios."}
{"id": "2512.09622", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09622", "abs": "https://arxiv.org/abs/2512.09622", "authors": ["Xiao Yan", "Tiezheng Nie", "Boyang Fang", "Derong Shen", "Kou Yue", "Yu Ge"], "title": "CUBE: A Cardinality Estimator Based on Neural CDF", "comment": "13 pages", "summary": "Modern database optimizer relies on cardinality estimator, whose accuracy directly affects the optimizer's ability to choose an optimal execution plan. Recent work on data-driven methods has leveraged probabilistic models to achieve higher estimation accuracy, but these approaches cannot guarantee low inference latency at the same time and neglect scalability. As data dimensionality grows, optimization time can even exceed actual query execution time. Furthermore, inference with probabilistic models by sampling or integration procedures unpredictable estimation result and violate stability, which brings unstable performance with query execution and make database tuning hard for database users. In this paper, we propose a novel approach to cardinality estimation based on cumulative distribution function(CDF), which calculates range query cardinality without sampling or integration, ensuring accurate and predictable estimation results. With inference acceleration by merging calculations, we can achieve fast and nearly constant inference speed while maintaining high accuracy, even as dimensionality increases, which is over 10x faster than current state-of-the-art data-driven cardinality estimator. This demonstrates its excellent dimensional scalability, making it well-suited for real-world database applications."}
{"id": "2512.09080", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.09080", "abs": "https://arxiv.org/abs/2512.09080", "authors": ["Ron Mosenzon"], "title": "Almost-Optimal Approximation Algorithms for Global Minimum Cut in Directed Graphs", "comment": "40 pages. Submitted to STOC 2026", "summary": "We develop new $(1+ε)$-approximation algorithms for finding the global minimum edge-cut in a directed edge-weighted graph, and for finding the global minimum vertex-cut in a directed vertex-weighted graph. Our algorithms are randomized, and have a running time of $O\\left(m^{1+o(1)}/ε\\right)$ on any $m$-edge $n$-vertex input graph, assuming all edge/vertex weights are polynomially-bounded. In particular, for any constant $ε>0$, our algorithms have an almost-optimal running time of $O\\left(m^{1+o(1)}\\right)$. The fastest previously-known running time for this setting, due to (Cen et al., FOCS 2021), is $\\tilde{O}\\left(\\min\\left\\{n^2/ε^2,m^{1+o(1)}\\sqrt{n}\\right\\}\\right)$ for Minimum Edge-Cut, and $\\tilde{O}\\left(n^2/ε^2\\right)$ for Minimum Vertex-Cut. Our results further extend to the rooted variants of the Minimum Edge-Cut and Minimum Vertex-Cut problems, where the algorithm is additionally given a root vertex $r$, and the goal is to find a minimum-weight cut separating any vertex from the root $r$. In terms of techniques, we build upon and extend a framework that was recently introduced by (Chuzhoy et al., SODA 2026) for solving the Minimum Vertex-Cut problem in unweighted directed graphs. Additionally, in order to obtain our result for the Global Minimum Vertex-Cut problem, we develop a novel black-box reduction from this problem to its rooted variant. Prior to our work, such reductions were only known for more restricted settings, such as when all vertex-weights are unit."}
{"id": "2512.09200", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09200", "abs": "https://arxiv.org/abs/2512.09200", "authors": ["Liang Luo", "Yuxin Chen", "Zhengyu Zhang", "Mengyue Hang", "Andrew Gu", "Buyun Zhang", "Boyang Liu", "Chen Chen", "Chengze Fan", "Dong Liang", "Fan Yang", "Feifan Gu", "Huayu Li", "Jade Nie", "Jiayi Xu", "Jiyan Yang", "Jongsoo Park", "Laming Chen", "Longhao Jin", "Qianru Li", "Qin Huang", "Shali Jiang", "Shiwen Shen", "Shuaiwen Wang", "Sihan Zeng", "Siyang Yuan", "Tongyi Tang", "Weilin Zhang", "Wenjun Wang", "Xi Liu", "Xiaohan Wei", "Xiaozhen Xia", "Yuchen Hao", "Yunlong He", "Yasmine Badr", "Zeliang Chen", "Maxim Naumov", "Yantao Yao", "Wenlin Chen", "Santanu Kolay", "GP Musumeci", "Ellie Dingqiao Wen"], "title": "Meta Lattice: Model Space Redesign for Cost-Effective Industry-Scale Ads Recommendations", "comment": null, "summary": "The rapidly evolving landscape of products, surfaces, policies, and regulations poses significant challenges for deploying state-of-the-art recommendation models at industry scale, primarily due to data fragmentation across domains and escalating infrastructure costs that hinder sustained quality improvements.\n  To address this challenge, we propose Lattice, a recommendation framework centered around model space redesign that extends Multi-Domain, Multi-Objective (MDMO) learning beyond models and learning objectives. Lattice addresses these challenges through a comprehensive model space redesign that combines cross-domain knowledge sharing, data consolidation, model unification, distillation, and system optimizations to achieve significant improvements in both quality and cost-efficiency.\n  Our deployment of Lattice at Meta has resulted in 10% revenue-driving top-line metrics gain, 11.5% user satisfaction improvement, 6% boost in conversion rate, with 20% capacity saving."}
{"id": "2512.09367", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.09367", "abs": "https://arxiv.org/abs/2512.09367", "authors": ["Eric Balkanski", "Nicholas DeFilippis", "Vasilis Gkatzelis", "Xizhi Tan"], "title": "Procurement Auctions with Predictions: Improved Frugality for Facility Location", "comment": "In proceeding at NeurIPS 2025", "summary": "We study the problem of designing procurement auctions for the strategic uncapacitated facility location problem: a company needs to procure a set of facility locations in order to serve its customers and each facility location is owned by a strategic agent. Each owner has a private cost for providing access to their facility (e.g., renting it or selling it to the company) and needs to be compensated accordingly. The goal is to design truthful auctions that decide which facilities the company should procure and how much to pay the corresponding owners, aiming to minimize the total cost, i.e., the monetary cost paid to the owners and the connection cost suffered by the customers (their distance to the nearest facility). We evaluate the performance of these auctions using the \\emph{frugality ratio}.\n  We first analyze the performance of the classic VCG auction in this context and prove that its frugality ratio is exactly $3$. We then leverage the learning-augmented framework and design auctions that are augmented with predictions regarding the owners' private costs. Specifically, we propose a family of learning-augmented auctions that achieve significant payment reductions when the predictions are accurate, leading to much better frugality ratios. At the same time, we demonstrate that these auctions remain robust even if the predictions are arbitrarily inaccurate, and maintain reasonable frugality ratios even under adversarially chosen predictions. We finally provide a family of ``error-tolerant'' auctions that maintain improved frugality ratios even if the predictions are only approximately accurate, and we provide upper bounds on their frugality ratio as a function of the prediction error."}
{"id": "2512.09695", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09695", "abs": "https://arxiv.org/abs/2512.09695", "authors": ["Hyunjoon Kim", "Chaerim Lim", "Hyeonjun An", "Rathijit Sen", "Kwanghyun Park"], "title": "Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries", "comment": null, "summary": "Vector similarity search is becoming increasingly important for data science pipelines, particularly in Retrieval-Augmented Generation (RAG), where it enhances large language model inference by enabling efficient retrieval of relevant external knowledge. As RAG expands with table-augmented generation to incorporate structured data, workloads integrating table and vector search are becoming more prevalent. However, efficiently executing such queries remains challenging due to inaccurate cardinality estimation for vector search components, leading to suboptimal query plans. In this paper, we propose Exqutor, an extended query optimizer for vector-augmented analytical queries. Exqutor is a pluggable cardinality estimation framework designed to address this issue, leveraging exact cardinality query optimization techniques to enhance estimation accuracy when vector indexes (e.g., HNSW, IVF) are available. In scenarios lacking these indexes, we employ a sampling-based approach with adaptive sampling size adjustment, dynamically tuning the sample size to balance estimation accuracy and sampling overhead. This allows Exqutor to efficiently approximate vector search cardinalities while minimizing computational costs. We integrate our framework into pgvector, VBASE, and DuckDB, demonstrating performance improvements of up to four orders of magnitude on vector-augmented analytical queries."}
{"id": "2512.09218", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.09218", "abs": "https://arxiv.org/abs/2512.09218", "authors": ["Mohsen Ghaffari", "Jaehyun Koo"], "title": "Dynamic Graph Coloring: Sequential, Parallel, and Distributed", "comment": null, "summary": "We present a simple randomized algorithm that can efficiently maintain a $(Δ+1)$ coloring as the graph undergoes edge insertion and deletion updates, where $Δ$ denotes an upper bound on the maximum degree. A key advantage is the algorithm's ability to process many updates simultaneously, which makes it naturally adaptable to the parallel and distributed models. Concretely, it gives a unified framework across the models, leading to the following results:\n  - In the sequential setting, the algorithm processes each update in $O(1)$ expected time, worst-case. This matches and strengthens the results of Henzinger and Peng [TALG 2022] and Bhattacharya et al. [TALG 2022], who achieved an $O(1)$ bound but amortized (in expectation and with high probability, respectively), whose work was an improvement of the $O(\\log Δ)$ expected amortized bound of Bhattacharya et al. [SODA'18].\n  - In the parallel setting, the algorithm processes each (arbitrary size) batch of updates using $O(1)$ work per update in the batch in expectation, and in $\\text{poly}(\\log n)$ depth with high probability. This is, in a sense, an ideal parallelization of the above results.\n  - In the distributed setting, the algorithm can maintain a coloring of the network graph as (potentially many) edges are added or deleted. The maintained coloring is always proper; it may become partial upon updates, i.e., some nodes may temporarily lose their colors, but quickly converges to a full, proper coloring. Concretely, each insertion and deletion causes at most $O(1)$ nodes to become uncolored, but this is resolved within $O(\\log n)$ rounds with high probability (e.g., in the absence of further updates nearby--the precise guarantee is stronger, but technical). Importantly, the algorithm incurs only $O(1)$ expected message complexity and computation per update."}
{"id": "2512.09762", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09762", "abs": "https://arxiv.org/abs/2512.09762", "authors": ["Jonathan Edwards", "Tomas Petricek"], "title": "Baseline: Operation-Based Evolution and Versioning of Data", "comment": "Submitted to The Art, Science, and Engineering of Programming", "summary": "Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases. This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging. We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins. Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change \"for free\" by the machinery of operational differencing. Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper."}
{"id": "2512.09836", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09836", "abs": "https://arxiv.org/abs/2512.09836", "authors": ["Bernhard Stöckl", "Maximilian E. Schüle"], "title": "Fast Factorized Learning: Powered by In-Memory Database Systems", "comment": null, "summary": "Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training."}
