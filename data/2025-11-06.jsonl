{"id": "2511.03155", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03155", "abs": "https://arxiv.org/abs/2511.03155", "authors": ["Zhefan Wang", "Guokai Yan", "Jinbei Yu", "Siyu Gu", "Jingyan Chen", "Peng Jiang", "Zhiqiang Guo", "Min Zhang"], "title": "Generative Sequential Recommendation via Hierarchical Behavior Modeling", "comment": null, "summary": "Recommender systems in multi-behavior domains, such as advertising and\ne-commerce, aim to guide users toward high-value but inherently sparse\nconversions. Leveraging auxiliary behaviors (e.g., clicks, likes, shares) is\ntherefore essential. Recent progress on generative recommendations has brought\nnew possibilities for multi-behavior sequential recommendation. However,\nexisting generative approaches face two significant challenges: 1) Inadequate\nSequence Modeling: capture the complex, cross-level dependencies within user\nbehavior sequences, and 2) Lack of Suitable Datasets: publicly available\nmulti-behavior recommendation datasets are almost exclusively derived from\ne-commerce platforms, limiting the validation of feasibility in other domains,\nwhile also lacking sufficient side information for semantic ID generation. To\naddress these issues, we propose a novel generative framework, GAMER\n(Generative Augmentation and Multi-lEvel behavior modeling for Recommendation),\nbuilt upon a decoder-only backbone. GAMER introduces a cross-level interaction\nlayer to capture hierarchical dependencies among behaviors and a sequential\naugmentation strategy that enhances robustness in training. To further advance\nthis direction, we collect and release ShortVideoAD, a large-scale\nmulti-behavior dataset from a mainstream short-video platform, which differs\nfundamentally from existing e-commerce datasets and provides pretrained\nsemantic IDs for research on generative methods. Extensive experiments show\nthat GAMER consistently outperforms both discriminative and generative\nbaselines across multiple metrics."}
{"id": "2511.03298", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03298", "abs": "https://arxiv.org/abs/2511.03298", "authors": ["Oleg Senkevich", "Siyang Xu", "Tianyi Jiang", "Alexander Radionov", "Jan Tabaszewski", "Dmitriy Malyshev", "Zijian Li", "Daihao Xue", "Licheng Yu", "Weidi Zeng", "Meiling Wang", "Xin Yao", "Siyu Huang", "Gleb Neshchetkin", "Qiuling Pan", "Yaoyao Fu"], "title": "KScaNN: Scalable Approximate Nearest Neighbor Search on Kunpeng", "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) is a cornerstone algorithm for\ninformation retrieval, recommendation systems, and machine learning\napplications. While x86-based architectures have historically dominated this\ndomain, the increasing adoption of ARM-based servers in industry presents a\ncritical need for ANNS solutions optimized on ARM architectures. A naive port\nof existing x86 ANNS algorithms to ARM platforms results in a substantial\nperformance deficit, failing to leverage the unique capabilities of the\nunderlying hardware. To address this challenge, we introduce KScaNN, a novel\nANNS algorithm co-designed for the Kunpeng 920 ARM architecture. KScaNN\nembodies a holistic approach that synergizes sophisticated, data aware\nalgorithmic refinements with carefully-designed hardware specific\noptimizations. Its core contributions include: 1) novel algorithmic techniques,\nincluding a hybrid intra-cluster search strategy and an improved PQ residual\ncalculation method, which optimize the search process at a higher level; 2) an\nML-driven adaptive search module that provides adaptive, per-query tuning of\nsearch parameters, eliminating the inefficiencies of static configurations; and\n3) highly-optimized SIMD kernels for ARM that maximize hardware utilization for\nthe critical distance computation workloads. The experimental results\ndemonstrate that KScaNN not only closes the performance gap but establishes a\nnew standard, achieving up to a 1.63x speedup over the fastest x86-based\nsolution. This work provides a definitive blueprint for achieving\nleadership-class performance for vector search on modern ARM architectures and\nunderscores"}
{"id": "2511.03330", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03330", "abs": "https://arxiv.org/abs/2511.03330", "authors": ["Shenghua Wang", "Zhen Yin"], "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning", "comment": null, "summary": "The rapid growth of open-access (OA) publications has intensified the\nchallenge of identifying relevant scientific papers. Due to privacy constraints\nand limited access to user interaction data, recent efforts have shifted toward\ncontent-based recommendation, which relies solely on textual information.\nHowever, existing models typically treat papers as unstructured text,\nneglecting their discourse organization and thereby limiting semantic\ncompleteness and interpretability. To address these limitations, we propose\nOMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,\nMethod, Result, Conclusion) summarization, multi-level contrastive learning,\nand structure-aware re-ranking for scholarly recommendation. The QA-style\nsummarization module converts raw papers into structured and\ndiscourse-consistent representations, while multi-level contrastive objectives\nalign semantic representations across metadata, section, and document levels.\nThe final re-ranking stage further refines retrieval precision through\ncontextual similarity calibration. Experiments on DBLP, S2ORC, and the newly\nconstructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses\nstate-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in\nPrecision@10 and Recall@10, respectively. Additional evaluations confirm that\nQA-style summarization produces more coherent and factually complete\nrepresentations. Overall, OMRC-MR provides a unified and interpretable\ncontent-based paradigm for scientific paper recommendation, advancing\ntrustworthy and privacy-aware scholarly information retrieval."}
{"id": "2511.03351", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03351", "abs": "https://arxiv.org/abs/2511.03351", "authors": ["Saba Latif", "Fajar J. Ekaputra", "Maxim Vidgof", "Sabrina Kirrane", "Claudio Di Ciccio"], "title": "A Semantic Encoding of Object Centric Event Data", "comment": "12 pages, 3 figures, Wil60", "summary": "The Object-Centric Event Data (OCED) is a novel meta-model aimed at providing\na common ground for process data records centered around events and objects.\nOne of its objectives is to foster interoperability and process information\nexchange. In this context, the integration of data from different providers,\nthe combination of multiple processes, and the enhancement of knowledge\ninference are novel challenges. Semantic Web technologies can enable the\ncreation of a machine-readable OCED description enriched through ontology-based\nrelationships and entity categorization. In this paper, we introduce an\napproach built upon Semantic Web technologies for the realization of\nsemantic-enhanced OCED, with the aim to strengthen process data reasoning,\ninterconnect information sources, and boost expressiveness."}
{"id": "2511.03393", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.03393", "abs": "https://arxiv.org/abs/2511.03393", "authors": ["Chiara Rucco", "Motaz Saad", "Antonella Longo"], "title": "Formalizing ETLT and ELTL Design Patterns and Proposing Enhanced Variants: A Systematic Framework for Modern Data Engineering", "comment": null, "summary": "Traditional ETL and ELT design patterns struggle to meet modern requirements\nof scalability, governance, and real-time data processing. Hybrid approaches\nsuch as ETLT (Extract-Transform-Load-Transform) and ELTL\n(Extract-Load-Transform-Load) are already used in practice, but the literature\nlacks best practices and formal recognition of these approaches as design\npatterns. This paper formalizes ETLT and ELTL as reusable design patterns by\ncodifying implicit best practices and introduces enhanced variants, ETLT++ and\nELTL++, to address persistent gaps in governance, quality assurance, and\nobservability. We define ETLT and ELTL patterns systematically within a design\npattern framework, outlining their structure, trade-offs, and use cases.\nBuilding on this foundation, we extend them into ETLT++ and ELTL++ by embedding\nexplicit contracts, versioning, semantic curation, and continuous monitoring as\nmandatory design obligations. The proposed framework offers practitioners a\nstructured roadmap to build auditable, scalable, and cost-efficient pipelines,\nunifying quality enforcement, lineage, and usability across multi-cloud and\nreal-time contexts. By formalizing ETLT and ELTL, and enhancing them through\nETLT++ and ELTL++, this work bridges the gap between ad hoc practice and\nsystematic design, providing a reusable foundation for modern, trustworthy data\nengineering."}
{"id": "2511.02943", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.02943", "abs": "https://arxiv.org/abs/2511.02943", "authors": ["Henry Fleischmann", "George Z. Li", "Jason Li"], "title": "Faster Weak Expander Decompositions and Approximate Max Flow", "comment": "48 pages", "summary": "We give faster algorithms for weak expander decompositions and approximate\nmax flow on undirected graphs. First, we show that it is possible to \"warm\nstart\" the cut-matching game when computing weak expander decompositions,\navoiding the cost of the recursion depth. Our algorithm is also flexible enough\nto support weaker flow subroutines than previous algorithms.\n  Our second contribution is to streamline the recent non-recursive approximate\nmax flow algorithm of Li, Rao, and Wang (SODA, 2025) and adapt their framework\nto use our new weak expander decomposition primitive. Consequently, we give an\napproximate max flow algorithm within a few logarithmic factors of the limit of\nexpander decomposition-based approaches."}
{"id": "2511.03340", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.03340", "abs": "https://arxiv.org/abs/2511.03340", "authors": ["AloÃ¯s Duguet", "Tobias Harks", "Martin Schmidt", "Julian Schwarz"], "title": "Branch-and-Cut for Computing Approximate Equilibria of Mixed-Integer Generalized Nash Games", "comment": null, "summary": "Generalized Nash equilibrium problems with mixed-integer variables constitute\nan important class of games in which each player solves a mixed-integer\noptimization problem, where both the objective and the feasible set is\nparameterized by the rivals' strategies. However, such games are known for\nfailing to admit exact equilibria and also the assumption of all players being\nable to solve nonconvex problems to global optimality is questionable. This\nmotivates the study of approximate equilibria. In this work, we consider an\napproximation concept that incorporates both multiplicative and additive\nrelaxations of optimality. We propose a branch-and-cut (B&C) method that\ncomputes such approximate equilibria or proves its non-existence. For this, we\nadopt the idea of intersection cuts and show the existence of such cuts under\nthe condition that the constraints are linear and each player's cost function\nis either convex in the entire strategy profile, or, concave in the entire\nstrategy profile and linear in the rivals' strategies. For the special case of\nstandard Nash equilibrium problems, we introduce an alternative type of cut and\nshow that the method terminates finitely, provided that each player has only\nfinitely many distinct best-response sets. Finally, on the basis of the B&C\nmethod, we introduce a single-tree binary-search method to compute\nbest-approximate equilibria under some simplifying assumptions. We implemented\nthese methods and present numerical results for a class of mixed-integer flow\ngames."}
{"id": "2511.02951", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02951", "abs": "https://arxiv.org/abs/2511.02951", "authors": ["Sheida Rabeti", "Hessam Mahdavifar"], "title": "List Decoding and New Bicycle Code Constructions for Quantum LDPC Codes", "comment": null, "summary": "In this paper, we propose a new decoder, called the Multiple-Bases\nBelief-Propagation List Decoder (MBBP-LD), for Quantum Low-Density Parity-Check\n(QLDPC) codes. It extends the Multiple-Bases Belief-Propagation (MBBP)\nframework, originally developed for classical cyclic LDPC codes. The proposed\nmethod preserves the linear-time complexity of standard BP decoder while\nimproving the logical error rate. To further reduce the logical error rate, a\nnew decision rule is introduced for the post-processing list decoder,\noutperforming the conventional least-metric selector (LMS) criterion. For the\nrecently developed and implemented bivariate bicycle (BB) code with parameters\n\\([[144,12,12]]\\), our proposed MBBP-LD decoder achieves up to 40\\% lower\nlogical error rate compared to the state-of-the-art decoder for short QLDPC\ncodes, i.e., BP with ordered-statistics decoding (BP-OSD), while retaining the\nlinear-time complexity of the plain BP decoder. In addition, we explore a new\nsubclass of BB codes, that we refer to as the univariate bicycle (UB) codes,\nspecifically with lower-weight parity checks (\\(w=6,8\\)). This reduces the\npolynomial search space for the code compared to general BB codes, i.e., by\nreducing the search space over two polynomial components in BB codes to just a\nsingle polynomial component in UB codes. Simulations demonstrate the promising\nperformance of these codes under various types of BP decoders."}
{"id": "2511.03620", "categories": ["cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03620", "abs": "https://arxiv.org/abs/2511.03620", "authors": ["Philipp Hager", "Onno Zoeter", "Maarten de Rijke"], "title": "CLAX: Fast and Flexible Neural Click Models in JAX", "comment": null, "summary": "CLAX is a JAX-based library that implements classic click models using modern\ngradient-based optimization. While neural click models have emerged over the\npast decade, complex click models based on probabilistic graphical models\n(PGMs) have not systematically adopted gradient-based optimization, preventing\npractitioners from leveraging modern deep learning frameworks while preserving\nthe interpretability of classic models. CLAX addresses this gap by replacing\nEM-based optimization with direct gradient-based optimization in a numerically\nstable manner. The framework's modular design enables the integration of any\ncomponent, from embeddings and deep networks to custom modules, into classic\nclick models for end-to-end optimization. We demonstrate CLAX's efficiency by\nrunning experiments on the full Baidu-ULTR dataset comprising over a billion\nuser sessions in $\\approx$ 2 hours on a single GPU, orders of magnitude faster\nthan traditional EM approaches. CLAX implements ten classic click models,\nserving both industry practitioners seeking to understand user behavior and\nimprove ranking performance at scale and researchers developing new click\nmodels. CLAX is available at: https://github.com/philipphager/clax"}
{"id": "2511.03437", "categories": ["cs.DB", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.03437", "abs": "https://arxiv.org/abs/2511.03437", "authors": ["Md Mizanur Rahaman Nayan", "Zheyu Li", "Flavio Ponzina", "Sumukh Pinge", "Tajana Rosing", "Azad J. Naeemi"], "title": "HERP: Hardware for Energy Efficient and Realtime DB Search and Cluster Expansion in Proteomics", "comment": null, "summary": "Database (DB) search and clustering are fundamental in proteomics but\nconventional full clustering and search approaches demand high resources and\nincur long latency. We propose a lightweight incremental clustering and highly\nparallelizable DB search platform tailored for resource-constrained\nenvironments, delivering low energy and latency without compromising\nperformance. By leveraging mass-spectrometry insights, we employ bucket-wise\nparallelization and query scheduling to reduce latency. A one-time hardware\ninitialization with pre-clustered proteomics data enables continuous DB search\nand local re-clustering, offering a more practical and efficient alternative to\nclustering from scratch. Heuristics from pre-clustered data guide incremental\nclustering, accelerating the process by 20x with only a 0.3% increase in\nclustering error. DB search results overlap by 96% with state-of-the-art tools,\nvalidating search quality. The hardware leverages a 3T 2M T J SOT-CAM at the\n7nm node with a compute-in-memory design. For the human genome draft dataset\n(131GB), setup requires 1.19mJ for 2M spectra, while a 1000 query search\nconsumes 1.1{\\mu}J. Bucket-wise parallelization further achieves 100x speedup."}
{"id": "2511.02954", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.02954", "abs": "https://arxiv.org/abs/2511.02954", "authors": ["Ivor van der Hoog", "Eva Rotenberg", "Daniel Rutschmann"], "title": "Tight Better-Than-Worst-Case Bounds for Element Distinctness and Set Intersection", "comment": null, "summary": "The element distinctness problem takes as input a list $I$ of $n$ values from\na totally ordered universe and the goal is to decide whether $I$ contains any\nduplicates. It is a well-studied problem with a classical worst-case $\\Omega(n\n\\log n)$ comparison-based lower bound by Fredman. At first glance, this lower\nbound appears to rule out any algorithm more efficient than the naive approach\nof sorting $I$ and comparing adjacent elements. However, upon closer\ninspection, the $\\Omega(n \\log n)$ bound does not apply if the input has many\nduplicates. We therefore ask: Are there comparison-based lower bounds for\nelement distinctness that are sensitive to the amount of duplicates in the\ninput?\n  To address this question, we derive instance-specific lower bounds. For any\ninput instance $I$, we represent the combinatorial structure of the duplicates\nin $I$ by an undirected graph $G(I)$ that connects identical elements. Each\nsuch graph $G$ is a union of cliques, and we study algorithms by their\nworst-case running time over all inputs $I'$ with $G(I') \\cong G$. We establish\nan adversarial lower bound showing that, for any deterministic algorithm\n$\\mathcal{A}$, there exists a graph $G$ and an algorithm $\\mathcal{A}'$ that,\nfor all inputs $I$ with $G(I) \\cong G$, is a factor $O(\\log \\log n)$ faster\nthan $\\mathcal{A}$. Consequently, no deterministic algorithm can be $o(\\log\n\\log n)$-competitive for all graphs $G$. We complement this with an $O(\\log\n\\log n)$-competitive deterministic algorithm, thereby obtaining tight bounds\nfor element distinctness that go beyond classical worst-case analysis.\n  We subsequently study the related problem of set intersection. We show that\nno deterministic set intersection algorithm can be $o(\\log n)$-competitive, and\nprovide an $O(\\log n)$-competitive deterministic algorithm. This shows a\nseparation between element distinctness and the set intersection problem."}
{"id": "2511.03629", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03629", "abs": "https://arxiv.org/abs/2511.03629", "authors": ["Hadi Hosseini", "Shraddha Pathak", "Yu Zhou"], "title": "Non-Monotonicity in Fair Division of Graphs", "comment": null, "summary": "We consider the problem of fairly allocating the vertices of a graph among\n$n$ agents, where the value of a bundle is determined by its cut value -- the\nnumber of edges with exactly one endpoint in the bundle. This model naturally\ncaptures applications such as team formation and network partitioning, where\nvaluations are inherently non-monotonic: the marginal values may be positive,\nnegative, or zero depending on the composition of the bundle. We focus on the\nfairness notion of envy-freeness up to one item (EF1) and explore its\ncompatibility with several efficiency concepts such as Transfer Stability (TS)\nthat prohibits single-item transfers that benefit one agent without making the\nother worse-off. For general graphs, our results uncover a non-monotonic\nrelationship between the number of agents $n$ and the existence of allocations\nsatisfying EF1 and transfer stability (TS): such allocations always exist for\n$n=2$, may fail to exist for $n=3$, but exist again for all $n\\geq 4$. We\nfurther show that existence can be guaranteed for any $n$ by slightly weakening\nthe efficiency requirement or by restricting the graph to forests. All of our\npositive results are achieved via efficient algorithms."}
{"id": "2511.03063", "categories": ["cs.IT", "cs.CE", "math.IT", "H.1.1; J.3"], "pdf": "https://arxiv.org/pdf/2511.03063", "abs": "https://arxiv.org/abs/2511.03063", "authors": ["Margarita Geleta", "Daniel Mas Montserrat", "Alexander G. Ioannidis"], "title": "A Tsallis-Entropy Lens on Genetic Variation", "comment": "5 pages, 2 figures", "summary": "We introduce an information-theoretic generalization of the fixation\nstatistic, the Tsallis-order $q$ F-statistic, $F_q$, which measures the\nfraction of Tsallis $q$-entropy lost within subpopulations relative to the\npooled population. The family nests the classical variance-based fixation index\n$F_{\\textbf{ST}}$ at $q{=}2$ and a Shannon-entropy analogue at $q{=}1$, whose\nabsolute form equals the mutual information between alleles and population\nlabels. By varying $q$, $F_q$ acts as a spectral differentiator that up-weights\nrare variants at low $q$, while $q{>}1$ increasingly emphasizes common\nvariants, providing a more fine-grained view of differentiation than\n$F_{\\textbf{ST}}$ when allele-frequency spectra are skewed. On real data (865\nOceanian genomes with 1,823,000 sites) and controlled genealogical simulations\n(seeded from 1,432 founders from HGDP and 1000 Genomes panels, with 322,216\nsites), we show that $F_q$ in One-vs-Rest (OVR) and Leave-One-Out (LOO) modes\nprovides clear attribution of which subpopulations drive regional structure,\nand sensitively timestamps isolation-migration events and founder effects.\n$F_q$ serves as finer-resolution complement for simulation audits and\npopulation-structure summaries."}
{"id": "2511.03480", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.03480", "abs": "https://arxiv.org/abs/2511.03480", "authors": ["Khalid Belhajjame", "Haroun Mezrioui", "Yuyan Zhao"], "title": "In-Memory Indexing and Querying of Provenance in Data Preparation Pipelines", "comment": null, "summary": "Data provenance has numerous applications in the context of data preparation\npipelines. It can be used for debugging faulty pipelines, interpreting results,\nverifying fairness, and identifying data quality issues, which may affect the\nsources feeding the pipeline execution. In this paper, we present an indexing\nmechanism to efficiently capture and query pipeline provenance. Our solution\nleverages tensors to capture fine-grained provenance of data processing\noperations, using minimal memory. In addition to record-level lineage\nrelationships, we provide finer granularity at the attribute level. This is\nachieved by augmenting tensors, which capture retrospective provenance, with\nprospective provenance information, drawing connections between input and\noutput schemas of data processing operations. We demonstrate how these two\ntypes of provenance (retrospective and prospective) can be combined to answer a\nbroad range of provenance queries efficiently, and show effectiveness through\nevaluation exercises using both real and synthetic data."}
{"id": "2511.03007", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03007", "abs": "https://arxiv.org/abs/2511.03007", "authors": ["Lucas Castro", "Thailsson Clementino", "Rosiane de Freitas"], "title": "Implementation and Brief Experimental Analysis of the Duan et al. (2025) Algorithm for Single-Source Shortest Paths", "comment": null, "summary": "We present an implementation and a brief experimental analysis of the\ndeterministic algorithm proposed by Duan et al. (2025) for the Single-Source\nShortest Path (SSSP) problem, which achieves the best known asymptotic upper\nbound in the comparison-addition model, with running time $O(m \\log^{2/3} n)$.\nWe provide a faithful C++ implementation of this algorithm, following all\nstructural details described in the original paper, and compare its empirical\nperformance with the classical Dijkstra's algorithm using binary heaps. The\nexperiments were conducted on both synthetic sparse random graphs and\nreal-world road network instances from the DIMACS benchmark. Our results show\nthat, despite its superior asymptotic complexity, the new algorithm presents\nsignificantly larger constant factors, making Dijkstra's algorithm faster for\nall tested sparse graph sizes, including instances with tens of millions of\nvertices. Our implementation achieves $O(m \\log^{2/3} n)$ expected time, due to\nthe use of hash tables, and some possibilities for making it worst-case are\nbeing considered. (This is a ongoing work.)"}
{"id": "2511.03305", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.03305", "abs": "https://arxiv.org/abs/2511.03305", "authors": ["Haoqin Zhao", "Zan Li", "Jiangbo Si", "Rui Huang", "Hang Hu", "Tony Q. S. Quek", "Naofal Al-Dhahir"], "title": "DRL-Based Robust Multi-Timescale Anti-Jamming Approaches under State Uncertainty", "comment": "13pages,12figures", "summary": "Owing to the openness of wireless channels, wireless communication systems\nare highly susceptible to malicious jamming. Most existing anti-jamming methods\nrely on the assumption of accurate sensing and optimize parameters on a single\ntimescale. However, such methods overlook two practical issues: mismatched\nexecution latencies across heterogeneous actions and measurement errors caused\nby sensor imperfections. Especially for deep reinforcement learning (DRL)-based\nmethods, the inherent sensitivity of neural networks implies that even minor\nperturbations in the input can mislead the agent into choosing suboptimal\nactions, with potentially severe consequences. To ensure reliable wireless\ntransmission, we establish a multi-timescale decision model that incorporates\nstate uncertainty. Subsequently, we propose two robust schemes that sustain\nperformance under bounded sensing errors. First, a Projected Gradient\nDescent-assisted Double Deep Q-Network (PGD-DDQN) algorithm is designed, which\nderives worst-case perturbations under a norm-bounded error model and applies\nPGD during training for robust optimization. Second, a Nonlinear Q-Compression\nDDQN (NQC-DDQN) algorithm introduces a nonlinear compression mechanism that\nadaptively contracts Q-value ranges to eliminate action aliasing. Simulation\nresults indicate that, compared with the perfect-sensing baseline, the proposed\nalgorithms show only minor degradation in anti-jamming performance while\nmaintaining robustness under various perturbations, thereby validating their\npracticality in imperfect sensing conditions."}
{"id": "2511.03489", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.03489", "abs": "https://arxiv.org/abs/2511.03489", "authors": ["Daniel Kang"], "title": "Analytical Queries for Unstructured Data", "comment": null, "summary": "Unstructured data, in the form of text, images, video, and audio, is produced\nat exponentially higher rates. In tandem, machine learning (ML) methods have\nbecome increasingly powerful at analyzing unstructured data. Modern ML methods\ncan now detect objects in images, understand actions in videos, and even\nclassify complex legal texts based on legal intent. Combined, these trends make\nit increasingly feasible for analysts and researchers to automatically\nunderstand the \"real world.\" However, there are major challenges in deploying\nthese techniques: 1) executing queries efficiently given the expense of ML\nmethods, 2) expressing queries over bespoke forms of data, and 3) handling\nerrors in ML methods.\n  In this monograph, we discuss challenges and advances in data management\nsystems for unstructured data using ML, with a particular focus on video\nanalytics. Using ML to answer queries introduces new challenges.First, even\nturning user intent into queries can be challenging: it is not obvious how to\nexpress a query of the form \"select instances of cars turning left.\" Second, ML\nmodels can be orders of magnitude more expensive compared to processing\ntraditional structured data. Third, ML models and the methods to accelerate\nanalytics with ML models can be error-prone.\n  Recent work in the data management community has aimed to address all of\nthese challenges. Users can now express queries via user-defined functions,\nopaquely through standard structured schemas, and even by providing examples.\nGiven a query, recent work focuses on optimizing queries by approximating\nexpensive \"gold\" methods with varying levels of guarantees. Finally, to handle\nerrors in ML models, recent work has focused on applying outlier and drift\ndetection to data analytics with ML."}
{"id": "2511.03157", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03157", "abs": "https://arxiv.org/abs/2511.03157", "authors": ["Yi Zhoua", "Chunyu Luoa", "Zhengren Wangb", "Zhang-Hua Fuc"], "title": "A Branch-and-Bound Approach for Maximum Low-Diameter Dense Subgraph Problems", "comment": null, "summary": "A graph with $n$ vertices is an $f(\\cdot)$-dense graph if it has at least\n$f(n)$ edges, $f(\\cdot)$ being a well-defined function. The notion\n$f(\\cdot)$-dense graph encompasses various clique models like $\\gamma$-quasi\ncliques, $k$-defective cliques, and dense cliques, arising in cohesive subgraph\nextraction applications. However, the $f(\\cdot)$-dense graph may be\ndisconnected or weakly connected. To conquer this, we study the problem of\nfinding the largest $f(\\cdot)$-dense subgraph with a diameter of at most two in\nthe paper. Specifically, we present a decomposition-based branch-and-bound\nalgorithm to optimally solve this problem. The key feature of the algorithm is\na decomposition framework that breaks the graph into $n$ smaller subgraphs,\nallowing independent searches in each subgraph. We also introduce decomposition\nstrategies including degeneracy and two-hop degeneracy orderings, alongside a\nbranch-and-bound algorithm with a novel sorting-based upper bound to solve each\nsubproblem. Worst-case complexity for each component is provided. Empirical\nresults on 139 real-world graphs under two $f(\\cdot)$ functions show our\nalgorithm outperforms the MIP solver and pure branch-and-bound, solving nearly\ntwice as many instances optimally within one hour."}
{"id": "2511.03323", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.03323", "abs": "https://arxiv.org/abs/2511.03323", "authors": ["Zekai Chen", "Min Sha"], "title": "Constacyclic codes with best-known parameters", "comment": null, "summary": "In this paper, we construct several infinite families of $q$-ary constacyclic\ncodes over a finite field $\\mathbb{F}_q$ with length $n$, dimension around\n$n/2$, and minimum distance at least $cn/\\log_q n$ for some positive constant\n$c$. They contain many constacyclic codes with optimal, or almost-optimal, or\nbest-known parameters. We also consider various forms of the length $n$."}
{"id": "2511.03289", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03289", "abs": "https://arxiv.org/abs/2511.03289", "authors": ["Tian Bai", "Zhiyi Huang", "Chui Shan Lee", "Dongchen Li"], "title": "Optimal Stopping with a Predicted Prior", "comment": null, "summary": "There are two major models of value uncertainty in the optimal stopping\nliterature: the secretary model, which assumes no prior knowledge, and the\nprophet inequality model, which assumes full information about value\ndistributions. In practice, decision makers often rely on machine-learned\npriors that may be erroneous. Motivated by this gap, we formulate the model of\noptimal stopping with a predicted prior to design algorithms that are both\nconsistent, exploiting the prediction when accurate, and robust, retaining\nworst-case guarantees when it is not.\n  Existing secretary and prophet inequality algorithms are either pessimistic\nin consistency or not robust to misprediction. A randomized combination only\ninterpolates their guarantees linearly. We show that a family of bi-criteria\nalgorithms achieves improved consistency-robustness trade-offs, both for\nmaximizing the expected accepted value and for maximizing the probability of\naccepting the maximum value. We further prove that for the latter objective, no\nalgorithm can simultaneously match the best prophet inequality algorithm in\nconsistency, and the best secretary algorithm in robustness."}
{"id": "2511.03398", "categories": ["cs.IT", "math.IT", "94B05, 11T71", "E.4"], "pdf": "https://arxiv.org/pdf/2511.03398", "abs": "https://arxiv.org/abs/2511.03398", "authors": ["Zhonghao Liang", "Chenlu Jia", "Qunying Liao"], "title": "The (+)-(L, P)-TGRS code", "comment": "23pages", "summary": "The construction of the non-Reed-Solomon (in short, non-RS) type linear code\nhas been one of the research hotspots in recent years. In 2025, Hu et al.\nconstructed some non-RS MDS codes by defining the (L, P)-twisted generalized\nReed-Solomon code (in short, (L, P)-TGRS). In this paper, we focus on the\n(+)-(L, P)-TGRS code C. We firstly present a parity-check matrix. Secondly, we\ngive a sufficient and necessary condition for C to be NMDS which partially\nanswers two open problems proposed by Hu et al. in 2025, and prove that C is\nnon-RS for 2k > n which partially improves the corresponding result given by Hu\net al. in 2025,. Thirdly, we give a sufficient condition for C not to be\nself-dual or self-orthogonal, respectively, furthermore, we construct two\nclasses of self-orthogonal codes which is a promotion of the corresponding\nresult given by Ding et al. in 2025. Finally, some examples are given."}
{"id": "2511.03345", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03345", "abs": "https://arxiv.org/abs/2511.03345", "authors": ["Sander Borst", "Danish Kashaev"], "title": "Improved Online Load Balancing in the Two-Norm", "comment": null, "summary": "We study the online load balancing problem on unrelated machines, with the\nobjective of minimizing the square of the $\\ell_2$ norm of the loads on the\nmachines. The greedy algorithm of Awerbuch et al. (STOC'95) is optimal for\ndeterministic algorithms and achieves a competitive ratio of $3 + 2 \\sqrt{2}\n\\approx 5.828$, and an improved $5$-competitive randomized algorithm based on\nindependent rounding has been shown by Caragiannis (SODA'08). In this work, we\npresent the first algorithm breaking the barrier of $5$ on the competitive\nratio, achieving a bound of $4.9843$. To obtain this result, we use a new\nprimal-dual framework to analyze this problem based on a natural semidefinite\nprogramming relaxation, together with an online implementation of a correlated\nrandomized rounding procedure of Im and Shadloo (SODA'20). This novel\nprimal-dual framework also yields new, simple and unified proofs of the\ncompetitive ratio of the $(3 + 2 \\sqrt{2})$-competitive greedy algorithm, the\n$5$-competitive randomized independent rounding algorithm, and that of a new\n$4$-competitive optimal fractional algorithm. We also provide lower bounds\nshowing that the previous best randomized algorithm is optimal among\nindependent rounding algorithms, that our new fractional algorithm is optimal,\nand that a simple greedy algorithm is optimal for the closely related online\nscheduling problem $R || \\sum w_j C_j$."}
{"id": "2511.03415", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.03415", "abs": "https://arxiv.org/abs/2511.03415", "authors": ["Xusheng Zhu", "Farshad Rostami Ghadi", "Tuo Wu", "Kaitao Meng", "Chao Wang", "Gui Zhou"], "title": "On the Fundamental Scaling Laws of Fluid Antenna Systems", "comment": null, "summary": "Fluid antenna systems (FAS) offer a promising paradigm for enhancing wireless\ncommunication by exploiting spatial diversity, yet a rigorous analytical\nframework for their error probability has been notably absent. To this end,\nthis paper addresses this critical gap by unveiling the \\textbf{fundamental\nscaling laws} that govern the symbol error rate (SER) of FAS in realistic,\nspatially correlated channels. To establish these laws, we derive a tight,\nclosed-form asymptotic expression for the SER applicable to a general class of\nmodulation schemes. This result is pivotal as it establishes the fundamental\nscaling law governing the relationship between SER and the channel's spatial\ncorrelation structure. Based on this framework, we provide a complete\ncharacterization of the diversity and coding gains. The analysis culminates in\na definitive design directive: SER can be fundamentally improved by expanding\nthe antenna's movement space to increase diversity, while merely increasing\nport density within a constrained space yields diminishing returns."}
{"id": "2511.03440", "categories": ["cs.DS", "cs.CC", "math.AG", "math.OC", "90C23, 90C25, 14P10", "F.2.0; G.1.6"], "pdf": "https://arxiv.org/pdf/2511.03440", "abs": "https://arxiv.org/abs/2511.03440", "authors": ["Lucas Slot", "David Steurer", "Manuel Wiedmer"], "title": "Hesse's Redemption: Efficient Convex Polynomial Programming", "comment": null, "summary": "Efficient algorithms for convex optimization, such as the ellipsoid method,\nrequire an a priori bound on the radius of a ball around the origin guaranteed\nto contain an optimal solution if one exists. For linear and convex quadratic\nprogramming, such solution bounds follow from classical characterizations of\noptimal solutions by systems of linear equations. For other programs, e.g.,\nsemidefinite ones, examples due to Khachiyan show that optimal solutions may\nrequire huge coefficients with an exponential number of bits, even if we allow\napproximations. Correspondingly, semidefinite programming is not even known to\nbe in NP. The unconstrained minimization of convex polynomials of degree four\nand higher has remained a fundamental open problem between these two extremes:\nits optimal solutions do not admit a linear characterization and, at the same\ntime, Khachiyan-type examples do not apply. We resolve this problem by\ndeveloping new techniques to prove solution bounds when no linear\ncharacterizations are available. Even for programs minimizing a convex\npolynomial (of arbitrary degree) over a polyhedron, we prove that the existence\nof an optimal solution implies that an approximately optimal one with\npolynomial bit length also exists. These solution bounds, combined with the\nellipsoid method, yield the first polynomial-time algorithm for convex\npolynomial programming, settling a question posed by Nesterov (Math. Program.,\n2019). Before, no polynomial-time algorithm was known even for unconstrained\nminimization of a convex polynomial of degree four."}
{"id": "2511.03632", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.03632", "abs": "https://arxiv.org/abs/2511.03632", "authors": ["Cemil Vahapoglu", "Timothy J. O'Shea", "Wan Liu", "Sennur Ulukus"], "title": "Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility Environments", "comment": null, "summary": "Beamforming has significance for enhancing spectral efficiency and mitigating\ninterference in multi-antenna wireless systems, facilitating spatial\nmultiplexing and diversity in dense and high mobility scenarios. Traditional\nbeamforming techniques such as zero-forcing beamforming (ZFBF) and minimum mean\nsquare error (MMSE) beamforming experience performance deterioration under\nadverse channel conditions. Deep learning-based beamforming offers an\nalternative with nonlinear mappings from channel state information (CSI) to\nbeamforming weights by improving robustness against dynamic channel\nenvironments. Transformer-based models are particularly effective due to their\nability to model long-range dependencies across time and frequency. However,\ntheir quadratic attention complexity limits scalability in large OFDM grids.\nRecent studies address this issue through sparse attention mechanisms that\nreduce complexity while maintaining expressiveness, yet often employ patterns\nthat disregard channel dynamics, as they are not specifically designed for\nwireless communication scenarios. In this work, we propose a Doppler-aware\nSparse Neural Network Beamforming (Doppler-aware Sparse NNBF) model that\nincorporates a channel-adaptive sparse attention mechanism in a multi-user\nsingle-input multiple-output (MU-SIMO) setting. The proposed sparsity structure\nis configurable along 2D time-frequency axes based on channel dynamics and is\ntheoretically proven to ensure full connectivity within p hops, where p is the\nnumber of attention heads. Simulation results under urban macro (UMa) channel\nconditions show that Doppler-aware Sparse NNBF significantly outperforms both a\nfixed-pattern baseline, referred to as Standard Sparse NNBF, and conventional\nbeamforming techniques ZFBF and MMSE beamforming in high mobility scenarios,\nwhile maintaining structured sparsity with a controlled number of attended keys\nper query."}
{"id": "2511.03461", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03461", "abs": "https://arxiv.org/abs/2511.03461", "authors": ["Christian Bertram", "Deborah Haun", "Mads Vestergaard Jensen", "Tuukka Korhonen"], "title": "Dynamic Meta-Kernelization", "comment": null, "summary": "Kernelization studies polynomial-time preprocessing algorithms. Over the last\n20 years, the most celebrated positive results of the field have been linear\nkernels for classical NP-hard graph problems on sparse graph classes. In this\npaper, we lift these results to the dynamic setting.\n  As the canonical example, Alber, Fellows, and Niedermeier [J. ACM 2004] gave\na linear kernel for dominating set on planar graphs. We provide the following\ndynamic version of their kernel: Our data structure is initialized with an\n$n$-vertex planar graph $G$ in $O(n \\log n)$ amortized time, and, at\ninitialization, outputs a planar graph $K$ with $\\mathrm{OPT}(K) =\n\\mathrm{OPT}(G)$ and $|K| = O(\\mathrm{OPT}(G))$, where $\\mathrm{OPT}(\\cdot)$\ndenotes the size of a minimum dominating set. The graph $G$ can be updated by\ninsertions and deletions of edges and isolated vertices in $O(\\log n)$\namortized time per update, under the promise that it remains planar. After each\nupdate to $G$, the data structure outputs $O(1)$ updates to $K$, maintaining\n$\\mathrm{OPT}(K) = \\mathrm{OPT}(G)$, $|K| = O(\\mathrm{OPT}(G))$, and planarity\nof $K$.\n  Furthermore, we obtain similar dynamic kernelization algorithms for all\nproblems satisfying certain conditions on (topological-)minor-free graph\nclasses. Besides kernelization, this directly implies new dynamic\nconstant-approximation algorithms and improvements to dynamic FPT algorithms\nfor such problems.\n  Our main technical contribution is a dynamic data structure for maintaining\nan approximately optimal protrusion decomposition of a dynamic\ntopological-minor-free graph. Protrusion decompositions were introduced by\nBodlaender, Fomin, Lokshtanov, Penninkx, Saurabh, and Thilikos [J. ACM 2016],\nand have since developed into a part of the core toolbox in kernelization and\nparameterized algorithms."}
{"id": "2511.03485", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03485", "abs": "https://arxiv.org/abs/2511.03485", "authors": ["Yutong Geng", "Enze Sun", "Zonghan Yang", "Yuhao Zhang"], "title": "Online Flow Time Minimization: Tight Bounds for Non-Preemptive Algorithms", "comment": null, "summary": "This paper studies the classical online scheduling problem of minimizing\ntotal flow time for $n$ jobs on $m$ identical machines. Prior work often cites\nthe $\\Omega(n)$ lower bound for non-preemptive algorithms to argue for the\nnecessity of preemption or resource augmentation, which shows the trivial\n$O(n)$-competitive greedy algorithm is tight. However, this lower bound applies\nonly to \\emph{deterministic} algorithms in the \\emph{single-machine} case,\nleaving several fundamental questions unanswered. Can randomness help in the\nnon-preemptive setting, and what is the optimal online deterministic algorithm\nwhen $m \\geq 2$? We resolve both questions. We present a polynomial-time\nrandomized algorithm with competitive ratio $\\Theta(\\sqrt{n/m})$ and prove a\nmatching randomized lower bound, settling the randomized non-preemptive setting\nfor every $m$. This also improves the best-known offline approximation ratio\nfrom $O(\\sqrt{n/m}\\log(n/m))$ to $O(\\sqrt{n/m})$. On the deterministic side, we\npresent a non-preemptive algorithm with competitive ratio\n$O(n/m^{2}+\\sqrt{n/m}\\log m)$ and prove a nearly matching lower bound.\n  Our framework also extends to the kill-and-restart model, where we reveal a\nsharp transition of deterministic algorithms: we design an asymptotically\noptimal algorithm with the competitive ratio $O(\\sqrt{n/m})$ for $m\\ge 2$, yet\nestablish a strong $\\Omega(n/\\log n)$ lower bound for $m=1$. Moreover, we show\nthat randomization provides no further advantage, as the lower bound coincides\nwith that of the non-preemptive setting.\n  While our main results assume prior knowledge of $n$, we also investigate the\nsetting where $n$ is unknown. We show kill-and-restart is powerful enough to\nbreak the $O(n)$ barrier for $m \\geq 2$ even without knowing $n$. Conversely,\nwe prove randomization alone is insufficient, as no algorithm can achieve an\n$o(n)$ competitive ratio in this setting."}
{"id": "2511.03490", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03490", "abs": "https://arxiv.org/abs/2511.03490", "authors": ["Etienne Bamas", "Shi Li", "Lars Rohwedder"], "title": "Randomized Rounding over Dynamic Programs", "comment": null, "summary": "We show that under mild assumptions for a problem whose solutions admit a\ndynamic programming-like recurrence relation, we can still find a solution\nunder additional packing constraints, which need to be satisfied approximately.\nThe number of additional constraints can be very large, for example, polynomial\nin the problem size. Technically, we reinterpret the dynamic programming\nsubproblems and their solutions as a network design problem. Inspired by\ntechniques from, for example, the Directed Steiner Tree problem, we construct a\nstrong LP relaxation, on which we then apply randomized rounding. Our\napproximation guarantees on the packing constraints have roughly the form of a\n$(n^{\\epsilon} \\mathrm{polylog}\\ n)$-approximation in time $n^{O(1/\\epsilon)}$,\nfor any $\\epsilon > 0$. By setting $\\epsilon=\\log \\log n/\\log n$, we obtain a\npolylogarithmic approximation in quasi-polynomial time, or by setting\n$\\epsilon$ as a constant, an $n^\\epsilon$-approximation in polynomial time.\n  While there are necessary assumptions on the form of the DP, it is general\nenough to capture many textbook dynamic programs from Shortest Path to Longest\nCommon Subsequence. Our algorithm then implies that we can impose additional\nconstraints on the solutions to these problems. This allows us to model various\nproblems from the literature in approximation algorithms, many of which were\nnot thought to be connected to dynamic programming. In fact, our result can\neven be applied indirectly to some problems that involve covering instead of\npacking constraints, for example, the Directed Steiner Tree problem, or those\nthat do not directly follow a recurrence relation, for example, variants of the\nMatching problem."}
{"id": "2511.03525", "categories": ["cs.DS", "G.2.2"], "pdf": "https://arxiv.org/pdf/2511.03525", "abs": "https://arxiv.org/abs/2511.03525", "authors": ["Marco D'Elia", "Irene Finocchi", "Maurizio Patrignani"], "title": "Engineering Algorithms for $\\ell$-Isolated Maximal Clique Enumeration", "comment": null, "summary": "Maximal cliques play a fundamental role in numerous application domains,\nwhere their enumeration can prove extremely useful. Yet their sheer number,\neven in sparse real-world graphs, can make them impractical to be exploited\neffectively. To address this issue, one approach is to enumerate\n$\\ell$-isolated maximal cliques, whose vertices have (on average) less than\n$\\ell$ edges toward the rest of the graph. By tuning parameter $\\ell$, the\ndegree of isolation can be controlled, and cliques that are overly connected to\nthe outside are filtered out. Building on Tomita et al.'s very practical\nrecursive algorithm for maximal clique enumeration, we propose four pruning\nheuristics, applicable individually or in combination, that discard recursive\nsearch branches that are guaranteed not to yield $\\ell$-isolated maximal\ncliques. Besides proving correctness, we characterize both the pruning power\nand the computational cost of these heuristics, and we conduct an extensive\nexperimental study comparing our methods with Tomita's baseline and with a\nstate-of-the-art approach. Results show that two of our heuristics offer\nsubstantial efficiency improvements, especially on real-world graphs with\nsocial network properties."}
{"id": "2511.03650", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03650", "abs": "https://arxiv.org/abs/2511.03650", "authors": ["Debarshi Chanda"], "title": "Improved Bounds with a Simple Algorithm for Edge Estimation for Graphs of Unknown Size", "comment": "25 pages, 2 Figures", "summary": "We propose a randomized algorithm with query access that given a graph $G$\nwith arboricity $\\alpha$, and average degree $d$, makes\n$\\widetilde{O}\\left(\\frac{\\alpha}{\\varepsilon^2d}\\right)$ \\texttt{Degree} and\n$\\widetilde{O}\\left(\\frac{1}{\\varepsilon^2}\\right)$ \\texttt{Random Edge}\nqueries to obtain an estimate $\\widehat{d}$ satisfying $\\widehat{d} \\in\n(1\\pm\\varepsilon)d$. This improves the $\\widetilde{O}_{\\varepsilon,\\log\nn}\\left(\\sqrt{\\frac{n}{d}}\\right)$ query algorithm of [Beretta et al., SODA\n2026] that has access to \\texttt{Degree}, \\texttt{Neighbour}, and\n\\texttt{Random Edge} queries. Our algorithm does not require any graph\nparameter as input, not even the size of the vertex set, and attains both\nsimplicity and practicality through a new estimation technique. We complement\nour upper bounds with a lower bound that shows for all valid $n,d$, and\n$\\alpha$, any algorithm that has access to \\texttt{Degree}, \\texttt{Neighbour},\nand \\texttt{Random Edge} queries, must make at least\n$\\Omega\\left(\\min\\left(d,\\frac{\\alpha}{d}\\right)\\right)$ queries to obtain a\n$(1\\pm\\varepsilon)$-multiplicative estimate of $d$, even with the knowledge of\n$n$ and $\\alpha$. We also show that even with \\texttt{Pair} and\n\\texttt{FullNbr} queries, an algorithm must make\n$\\Omega\\left(\\min\\left(d,\\frac{\\alpha}{d}\\right)\\right)$ queries to obtain a\n$(1\\pm\\varepsilon)$-multiplicative estimate of $d$. Our work addresses both the\nquestions raised by the work of [Beretta et al., SODA 2026]."}
{"id": "2511.03716", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03716", "abs": "https://arxiv.org/abs/2511.03716", "authors": ["Monika Henzinger", "Robin MÃ¼nk", "Harald RÃ¤cke"], "title": "An Improved Quality Hierarchical Congestion Approximator in Near-Linear Time", "comment": null, "summary": "A congestion approximator for a graph is a compact data structure that\napproximately predicts the edge congestion required to route any set of flow\ndemands in a network. A congestion approximator is hierarchical if it consists\nof a laminar family of cuts in the graph. There is a tradeoff between the\nrunning time for computing a congestion approximator and its approximation\nquality. Currently, for an $n$-node graph there exists a polynomial time\nalgorithm that achieves a $O(\\log^{1.5}n \\log \\log n)$ approximation and a\nnear-linear time algorithm that achieves w.h.p. a $O(\\log^4 n)$ approximation.\nIn this paper we give the first near-linear time algorithm, that achieves\nw.h.p. a $O(\\log^2 n \\log \\log n)$ approximation, using an hierarchical\ncongestion approximator with $O(n \\log n)$ cuts. Based on a reduction from\noblivious routing, we also present a lower bound of $\\Omega(\\log n)$ for the\napproximation quality of hierarchical congestion approximators.\n  Our algorithm can also be implemented in the parallel setting achieving the\nsame approximation quality, polylogarithmic span and near-linear work. This\nimproves upon the best prior parallel algorithm, which has a $O(\\log^9n)$\napproximation.\n  Crucial for achieving a near linear running time is a new partitioning\nroutine that, unlike previous such routines, manages to avoid recursing on\nlarge subgraphs. To achieve the improved approximation quality, we introduce\nthe new concept of border routability of a cut and give an improved sparsest\ncut oracle for general vertex weights."}
{"id": "2511.03629", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03629", "abs": "https://arxiv.org/abs/2511.03629", "authors": ["Hadi Hosseini", "Shraddha Pathak", "Yu Zhou"], "title": "Non-Monotonicity in Fair Division of Graphs", "comment": null, "summary": "We consider the problem of fairly allocating the vertices of a graph among\n$n$ agents, where the value of a bundle is determined by its cut value -- the\nnumber of edges with exactly one endpoint in the bundle. This model naturally\ncaptures applications such as team formation and network partitioning, where\nvaluations are inherently non-monotonic: the marginal values may be positive,\nnegative, or zero depending on the composition of the bundle. We focus on the\nfairness notion of envy-freeness up to one item (EF1) and explore its\ncompatibility with several efficiency concepts such as Transfer Stability (TS)\nthat prohibits single-item transfers that benefit one agent without making the\nother worse-off. For general graphs, our results uncover a non-monotonic\nrelationship between the number of agents $n$ and the existence of allocations\nsatisfying EF1 and transfer stability (TS): such allocations always exist for\n$n=2$, may fail to exist for $n=3$, but exist again for all $n\\geq 4$. We\nfurther show that existence can be guaranteed for any $n$ by slightly weakening\nthe efficiency requirement or by restricting the graph to forests. All of our\npositive results are achieved via efficient algorithms."}
