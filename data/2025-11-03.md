<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 2]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.IR](#cs.IR) [Total: 6]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Market Equilibria With Buying Rights](https://arxiv.org/abs/2510.26891)
*Martin Loebl,Anetta Jedličková,Jakub Černý*

Main category: cs.GT

TL;DR: 该研究将购买权嵌入Arrow-Debreu模型，分析通过购买权监管对长期不平等的影响，提出市场清算价格近似算法，并建立了公平性损失的界。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于分配危机场景，即需求与供给持续错配的情况。在这种情境下，稀缺资源倾向于集中在富裕个体手中，而广大民众需求无法满足。完全集中分配在物流或政治上不可行，而数字化的购买权提供更实用的替代方案。

Method: 构建了迭代市场模型，监管者定期分配购买权以促进更公平分配。方法包括：(i)定义每轮分配和交易购买权与资源的市场；(ii)每轮市场清算价格的近似算法；(iii)建立"挫败感"的上界。

Result: 提出了购买权监管框架下的市场模型和价格近似算法，并建立了公平性损失的理论上界。挫败感概念类似于无政府状态价格，但适用于购买权监管系统。

Conclusion: 通过购买权监管可以在分配危机中实现更公平的资源分配，数字化的购买权实施提供了可行的监管替代方案，同时建立了理论保证来限制公平性损失。

Abstract: We embed buying rights into a (repeated) Arrow-Debreu model to study the
long-term effects of regulation through buying rights on arising inequality.
Our motivation stems from situations that typically call for regulatory
interventions, such as rationing, namely, distribution crises in which demand
and supply are persistently misaligned. In such settings, scarce resources tend
to become increasingly concentrated among more affluent individuals, while the
needs of the broader population remain unmet. While fully centralized
distribution may be logistically or politically unfeasible, issuing buying
rights offers a more practical alternative: they can be implemented digitally,
e.g., via tokens traded on online platforms, making them significantly easier
to administer. We model a scenario in which a regulator periodically
distributes buying rights with the aim of promoting a more equitable
allocation. Our contributions include (i) the definition of the (iterated)
market where in each round the buying rights are distributed and then traded
alongside the resource, (ii) the approximation algorithm of the market-clearing
prices in every round, and (iii) the upper bound on \textit{frustration} -- a
notion conceptually similar to the Price of Anarchy, but for systems regulated
through buying rights, defined as the arising loss in fairness the individual
buyers have to take when the distribution is handled via the market.

</details>


### [2] [Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing](https://arxiv.org/abs/2510.27008)
*Fabian Raoul Pieroth,Ole Petersen,Martin Bichler*

Main category: cs.GT

TL;DR: 使用深度强化学习计算和验证有限期动态寡头博弈中的均衡，发现在不对称成本结构下会出现掠夺性定价行为。


<details>
  <summary>Details</summary>
Motivation: 解决Selten(1965)动态寡头模型中关于掠夺性定价均衡存在性的长期开放问题，特别是在允许企业退出的有限期博弈中。

Method: 利用深度强化学习的最新进展，在完美和不完美信息寡头模型中计算和验证均衡策略。

Result: 深度强化学习算法能可靠收敛到均衡；当企业面临不对称成本结构时，均衡策略表现出掠夺性定价行为。

Conclusion: 掠夺性定价可以在各种模型设置中作为理性均衡策略出现，为竞争监管机构提供了新见解。

Abstract: Predatory pricing -- where a firm strategically lowers prices to undermine
competitors -- is a contentious topic in dynamic oligopoly theory, with
scholars debating practical relevance and the existence of predatory
equilibria. Although finite-horizon dynamic models have long been proposed to
capture the strategic intertemporal incentives of oligopolists, the existence
and form of equilibrium strategies in settings that allow for firm exit
(drop-outs following loss-making periods) have remained an open question. We
focus on the seminal dynamic oligopoly model by Selten (1965) that introduces
the subgame perfect equilibrium and analyzes smooth market sharing. Equilibrium
can be derived analytically in models that do not allow for dropouts, but not
in models that can lead to predatory pricing. In this paper, we leverage recent
advances in deep reinforcement learning to compute and verify equilibria in
finite-horizon dynamic oligopoly games. Our experiments reveal two key
findings: first, state-of-the-art deep reinforcement learning algorithms
reliably converge to equilibrium in both perfect- and imperfect-information
oligopoly models; second, when firms face asymmetric cost structures, the
resulting equilibria exhibit predatory pricing behavior. These results
demonstrate that predatory pricing can emerge as a rational equilibrium
strategy across a broad variety of model settings. By providing equilibrium
analysis of finite-horizon dynamic oligopoly models with drop-outs, our study
answers a decade-old question and offers new insights for competition
authorities and regulators.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [3] [Category-Aware Semantic Caching for Heterogeneous LLM Workloads](https://arxiv.org/abs/2510.26835)
*Chen Wang,Xunzhuo Liu,Yue Zhu,Alaa Youssef,Priya Nagpurkar,Huamin Chen*

Main category: cs.DB

TL;DR: 本文提出了一种类别感知的语义缓存系统，针对LLM服务中不同查询类别的特性差异，通过动态调整相似度阈值、TTL和配额来优化缓存性能。


<details>
  <summary>Details</summary>
Motivation: 传统统一缓存策略在处理异构查询负载时效率低下，因为不同类别的查询在嵌入空间分布、内容陈旧度和重复模式上存在显著差异，导致缓存命中率分布呈现长尾特征。

Method: 采用混合架构，将内存中的HNSW搜索与外部文档存储分离，并实现基于类别的动态缓存策略，包括可变的相似度阈值、TTL和配额。

Result: 将缓存未命中成本从30ms降低到2ms，使得低命中率类别在经济上可行（盈亏平衡点从15-20%降至3-5%），理论上可将过载模型流量减少9-17%。

Conclusion: 类别感知的语义缓存能够有效覆盖整个工作负载分布，显著提高缓存效率和经济性，特别适合处理LLM服务中的异构查询负载。

Abstract: LLM serving systems process heterogeneous query workloads where different
categories exhibit different characteristics. Code queries cluster densely in
embedding space while conversational queries distribute sparsely. Content
staleness varies from minutes (stock data) to months (code patterns). Query
repetition patterns range from power-law (code) to uniform (conversation),
producing long tail cache hit rate distributions: high-repetition categories
achieve 40-60% hit rates while low-repetition or volatile categories achieve
5-15% hit rates. Vector databases must exclude the long tail because remote
search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of
production traffic uncached. Uniform cache policies compound this problem:
fixed thresholds cause false positives in dense spaces and miss valid
paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This
paper presents category-aware semantic caching where similarity thresholds,
TTLs, and quotas vary by query category. We present a hybrid architecture
separating in-memory HNSW search from external document storage, reducing miss
cost from 30ms to 2ms. This reduction makes low-hit-rate categories
economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage
across the entire workload distribution. Adaptive load-based policies extend
this framework to respond to downstream model load, dynamically adjusting
thresholds and TTLs to reduce traffic to overloaded models by 9-17% in
theoretical projections.

</details>


### [4] [SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification](https://arxiv.org/abs/2510.26840)
*Rocky Klopfenstein,Yang He,Andrew Tremante,Yuepeng Wang,Nina Narodytska,Haoze Wu*

Main category: cs.DB

TL;DR: 提出SpotIt评估方法，通过形式化有界等价验证引擎主动寻找能区分生成SQL与真实SQL的数据库，发现传统基于测试的评估方法过于乐观，常忽略查询间的实际差异。


<details>
  <summary>Details</summary>
Motivation: 当前基于测试的Text-to-SQL评估方法存在局限性，两个不同的SQL查询可能在测试数据库上产生相同结果，导致评估结果过于乐观，无法准确反映模型性能。

Method: 开发SpotIt评估管道，扩展现有验证器以支持更丰富的SQL子集，通过有界等价验证主动寻找能区分生成查询与真实查询的数据库。

Result: 在BIRD数据集上评估10种Text-to-SQL方法，发现基于测试的方法经常忽略生成查询与真实查询之间的差异，验证结果揭示了更复杂的评估现状。

Conclusion: SpotIt方法提供了更可靠的Text-to-SQL评估，揭示了传统测试方法的局限性，为领域进步提供了更准确的方向。

Abstract: Community-driven Text-to-SQL evaluation platforms play a pivotal role in
tracking the state of the art of Text-to-SQL performance. The reliability of
the evaluation process is critical for driving progress in the field. Current
evaluation methods are largely test-based, which involves comparing the
execution results of a generated SQL query and a human-labeled ground-truth on
a static test database. Such an evaluation is optimistic, as two queries can
coincidentally produce the same output on the test database while actually
being different. In this work, we propose a new alternative evaluation
pipeline, called SpotIt, where a formal bounded equivalence verification engine
actively searches for a database that differentiates the generated and
ground-truth SQL queries. We develop techniques to extend existing verifiers to
support a richer SQL subset relevant to Text-to-SQL. A performance evaluation
of ten Text-to-SQL methods on the high-profile BIRD dataset suggests that
test-based methods can often overlook differences between the generated query
and the ground-truth. Further analysis of the verification results reveals a
more complex picture of the current Text-to-SQL evaluation.

</details>


### [5] [The Impact of Data Compression in Real-Time and Historical Data Acquisition Systems on the Accuracy of Analytical Solutions](https://arxiv.org/abs/2510.26868)
*Reham Faqehi,Haya Alhuraib,Hamad Saiari,Zyad Bamigdad*

Main category: cs.DB

TL;DR: 本文评估了工业物联网环境中数据压缩机制与分析准确性之间的关系，发现过度压缩会丢失关键模式、扭曲统计指标并降低预测准确性，提出了在分析完整性和压缩效率之间取得平衡的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境产生海量实时和历史过程数据，压缩算法被广泛用于降低存储成本，但压缩带来的权衡可能影响依赖这些数据的工程分析的准确性和可靠性，需要理解这些权衡以制定支持运营效率和准确分析的数据策略。

Method: 通过理论分析、模拟信号压缩和实证评估，研究常见压缩机制对统计分析、异常检测和机器学习模型准确性的影响。

Result: 研究表明过度压缩会丢失关键模式、扭曲统计指标并降低预测准确性，压缩程度与分析方法之间存在复杂关系。

Conclusion: 需要在分析完整性和压缩效率之间找到平衡点，提出了优化方法和最佳实践来实现这种平衡。

Abstract: In industrial and IoT environments, massive amounts of real-time and
historical process data are continuously generated and archived. With sensors
and devices capturing every operational detail, the volume of time-series data
has become a critical challenge for storage and processing systems. Efficient
data management is essential to ensure scalability, cost-effectiveness, and
timely analytics. To minimize storage expenses and optimize performance, data
compression algorithms are frequently utilized in data historians and
acquisition systems. However, compression comes with trade-offs that may
compromise the accuracy and reliability of engineering analytics that depend on
this compressed data. Understanding these trade-offs is essential for
developing data strategies that support both operational efficiency and
accurate, reliable analytics. This paper assesses the relation of common
compression mechanisms used in real-time and historical data systems and the
accuracy of analytical solutions, including statistical analysis, anomaly
detection, and machine learning models. Through theoretical analysis, simulated
signal compression, and empirical assessment, we illustrate that excessive
compression can lose critical patterns, skew statistical measures, and diminish
predictive accuracy. The study suggests optimum methods and best practices for
striking a compromise between analytical integrity and compression efficiency.

</details>


### [6] [Unstructured Data Analysis using LLMs: A Comprehensive Benchmark](https://arxiv.org/abs/2510.27119)
*Qiyan Deng,Jianhui Li,Chengliang Chai,Jinqi Liu,Junzhi She,Kaisen Jin,Zhaoze Sun,Yuhao Deng,Jia Yuan,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.DB

TL;DR: UDA-Bench是首个针对非结构化数据分析系统的综合基准测试，包含5个领域的数据集和多样化查询工作负载，用于全面评估不同UDA系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、大规模、多样化的基准测试来评估不同非结构化数据分析系统在各种场景下的性能表现，这些系统在查询接口、优化策略和操作实现等方面存在显著差异。

Method: 组织30名研究生花费超过10,000小时手动标注5个不同领域的数据集，构建关系数据库作为基准测试的真实标签，并设计覆盖不同类型分析操作符的多样化查询。

Result: 创建了包含高质量数据集和丰富查询工作负载的基准测试，能够全面评估UDA系统的查询接口、查询优化、操作符设计和数据处理等关键构建模块。

Conclusion: UDA-Bench填补了非结构化数据分析系统评估的空白，为研究人员提供了全面评估不同UDA系统性能的有效工具。

Abstract: Nowadays, the explosion of unstructured data presents immense analytical
value. Leveraging the remarkable capability of large language models (LLMs) in
extracting attributes of structured tables from unstructured data, researchers
are developing LLM-powered data systems for users to analyze unstructured
documents as working with a database. These unstructured data analysis (UDA)
systems differ significantly in all aspects, including query interfaces, query
optimization strategies, and operator implementations, making it unclear which
performs best in which scenario. Unfortunately, there does not exist a
comprehensive benchmark that offers high-quality, large-volume, and diverse
datasets as well as rich query workload to thoroughly evaluate such systems. To
fill this gap, we present UDA-Bench, the first benchmark for unstructured data
analysis that meets all the above requirements. Specifically, we organize a
team with 30 graduate students that spends over in total 10,000 hours on
curating 5 datasets from various domains and constructing a relational database
view from these datasets by manual annotation. These relational databases can
be used as ground truth to evaluate any of these UDA systems despite their
differences in programming interfaces. Moreover, we design diverse queries to
analyze the attributes defined in the database schema, covering different types
of analytical operators with varying selectivities and complexities. We conduct
in-depth analysis of the key building blocks of existing UDA systems: query
interface, query optimization, operator design, and data processing. We run
exhaustive experiments over the benchmark to fully evaluate these systems and
different techniques w.r.t. the above building blocks.

</details>


### [7] [Compass: General Filtered Search across Vector and Structured Data](https://arxiv.org/abs/2510.27141)
*Chunxiao Ye,Xiao Yan,Eric Lo*

Main category: cs.DB

TL;DR: Compass是一个统一的框架，支持在向量和结构化数据上进行通用的过滤搜索，无需依赖新的索引设计，通过协调不同模态的候选生成和谓词评估来实现高效查询。


<details>
  <summary>Details</summary>
Motivation: 混合向量和关系数据的日益普遍需要高效支持结合高维向量搜索和复杂关系过滤的查询，但现有过滤搜索解决方案受限于专用索引，限制了任意过滤并与通用DBMS集成困难。

Method: Compass利用已建立的索引结构（如HNSW和IVF用于向量属性，B+树用于关系属性），实现原则性的协作查询执行策略，协调跨模态的候选生成和谓词评估。

Result: 综合实证评估表明，Compass在多样混合查询工作负载中始终优于现有的唯一高性能通用框架NaviX，同时在仅涉及单一属性的情况下匹配专用单属性索引的查询吞吐量。

Conclusion: Compass为在向量数据库系统中实现真正通用的过滤搜索提供了一个实用且稳健的解决方案，同时保持完全通用性和DBMS兼容性。

Abstract: The increasing prevalence of hybrid vector and relational data necessitates
efficient, general support for queries that combine high-dimensional vector
search with complex relational filtering. However, existing filtered search
solutions are fundamentally limited by specialized indices, which restrict
arbitrary filtering and hinder integration with general-purpose DBMSs. This
work introduces \textsc{Compass}, a unified framework that enables general
filtered search across vector and structured data without relying on new index
designs. Compass leverages established index structures -- such as HNSW and IVF
for vector attributes, and B+-trees for relational attributes -- implementing a
principled cooperative query execution strategy that coordinates candidate
generation and predicate evaluation across modalities. Uniquely, Compass
maintains generality by allowing arbitrary conjunctions, disjunctions, and
range predicates, while ensuring robustness even with highly-selective or
multi-attribute filters. Comprehensive empirical evaluations demonstrate that
Compass consistently outperforms NaviX, the only existing performant general
framework, across diverse hybrid query workloads. It also matches the query
throughput of specialized single-attribute indices in their favorite settings
with only a single attribute involved, all while maintaining full generality
and DBMS compatibility. Overall, Compass offers a practical and robust solution
for achieving truly general filtered search in vector database systems.

</details>


### [8] [ShapleyPipe: Hierarchical Shapley Search for Data Preparation Pipeline Construction](https://arxiv.org/abs/2510.27168)
*Jing Chang,Chang Liu,Jinbin Huang,Shuyuan Zheng,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: ShapleyPipe是一个基于博弈论Shapley值的数据准备管道自动构建框架，通过分层分解将搜索复杂度从指数级降低到多项式级，在减少24%评估次数的同时达到98.1%的高预算基线性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据准备管道自动构建方法存在两个根本限制：将管道构建视为黑盒优化而不量化单个算子的贡献，以及面临组合爆炸的搜索空间问题（N^M配置）。

Method: 采用分层分解方法，将类别级结构搜索与算子级优化分离；开发多臂老虎机机制进行智能类别评估，以及置换Shapley值来捕捉位置相关的算子交互。

Result: 在18个多样化数据集上的评估显示，ShapleyPipe达到98.1%的高预算基线性能，使用24%更少的评估，比最先进的强化学习方法性能提升3.6%。

Conclusion: ShapleyPipe不仅提供性能提升，还提供可解释的算子估值（与经验性能相关性ρ=0.933），支持数据驱动的管道分析和系统化的算子库优化。

Abstract: Automated data preparation pipeline construction is critical for machine
learning success, yet existing methods suffer from two fundamental limitations:
they treat pipeline construction as black-box optimization without quantifying
individual operator contributions, and they struggle with the combinatorial
explosion of the search space ($N^M$ configurations for N operators and
pipeline length M). We introduce ShapleyPipe, a principled framework that
leverages game-theoretic Shapley values to systematically quantify each
operator's marginal contribution while maintaining full interpretability. Our
key innovation is a hierarchical decomposition that separates category-level
structure search from operator-level refinement, reducing the search complexity
from exponential to polynomial. To make Shapley computation tractable, we
develop: (1) a Multi-Armed Bandit mechanism for intelligent category evaluation
with provable convergence guarantees, and (2) Permutation Shapley values to
correctly capture position-dependent operator interactions. Extensive
evaluation on 18 diverse datasets demonstrates that ShapleyPipe achieves 98.1\%
of high-budget baseline performance while using 24\% fewer evaluations, and
outperforms the state-of-the-art reinforcement learning method by 3.6\%. Beyond
performance gains, ShapleyPipe provides interpretable operator valuations
($\rho$=0.933 correlation with empirical performance) that enable data-driven
pipeline analysis and systematic operator library refinement.

</details>


### [9] [DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries](https://arxiv.org/abs/2510.27238)
*Chuxuan Hu,Maxwell Yang,James Weiland,Yeji Lim,Suhas Palawala,Daniel Kang*

Main category: cs.DB

TL;DR: DRAMA是一个端到端的数据分析范式，通过自然语言处理大规模开放域数据，统一了数据收集、转换和分析流程，在任务准确性和成本效益方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据科学工作流自动化系统无法同时支持开放域数据收集、结构化数据转换和分析推理这三个关键能力，手动数据分析效率低下。

Method: 提出DRAMA端到端范式，包含数据检索器（协调子代理执行数据收集和转换）和数据分析器（对检索数据进行结构化推理）的多代理系统DRAMA-Bot。

Result: 在DRAMA-Bench基准测试中，DRAMA-Bot达到86.5%的任务准确率，成本仅0.05美元，比现有最佳基线准确率高6.9倍，成本不到1/6。

Conclusion: DRAMA范式有效解决了开放域数据分析的挑战，在准确性和成本效益方面显著优于现有方法，为自动化数据科学工作流提供了可行方案。

Abstract: Manually conducting real-world data analyses is labor-intensive and
inefficient. Despite numerous attempts to automate data science workflows, none
of the existing paradigms or systems fully demonstrate all three key
capabilities required to support them effectively: (1) open-domain data
collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that
answers users' analytic queries in natural language on large-scale open-domain
data. DRAMA unifies data collection, transformation, and analysis as a single
pipeline. To quantitatively evaluate system performance on tasks representative
of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories
of tasks: claim verification and question answering, each comprising 100
instances. These tasks are derived from real-world applications that have
gained significant public attention and require the retrieval and analysis of
open-domain data. We develop DRAMA-Bot, a multi-agent system designed following
DRAMA. It comprises a data retriever that collects and transforms data by
coordinating the execution of sub-agents, and a data analyzer that performs
structured reasoning over the retrieved data. We evaluate DRAMA-Bot on
DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot
achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines
with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is
publicly available at https://github.com/uiuc-kang-lab/drama.

</details>


### [10] [Approximate Diverse $k$-nearest Neighbor Search in Vector Database](https://arxiv.org/abs/2510.27243)
*Jiachen Zhao,Xiao Yan,Eric Lo*

Main category: cs.DB

TL;DR: 提出了一种将结果多样化集成到近似k近邻搜索中的渐进式搜索框架，能够在用户指定的多样化水平下高效近似最优多样化结果集。


<details>
  <summary>Details</summary>
Motivation: 现有的基于贪心的多样化方法经常产生次优结果，无法充分近似特定多样化水平下的最优相似度得分，且需要能够适应不同用户定义结果大小和多样化需求的灵活算法。

Method: 渐进式搜索框架，包含迭代搜索、多样化和验证阶段，通过精心设计的多样化和验证步骤，无需额外索引开销即可高效近似最优多样化结果集。

Result: 在三个百万级基准数据集上的实验表明，该方法能够以最小的延迟开销持续检索接近最优的多样化结果，特别是在中等和高多样性设置下表现优异。

Conclusion: 该方法成功解决了近似k近邻搜索中结果多样化的问题，提供了一种高效且灵活的解决方案，能够满足用户对多样化结果集的需求。

Abstract: Approximate $k$-nearest neighbor search (A$k$-NNS) is a core operation in
vector databases, underpinning applications such as retrieval-augmented
generation (RAG) and image retrieval. In these scenarios, users often prefer
diverse result sets to minimize redundancy and enhance information value.
However, existing greedy-based diverse methods frequently yield sub-optimal
results, failing to adequately approximate the optimal similarity score under
certain diversification level. Furthermore, there is a need for flexible
algorithms that can adapt to varying user-defined result sizes and diversity
requirements.
  To address these challenges, we propose a novel approach that seamlessly
integrates result diversification into state-of-the-art (SOTA) A$k$-NNS
methods. Our approach introduces a progressive search framework, consisting of
iterative searching, diversification, and verification phases. Carefully
designed diversification and verification steps enable our approach to
efficiently approximate the optimal diverse result set according to
user-specified diversification levels without additional indexing overhead.
  We evaluate our method on three million-scale benchmark datasets, LAION-art,
Deep1M, and Txt2img, using latency, similarity, and recall as performance
metrics across a range of $k$ values and diversification thresholds.
Experimental results demonstrate that our approach consistently retrieves
near-optimal diverse results with minimal latency overhead, particularly under
medium and high diversity settings.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [11] [Multi-hop Parallel Image Semantic Communication for Distortion Accumulation Mitigation](https://arxiv.org/abs/2510.26844)
*Bingyan Xie,Jihong Park,Yongpeng Wu,Wenjun Zhang,Tony Quek*

Main category: cs.IT

TL;DR: 提出了多跳并行图像语义通信框架，通过在每个跳点引入并行残差补偿链路来对抗失真累积，并设计了粗到细的残差压缩方案来最小化传输带宽开销。


<details>
  <summary>Details</summary>
Motivation: 现有的语义通信方案主要关注单跳场景，忽视了多跳无线图像传输的挑战。由于语义通信本质上是失真的，失真会在多跳中累积，导致显著的性能下降。

Method: 提出MHPSC框架，在每个跳点引入并行残差补偿链路。设计了粗到细的残差压缩方案：深度学习残差压缩器先压缩残差，然后自适应算术编码进一步压缩。残差分布估计模块预测先验分布以实现精细压缩。

Result: 实验结果表明，MHPSC在仅略微增加传输带宽的情况下，优于现有的语义通信和传统分离编码方案。

Conclusion: 该方法确保了鲁棒的多跳图像传输，仅带来较小的传输带宽增加，在多跳语义通信中表现出优越性能。

Abstract: Existing semantic communication schemes primarily focus on single-hop
scenarios, overlooking the challenges of multi-hop wireless image transmission.
As semantic communication is inherently lossy, distortion accumulates over
multiple hops, leading to significant performance degradation. To address this,
we propose the multi-hop parallel image semantic communication (MHPSC)
framework, which introduces a parallel residual compensation link at each hop
against distortion accumulation. To minimize the associated transmission
bandwidth overhead, a coarse-to-fine residual compression scheme is designed. A
deep learning-based residual compressor first condenses the residuals, followed
by the adaptive arithmetic coding (AAC) for further compression. A residual
distribution estimation module predicts the prior distribution for the AAC to
achieve fine compression performances. This approach ensures robust multi-hop
image transmission with only a minor increase in transmission bandwidth.
Experimental results confirm that MHPSC outperforms both existing semantic
communication and traditional separated coding schemes.

</details>


### [12] [Inferring the Chemotaxis Distortion Function from Cellular Decision Strategies](https://arxiv.org/abs/2510.26988)
*Fardad Vakilipoor,Johannes Konrad,Maximilian Schäfer*

Main category: cs.IT

TL;DR: 该论文提出了逆Blahut-Arimoto算法(IBAA)来量化细胞在不确定性下的决策标准，通过信息论框架分析细胞如何处理环境信号并做出上下文依赖的决策。


<details>
  <summary>Details</summary>
Motivation: 研究细胞如何在噪声信号通路中处理不确定性，理解细胞智能如何使细胞能够处理环境信号并做出上下文依赖的决策，如化学趋向性中的梯度导航。

Method: 应用基于率失真理论(RDT)的信息论框架，提出逆Blahut-Arimoto算法(IBAA)来计算失真函数，量化系统的决策标准。使用局部激发全局抑制(LEGI)模型模拟化学趋向性响应。

Result: 通过细胞凋亡场景准确估计了理论失真函数，在化学趋向性模拟中从细胞角度计算了失真函数，揭示了细胞的状态依赖性决策标准。

Conclusion: 该通用框架可扩展到生物和工程系统中需要高效信息处理的情况，为理解细胞在不确定性下的决策机制提供了新视角。

Abstract: Cellular intelligence enables cells to process environmental signals and make
context-dependent decisions, as exemplified by chemotaxis, where cells navigate
chemical gradients despite noisy signaling pathways. To investigate how cells
deal with uncertainty, we apply an information-theoretic framework based on
rate distortion theory (RDT). The Blahut-Arimoto algorithm (BAA) computes
optimal decision strategies that minimize mutual information while satisfying
distortion constraints, balancing sensing accuracy with distortion constraint
equivalent to resource cost. We propose the inverse Blahut-Arimoto algorithm
(IBAA) to compute the distortion function, which quantifies the system's
decision-making criteria for realizing a decision strategy to map input signals
to outputs. This general framework extends beyond chemotaxis to biological and
engineered systems requiring efficient information processing under
uncertainty. We validate the proposed IBAA by accurately estimating theoretical
distortion functions in a cellular apoptosis scenario. Additionally, using the
local excitation global inhibition (LEGI) model to simulate chemotactic
responses, we compute the distortion functions from the cell's perspective. Our
finding reveals a state-dependent decision criteria by the cell.

</details>


### [13] [Multilevel constructions of constant dimension codes based on one-factorization of complete graphs](https://arxiv.org/abs/2510.27071)
*Dengming Xu,Mengmeng LI*

Main category: cs.IT

TL;DR: 该论文通过多级构造方法改进常数维码，利用与完全图一因子化相关的二进制向量变换选择骨架码，使用准挂起块构造CDCs，并改进了特定参数下的下界。


<details>
  <summary>Details</summary>
Motivation: 常数维码在随机网络编码中有重要应用，多级构造是构建常数维码的最有效方法之一。本文旨在通过改进的多级构造方法来提升常数维码的性能。

Method: 首先基于与完全图一因子化相关的二进制向量变换选择适当的骨架码；然后使用准挂起块构造常数维码；最后利用已知的最优Ferrers图秩度量码构造来计算维度。

Result: 改进了参数$\overline{A}_q(n,8,6)$在$16\leq n\leq 19$范围内的下界。

Conclusion: 提出的多级构造方法能有效改进常数维码的下界，特别是在特定参数范围内取得了更好的性能。

Abstract: Constant dimension codes (CDCs) have become an important object in coding
theory due to their application in random network coding. The multilevel
construction is one of the most effective ways to construct constant dimension
codes. The paper is devoted to constructing CDCs by the multilevel
construction. Precisely, we first choose an appropriate skeleton code based on
the transformations of binary vectors related to the one-factorization of
complete graphs; then we construct CDCs by using the chosen skeleton code,
where quasi-pending blocks are used; finally, we calculate the dimensions by
use of known constructions of optimal Ferrers diagram rank metric codes. As
applications, we improve the lower bounds of $\overline{A}_q(n,8,6)$ for
$16\leq n\leq 19.$

</details>


### [14] [Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks](https://arxiv.org/abs/2510.27147)
*Gaoyuan Zhang,Ruisong Si,Boyuan Li,Zijian Li,Baofeng Ji,Chenqi Zhu,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 提出一种针对RIS增强移动窃听攻击的MIMO无线网络安全通信方案，通过随机比特翻转和SVD预编码来最小化窃听者与秘密消息之间的互信息，无需窃听者完整瞬时信道状态信息。


<details>
  <summary>Details</summary>
Motivation: 解决RIS增强移动窃听攻击对MIMO无线网络的安全威胁，传统方法需要窃听者完整信道状态信息，这在现实中难以获取。

Method: 采用随机比特翻转方案结合SVD预编码策略，将数据处理视为通信信道，优化功率分配，同时考虑窃听者视角的RIS相位偏移设计。

Result: 提出的安全通信方案在实际中具有可行性，无需窃听者完整瞬时信道状态信息，在各种攻击场景下都表现出有效性和鲁棒性。

Conclusion: 该方案为MIMO无线网络中的RIS增强窃听攻击提供了轻量级且有效的安全通信解决方案，具有实际应用价值。

Abstract: We pay our attention towards secure and robust communication in the presence
of a Reconfigurable Intelligent Surface (RIS)-enhanced mobile eavesdropping
attacker in Multiple-Input Multiple-Output (MIMO)wireless
networks.Specifically,we first provide a unifying framework that generalizes
specific intelligent wiretap model wherein the passive eavesdropper configured
with any number of antennas is potentially mobile and can actively optimize its
received signal strength with the help of RIS by intelligently manipulating
wiretap channel characteristics.To effectively mitigate this intractable
threat,we then propose a novel and lightweight secure communication scheme from
the perspective of information theory.The main idea is that the data processing
can in some cases be observed as communication channel,and a random
bit-flipping scheme is then carefully involved for the legitimate transmitter
to minimize the mutual information between the secret message and the passive
eavesdropper's received data.The Singular Value Decomposition (SVD)-based
precoding strategy is also implemented to optimize power allocation,and thus
ensure that the legitimate receiver is not subject to interference from this
random bit-flipping.The corresponding results depict that our secure
communication scheme is practically desired, which does not require any a prior
knowledge of the eavesdropper's full instantaneous Channel State Information
(ICSI). Furthermore,we consider the RIS optimization problem from the
eavesdropper's perspective,and provide RIS phase shift design solutions under
different attacking scenarios.Finally,the optimal detection schemes
respectively for the legitimate user and the eavesdropper are provided,and
comprehensive simulations are presented to verify our theoretical analysis and
show the effectiveness and robustness of our secure communication scheme across
a wide range of attacking scenarios.

</details>


### [15] [Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective](https://arxiv.org/abs/2510.27175)
*Gaoyuan Zhang,Gaolei Song,Boyuan Li,Zijian Li,Baofeng Ji,Ruijuan Zheng,Guoqiang Zheng,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 研究了RIS增强和中继辅助的协作频谱感知中的拜占庭攻击，提出了不依赖全局瞬时信道状态信息和决策融合规则的统一攻击框架。


<details>
  <summary>Details</summary>
Motivation: 在移动认知无线电网络中，RIS和中继辅助的协作频谱感知面临拜占庭攻击威胁，传统方法严重依赖全局信道信息和决策规则，不实用。

Method: 构建RIS增强和中继辅助的CSS配置，开发信道和攻击感知的硬决策融合规则，推导小规模和大规模攻击场景下的最优拜占庭攻击策略。

Result: 最优攻击策略不需要全局瞬时信道信息，攻击效果主要取决于拜占庭节点比例而非信道动态，攻击策略可能不唯一但可通过统一框架实现。

Conclusion: 成功减轻了对全局信道信息和决策融合规则的依赖，使信道感知方法更实用，并通过仿真验证了理论分析。

Abstract: From the perspective of hard decision fusion, we investigate Byzantine
attacks in Reconfigurable Intelligent Surface (RIS)-enhanced and
decode-and-forward relay-assisted Cooperative Spectrum Sensing (CSS) for mobile
Cognitive Radio Networks (CRNs) in this paper. Specially, a RIS-enhanced and
decode-and-forward relay-assisted CSS configuration is first constructed under
dynamic channel scenarios due to user mobility. Subsequently, the channel- and
attack-aware hard decision fusion rules are developed, and the optimal
channel-aware Byzantine attack strategies are then developed under both
small-scale and large-scale attacking scenarios. The corresponding results
depict that the optimal attack strategy does not require any a prior knowledge
of the global instantaneous Channel State Information (ICSI) (e.g. false alarm
probability and detection probability of all the secondary users), although
perfect acquisition of ICSI is clearly always not affordable from the attacker
perspective, which is further exacerbated by the RIS and decode-and-forward
relays involved in CSS and the potential high mobility of secondary users that
leads to fast fading channels. Furthermore, our counterintuitive results also
indicate that, regardless of the attacker's awareness of the decision fusion
rule, the optimal Byzantine attack can be achieved through a unifying
framework, the explicit attack strategy may be not unique, and the attacking
effectiveness is primarily determined by the fraction of the Byzantine nodes
rather than the channel dynamics. That is, to make the channel-aware approach
more practical, the challenge that the heavy reliance on the global ICSI and
decision fusion rule in obtaining the Byzantine attacks is successfully
relaxed. Finally, we empirically validate our theoretical analysis through
extensive simulations across a wide range of attacking scenarios.

</details>


### [16] [Dual-Scale Antenna Deployment for Pinching Antenna Systems](https://arxiv.org/abs/2510.27185)
*Xu Gan,Zhaolin Wang,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出了一种用于夹持天线系统的双尺度部署框架，包含粗调阶段和精调阶段，通过联合优化传输预编码、天线辐射功率和天线部署来最大化能效。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统天线系统能效低的问题，需要开发更高效的天线部署和优化方法。

Method: 采用双尺度部署框架，包含粗调和精调阶段，提出四种天线部署协议，并开发基于惩罚的交替优化算法来解决非凸耦合问题。

Result: 仿真验证了理论结果的准确性，PASS比传统无蜂窝架构提高约70%能效，比MIMO系统提高近两倍。

Conclusion: DSD分辨率和部署协议对实现PASS最大能效至关重要，所提算法具有低复杂度和良好收敛性。

Abstract: A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)
is proposed. 1) In the first coarse stage, the pinching antenna (PA) is
transferred over a large-scale range at the waveguide level. 2) The refinement
stage performs small-scale relocation of the PA with high precision. Four PA
deployment protocols are provided in the proposed DSD framework. Then, a
practical power consumption model is proposed, based on which the theoretical
energy efficiency formulas for PASS are derived. The transmit precoding, PA
radiation power, and PA deployment are jointly optimized to maximize the energy
efficiency under the provided PA deployment protocols. To solve this
non-convex, highly coupled problem, a low-complexity penalty-based alternating
optimization algorithm is proposed. Simulation results validate the accuracy of
theoretical results and the convergence of the proposed algorithm. It is
demonstrated that: 1) PASS delivers about 70% higher energy efficiency than the
conventional cell-free architecture and nearly twofold improvement relative to
MIMO systems; 2) it is essential to specify the DSD resolution and deployment
protocol to achieve the maximum energy efficiency for PASS.

</details>


### [17] [Cross-Band Channel Impulse Response Prediction: Leveraging 3.5 GHz Channels for Upper Mid-Band](https://arxiv.org/abs/2510.27349)
*Fan-Hao Lin,Chi-Jui Sung,Chu-Hsiang Huang,Hui Chen,Chao-Kai Wen,Henk Wymeersch*

Main category: cs.IT

TL;DR: 提出了CIR-UNext深度学习框架，利用3.5 GHz信道脉冲响应预测7 GHz信道，解决了6G网络中高频段信道预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络中7-24 GHz频段的穿透损耗和阻塞问题严重，传统射线追踪计算量大且高频数据采集成本高，需要高效的跨频段信道预测方法。

Method: 集成射线追踪数据集管道与注意力U-Net变体，分别预测增益和相位，并扩展为Channel2ComMap基础模型用于MIMO-OFDM系统吞吐量预测。

Result: AU-Net-Aux模型在未见复杂环境中实现中值增益误差0.58 dB和相位预测误差0.27 rad，在吞吐量预测方面优于现有方法。

Conclusion: CIR-UNext为6G网络提供了高效可扩展的跨频段预测解决方案，支持定位、波束管理、数字孪生和智能资源分配等应用。

Abstract: Accurate cross-band channel prediction is essential for 6G networks,
particularly in the upper mid-band (FR3, 7--24 GHz), where penetration loss and
blockage are severe. Although ray tracing (RT) provides high-fidelity modeling,
it remains computationally intensive, and high-frequency data acquisition is
costly. To address these challenges, we propose CIR-UNext, a deep learning
framework designed to predict 7 GHz channel impulse responses (CIRs) by
leveraging abundant 3.5 GHz CIRs. The framework integrates an RT-based dataset
pipeline with attention U-Net (AU-Net) variants for gain and phase prediction.
The proposed AU-Net-Aux model achieves a median gain error of 0.58 dB and a
phase prediction error of 0.27 rad on unseen complex environments. Furthermore,
we extend CIR-UNext into a foundation model, Channel2ComMap, for throughput
prediction in MIMO-OFDM systems, demonstrating superior performance compared
with existing approaches. Overall, CIR-UNext provides an efficient and scalable
solution for cross-band prediction, enabling applications such as localization,
beam management, digital twins, and intelligent resource allocation in 6G
networks.

</details>


### [18] [Weight Enumerators From Equivalence Relations and MacWilliams Identities](https://arxiv.org/abs/2510.27358)
*S. T. Dougherty,C. Fernández-Córdoba*

Main category: cs.IT

TL;DR: 本文研究了有限域、有限阿贝尔群和有限Frobenius环上的码，定义了基于等价关系的权重枚举器，并确定了MacWilliams关系成立的条件。


<details>
  <summary>Details</summary>
Motivation: 研究不同类型码的权重枚举器及其MacWilliams关系，扩展传统Hamming权重枚举器和完全权重枚举器的适用范围。

Method: 定义基于等价关系的权重枚举器，分析其在各种代数结构（有限域、有限阿贝尔群、有限Frobenius环）上的性质，并确定MacWilliams关系成立的条件。

Result: 确定了在哪些情况下基于等价关系的权重枚举器满足MacWilliams关系，并对特定等价关系的权重枚举器进行了研究。

Conclusion: 成功扩展了权重枚举器的概念，为不同代数结构上的码提供了统一的权重枚举器框架，并建立了相应的MacWilliams关系理论。

Abstract: In this paper, we consider codes over finite fields, finite abelian groups,
and finite Frobenius rings. For such codes, the complete weight enumerator and
the Hamming weight enumerator serve as powerful tools. These two types of
weight enumerators satisfy the MacWilliams relations. We define the weight
enumerator of a code with respect to an equivalence relation and determine in
which cases the MacWilliams relations hold for this weight enumerator. We also
study some weight enumerators for specific equivalence relations.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [19] [Inclusive and Exclusive Vertex Splitting into Specific Graph Classes: NP Hardness and Algorithms](https://arxiv.org/abs/2510.26938)
*Ajinkya Gaikwad,Hitendra Kumar,S. Padmapriya,Praneet Kumar Patra,Harsh Sanklecha,Soumen Maity*

Main category: cs.DS

TL;DR: 研究了F-顶点分割问题，该问题涉及通过最多k次顶点分割将图G转换为属于特定图类F的图G'。分析了星座图、环图、线性森林和二分图等目标图类，发现环图和线性森林的问题可在多项式时间内解决，而星座图和二分图的问题则是NP完全的。


<details>
  <summary>Details</summary>
Motivation: 研究图修改问题中的顶点分割操作，探索不同图类在顶点分割变换下的计算复杂性，为图变换理论提供新的复杂性分析结果。

Method: 分析顶点分割问题的两种变体（包含性和排他性），针对星座图、环图、线性森林和二分图等具体图类，分别研究其计算复杂性。

Result: 环图和线性森林的F-顶点分割问题在多项式时间内可解；星座图和二分图的F-顶点分割问题是NP完全的。

Conclusion: 顶点分割问题的计算复杂性高度依赖于目标图类的性质，某些图类（如环图、线性森林）具有高效算法，而其他图类（如星座图、二分图）则具有计算困难性。

Abstract: We study a family of graph modification problems called the F-Vertex
Splitting problem. Given a graph G, the task is to determine whether G can be
transformed into a graph G-prime belonging to a graph class F through a
sequence of at most k vertex splits. We investigate this problem for several
target graph classes, namely constellations, cycle graphs, linear forests, and
bipartite graphs. We analyze both inclusive and exclusive variants of vertex
splitting, as introduced by Abu-Khzam and collaborators (ISCO 2018). Our
results show that the F-Vertex Splitting problem is polynomial-time solvable
when F is a cycle graph or a linear forest, for both variants. In contrast,
when F is a constellation or a bipartite graph, the problem is NP-complete for
both variants.

</details>


### [20] [Green Bin Packing](https://arxiv.org/abs/2510.26968)
*Jackson Bibbens,Cooper Sigrist,Bo Sun,Shahin Kamali,Mohammad Hajiesmaili*

Main category: cs.DS

TL;DR: 提出了绿色装箱问题，这是一种在线装箱变体，当箱子填充超过固定水平G时会产生线性成本β。当βG≤1时，经典算法表现良好；当βG>1时，新算法能改进性能。


<details>
  <summary>Details</summary>
Motivation: 云计算的可持续性和过度承诺问题促使需要建模高利用率服务器的成本。

Method: 引入绿色装箱问题，提出经典在线装箱算法的变体，并建立理论界限和实证测试。

Result: 当βG≤1时，FirstFit或Harmonic等经典算法表现良好，竞争比低于经典设置；当βG>1时，新算法能改进最坏情况和典型性能。

Conclusion: 绿色装箱问题为高利用率服务器成本建模提供了新框架，在不同参数条件下需要不同的算法策略。

Abstract: The online bin packing problem and its variants are regularly used to model
server allocation problems. Modern concerns surrounding sustainability and
overcommitment in cloud computing motivate bin packing models that capture
costs associated with highly utilized servers. In this work, we introduce the
green bin packing problem, an online variant with a linear cost $\beta$ for
filling above a fixed level $G$. For a given instance, the goal is to minimize
the sum of the number of opened bins and the linear cost. We show that when
$\beta G \le 1$, classical online bin packing algorithms such as FirstFit or
Harmonic perform well, and can achieve competitive ratios lower than in the
classic setting. However, when $\beta G > 1$, new algorithmic solutions can
improve both worst-case and typical performance. We introduce variants of
classic online bin packing algorithms and establish theoretical bounds, as well
as test their empirical performance.

</details>


### [21] [A Simple Deterministic Reduction From Gomory-Hu Tree to Maxflow and Expander Decomposition](https://arxiv.org/abs/2510.27330)
*Maximilian Probst Gutenberg,Weixuan Yuan*

Main category: cs.DS

TL;DR: 提出了一种简单高效的随机化归约方法，将Gomory-Hu树的构建问题归约为多对数次最大流计算，在无权图中达到接近最优的复杂度。


<details>
  <summary>Details</summary>
Motivation: Gomory-Hu树能够精确保存图的所有点对最小割信息，但传统构建方法效率较低。本文旨在开发更高效的构建算法，特别是在无权图和超图上的应用。

Method: 使用随机化归约技术，将Gomory-Hu树的构建问题转化为多对数次最大流计算。在无权图中，归约到总实例规模为Õ(m)的图，在加权图中扩展到Õ(n²)规模。

Result: 在无权图中，算法仅需Õ(m)额外时间；在加权图中，实例规模和运行时间增加到Õ(n²)。该归约方法首次达到多对数因子的紧界，并可扩展到超图的Gomory-Hu树构建。

Conclusion: 提出的归约方法是构建Gomory-Hu树的高效技术，在无权图和超图上达到接近最优的复杂度，为相关算法设计提供了新的工具。

Abstract: Given an undirected graph $G=(V,E,w)$, a Gomory-Hu tree $T$ (Gomory and Hu,
1961) is a tree on $V$ that preserves all-pairs mincuts of $G$ exactly.
  We present a simple and efficient randomized reduction from Gomory-Hu trees
to polylog maxflow computations. On unweighted graphs, our reduction reduces to
maxflow computations on graphs of total instance size $\tilde{O}(m)$ and the
algorithm requires only $\tilde{O}(m)$ additional time. Our reduction is the
first that is tight up to polylog factors. The reduction also seamlessly
extends to weighted graphs, however, instance sizes and runtime increase to
$\tilde{O}(n^2)$.
  Finally, we show how to extend our reduction to reduce Gomory-Hu trees for
unweighted hypergraphs to maxflow in hypergraphs. Again, our reduction is the
first that is tight up to polylog factors.

</details>


### [22] [Learned Static Function Data Structures](https://arxiv.org/abs/2510.27588)
*Stefan Hermann,Hans-Peter Lehmann,Giorgio Vinciguerra,Stefan Walzer*

Main category: cs.DS

TL;DR: 提出了一种基于机器学习的静态函数数据结构，通过模型预测键值对的概率分布，使用键特定的前缀编码来压缩存储值，突破了零阶熵的存储限制。


<details>
  <summary>Details</summary>
Motivation: 传统静态函数数据结构虽然比哈希表节省内存，但只能逼近值序列的零阶经验熵。希望利用键值之间的相关性来进一步压缩存储空间。

Method: 为每个键使用机器学习模型预测值的概率分布，基于该分布生成键特定的前缀编码来紧凑地存储真实值，然后将编码存储在经典静态函数数据结构中。

Result: 在真实数据上节省了高达一个数量级的空间，在合成数据上节省了高达三个数量级的空间。

Conclusion: 学习型静态函数通过利用键值相关性成功突破了零阶熵的存储限制，在保持点查询功能的同时实现了显著的空间节省。

Abstract: We consider the task of constructing a data structure for associating a
static set of keys with values, while allowing arbitrary output values for
queries involving keys outside the set. Compared to hash tables, these
so-called static function data structures do not need to store the key set and
thus use significantly less memory. Several techniques are known, with
compressed static functions approaching the zero-order empirical entropy of the
value sequence. In this paper, we introduce learned static functions, which use
machine learning to capture correlations between keys and values. For each key,
a model predicts a probability distribution over the values, from which we
derive a key-specific prefix code to compactly encode the true value. The
resulting codeword is stored in a classic static function data structure. This
design allows learned static functions to break the zero-order entropy barrier
while still supporting point queries. Our experiments show substantial space
savings: up to one order of magnitude on real data, and up to three orders of
magnitude on synthetic data.

</details>


### [23] [Rateless Bloom Filters: Set Reconciliation for Divergent Replicas with Variable-Sized Elements](https://arxiv.org/abs/2510.27614)
*Pedro Silva Gomes,Carlos Baquero*

Main category: cs.DS

TL;DR: 提出了一种用于集合协调的两阶段混合协议，通过引入动态的Rateless Bloom Filter来解决传统方法在处理可变大小元素和大规模差异时的效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统集合协调协议假设固定大小元素且差异数量很小，当处理可变大小元素或大规模差异（如网络分区后）时效率低下。

Method: 采用两阶段混合协议：首先使用Rateless Bloom Filter（RBF）动态适应任意对称差异，无需预先参数化；然后结合IBLT协议。

Result: 在可变大小元素集合中，当Jaccard指数低于85%时，RBF-IBLT混合协议相比最先进方法将总通信成本降低超过20%。

Conclusion: RBF-IBLT混合协议有效解决了集合协调中处理可变大小元素和大规模差异的挑战，显著提升了通信效率。

Abstract: Set reconciliation protocols typically make two critical assumptions: they
are designed for fixed-sized elements and they are optimized for when the
difference cardinality, d, is very small. When adapting to variable-sized
elements, the current practice is to synchronize fixed-size element digests.
However, when the number of differences is considerable, such as after a
network partition, this approach can be inefficient. Our solution is a
two-stage hybrid protocol that introduces a preliminary Bloom filter step,
specifically designed for this regime. The novelty of this approach, however,
is in solving a core technical challenge: determining the optimal Bloom filter
size without knowing d. Our solution is the Rateless Bloom Filter (RBF), a
dynamic filter that naturally adapts to arbitrary symmetric differences,
closely matching the communication complexity of an optimally configured static
filter without requiring any prior parametrization. Our evaluation in sets of
variable-sized elements shows that for Jaccard indices below 85%, our RBF-IBLT
hybrid protocol reduces the total communication cost by up to over 20% compared
to the state-of-the-art.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [24] [Evaluating Perspectival Biases in Cross-Modal Retrieval](https://arxiv.org/abs/2510.26861)
*Teerapol Saengsukhiran,Peerawat Chomphooyod,Narabodee Rodjananant,Chompakorn Chaksangchaichot,Patawee Prakrankamanant,Witthawin Sripheanpol,Pak Lovichit,SarChaksaana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.IR

TL;DR: 该论文研究了多模态检索系统中的两种偏见：流行偏见（图像到文本检索中偏向主流语言）和关联偏见（文本到图像检索中偏向文化关联图像），发现显式对齐能有效缓解流行偏见，但关联偏见是更复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态检索系统理论上应在语义空间中运作，不受查询语言或文化背景影响，但实际上存在系统性的视角偏见，由语言流行度和文化关联塑造。

Method: 研究两种具体偏见：流行偏见（图像到文本检索中偏向主流语言）和关联偏见（文本到图像检索中偏向文化关联图像），并评估不同缓解策略的效果。

Result: 显式对齐是缓解流行偏见的有效策略，但关联偏见仍然是一个独特且更具挑战性的问题。

Conclusion: 实现真正公平的多模态系统需要超越简单数据扩展的有针对性策略，由文化关联产生的偏见比语言流行度产生的偏见更具挑战性。

Abstract: Multimodal retrieval systems are expected to operate in a semantic space,
agnostic to the language or cultural origin of the query. In practice, however,
retrieval outcomes systematically reflect perspectival biases: deviations
shaped by linguistic prevalence and cultural associations. We study two such
biases. First, prevalence bias refers to the tendency to favor entries from
prevalent languages over semantically faithful entries in image-to-text
retrieval. Second, association bias refers to the tendency to favor images
culturally associated with the query over semantically correct ones in
text-to-image retrieval. Results show that explicit alignment is a more
effective strategy for mitigating prevalence bias. However, association bias
remains a distinct and more challenging problem. These findings suggest that
achieving truly equitable multimodal systems requires targeted strategies
beyond simple data scaling and that bias arising from cultural association may
be treated as a more challenging problem than one arising from linguistic
prevalence.

</details>


### [25] [A Survey on Generative Recommendation: Data, Model, and Tasks](https://arxiv.org/abs/2510.27157)
*Min Hou,Le Wu,Yuxin Liao,Yonghui Yang,Zhen Zhang,Changlong Zheng,Han Wu,Richang Hong*

Main category: cs.IR

TL;DR: 该调查系统分析了生成式推荐系统的新范式，通过数据、模型和任务三个维度构建统一框架，探讨了生成模型在推荐领域的应用、优势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型（特别是大语言模型和扩散模型）的兴起，推荐系统正经历从判别式评分到生成式任务的范式转变，需要系统性地梳理这一新兴领域。

Method: 采用统一的三元框架：数据层面（知识增强和智能体模拟）、模型层面（LLM方法、大推荐模型和扩散方法）、任务层面（对话交互、可解释推理和个性化内容生成）。

Result: 识别了生成式推荐的五大优势：世界知识整合、自然语言理解、推理能力、规模法则和创造性生成，同时揭示了基准设计、模型鲁棒性和部署效率等挑战。

Conclusion: 生成式推荐系统将重塑人机信息交互，朝着智能推荐助手的方向发展，但仍需解决实际部署中的关键挑战。

Abstract: Recommender systems serve as foundational infrastructure in modern
information ecosystems, helping users navigate digital content and discover
items aligned with their preferences. At their core, recommender systems
address a fundamental problem: matching users with items. Over the past
decades, the field has experienced successive paradigm shifts, from
collaborative filtering and matrix factorization in the machine learning era to
neural architectures in the deep learning era. Recently, the emergence of
generative models, especially large language models (LLMs) and diffusion
models, have sparked a new paradigm: generative recommendation, which
reconceptualizes recommendation as a generation task rather than discriminative
scoring. This survey provides a comprehensive examination through a unified
tripartite framework spanning data, model, and task dimensions. Rather than
simply categorizing works, we systematically decompose approaches into
operational stages-data augmentation and unification, model alignment and
training, task formulation and execution. At the data level, generative models
enable knowledge-infused augmentation and agent-based simulation while unifying
heterogeneous signals. At the model level, we taxonomize LLM-based methods,
large recommendation models, and diffusion approaches, analyzing their
alignment mechanisms and innovations. At the task level, we illuminate new
capabilities including conversational interaction, explainable reasoning, and
personalized content generation. We identify five key advantages: world
knowledge integration, natural language understanding, reasoning capabilities,
scaling laws, and creative generation. We critically examine challenges in
benchmark design, model robustness, and deployment efficiency, while charting a
roadmap toward intelligent recommendation assistants that fundamentally reshape
human-information interaction.

</details>


### [26] [A Survey on Deep Text Hashing: Efficient Semantic Text Retrieval with Binary Representation](https://arxiv.org/abs/2510.27232)
*Liyang He,Zhenya Huang,Cheng Yang,Rui Li,Zheng Zhang,Kai Zhang,Zhi Li,Qi Liu,Enhong Chen*

Main category: cs.IR

TL;DR: 本文综述了深度文本哈希技术，该方法将文本转换为紧凑的二进制哈希码，通过汉明距离计算加速语义相似性检索并降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 随着互联网文本内容的快速增长，大规模语义文本检索的需求日益增长，需要高效的哈希方法来加速相似性计算和减少存储开销。

Method: 基于深度神经网络直接从数据中学习紧凑且语义丰富的二进制表示，通过语义提取、哈希码质量保持等核心技术克服传统方法的性能限制。

Result: 深度文本哈希方法相比传统数据无关哈希技术展现出显著优势，在多个流行数据集上进行了详细评估。

Conclusion: 讨论了关键挑战和未来研究方向，包括与大型语言模型的集成以进一步推动该领域发展。

Abstract: With the rapid growth of textual content on the Internet, efficient
large-scale semantic text retrieval has garnered increasing attention from both
academia and industry. Text hashing, which projects original texts into compact
binary hash codes, is a crucial method for this task. By using binary codes,
the semantic similarity computation for text pairs is significantly accelerated
via fast Hamming distance calculations, and storage costs are greatly reduced.
With the advancement of deep learning, deep text hashing has demonstrated
significant advantages over traditional, data-independent hashing techniques.
By leveraging deep neural networks, these methods can learn compact and
semantically rich binary representations directly from data, overcoming the
performance limitations of earlier approaches. This survey investigates current
deep text hashing methods by categorizing them based on their core components:
semantic extraction, hash code quality preservation, and other key
technologies. We then present a detailed evaluation schema with results on
several popular datasets, followed by a discussion of practical applications
and open-source tools for implementation. Finally, we conclude by discussing
key challenges and future research directions, including the integration of
deep text hashing with large language models to further advance the field. The
project for this survey can be accessed at
https://github.com/hly1998/DeepTextHashing.

</details>


### [27] [Traceable Drug Recommendation over Medical Knowledge Graphs](https://arxiv.org/abs/2510.27274)
*Yu Lin,Zhen Jia,Philipp Christmann,Xu Zhang,Shengdong Du,Tianrui Li*

Main category: cs.IR

TL;DR: TraceDR是一个基于医学知识图谱的药物推荐系统，通过多任务学习框架同时预测药物推荐和相关证据，实现推荐的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在药物推荐中缺乏对推荐推导过程的解释，这在医疗等高风险应用中是一个关键限制。

Method: 在医学知识图谱上操作，采用多任务学习框架同时预测药物推荐和相关证据，并开发了自动构建患者健康记录的框架。

Result: 创建了DrugRec新的大规模药物推荐测试平台，覆盖比现有工作更多样化的疾病和药物。

Conclusion: TraceDR通过提供可追溯的药物推荐，解决了现有方法缺乏解释性的问题，为医疗专业人员提供了更可靠的决策支持。

Abstract: Drug recommendation (DR) systems aim to support healthcare professionals in
selecting appropriate medications based on patients' medical conditions.
State-of-the-art approaches utilize deep learning techniques for improving DR,
but fall short in providing any insights on the derivation process of
recommendations -- a critical limitation in such high-stake applications. We
propose TraceDR, a novel DR system operating over a medical knowledge graph
(MKG), which ensures access to large-scale and high-quality information.
TraceDR simultaneously predicts drug recommendations and related evidence
within a multi-task learning framework, enabling traceability of medication
recommendations. For covering a more diverse set of diseases and drugs than
existing works, we devise a framework for automatically constructing patient
health records and release DrugRec, a new large-scale testbed for DR.

</details>


### [28] [Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation for Cold-Start Recommendation](https://arxiv.org/abs/2510.27342)
*Alireza Gharahighehi,Felipe Kenji Nakano,Xuehua Yang,Wenhan Cu,Celine Vens*

Main category: cs.IR

TL;DR: 提出了一种改进的决策树评分获取方法，用于音乐推荐系统中的冷启动问题，通过获取属性偏好和使用物品对来更有效地学习用户偏好。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动用户缺乏历史数据的问题，传统决策树方法在个性化推荐方面存在局限性。

Method: 扩展决策树评分获取方法：(i) 获取物品评分和属性偏好（如流派）以更好地聚类用户；(ii) 在每个节点使用物品对而非单个物品来更有效地学习用户偏好。

Result: 实验结果表明两种改进都提升了性能，特别是在减少查询次数的情况下效果更明显。

Conclusion: 提出的扩展决策树方法在音乐推荐系统中能更有效地解决冷启动问题，通过结合属性偏好和物品对查询来提高推荐质量。

Abstract: Recommender systems (RSs) are intelligent filtering methods that suggest
items to users based on their inferred preferences, derived from their
interaction history on the platform. Collaborative filtering-based RSs rely on
users past interactions to generate recommendations. However, when a user is
new to the platform, referred to as a cold-start user, there is no historical
data available, making it difficult to provide personalized recommendations. To
address this, rating elicitation techniques can be used to gather initial
ratings or preferences on selected items, helping to build an early
understanding of the user's tastes. Rating elicitation approaches are generally
categorized into two types: non-personalized and personalized. Decision
tree-based rating elicitation is a personalized method that queries users about
their preferences at each node of the tree until sufficient information is
gathered. In this paper, we propose an extension to the decision tree approach
for rating elicitation in the context of music recommendation. Our method: (i)
elicits not only item ratings but also preferences on attributes such as genres
to better cluster users, and (ii) uses item pairs instead of single items at
each node to more effectively learn user preferences. Experimental results
demonstrate that both proposed enhancements lead to improved performance,
particularly with a reduced number of queries.

</details>


### [29] [Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval](https://arxiv.org/abs/2510.27566)
*Yulong Hui,Chao Chen,Zhihang Fu,Yihao Liu,Jieping Ye,Huanchen Zhang*

Main category: cs.IR

TL;DR: Interact-RAG是一种新的检索增强生成范式，将LLM代理从被动的查询发起者转变为检索过程的主动操控者，通过细粒度控制显著提升了处理复杂信息检索任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代理式RAG方法将检索过程视为黑盒查询操作，限制了代理只能进行查询操作，无法处理复杂的信息搜索任务。

Method: 引入Corpus Interaction Engine，为代理提供一组动作原语来细粒度控制信息检索；开发推理增强的工作流程，支持零样本执行和交互轨迹合成；通过监督微调和强化学习训练完全自主的端到端代理。

Result: 在六个基准测试上的广泛实验表明，Interact-RAG显著优于其他先进方法，验证了推理-交互策略的有效性。

Conclusion: Interact-RAG通过将代理从被动查询者转变为主动操控者，成功解决了传统RAG方法的局限性，为复杂信息检索任务提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by
incorporating external information. However, prevailing agentic RAG approaches
are constrained by a critical limitation: they treat the retrieval process as a
black-box querying operation. This confines agents' actions to query issuing,
hindering its ability to tackle complex information-seeking tasks. To address
this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent
from a passive query issuer into an active manipulator of the retrieval
process. We dismantle the black-box with a Corpus Interaction Engine, equipping
the agent with a set of action primitives for fine-grained control over
information retrieval. To further empower the agent on the entire RAG pipeline,
we first develop a reasoning-enhanced workflow, which enables both zero-shot
execution and the synthesis of interaction trajectories. We then leverage this
synthetic data to train a fully autonomous end-to-end agent via Supervised
Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).
Extensive experiments across six benchmarks demonstrate that Interact-RAG
significantly outperforms other advanced methods, validating the efficacy of
our reasoning-interaction strategy.

</details>
