<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.DS](#cs.DS) [Total: 7]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [NeurIDA: Dynamic Modeling for Effective In-Database Analytics](https://arxiv.org/abs/2512.08483)
*Lingze Zeng,Naili Xing,Shaofeng Cai,Peng Lu,Gang Chen,Jian Pei,Beng Chin Ooi*

Main category: cs.DB

TL;DR: NeurIDA是一个端到端的自主数据库内分析系统，通过动态调整最佳可用基础模型来服务不同的分析任务，支持自然语言查询，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型是静态且任务特定的，而RDBMS环境是动态的且需要支持多样化的分析查询。每个分析任务都需要从头构建定制化流程，导致开发成本高，限制了ML在分析中的广泛应用。

Method: 提出动态数据库内建模新范式：1) 在关系数据上预训练可组合的基础模型架构；2) 接收任务时，根据任务和数据特征动态选择和配置相关组件；3) 支持自然语言查询，通过LLM代理解释用户意图并生成分析报告。

Result: 在5个真实数据集上的10个任务中，NeurIDA在AUC-ROC指标上持续提升达12%，在MAE指标上相对减少25%。系统提供友好的用户体验和高效的数据信内AI分析。

Conclusion: NeurIDA通过动态调整基础模型的方法，解决了传统ML模型静态性与数据库环境动态性之间的矛盾，实现了易用、有效且高效的数据库内AI分析，推动了ML在RDBMS中的深度集成。

Abstract: Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.
  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically "tweaks" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA

</details>


### [2] [Analyzing Deviations from Monotonic Trends through Database Repair](https://arxiv.org/abs/2512.08526)
*Shunit Agmon,Jonathan Gal,Amir Gilad,Ester Livshits,Or Mutay,Brit Youngmann,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 该论文提出聚合顺序依赖(AODs)来量化数据集违反单调趋势的程度，并研究通过最小化删除元组来修复AOD违反的问题，开发了高效算法和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中经常出现违反预期单调趋势的情况（如教育水平越高平均工资越高），需要一种系统方法来量化这种违反程度，并修复数据以符合预期的单调关系。

Method: 提出聚合顺序依赖(AODs)作为顺序依赖的聚合扩展，将AOD修复问题形式化为寻找最小删除元组集以满足AOD约束。分析计算复杂性，提出通用算法模板，为常见聚合函数实例化模板，引入优化技术提高运行效率，并开发启发式替代方法。

Result: 在真实世界和合成数据集上的实验研究表明，所提算法具有实际效率，启发式方法性能良好。案例研究展示了如何使用该框架发现和解释意外的AOD违反。

Conclusion: AODs提供了一种系统方法来量化和修复数据集中的单调趋势违反，所提算法框架在实际应用中高效可行，有助于数据质量管理和异常检测。

Abstract: Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).
  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework.

</details>


### [3] [Causal Explanations for Disparate Trends: Where and Why?](https://arxiv.org/abs/2512.08679)
*Tal Blau,Brit Youngmann,Anna Fariha,Yuval Moskovitch*

Main category: cs.DB

TL;DR: ExDis是一个用于发现两组数据间差异因果解释的框架，能自动识别差异最显著的数据区域（子群体）及导致差异的因果因素。


<details>
  <summary>Details</summary>
Motivation: 在数据分析中，经常观察到两组数据间的差异，但理解这些差异需要能识别差异最显著的数据区域及其原因的解释。传统方法复杂且耗时，特别是对于大规模高维数据集，需要自动化的解释系统。解释不仅要可解释，还要具有可操作性，能够捕捉因果关系而非表面相关性。

Method: 提出ExDis框架，正式定义了解释发现问题和优化问题，分析了其复杂性，并开发了高效的求解算法。该框架能自动识别差异最显著的数据区域（子群体），并在每个区域内关联导致差异的因果因素。

Result: 在三个真实世界数据集上的广泛实验表明，ExDis能生成有意义的因果解释，优于先前方法，并能有效扩展到处理大规模高维数据集。

Conclusion: ExDis框架为自动发现数据差异的因果解释提供了有效解决方案，能够识别差异显著的数据区域和因果因素，具有可解释性和可操作性，适用于大规模高维数据分析。

Abstract: During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators](https://arxiv.org/abs/2512.07901)
*Kevin Vallier*

Main category: cs.GT

TL;DR: 该论文提出了战略演化理论，这是一个用于描述玩家群体、策略和制度规则共同演化的通用模型，扩展了复制动态，包含内生玩家、多层次选择、创新、宪法变革和元治理。


<details>
  <summary>Details</summary>
Motivation: 现有演化博弈理论主要关注固定玩家群体和策略的演化，缺乏对玩家群体、策略和制度规则共同演化的统一理论框架。需要建立一个能够同时处理多层次选择、创新、宪法变革和元治理的通用模型。

Method: 提出Poiesis堆栈作为核心数学对象，这是一个通过跨层增益矩阵连接的战略层次结构。在小增益条件下，系统具有全局Lyapunov函数，并在每个有限深度满足选择、跟踪和随机稳定性结果。证明了该类在块扩展、创新事件、异质效用、连续策略空间和宪法演化下的封闭性。

Result: 证明了封闭性定理：在更高层次不会出现新的动态，无限制的自我修改无法保持Lyapunov结构。该理论统一了演化博弈论、制度设计、创新动态和宪法政治经济学的结果。

Conclusion: 战略演化理论提供了一个长期战略适应的通用数学模型，能够统一处理多层次演化系统，为理解复杂战略系统的长期演化提供了理论基础。

Abstract: This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation.

</details>


### [5] [Selling Privacy in Blockchain Transactions](https://arxiv.org/abs/2512.08096)
*Georgios Chionas,Olga Gorelkina,Piotr Krysta,Rida Laraki*

Main category: cs.GT

TL;DR: 该论文从经济学角度研究区块链交易隐私增强方法，分析隐私意识用户的拍卖机制，包括订单流拍卖、荷兰拍卖变体，并提出隐私市场双边匹配机制。


<details>
  <summary>Details</summary>
Motivation: 区块链交易中用户不仅关注交易结果，还关注其经济偏好的隐私暴露程度。现有机制未能充分考虑隐私意识用户的效用函数，需要设计既能保护隐私又能实现经济效益的机制。

Method: 1. 分析订单流拍卖：用户向搜索者拍卖交易执行权，研究隐私程度对拍卖收益和用户效用的影响，提出最优密封投标拍卖。
2. 分析荷兰拍卖变体：用户逐步降低价格和隐私程度直到交易售出，比较其与最优拍卖的收益。
3. 设计隐私市场双边匹配机制：提出保证激励兼容、预算平衡的定价机制，实现最优社会福利的常数近似。

Result: 1. 在订单流拍卖中，隐私程度影响拍卖收益和用户净效用，最优机制是密封投标拍卖。
2. 荷兰拍卖变体的收益随通信轮数变化，可与最优拍卖比较。
3. 提出的双边市场定价机制能保证激励兼容和预算平衡，同时实现最优社会福利的常数近似。

Conclusion: 该研究为区块链隐私保护提供了经济学分析框架，将密码学原语与经济机制结合，设计了适应隐私意识用户需求的拍卖和匹配机制，为隐私市场设计提供了理论基础。

Abstract: We study methods to enhance privacy in blockchain transactions from an economic angle. We consider mechanisms for privacy-aware users whose utility depends not only on the outcome of the mechanism but also negatively on the exposure of their economic preferences. Specifically, we study two auction-theoretic settings with privacy-aware users. First, we analyze an order flow auction, where a user auctions off to specialized agents, called searchers, the right to execute her transaction while maintaining a degree of privacy. We examine how the degree of privacy affects the revenue of the auction and, broadly, the net utility of the privacy-aware user. In this new setting, we describe the optimal auction, which is a sealed-bid auction. Subsequently, we analyze a variant of a Dutch auction in which the user gradually decreases the price and the degree of privacy until the transaction is sold. We compare the revenue of this auction to that of the optimal one as a function of the number of communication rounds. Then, we introduce a two-sided market - a privacy marketplace - with multiple users selling their transactions under their privacy preferences to multiple searchers. We propose a posted-price mechanism for the two-sided market that guarantees constant approximation of the optimal social welfare while maintaining incentive compatibility (from both sides of the market) and budget balance. This work builds on the emerging line of research that attempts to improve the performance of economic mechanisms by appending cryptographic primitives to them.

</details>


### [6] [Beyond Revenue and Welfare: Counterfactual Analysis of Spectrum Auctions with Application to Canada's 3800MHz Allocation](https://arxiv.org/abs/2512.08106)
*Sara Jalili Shani,Kris Joseph,Michael B. McNally,James R. Wright*

Main category: cs.GT

TL;DR: 该研究使用简单的近视投标模型成功预测了加拿大2023年3800 MHz频谱拍卖结果，并通过反事实模拟展示了替代拍卖机制如何改善农村地区覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统频谱拍卖模型依赖强均衡假设，本研究旨在开发更简约的行为模型来预测拍卖结果，并为政策制定者提供评估替代拍卖设计的实用工具。

Method: 采用近视且直接的投标者模型：每轮中企业选择当前价格下效用最大化的频谱组合。使用线性规划框架从详细的逐轮投标数据中估计投标者估值，并验证模型对实际分配和价格演化的预测能力。

Result: 该模型成功再现了加拿大2023年3800 MHz频谱拍卖的关键特征。反事实模拟显示，替代机制能显著改善农村和偏远地区的人口覆盖率，符合加拿大电信法的政策目标。

Conclusion: 简约的行为模型足以生成可靠的反事实预测，为政策制定者提供了评估不同拍卖设计对政策目标（如公平部署）影响的实用框架，展示了反事实机制设计的方法。

Abstract: Spectrum auctions are the primary mechanism through which governments allocate scarce radio frequencies, with outcomes that shape competition, coverage, and innovation in telecommunications markets. While traditional models of spectrum auctions often rely on strong equilibrium assumptions, we take a more parsimonious approach by modeling bidders as myopic and straightforward: in each round, firms simply demand the bundle that maximizes their utility given current prices. Despite its simplicity, this model proves effective in predicting the outcomes of Canada's 2023 auction of 3800 MHz spectrum licenses. Using detailed round-by-round bidding data, we estimate bidders' valuations through a linear programming framework and validate that our model reproduces key features of the observed allocation and price evolution. We then use these estimated valuations to simulate a counterfactual auction under an alternative mechanism that incentivizes deployment in rural and remote regions, aligning with one of the key objectives set out in the Canadian Telecommunications Act. The results show that the proposed mechanism substantially improves population coverage in underserved areas. These findings demonstrate that a behavioral model with minimal assumptions is sufficient to generate reliable counterfactual predictions, making it a practical tool for policymakers to evaluate how alternative auction designs may influence future outcomes. In particular, our study demonstrates a method for counterfactual mechanism design, providing a framework to evaluate how alternative auction rules could advance policy goals such as equitable deployment across Canada.

</details>


### [7] [Multi-agent learning under uncertainty: Recurrence vs. concentration](https://arxiv.org/abs/2512.08132)
*Kyriakos Lotidis,Panayotis Mertikopoulos,Nicholas Bambos,Jose Blanchet*

Main category: cs.GT

TL;DR: 研究多智能体随机正则化学习的收敛性，发现在强单调博弈中，动态虽不收敛但会频繁返回均衡邻域，而在非强单调博弈中这些性质可能失效。


<details>
  <summary>Details</summary>
Motivation: 研究在不确定性和随机性下的多智能体学习收敛行为，特别是与确定性、完全信息模型相比，随机正则化学习在连续博弈中的长期行为特征。

Method: 分析两种随机正则化学习模型（连续时间和离散时间），研究在强单调博弈中的动态行为，量化长期分布对均衡邻域的集中程度。

Result: 在强单调博弈中，正则化学习动态会无限次远离均衡，但总是有限时间内返回其邻域；长期分布在均衡邻域内高度集中。这些性质在非强单调博弈中可能失效。

Conclusion: 随机正则化学习在强单调博弈中表现出良好的局部稳定性，但在非强单调博弈中可能失效，揭示了在持续随机性和不确定性下正则化学习的局限性。

Abstract: In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games -- one in continuous and one in discrete time with the aim of characterizing the long-run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone -- underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty.

</details>


### [8] [Robust equilibria in continuous games: From strategic to dynamic robustness](https://arxiv.org/abs/2512.08138)
*Kyriakos Lotidis,Panayotis Mertikopoulos,Nicholas Bambos,Jose Blanchet*

Main category: cs.GT

TL;DR: 该论文研究了连续博弈中纳什均衡的鲁棒性，包括战略鲁棒性和动态鲁棒性，并建立了两种鲁棒性之间的结构对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究连续博弈中纳什均衡在战略不确定性和动态不确定性下的鲁棒性，探讨哪些均衡能够在扰动和随机性下保持稳定。

Method: 1. 引入战略鲁棒均衡概念，即对博弈支付结构的小扰动保持不变的均衡，并给出几何特征描述；2. 研究FTRL动态在随机性和不确定性下的稳定极限点；3. 建立两种鲁棒性之间的结构对应关系；4. 分析收敛速率与正则化器的关系。

Result: 1. 战略鲁棒性蕴含动态鲁棒性；2. 要维持动态鲁棒性，战略鲁棒性要求不能放松；3. 在仿射约束行动空间的博弈中，熵正则化学习以几何速率收敛到鲁棒均衡。

Conclusion: 论文建立了连续博弈中战略鲁棒性和动态鲁棒性之间的深刻联系，证明了两种鲁棒性概念的内在一致性，并为熵正则化学习在特定博弈中的快速收敛提供了理论保证。

Abstract: In this paper, we examine the robustness of Nash equilibria in continuous games, under both strategic and dynamic uncertainty. Starting with the former, we introduce the notion of a robust equilibrium as those equilibria that remain invariant to small -- but otherwise arbitrary -- perturbations to the game's payoff structure, and we provide a crisp geometric characterization thereof. Subsequently, we turn to the question of dynamic robustness, and we examine which equilibria may arise as stable limit points of the dynamics of "follow the regularized leader" (FTRL) in the presence of randomness and uncertainty. Despite their very distinct origins, we establish a structural correspondence between these two notions of robustness: strategic robustness implies dynamic robustness, and, conversely, the requirement of strategic robustness cannot be relaxed if dynamic robustness is to be maintained. Finally, we examine the rate of convergence to robust equilibria as a function of the underlying regularizer, and we show that entropically regularized learning converges at a geometric rate in games with affinely constrained action spaces.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [9] [Structure Theorems (and Fast Algorithms) for List Recovery of Subspace-Design Codes](https://arxiv.org/abs/2512.08017)
*Rohan Goyal,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: 该论文展示了折叠Reed-Solomon码和单变量重数码在列表恢复中的高效算法，能够输出紧凑的列表描述，运行时间仅与1/ε多项式相关，改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 列表恢复是编码理论和理论计算机科学中的基本概念。虽然折叠Reed-Solomon码和重数码可以达到容量，但Chen和Zhang等人的工作表明这些码的列表大小在1/ε上是指数级的，这意味着无法在1/ε的多项式时间内进行列表恢复。本文旨在克服这一限制。

Method: 扩展了Ashvinkumar、Habib和Srivastava在列表解码方面的算法进展，利用列表的结构特性。即使列表很大，也能输出紧凑描述，该描述仅包含ℓ^O((log ℓ)/ε)大小的集合，运行时间仅与1/ε多项式相关。

Result: 能够输出大小为ℓ^O((log ℓ)/ε)的紧凑描述，运行时间仅与1/ε多项式相关，显著改进了Guruswami和Wang的≈n^(ℓ/ε)大小描述。同时改进了列表恢复任务的最新算法结果。

Conclusion: 尽管折叠Reed-Solomon码和重数码在列表恢复中需要指数大小的列表，但这些列表具有高度结构化的特性，使得可以在多项式时间内输出紧凑描述，突破了先前认为的时间复杂度限制。

Abstract: List recovery of error-correcting codes has emerged as a fundamental notion with broad applications across coding theory and theoretical computer science. Folded Reed-Solomon (FRS) and univariate multiplicity codes are explicit constructions which can be efficiently list-recovered up to capacity, namely a fraction of errors approaching $1-R$ where $R$ is the code rate.
  Chen and Zhang and related works showed that folded Reed-Solomon codes and linear codes must have list sizes exponential in $1/ε$ for list-recovering from an error-fraction $1-R-ε$. These results suggest that one cannot list-recover FRS codes in time that is also polynomial in $1/ε$. In contrast to such limitations, we show, extending algorithmic advances of Ashvinkumar, Habib, and Srivastava for list decoding, that even if the lists in the case of list-recovery are large, they are highly structured. In particular, we can output a compact description of a set of size only $\ell^{O((\log \ell)/ε)}$ which contains the relevant list, while running in time only polynomial in $1/ε$ (the previously known compact description due to Guruswami and Wang had size $\approx n^{\ell/ε}$). We also improve on the state-of-the-art algorithmic results for the task of list-recovery.

</details>


### [10] [Expectations in Expectation Propagation](https://arxiv.org/abs/2512.08034)
*Zilu Zhao,Fangqing Xiao,Dirk Slock*

Main category: cs.IT

TL;DR: 本文研究期望传播(EP)算法中消息积分值无限的问题，在线性模型背景下分析信念关系，提出防止算法被阻塞的解决方案。


<details>
  <summary>Details</summary>
Motivation: 期望传播算法中，虽然消息可以具有无限积分值，但负方差消息会阻碍算法进展。需要解决EP在线性模型中因消息积分值无限而导致的算法阻塞问题。

Method: 1. 分析线性模型中EP信念之间的关系；2. 提出非持久性和持久性方法防止算法被无限积分值消息阻塞；3. 通过检查EP消息关系开发避免无限积分值消息出现的方法。

Result: 提出了三种方法：基于信念关系分析的非持久性方法、持久性方法，以及通过消息关系分析开发的避免无限积分值消息的方法，这些都能有效防止EP算法被阻塞。

Conclusion: 在线性模型中，通过分析EP信念和消息关系，可以开发有效方法解决负方差消息导致的算法阻塞问题，提高期望传播算法的稳定性和收敛性。

Abstract: Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have "negative" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.
  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.
  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values.

</details>


### [11] [Adaptive Matched Filtering for Sensing With Communication Signals in Cluttered Environments](https://arxiv.org/abs/2512.08157)
*Lei Xie,Hengtao He,Yifeng Xiong,Fan Liu,Shi Jin*

Main category: cs.IT

TL;DR: 该论文研究了杂波环境下自适应匹配滤波的性能，提出从瞬时SCNR优化转向统计平均SCNR优化，利用随机矩阵理论推导渐近近似，发现PSK优于QAM和Gaussian，OFDM优于SC和AFDM，并提出DPD和DPI两种导频设计方案。


<details>
  <summary>Details</summary>
Motivation: 在杂波环境中，瞬时信杂噪比(SCNR)是依赖于数据载荷的随机变量，直接作为优化目标存在计算负担重和信令开销大的实际问题。需要寻找更实用的优化方法。

Method: 1) 将优化目标从瞬时SCNR转向统计平均SCNR；2) 利用随机矩阵理论推导平均SCNR的渐近近似；3) 提出两种导频设计方案：数据载荷相关(DPD)和数据载荷无关(DPI)；4) 为DPD问题使用分数优化和KKT条件获得闭式解，为DPI问题采用流形优化处理秩一约束。

Result: 理论分析表明：固定调制基时，PSK的平均SCNR优于QAM和高斯星座；对于给定星座，OFDM的平均SCNR高于SC和AFDM。仿真验证了理论分析的准确性，并证明了所提方法的有效性。

Conclusion: 通过将优化目标从瞬时SCNR转向统计平均SCNR，并利用随机矩阵理论工具，该研究为杂波环境下的自适应匹配滤波提供了实用的解决方案。提出的DPD和DPI导频设计方案在感知性能和实现复杂度之间提供了灵活权衡。

Abstract: This paper investigates the performance of the adaptive matched filtering (AMF) in cluttered environments, particularly when operating with superimposed signals. Since the instantaneous signal-to-clutter-plus-noise ratio (SCNR) is a random variable dependent on the data payload, using it directly as a design objective poses severe practical challenges, such as prohibitive computational burdens and signaling overhead. To address this, we propose shifting the optimization objective from an instantaneous to a statistical metric, which focuses on maximizing the average SCNR over all possible payloads. Due to its analytical intractability, we leverage tools from random matrix theory (RMT) to derive an asymptotic approximation for the average SCNR, which remains accurate even in moderate-dimensional regimes. A key finding from our theoretical analysis is that, for a fixed modulation basis, the PSK achieves a superior average SCNR compared to QAM and the pure Gaussian constellation. Furthermore, for any given constellation, the OFDM achieves a higher average SCNR than SC and AFDM. Then, we propose two pilot design schemes to enhance system performance: a Data-Payload-Dependent (DPD) scheme and a Data-Payload-Independent (DPI) scheme. The DPD approach maximizes the instantaneous SCNR for each transmission. Conversely, the DPI scheme optimizes the average SCNR, offering a flexible trade-off between sensing performance and implementation complexity. Then, we develop two dedicated optimization algorithms for DPD and DPI schemes. In particular, for the DPD problem, we employ fractional optimization and the KKT conditions to derive a closed-form solution. For the DPI problem, we adopt a manifold optimization approach to handle the inherent rank-one constraint efficiently. Simulation results validate the accuracy of our theoretical analysis and demonstrate the effectiveness of the proposed methods.

</details>


### [12] [On the Fundamental Tradeoff of Joint Communication and QCD: The Monostatic Case](https://arxiv.org/abs/2512.08332)
*Sung Hoon Lim,Daewon Seo*

Main category: cs.IT

TL;DR: 该论文研究了ISAC系统中通信与最快变化检测之间的基本权衡，提出了一种利用反馈动态调整编码的JCCS策略，并分析了其可达速率-延迟区域。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信（ISAC）系统中，通信性能与变化检测延迟之间存在固有的权衡关系。当前缺乏能够同时优化这两个指标的框架，特别是在单基地设置下需要实时状态估计和动态编码调整的场景。

Method: 提出了一种新颖的联合通信与最快变化子块编码策略（JCCS），利用反馈机制基于实时状态估计动态调整编码。使用状态依赖的互信息和KL散度来表征可达速率-延迟区域，并提供了部分逆定理证明检测算法的渐近最优性。

Result: 建立了通信性能与检测延迟之间的权衡框架，证明了JCCS策略的渐近最优性。通过二进制和MIMO高斯信道的具体分析，揭示了在ISAC系统设计中实现最优权衡的洞见。

Conclusion: JCCS策略为ISAC系统中的通信与变化检测权衡提供了有效的解决方案，通过反馈驱动的动态编码实现了性能优化，为实际ISAC系统设计提供了理论指导。

Abstract: This paper investigates the fundamental tradeoff between communication and quickest change detection (QCD) in integrated sensing and communication (ISAC) systems under a monostatic setup. We introduce a novel Joint Communication and quickest Change subblock coding Strategy (JCCS) that leverages feedback to adapt coding dynamically based on real-time state estimation. The achievable rate-delay region is characterized using state-dependent mutual information and KL divergence, providing a comprehensive framework for analyzing the interplay between communication performance and detection delay. Moreover, we provide a partial converse demonstrating the asymptotic optimality of the proposed detection algorithm within the JCCS framework. To illustrate the practical implications, we analyze binary and MIMO Gaussian channels, revealing insights into achieving optimal tradeoffs in ISAC system design.

</details>


### [13] [On Discrete Ambiguity Functions of Random Communication Waveforms](https://arxiv.org/abs/2512.08352)
*Ying Zhang,Fan Liu,Yifeng Xiong,Weijie Yuan,Shuangyang Li,Le Zheng,Tony Xiao Han,Christos Masouros,Shi Jin*

Main category: cs.IT

TL;DR: 该论文为随机通信波形在任意正交调制下的离散模糊函数提供了基础性表征，推导了两种AF的期望旁瓣水平和期望积分旁瓣水平的闭式表达式，揭示了OFDM和OTFS在不同星座分布下的最优性，并证明了DP-AF框架下不存在二维区域最小旁瓣的波形。


<details>
  <summary>Details</summary>
Motivation: 为未来ISAC应用中的延迟-多普勒感知性能评估提供关键指标，建立统一的分析框架来表征随机通信波形的离散模糊函数特性。

Method: 开发了统一的解析框架分析两种AF（DP-AF和FST-AF），通过分析平方AF的期望推导闭式表达式，引入基于有限Weyl-Heisenberg群的矩阵表示，从几何约束角度分析旁瓣特性。

Result: 推导出DP-AF和FST-AF的ESL和EISL闭式表达式；证明DP-AF中归一化EISL对所有正交波形相同；发现FST-AF中OFDM对亚高斯星座最优，OTFS对超高斯星座最优；证明DP-AF框架下不存在二维区域最小旁瓣的波形。

Conclusion: 该研究为ISAC波形设计提供了理论基础，揭示了不同调制方案在延迟-多普勒感知中的性能特性，特别是OFDM和OTFS在不同星座分布下的最优性，以及DP-AF框架下波形设计的固有局限性。

Abstract: This paper provides a fundamental characterization of the discrete ambiguity functions (AFs) of random communication waveforms under arbitrary orthonormal modulation with random constellation symbols, which serve as a key metric for evaluating the delay-Doppler sensing performance in future ISAC applications. A unified analytical framework is developed for two types of AFs, namely the discrete periodic AF (DP-AF) and the fast-slow time AF (FST-AF), where the latter may be seen as a small-Doppler approximation of the DP-AF. By analyzing the expectation of squared AFs, we derive exact closed-form expressions for both the expected sidelobe level (ESL) and the expected integrated sidelobe level (EISL) under the DP-AF and FST-AF formulations. For the DP-AF, we prove that the normalized EISL is identical for all orthogonal waveforms. To gain structural insights, we introduce a matrix representation based on the finite Weyl-Heisenberg (WH) group, where each delay-Doppler shift corresponds to a WH operator acting on the ISAC signal. This WH-group viewpoint yields sharp geometric constraints on the lowest sidelobes: The minimum ESL can only occur along a one-dimensional cut or over a set of widely dispersed delay-Doppler bins. Consequently, no waveform can attain the minimum ESL over any compact two-dimensional region, leading to a no-optimality (no-go) result under the DP-AF framework. For the FST-AF, the closed-form ESL and EISL expressions reveal a constellation-dependent regime governed by its kurtosis: The OFDM modulation achieves the minimum ESL for sub-Gaussian constellations, whereas the OTFS waveform becomes optimal for super-Gaussian constellations. Finally, four representative waveforms, namely, SC, OFDM, OTFS, and AFDM, are examined under both frameworks, and all theoretical results are verified through numerical examples.

</details>


### [14] [Skew polynomial representations of matrix algebras and applications to coding theory](https://arxiv.org/abs/2512.08602)
*Alessandro Neri,Paolo Santonastaso*

Main category: cs.IT

TL;DR: 该论文扩展了矩阵代数的斜多项式表示，利用这种表示构造了新的最大和秩距离(MSRD)码族，推广了现有多种最优码构造。


<details>
  <summary>Details</summary>
Motivation: 现有斜多项式表示仅适用于某些矩阵代数结构，需要扩展到更一般的矩阵空间直和结构，以更好地捕捉和秩距离度量，并构造更广泛的MSRD码。

Method: 扩展矩阵代数的斜多项式表示到除法环上矩阵空间的直和结构，定义基于度和最大公右除子的权重函数来捕捉和秩距离，利用此表示构造新的MSRD码族。

Result: 获得了有限域、无限域和除法环上的新MSRD码族，推广了已知的MSRD码、秩度量最优码和汉明度量最优码构造，在有限域情况下得到了接近域大小的MDS码族。

Conclusion: 扩展的斜多项式表示为构造各种度量下的最优码提供了统一框架，所得新码族具有理论意义和应用价值，特别是在有限域上获得了接近理论界限的MDS码。

Abstract: We extend the existing skew polynomial representations of matrix algebras which are direct sum of matrix spaces over division rings. In this representation, the sum-rank distance between two tuples of matrices is captured by a weight function on their associated skew polynomials, defined through degrees and greatest common right divisors with the polynomial that defines the representation. We exploit this representation to construct new families of maximum sum-rank distance (MSRD) codes over finite and infinite fields, and over division rings. These constructions generalize many of the known existing constructions of MSRD codes as well as of optimal codes in the rank and in the Hamming metric. As a byproduct, in the case of finite fields we obtain new families of MDS codes which are linear over a subfield and whose length is close to the field size.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [15] [MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction](https://arxiv.org/abs/2512.07846)
*Guoyao Li,Ran He,Shusen Jing,Kayhan Behdin,Yubo Wang,Sundara Raman Ramachandran,Chanh Nguyen,Jian Sheng,Xiaojing Ma,Chuanrui Zhu,Sriram Vasudevan,Muchen Wu,Sayan Ghosh,Lin Su,Qingquan Song,Xiaoqing Wang,Zhipeng Wang,Qing Lan,Yanning Chen,Jingwei Wu,Luke Simon,Wenjing Zhang,Qi Guo,Fedor Borisyuk*

Main category: cs.IR

TL;DR: MixLM提出了一种新颖的LLM排序框架，通过将物品描述编码为少量嵌入令牌来大幅减少输入上下文长度，在保持相关性的同时显著提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义理解方面表现出色，但在工业级延迟和吞吐量要求下计算开销过高。特别是交叉编码器排序系统需要处理长上下文预填充工作负载，因为模型需要同时处理用户、查询和物品信息。

Method: 提出MixLM框架，采用混合交互方式：将物品描述预先编码为少量嵌入令牌并存储在近线缓存中，在线推理时使用这些嵌入令牌代替原始文本，从而将物品长度从数千文本令牌减少到几个嵌入令牌。

Result: 在相同延迟预算下，MixLM将吞吐量提高了10.0倍，同时保持了相关性指标。在LinkedIn搜索应用中的在线A/B测试显示，该框架实现了0.47%的日活跃用户增长。

Conclusion: MixLM通过创新的混合交互设计有效解决了LLM排序系统的效率瓶颈，实现了相关性保持与系统性能的平衡，为工业级LLM搜索应用提供了可行的解决方案。

Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.

</details>


### [16] [Detecting Privileged Documents by Ranking Connected Network Entities](https://arxiv.org/abs/2512.08073)
*Jianping Zhang,Han Qin,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 该论文提出了一种基于链接分析的方法，通过从电子邮件头元数据构建人际关系网络来识别特权文档，利用实体与律师的互动频率来量化特权通信可能性。


<details>
  <summary>Details</summary>
Motivation: 在电子发现（e-discovery）过程中，识别特权文档（如律师-客户保密通信）是一项耗时且容易出错的任务。传统方法主要依赖关键词搜索和人工审查，效率低下且可能遗漏重要文档。需要一种更自动化的方法来准确识别可能包含特权通信的文档。

Method: 方法包括：1) 从电子邮件头元数据中提取人际关系网络；2) 基于已知法律专业人士列表将实体分类为律师或非律师；3) 假设与律师频繁互动的个人更可能参与特权通信；4) 开发算法为网络中的每个实体分配特权可能性分数；5) 结合实体分数和连接强度来增强特权文档识别。

Result: 实验结果表明，该算法在特权文档检测的法律实体排名方面表现有效。通过量化实体与律师的互动频率和网络连接强度，能够更准确地识别可能包含特权通信的文档。

Conclusion: 基于链接分析的方法为特权文档识别提供了一种有效的自动化解决方案。通过分析人际关系网络和实体分类，该方法能够量化特权通信可能性，提高电子发现过程的效率和准确性。未来工作可以扩展到其他类型的元数据和更复杂的网络分析技术。

Abstract: This paper presents a link analysis approach for identifying privileged documents by constructing a network of human entities derived from email header metadata. Entities are classified as either counsel or non-counsel based on a predefined list of known legal professionals. The core assumption is that individuals with frequent interactions with lawyers are more likely to participate in privileged communications. To quantify this likelihood, an algorithm assigns a score to each entity within the network. By utilizing both entity scores and the strength of their connections, the method enhances the identification of privileged documents. Experimental results demonstrate the algorithm's effectiveness in ranking legal entities for privileged document detection.

</details>


### [17] [A Comparative Study of Retrieval Methods in Azure AI Search](https://arxiv.org/abs/2512.08078)
*Qiang Mao,Han Qin,Robert Neary,Charles Wang,Fusheng Wei,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 评估微软Azure RAG框架中不同检索策略在电子取证早期案件评估中的效果，比较关键词、语义、向量、混合和混合语义检索方法的性能。


<details>
  <summary>Details</summary>
Motivation: 律师需要超越传统的关键词和语义搜索，利用大语言模型通过自然语言提问从文档中获取准确简洁的答案，以提高文档审查效率。特别是在电子取证的早期案件评估阶段，法律团队需要在全面审查前快速理解数据、确定关键事实和风险。

Method: 在微软Azure的检索增强生成框架中，比较Azure AI Search的五种检索方法：关键词检索、语义检索、向量检索、混合检索和混合语义检索。评估每种方法生成AI响应的准确性、相关性和一致性。

Result: 研究展示了不同检索方法在准确性、相关性和一致性方面的表现差异，为法律从业者提供了实证数据，帮助他们选择最适合电子取证早期案件评估的RAG配置。

Conclusion: 法律从业者可以利用本研究结果来优化未来RAG配置的选择，从而提高电子取证早期案件评估的效率和效果，实现更智能的文档审查流程。

Abstract: Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.

</details>


### [18] [Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery](https://arxiv.org/abs/2512.08079)
*Qiang Mao,Fusheng Wei,Robert Neary,Charles Wang,Han Qin,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文系统研究了自动聚类描述生成方法，通过集成图像聚类、图像描述和LLM技术，为大规模图像数据集提供高效组织方案。


<details>
  <summary>Details</summary>
Motivation: 数字图像数量的快速增长给法律发现、数字存档和内容管理带来巨大挑战。企业和法律团队需要在严格时间压力下组织和分析大规模图像集合，手动审查不切实际且成本高昂，因此迫切需要自动化方法来高效组织和描述大规模图像数据集。

Method: 采用K-means聚类将图像分为20个视觉连贯的簇，使用Azure AI Vision API生成基础描述。评估三个关键维度：(1)图像采样策略：随机、基于质心、分层、混合和基于密度采样与使用所有图像对比；(2)提示技术：标准提示与思维链提示对比；(3)描述生成方法：基于LLM的方法与传统TF-IDF和基于模板的方法对比。使用语义相似性和覆盖度指标评估描述质量。

Result: 结果显示，每个簇使用20张图像的战略采样与穷尽包含方法表现相当，同时显著降低计算成本，只有分层采样显示适度退化。基于LLM的方法始终优于TF-IDF基线，标准提示在此任务中优于思维链提示。这些发现为部署可扩展、准确的聚类描述系统提供了实用指导。

Conclusion: 该研究为法律发现和其他需要自动组织大规模图像集合的领域提供了实用的可扩展解决方案指导。战略采样与LLM结合的方法能够在保持质量的同时显著降低计算成本，支持高容量工作流程。

Abstract: The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.

</details>


### [19] [Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters](https://arxiv.org/abs/2512.08083)
*Keith Huffman,Jianping Zhang,Nathaniel Huber-Fliflet,Fusheng Wei,Peter Gronvall*

Main category: cs.IR

TL;DR: 本文通过实证研究探讨了随机性在基于LLM的律师-客户特权文件检测分类中的作用，发现LLM能有效识别特权文件，随机性控制参数对分类性能影响很小，而利用随机性的方法能显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 在法律事务中，文本分类模型常用于筛选大型数据集以寻找符合特定标准（如法律特权通信）的文件。虽然LLM在此类任务中表现出色，但随机性对分类输出的影响尚未得到充分研究，特别是在法律合规这种对准确性要求极高的领域。

Method: 本文进行了实证研究，从四个维度分析随机性在LLM分类中的作用：(1) LLM识别法律特权文件的有效性；(2) 随机性控制参数对分类输出的影响；(3) 随机性对整体分类性能的影响；(4) 开发了一种利用随机性提升准确性的方法。

Result: 实验结果表明：LLM能有效识别特权文件；随机性控制参数对分类性能影响很小；更重要的是，开发的利用随机性的方法能显著提升准确性。该方法还能增强企业对LLM输出的信心，特别是在制裁合规流程中。

Conclusion: 随着组织越来越多地依赖LLM增强合规工作流程，减少输出变异性有助于建立内部和监管机构对LLM衍生制裁筛查决策的信心。利用随机性的方法不仅能提升准确性，还能增强企业在法律合规应用中对LLM输出的信任。

Abstract: In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.

</details>


### [20] [Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring](https://arxiv.org/abs/2512.08398)
*Jiin Park,Hyuna Jeon,Yoonseo Lee,Jisu Hong,Misuk Kim*

Main category: cs.IR

TL;DR: 提出一种基于本体的知识图谱构建方法，专门处理工业标准文档中的复杂结构、条件和数值规则，通过分层语义组织和LLM三元组抽取实现高质量知识表示。


<details>
  <summary>Details</summary>
Motivation: 工业标准文档包含大量技术信息和复杂规则，具有高度结构化格式（表格、适用范围、约束、例外、数值计算），传统方法难以有效处理这些复杂结构进行知识图谱构建。

Method: 1) 将文档组织为分层语义结构；2) 将句子和表格分解为基于条件和数值规则的原子命题；3) 通过LLM三元组抽取整合到本体知识图谱中；4) 实现本体感知的KG-RAG框架进行验证。

Result: 构建了规则、表格、多跳QA和有毒条款检测数据集，实验结果显示在所有QA类型上都显著优于现有KG-RAG方法，证明了方法的有效性。

Conclusion: 即使对于条件、约束和适用范围交织的工业文档，也能实现可靠且可扩展的知识表示，为未来领域特定RAG开发和智能文档管理做出贡献。

Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.

</details>


### [21] [VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation](https://arxiv.org/abs/2512.08702)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Zitong Wan,Hewei Wang,Weijie Liu,Yijie Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: VI-MMRec是一个模型无关、无需训练成本的框架，通过基于模态特征相似性的虚拟用户-物品交互来缓解多模态推荐中的数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐模型受限于数据稀疏问题，用户通常只与少量物品交互，导致模型将未观察到的物品任意视为负样本，限制了推荐效果。

Method: 提出两种策略：1) Overlay策略独立聚合模态特定相似性以保留模态特定用户偏好；2) Synergistic策略整体融合跨模态相似性以捕捉互补用户偏好。采用统计信息权重分配机制，根据数据集特定模态相关性自适应分配虚拟交互权重。

Result: 在六个真实世界数据集上使用七个最先进的多模态推荐模型进行综合实验，验证了VI-MMRec的有效性。

Conclusion: VI-MMRec是一个即插即用框架，无需修改核心架构即可增强现有模型性能，训练时无额外开销，便于实际部署。

Abstract: Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [22] [The Bichromatic Two-Center Problem on Graphs](https://arxiv.org/abs/2512.08111)
*Qi Sun,Jingru Zhang*

Main category: cs.DS

TL;DR: 研究图上的双色双中心问题，提出针对不同图类型的算法：一般图O(m²n log n log mn)，树O(n log n)，无权树线性时间


<details>
  <summary>Details</summary>
Motivation: 该问题在图（包括树图）上尚未被研究，需要设计高效算法解决双色双中心分配问题，以最小化顶点到分配中心的最大（加权）距离

Method: 针对不同图类型设计不同算法：1）基于距离矩阵的通用图算法；2）利用树结构特性的树算法；3）针对无权树的线性时间优化方法

Result: 提出了三个算法：一般无向图O(m²n log n log mn)时间，树O(n log n)时间，无权树线性时间，填补了该问题在图上的研究空白

Conclusion: 首次系统研究了图上的双色双中心问题，为不同图类型提供了高效算法，其中树结构算法特别高效，无权树可达线性时间

Abstract: In this paper, we study the (weighted) bichromatic two-center problem on graphs. The input consists of a graph $G$ of $n$ (weighted) vertices and $m$ edges, and a set $\mathcal{P}$ of pairs of distinct vertices, where no vertex appears in more than one pair. The problem aims to find two points (i.e., centers) on $G$ by assigning vertices of each pair to different centers so as to minimize the maximum (weighted) distance of vertices to their assigned centers (so that the graph can be bi-colored with this goal). To the best of our knowledge, this problem has not been studied on graphs, including tree graphs. In this paper, we propose an $O(m^2n\log n\log mn)$ algorithm for solving the problem on an undirected graph provided with the distance matrix, an $O(n\log n)$-time algorithm for the problem on trees, and a linear-time approach for the unweighted tree version.

</details>


### [23] [A tight example for approximation ratio 5 for covering small cuts by the primal-dual method](https://arxiv.org/abs/2512.08350)
*Zeev Nutov*

Main category: cs.DS

TL;DR: 证明了Williamson-Goemans-Mihail-Vazirani原始对偶算法在Small Cuts Cover问题上的近似比为5是最优的，通过构造一个使算法解与最优解比值任意接近5的实例。


<details>
  <summary>Details</summary>
Motivation: Simmons最近证明该原始对偶算法在Small Cuts Cover问题上的近似比为5，并询问这个界限是否紧致。本文旨在回答这个问题。

Method: 通过构造一个具体的图实例，展示原始对偶算法产生的解与最优解之间的比值可以任意接近5。

Result: 成功构造了一个实例，证明原始对偶算法的近似比5是最优的，即不存在比5更小的近似比。

Conclusion: 原始对偶算法在Small Cuts Cover问题上的近似比5是紧致的，这是该算法性能的理论极限。

Abstract: In the Small Cuts Cover problem we seek to cover by a min-cost edge-set the set family of cuts of size/capacity $<k$ of a graph. Recently, Simmons showed that the primal-dual algorithm of Williamson, Goemans, Mihail, and Vazirani achieves approximation ratio $5$ for this problem, and asked whether this bound is tight. We will answer this question positively, by providing an example in which the ratio between the solution produced by the primal-dual algorithm and the optimum is arbitrarily close to $5$.

</details>


### [24] [A Distribution Testing Approach to Clustering Distributions](https://arxiv.org/abs/2512.08376)
*Gunjan Kumar,Yash Pote,Jonathan Scarlett*

Main category: cs.DS

TL;DR: 研究分布聚类问题：给定k个分布被隐藏地划分为两组，组内分布相同，组间分布总变差距离为ε，目标是恢复划分。建立了已知一个聚类分布和两个分布都未知两种情况下的样本复杂度上下界。


<details>
  <summary>Details</summary>
Motivation: 研究分布聚类问题的样本复杂度，特别是在已知一个聚类分布和两个分布都未知的情况下，理解样本复杂度如何依赖于域大小n、分布数量k、聚类大小r和距离ε。

Method: 建立了分布聚类问题的理论框架，通过推导上下界来分析样本复杂度。考虑了两种情况：1）已知一个聚类的分布；2）两个分布都未知。分析了样本复杂度对参数(n,k,r,ε)的依赖关系。

Result: 获得了样本复杂度的上下界，在所有参数范围内实现了对(n,k,r,ε)的紧致性（最多相差O(log k)因子）。具体刻画了样本复杂度如何依赖于这些参数。

Conclusion: 该研究为分布聚类问题建立了完整的样本复杂度理论，揭示了参数(n,k,r,ε)对样本需求的影响，为实际应用中的聚类算法设计提供了理论指导。

Abstract: We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\varepsilon)$ (up to an $O(\log k)$ factor) for all regimes.

</details>


### [25] [Finding All Bounded-Length Simple Cycles in a Directed Graphs -- Revisited](https://arxiv.org/abs/2512.08392)
*Frank Bauernöppel,Jörg-Rüdiger Sack*

Main category: cs.DS

TL;DR: 本文通过具体反例指出Gupta和Suzumura（2021）的有向图有界长度简单环枚举算法存在缺陷，分析了证明中的逻辑漏洞，并提出修正方案，同时保持原算法的计算复杂度不变。


<details>
  <summary>Details</summary>
Motivation: Gupta和Suzumura在2021年提出的有向图有界长度简单环枚举算法声称能够枚举所有满足长度限制的简单环，但本文作者发现该算法实际上会遗漏某些有效环，因此需要指出这些缺陷并加以修正。

Method: 通过构造具体反例展示原算法的失败情况，详细分析原证明中的逻辑漏洞，并提出修正的算法表述，确保算法能够正确枚举所有有界长度简单环。

Result: 成功识别出原算法遗漏有效环的具体情况，分析了证明缺陷的根本原因，提出了修正后的算法，该修正算法能够正确枚举所有有界长度简单环，同时保持原算法的计算复杂度O((c+1)·k·(n+e))。

Conclusion: Gupta和Suzumura（2021）的有向图有界长度简单环枚举算法存在缺陷，本文通过反例揭示了这些问题，提出了修正方案，修正后的算法在保持相同计算复杂度的前提下能够正确完成枚举任务。

Abstract: In 2021, Gupta and Suzumura proposed a novel algorithm for enumerating all bounded-length simple cycles in directed graphs. In this work, we present concrete examples demonstrating that the proposed algorithm fails to enumerate certain valid cycles. Via these examples, we perform a detailed analysis pinpointing the specific points at which the proofs exhibit logical gaps. Furthermore, we propose a corrected formulation that resolves these issues while preserving the desirable property that the algorithm's computational complexity remains $O((c + 1) \cdot k \cdot (n + e))$ where $c$ is the number of simple cycles of a specified maximum length $k$, and $n$ and $e$ the number of graph nodes and edges respectively.

</details>


### [26] [Weighted $k$-Path and Other Problems in Almost $O^*(2^k)$ Deterministic Time via Dynamic Representative Sets](https://arxiv.org/abs/2512.08583)
*Jesper Nederlof*

Main category: cs.DS

TL;DR: 提出动态代表集数据结构，支持集合族的高效更新和查询操作，应用于有向加权k-路径问题，显著改进算法复杂度


<details>
  <summary>Details</summary>
Motivation: 解决参数化复杂性中的核心问题——有向加权k-路径问题，改进现有算法的时间复杂度，回答该领域长期存在的开放性问题

Method: 设计动态代表集数据结构，支持集合族的并集和元素卷积操作，以及查询是否存在与给定集合不相交的小规模子集

Result: 数据结构预处理时间2^{k+O(√k log²k)}n log n，操作时间2^{k+O(√k log²k)}log n；有向加权k-路径算法时间2^{k+O(√k log²k)}(n+m)log n

Conclusion: 动态代表集数据结构在参数化算法中有广泛应用，显著改进了有向加权k-路径问题的确定性算法，回答了该领域的重要开放性问题

Abstract: We present a data structure that we call a Dynamic Representative Set. In its most basic form, it is given two parameters $0< k < n$ and allows us to maintain a representation of a family $\mathcal{F}$ of subsets of $\{1,\ldots,n\}$. It supports basic update operations (unioning of two families, element convolution) and a query operation that determines for a set $B \subseteq \{1,\ldots,n\}$ whether there is a set $A \in \mathcal{F}$ of size at most $k-|B|$ such that $A$ and $B$ are disjoint. After $2^{k+O(\sqrt{k}\log^2k)}n \log n$ preprocessing time, all operations use $2^{k+O(\sqrt{k}\log^2k)}\log n$ time.
  Our data structure has many algorithmic consequences that improve over previous works. One application is a deterministic algorithm for the Weighted Directed $k$-Path problem, one of the central problems in parameterized complexity. Our algorithm takes as input an $n$-vertex directed graph $G=(V,E)$ with edge lengths and an integer $k$, and it outputs the minimum edge length of a path on $k$ vertices in $2^{k+O(\sqrt{k}\log^2k)}(n+m)\log n$ time (in the word RAM model where weights fit into a single word). Modulo the lower order term $2^{O(\sqrt{k}\log^2k)}$, this answers a question that has been repeatedly posed as a major open problem in the field.

</details>


### [27] [Fast exact algorithms via the Matrix Tree Theorem](https://arxiv.org/abs/2512.08600)
*V. Arvind,Srijan Chakraborty,Samir Datta,Asif Khan*

Main category: cs.DS

TL;DR: 本文提出了一种基于矩阵树定理和单位根筛选的统一框架，用于解决哈密顿路径计数、完美匹配计数、k-星划分计数等问题，所有算法均运行在多项式空间内。


<details>
  <summary>Details</summary>
Motivation: 现有哈密顿路径算法在无向图和有向二分图中虽然高效但复杂且不统一，本文旨在开发简单统一的算法框架，同时扩展该框架解决其他组合计数问题。

Method: 使用矩阵树定理和单位根筛选技术构建统一算法框架，通过代数方法将组合计数问题转化为多项式计算问题，并利用Gallai-Edmonds分解定理处理最大匹配计数。

Result: 1) 提出了简单统一的哈密顿路径计数算法；2) 提供了与Ryser算法时间类似的二分图完美匹配计数算法；3) 设计了k-星划分计数算法，其时间复杂度随k增大而趋近于O*(1.000...^n)；4) 利用Björklund算法和Gallai-Edmonds分解，实现了O*(2^ν)时间内的最大匹配计数。

Conclusion: 提出的代数框架具有灵活性和统一性，能够高效解决多种组合计数问题，所有算法均在多项式空间内运行，展示了代数方法在组合算法设计中的强大能力。

Abstract: Fast exact algorithms are known for Hamiltonian paths in undirected and directed bipartite graphs through elegant though involved algorithms that are quite different from each other. We devise algorithms that are simple and similar to each other while having the same upper bounds. The common features of these algorithms is the use of the Matrix-Tree theorem and sieving using roots of unity.
  Next, we use the framework to provide alternative algorithms to count perfect matchings in bipartite graphs on $n$ vertices, i.e., computing the $\{0,1\}$-permanent of a square $n/2 \times n/2$ matrix which runs in a time similar to Ryser.
  We demonstrate the flexibility of our method by counting the number of ways to vertex partition the graph into $k$-stars (a $k$-star consist of a tree with a root having $k-1$ children that are all leaves). Interestingly, our running time improves to $O^*((1+ε_k)^n)$ with $ε_k \rightarrow 0$ as $k \rightarrow \infty$.
  As an aside, making use of Björklund's algorithm for exact counting perfect matchings in general graphs, we show that the count of maximum matchings can be computed in time $O^*(2^ν)$ where $ν$ is the size of a maximum matching. The crucial ingredient here is the famous Gallai-Edmonds decomposition theorem.
  All our algorithms run in polynomial space.

</details>


### [28] [Parallel Batch Dynamic Vertex Coloring in $O(\log Δ)$ Amortized Update Time](https://arxiv.org/abs/2512.08742)
*Chase Hutton,Adam Melrod*

Main category: cs.DS

TL;DR: 首个并行批处理动态算法，用于维护(Δ+1)-顶点着色，具有O(log Δ)期望摊销更新时间和O(polylog b + polylog n)并行跨度


<details>
  <summary>Details</summary>
Motivation: 现有的动态图着色算法大多是顺序的，缺乏高效的并行批处理动态算法。随着图数据规模的增大和动态更新的频繁，需要能够高效处理批量更新的并行算法来提高性能。

Method: 基于Bhattacharya等人(SODA'18)的顺序动态算法，设计新的并行批处理动态算法。采用随机化方法，通过并行处理批量更新来实现高效维护(Δ+1)-顶点着色。

Result: 算法达到O(log Δ)期望摊销更新时间，对于任意大小为b的批量更新，具有高概率的O(polylog b + polylog n)并行跨度。这是首个支持并行批处理的动态顶点着色算法。

Conclusion: 该工作填补了并行批处理动态图着色算法的空白，为大规模动态图处理提供了高效的并行解决方案，在理论和实践上都有重要意义。

Abstract: We present the first parallel batch-dynamic algorithm for maintaining a proper $(Δ+ 1)$-vertex coloring. Our approach builds on a new sequential dynamic algorithm inspired by the work of Bhattacharya et al. (SODA'18). The resulting randomized algorithm achieves $O(\log Δ)$ expected amortized update time and, for any batch of $b$ updates, has parallel span $O(\operatorname{polylog} b + \operatorname{polylog} n)$ with high probability.

</details>
