<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 10]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [The Noncomputability of Immune Reaction Complexity: Algorithmic Information Gaps under Effective Constraints](https://arxiv.org/abs/2601.20865)
*Emmanuel Pio Pastore,Francesco De Rango*

Main category: cs.IT

TL;DR: 该论文基于算法信息论提出了一个有效性过滤、基于证书的反应视图，定义了归一化建议分位数(NAQ)作为任务难度的尺度无关指标，并建立了与率失真理论的联系。


<details>
  <summary>Details</summary>
Motivation: 需要一种理论框架来量化任务难度，该框架应具有尺度无关性、对通用机选择鲁棒，并能在不同任务族之间进行比较。传统方法缺乏对最小信息需求的精确度量。

Method: 引入有效性过滤的执行器模型：固定、总体、输入盲的执行器将自定界建议字符串映射到候选响应，仅当可判定或半可判定的有效性谓词V(x,r)成立时才接受。定义最小可行实现器复杂度M(x) = min_{r: V(x,r)=1} K(r)，其中K为前缀Kolmogorov复杂度。基于参考池定义归一化建议分位数(NAQ)。

Result: 建立了精确实现器恒等式：任何输入盲执行器的最小建议等于M(x)至多O(1)误差。在有限模糊度机制中M(x)约等于min_y K(y)；在通用纤维机制中边界是紧的。NAQ在有限枚举变化下是准不变的，并与率失真理论存在操作逆关系。扩展包括资源有界变体NAQ_t和NP风格设置。

Conclusion: NAQ提供了一个鲁棒、尺度无关的任务难度度量，对通用机选择不敏感，可在不同任务族间比较。该框架通过压缩器代理支持数据驱动的校准，为量化计算任务的固有难度提供了理论基础。

Abstract: We introduce a validity-filtered, certificate-based view of reactions grounded in Algorithmic Information Theory. A fixed, total, input-blind executor maps a self-delimiting advice string to a candidate response, accepted only if a decidable or semi-decidable validity predicate V(x, r) holds. The minimum feasible realizer complexity M(x) = min_{r: V(x,r)=1} K(r), with K denoting prefix Kolmogorov complexity, measures the minimal information required for a valid outcome. We define the Normalized Advice Quantile (NAQ) as the percentile of M(x) across a reference pool, yielding a scale-free hardness index on [0, 1] robust to the choice of universal machine and comparable across task families. An Exact Realizer Identity shows that the minimal advice for any input-blind executor equals M(x) up to O(1), while a description plus selection upper bound refines it via computable feature maps, separating description cost K(y) from selection cost log i_y(x). In finite-ambiguity regimes M(x) approximately equals min_y K(y); in generic-fiber regimes the bound is tight. NAQ is quasi-invariant under bounded enumeration changes. An operational converse links NAQ to rate-distortion: communicating advice with error epsilon requires average length near the entropy of target features. Extensions include a resource-bounded variant NAQ_t incorporating time-penalized complexity (Levin's Kt) and an NP-style setting showing linear worst-case advice n - O(1). Finally, a DKW bound guarantees convergence of empirical NAQ estimates, enabling data-driven calibration via compressor-based proxies.

</details>


### [2] [Cramér-Rao Bound Analysis and Near-Optimal Performance of the Synchronous Nyquist-Folding Generalized Eigenvalue Method (SNGEM) for Sub-Nyquist Multi-Tone Parameter Estimation](https://arxiv.org/abs/2601.20866)
*Huiguang Zhang*

Main category: cs.IT

TL;DR: SNGEM方法在极端亚奈奎斯特速率下实现多音信号的全频/幅/相估计，通过联合处理原始信号及其时间导数，在噪声条件下接近CRB性能，优于传统压缩感知方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法如OMP在亚奈奎斯特采样中存在不可约的误差平台问题，主要由于DFT网格偏差和混叠噪声，需要开发更优的确定性亚奈奎斯特参数谱分析方法。

Method: 提出同步奈奎斯特折叠广义特征值方法(SNGEM)，通过联合处理原始信号及其时间导数，在极端亚奈奎斯特速率下实现多音信号的频率、幅度和相位全参数估计。

Result: 推导了双通道等SNR条件下振幅比参数R=A/B=1/(2πf)的精确克拉美-罗界；蒙特卡洛仿真显示SNGEM在无噪声时达到机器精度，在所有SNR水平下接近CRB，即使在10-20倍压缩下也表现优异，而传统OMP方法存在不可约的误差平台。

Conclusion: SNGEM被确立为统计上近乎最优的确定性亚奈奎斯特参数谱分析方法，在极端压缩比下仍能保持接近理论最优的性能。

Abstract: The synchronous Nyquist folding generalized eigenvalue method (SNGEM) realizes full frequency/amplitude/phase estimation of multitone signals at extreme sub-Nyquist rates by jointly processing the original signals and their time derivatives. In this paper, accurate Cramer-Rao bounds for amplitude ratio parameter R=A/B=1/(2\pif) are derived for two channels with equal SNR. Monte-Carlo simulations confirm that SNGEM achieves machine accuracy in noise-free conditions and closely approaches the derived CRB at all SNR levels, even at 10- 20x compression, whereas classical compressive sensing OMP exhibits irreducible error flattening due to DFT grid bias and aliasing noise. These results establish SNGEM as a statistically nearly optimal deterministic sub-Nyquist parameter spectrum analysis

</details>


### [3] [Power consumption Reduction in ELAA-Assisted ISAC Systems](https://arxiv.org/abs/2601.21010)
*Xiaomin Cao,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 提出了一种用于集成感知与通信（ISAC）的超大规模天线阵列（ELAA）节能子阵列激活框架，通过选择最优子阵列子集来最小化总功耗，同时满足感知和通信的QoS约束。


<details>
  <summary>Details</summary>
Motivation: 超大规模天线阵列（ELAAs）对于实现高分辨率近场感知至关重要，但传统数字架构中完全激活所有天线单元会导致功耗过高，需要解决这一能源效率问题。

Method: 提出了一种节能的子阵列激活框架，将优化问题建模为在满足感知和通信QoS约束下最小化总功耗，并采用基于逐次凸逼近（SCA）的迭代算法求解。

Result: 仿真结果表明，所提出的方法在保持双功能性能的同时，显著降低了功耗。

Conclusion: 该研究为ISAC应用中的ELAA系统提供了一种有效的节能解决方案，通过智能子阵列激活实现了功耗与性能的良好平衡。

Abstract: In this paper, we consider power consumption reduction in extremely large antenna arrays (ELAAs) for integrated sensing and communication (ISAC) applications. Although ELAAs are critical for achieving high-resolution near-field sensing, fully activating all antenna elements in conventional digital architectures leads to prohibitive power demands. To address this, we propose an energy-efficient subarray activation framework that selects an optimal subset of subarrays to minimize the total power consumption, subject to quality-of-service (QoS) constraints for both sensing and communication. We formulate a novel optimization problem and solve it using a successive convex approximation (SCA)-based iterative algorithm. The simulation results confirm that the proposed method significantly reduces power consumption while maintaining dual-function performance.

</details>


### [4] [Deletion-correcting codes for an adversarial nanopore channel](https://arxiv.org/abs/2601.21236)
*Huiling Xie,Zitan Chen*

Main category: cs.IT

TL;DR: 论文研究了对抗性纳米孔通道的删除纠错码，提出了一个具有2t log_q n + Θ(log log n)冗余符号的显式构造，并证明了最优冗余在t log_q n + Ω(1)和2t log_q n - log_q log_2 n + O(1)之间。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性纳米孔通道中的删除纠错问题，该通道最多可能发生t次删除。与经典对抗性q进制删除通道相比，现有显式构造需要4t(1+ε)log_q n + o(log n)的冗余，存在改进空间。

Method: 提出了对抗性纳米孔通道删除纠错码的显式构造方法，构建了长度为n的q进制码，具有2t log_q n + Θ(log log n)的冗余符号。

Result: 证明了最优冗余的下界为t log_q n + Ω(1)，上界为2t log_q n - log_q log_2 n + O(1)。提出的显式构造与存在性上界的一阶项匹配。

Conclusion: 对于对抗性纳米孔删除通道，提出的显式构造在冗余方面达到了接近最优的性能，显著优于经典对抗性删除通道的已知显式构造。

Abstract: We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\log_q n+Θ(\log\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\log_q n+Ω(1)$ and $2t\log_q n-\log_q\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+ε)\log_q n+o(\log n)$.

</details>


### [5] [Belief Propagation with Quantum Messages for Symmetric Q-ary Pure-State Channels](https://arxiv.org/abs/2601.21330)
*Avijit Mandal,Henry D. Pfister*

Main category: cs.IT

TL;DR: 本文扩展了量子消息的置信传播（BPQM）到对称q元纯态信道，提出了基于Gram矩阵特征值的闭式递推跟踪方法，为LDPC码和极化码提供了密度演化分析框架。


<details>
  <summary>Details</summary>
Motivation: 先前BPQM构造和密度演化分析主要针对二元字母表，需要将其扩展到更一般的q元对称纯态信道，以支持更广泛的量子通信应用。

Method: 将BPQM推广到Gram矩阵为循环矩阵的对称q元纯态信道，通过Gram矩阵特征值的闭式递推来高效跟踪比特节点和校验节点合并，独立于输出态的具体物理实现。

Result: 推导出显式的BPQM酉变换，并给出了合并信道保真度的解析界，建立了对称q元纯态信道的密度演化框架，可用于估计LDPC码的BPQM解码阈值和构造极化码。

Conclusion: 该工作为对称q元纯态信道提供了通用的BPQM分析和设计框架，扩展了量子消息置信传播的应用范围，为量子通信中的低复杂度解码方案提供了理论支持。

Abstract: Belief propagation with quantum messages (BPQM) provides a low-complexity alternative to collective measurements for communication over classical--quantum channels. Prior BPQM constructions and density-evolution (DE) analyses have focused on binary alphabets. Here, we generalize BPQM to symmetric q-ary pure-state channels (PSCs) whose output Gram matrix is circulant. For this class, we show that bit-node and check-node combining can be tracked efficiently via closed-form recursions on the Gram-matrix eigenvalues, independent of the particular physical realization of the output states. These recursions yield explicit BPQM unitaries and analytic bounds on the fidelities of the combined channels in terms of the input-channel fidelities. This provides a DE framework for symmetric q-ary PSCs that allows one to estimate BPQM decoding thresholds for LDPC codes and to construct polar codes on these channels.

</details>


### [6] [Learning-Based Sensor Scheduling for Delay-Aware and Stable Remote State Estimation](https://arxiv.org/abs/2601.21482)
*Nho-Duc Tran,Aamir Mahmood,Mikael Gidlund*

Main category: cs.IT

TL;DR: 提出一个考虑延迟感知的无线远程状态估计统一框架，引入延迟依赖的信息增益度量，并开发基于PPO的调度器，在异构传感器、随机延迟和能量约束下实现更优的估计性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于信息新鲜度（AoI）的调度方法忽略了传感器到估计器之间不可预测延迟对状态估计的根本影响，特别是延迟如何与传感器信息量和能量效率相互作用。需要一种能显式建模这种耦合关系的延迟感知框架。

Method: 1) 提出高效的后验融合更新方法，无需状态增广即可合并延迟测量；2) 推导可处理的稳定性条件；3) 将调度建模为马尔可夫决策过程，开发基于近端策略优化（PPO）的调度器，通过归一化奖励在估计精度、新鲜度、传感器异质性和传输能量间进行权衡。

Result: 在异构传感器、现实链路能量模型和随机延迟的仿真中，所提方法稳定学习，在可比能量消耗下比随机调度和强化学习基线（DQN、A2C）获得更低的估计误差，同时对测量可用性和过程/测量噪声变化保持鲁棒性。

Conclusion: 延迟感知框架和基于学习的调度方法能有效处理无线远程状态估计中的延迟-信息-能量权衡问题，超越传统AoI代理方法，为实际部署提供了实用解决方案。

Abstract: Unpredictable sensor-to-estimator delays fundamentally distort what matters for wireless remote state estimation: not just freshness, but how delay interacts with sensor informativeness and energy efficiency. In this paper, we present a unified, delay-aware framework that models this coupling explicitly and quantifies a delay-dependent information gain, motivating an information-per-joule scheduling objective beyond age of information proxies (AoI). To this end, we first introduce an efficient posterior-fusion update that incorporates delayed measurements without state augmentation, providing a consistent approximation to optimal delayed Kalman updates, and then derive tractable stability conditions ensuring that bounded estimation error is achievable under stochastic, delayed scheduling. This conditions highlight the need for unstable modes to be observable across sensors. Building on this foundation, we cast scheduling as a Markov decision process and develop a proximal policy optimization (PPO) scheduler that learns directly from interaction, requires no prior delay model, and explicitly trades off estimation accuracy, freshness, sensor heterogeneity, and transmission energy through normalized rewards. In simulations with heterogeneous sensors, realistic link-energy models, and random delays, the proposed method learns stably and consistently achieves lower estimation error at comparable energy than random scheduling and strong RL baselines (DQN, A2C), while remaining robust to variations in measurement availability and process/measurement noise.

</details>


### [7] [Transversal gates for quantum CSS codes](https://arxiv.org/abs/2601.21514)
*Eduardo Camps-Moreno,Hiram H. López,Gretchen L. Matthews,Narayanan Rengaswamy,Rodrigo San-José*

Main category: cs.IT

TL;DR: 该论文研究了CSS码的横向门计算问题，确定了逻辑作用、非平凡逻辑门和逻辑恒等式的群结构，并给出了明确的方程定义。作者计算了来自单项式码（包括递减单项式码和极化码）的所有CSS码的完整横向稳定子和横向门集合。


<details>
  <summary>Details</summary>
Motivation: 研究CSS码的横向门计算问题，特别是确定对角横向门固定CSS码的集合。横向门在量子纠错中很重要，因为它们可以在不传播错误的情况下实现逻辑操作。现有研究对特定码类（如CSS-T码、三正交码）有结果，但缺乏系统方法。

Method: 作者开发了一种系统方法来计算CSS码的横向门集合。他们确定了逻辑作用以及诱导非平凡逻辑门和逻辑恒等式的群结构。关键创新是明确声明定义这些群的方程组。该方法适用于来自单项式码（包括递减单项式码和极化码）的任何CSS码。

Result: 计算了来自单项式码的所有CSS码的完整横向稳定子和横向门集合。恢复了文献中关于CSS-T码、三正交码和可除码的一些结果，并扩展了这些结果。提供了明确的方程组来定义横向门群。

Conclusion: 该论文为计算CSS码的横向门提供了一种系统方法，特别适用于来自单项式码的CSS码。该方法不仅恢复了现有结果，还扩展了它们，为量子纠错码的设计和分析提供了新工具。

Abstract: In this paper, we focus on the problem of computing the set of diagonal transversal gates fixing a CSS code. We determine the logical actions of the gates as well as the groups of transversal gates that induce non-trivial logical gates and logical identities. We explicitly declare the set of equations defining the groups, a key advantage and differentiator of our approach. We compute the complete set of transversal stabilizers and transversal gates for any CSS code arising from monomial codes, a family that includes decreasing monomial codes and polar codes. As a consequence, we recover and extend some results in the literature on CSS-T codes, triorthogonal codes, and divisible codes.

</details>


### [8] [Subjective Distortion: Achievability and Outer Bounds for Distortion Functions with Memory](https://arxiv.org/abs/2601.21757)
*Hamidreza Abin,Amin Gohari,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 论文研究具有历史依赖性的率失真问题，其中失真函数不仅取决于当前源符号与表示符号，还依赖于过去的表示。提出了该问题的形式化定义，并给出了率失真权衡的内界（可达界）和外界。讨论了问题的凸化以简化界求解。


<details>
  <summary>Details</summary>
Motivation: 在某些率失真类型问题中，所需的信息保真度受过去动作影响，导致失真函数不仅依赖于当前源符号与其表示符号之间的瞬时失真，还依赖于过去的表示。这类问题出现在生物信息处理和推荐系统等场景中。

Method: 首先给出了具有历史依赖性的率失真问题的形式化定义。然后推导了该问题的内界（可达界）和外界。为了简化界的求解，讨论了问题的凸化方法。

Result: 提出了具有历史依赖性失真函数的率失真问题的完整理论框架，包括形式化定义、内界和外界。通过凸化方法使问题更易处理。将理论应用于简化的生物信息处理问题作为示例。

Conclusion: 该研究为具有历史依赖性的率失真问题建立了理论基础，提供了分析此类问题的工具和方法。这类问题在生物信息处理和推荐系统等实际应用中具有重要意义。

Abstract: In some rate-distortion-type problems, the required fidelity of information is affected by past actions. As a result, the distortion function depends not only on the instantaneous distortion between a source symbol and its representation symbol, but also on past representations. In this paper, we give a formal definition of this problem and introduce both inner (achievable) and outer bounds on the rate-distortion tradeoff. We also discuss convexification of the problem, which makes it easier to find bounds. Problems of this type arise in biological information processing, as well as in recommendation engines; we provide an example applied to a simplified biological information processing problem.

</details>


### [9] [Adaptive Privacy of Sequential Data Releases Under Collusion](https://arxiv.org/abs/2601.21859)
*Sophie Taylor,Praneeth Kumar Vippathalla,Justin Coon*

Main category: cs.IT

TL;DR: 论文提出了一种自适应算法来处理不同实体（可能共谋）的顺序数据请求，在隐私与效用之间取得平衡，考虑了预期失真和互信息两种效用度量方式。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于两个观察：1）一次性数据发布的隐私机制不能直接扩展到顺序发布；2）实际数据库可能对多个不同方有用，且无法排除数据共享（共谋）的可能性。

Method: 采用攻击模型假设数据接收者之间可能存在非法数据共享（共谋），开发了一种自适应数据发布算法，使用改进的Blahut-Arimoto算法来处理顺序数据请求。

Result: 当使用预期失真度量效用时，得到的数据发布是最优的；当使用互信息度量效用时，得到的数据发布是局部最优的。

Conclusion: 该研究为处理顺序数据发布中的隐私-效用权衡问题提供了解决方案，并讨论了在机器学习应用中的潜在扩展。

Abstract: The fundamental trade-off between privacy and utility remains an active area of research. Our contribution is motivated by two observations. First, privacy mechanisms developed for one-time data release cannot straightforwardly be extended to sequential releases. Second, practical databases are likely to be useful to multiple distinct parties. Furthermore, we can not rule out the possibility of data sharing between parties. With utility in mind, we formulate a privacy-utility trade-off problem to adaptively tackle sequential data requests made by different, potentially colluding entities. We consider both expected distortion and mutual information as measures to quantify utility, and use mutual information to measure privacy. We assume an attack model whereby illicit data sharing, which we call collusion, can occur between data receivers. We develop an adaptive algorithm for data releases that makes use of a modified Blahut-Arimoto algorithm. We show that the resulting data releases are optimal when expected distortion quantifies utility, and locally optimal when mutual information quantifies utility. Finally, we discuss how our findings may extend to applications in machine learning.

</details>


### [10] [Beyond Martingale Estimators: Structured Estimators for Maximizing Information Freshness in Query-Based Update Systems](https://arxiv.org/abs/2601.22098)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 本文研究连续时间马尔可夫链远程估计系统中的信息新鲜度，提出结构化估计器，在可分析性和最优性之间取得平衡，并推导最优查询策略。


<details>
  <summary>Details</summary>
Motivation: 现有远程估计系统主要使用鞅估计器，虽然简单易分析但远非最优，尤其是在查询式系统中。最大后验概率估计器虽然最优，但在连续时间设置中分析困难。需要一种既能保留MAP估计器优点又具有分析可处理性的新估计器。

Method: 提出结构化估计器，特别是p-MAP估计器，作为MAP估计器的分段常数近似。使用二进制新鲜度过程表征信息新鲜度，推导新鲜度表达式，并为单源和多源系统提供最优状态依赖采样策略和查询率分配策略。

Result: 对于时间可逆CTMC，MAP估计器可简化为p-MAP估计器。推导了结构化估计器下的新鲜度表达式，并提供了最大化平均二进制新鲜度的最优查询策略。对于多源系统，给出了在总查询率约束下的最优查询率分配策略。

Conclusion: 结构化估计器在鞅估计器和MAP估计器之间架起桥梁，既保留了MAP估计器的有用特性，又保持了分析可处理性。提出的p-MAP估计器为完全表征MAP估计器提供了重要进展，并为查询式远程估计系统提供了实用的最优策略。

Abstract: This paper investigates information freshness in a remote estimation system in which the remote information source is a continuous-time Markov chain (CTMC). For such systems, estimators have been mainly restricted to the class of martingale estimators in which the remote estimate at any time is equal to the value of the most recently received update. This is mainly due to the simplicity and ease of analysis of martingale estimators, which however are far from optimal, especially in query-based (i.e., pull-based) update systems. In such systems, maximum a-posteriori probability (MAP) estimators are optimal. However, MAP estimators can be challenging to analyze in continuous-time settings. In this paper, we introduce a new class of estimators, called structured estimators, which can seamlessly shift from a martingale estimator to a MAP estimator, enabling them to retain useful characteristics of the MAP estimate, while still being analytically tractable. Particularly, we introduce a new estimator termed as the $p$-MAP estimator which is a piecewise-constant approximation of the MAP estimator with finitely many discontinuities, bringing us closer to a full characterization of MAP estimators when modeling information freshness. In fact, we show that for time-reversible CTMCs, the MAP estimator reduces to a $p$-MAP estimator. Using the binary freshness (BF) process for the characterization of information freshness, we derive the freshness expressions and provide optimal state-dependent sampling policies (i.e., querying policies) for maximizing the mean BF (MBF) for pull-based remote estimation of a single CTMC information source, when structured estimators are used. Moreover, we provide optimal query rate allocation policies when a monitor pulls information from multiple heterogeneous CTMCs with a constraint on the overall query rate.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [11] [HADUA: Hierarchical Attention and Dynamic Uniform Alignment for Robust Cross-Subject Emotion Recognition](https://arxiv.org/abs/2601.21488)
*Jiahao Tang,Youjun Li,Yangxuan Zheng,Xiangting Fan,Siyuan Lu,Nuo Zhang,Zi-Gang Huang*

Main category: cs.MM

TL;DR: 提出HADUA框架，通过分层注意力机制和动态均匀对齐，解决跨被试情感识别中的模态异质性和分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 跨被试情感识别面临模态异质性和被试间分布偏移两大挑战，现有方法难以同时有效处理这些问题。

Method: 1. 分层注意力模块建模模态内时序动态和模态间语义交互；2. 置信感知高斯加权平滑伪标签监督；3. 均匀对齐损失正则化伪标签分布。

Result: 在多个跨被试情感识别基准测试中，HADUA在准确性和鲁棒性上均优于现有最优方法，有效处理模态差异、噪声伪标签和类别不平衡。

Conclusion: HADUA为构建鲁棒的跨被试情感计算系统提供了实用且可泛化的解决方案，统一了多模态表示学习和域适应。

Abstract: Robust cross-subject emotion recognition from multimodal physiological signals remains a challenging problem, primarily due to modality heterogeneity and inter-subject distribution shift. To tackle these challenges, we propose a novel adaptive learning framework named Hierarchical Attention and Dynamic Uniform Alignment (HADUA). Our approach unifies the learning of multimodal representations with domain adaptation. First, we design a hierarchical attention module that explicitly models intra-modal temporal dynamics and inter-modal semantic interactions (e.g., between electroencephalogram(EEG) and eye movement(EM)), yielding discriminative and semantically coherent fused features. Second, to overcome the noise inherent in pseudo-labels during adaptation, we introduce a confidence-aware Gaussian weighting scheme that smooths the supervision from target-domain samples by down-weighting uncertain instances. Third, a uniform alignment loss is employed to regularize the distribution of pseudo-labels across classes, thereby mitigating imbalance and stabilizing conditional distribution matching. Extensive experiments on multiple cross-subject emotion recognition benchmarks show that HADUA consistently surpasses existing state-of-the-art methods in both accuracy and robustness, validating its effectiveness in handling modality gaps, noisy pseudo-labels, and class imbalance. Taken together, these contributions offer a practical and generalizable solution for building robust cross-subject affective computing systems.

</details>


### [12] [Rethinking Fusion: Disentangled Learning of Shared and Modality-Specific Information for Stance Detection](https://arxiv.org/abs/2601.21675)
*Zhiyu Xie,Fuqiang Niu,Genan Dai,Qianlong Wang,Li Dong,Bowen Zhang,Hu Huang*

Main category: cs.MM

TL;DR: 提出DiME架构，通过分离文本主导、视觉主导和跨模态共享组件来改进多模态立场检测，在多个数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有多模态立场检测方法大多未能区分模态特定信号和跨模态证据，导致性能不佳，需要更有效的模态分离和融合机制

Method: 提出DiME架构：1) 使用目标感知的思维链提示生成推理引导的文本输入；2) 双编码器提取模态特征；3) 三个专家模块（文本主导、视觉主导、跨模态共享）分别处理，采用对比学习（模态特定）和余弦对齐（共享表示）损失；4) 门控网络自适应融合专家输出

Result: 在四个基准数据集上的实验表明，DiME在目标内和零样本设置下均一致优于强大的单模态和多模态基线方法

Conclusion: 通过显式分离模态特定和跨模态信息，DiME能够更有效地进行多模态立场检测，证明了模态分离架构的价值

Abstract: Multi-modal stance detection (MSD) aims to determine an author's stance toward a given target using both textual and visual content. While recent methods leverage multi-modal fusion and prompt-based learning, most fail to distinguish between modality-specific signals and cross-modal evidence, leading to suboptimal performance. We propose DiME (Disentangled Multi-modal Experts), a novel architecture that explicitly separates stance information into textual-dominant, visual-dominant, and cross-modal shared components. DiME first uses a target-aware Chain-of-Thought prompt to generate reasoning-guided textual input. Then, dual encoders extract modality features, which are processed by three expert modules with specialized loss functions: contrastive learning for modality-specific experts and cosine alignment for shared representation learning. A gating network adaptively fuses expert outputs for final prediction. Experiments on four benchmark datasets show that DiME consistently outperforms strong unimodal and multi-modal baselines under both in-target and zero-shot settings.

</details>


### [13] [MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding](https://arxiv.org/abs/2601.21740)
*Meng Yang,Jon McCormack,Maria Teresa Llano,Wanchao Su,Chao Lei*

Main category: cs.MM

TL;DR: MIDI-LLaMA是首个用于符号音乐理解的指令跟随多模态大语言模型，通过两阶段训练流程（特征对齐和指令微调）将MusicBERT与Llama-3-8B对齐，在音乐理解、情感识别和创造力方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在音频音乐理解方面表现出色，但符号音乐（音乐结构的基本表示）尚未被探索。符号音乐作为音乐结构的基础表示，对于深入理解音乐具有重要意义。

Method: 采用两阶段训练流程：1）特征对齐阶段，将MIDI编码器MusicBERT与Llama-3-8B对齐；2）指令微调阶段。同时设计可扩展的标注流程，为GiantMIDI-Piano数据集添加细粒度元数据，构建MIDI-文本数据集。

Result: 与基线模型相比，MIDI-LLaMA在字幕生成和问答语义对齐方面显著优于基于ABC记谱法的转换模型。人工评估进一步证实了MIDI-LLaMA在音乐理解、情感识别、创造力和整体偏好方面的优势。

Conclusion: 将符号音乐整合到大语言模型中能够增强其音乐理解能力，MIDI-LLaMA为符号音乐理解开辟了新方向，展示了符号音乐表示在多模态音乐理解中的重要性。

Abstract: Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [14] [Stochastic Indexing Primitives for Non-Deterministic Molecular Archives](https://arxiv.org/abs/2601.20921)
*Faruk Alpay,Levent Sarioglu*

Main category: cs.DS

TL;DR: Holographic Bloom Filter (HBF) 是一种用于DNA数据存储的概率索引原语，通过高维记忆向量实现单次关联检索，解决随机访问瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储中的随机访问存在瓶颈，现有系统通常通过PCR富集或多步生化程序检索记录，这些方法不支持快速、大规模并行、内容可寻址的查询。

Method: HBF将键-指针关联存储为单个高维记忆向量，使用循环卷积绑定键向量和值（指针）向量，并在所有记录上叠加绑定。查询时通过将记忆向量与查询键相关，并使用基于边界的决策规则选择最佳匹配值。

Result: 提供了构建和解码算法，以及在显式噪声模型下的概率分析。分析提供了匹配和非匹配分数分布的集中界限、top K解码器的显式阈值和边界设置，以及在标准随机性假设下向量维度的指数误差衰减。

Conclusion: HBF为指针追踪分子数据结构提供了具体可分析的替代方案，实现了单次关联检索，同时量化了维度、数据集大小和噪声之间的权衡。

Abstract: Random access remains a central bottleneck in DNA-based data storage. Existing systems typically retrieve records by PCR enrichment or other multi-step biochemical procedures, which do not naturally support fast, massively parallel, content-addressable queries.
  We introduce the Holographic Bloom Filter (HBF), a probabilistic indexing primitive that stores key-pointer associations as a single high-dimensional memory vector. HBF binds a key vector and a value (pointer) vector using circular convolution and superposes bindings across all records. A query decodes by correlating the memory with the query key and selecting the best matching value using a margin-based decision rule.
  We give construction and decoding algorithms and a probabilistic analysis under explicit noise models (memory corruption and query/key mismatches). The analysis provides concentration bounds for match and non-match score distributions, explicit threshold and margin settings for a top K decoder, and exponential error decay in the vector dimension under standard randomness assumptions.
  HBF offers a concrete, analyzable alternative to pointer-chasing molecular data structures, enabling one-shot associative retrieval while quantifying trade-offs among dimensionality, dataset size, and noise.

</details>


### [15] [Exact (n + 2) Comparison Complexity for the N-Repeated Element Problem](https://arxiv.org/abs/2601.21202)
*Andrew Au*

Main category: cs.DS

TL;DR: 该论文确定了在O(1)额外空间的相等比较模型中，在包含n+1个不同值的2n元素数组中查找重复n次的元素所需的确切比较复杂度为n+2次比较。


<details>
  <summary>Details</summary>
Motivation: 研究在受限计算模型下（相等比较模型，O(1)额外空间）解决特定搜索问题的精确复杂度。这个问题介于简单玩具问题和需要非平凡组合推理的问题之间，为展示精确下界技术提供了一个具体、自包含的示例。

Method: 提出了一个简单的确定性算法，恰好执行n+2次比较。下界证明采用对手论证，利用图论结构：相等查询构建不等式图I，其补图P（潜在相等关系）必须包含两个不相交的n-团或一个(n+1)-团来保持模糊性。通过"支柱匹配"构造和边翻转重构，证明这些结构在n+1次比较后仍然存在，但在n+2次比较时失效。

Result: 证明了n+2次比较是精确的复杂度界限：提出的算法恰好需要n+2次比较，并且任何正确算法在最坏情况下至少需要n+2次比较。

Conclusion: 该研究为精确下界技术提供了一个具体、自包含的演示，连接了玩具问题与非平凡组合推理，建立了在特定约束条件下解决该搜索问题的精确复杂度界限。

Abstract: This paper establishes the exact comparison complexity of finding an element repeated $n$ times in a $2n$-element array containing $n+1$ distinct values, under the equality-comparison model with $O(1)$ extra space. We present a simple deterministic algorithm performing exactly $n+2$ comparisons and prove this bound tight: any correct algorithm requires at least $n+2$ comparisons in the worst case. The lower bound follows from an adversary argument using graph-theoretic structure. Equality queries build an inequality graph $I$; its complement $P$ (potential-equalities) must contain either two disjoint $n$-cliques or one $(n+1)$-clique to maintain ambiguity. We show these structures persist up through $n+1$ comparisons via a "pillar matching" construction and edge-flip reconfiguration, but fail at $n+2$. This result provides a concrete, self-contained demonstration of exact lower-bound techniques, bridging toy problems with nontrivial combinatorial reasoning.

</details>


### [16] [Quantifying Noise in Language Generation](https://arxiv.org/abs/2601.21237)
*Aaron Li,Ian Zhang*

Main category: cs.DS

TL;DR: 该论文研究了语言生成噪声模型，证明了单个噪声字符串会严格减少可生成的集合，且单个噪声与有限噪声等价，这与Bai等人的严格层次结构形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究噪声模型中的噪声量化效应，特别是每个额外噪声字符串的影响。回答Raman和Raman (2025)中关于单个噪声字符串是否严格减少可生成集合的开放问题。

Method: 在Kleinberg和Mullainathan的语言生成框架下，分析Raman和Raman (2025)引入的噪声模型。通过理论证明展示两个互补结果：1) 单个噪声字符串对可生成集合的影响；2) 单个噪声与有限噪声的等价性。

Result: 1) 对于均匀和非均匀生成，单个噪声字符串严格减少可生成的集合；2) 单个噪声字符串的生成与任何有限数量噪声的生成等价；3) 提供了首个已知的非均匀噪声依赖生成性表征。

Conclusion: 噪声模型中的单个噪声字符串具有显著影响：既严格限制生成能力，又与有限噪声等价。这为噪声依赖生成性提供了新的理论理解，并与先前研究的严格层次结构形成鲜明对比。

Abstract: Kleinberg and Mullainathan recently proposed a formal framework for studying the phenomenon of language generation, called language generation in the limit. In this model, an adversary gives an enumeration of example strings from an unknown target language, and the algorithm is tasked with correctly generating unseen strings from the target language within finite time. Refined notions of non-uniform and uniform generation were later introduced by Li, Raman, and Tewari (2025), and a noisy model was introduced by Raman and Raman (2025), which allows the adversary to insert extraneous strings. A natural question in the noisy model is to quantify the effect of noise, by studying the impact of each additional extraneous string. We show two complementary results in this setting. We first show that for both uniform and non-uniform generation, a single noisy string strictly reduces the set of collections that can be generated, thus answering an open question in Raman and Raman (2025). Then, we show for both uniform and non-uniform generation that generation with a single noisy string is equivalent to generation with any finite amount of noise, sharply contrasting with the strict hierarchy for noisy generation in the limit shown by Bai, Panigrahi, and Zhang (2026). Finally, we leverage our previous results to provide the first known characterization for non-uniform noise-dependent generatability.

</details>


### [17] [Algorithms for the local and the global postage stamp problem](https://arxiv.org/abs/2601.21423)
*Léo Colisson Palais,Jean-Guillaume Dumas,Alexis Galan,Bruno Grenet,Aude Maignan*

Main category: cs.DS

TL;DR: 提出改进邮票问题的算法：本地问题的新算法降低时间复杂度和内存需求，全局问题的多项式近似算法，并应用于安全多方计算


<details>
  <summary>Details</summary>
Motivation: 邮票问题（本地和全局）在组合优化中有重要应用，特别是本地问题是NP难的，需要更高效的算法。同时，该问题与安全多方计算中的多项式同态评估有直接联系

Method: 1. 针对本地邮票问题提出新算法，改进时间复杂度和内存需求；2. 针对全局邮票问题提出多项式近似算法并进行复杂度分析；3. 将算法应用于安全多方计算，实现更高效的多项式同态评估

Result: 1. 本地问题算法在时间和空间复杂度上均有改进；2. 全局问题获得多项式近似解；3. 安全多方计算中的集合运算效率得到提升

Conclusion: 提出的算法有效解决了邮票问题的计算挑战，并将理论成果应用于实际的安全多方计算场景，展示了组合优化与密码学的交叉应用价值

Abstract: We consider stamps with different values (denominations) and same dimensions, and an envelope with a fixed maximum number of stamp positions. The local postage stamp problem is to find the smallest value that cannot be realized by the sum of the stamps on the envelope. The global postage stamp problem is to find the set of denominations that maximize that smallest value for a fixed number of distinct denominations. The local problem is NP-hard and we propose here a novel algorithm that improves on both the time complexity bound and the amount of required memory.  We also propose a polynomial approximation algorithm for the global problem together with its complexity analysis. Finally we show that our algorithms allow to improve secure multi-party computations on sets via a more efficient homomorphic evaluation of polynomials on ciphered values.

</details>


### [18] [When Local and Non-Local Meet: Quadratic Improvement for Edge Estimation with Independent Set Queries](https://arxiv.org/abs/2601.21457)
*Tomer Adar,Yahel Hotam,Amit Levi*

Main category: cs.DS

TL;DR: 本文研究了在混合查询模型下估计未知图边数的问题，相比单独使用局部查询或独立集查询模型，混合模型能实现二次改进的查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有图边数估计方法主要基于局部查询模型或独立集查询模型，但单独使用这些模型时查询复杂度较高。本文探索混合查询模型（结合独立集查询、度查询和邻居查询）是否能显著提高边数估计的效率。

Method: 提出混合查询模型，允许算法同时使用独立集查询、度查询和邻居查询。设计了一个随机算法，通过精心设计的查询策略来估计边数，并证明了该算法的查询复杂度上界。

Result: 算法能以O(min(√m, √(n/√m))·(log n)/ε^(5/2))次查询输出(1±ε)-近似边数，并证明了近乎匹配的下界。相比单独使用局部查询或独立集查询模型需要Θ̃(n/√m)次查询，实现了二次改进。

Conclusion: 混合查询模型在边数估计问题上确实比单独使用任何一种查询模型更高效，实现了二次改进的查询复杂度，且这种改进是渐近最优的，无法获得更好的改进。

Abstract: We study the problem of estimating the number of edges in an unknown graph. We consider a hybrid model in which an algorithm may issue independent set, degree, and neighbor queries. We show that this model admits strictly more efficient edge estimation than either access type alone. Specifically, we give a randomized algorithm that outputs a $(1\pm\varepsilon)$-approximation of the number of edges using $O\left(\min\left(\sqrt{m}, \sqrt{\frac{n}{\sqrt{m}}}\right)\cdot\frac{\log n}{\varepsilon^{5/2}}\right)$ queries, and prove a nearly matching lower bound.
  In contrast, prior work shows that in the local query model (Goldreich and Ron, \textit{Random Structures \& Algorithms} 2008) and in the independent set query model (Beame \emph{et al.} ITCS 2018, Chen \emph{et al.} SODA 2020), edge estimation requires $\widetildeΘ(n/\sqrt{m})$ queries in the same parameter regimes. Our results therefore yield a quadratic improvement in the hybrid model, and no asymptotically better improvement is possible.

</details>


### [19] [Improved Approximations for Dial-a-Ride Problems](https://arxiv.org/abs/2601.21652)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文针对多车辆按需乘车问题(mDaRP)提出了两种简单算法，改进了现有近似比和时间复杂度，并将结果应用于经典单车辆DaRP问题。


<details>
  <summary>Details</summary>
Motivation: 多车辆按需乘车问题是车辆路径规划中的基本问题，在拼车、经济和交通领域有广泛应用。现有最佳近似比为O(√λ log m)，需要改进算法性能和近似比。

Method: 提出了两种简单算法：第一种保持O(√λ log m)近似比但改进运行时间；第二种达到O(√(m/λ))近似比。结合两种算法在m=Θ(n)时获得O(∜n log^{1/2}n)近似比。对于m≫n的情况，扩展算法得到O(√(n log n))近似比。

Result: 1. 第一种算法：O(√λ log m)近似比，改进运行时间；2. 第二种算法：O(√(m/λ))近似比；3. 结合算法：m=Θ(n)时O(∜n log^{1/2}n)近似比；4. m≫n时：O(√(n log n))近似比，改进了经典单车辆DaRP的现有最佳近似比O(√n log^2 n)。

Conclusion: 本文为mDaRP问题提出了简单有效的算法，改进了近似比和运行时间，并将结果应用于经典DaRP问题，取得了比现有最佳结果更好的近似比。

Abstract: The multi-vehicle dial-a-ride problem (mDaRP) is a fundamental vehicle routing problem with pickups and deliveries, widely applicable in ride-sharing, economics, and transportation. Given a set of $n$ locations, $h$ vehicles of identical capacity $λ$ located at various depots, and $m$ ride requests each defined by a source and a destination, the goal is to plan non-preemptive routes that serve all requests while minimizing the total travel distance, ensuring that no vehicle carries more than $λ$ passengers at any time. The best-known approximation ratio for the mDaRP remains $\mathcal{O}(\sqrtλ\log m)$.
  We propose two simple algorithms: the first achieves the same approximation ratio of $\mathcal{O}(\sqrtλ\log m)$ with improved running time, and the second attains an approximation ratio of $\mathcal{O}(\sqrt{\frac{m}λ})$. A combination of them yields an approximation ratio of $\mathcal{O}(\sqrt[4]{n}\log^{\frac{1}{2}}n)$ under $m=Θ(n)$. Moreover, for the case $m\gg n$, by extending our algorithms, we derive an $\mathcal{O}(\sqrt{n\log n})$-approximation algorithm, which also improves the current best-known approximation ratio of $\mathcal{O}(\sqrt{n}\log^2n)$ for the classic (single-vehicle) DaRP, obtained by Gupta et al. (ACM Trans. Algorithms, 2010).

</details>


### [20] [Improved Approximations for the Unsplittable Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2601.21660)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文改进了容量限制车辆路径问题（CVRP）的近似算法，针对固定容量和一般容量两种情况分别提出了新的近似比：固定容量下为α+1+ln(2-½y₀)<3.0897，一般容量下为α+1+y₁+ln(2-2y₁)+δ<3.1759，均优于之前的最佳结果α+1+ln2+δ<3.1932。


<details>
  <summary>Details</summary>
Motivation: 容量限制车辆路径问题是组合优化中研究最广泛的问题之一，但现有的近似算法仍有改进空间。之前的最佳近似比为α+1+ln2+δ<3.1932，本文旨在进一步改进这一结果。

Method: 提出了两种改进的近似算法：第一种针对固定车辆容量情况，通过求解特定方程得到优化参数y₀；第二种针对一般车辆容量情况，通过求解另一方程得到参数y₁。两种算法都利用了Blauth、Traub和Vygen的改进结果。

Result: 固定容量CVRP的近似比改进到α+1+ln(2-½y₀)<3.0897，其中y₀>0.39312；一般容量CVRP的近似比改进到α+1+y₁+ln(2-2y₁)+δ<3.1759，其中y₁>0.17458。两种结果都优于之前的最佳近似比。

Conclusion: 本文显著改进了CVRP问题的近似算法性能，为固定容量和一般容量两种情况分别提供了更好的近似保证，推进了该领域的研究进展。

Abstract: The capacitated vehicle routing problem (CVRP) is one of the most extensively studied problems in combinatorial optimization. In this problem, we are given a depot and a set of customers, each with a demand, embedded in a metric space. The objective is to find a set of tours, each starting and ending at the depot, operated by the capacititated vehicle at the depot to serve all customers, such that all customers are served, and the total travel cost is minimized. We consider the unplittable variant, where the demand of each customer must be served entirely by a single tour. Let $α$ denote the current best-known approximation ratio for the metric traveling salesman problem. The previous best approximation ratio was $α+1+\ln 2+δ<3.1932$ for a small constant $δ>0$ (Friggstad et al., Math. Oper. Res. 2025), which can be further improved by a small constant using the result of Blauth, Traub, and Vygen (Math. Program. 2023). In this paper, we propose two improved approximation algorithms. The first algorithm focuses on the case of fixed vehicle capacity and achieves an approximation ratio of $α+1+\ln\bigl(2-\frac{1}{2}y_0\bigr)<3.0897$, where $y_0>0.39312$ is the unique root of $\ln\bigl(2-\frac{1}{2}y\bigr)=\frac{3}{2}y$. The second algorithm considers general vehicle capacity and achieves an approximation ratio of $α+1+y_1+\ln\left(2-2y_1\right)+δ<3.1759$ for a small constant $δ>0$, where $y_1>0.17458$ is the unique root of $\frac{1}{2} y_1+ 6 (1-y_1)\bigl(1-e^{-\frac{1}{2} y_1}\bigr) =\ln\left(2-2y_1\right)$. Both approximations can be further improved by a small constant using the result of Blauth, Traub, and Vygen (Math. Program. 2023).

</details>


### [21] [Adaptively Robust Resettable Streaming](https://arxiv.org/abs/2601.21989)
*Edith Cohen,Elena Gribelyuk,Jelani Nelson,Uri Stemmer*

Main category: cs.DS

TL;DR: 本文提出了首个针对可重置流的自适应鲁棒性草图算法，突破了现有算法在多项式空间下易受自适应攻击的限制，实现了对数多项式空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有可重置流模型中的草图算法即使在多项式空间下也容易受到自适应对抗攻击，这限制了其在主动资源监控、支持删除和机器遗忘等应用中的安全性。

Method: 通过差分隐私技术（特别是持续观察的二叉树机制）来保护草图的内部随机性，绕过线性和可组合草图的不可能性结果，设计专门的流式草图。

Result: 实现了首个自适应鲁棒性草图，支持L_p矩（p∈[0,1]）、基数、和以及Bernstein统计量等（次）线性统计量，在流长度上保持对数多项式空间复杂度。

Conclusion: 通过差分隐私技术成功构建了可重置流模型中的自适应鲁棒性草图，解决了现有算法的安全漏洞，为实际应用提供了安全高效的数据流处理方案。

Abstract: We study algorithms in the resettable streaming model, where the value of each key can either be increased or reset to zero. The model is suitable for applications such as active resource monitoring with support for deletions and machine unlearning. We show that all existing sketches for this model are vulnerable to adaptive adversarial attacks that apply even when the sketch size is polynomial in the length of the stream.
  To overcome these vulnerabilities, we present the first adaptively robust sketches for resettable streams that maintain polylogarithmic space complexity in the stream length. Our framework supports (sub) linear statistics including $L_p$ moments for $p\in[0,1]$ (in particular, Cardinality and Sum) and Bernstein statistics. We bypass strong impossibility results known for linear and composable sketches by designing dedicated streaming sketches robustified via Differential Privacy. Unlike standard robustification techniques, which provide limited benefits in this setting and still require polynomial space in the stream length, we leverage the Binary Tree Mechanism for continual observation to protect the sketch's internal randomness. This enables accurate prefix-max error guarantees with polylogarithmic space.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [22] [A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning](https://arxiv.org/abs/2601.21162)
*Jiate Liu,Zebin Chen,Shaobo Qiao,Mingchen Ju,Danting Zhang,Bocheng Han,Shuyue Yu,Xin Shu,Jingling Wu,Dong Wen,Xin Cao,Guanfeng Liu,Zhengyi Yang*

Main category: cs.IR

TL;DR: A2RAG是一个自适应智能Graph-RAG框架，通过自适应控制器和智能检索器解决混合难度工作负载和提取损失问题，在提高召回率的同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前Graph-RAG在实际部署中面临两个瓶颈：1) 混合难度工作负载中，一刀切的检索方法要么在简单查询上浪费成本，要么无法处理复杂的多跳问题；2) 提取损失问题，图抽象会丢失源文本中的细粒度限定信息。

Method: 提出A2RAG框架，包含自适应控制器（验证证据充分性，仅在必要时触发针对性细化）和智能检索器（逐步提升检索力度，将图信号映射回源文本以应对提取损失和不完整图）。

Result: 在HotpotQA和2WikiMultiHopQA数据集上，A2RAG在Recall@2指标上分别获得+9.9和+11.8的绝对提升，同时将token消耗和端到端延迟降低约50%（相对于迭代多跳基线）。

Conclusion: A2RAG通过自适应控制和智能检索机制，有效解决了Graph-RAG的实际部署瓶颈，在保证可靠推理的同时显著提升了成本效率和性能。

Abstract: Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

</details>


### [23] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 本文提出了SteerEval评估框架，用于测试自然语言推荐系统对用户多样化控制指令的响应能力，超越了传统基于属性的可操控性评估。


<details>
  <summary>Details</summary>
Motivation: 自然语言用户配置文件虽然提高了推荐系统的可解释性和可操控性，但现有评估方法主要关注明显属性（如电影类型），未能充分测试更丰富、更细致的用户控制形式，这限制了可操控推荐系统的实际应用潜力。

Method: 作者提出了SteerEval评估框架，使用从类型到内容警告等多种干预措施来衡量更细致多样的可操控性形式。评估了一系列预训练的自然语言推荐系统，分析了在相对小众主题上的可操控潜力与限制，并比较了不同配置文件和推荐干预措施对操控效果的影响。

Result: 研究评估了自然语言推荐系统的可操控性，发现在相对小众主题上存在操控潜力但也有限制，不同干预措施对操控效果有不同影响。基于研究结果提出了实用的设计建议。

Conclusion: SteerEval框架填补了现有可操控性评估的空白，为更丰富多样的用户控制形式提供了评估方法。研究结果为可操控推荐系统的设计提供了实用指导，并指出了未来研究方向。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [24] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 提出MPCoT+LRKD框架，通过多视角思维链推理和潜在推理知识蒸馏，提升电商搜索相关性建模的准确性和实时性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的相关性模型存在两个关键限制：1) 单视角思维链推理无法捕捉电商相关性的多维度特性（用户意图、属性匹配、业务规则等）；2) 现有知识蒸馏方法在推理时丢弃了思维链的推理结构，只将其作为辅助信号，丧失了推理效用

Method: 1) 教师模型采用多视角思维链(MPCoT)生成多样化推理依据，结合监督微调(SFT)和直接偏好优化(DPO)构建鲁棒推理器；2) 提出潜在推理知识蒸馏(LRKD)，为学生模型配备轻量级推理时潜在推理提取器，高效内部化LLM的复杂推理能力

Result: 在服务数千万用户的电商搜索广告平台上进行离线和在线A/B测试，方法在商业性能和用户体验方面均显示出显著提升

Conclusion: 提出的MPCoT+LRKD框架能更好地利用思维链语义，在优化流程中提升电商搜索相关性建模的准确性和实时部署能力

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [25] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: Inf-DDS：基于强化学习的轻量级自适应训练数据采样框架，通过影响奖励信号优化多数据集训练策略，显著提升检索性能并降低GPU消耗


<details>
  <summary>Details</summary>
Motivation: 通用开放域稠密检索系统通常使用多样化的语料库和搜索任务进行训练，但如何最优地采样这些训练数据尚未得到充分研究。传统方法要么均匀采样，要么按实例数量比例采样，要么依赖专家监督，这些方法可能不是最优的。

Method: 提出Inf-DDS框架，使用强化学习自适应地重新加权训练数据集。该框架通过影响奖励信号指导采样策略，迭代优化采样策略，优先选择能最大化目标开发集性能的数据集。相比基于梯度的方法更加轻量级。

Result: 在广泛的文本检索任务中，该方法显著提升了检索性能：训练多语言bge-m3模型时获得5.03的绝对NDCG@10提升，训练all-MiniLM-L6-v2时获得0.94的绝对NDCG@10提升。同时GPU计算成本降低1.5-4倍。

Conclusion: Inf-DDS是一种有效的自适应训练数据采样框架，能够在显著降低计算成本的同时，通过优化训练数据采样策略来提升嵌入模型的检索性能，特别是在多数据集训练场景中表现优异。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [26] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: 快手电商推出OneMall端到端生成式推荐框架，统一商品卡片、短视频、直播等多种电商场景，通过语义分词器、Transformer架构和强化学习管道实现全链路优化，显著提升各场景业务指标。


<details>
  <summary>Details</summary>
Motivation: 电商平台存在多种商品分发场景（商品卡片、短视频、直播），传统推荐系统难以统一处理，需要端到端的生成式推荐框架来整合多场景并提升推荐效果。

Method: 1. 电商语义分词器：捕获真实世界语义和跨场景业务特定商品关系；2. Transformer架构：使用Query-Former压缩长序列，Cross-Attention融合多行为序列，Sparse MoE实现可扩展自回归生成；3. 强化学习管道：连接检索和排序模型，用排序模型作为奖励信号优化端到端策略检索模型。

Result: 在所有电商场景均取得显著提升：商品卡片GMV提升13.01%，短视频订单提升15.32%，直播订单提升2.78%。已部署服务快手超过4亿日活用户。

Conclusion: OneMall成功构建了统一的端到端生成式电商推荐框架，有效整合多场景推荐，通过语义理解、Transformer架构和强化学习的结合实现了业务指标的全面提升。

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [27] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 跨域推荐中的公平性问题分析及解决方案：识别了跨域差异转移和跨域信息增益不公平两大挑战，提出了包含自适应数据整合和信息理论方法的公平增强框架。


<details>
  <summary>Details</summary>
Motivation: 跨域推荐虽然能提高推荐质量，但研究发现它会加剧群体层面的不公平性。本文旨在深入分析这种不公平现象产生的原因，并提出解决方案。

Method: 提出了跨域公平增强(CDFA)框架，包含两个核心组件：1)通过自适应整合未标记数据来平衡不同群体间训练信号的信息量，缓解跨域差异转移；2)采用信息理论方法重新分配跨域信息增益，确保不同群体间利益分配的公平性。

Result: 在多个数据集和基线方法上的实验表明，该框架能显著减少跨域推荐中的不公平性，同时不牺牲整体推荐性能，甚至还能提升性能。

Conclusion: 本文首次系统分析了跨域推荐中公平性问题的根源，并提出了有效的解决方案，为构建更公平的跨域推荐系统提供了理论和实践基础。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [28] [LEMUR: Learned Multi-Vector Retrieval](https://arxiv.org/abs/2601.21853)
*Elias Jääsaari,Ville Hyvönen,Teemu Roos*

Main category: cs.IR

TL;DR: LEMUR是一个高效的多向量相似性搜索框架，通过两层问题约简将多向量搜索转化为单向量ANN搜索，比现有方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 多向量表示（如ColBERT）在信息检索中比单向量表示有更好的召回率，但带来了显著的延迟增加，需要设计高效的多向量近似最近邻搜索算法。

Method: LEMUR采用两层问题约简：1) 将多向量相似性搜索公式化为可用单隐藏层神经网络解决的监督学习问题；2) 将该模型的推理约简为其潜在空间中的单向量相似性搜索，从而利用现有单向量ANNS方法加速检索。

Result: LEMUR在ColBERTv2嵌入、现代多向量文本模型和多向量视觉文档检索模型的嵌入上都进行了评估，比早期多向量相似性搜索方法快一个数量级。

Conclusion: LEMUR提供了一个简单而高效的框架，通过将多向量搜索转化为单向量ANN搜索，显著提高了多向量检索的效率，同时保持了检索质量。

Abstract: Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.

</details>


### [29] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: SpecTran是一个基于频谱感知的transformer适配器，在频谱域中操作，通过关注完整频谱来选择聚合信息组件，解决传统方法维度塌陷和频谱信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型主要学习低维物品ID嵌入，忽略了物品文本信息。现有基于LLM的文本嵌入注入方法存在两个主要问题：适配器方法有维度塌陷问题，SVD方法则过于刚性且只考虑主要频谱成分而丢弃丰富信息。

Method: 提出SpecTran，一个基于频谱感知的transformer适配器，在频谱域中操作，关注完整频谱来选择聚合信息组件。使用可学习的频谱位置编码将奇异值线索作为归纳偏置，引导注意力关注显著频谱成分并促进嵌入维度的多样性。

Result: 在四个真实世界数据集和三个序列推荐骨干网络上，SpecTran始终优于强基线方法，平均提升9.17%。

Conclusion: SpecTran通过频谱感知的transformer适配器有效解决了现有文本嵌入注入方法的局限性，在频谱域中充分利用完整频谱信息，显著提升了序列推荐性能。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [30] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: LANCER是一种基于LLM的重排序方法，专注于提升长文本检索的信息覆盖率，通过生成子问题并预测文档回答能力来优化检索结果。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法主要针对相关性排序优化，而长文本检索增强生成（如自动报告生成）需要覆盖广泛的相关信息，现有方法在信息覆盖率方面存在不足。

Method: 提出LANCER方法：1) 预测满足信息需求需要回答的子问题；2) 预测哪些文档能回答这些子问题；3) 重新排序文档以最大化信息块覆盖率。

Result: 实验结果显示LANCER在信息块覆盖率指标上优于其他基于LLM的重排序方法，获得了更高的α-nDCG和信息覆盖率，子问题生成是关键因素。

Conclusion: LANCER通过关注信息覆盖率而非单纯相关性，有效提升了长文本检索增强生成的质量，子问题生成是实现这一目标的核心机制。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [31] [On Approximate Nash Equilibria in Mean Field Games](https://arxiv.org/abs/2601.20910)
*Mao Fabrice Djete,Nizar Touzi*

Main category: cs.GT

TL;DR: 论文研究大群体对称博弈中的近似纳什均衡，通过均值场博弈的均衡解引入，证明这些策略满足L∞近似纳什均衡概念，确保个体最优单边偏离收益在玩家和初始特征上均匀小。


<details>
  <summary>Details</summary>
Motivation: 研究大群体对称博弈中如何通过均值场博弈方法构建近似纳什均衡，解决传统纳什均衡计算复杂度高的问题，为大规模博弈提供理论分析框架。

Method: 采用均值场博弈理论，将大群体博弈转化为对应的均值场博弈，通过分析均衡解的性质，证明这些策略满足L∞近似纳什均衡条件，涵盖静态模型和动态连续时间设置。

Result: 证明均值场博弈均衡解在大群体渐近下构成近似纳什均衡，且满足L∞一致性条件，个体最优单边偏离收益在玩家间和初始特征上均匀收敛到零。

Conclusion: 均值场博弈方法为大规模对称博弈提供了有效的近似纳什均衡构建框架，L∞近似均衡概念具有强一致性保证，适用于静态和动态连续时间场景。

Abstract: In the context of large population symmetric games, approximate Nash equilibria are introduced through equilibrium solutions of the corresponding mean field game in the sense that the individual gain from optimal unilateral deviation under such strategies converges to zero in the large population size asymptotic. We show that these strategies satisfy an $Ł^\infty$ notion of approximate Nash equilibrium which guarantees that the individual gain from optimal unilateral deviation is small uniformly among players and uniformly on their initial characteristics. We establish these results in the context of static models and in the dynamic continuous time setting, and we cover situations where the agents' criteria depend on the conditional law of the controlled state process.

</details>


### [32] [Shortlisting: a Principled Approach](https://arxiv.org/abs/2601.21277)
*Edith Elkind,Qishen Han,Lirong Xia*

Main category: cs.GT

TL;DR: 本文是一篇关于短名单选择问题的蓝海研究论文，旨在强调这一被忽视问题的重要性，区分相关概念，提供初步思考，并呼吁更多研究关注。


<details>
  <summary>Details</summary>
Motivation: 随着参与式决策需求的增长和候选空间的不断扩大，对高效公平的短名单选择程序的需求日益迫切。然而，目前对这一问题的原则性研究很少，本文旨在填补这一空白。

Method: 本文是一篇蓝海研究论文，采用概念分析和理论探讨的方法，通过区分短名单选择与相关问题的差异，提供初步的理论框架和思考方向。

Result: 论文提出了短名单选择问题的重要性，区分了它与传统选择问题的不同，并阐述了原则性短名单选择可以带来的潜在好处。

Conclusion: 原则性短名单选择能够减少认知负担、实现公平集体决策、鼓励更广泛参与，并最终建立对民主系统的信任。本文呼吁更多研究者关注这一重要但被忽视的问题。

Abstract: Shortlisting is the process of selecting a subset of alternatives from a larger pool for further consideration or final decision-making. It is widely applied in social choice and multi-agent system scenarios. The growing demand for participatory decision-making and the continuously expanding space of candidates create an urgent need for efficient and fair shortlisting procedures. However, little principled study has been done on this problem. This blue-sky paper aims to highlight the overlooked significance of shortlisting, distinguish it from related problems, provide initial thoughts, and, more importantly, serve as a call to arms. We envision that principled shortlisting can reduce cognitive burden, enable fair collective decisions, encourage broader participation, and ultimately build trust in democratic systems.

</details>


### [33] [Alliance Mechanisms in General Lotto Games](https://arxiv.org/abs/2601.21319)
*Vade Shah,Jason R. Marden*

Main category: cs.GT

TL;DR: 本文比较了联盟博弈中三种联盟机制：预算转移、竞争转移和联合转移，发现它们在集体改进方面等效，但在互利改进方面存在根本差异。


<details>
  <summary>Details</summary>
Motivation: 研究不同联盟机制在竞争性资源分配中的比较，特别是在联盟博弈中，玩家如何通过不同机制合作对抗共同对手。

Method: 使用联盟性一般乐透博弈模型，分析三种联盟机制：预算转移（资源捐赠）、竞争转移（竞争重新分配）和联合转移（两者同时）。研究这些机制在集体改进和互利改进方面的表现。

Result: 1. 三种机制在互利改进方面根本不同：预算和竞争转移只在有限游戏子集中存在互利机会，而联合转移在几乎所有游戏中都存在互利机会。2. 三种机制在集体改进方面等效：几乎所有游戏实例中都存在集体改进机会，且三种机制都能达到相同的最大集体收益。

Conclusion: 不同联盟机制的有效性取决于联盟的目标：对于集体改进，三种机制等效；但对于互利改进，联合转移明显优于其他两种机制。

Abstract: How do different alliance mechanisms compare? In this work, we analyze various methods of forming an alliance in the Coalitional General Lotto game, a simple model of competitive resource allocation. In the game, Players 1 and 2 independently compete against a common Adversary by allocating their limited resource budgets towards separate sets of contests; an agent wins a contest by allocating more resources towards it than their opponent. In this setting, we study three alliance mechanisms: budget transfers (resource donation), contest transfers (contest redistribution), and joint transfers (both simultaneously). For all three mechanisms, we study when they present opportunities for collective improvement (the sum of the Players' payoffs increases) or mutual improvement (both Players' individual payoffs increase). In our first result, we show that all three are fundamentally different with regards to mutual improvement; in particular, mutually beneficial budget and contest transfers exist in distinct, limited subsets of games, whereas mutually beneficial joint transfers exist in almost all games. However, in our second result, we demonstrate that all three mechanisms are equivalent when it comes to collective improvement; that is, collectively beneficial budget, contest, and joint transfers exist in almost all game instances, and all three mechanisms achieve the same maximum collective payoff. Together, these results demonstrate that differences between mechanisms depend fundamentally on the objective of the alliance.

</details>
