{"id": "2510.19095", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.19095", "abs": "https://arxiv.org/abs/2510.19095", "authors": ["Alain Couvreur", "Rakhi Pratihar"], "title": "Recursive decoding of binary rank Reed-Muller codes and Plotkin construction for matrix codes", "comment": null, "summary": "In 2021, Augot, Couvreur, Lavauzelle and Neri introduced a new class of rank\nmetric codes which can be regarded as rank metric counterparts of Reed-Muller\ncodes. Given a finite Galois extension $\\mathbb{L} / \\mathbb{K}$, these codes\nare defined as some specific $\\mathbb{L}$-subspaces of the twisted group\nalgebra $\\mathbb{L} [\\textrm{G}]$. We investigate the decoding of such codes in\nthe \"binary\" case, \\emph{i.e.,} when $\\textrm{G} = (\\mathbb{Z}/2\\mathbb{Z})^m$.\nOur approach takes its inspiration from the decoding of Hamming metric binary\nReed-Muller codes using their recursive Plotkin \"$(u ~|~ u+v)$\" structure. If\nour recursive algorithm restricts to a specific subclass of rank metric\nReed-Muller codes, its asymptotic complexity beats that of the recently\nproposed decoding algorithm for arbitrary rank metric Reed-Muller codes based\non Dickson matrices. Also, this decoder is of completely different nature and\nleads a natural rank metric counterpart of the Plotkin construction. To\nillustrate this, we also propose a generic Plotkin-like construction for matrix\nrank metric codes with an associate decoder, which can be applied to any pair\nof codes equipped with an efficient decoder."}
{"id": "2510.19759", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.19759", "abs": "https://arxiv.org/abs/2510.19759", "authors": ["Nemanja Stefan PeroviÄ‡", "Keshav Singh", "Chih-Peng Li", "Mark F. Flanagan"], "title": "Weighted Sum Rate Optimization for Movable Antenna Enabled Near-Field ISAC", "comment": "6 pages, 5 figures", "summary": "Integrated sensing and communication (ISAC) has been recognized as one of the\nkey technologies capable of simultaneously improving communication and sensing\nservices in future wireless networks. Moreover, the introduction of recently\ndeveloped movable antennas (MAs) has the potential to further increase the\nperformance gains of ISAC systems. Achieving these gains can pose a significant\nchallenge for MA-enabled ISAC systems operating in the near-field due to the\ncorresponding spherical wave propagation. Motivated by this, in this paper we\nmaximize the weighted sum rate (WSR) for communication users while maintaining\na minimal sensing requirement in an MA-enabled near-field ISAC system. To\nachieve this goal, we propose an algorithm that optimizes the sensing receive\ncombiner, the communication precoding matrices, the sensing transmit beamformer\nand the positions of the users' MAs in an alternating manner. Simulation\nresults show that using MAs in near-field ISAC systems provides a substantial\nperformance advantage compared to near-field ISAC systems with only fixed\nantennas. Additionally, we demonstrate that the highest WSR is obtained when\nlarger weights are allocated to the users placed closer to the BS, and that the\nsensing performance is significantly more affected by the minimum sensing\nsignal-to-interference-plus-noise ratio (SINR) threshold compared to the\ncommunication performance."}
{"id": "2510.19025", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19025", "abs": "https://arxiv.org/abs/2510.19025", "authors": ["Hamed Jelodar", "Samita Bai", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains", "comment": null, "summary": "Dataset availability and quality remain critical challenges in machine\nlearning, especially in domains where data are scarce, expensive to acquire, or\nconstrained by privacy regulations. Fields such as healthcare, biomedical\nresearch, and cybersecurity frequently encounter high data acquisition costs,\nlimited access to annotated data, and the rarity or sensitivity of key events.\nThese issues-collectively referred to as the dataset challenge-hinder the\ndevelopment of accurate and generalizable machine learning models in such\nhigh-stakes domains. To address this, we introduce FlexiDataGen, an adaptive\nlarge language model (LLM) framework designed for dynamic semantic dataset\ngeneration in sensitive domains. FlexiDataGen autonomously synthesizes rich,\nsemantically coherent, and linguistically diverse datasets tailored to\nspecialized fields. The framework integrates four core components: (1)\nsyntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic\nelement injection, and (4) iterative paraphrasing with semantic validation.\nTogether, these components ensure the generation of high-quality,\ndomain-relevant data. Experimental results show that FlexiDataGen effectively\nalleviates data shortages and annotation bottlenecks, enabling scalable and\naccurate machine learning model development."}
{"id": "2510.19002", "categories": ["cs.GT", "cs.LG", "econ.TH", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19002", "abs": "https://arxiv.org/abs/2510.19002", "authors": ["Javier Cembrano", "Felix Fischer", "Max Klimm"], "title": "Impartial Selection with Predictions", "comment": null, "summary": "We study the selection of agents based on mutual nominations, a theoretical\nproblem with many applications from committee selection to AI alignment. As\nagents both select and are selected, they may be incentivized to misrepresent\ntheir true opinion about the eligibility of others to influence their own\nchances of selection. Impartial mechanisms circumvent this issue by\nguaranteeing that the selection of an agent is independent of the nominations\ncast by that agent. Previous research has established strong bounds on the\nperformance of impartial mechanisms, measured by their ability to approximate\nthe number of nominations for the most highly nominated agents. We study to\nwhat extent the performance of impartial mechanisms can be improved if they are\ngiven a prediction of a set of agents receiving a maximum number of\nnominations. Specifically, we provide bounds on the consistency and robustness\nof such mechanisms, where consistency measures the performance of the\nmechanisms when the prediction is accurate and robustness its performance when\nthe prediction is inaccurate. For the general setting where up to $k$ agents\nare to be selected and agents nominate any number of other agents, we give a\nmechanism with consistency $1-O\\big(\\frac{1}{k}\\big)$ and robustness\n$1-\\frac{1}{e}-O\\big(\\frac{1}{k}\\big)$. For the special case of selecting a\nsingle agent based on a single nomination per agent, we prove that\n$1$-consistency can be achieved while guaranteeing $\\frac{1}{2}$-robustness. A\nclose comparison with previous results shows that (asymptotically) optimal\nconsistency can be achieved with little to no sacrifice in terms of robustness."}
{"id": "2510.19049", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19049", "abs": "https://arxiv.org/abs/2510.19049", "authors": ["Aaron Bernstein", "Jiale Chen"], "title": "From Unweighted to Weighted Dynamic Matching in Non-Bipartite Graphs: A Low-Loss Reduction", "comment": null, "summary": "We study the approximate maximum weight matching (MWM) problem in a fully\ndynamic graph subject to edge insertions and deletions. We design\nmeta-algorithms that reduce the problem to the unweighted approximate maximum\ncardinality matching (MCM) problem. Despite recent progress on bipartite graphs\n-- Bernstein-Dudeja-Langley (STOC 2021) and\nBernstein-Chen-Dudeja-Langley-Sidford-Tu (SODA 2025) -- the only previous\nmeta-algorithm that applied to non-bipartite graphs suffered a $\\frac{1}{2}$\napproximation loss (Stubbs-Williams, ITCS 2017). We significantly close the\nweighted-and-unweighted gap by showing the first low-loss reduction that\ntransforms any fully dynamic $(1-\\varepsilon)$-approximate MCM algorithm on\nbipartite graphs into a fully dynamic $(1-\\varepsilon)$-approximate MWM\nalgorithm on general (not necessarily bipartite) graphs, with only a\n$\\mathrm{poly}(\\log n/\\varepsilon)$ overhead in the update time. Central to our\napproach is a new primal-dual framework that reduces the computation of an\napproximate MWM in general graphs to a sequence of approximate induced matching\nqueries on an auxiliary bipartite extension. In addition, we give the first\nconditional lower bound on approximate partially dynamic matching with\nworst-case update time."}
{"id": "2510.19166", "categories": ["cs.MM", "68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19166", "abs": "https://arxiv.org/abs/2510.19166", "authors": ["Hongjun Liu", "Leyu Zhou", "Zijianghao Yang", "Chao Yao"], "title": "Step-Aware Residual-Guided Diffusion for EEG Spatial Super-Resolution", "comment": "ICLR 2026 Conference Submission", "summary": "For real-world BCI applications, lightweight Electroencephalography (EEG)\nsystems offer the best cost-deployment balance. However, such spatial sparsity\nof EEG limits spatial fidelity, hurting learning and introducing bias. EEG\nspatial super-resolution methods aim to recover high-density EEG signals from\nsparse measurements, yet is often hindered by distribution shift and signal\ndistortion and thus reducing fidelity and usability for EEG analysis and\nvisualization. To overcome these challenges, we introduce SRGDiff, a step-aware\nresidual-guided diffusion model that formulates EEG spatial super-resolution as\ndynamic conditional generation. Our key idea is to learn a dynamic residual\ncondition from the low-density input that predicts the step-wise temporal and\nspatial details to add and uses the evolving cue to steer the denoising process\ntoward high-density reconstructions. At each denoising step, the proposed\nresidual condition is additively fused with the previous denoiser feature maps,\nthen a step-dependent affine modulation scales and shifts the activation to\nproduce the current features. This iterative procedure dynamically extracts\nstep-wise temporal rhythms and spatial-topographic cues to steer high-density\nrecovery and maintain a fidelity-consistency balance. We adopt a comprehensive\nevaluation protocol spanning signal-, feature-, and downstream-level metrics\nacross SEED, SEED-IV, and Localize-MI and multiple upsampling scales. SRGDiff\nachieves consistent gains of up to 40% over strong baselines, proving its\nsuperiority in the task of EEG spatial super-resolution. Moreover, topographic\nvisualizations comparison and substantial EEG-FID gains jointly indicate that\nour SR EEG mitigates the spatial-spectral shift between low- and high-density\nrecordings."}
{"id": "2510.18936", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18936", "abs": "https://arxiv.org/abs/2510.18936", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Samita Bai", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "SBAN: A Framework \\& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining", "comment": null, "summary": "This paper introduces SBAN (Source code, Binary, Assembly, and Natural\nLanguage Description), a large-scale, multi-dimensional dataset designed to\nadvance the pre-training and evaluation of large language models (LLMs) for\nsoftware code analysis. SBAN comprises more than 3 million samples, including\n2.9 million benign and 672,000 malware respectively, each represented across\nfour complementary layers: binary code, assembly instructions, natural language\ndescriptions, and source code. This unique multimodal structure enables\nresearch on cross-representation learning, semantic understanding of software,\nand automated malware detection. Beyond security applications, SBAN supports\nbroader tasks such as code translation, code explanation, and other software\nmining tasks involving heterogeneous data. It is particularly suited for\nscalable training of deep models, including transformers and other LLM\narchitectures. By bridging low-level machine representations and high-level\nhuman semantics, SBAN provides a robust foundation for building intelligent\nsystems that reason about code. We believe that this dataset opens new\nopportunities for mining software behavior, improving security analytics, and\nenhancing LLM capabilities in pre-training and fine-tuning tasks for software\ncode mining."}
{"id": "2510.19197", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19197", "abs": "https://arxiv.org/abs/2510.19197", "authors": ["Nofar Carmeli", "Nikolaos Tziavelis"], "title": "Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum", "comment": null, "summary": "We investigate the fine-grained complexity of direct access to Conjunctive\nQuery (CQ) answers according to their position, ordered by the minimum (or\nmaximum) value between attributes. We further use the tools we develop to\nexplore a wealth of related tasks. We consider the task of ranked enumeration\nunder min/max orders, as well as tasks concerning CQs with predicates of the\nform x <= min X , where X is a set of variables and x is a single variable:\ncounting, enumeration, direct access, and predicate elimination (i.e.,\ntransforming the pair of query and database to an equivalent pair without\nmin-predicates). For each task, we establish a complete dichotomy for\nself-join-free CQs, precisely identifying the cases that are solvable in\nnear-ideal time, i.e., (quasi)linear preprocessing time followed by constant or\nlogarithmic time per output."}
{"id": "2510.19098", "categories": ["cs.GT", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19098", "abs": "https://arxiv.org/abs/2510.19098", "authors": ["Valia Efthymiou", "Ekaterina Fedorova", "Chara Podimata"], "title": "Desirable Effort Fairness and Optimality Trade-offs in Strategic Learning", "comment": null, "summary": "Strategic learning studies how decision rules interact with agents who may\nstrategically change their inputs/features to achieve better outcomes. In\nstandard settings, models assume that the decision-maker's sole scope is to\nlearn a classifier that maximizes an objective (e.g., accuracy) assuming that\nagents best respond. However, real decision-making systems' goals do not align\nexclusively with producing good predictions. They may consider the external\neffects of inducing certain incentives, which translates to the change of\ncertain features being more desirable for the decision maker. Further, the\nprincipal may also need to incentivize desirable feature changes fairly across\nheterogeneous agents. How much does this constrained optimization (i.e.,\nmaximize the objective, but restrict agents' incentive disparity) cost the\nprincipal? We propose a unified model of principal-agent interaction that\ncaptures this trade-off under three additional components: (1) causal\ndependencies between features, such that changes in one feature affect others;\n(2) heterogeneous manipulation costs between agents; and (3) peer learning,\nthrough which agents infer the principal's rule. We provide theoretical\nguarantees on the principal's optimality loss constrained to a particular\ndesirability fairness tolerance for multiple broad classes of fairness\nmeasures. Finally, through experiments on real datasets, we show the explicit\ntradeoff between maximizing accuracy and fairness in desirability effort."}
{"id": "2510.19175", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19175", "abs": "https://arxiv.org/abs/2510.19175", "authors": ["William Kuszmaul", "Jingxun Liang", "Renfei Zhou"], "title": "Succinct Dynamic Rank/Select: Bypassing the Tree-Structure Bottleneck", "comment": "47 pages, 3 figures, in SODA 2026", "summary": "We show how to construct a dynamic ordered dictionary, supporting\ninsert/delete/rank/select on a set of $n$ elements from a universe of size $U$,\nthat achieves the optimal amortized expected time complexity of $O(1 + \\log n /\n\\log \\log U)$, while achieving a nearly optimal space consumption of $\\log\n\\binom{U}{n} + n / 2^{(\\log n)^{\\Omega(1)}} + \\text{polylog}\\, U$ bits in the\nregime where $U = \\text{poly}(n)$. This resolves an open question by Pibiri and\nVenturini as to whether a redundancy (a.k.a. space overhead) of $o(n)$ bits is\npossible, and is the first dynamic solution to bypass the so-called\ntree-structure bottleneck, in which the bits needed to encode some dynamic tree\nstructure are themselves enough to force a redundancy of\n$\\widetilde{\\Omega}(n)$ bits. Our main technical building block is a dynamic\nbalanced binary search tree, which we call the compressed tabulation-weighted\ntreap, that itself achieves a surprising time/space tradeoff. The tree supports\n$\\text{polylog}\\, n$-time operations and requires a static lookup table of size\n$\\text{poly}(n) + \\text{polylog}\\, U$ -- but, in exchange for these, the tree\nis able to achieve a remarkable space guarantee. Its total space redundancy is\n$O(\\log U)$ bits. In fact, if the tree is given $n$ and $U$ for free, then the\nredundancy further drops to $O(1)$ bits."}
{"id": "2510.19520", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.19520", "abs": "https://arxiv.org/abs/2510.19520", "authors": ["Xiangyu Li", "Haojie Yang", "Kaimiao Hu", "Runzhi Wu", "Liangliang Liu", "Ran Su"], "title": "CDI-DTI: A Strong Cross-domain Interpretable Drug-Target Interaction Prediction Framework Based on Multi-Strategy Fusion", "comment": null, "summary": "Accurate prediction of drug-target interactions (DTI) is pivotal for drug\ndiscovery, yet existing methods often fail to address challenges like\ncross-domain generalization, cold-start prediction, and interpretability. In\nthis work, we propose CDI-DTI, a novel cross-domain interpretable framework for\nDTI prediction, designed to overcome these limitations. By integrating\nmulti-modal features-textual, structural, and functional-through a\nmulti-strategy fusion approach, CDI-DTI ensures robust performance across\ndifferent domains and in cold-start scenarios. A multi-source cross-attention\nmechanism is introduced to align and fuse features early, while a bidirectional\ncross-attention layer captures fine-grained intra-modal drug-target\ninteractions. To enhance model interpretability, we incorporate Gram Loss for\nfeature alignment and a deep orthogonal fusion module to eliminate redundancy.\nExperimental results on several benchmark datasets demonstrate that CDI-DTI\nsignificantly outperforms existing methods, particularly in cross-domain and\ncold-start tasks, while maintaining high interpretability for practical\napplications in drug-target interaction prediction."}
{"id": "2510.19006", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19006", "abs": "https://arxiv.org/abs/2510.19006", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security", "comment": null, "summary": "Generative AI and large language models (LLMs) have shown strong capabilities\nin code understanding, but their use in cybersecurity, particularly for malware\ndetection and analysis, remains limited. Existing detection systems often fail\nto generalize to obfuscated or previously unseen threats, underscoring the need\nfor more adaptable and explainable models. To address this challenge, we\nintroduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and\npretrained on a large-scale corpus of over one million malware samples,\nspanning both source and assembly code. XGen-Q uses a multi-stage prompt\nstrategy combined with retrieval-augmented generation (RAG) to deliver reliable\nmalware identification and detailed forensic reporting, even in the presence of\ncomplex code obfuscation. To further enhance generalization, we design a\ntraining pipeline that systematically exposes the model to diverse obfuscation\npatterns. Experimental results show that XGen-Q achieves significantly lower\nperplexity than competitive baselines and exhibits strong performance on novel\nmalware samples, demonstrating the promise of LLM-based approaches for\ninterpretable and robust malware analysis."}
{"id": "2510.19357", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19357", "abs": "https://arxiv.org/abs/2510.19357", "authors": ["Andrey Pudovikov", "Alexandra Khirianova", "Ekaterina Solodneva", "Aleksandr Katrutsa", "Egor Samosvat", "Yuriy Dorn"], "title": "Autobidding Arena: unified evaluation of the classical and RL-based autobidding algorithms", "comment": null, "summary": "Advertisement auctions play a crucial role in revenue generation for\ne-commerce companies. To make the bidding procedure scalable to thousands of\nauctions, the automatic bidding (autobidding) algorithms are actively developed\nin the industry. Therefore, the fair and reproducible evaluation of autobidding\nalgorithms is an important problem. We present a standardized and transparent\nevaluation protocol for comparing classical and reinforcement learning (RL)\nautobidding algorithms. We consider the most efficient autobidding algorithms\nfrom different classes, e.g., ones based on the controllers, RL, optimal\nformulas, etc., and benchmark them in the bidding environment. We utilize the\nmost recent open-source environment developed in the industry, which accurately\nemulates the bidding process. Our work demonstrates the most promising use\ncases for the considered autobidding algorithms, highlights their surprising\ndrawbacks, and evaluates them according to multiple metrics. We select the\nevaluation metrics that illustrate the performance of the autobidding\nalgorithms, the corresponding costs, and track the budget pacing. Such a choice\nof metrics makes our results applicable to the broad range of platforms where\nautobidding is effective. The presented comparison results help practitioners\nto evaluate the candidate autobidding algorithms from different perspectives\nand select ones that are efficient according to their companies' targets."}
{"id": "2510.19480", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19480", "abs": "https://arxiv.org/abs/2510.19480", "authors": ["Iasonas Nikolaou", "Miltiadis Stouras", "Stratis Ioannidis", "Evimaria Terzi"], "title": "Online Two-Stage Submodular Maximization", "comment": "To appear at NeurIPS 2025", "summary": "Given a collection of monotone submodular functions, the goal of Two-Stage\nSubmodular Maximization (2SSM) [Balkanski et al., 2016] is to restrict the\nground set so an objective selected u.a.r. from the collection attains a high\nmaximal value, on average, when optimized over the restricted ground set. We\nintroduce the Online Two-Stage Submodular Maximization (O2SSM) problem, in\nwhich the submodular objectives are revealed in an online fashion. We study\nthis problem for weighted threshold potential functions, a large and important\nsubclass of monotone submodular functions that includes influence maximization,\ndata summarization, and facility location, to name a few. We design an\nalgorithm that achieves sublinear $(1 - 1/e)^2$-regret under general matroid\nconstraints and $(1 - 1/e)(1-e^{-k}k^k/k!)$-regret in the case of uniform\nmatroids of rank $k$; the latter also yields a state-of-the-art bound for the\n(offline) 2SSM problem. We empirically validate the performance of our online\nalgorithm with experiments on real datasets."}
{"id": "2510.19221", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19221", "abs": "https://arxiv.org/abs/2510.19221", "authors": ["Yingchen Zhang", "Ruqing Zhang", "Jiafeng Guo", "Wenjun Peng", "Sen Li", "Fuyu Lv", "Xueqi Cheng"], "title": "C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search", "comment": null, "summary": "Designing document identifiers (docids) that carry rich semantic information\nwhile maintaining tractable search spaces is a important challenge in\ngenerative retrieval (GR). Popular codebook methods address this by building a\nhierarchical semantic tree and constraining generation to its child nodes, yet\ntheir numeric identifiers cannot leverage the large language model's pretrained\nnatural language understanding. Conversely, using text as docid provides more\nsemantic expressivity but inflates the decoding space, making the system\nbrittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)\nfirst construct semantic numerical docid via hierarchical clustering; (ii) then\nextract high-frequency metadata keywords and iteratively replace each numeric\nlabel with its cluster's top-K keywords; and (iii) an optional two-level\nsemantic smoothing step further enhances the fluency of C2T-ID. Experiments on\nNatural Questions and Taobao's product search demonstrate that C2T-ID\nsignificantly outperforms atomic, semantic codebook, and pure-text docid\nbaselines, demonstrating its effectiveness in balancing semantic expressiveness\nwith search space constraints."}
{"id": "2510.19591", "categories": ["cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.19591", "abs": "https://arxiv.org/abs/2510.19591", "authors": ["Marius Potfer", "Vianney Perchet"], "title": "Comparing Uniform Price and Discriminatory Multi-Unit Auctions through Regret Minimization", "comment": "Neurips 2025", "summary": "Repeated multi-unit auctions, where a seller allocates multiple identical\nitems over many rounds, are common mechanisms in electricity markets and\ntreasury auctions. We compare the two predominant formats: uniform-price and\ndiscriminatory auctions, focusing on the perspective of a single bidder\nlearning to bid against stochastic adversaries. We characterize the learning\ndifficulty in each format, showing that the regret scales similarly for both\nauction formats under both full-information and bandit feedback, as\n$\\tilde{\\Theta} ( \\sqrt{T} )$ and $\\tilde{\\Theta} ( T^{2/3} )$, respectively.\nHowever, analysis beyond worst-case regret reveals structural differences:\nuniform-price auctions may admit faster learning rates, with regret scaling as\n$\\tilde{\\Theta} ( \\sqrt{T} )$ in settings where discriminatory auctions remain\nat $\\tilde{\\Theta} ( T^{2/3} )$. Finally, we provide a specific analysis for\nauctions in which the other participants are symmetric and have unit-demand,\nand show that in these instances, a similar regret rate separation appears."}
{"id": "2510.19750", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19750", "abs": "https://arxiv.org/abs/2510.19750", "authors": ["Rajat De", "Dominik Kempa"], "title": "Optimal Random Access and Conditional Lower Bounds for 2D Compressed Strings", "comment": "Full version of a SODA 2026 paper", "summary": "Compressed indexing is a powerful technique that enables efficient querying\nover data stored in compressed form, significantly reducing memory usage and\noften accelerating computation. While extensive progress has been made for\none-dimensional strings, many real-world datasets (such as images, maps, and\nadjacency matrices) are inherently two-dimensional and highly compressible.\nUnfortunately, naively applying 1D techniques to 2D data leads to suboptimal\nresults, as fundamental structural repetition is lost during linearization.\nThis motivates the development of native 2D compressed indexing schemes that\npreserve both compression and query efficiency.\n  We present three main contributions that advance the theory of compressed\nindexing for 2D strings: (1) We design the first data structure that supports\noptimal-time random access to a 2D string compressed by a 2D grammar.\nSpecifically, for a 2D string $T\\in\\Sigma^{r\\times c}$ compressed by a 2D\ngrammar $G$ and any constant $\\epsilon>0$, we achieve $O(\\log n/\\log \\log n)$\nquery time and $O(|G|\\log^{2+\\epsilon}n)$ space, where $n=\\max(r,c)$. (2) We\nprove conditional lower bounds for pattern matching over 2D-grammar compressed\nstrings. Assuming the Orthogonal Vectors Conjecture, no algorithm can solve\nthis problem in time $O(|G|^{2-\\epsilon}\\cdot |P|^{O(1)})$ for any\n$\\epsilon>0$, demonstrating a separation from the 1D case, where optimal\nsolutions exist. (3) We show that several fundamental 2D queries, such as the\n2D longest common extension, rectangle sum, and equality, cannot be supported\nefficiently under hardness assumptions for rank and symbol occurrence queries\non 1D grammar-compressed strings. This is the first evidence connecting the\ncomplexity of 2D compressed indexing to long-standing open problems in the 1D\nsetting."}
{"id": "2510.19340", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19340", "abs": "https://arxiv.org/abs/2510.19340", "authors": ["L. Caspari", "M. Dinzinger", "K. Gosh Dastidar", "C. Fellicious", "J. MitroviÄ‡", "M. Granitzer"], "title": "CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale", "comment": null, "summary": "Dense retrieval systems have proven to be effective across various\nbenchmarks, but require substantial memory to store large search indices.\nRecent advances in embedding compression show that index sizes can be greatly\nreduced with minimal loss in ranking quality. However, existing studies often\noverlook the role of corpus complexity -- a critical factor, as recent work\nshows that both corpus size and document length strongly affect dense retrieval\nperformance. In this paper, we introduce CoRECT (Controlled Retrieval\nEvaluation of Compression Techniques), a framework for large-scale evaluation\nof embedding compression methods, supported by a newly curated dataset\ncollection. To demonstrate its utility, we benchmark eight representative types\nof compression methods. Notably, we show that non-learned compression achieves\nsubstantial index size reduction, even on up to 100M passages, with\nstatistically insignificant performance loss. However, selecting the optimal\ncompression method remains challenging, as performance varies across models.\nSuch variability highlights the necessity of CoRECT to enable consistent\ncomparison and informed selection of compression methods. All code, data, and\nresults are available on GitHub and HuggingFace."}
{"id": "2510.19620", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.19620", "abs": "https://arxiv.org/abs/2510.19620", "authors": ["Patrick Becker", "Fabian Frank"], "title": "On Minimal Achievable Quotas in Multiwinner Voting", "comment": null, "summary": "Justified representation (JR) and extended justified representation (EJR) are\nwell-established proportionality axioms in approval-based multiwinner voting.\nBoth axioms are always satisfiable, but they rely on a fixed quota (typically\nHare or Droop), with the Droop quota being the smallest one that guarantees\nexistence across all instances. With this observation in mind, we take a first\nstep beyond the fixed-quota paradigm and introduce proportionality notions\nwhere the quota is instance-dependent. We demonstrate that all commonly studied\nvoting rules can have an additive distance to the optimum of\n$\\frac{k^2}{(k+1)^2}$. Moreover, we look into the computational aspects of our\ninstance-dependent quota and prove that determining the optimal value of\n$\\alpha$ for a given approval profile satisfying $\\alpha$-JR is NP-complete. To\naddress this, we introduce an integer linear programming (ILP) formulation for\ncomputing committees that satisfy $\\alpha$-JR, and we provide positive results\nin the voter interval (VI) and candidate interval (CI) domains."}
{"id": "2510.19780", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19780", "abs": "https://arxiv.org/abs/2510.19780", "authors": ["Adam Karczmarz", "Wojciech Nadara", "Marek SokoÅ‚owski"], "title": "Strongly Polynomial Parallel Work-Depth Tradeoffs for Directed SSSP", "comment": "To appear in SODA 2026", "summary": "In this paper, we show new strongly polynomial work-depth tradeoffs for\ncomputing single-source shortest paths (SSSP) in non-negatively weighted\ndirected graphs in parallel. Most importantly, we prove that directed SSSP can\nbe solved within $\\tilde{O}(m+n^{2-\\epsilon})$ work and\n$\\tilde{O}(n^{1-\\epsilon})$ depth for some positive $\\epsilon>0$. In\nparticular, for dense graphs with non-negative real weights, we provide the\nfirst nearly work-efficient strongly polynomial algorithm with sublinear depth.\n  Our result immediately yields improved strongly polynomial parallel\nalgorithms for min-cost flow and the assignment problem. It also leads to the\nfirst non-trivial strongly polynomial dynamic algorithm for minimum mean cycle.\nMoreover, we develop efficient parallel algorithms in the Word RAM model for\nseveral variants of SSSP in graphs with exponentially large edge weights."}
{"id": "2510.19758", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19758", "abs": "https://arxiv.org/abs/2510.19758", "authors": ["Joseph Casale", "Andrew Silverschotz", "Joseph DeSimone"], "title": "Top-P Masking for Cross Language Information Retrieval", "comment": "Unsubmitted", "summary": "Top-K masking schemes have been proposed as a method to promote sparse\nrepresentations in Information Retrieval (IR) tasks, as a simple alternative to\nFloating Point Operations per Second (FLOPS) regularization. Algorithms such as\nBilingual Lexical and Document Expansion Model (BLADE), adopt this approach as\na post-processing stage. We propose using Top-P Dynamic Masking similar to\nNucleus Sampling in Large Language Models, and demonstrate better performance\nthan Top-K masking. Specifically, we evaluate our methods in the domain of\nCross Language Information Retrieval (CLIR)"}
{"id": "2510.19793", "categories": ["cs.DS", "cs.LO", "68Q27, 05C85", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.19793", "abs": "https://arxiv.org/abs/2510.19793", "authors": ["Benjamin Bergougnoux", "Vera Chekan", "Giannos Stamoulis"], "title": "A Logic-based Algorithmic Meta-Theorem for Treedepth: Single Exponential FPT Time and Polynomial Space", "comment": "Accepted at SODA 2026", "summary": "For a graph $G$, the parameter treedepth measures the minimum depth among all\nforests $F$, called elimination forests, such that $G$ is a subgraph of the\nancestor-descendant closure of $F$. We introduce a logic, called neighborhood\noperator logic with acyclicity, connectivity and clique constraints\n($\\mathsf{NEO}_2[\\mathsf{FRec}]+\\mathsf{ACK}$ for short), that captures all\nNP-hard problems$\\unicode{x2013}$like Independent Set or Hamiltonian\nCycle$\\unicode{x2013}$that are known to be tractable in time\n$2^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$ and space $n^{\\mathcal{O}(1)}$ on\n$n$-vertex graphs provided with elimination forests of depth $k$. We provide a\nmodel checking algorithm for $\\mathsf{NEO}_2[\\mathsf{FRec}]+\\mathsf{ACK}$ with\nsuch complexity that unifies and extends these results. For\n$\\mathsf{NEO}_2[\\mathsf{FRec}]+\\mathsf{k}$, the fragment of the above logic\nthat does not use acyclicity and connectivity constraints, we get a\nstrengthening of this result, where the space complexity is reduced to\n$\\mathcal{O}(k\\log(n))$.\n  With a similar mechanism as the distance neighborhood logic introduced in\n[Bergougnoux, Dreier and Jaffke, SODA 2023], the logic\n$\\mathsf{NEO}_2[\\mathsf{FRec}]+\\mathsf{ACK}$ is an extension of the\nfully-existential $\\mathsf{MSO}_2$ with predicates for (1) querying\ngeneralizations of the neighborhoods of vertex sets, (2) verifying the\nconnectivity and acyclicity of vertex and edge sets, and (3) verifying that a\nvertex set induces a clique. Our results provide\n$2^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$ time and $n^{\\mathcal{O}(1)}$ space\nalgorithms for problems for which the existence of such algorithms was\npreviously unknown. In particular, $\\mathsf{NEO}_2[\\mathsf{FRec}]$ captures\nCNF-SAT via the incidence graphs associated to CNF formulas, and it also\ncaptures several modulo counting problems like Odd Dominating Set."}
{"id": "2510.19815", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19815", "abs": "https://arxiv.org/abs/2510.19815", "authors": ["Dominik Kempa", "Tomasz Kociumaka"], "title": "Explaining the Inherent Tradeoffs for Suffix Array Functionality: Equivalences between String Problems and Prefix Range Queries", "comment": "Full version of a SODA 2026 paper", "summary": "We study the fundamental question of how efficiently suffix array entries can\nbe accessed when the array cannot be stored explicitly. The suffix array\n$SA_T[1..n]$ of a text $T$ of length $n$ encodes the lexicographic order of its\nsuffixes and underlies numerous applications in pattern matching, data\ncompression, and bioinformatics. Previous work established one-way reductions\nshowing how suffix array queries can be answered using, for example, rank\nqueries on the Burrows-Wheeler Transform. More recently, a new class of prefix\nqueries was introduced, together with reductions that, among others, transform\na simple tradeoff for prefix-select queries into a suffix array tradeoff\nmatching state-of-the-art space and query-time bounds, while achieving\nsublinear construction time. For binary texts, the resulting data structure\nachieves space $O(n)$ bits, preprocessing time $O(n / \\sqrt{\\log n})$,\npreprocessing space of $O(n)$ bits, and query time $O(\\log^{\\epsilon} n)$ for\nany constant $\\epsilon > 0$. However, whether these bounds could be improved\nusing different techniques has remained open.\n  We resolve this question by presenting the first bidirectional reduction\nshowing that suffix array queries are, up to an additive $O(\\log\\log n)$ term\nin query time, equivalent to prefix-select queries in all parameters. This\nresult unifies prior approaches and shows that essentially all efficient suffix\narray representations can be expressed via prefix-select structures. Moreover,\nwe prove analogous equivalences for inverse suffix array queries, pattern\nranking, lexicographic range, and SA-interval queries, identifying six core\nproblem pairs that connect string and prefix query models. Our framework thus\nprovides a unified foundation for analyzing and improving the efficiency of\nfundamental string-processing problems through the lens of prefix queries."}
{"id": "2510.19820", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19820", "abs": "https://arxiv.org/abs/2510.19820", "authors": ["Dominik Kempa", "Tomasz Kociumaka"], "title": "Tight Lower Bounds for Central String Queries in Compressed Space", "comment": "Full version of a SODA 2026 paper", "summary": "In this work, we study the limits of compressed data structures, i.e.,\nstructures that support various queries on an input text $T\\in\\Sigma^n$ using\nspace proportional to the size of $T$ in compressed form. Nearly all\nfundamental queries can currently be efficiently supported in\n$O(\\delta(T)\\log^{O(1)}n)$ space, where $\\delta(T)$ is the substring\ncomplexity, a strong compressibility measure that lower-bounds the optimal\nspace to represent the text [Kociumaka, Navarro, Prezza, IEEE Trans. Inf.\nTheory 2023]. However, optimal query time has been characterized only for\nrandom access.\n  We address this gap by developing tight lower bounds for nearly all other\nfundamental queries: (1) We prove that suffix array (SA), inverse suffix array\n(SA$^{-1}$), longest common prefix (LCP) array, and longest common extension\n(LCE) queries all require $\\Omega(\\log n/\\log\\log n)$ time within\n$O(\\delta(T)\\log^{O(1)}n)$ space, matching known upper bounds. (2) We further\nshow that other common queries, currently supported in $O(\\log\\log n)$ time and\n$O(\\delta(T)\\log^{O(1)}n)$ space, including the Burrows-Wheeler Transform\n(BWT), permuted longest common prefix (PLCP) array, Last-to-First (LF), inverse\nLF, lexicographic predecessor ($\\Phi$), and inverse $\\Phi$ queries, all require\n$\\Omega(\\log\\log n)$ time, yielding another set of tight bounds.\n  Our lower bounds hold even for texts over a binary alphabet. This work\nestablishes a clean dichotomy: the optimal time complexity to support central\nstring queries in compressed space is either $\\Theta(\\log n/\\log\\log n)$ or\n$\\Theta(\\log\\log n)$. This completes the theoretical foundation of compressed\nindexing, closing a crucial gap between upper and lower bounds and providing a\nclear target for future data structures: seeking either the optimal time in the\nsmallest space or the fastest time in the optimal space, both of which are now\nknown for central string queries."}
{"id": "2510.19197", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19197", "abs": "https://arxiv.org/abs/2510.19197", "authors": ["Nofar Carmeli", "Nikolaos Tziavelis"], "title": "Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum", "comment": null, "summary": "We investigate the fine-grained complexity of direct access to Conjunctive\nQuery (CQ) answers according to their position, ordered by the minimum (or\nmaximum) value between attributes. We further use the tools we develop to\nexplore a wealth of related tasks. We consider the task of ranked enumeration\nunder min/max orders, as well as tasks concerning CQs with predicates of the\nform x <= min X , where X is a set of variables and x is a single variable:\ncounting, enumeration, direct access, and predicate elimination (i.e.,\ntransforming the pair of query and database to an equivalent pair without\nmin-predicates). For each task, we establish a complete dichotomy for\nself-join-free CQs, precisely identifying the cases that are solvable in\nnear-ideal time, i.e., (quasi)linear preprocessing time followed by constant or\nlogarithmic time per output."}
