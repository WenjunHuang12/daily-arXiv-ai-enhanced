<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 5]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Carbon Pricing in Traffic Networks](https://arxiv.org/abs/2508.09280)
*Svenja M. Griesbach,Tobias Harks,Max Klimm,Michael Markl,Philipp Warode*

Main category: cs.GT

TL;DR: 研究碳定价如何引导交通流实现给定排放预算的均衡，证明了所有可行排放目标均可通过定价实现。


<details>
  <summary>Details</summary>
Motivation: 交通是全球碳排放的重要来源，研究碳定价如何引导交通流以实现排放预算。

Method: 采用多商品流模型，分析通过定价实现Wardrop均衡的方法，包括凸优化和多项式时间算法。

Result: 证明所有可行排放目标均可通过定价实现，并提供了计算方法和算法。

Conclusion: 碳定价是实现交通网络排放目标的有效且可行的方法。

Abstract: Traffic is a significant source of global carbon emissions. In this paper, we
study how carbon pricing can be used to guide traffic towards equilibria that
respect given emission budgets. In particular, we consider a general
multi-commodity flow model with flow-dependent externalities. These
externalities may represent carbon emissions, entering a priced area, or the
traversal of paths regulated by tradable credit schemes.
  We provide a complete characterization of all flows that can be attained as
Wardrop equilibria when assigning a single price to each externality. More
precisely, we show that every externality budget achievable by any feasible
flow in the network can also be achieved as a Wardrop equilibrium by setting
appropriate prices. For extremal and Pareto-minimal budgets, we show that there
are prices such that all equilibria respect the budgets. Although the proofs of
existence of these particular prices rely on fixed-point arguments and are
non-constructive, we show that in the case where the equilibrium minimizes a
convex potential, the prices can be obtained as Lagrange multipliers of a
suitable convex program. In the case of a single externality, we prove that the
total externality caused by the traffic flow is decreasing in the price. For
increasing, continuous, and piecewise affine travel time functions with a
single externality, we give an output-polynomial algorithm that computes all
equilibria implementable by pricing the externality. Even though there are
networks where the output size is exponential in the input size, we show that
the minimal price obeying a given budget can be computed in polynomial time.
This allows the efficient computation of the market price of tradable credit
schemes. Overall, our results show that carbon pricing is a viable and (under
mild assumptions) tractable approach to achieve all feasible emission goals in
traffic networks.

</details>


### [2] [Collective dynamics of strategic classification](https://arxiv.org/abs/2508.09340)
*Marta C. Couto,Flavia Barsotti,Fernando P. Santos*

Main category: cs.GT

TL;DR: 该论文研究了AI分类算法在高风险决策中用户策略性适应与算法重新训练之间的反馈循环，利用进化博弈论分析集体动态，并提出干预措施以减少负面影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在高风险决策中，用户策略性适应与算法重新训练之间的反馈循环对集体动态的影响，并提出解决方案。

Method: 应用进化博弈论框架，分析用户与机构之间的互动范式，测试改进检测能力和提供算法补救的效果。

Result: 提高检测能力可降低社会成本并促进用户改进；在不完美分类器情况下，算法补救可引导高用户改进率。机构重新适应的速度影响最终结果。

Conclusion: 研究揭示了用户策略性适应与算法重新训练之间的复杂动态，为减少负面影响提供了理论支持。

Abstract: Classification algorithms based on Artificial Intelligence (AI) are nowadays
applied in high-stakes decisions in finance, healthcare, criminal justice, or
education. Individuals can strategically adapt to the information gathered
about classifiers, which in turn may require algorithms to be re-trained. Which
collective dynamics will result from users' adaptation and algorithms'
retraining? We apply evolutionary game theory to address this question. Our
framework provides a mathematically rigorous way of treating the problem of
feedback loops between collectives of users and institutions, allowing to test
interventions to mitigate the adverse effects of strategic adaptation. As a
case study, we consider institutions deploying algorithms for credit lending.
We consider several scenarios, each representing different interaction
paradigms. When algorithms are not robust against strategic manipulation, we
are able to capture previous challenges discussed in the strategic
classification literature, whereby users either pay excessive costs to meet the
institutions' expectations (leading to high social costs) or game the algorithm
(e.g., provide fake information). From this baseline setting, we test the role
of improving gaming detection and providing algorithmic recourse. We show that
increased detection capabilities reduce social costs and could lead to users'
improvement; when perfect classifiers are not feasible (likely to occur in
practice), algorithmic recourse can steer the dynamics towards high users'
improvement rates. The speed at which the institutions re-adapt to the user's
population plays a role in the final outcome. Finally, we explore a scenario
where strict institutions provide actionable recourse to their unsuccessful
users and observe cycling dynamics so far unnoticed in the literature.

</details>


### [3] [Multidimensional Budget-Feasible Mechanism Design](https://arxiv.org/abs/2508.09367)
*Rian Neogi,Kanstantsin Pashkovich,Chaitanya Swamy*

Main category: cs.GT

TL;DR: 该论文研究了多维预算可行机制设计，提出了新的基准和机制，解决了单维设置中的局限性，并提供了常数近似保证。


<details>
  <summary>Details</summary>
Motivation: 当前预算可行机制设计仅限于单维设置，无法处理多维场景。论文旨在填补这一空白，解决多维环境下的挑战。

Method: 引入多维预算可行机制设计，提出新基准$OPT_{Bench}$，并设计适用于XOS估值的预算可行机制。

Result: 证明了单维基准的不足，提出了新基准，并设计了常数近似保证的机制。

Conclusion: 多维预算可行机制设计通过新基准和机制实现了有效的近似保证，扩展了现有研究的适用范围。

Abstract: In budget-feasible mechanism design, a buyer wishes to procure a set of items
of maximum value from self-interested players. We have a valuation function
$v:2^U \to \mathbb{R}_+$, where $U$ is the set of all items, where $v(S)$
specifies the value obtained from set $S$ of items. The entirety of current
work on budget-feasible mechanisms has focused on the single-dimensional
setting, wherein each player holds a single item $e$ and incurs a private cost
$c_e$ for supplying item $e$.
  We introduce multidimensional budget feasible mechanism design: the universe
$U$ is now partitioned into item-sets $\{G_i\}$ held by the different players,
and each player $i$ incurs a private cost $c_i(S_i)$ for supplying the set
$S_i\subseteq G_i$ of items. A budget-feasible mechanism is a mechanism that is
truthful, and where the total payment made to the players is at most some given
budget $B$. The goal is to devise a budget-feasible mechanism that procures a
set of items of large value. We obtain the first approximation guarantees for
multidimensional budget feasible mechanism design.
  Our contributions are threefold. First, we prove an impossibility result
showing that the standard benchmark used in single-dimensional budget-feasible
mechanism design, namely the algorithmic optimum is inadequate in that no
budget-feasible mechanism can achieve good approximation relative to this. We
identify that the chief underlying issue here is that there could be a
monopolist which prevents a budget-feasible mechanism from obtaining good
guarantees. Second, we devise an alternate benchmark, $OPT_{Bench}$, that
allows for meaningful approximation guarantees, thereby yielding a metric for
comparing mechanisms. Third, we devise budget-feasible mechanisms that achieve
constant-factor approximation guarantees with respect to this benchmark for XOS
valuations.

</details>


### [4] [Project Submission Games in Participatory Budgeting](https://arxiv.org/abs/2508.09741)
*Piotr Faliszewski,Łukasz Janeczko,Andrzej Kaczmarczyk,Grzegorz Lisowski,Grzegorz Pierczyński*

Main category: cs.GT

TL;DR: 论文提出了项目提交博弈框架，研究参与式预算中提案者的行为，探讨纯纳什均衡的存在条件及其复杂性，并寻求计算最佳响应的算法。


<details>
  <summary>Details</summary>
Motivation: 研究参与式预算和多赢选举中提案者的策略行为，旨在理解其博弈动态。

Method: 提出项目提交博弈框架，分析纯纳什均衡的存在条件及计算复杂性，设计计算最佳响应的算法。

Result: 确定了纯纳什均衡存在的条件，并探讨了验证其存在的复杂性。

Conclusion: 该框架为理解提案者行为提供了理论基础，并提出了未来研究方向。

Abstract: We introduce the framework of project submission games, capturing the
behavior of project proposers in participatory budgeting (and multiwinner
elections). Here, each proposer submits a subset of project proposals, aiming
at maximizing the total cost of those that get funded. We focus on finding
conditions under which pure Nash equilibria (NE) exist in our games, and on the
complexity of checking whether they exist. We also seek algorithms for
computing best responses for the proposers

</details>


### [5] [The Price of EF1 for Few Agents with Additive Ternary Valuations](https://arxiv.org/abs/2508.09869)
*Maria Kyropoulou,Alexandros A. Voudouris*

Main category: cs.GT

TL;DR: 研究了具有加性三元估值的资源分配问题，证明了EF1分配的价格下界为Ω(√n)，并针对少量代理人的情况给出了具体价格范围。


<details>
  <summary>Details</summary>
Motivation: 探讨在资源分配问题中，代理人具有加性三元估值时，EF1分配的价格界限，以填补现有研究的空白。

Method: 通过理论分析，对不同代理人数量（n）的情况分别推导EF1分配的价格下界和具体范围。

Result: 对于大量代理人（n），EF1价格下界为Ω(√n)；对于n=2和n=3，价格分别为12/11和1.2-1.256。

Conclusion: EF1分配的价格在代理人数量较多时表现较差，与一般子加性估值情况类似，但在少量代理人时价格较低。

Abstract: We consider a resource allocation problem with agents that have additive
ternary valuations for a set of indivisible items, and bound the price of
envy-free up to one item (EF1) allocations. For a large number $n$ of agents,
we show a lower bound of $\Omega(\sqrt{n})$, implying that the price of EF1 is
no better than when the agents have general subadditive valuations. We then
focus on instances with few agents and show that the price of EF1 is $12/11$
for $n=2$, and between $1.2$ and $1.256$ for $n=3$.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [6] [PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research](https://arxiv.org/abs/2508.09232)
*Nick Oh,Giorgos D. Vrakas,Siân J. M. Brooke,Sasha Morinière,Toju Duke*

Main category: cs.MM

TL;DR: 论文提出PETLP框架，整合GDPR、版权法和平台条款的合规要求，通过嵌入法律保障的ETL流程帮助研究人员应对监管复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能整合不同监管领域（GDPR、版权法、平台条款），导致研究人员缺乏统一指导。

Method: 引入PETLP框架，将数据保护影响评估作为动态文档嵌入ETL流程，并通过Reddit分析展示不同实体的权利差异。

Result: 研究发现真实匿名化在社交媒体数据中不可行，并揭示了数据集创建与模型分发之间的法律空白。

Conclusion: PETLP通过结构化合规决策和简化数据管理计划，帮助研究人员自信应对监管挑战，弥合法律要求与研究实践之间的差距。

Abstract: Social media data presents AI researchers with overlapping obligations under
the GDPR, copyright law, and platform terms -- yet existing frameworks fail to
integrate these regulatory domains, leaving researchers without unified
guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and
Present), a compliance framework that embeds legal safeguards directly into
extended ETL pipelines. Central to PETLP is treating Data Protection Impact
Assessments as living documents that evolve from pre-registration through
dissemination. Through systematic Reddit analysis, we demonstrate how
extraction rights fundamentally differ between qualifying research
organisations (who can invoke DSM Article 3 to override platform restrictions)
and commercial entities (bound by terms of service), whilst GDPR obligations
apply universally. We reveal why true anonymisation remains unachievable for
social media data and expose the legal gap between permitted dataset creation
and uncertain model distribution. By structuring compliance decisions into
practical workflows and simplifying institutional data management plans, PETLP
enables researchers to navigate regulatory complexity with confidence, bridging
the gap between legal requirements and research practice.

</details>


### [7] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! 是一个实验性系统，利用语义编目和大型语言模型（LLMs）重新组织和检索电视档案内容。


<details>
  <summary>Details</summary>
Motivation: 探索语义技术和LLMs在电视档案检索与重构中的潜力，推动媒体史和AI驱动档案研究的发展。

Method: 结合自动语音识别（ASR）、语义嵌入和检索增强生成（RAG），对意大利电视视频进行转录、分段和向量化存储，支持语义查询和自动化叙事构建。

Result: 系统成功生成了具有讽刺性并置和主题连贯性的叙事蒙太奇，展示了语义技术在档案研究中的新应用。

Conclusion: AI Blob! 为档案研究提供了新方法，支持自动化叙事构建和文化分析，并公开数据集促进跨学科实验。

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


### [8] [In-place Double Stimulus Methodology for Subjective Assessment of High Quality Images](https://arxiv.org/abs/2508.09777)
*Shima Mohammadi,Mohsen Jenadeleh,Michela Testolina,Jon Sneyers,Touradj Ebrahimi,Dietmar Saupe,João Ascenso*

Main category: cs.MM

TL;DR: 提出了一种新的双刺激主观评估方法（IDSQS），用于高质量图像的评估，解决了现有协议在检测细微感知差异上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有协议难以检测高质量图像中的细微感知差异，需要更直观的方法。

Method: 采用IDSQS方法，让受试者在同一空间位置交替查看参考图像和失真图像，并通过大规模众包研究生成数据集。

Result: IDSQS方法在高质量至视觉无损质量级别上表现优异，且通过Beta分布建模质量分数，评估了变异性与受试者一致性。

Conclusion: IDSQS方法有效且与更精确的主观评估基准高度相关，相关数据和工具已公开。

Abstract: This paper introduces a novel double stimulus subjective assessment
methodology for the evaluation of high quality images to address the
limitations of existing protocols in detecting subtle perceptual differences.
The In-place Double Stimulus Quality Scale (IDSQS) allows subjects to
alternately view a reference and a distorted image at the same spatial
location, facilitating a more intuitive detection of differences in quality,
especially at high to visually lossless quality levels. A large-scale
crowdsourcing study employing this methodology was conducted, generating a
comprehensive public dataset to evaluate perceived image quality across several
compression algorithms and distortion levels. An additional contribution is the
modeling of quality scores using a Beta distribution, allowing for the
assessment of variability and subject consistency. Our findings demonstrate the
effectiveness of the IDSQS methodology in achieving high correlation with more
precise subjective evaluation benchmarks. The dataset, subjective data, and
graphical user interface developed for this study are publicly available at
https://github.com/shimamohammadi/IDSQS

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [9] [Deviation Inequalities for Rényi Divergence Estimators via Variational Expression](https://arxiv.org/abs/2508.09382)
*Sreejith Sreekumar,Kengo Kato*

Main category: cs.IT

TL;DR: 本文针对Rényi散度的估计误差提出了指数偏差不等式，适用于平滑插件估计器和神经网络估计器，无需传统假设。


<details>
  <summary>Details</summary>
Motivation: 现有文献对Rényi散度估计误差的概率界限研究不足，本文旨在填补这一空白。

Method: 通过将误差与适当的经验过程关联，并利用经验过程理论工具，建立指数偏差不等式。

Result: 提出了适用于非紧支撑或无密度下界假设的偏差不等式，并应用于隐私审计假设检验。

Conclusion: 本文方法为Rényi散度估计提供了更通用的概率界限，并展示了在信息理论和隐私审计中的潜在应用。

Abstract: R\'enyi divergences play a pivotal role in information theory, statistics,
and machine learning. While several estimators of these divergences have been
proposed in the literature with their consistency properties established and
minimax convergence rates quantified, existing accounts of probabilistic bounds
governing the estimation error are premature. Here, we make progress in this
regard by establishing exponential deviation inequalities for smoothed plug-in
estimators and neural estimators by relating the error to an appropriate
empirical process and leveraging tools from empirical process theory. In
particular, our approach does not require the underlying distributions to be
compactly supported or have densities bounded away from zero, an assumption
prevalent in existing results. The deviation inequality also leads to a
one-sided concentration bound from the expectation, which is useful in
random-coding arguments over continuous alphabets in information theory with
potential applications to physical-layer security. As another concrete
application, we consider a hypothesis testing framework for auditing R\'{e}nyi
differential privacy using the neural estimator as a test statistic and obtain
non-asymptotic performance guarantees for such a test.

</details>


### [10] [Hermitian Self-dual Twisted Generalized Reed-Solomon Codes](https://arxiv.org/abs/2508.09687)
*Chun'e Zhao,Yuxin Han,Wenping Ma,Tongjiang Yan,Yuhua Sun*

Main category: cs.IT

TL;DR: 本文研究了广义扭曲Reed-Solomon（A-TGRS）码的Hermitian自对偶性，提出了四个构造自对偶TGRS码的方法，并获得了新的灵活参数类。同时，给出了A-TGRS码为Hermitian自对偶且MDS的充要条件，并通过选择评估点构造了一类MDS Hermitian自对偶TGRS码。


<details>
  <summary>Details</summary>
Motivation: 自对偶MDS码在组合和密码学中有重要应用，而TGRS码可以同时满足MDS和自对偶性。本文旨在全面研究A-TGRS码的Hermitian自对偶性，并扩展相关构造方法。

Method: 通过矩阵表示的视角分析TGRS码的Hermitian自对偶性，提出了四个构造方法，并给出了充要条件。同时，通过选择评估点构造了一类MDS Hermitian自对偶TGRS码。

Result: 提出了四个构造自对偶TGRS码的方法，获得了新的灵活参数类，并给出了A-TGRS码为Hermitian自对偶且MDS的充要条件。

Conclusion: 本文为TGRS码的Hermitian自对偶性提供了更简洁透明的分析框架，并扩展了相关构造方法，同时为Euclidean自对偶TGRS码和Hermitian自对偶GRS码的研究提供了新视角。

Abstract: Self-dual maximum distance separable (MDS) codes over finite fields are
linear codes with significant combinatorial and cryptographic applications.
Twisted generalized Reed-Solomon (TGRS) codes can be both MDS and self-dual. In
this paper, we study a general class of TGRS codes (A-TGRS), which encompasses
all previously known special cases. First, we establish a sufficient and
necessary condition for an A-TGRS code to be Hermitian self-dual. Furthermore,
we present four constructions of self-dual TGRS codes, which, to the best of
our knowledge, nearly cover all the related results previously reported in the
literature. More importantly, we also obtain several new classes of Hermitian
self-dual TGRS codes with flexible parameters. Based on this framework, we
derive a sufficient and necessary condition for an A-TGRS code to be Hermitian
self-dual and MDS. In addition, we construct a class of MDS Hermitian self-dual
TGRS code by appropriately selecting the evaluation points. This work
investigates the Hermitian self-duality of TGRS codes from the perspective of
matrix representation, leading to more concise and transparent analysis. More
generally, the Euclidean self-dual TGRS codes and the Hermitian self-dual GRS
codes can also be understood easily from this point.

</details>


### [11] [Fluid Reconfigurable Intelligent Surface with Element-Level Pattern Reconfigurability: Beamforming and Pattern Co-Design](https://arxiv.org/abs/2508.09695)
*Han Xiao,Xiaoyan Hu,Kai-Kit Wong,Xusheng Zhu,Hanjiang Hong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 提出了一种新型流体可重构智能表面（FRIS）框架，通过动态调整辐射模式优化通信性能，显著优于传统RIS架构。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过动态调整流体元素的辐射模式，优化点对点和多用户通信系统的信号传输性能。

Method: 提出模式可重构FRIS框架，采用球谐正交分解（SHOD）建模辐射模式，并通过MMSE和RCG算法联合优化波束成形和球谐系数。

Result: 仿真结果显示，模式可重构FRIS在性能上分别比3GPP 38.901和各向同性辐射模型高出161.5%和176.2%。

Conclusion: 模式可重构FRIS在通信系统中具有显著优势，为未来智能表面设计提供了新思路。

Abstract: This paper proposes a novel pattern-reconfigurable fluid reconfigurable
intelligent surface (FRIS) framework, where each fluid element can dynamically
adjust its radiation pattern based on instantaneous channel conditions. To
evaluate its potential, we first conduct a comparative analysis of the received
signal power in point-to-point communication systems assisted by three types of
surfaces: (1) the proposed pattern-reconfigurable FRIS, (2) a
position-reconfigurable FRIS, and (3) a conventional RIS. Theoretical results
demonstrate that the pattern-reconfigurable FRIS provides a significant
advantage in modulating transmission signals compared to the other two
configurations. To further study its capabilities, we extend the framework to a
multiuser communication scenario. In this context, the spherical harmonics
orthogonal decomposition (SHOD) method is employed to accurately model the
radiation patterns of individual fluid elements, making the pattern design
process more tractable. An optimization problem is then formulated with the
objective of maximizing the weighted sum rate among users by jointly designing
the active beamforming vectors and the spherical harmonics coefficients,
subject to both transmit power and pattern energy constraints. To tackle the
resulting non-convex optimization problem, we propose an iterative algorithm
that alternates between a minimum mean-square error (MMSE) approach for active
beamforming and a Riemannian conjugate gradient (RCG) method for updating the
spherical harmonics coefficients. Simulation results show that the proposed
pattern-reconfigurable FRIS significantly outperforms traditional RIS
architectures based on the 3GPP 38.901 and isotropic radiation models,
achieving average performance gains of 161.5% and 176.2%, respectively.

</details>


### [12] [ORCAS Codes: A Flexible Generalization of Polar Codes with Low-Complexity Decoding](https://arxiv.org/abs/2508.09744)
*Andreas Zunker,Marvin Rübenacke,Stephan ten Brink*

Main category: cs.IT

TL;DR: 提出了一种基于递归Plotkin级联的低复杂度软判决解码算法ORCAS码，性能优于或等于极化码，且具有更高的灵活性。


<details>
  <summary>Details</summary>
Motivation: 需要低复杂度软判决解码的信道编码。

Method: 递归级联基于单纯形码及其对偶码的低码率和高码率最优码，采用低复杂度ML解码和SC解码。

Result: ORCAS码性能至少与极化码相当，实际参数下块错误率优于极化码0.5 dB，解码复杂度相近。

Conclusion: ORCAS码在性能和灵活性上优于极化码，适用于实际应用。

Abstract: Motivated by the need for channel codes with low-complexity soft-decision
decoding algorithms, we consider the recursive Plotkin concatenation of optimal
low-rate and high-rate codes based on simplex codes and their duals. These
component codes come with low-complexity maximum likelihood (ML) decoding
which, in turn, enables efficient successive cancellation (SC)-based decoding.
As a result, the proposed optimally recursively concatenated simplex (ORCAS)
codes achieve a performance that is at least as good as that of polar codes.
For practical parameters, the proposed construction significantly outperforms
polar codes in terms of block error rate by up to 0.5 dB while maintaining
similar decoding complexity. Furthermore, the codes offer greater flexibility
in codeword length than conventional polar codes.

</details>


### [13] [Non-Orthogonal Affine Frequency Division Multiplexing for Spectrally Efficient High-Mobility Communications](https://arxiv.org/abs/2508.09782)
*Qin Yi,Zilong Liu,Leila Musavian,Zeping Sui*

Main category: cs.IT

TL;DR: 本文提出了一种新型非正交仿射频分复用（nAFDM）波形，用于高移动性通信，具有更高的频谱效率。通过引入带宽压缩因子实现可控子载波重叠，并提出了基于IDFT的信号生成方法。仿真结果显示nAFDM在BER和频谱效率上优于现有波形。


<details>
  <summary>Details</summary>
Motivation: 解决高移动性通信中频谱效率低的问题，通过非正交调制提升性能。

Method: 引入带宽压缩因子实现子载波重叠，提出基于IDFT的信号生成方法，并开发软迭代检测算法降低干扰。

Result: nAFDM在BER上与常规AFDM相近，频谱效率更高，且能实现BER与复杂度的平衡。

Conclusion: nAFDM是一种高效的高移动性通信波形，具有实际应用潜力。

Abstract: This paper proposes a novel non-orthogonal affine frequency division
multiplexing {(nAFDM)} waveform for reliable high-mobility communications with
enhanced spectral efficiency {(SE)}. The key idea is {to introduce} a bandwidth
compression factor into the AFDM {modulator} to enable controllable subcarrier
overlapping. We first {detail the proposed nAFDM transceiver} and derive the
corresponding input-output {signal} relationship. Then, an efficient {nAFDM}
signal generation method based on the inverse discrete Fourier transform (IDFT)
is proposed, enabling practical implementation using existing inverse fast
Fourier transform (IFFT) modules without additional hardware complexity. Next,
to characterize the impact of non-orthogonal modulation, we derive a
closed-form expression {of} inter-carrier interference (ICI), showing its
dependence on the bandwidth compression factor. To mitigate the resulting
interference, we propose a soft iterative detection algorithm and a
low-complexity implementation approach that leverages the distribution
characteristics of ICI. {Simulation results demonstrate that 1) in terms of bit
error rate (BER), the proposed nAFDM can achieve near identical BER compared to
conventional AFDM, while outperforms other waveform counterparts; 2) nAFDM is
capable of striking higher SE compared to other existing waveforms; and 3) the
proposed nAFDM achieves an attractive BER vs. SE trade-off, and the proposed
soft ID scheme can attain a trade-off between BER and complexity.}

</details>


### [14] [Unified Design of Space-Air-Ground-Sea Integrated Maritime Communications](https://arxiv.org/abs/2508.09817)
*Zhehan Zhou,Xiaoming Chen,Ming Ying,Zhaohui Yang,Chongwen Huang,Yunlong Cai,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种结合卫星、无人机、地面基站和无人船的海洋通信架构，并通过联合波束成形和轨迹优化算法提升用户传输速率。


<details>
  <summary>Details</summary>
Motivation: 随着海洋活动的快速增长，需要为广阔海域提供无缝且具有服务质量保障的通信。

Method: 将海洋区域按距离划分为不同区域，分别由地面基站、无人船、无人机和卫星提供服务，并设计联合波束成形和轨迹优化算法。

Result: 理论分析和仿真结果验证了所提算法的有效性。

Conclusion: 该架构和算法能够有效提升海洋通信的性能。

Abstract: With the explosive growth of maritime activities, it is expected to provide
seamless communications with quality of service (QoS) guarantee over broad sea
area. In the context, this paper proposes a space-air-ground-sea integrated
maritime communication architecture combining satellite, unmanned aerial
vehicle (UAV), terrestrial base station (TBS) and unmanned surface vessel
(USV). Firstly, according to the distance away from the shore, the whole marine
space is divided to coastal area, offshore area, middle-sea area and open-sea
area, the maritime users in which are served by TBS, USV, UAV and satellite,
respectively. Then, by exploiting the potential of integrated maritime
communication system, a joint beamforming and trajectory optimization algorithm
is designed to maximize the minimum transmission rate of maritime users.
Finally, theoretical analysis and simulation results validate the effectiveness
of the proposed algorithm.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [15] [ELASTIC: Event-Tracking Data Synchronization in Soccer Without Annotated Event Locations](https://arxiv.org/abs/2508.09238)
*Hyunsung Kim,Hoyoung Choi,Sangwoo Seo,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.DB

TL;DR: ELASTIC是一种仅使用跟踪数据特征的事件同步框架，解决了足球分析中事件与跟踪数据同步的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 足球分析中事件与跟踪数据的同步因时间与空间误差而困难，现有方法依赖易出错的事件位置标注。

Method: ELASTIC通过跟踪数据特征同步事件，明确检测传球类事件的结束时间，并区分主次事件。

Result: 实验显示ELASTIC在2134个事件的同步准确性上大幅超越现有方法。

Conclusion: ELASTIC提供了一种更准确、完整的事件同步解决方案。

Abstract: The integration of event and tracking data has become essential for advanced
analysis in soccer. However, synchronizing these two modalities remains a
significant challenge due to temporal and spatial inaccuracies in manually
recorded event timestamps. Existing synchronizers typically rely on annotated
event locations, which themselves are prone to spatial errors and thus can
distort synchronization results. To address this issue, we propose ELASTIC
(Event-Location-AgnoSTIC synchronizer), a synchronization framework that only
uses features derived from tracking data. ELASTIC also explicitly detects the
end times of pass-like events and separates the detection of major and minor
events, which improves the completeness of the synchronized output and reduces
error cascade across events. We annotated the ground truth timestamps of 2,134
events from three Eredivisie matches to measure the synchronization accuracy,
and the experimental results demonstrate that ELASTIC outperforms existing
synchronizers by a large margin.

</details>


### [16] [LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation](https://arxiv.org/abs/2508.09594)
*Fei Teng,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: LLMLog提出了一种基于多轮标注和自适应上下文学习的框架，用于提高日志模板生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有启发式和神经网络方法在日志模板生成中因依赖手工规则或特定训练模式而准确性不足，LLM虽潜力大但处理复杂日志时易出错。

Method: 提出基于编辑距离的相似性度量，选择最具代表性的未标注日志进行标注，并设计自适应上下文选择策略。

Result: 在16个数据集上的实验表明，LLMLog优于现有最佳方法。

Conclusion: LLMLog通过多轮标注和自适应学习显著提升了日志模板生成的准确性。

Abstract: Modern computing systems, such as HDFS and Spark, produce vast quantities of
logs that developers use for tasks like anomaly detection and error analysis.
To simplify log analysis, template generation methods have been proposed to
standardize log formats, transforming unstructured data into structured
templates. Existing heuristic-based methods and neural network-based methods
suffer from low accuracy problems due to the reliance on handcrafted heuristics
or specific log patterns in training sets. Recently, large language models
(LLMs) have shown great potential in log template generation. However, they
often struggle with ambiguous, complex, or highly specific log content, which
can lead to errors in generating accurate templates. To address these
challenges, we propose LLMLog, a multi-round annotation framework with adaptive
in-context learning. We first propose an edit-distance-based similarity metric
to evaluate log similarity. Then, we introduce a method to select the most
informative $k$ unlabeled logs for annotation by considering both the
representativeness of the logs and the confidence of LLM predictions.
Additionally, we design an adaptive context selection strategy that adaptively
selects labeled logs to ensure comprehensive keyword coverage for unlabeled
logs. These labeled logs serve as the context for LLMs to better understand the
unlabeled logs, thereby enhancing the accuracy of template generation.
Extensive experiments on sixteen datasets demonstrate that LLMLog outperforms
the state-of-the-art approaches.

</details>


### [17] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: 论文提出了一种名为CoDe的新方法，通过覆盖设计和张量分解技术，高效且准确地解决基数估计问题。


<details>
  <summary>Details</summary>
Motivation: 基数估计是数据库管理中的关键任务，现有方法在准确性和速度上难以兼顾，亟需一种同时满足高精度和低延迟的解决方案。

Method: CoDe利用覆盖设计将表划分为多个重叠的小段，并通过张量分解建模数据分布，结合创新算法选择最佳分布组合。

Result: 实验表明，CoDe在多个数据集上实现了高精度和高效性，超过半数查询达到绝对准确。

Conclusion: CoDe在基数估计领域取得了显著进展，兼具高准确性和计算效率。

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [18] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: 论文提出了一种评估大型语言模型（LLMs）处理图结构数据查询中歧义问题的分类法，并引入AmbiGraph-Eval基准测试，发现现有模型在歧义查询中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图结构查询常存在歧义，而LLMs在处理这些歧义时表现不足，需要系统评估和改进。

Method: 提出一种图查询歧义的分类法（属性歧义、关系歧义、属性-关系歧义），并构建AmbiGraph-Eval基准测试，评估9种代表性LLMs。

Result: 即使顶级LLMs在处理歧义图查询时也表现不佳，揭示了歧义处理能力的不足。

Conclusion: 研究揭示了LLMs在图查询歧义处理上的关键缺陷，为未来开发专门解决技术提供了动力。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [19] [An improved local search based algorithm for $k^-$-star partition](https://arxiv.org/abs/2508.09361)
*Mingyang Gong,Guohui Lin,Brendan Mumey*

Main category: cs.DS

TL;DR: 本文提出了一种改进的近似算法，用于解决$k^-$-星划分问题，时间复杂度为$O(|V|^3)$，近似比为$\frac{k}{2} - \frac{k-2}{8k-14}$。


<details>
  <summary>Details</summary>
Motivation: 研究如何在简单无向图中用最少的顶点不相交的星（每颗星最多$k$个顶点）覆盖所有顶点，提升现有算法的效率与近似比。

Method: 算法从具有最少1-星的初始划分开始，通过识别关键顶点（位于2-星或作为3-星的中心），并迭代应用三种局部搜索操作优化解。

Result: 最终解中每颗星的顶点不会与过多关键顶点相邻，通过令牌分摊方案证明了近似比。

Conclusion: 该算法在时间和近似比上均优于现有方法，为$k^-$-星划分问题提供了更优解。

Abstract: We study the $k^-$-star partition problem that aims to find a minimum
collection of vertex-disjoint stars, each having at most $k$ vertices to cover
all vertices in a simple undirected graph $G = (V, E)$. Our main contribution
is an improved $O(|V|^3)$-time $(\frac k2 - \frac {k-2}{8k-14})$-approximation
algorithm.
  Our algorithm starts with a $k^-$-star partition with the least $1$-stars and
a key idea is to distinguish critical vertices, each of which is either in a
$2$-star or is the center of a $3$-star in the current solution. Our algorithm
iteratively updates the solution by three local search operations so that the
vertices in each star in the final solution produced cannot be adjacent to too
many critical vertices. We present an amortization scheme to prove the
approximation ratio in which the critical vertices are allowed to receive more
tokens from the optimal solution.

</details>


### [20] [A Classical Quadratic Speedup for Planted $k$XOR](https://arxiv.org/abs/2508.09422)
*Meghal Gupta,William He,Ryan O'Donnell,Noah G. Singer*

Main category: cs.DS

TL;DR: 本文提出了一种新的经典算法，比之前的最佳算法快二次方，适用于大常数k的噪声植入kXOR问题，缩小了量子算法的优势。


<details>
  <summary>Details</summary>
Motivation: Schmidhuber等人的量子算法在噪声植入kXOR问题上比经典算法快四次方，本文旨在缩小这一差距。

Method: 结合亚线性时间算法（生日悖论）和多项式反集中工具，设计新算法。

Result: 新算法在大常数k情况下比之前最佳经典算法快二次方，量子优势缩小为二次方。

Conclusion: 新算法在经典框架下显著提升性能，但仍保留量子算法的空间优势。

Abstract: A recent work of Schmidhuber et al (QIP, SODA, & Phys. Rev. X 2025) exhibited
a quantum algorithm for the noisy planted $k$XOR problem running quartically
faster than all known classical algorithms. In this work, we design a new
classical algorithm that is quadratically faster than the best previous one, in
the case of large constant $k$. Thus for such $k$, the quantum speedup of
Schmidhuber et al. becomes only quadratic (though it retains a space
advantage). Our algorithm, which also works in the semirandom case, combines
tools from sublinear-time algorithms (essentially, the birthday paradox) and
polynomial anticoncentration.

</details>


### [21] [Retroactive Monotonic Priority Queues via Range Searching](https://arxiv.org/abs/2508.09892)
*Lucas Castro,Rosiane de Freitas*

Main category: cs.DS

TL;DR: 本文研究了单调优先队列的完全回溯版本，提出了一种时间复杂度为O(log m + T(m))的解决方案，并进一步优化至O(log m log log m)。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在完全回溯优先队列中实现与非回溯或部分回溯队列相同的O(log m)时间复杂度。

Method: 将单调优先队列的最小值查找问题转化为范围搜索问题，并设计相应的数据结构。

Result: 提出了一种时间复杂度为O(log m + T(m))的完全回溯单调优先队列，并进一步优化至O(log m log log m)。

Conclusion: 证明了在单调优先队列中，完全回溯可以实现接近O(log m)的时间复杂度。

Abstract: The best known fully retroactive priority queue costs $O(\log^2 m \log \log
m)$ time per operation, where $m$ is the number of operations performed on the
data structure. In contrast, standard (non-retroactive) and partially
retroactive priority queues cost $O(\log m)$ time per operation. So far, it is
unknown whether this $O(\log m)$ bound can be achieved for fully retroactive
priority queues.
  In this work, we study a restricted variant of priority queues known as
monotonic priority queues. We show that finding the minimum in a retroactive
monotonic priority queue is a special case of the range-searching problem. We
design a fully retroactive monotonic priority queue with a cost of $O(\log m +
T(m))$ time per operation, where $T(m)$ is the maximum between the query and
the update time of a specific range-searching data structure with $m$ elements.
Finally, we design a fully retroactive monotonic priority queue that costs
$O(\log m \log \log m)$ time per operation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于特征标记化的Transformer模型，用于实时预测飞机的预计到达时间（ETA），在准确性和效率上均优于传统的提升树模型。


<details>
  <summary>Details</summary>
Motivation: 实时ETA预测对航空到达管理至关重要，特别是在跑道排序中。现有方法在效率和准确性上难以满足实时需求。

Method: 采用特征标记化将原始输入映射到潜在空间，并利用Transformer的多头自注意力机制捕捉关键特征，避免复杂的特征工程。模型支持高频率（1HZ）的并行计算。

Result: 实验表明，该方法比XGBoost准确率提高7%，计算时间仅需39%，且在40架飞机同时预测时，推理时间仅为51.7微秒。

Conclusion: 该方法高效且准确，适用于实时到达管理系统。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [23] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN是一个多模态感知的动态噪声编辑框架，通过分块和动态去噪强度分配，有效保留关键信息并抑制噪声。MoLAN+在此基础上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感分析中因无关或误导性信息导致的性能下降问题。

Method: 将模态特征分块，动态分配去噪强度，保留关键信息。

Result: 在多个模型和数据集上验证了框架的广泛有效性，MoLAN+达到最优性能。

Conclusion: MoLAN框架灵活且高效，适用于多模态模型，显著提升情感分析性能。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [24] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM transformer的上下文学习（ICL）理论，用于优化WiFi 7中的信道访问，解决了传统方法在动态信道环境下性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统二进制指数退避方案在动态信道环境下性能较差，且现有模型方法在节点密度估计不准确时仍会导致吞吐量损失。

Method: 设计了基于transformer的ICL优化器，通过预收集碰撞阈值数据示例和查询碰撞案例作为输入提示，生成预测的竞争窗口阈值（CWT）。

Result: 实验证明该方法在未知节点密度下具有快速收敛和接近最优的吞吐量性能。

Conclusion: 提出的方法在理论和实验上均优于现有模型和深度强化学习方法。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [25] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 论文提出了一种有限选择性预测（PLS）模型，研究在预测窗口受限时的最优预测误差，并引入了一种复杂度度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测模型允许预测者在任意时间开始预测，但现实中预测窗口可能受限，因此需要研究这种受限情况下的预测性能。

Method: 引入PLS模型，分析实例相关和平均情况下的最优预测误差，并提出一种复杂度度量方法。

Result: 复杂度度量方法能够提供实例相关的最优误差界限，且在随机生成的PLS实例中高概率匹配。

Conclusion: PLS模型为受限预测窗口下的预测问题提供了理论框架和实用工具。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [26] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B是一个2.6B参数的基础模型，旨在平衡高性能与计算效率，通过创新架构（如差分注意力和PolyNorm激活函数）提升长上下文理解、减少幻觉并增强上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决新兴研究组在开发高性能且计算高效的基础LLM方面的挑战。

Method: 引入差分注意力和PolyNorm激活函数等创新架构，并通过实验优化模型设计。

Result: 在多样化基准测试中，Motif-2.6B表现优于或与同类先进模型相当，展示了高效性和可扩展性。

Conclusion: Motif-2.6B为高效、可扩展且强大的基础LLM提供了重要进展，为未来研究和部署奠定了基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [27] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级、无需训练的方法，利用检索增强生成（RAG）跨模态线性映射，解决多模态模型中的模态间隙问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（LMMs）存在模态间隙问题，即文本和视觉表征在共享嵌入空间中的不对齐。传统微调方法成本高且不实用，因此需要一种轻量级解决方案。

Method: 采用检索增强生成（RAG）技术，通过线性映射跨模态检索训练集中的文本描述，结合指令生成新的文本描述，并引入迭代技术优化映射。

Result: 在两个基准多模态数据集上的实验结果显示，该方法显著提升了性能。

Conclusion: 提出的轻量级方法有效解决了模态间隙问题，且无需昂贵训练，具有实际应用潜力。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [28] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 论文整合了15个数据集，构建了一个包含2510名受试者、1.49亿次血糖测量的大规模数据库，用于研究1型糖尿病和低血糖事件。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病（T1D）患者依赖胰岛素注射，但胰岛素可能导致低血糖，严重时危及生命。现有研究因数据不足受限，因此需要大规模数据集支持机器学习模型预测血糖水平。

Method: 系统整合15个数据集，构建包含血糖、人口统计和心率数据的数据库，并评估数据质量。

Result: 数据库包含1.49亿次测量，其中4%为低血糖范围。研究发现血糖与心率在低血糖前15至55分钟存在相关性。

Conclusion: 整合的数据集为糖尿病研究提供了重要资源，但数据不平衡和缺失值仍需解决。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [29] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 论文质疑复杂序列混合器在时间序列分析中的必要性，提出用简单密集层替代，实验证明效果相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明复杂序列混合器（如注意力机制）可能并非必要，其优势可能源于其他因素。论文旨在验证简单密集层是否能替代复杂序列混合器。

Method: 提出JustDense方法，将序列混合器替换为密集层，基于MatrixMixer框架进行理论分析，并在29个基准测试中验证。

Result: 实验显示密集层替代后性能相当或更优，挑战了复杂架构必然更好的假设。

Conclusion: 复杂序列混合器在时间序列分析中并非必需，简单密集层可提供类似或更优效果。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [30] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec是一个基于几何框架的推荐系统，通过Ricci曲率和流分析动态金融图中的因果关系，提升推荐鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融决策支持需要更鲁棒和可解释的推荐方法，尤其是在动态金融图中追踪冲击传播和局部压力。

Method: 利用离散Ricci曲率量化局部压力，通过Ricci流追踪冲击传播，并基于曲率梯度设计结构风险感知的排名函数。

Result: 在S&P 500数据和FinBERT情感分析上的初步结果显示，该方法在合成扰动下具有更好的鲁棒性和可解释性。

Conclusion: RicciFlowRec是首个将几何流推理应用于金融决策支持的推荐系统，未来计划扩展到投资组合优化和收益预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [31] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI是一种深度学习框架，结合I-G变换和2SRI技术，解决社交网络中同伴效应的反馈和未观察混杂问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时处理同伴效应的反馈和未观察混杂，导致估计不准确。

Method: 使用I-G变换消除反馈偏差，通过2SRI技术构建工具变量并训练神经网络控制混杂。

Result: 理论证明一致性，实验显示DIG2RSI优于现有方法。

Conclusion: DIG2RSI有效解决了复杂网络中的同伴效应估计问题。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [32] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究了非对称低秩矩阵补全问题，提出了一种无需正则化项的梯度下降方法，并通过理论和实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法通常需要正则化项来保证收敛，但本文发现去除正则化项并不影响收敛性能，因此探索了一种更简洁的方法。

Method: 采用谱初始化的普通梯度下降法，结合留一技术，理论证明其线性收敛率，并展示了梯度下降的隐式正则化特性。

Result: 实验表明，该方法计算成本更低，同时保持了与其他梯度下降算法相当的补全性能。

Conclusion: 去除正则化项的梯度下降方法在非对称低秩矩阵补全中表现优异，具有高效性和隐式正则化特性。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [33] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 论文提出AdaPO框架，通过自适应调整训练目标和动态奖励机制，解决多目标优化中的奖励攻击问题，提升模型的自我评估和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在自我评估能力上存在不足，传统强化学习的固定奖励机制在多目标优化中易导致奖励攻击和模型崩溃。

Method: 提出AdaPO框架，结合自适应奖励模型（ARM）和动态KL正则化机制，实时调整训练目标。

Result: 在8个基准测试中显著提升了模型的直接推理和自我评估能力。

Conclusion: AdaPO框架有效解决了奖励攻击问题，为模型自我改进提供了新思路。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [34] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 提出了一种微调流匹配生成模型的框架，用于强制物理约束并解决科学系统中的逆问题。通过可微的后训练程序最小化偏微分方程的弱形式残差，提升物理一致性。


<details>
  <summary>Details</summary>
Motivation: 解决科学系统中基于低保真或观测数据的生成模型在物理约束和逆问题上的不足。

Method: 采用可微的后训练程序优化弱形式残差，并结合可学习的潜在参数预测器进行联合优化。

Result: 在经典偏微分方程基准测试中验证了方法的有效性，提升了物理约束的满足和潜在系数的准确恢复。

Conclusion: 该方法将生成建模与科学推理结合，为物理系统的数据高效建模和仿真增强发现提供了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [35] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive提出了一种多目标强化学习框架，通过对抗优化实现轨迹生成与评估的闭环协同进化，解决了现有方法无法迭代优化和偏好标量化偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶决策方法中，轨迹生成与评估分离导致无法迭代优化，而强化学习方法将多维偏好压缩为标量奖励，掩盖了关键权衡。

Method: EvaDrive采用分层生成器（结合自回归意图建模和扩散模型）与可训练多目标评估器的对抗框架，通过帕累托前沿选择机制实现多轮迭代优化。

Result: 在NAVSIM和Bench2Drive基准测试中表现优异，分别达到94.9 PDMS和64.96 Driving Score，超越现有方法。

Conclusion: EvaDrive通过动态加权生成多样化驾驶风格，无需外部偏好数据，为类人迭代决策提供了无标量化的轨迹优化新方法。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [36] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导记忆网络（PgMN），结合深度学习和物理模型，解决历史数据不足或缺失时的能源消耗预测问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习和物理模型在能源预测中的局限性，特别是在历史数据不足或缺失的情况下。

Method: PgMN结合并行投影层、记忆单元和记忆经验模块，整合深度学习和物理模型的预测结果。

Result: PgMN在多种场景（如新建建筑、数据缺失等）中表现出高准确性和适用性。

Conclusion: PgMN为动态建筑环境中的能源预测提供了有效解决方案，扩展了模型在数据不足或无数据场景中的应用。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [37] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 论文提出了一种基于自编码器和定制化windowSHAP算法的无监督可解释AI框架，用于实时检测和表征核反应堆中的重放攻击。


<details>
  <summary>Details</summary>
Motivation: 下一代先进核反应堆依赖数字化系统，数据完整性对安全至关重要。现有方法多依赖合成数据或有限假设，无法满足实际需求。

Method: 结合自编码器和定制化windowSHAP算法，实现攻击检测、来源识别、时间和类型分析。

Result: 在PUR-1反应堆真实数据测试中，框架能95%以上准确率检测并识别攻击信号。

Conclusion: 该框架为核反应堆数据完整性提供了高效且可解释的解决方案。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [38] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种名为可调序列长度（ASL）的新方案，将混合精度概念应用于随机计算（SC）神经网络，以减少能量和延迟开销。


<details>
  <summary>Details</summary>
Motivation: 随机计算（SC）在资源受限场景（如物联网）中是一种高效低功耗的神经网络实现方式，但层间混合精度实现的进一步优化尚未探索。

Method: 通过引入基于算子范数的理论模型，分析截断噪声的累积传播，并提出两种截断策略（粗粒度和细粒度）。

Result: 在32nm工艺下合成的SC多层感知器上，ASL方案可减少60%以上的能量和延迟开销，且精度损失可忽略。

Conclusion: ASL方案在物联网应用中具有可行性，并凸显了混合精度截断在SC设计中的独特优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [39] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散模型的人口合成方法，旨在解决高维属性下样本数据稀疏的问题，并在生成采样零点的同时最小化结构零点。


<details>
  <summary>Details</summary>
Motivation: 人口合成是智能交通系统分析中的关键任务，但高维属性导致样本数据稀疏，难以准确建模。

Method: 采用扩散模型估计人口的联合分布，并与VAE和GAN方法进行比较。

Result: 新方法在边际分布相似性、可行性和多样性方面优于现有方法。

Conclusion: 扩散模型在人口合成中实现了可行性与多样性的更好平衡。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [40] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG框架通过掩码训练策略解决不同ECG布局导致的信号异步和部分缺失问题，显著提升心律失常诊断的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 不同医院的ECG布局差异导致信号异步和部分缺失，现有模型难以处理，需开发适应性强的解决方案。

Method: 提出PatchECG框架，基于掩码训练策略自适应学习关键区块，利用导联间协作依赖关系识别心律失常。

Result: 在PTB-XL数据集和生成数据上，AUROC达0.835；外部验证中房颤诊断AUROC为0.778，优于基线方法。

Conclusion: PatchECG在不同ECG布局下表现稳定且优于现有方法，为心律失常诊断提供了可靠工具。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [41] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M数据集和SVGen模型解决了从自然语言生成精确SVG代码的挑战，提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 将创意转化为精确的矢量图形耗时且困难，需要一种高效的方法。

Method: 利用SVG-1M数据集，结合课程学习和强化学习优化，开发了SVGen模型。

Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。

Conclusion: SVGen为前端开发和设计提供了高效的工具，代码和数据集已开源。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [42] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP是一种针对非独立同分布（non-IID）数据的联邦学习方法，通过随机特征流形补全和类原型对齐提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中非独立同分布数据导致的模型收敛和性能问题，尤其是在医学影像领域。

Method: 使用随机特征流形补全和类原型对齐，优化客户端特征流形的一致性。

Result: 在多个医学影像和自然图像数据集上表现优于现有联邦学习算法。

Conclusion: FedMP有效提升了非IID数据下的联邦学习性能，同时分析了流形维度、通信效率和隐私影响。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [43] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: 论文提出了一种动态量化训练（DQT）框架，通过嵌套整数表示和定制整数运算，实现了高效动态量化，避免了传统方法的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有动态量化方法需要昂贵的浮点运算来切换精度，破坏了整数硬件范式并影响性能。DQT旨在消除这一瓶颈。

Method: DQT采用嵌套整数表示和定制整数运算，通过低成本位移操作实现动态精度切换。

Result: 在ImageNet上，4位动态ResNet50达到77.00% top-1准确率，优于静态和动态方法，且切换成本显著降低。

Conclusion: DQT为高效自适应AI开辟了新途径，显著提升了动态量化的性能与效率。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [44] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: scAGC是一种基于对比学习的单细胞聚类方法，通过动态调整细胞图结构和特征表示，解决了传统方法在高维稀疏数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据的高维性和稀疏性使传统聚类方法面临挑战，现有图神经网络方法依赖静态图结构，对噪声敏感且难以捕捉长尾分布。

Method: scAGC结合拓扑自适应图自编码器和可微分Gumbel-Softmax采样策略，动态优化图结构，并引入ZINB损失和对比学习目标以提高鲁棒性和稳定性。

Result: 在9个真实数据集上，scAGC在NMI和ARI指标上均优于现有方法。

Conclusion: scAGC通过自适应图学习和对比指导，显著提升了单细胞聚类的准确性和鲁棒性。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [45] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于诚实拍卖的长期客户端选择联邦学习方案（LCSFLA），用于解决车联网中非独立同分布数据导致的模型收敛和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 车联网中智能车辆的联邦学习面临非独立同分布数据、资源浪费和信息不对称等挑战，传统客户端选择方法效果有限。

Method: 提出LCSFLA方案，结合长期数据质量评估机制和能源成本，采用拍卖机制激励客户端参与并确保信息真实性。

Result: 理论证明了激励机制的兼容性和个体合理性，实验结果表明LCSFLA能有效缓解非独立同分布数据导致的性能下降。

Conclusion: LCSFLA通过长期评估和拍卖机制，显著提升了联邦学习在车联网中的效果。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [46] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 综述探讨了基于接触和非接触的呼吸分析方法，重点介绍了机器学习和深度学习在呼吸分析中的应用，以及相关挑战和未来趋势。


<details>
  <summary>Details</summary>
Motivation: 传统接触式呼吸监测方法在舒适性和实用性上存在局限，需要探索非接触式方法以提升长期监测的可行性。

Method: 分析了Wi-Fi信道状态信息和声学传感等非接触技术，并比较了机器学习和深度学习模型在呼吸分析中的表现。

Result: 非接触方法能够实现准确、无创的呼吸监测，适用于多用户场景和疾病检测。

Conclusion: 综述为呼吸分析的未来研究提供了框架，强调了可解释AI、联邦学习等新兴技术的潜力。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [47] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种细粒度安全神经元（FGSN）方法，通过无训练持续投影技术减少微调的安全风险，平衡安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLM）可能破坏原有对齐机制并引入安全风险，现有防御方法缺乏对安全层和细粒度神经元的综合考虑。

Method: 提出FGSN方法，结合安全层与神经元的交互，定位稀疏且精确的安全神经元，并通过投影技术提升安全性。

Result: 实验表明，该方法显著降低有害性评分和攻击成功率，同时保持模型实用性。

Conclusion: FGSN通过多维度异构安全神经元集群优化机制，实现了对未知安全问题的持续防御和泛化能力。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [48] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast是一个基于LLM的框架，通过语言符号表示实现上下文感知的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在多个关键领域至关重要，但现有方法难以整合历史数值序列和上下文特征（如非结构化文本数据）。

Method: 使用离散标记化将连续数值序列转换为时间标记，并通过预训练LLM嵌入共享表示空间，最后通过监督微调预测未来标记。

Result: 在多个真实数据集上的实验验证了TokenCast的有效性和泛化能力。

Conclusion: TokenCast通过统一语义空间解决了多模态数据融合的挑战，提升了预测准确性。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [49] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出了一种名为离散扩散强迫（D2F）的策略，通过将扩散大语言模型（dLLMs）改造为自回归-扩散混合范式，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的开源dLLMs在推理速度上未能超越类似规模的自回归（AR）LLMs，本文旨在打破这一限制。

Method: D2F策略通过块级自回归生成和跨块并行解码，结合非对称蒸馏过程和流水线并行解码算法，实现高效推理。

Result: 实验表明，D2F dLLMs在GSM8K上的推理速度比LLaMA3和Qwen2.5快2.5倍以上，比LLaDA和Dream等vanilla dLLMs快50倍以上，且输出质量相当。

Conclusion: D2F策略成功将dLLMs改造为高效推理的混合范式，为文本生成提供了新的可能性。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [50] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL提出了一种多目标表示学习方法，通过结合句子嵌入和多标签分类网络，显著提升了生成模型对复杂文本指令的响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用文本输入的丰富表达性时表现不足，尤其是在多目标指令下，导致可控性受限。

Method: 结合句子嵌入作为条件，使用多标签分类和多头回归网络训练多目标嵌入空间。

Result: 实验结果显示，该方法在多目标指令下的可控性提升了13.8%。

Conclusion: MIPCGRL能够处理复杂指令，实现更具表达力和灵活性的内容生成。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [51] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的框架，用于在去中心化系统中自动选择最优的推理加速方法，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模模型（如LLMs）的部署成本高昂，且面临扩展性和数据安全的挑战。去中心化系统成为趋势，但需要高效的推理加速方案。

Method: 引入元学习框架，通过学习历史性能数据，自动选择适合不同任务的加速策略。

Result: 该框架在效率和性能上优于传统方法，为去中心化AI系统提供了更经济可行的解决方案。

Conclusion: 元学习框架为去中心化AI系统的推理加速提供了有效路径，推动了更民主和经济的人工智能发展。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [52] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 论文提出了一种名为ADT4Coupons的新框架，用于优化在线平台的优惠券分发策略，以提升长期收入。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能充分利用平台与用户之间的复杂序列交互，导致性能瓶颈。

Method: 提出ADT4Coupons框架，整合通用场景、序列建模和高效迭代更新，优化决策。

Result: 在真实工业数据集、公开数据集和合成数据集上的实验证明了框架的优越性。

Conclusion: ADT4Coupons能有效提升长期收入，适用于多种营销场景。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [53] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 论文提出了一种新的建筑安全数据集（CSDataset），整合了结构化属性和非结构化叙述，支持机器学习和大型语言模型的应用，并通过初步分析展示了其价值。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集数量有限且多样性不足，限制了深入分析。

Method: 引入CSDataset，整合OSHA记录的多种数据，并进行初步基准测试和跨层次分析。

Result: 发现投诉驱动的检查与后续事故减少17.3%的可能性相关。

Conclusion: CSDataset为建筑安全研究提供了新工具，支持未来更深入的分析和改进。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [54] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE是一种基于混合专家架构的量化推理框架，通过动态路由输入数据到最适合的量化专家模型，缓解单量化模型的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 量化方法在提高模型效率和降低部署成本方面至关重要，但量化过程会导致精度下降。MoQE旨在联合提升量化模型的性能。

Method: MoQE结合多个全精度模型的量化变体作为专门的“量化专家”，并基于输入数据的特性动态路由到最合适的专家。设计了轻量级、结构感知的路由器模型。

Result: 在ResNet、LLaMA和Qwen模型家族上的实验表明，MoQE性能接近SOTA量化模型，且未显著增加推理延迟。

Conclusion: MoQE通过混合专家架构有效提升了量化模型的性能，适用于CV和NLP任务。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [55] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出了一种基于可微分转移模块的修复算法，用于优化微LED制造中的转移步骤，减少XY平台移动，并支持灵活的目标设计。


<details>
  <summary>Details</summary>
Motivation: 在高通量微LED制造中，激光选择性转移需要计算模型来规划转移序列，以减少XY平台的运动并适应不同的优化目标。

Method: 提出了一种基于可微分转移模块的修复算法，能够建模转移平台的离散位移，并通过梯度优化进行训练。

Result: 实验显示，该方法在2000x2000阵列上减少了50%的转移步骤，规划时间不到2分钟。

Conclusion: 该方法为加速AR/VR和下一代显示器的微LED修复提供了实用且灵活的解决方案。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [56] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec是一种分层自适应网络，通过动态选择层和权重合并机制，提升预训练模型在测试时对复杂分布变化的适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖单维线性分类层，难以处理复杂分布变化，Hi-Vec旨在解决这一问题。

Method: 提出动态层选择、权重合并机制和线性层一致性门控函数，实现分层自适应。

Result: 实验证明Hi-Vec在多种挑战性场景中表现优异，提升了鲁棒性和抗噪能力。

Conclusion: Hi-Vec显著改进了现有方法，适用于复杂分布变化和噪声数据。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [57] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: 提出了一种名为GSMT的混合模型，结合图注意力网络和序列到序列RNN，用于公交车轨迹预测，并在真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的发展中地区，仅依赖GPS数据预测公交车轨迹具有挑战性，需改进现有方法。

Method: GSMT结合GAT和RNN，通过任务校正器提取行为模式，分两阶段进行预测和优化。

Result: 在吉隆坡的真实数据集上，GSMT在短期和长期轨迹预测中均显著优于现有方法。

Conclusion: GSMT通过混合模型和任务校正器，有效提升了复杂城市环境下的公交车轨迹预测精度。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [58] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 论文提出了一种结合量子启发图神经网络（QI-GNN）和集成模型（QBoost或随机森林分类器）的新方法，用于区块链网络中的反洗钱（AML）交易检测。通过引入CP分解层，增强了处理复杂数据结构的能力，实验结果显示F2分数达74.8%，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域快速发展，区块链网络中的非法交易检测仍是一个关键挑战，需要创新解决方案。

Method: 提出了一种结合QI-GNN和集成模型的方法，并在图神经网络框架中引入了CP分解层，以高效处理复杂数据结构。

Result: 实验结果显示，该方法在检测欺诈交易中F2分数达到74.8%，优于传统机器学习方法。

Conclusion: 量子启发算法在金融安全领域的复杂网络分析中具有潜力，值得进一步探索和推广。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [59] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出了一种基于LLM的原型估计框架，用于零样本和小样本表格学习，无需训练分类器或微调LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在表格数据建模中潜力巨大，但在零样本和小样本场景中有效利用仍具挑战性。

Method: 通过任务和特征描述生成特征值，构建零样本原型，并融合小样本数据增强。

Result: 实验证明该方法在零样本和小样本表格学习中有效。

Conclusion: 该框架避免了基于示例提示的限制，提供了可扩展且稳健的解决方案。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [60] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 研究提出了一种基于局部场电位（LFP）的深度学习模型，用于单次试验气味检测，验证了嗅觉球信号的有效性，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前气味检测传感器在复杂混合物中表现不佳，且非侵入性记录缺乏单次试验的可靠性。研究旨在验证LFP频谱特征和嗅觉球信号的适用性。

Method: 使用一维卷积网络（ResCNN和AttentionCNN）的集成模型，从多通道嗅觉球LFP中解码气味存在。

Result: 在七只清醒小鼠的2,349次试验中，模型平均准确率达86.6%，F1分数81.0%，AUC为0.9247，显著优于基准。

Conclusion: 研究证实了基于LFP的稳健单次试验气味检测的可行性，并展示了深度学习在理解嗅觉表征中的潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [61] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 该论文提出了一种评估图神经网络（GNNs）中过度压缩（over-squashing）问题的方法，并研究了重新布线（rewiring）技术对缓解该问题的效果。


<details>
  <summary>Details</summary>
Motivation: 消息传递GNNs存在长距离信息过度压缩的问题，限制了其表达能力，但缺乏直接的评估指标。

Method: 提出了一种基于拓扑的方法，通过节点对的相互敏感性衰减率评估过度压缩，并扩展为四种图级统计量。

Result: 实验表明，图分类数据集普遍存在过度压缩问题，重新布线能有效缓解，但效果因数据集和方法而异；节点分类中过度压缩不明显。

Conclusion: 重新布线在过度压缩严重且适度调整时最有效，过度或在不必要的情况下使用可能适得其反。论文还提供了诊断工具帮助实践者决策。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [62] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 论文研究了协作多智能体强化学习（c-MARL）在现实约束条件下的新漏洞，提出了一种高效的对抗扰动生成算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注训练时攻击或不现实场景，缺乏对实际部署中观察扰动漏洞的深入探讨。

Method: 在更现实的约束条件下（如仅能收集和扰动部署智能体的观察），提出简单高效的对抗扰动生成算法。

Result: 在三个基准和22个环境中验证了算法的有效性，且仅需1,000样本，远低于先前方法的百万级需求。

Conclusion: 研究揭示了c-MARL在实际部署中的新漏洞，并提供了高效的对抗攻击方法。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [63] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 提出了一种基于模式的知识组件（KC）自动发现框架，通过变分自编码器和注意力机制提取学生代码中的关键模式，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育中个性化学习需要准确建模学生知识，但现有知识组件提取方法因缺乏解释性和编程问题的开放性而面临挑战。

Method: 使用变分自编码器生成代表性代码模式，结合注意力机制识别重要模式，聚类形成模式化知识组件。

Result: 实验表明，该方法能生成有意义的学习轨迹，并在深度知识追踪（DKT）中优于传统方法。

Conclusion: 该框架为计算机科学教育提供了自动化、可扩展且可解释的知识建模方法。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [64] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 数据集蒸馏将大数据集压缩为小合成数据集，使学习合成数据集近似于原始数据集学习，甚至只需一步梯度下降。研究展示了其通用性，可将强化学习任务转化为监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 探索数据集蒸馏的通用性，尤其是将强化学习任务压缩并转化为监督学习任务的能力。

Method: 提出了一种基于近端策略优化的元学习方法，用于蒸馏多维经典问题（如cart-pole）、MuJoCo环境和Atari游戏。

Result: 展示了蒸馏能将复杂RL环境压缩为一步监督学习任务，并验证了其在不同学习架构中的通用性。

Conclusion: 数据集蒸馏不仅能高效压缩任务，还能跨学习模态转换，具有广泛适用性。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [65] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出了一种结合联邦学习和区块链技术的去中心化天气预测框架，以提高隐私性、安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前集中式天气预测系统存在安全漏洞、可扩展性差和单点故障问题，需要一种更安全、可靠的解决方案。

Method: 结合联邦学习（保护隐私）和区块链技术（透明验证），引入基于信誉的投票机制和IPFS存储。

Result: 实验表明，该方法提高了预测准确性，增强了系统韧性和可扩展性。

Conclusion: 该框架适用于现实世界中对安全性要求高的环境。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [66] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种精确验证方法GNNev，用于增强图神经网络（GNNs）对抗属性与结构扰动的鲁棒性，支持多种聚合函数。


<details>
  <summary>Details</summary>
Motivation: GNNs在高风险应用中易受对抗攻击，现有方法对常用聚合函数的支持不足。

Method: 采用约束求解与边界收紧技术，迭代解决松弛约束满足问题，支持sum、max和mean聚合函数。

Result: 在多个数据集（Cora、CiteSeer、Amazon、Yelp）上验证了GNNev的有效性和优越性。

Conclusion: GNNev在精确验证任务中表现优于现有工具，尤其在sum聚合任务中。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [67] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 提出了一种基于权重大小的突触修剪方法，模拟生物大脑的修剪机制，逐步移除低重要性连接，显著提升了时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 生物大脑通过突触修剪移除弱连接以提高效率，而人工神经网络中的Dropout随机失活神经元，缺乏活动依赖性修剪。

Method: 在训练过程中，基于权重的绝对大小计算重要性，采用立方进度表逐步增加全局稀疏性，定期永久移除低重要性权重。

Result: 在多个时间序列预测模型和数据集上表现优异，金融预测中平均绝对误差降低高达20%，部分Transformer模型误差降低52%。

Conclusion: 该方法通过动态修剪机制改进了正则化，易于集成到多种架构中，是传统Dropout的有效替代方案。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [68] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 领域限制的稀疏自编码器（SAEs）在医学文本上训练，显著提升了特征解释性和重建保真度，减少了线性残差。


<details>
  <summary>Details</summary>
Motivation: 传统SAEs在广泛数据分布上训练，导致特征碎片化和难以解释的线性残差，领域限制训练可解决这一问题。

Method: 在Gemma-2模型的第20层激活上，使用195k临床问答数据训练JumpReLU SAEs。

Result: 领域限制SAEs解释了更多方差（20%），提高了损失恢复，减少了线性残差，特征与临床概念对齐。

Conclusion: 领域限制训练改善了SAEs的局限，建议重新评估通用SAEs的扩展性。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [69] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 文本到图像模型能将痴呆相关语音信息与生成图像对齐，并通过可解释性方法分析语言部分对检测的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究文本到图像模型是否能将病理语音（如痴呆）与生成图像对齐，并探索其可解释性。

Method: 利用文本到图像模型生成图像，并通过可解释性方法分析语言部分对痴呆检测的贡献。

Result: 仅通过生成图像即可实现痴呆检测，在ADReSS数据集上准确率达75%。

Conclusion: 文本到图像模型能有效对齐痴呆相关语音与图像，为病理语音分析提供新方法。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [70] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习的金融风险评估框架，通过特征注意力和时序建模结构实现跨机构联合建模，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构金融风险分析中的数据隐私和协作建模挑战。

Method: 采用分布式优化策略，各机构训练本地子模型，参数通过差分隐私和噪声注入保护后上传，中央服务器聚合生成全局模型。

Result: 实验表明，该方法在通信效率、模型准确性和风险检测等方面优于传统集中式方法和现有联邦学习变体。

Conclusion: 该方法在保护数据主权的同时提升了风险识别的范围和效率，为智能金融风险分析提供了安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [71] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML是一种新的元学习算法，结合梯度匹配和锐度感知最小化，提升模型在有限数据下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决在少量训练数据下任务泛化的问题。

Method: 结合梯度匹配和锐度感知最小化的双层优化框架。

Result: 在基准数据集上表现优于现有方法，准确率和泛化能力更强。

Conclusion: DGS-MAML适用于少样本学习和快速适应场景，代码已开源。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [72] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出了一种无监督异常检测方法，针对分布式后端服务系统的复杂结构依赖、行为演化和无标签数据问题，结合动态图与Transformer建模，实现端到端检测。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端服务系统中异常检测的实际挑战，如复杂结构依赖、行为演化和缺乏标签数据。

Method: 构建动态图捕捉服务调用关系，使用图卷积提取高阶结构特征，Transformer建模节点时序行为，融合特征生成异常向量，非线性映射计算异常分数。

Result: 在真实云监控数据上表现优于现有模型，捕捉异常传播路径和动态行为序列的能力更强。

Conclusion: 方法具有较强表达能力和稳定性，适合实际部署。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [73] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种隐式超图神经网络（IHGNN），通过非线性固定点方程计算表示，解决了传统超图神经网络依赖固定层数的问题，提升了长距离依赖捕获能力和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多交互是基于群体的（如多作者论文），传统超图神经网络因固定层数限制长距离依赖捕获且训练不稳定。

Method: 引入隐式超图神经网络（IHGNN），通过非线性固定点方程计算表示，无需堆叠层，支持全局传播。开发了收敛性可证明的训练方案，分析了模型表达能力和过平滑条件。

Result: 在引用基准测试中，IHGNN在准确性和鲁棒性上均优于传统图/超图神经网络基线，且对初始化和超参数变化具有强鲁棒性。

Conclusion: IHGNN为高阶关系学习提供了稳定、高效的解决方案，具有强泛化能力和实用价值。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [74] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: CoGenT框架首次将对比学习和生成方法结合，通过联合优化提升多变量时间序列的自监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习和生成方法的互补潜力，解决各自在高类内相似性和大数据依赖上的局限性。

Method: 提出Contrastive Generative Time series框架（CoGenT），通过联合对比-生成优化统一两种范式。

Result: 在六个时间序列数据集上表现优异，F1分数分别比SimCLR和MAE提升59.2%和14.27%。

Conclusion: 混合目标在保持判别力的同时增强了生成鲁棒性，为时间域混合自监督学习奠定了基础。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [75] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: DeepFeatIoT是一种新型深度学习模型，通过融合局部和全局特征、随机卷积核特征及大语言模型特征，显著提升了物联网时间序列数据的分类性能，适用于标签数据有限的场景。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器数据存在元数据丢失、数据源异构、采样频率不一致等问题，导致原始数据难以解析，影响智能系统效果。

Method: 提出DeepFeatIoT模型，整合学习到的局部和全局特征、非学习的随机卷积核特征及大语言模型特征。

Result: 在多个真实物联网数据集上表现优于现有基准模型，展示了其泛化能力和一致性。

Conclusion: DeepFeatIoT有望推动物联网分析领域的重大进展，支持下一代智能系统的开发。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [76] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: NEXICA是一种新算法，用于识别高速公路系统中导致其他部分拥堵的区域，通过时间序列分析和概率模型提高准确性和计算速度。


<details>
  <summary>Details</summary>
Motivation: 解决交通拥堵问题，通过聚焦拥堵源头提高资源分配效率。

Method: 使用时间序列数据，开发新算法，包括事件检测、概率模型和二元分类器。

Result: 在洛杉矶地区195个传感器数据上测试，表现优于现有方法。

Conclusion: NEXICA在准确性和计算速度上优于现有技术，适用于交通拥堵分析。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [77] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 论文提出NeuronTune框架，通过动态调节稀疏神经元实现安全与效用的双重优化，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）的安全对齐技术存在鲁棒性不足、频繁拒绝良性查询、文本质量下降等问题，亟需解决安全与效用的平衡。

Method: 提出NeuronTune框架，通过归因分析识别安全关键和效用保留神经元，利用元学习动态调节神经元激活，支持灵活干预范围调整。

Result: 实验表明，NeuronTune在模型安全性和效用保持上显著优于现有技术。

Conclusion: NeuronTune通过细粒度神经元调节，实现了安全与效用的双重优化，为LLMs的可靠部署提供了有效解决方案。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [78] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 论文提出了一种联邦持续学习（FCL）框架，通过轻量级本地模型动态适应新任务，同时提升大模型的性能，解决了基础模型（FMs）在持续学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型在持续学习中表现不佳，尤其是无法利用本地私有数据，且容易遗忘先前知识。轻量级模型虽能适应资源受限环境，但缺乏与大模型的协同。

Method: 提出协作框架，轻量级本地模型作为动态桥梁，结合小模型持续微调和一对一蒸馏技术，实现异构本地知识的个性化融合。

Result: 实验表明，该框架在异构小模型环境下表现优异。

Conclusion: 该框架有效解决了基础模型在联邦持续学习中的挑战，提升了性能。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [79] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种名为FGCRN的新型开放集故障诊断模型，结合多尺度深度卷积、双向门控循环单元和时间注意力机制，通过无监督学习构建细粒度特征表示，有效识别未知故障。


<details>
  <summary>Details</summary>
Motivation: 在多模式过程中，同一健康状态的样本常呈现多簇分布，难以构建紧凑准确的决策边界，因此需要一种能够准确分类已知状态并识别未知故障的系统。

Method: FGCRN结合多尺度深度卷积、双向门控循环单元和时间注意力机制捕捉判别特征，设计基于距离的损失函数增强类内紧凑性，通过无监督学习构建细粒度特征表示，并利用极值理论建模样本特征与细粒度表示的距离。

Result: 实验表明，该方法在性能上表现优越。

Conclusion: FGCRN能够有效解决多模式过程中的故障诊断问题，并成功识别未知故障。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [80] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种分层联邦微调框架，用于动态车联网场景下的资源感知和移动弹性学习，结合LoRA和UCB-DUAL算法，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 车联网系统中，由于客户端移动性、资源异构性和间歇性连接，实现高效低延迟的多任务适应具有挑战性。

Method: 采用分层联邦微调框架，结合LoRA和能量感知的秩适应机制，提出UCB-DUAL算法解决多臂老虎机问题。

Result: 实验表明，该方法在延迟降低24%的同时，平均准确率提升2.5%，优于所有基线。

Conclusion: 该框架在动态车联网场景中实现了资源感知和移动弹性学习，显著提升了性能。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [81] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS是一种新的Meta-NAS框架，通过图建模和混合搜索策略提升任务感知架构的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统NAS局限于单一任务，Meta-NAS虽能跨任务迁移但存在泛化差、搜索空间有限或计算成本高的问题。

Method: GraB-NAS将架构建模为图，结合贝叶斯优化的全局搜索和梯度上升的局部探索，生成新架构。

Result: 实验表明GraB-NAS优于现有Meta-NAS方法，泛化能力和搜索效率更高。

Conclusion: GraB-NAS通过混合搜索策略成功解决了Meta-NAS的局限性，实现了更强的性能和适应性。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [82] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP是一种基于扩展图理论的N:M结构化剪枝方法，有效减少大型语言模型的规模和计算需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模扩大，部署时的计算和内存挑战日益严重，亟需更高效的模型变体。

Method: 利用扩展图理论指导N:M结构化剪枝设计，确保剪枝后网络的信息流和功能完整性。

Result: 实验表明EGGS-PTP显著加速模型并节省内存，同时在多种LLMs上保持更高准确性。

Conclusion: EGGS-PTP是一种高效的结构化剪枝方法，优于现有技术。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [83] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果能力的GDCC框架，用于强化学习中的高效环境探索，通过测量因果能力并识别关键点作为子目标，显著提升了探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示，动作与状态转移之间的因果关系能提升智能体的探索效率，但在复杂场景中测量因果性具有挑战性。

Method: 提出GDCC框架，首先定义状态空间中的因果能力测量方法，随后基于蒙特卡洛方法识别关键点，并将其作为子目标指导探索。

Result: 实验表明，高因果能力状态与预期子目标一致，GDCC在多目标任务中显著优于基线方法。

Conclusion: GDCC通过因果能力测量和子目标发现，有效提升了强化学习中的探索效率。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [84] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG是一个多模态因果推理框架，通过结合变量语义和历史数据提升时间序列建模的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽略变量语义信息，而TimeMKG旨在利用这些信息提升建模的鲁棒性和可解释性。

Method: TimeMKG使用大语言模型解析变量语义，构建多变量知识图谱，并通过双模态编码器和跨模态注意力融合语义与统计信息。

Result: 实验表明，引入变量级知识显著提升了预测性能和泛化能力。

Conclusion: TimeMKG通过结合语义与统计信息，为时间序列建模提供了更优的解决方案。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [85] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 本文提出MiCo框架，用于边缘AI应用中的混合精度量化（MPQ）探索与部署，通过优化算法搜索最优量化方案，并支持从PyTorch到裸机C代码的直接部署。


<details>
  <summary>Details</summary>
Motivation: 现有MPQ算法在灵活性和效率上受限，且缺乏端到端框架，MiCo旨在解决这些问题。

Method: 采用新型优化算法搜索满足延迟约束的最优量化方案，并构建硬件感知延迟模型以加速探索。

Result: 框架实现了端到端加速，同时最小化精度损失。

Conclusion: MiCo为MPQ提供了一种灵活高效的解决方案，适用于边缘AI应用。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [86] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: CGAD是一种基于因果图的异常检测框架，用于公共基础设施系统中的可靠网络攻击检测，通过因果分析和图结构比较显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 针对关键基础设施（如水处理网络）日益复杂的网络攻击，传统方法因分布偏移和类别不平衡问题导致高误报率，需要更鲁棒的异常检测策略。

Method: CGAD采用两阶段监督框架：1）使用动态贝叶斯网络学习因果不变图结构；2）通过因果图比较检测异常。

Result: CGAD在非平稳和不平衡时间序列环境中表现出更高的适应性和准确性，F1和ROC-AUC分数显著优于基线方法。

Conclusion: CGAD通过揭示因果结构，不仅提高了检测精度，还重新定义了异常检测的鲁棒性，适用于复杂和延迟的异常检测。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [87] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型（LLM）处理临床笔记的方法，以提升概念瓶颈模型（CBM）在急性呼吸窘迫综合征（ARDS）识别中的性能。


<details>
  <summary>Details</summary>
Motivation: 利用公开临床数据集研究疾病异质性和个性化治疗时，数据不完整且缺乏关键标签，现有AI工具解释性不足。

Method: 通过LLM处理临床笔记生成额外概念，改进CBM模型。

Result: 性能提升10%，学习到更全面的概念，减少信息泄漏和虚假依赖。

Conclusion: 结合LLM的CBM方法能更准确地表征ARDS，提升模型解释性和性能。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [88] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: Gauss-Tin是一种结合回放策略和高斯混合模型的新方法，通过优化样本选择和指导生成，显著提升大语言模型的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在学习新知识时遗忘旧知识的灾难性遗忘问题。

Method: 结合回放策略与高斯混合模型，优化样本选择，并引入指导生成机制。

Result: 实验显示，Gauss-Tin在保留指标上比传统方法提升了6%。

Conclusion: Gauss-Tin是一种有效的混合模型策略，能增强大语言模型在动态学习环境中的鲁棒性和适应性。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [89] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H是一种快速训练范式，结合闭式权重计算与梯度优化少量合成输入、软标签和隐藏激活。


<details>
  <summary>Details</summary>
Motivation: 旨在通过减少可训练参数和优化数据/激活空间，提高训练速度和模型性能。

Method: 采用闭式权重计算和梯度优化原型，通过岭正则化伪逆解更新权重矩阵。

Result: 在MNIST和Fashion-MNIST上分别达到97.8%和89.3%的测试准确率，训练时间仅3.9-4.5秒。

Conclusion: 该方法在准确性、速度和模型大小之间取得了优越的平衡，优于ELM、随机特征岭和浅层MLP。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [90] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种基于GNN的统一、可解释框架，用于预测业务流程监控中的未来事件，解决了现有方法在局部建模、时间相关性和语义表达上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的PBPM模型在捕捉时间相关性和语义表达上不足，需要更先进的框架。

Method: 结合前缀GCN和全局GAT，引入时间衰减注意力机制和语义边特征，增强模型性能。

Result: 在五个基准测试中表现优异，无需针对数据集调整即可达到高准确率。

Conclusion: 该框架为PBPM提供了一种鲁棒、通用且可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [91] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，解释了大语言模型（LLM）在上下文学习（ICL）中如何通过向量算术解决事实召回任务，并证明了其强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLM在ICL中存在潜在任务向量，并利用Word2Vec-like向量算术解决任务，但缺乏理论解释。本文旨在填补这一空白。

Method: 基于层次化概念建模，提出优化理论，分析非线性残差变换器通过梯度下降和交叉熵损失进行事实召回任务的机制。

Result: 证明了0-1损失的收敛性，展示了强泛化能力，包括对概念重组和分布偏移的鲁棒性。

Conclusion: 理论框架揭示了变换器优于静态嵌入模型的优势，并通过实验验证了理论结果。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [92] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种数据高效的蒸馏框架（DED），通过优化推理蒸馏的帕累托前沿，实现了在少量精选数据下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法通过扩展语料库和多阶段训练提升了推理能力，但推理的扩展规律仍不明确且计算成本高。DED旨在解决这一问题。

Method: 结合强化学习的策略和多样化轨迹策略，通过选择最优教师模型、精选小规模语料库和多样化推理轨迹，优化推理蒸馏。

Result: 在数学推理（AIME 2024/2025, MATH-500）和代码生成（LiveCodeBench）任务中，仅用0.8k精选数据即达到最先进性能。

Conclusion: DED提供了一种高效且实用的方法，在提升推理能力的同时保持模型的通用性。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [93] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: SYNAPSE-G利用LLMs生成合成数据解决罕见事件分类的冷启动问题，通过半监督标签传播扩展数据集，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标记数据稀缺，尤其是罕见事件，阻碍了有效机器学习模型的训练。

Method: 提出SYNAPSE-G，利用LLMs生成合成数据作为种子，通过半监督标签传播扩展数据集，并由人工或LLM标注候选正例。

Result: 在SST2和MHS数据集上，SYNAPSE-G在寻找正标签方面优于基线方法。

Conclusion: SYNAPSE-G通过合成数据扩展和标签传播，有效解决了罕见事件分类问题。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [94] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 论文研究了如何确定数据集中是否存在异常，提出了一个基于数据集大小、污染率和算法常数的下限条件。


<details>
  <summary>Details</summary>
Motivation: 异常检测中确认异常存在的方法尚未充分研究，本文旨在填补这一空白。

Method: 通过超过三百万次统计测试，分析数据集大小、污染率与算法常数之间的关系。

Result: 发现了一个下限条件：N ≥ α_algo/ν²，用于确认异常存在。

Conclusion: 该条件揭示了异常稀有性对检测可行性的限制。

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [95] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: EGI（边缘通用智能）通过世界模型实现分布式智能体的感知、推理和自主行动，填补了无线边缘领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 探索世界模型在无线边缘计算中的应用，以提升智能体在动态环境中的自主决策能力。

Method: 分析世界模型的架构基础（如潜在表示学习、动态建模和基于想象的规划），并展示其在EGI场景中的应用。

Result: 世界模型在车辆网络、无人机网络、物联网等场景中优化了延迟、能耗和隐私问题，并与基础模型和数字孪生协同。

Conclusion: 世界模型是EGI的认知核心，但仍需解决安全性、训练效率和部署限制等挑战，为未来研究提供了方向。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [96] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 论文提出了四种策略（ReDP、CorDP、IC-DP、RouteDP），以探索大型语言模型（LLMs）在上下文辅助预测任务中的零样本能力，并展示了这些策略在不同模型上的优势。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的预测任务需要模型整合历史数据和上下文信息，而现有方法对LLMs的潜力挖掘不足。

Method: 提出了四种策略：ReDP（提高可解释性）、CorDP（优化现有预测）、IC-DP（嵌入历史示例）、RouteDP（资源优化路由）。

Result: 在CiK基准测试中，这些策略在不同规模和家族的LLMs上均优于直接提示方法。

Conclusion: 这些策略为基于LLM的上下文辅助预测提供了简单而有效的改进方向。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [97] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种新型的未训练循环神经网络（ResRMN），结合线性记忆库和非线性库，通过残差正交连接增强长期输入传播。实验证明其在时间序列和像素级分类任务中优于传统RC模型。


<details>
  <summary>Details</summary>
Motivation: 传统RC模型在长期输入传播方面存在局限性，ResRMN通过残差正交连接改进这一问题。

Method: 结合线性记忆库和非线性库，利用残差正交连接增强长期输入传播，并通过线性稳定性分析研究状态动态。

Result: 在时间序列和像素级1-D分类任务中表现优于传统RC模型。

Conclusion: ResRMN通过残差正交连接显著提升了长期输入传播能力，为RC模型提供了新的优化方向。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [98] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 论文提出了一种物理和几何感知的时空谱图神经算子（πG-Sp²GNO），用于高效求解偏微分方程（PDEs），尤其在复杂几何和有限标注数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程的高效求解是科学与工程中的核心挑战，尤其是在复杂几何和有限数据条件下。现有方法在几何感知和物理信息融合方面存在不足。

Method: 改进现有的Sp²GNO，引入几何感知能力，并结合物理信息设计了一种无模拟的求解算子。针对时间相关问题，提出了一种混合物理信息损失函数，结合高阶时间推进方案和随机投影方案。

Result: 在多个基准测试中，该方法在复杂几何和动态几何变化下均表现出色，优于现有物理信息神经算子算法。

Conclusion: πG-Sp²GNO通过几何感知和物理信息融合，显著提升了PDE求解的效率和准确性，适用于多种复杂场景。

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [99] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个基于Python的统计框架，用于分析蛋白质热稳定性数据，克服了现有热蛋白质组分析（TPP）方法的关键限制。


<details>
  <summary>Details</summary>
Motivation: 传统TPP方法假设熔解曲线为S形，并受限于经验零分布，导致只能检测约5%的数据。Thermal Tracks旨在解决这一问题，尤其适用于分析显著改变蛋白质热稳定性的扰动（如通路抑制、基因修饰或环境压力）。

Method: 采用高斯过程（GP）模型和平方指数核，灵活建模任何熔解曲线形状，并通过核先验生成无偏零分布。

Result: Thermal Tracks能够分析非传统熔解曲线的蛋白质（如相分离蛋白和膜蛋白），并检测传统方法可能遗漏的生物相关变化。

Conclusion: Thermal Tracks是一个免费、灵活的工具，适用于全蛋白质组热稳定性分析研究。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [100] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 本文提出了一种基于算子理论的框架，用于嵌入空间中的时间锚定，通过漂移映射和事件索引块的交错实现，最终通过仿射投影完成。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入空间中时间锚定的数学基础，提供严格的证明和理论支持。

Method: 使用漂移映射与事件索引块的交错，结合仿射投影，构建算子理论框架，并证明相关定理。

Result: 证明了可变块收缩引理、漂移-投影收敛定理，以及嵌套仿射锚定下的本体收敛性，并提出了内部手稿计算机（MC）的严格等价定理。

Conclusion: 该框架为嵌入空间中的时间锚定提供了理论基础，并验证了其在注意力层中的应用有效性。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [101] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出了一种动态连接掩码（DCM）机制，用于增强MLPs和KANs对噪声标签的鲁棒性，通过自适应掩码不重要连接减少梯度误差，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中噪声标签不可避免，现有研究主要关注鲁棒损失函数和样本选择，对模型架构正则化的探索较少。

Method: 提出DCM机制，动态评估连接的重要性并掩码不重要连接，可与其他噪声鲁棒方法结合。

Result: 在合成和真实数据集上优于现有方法，并首次验证KANs在噪声标签下的优越性。

Conclusion: DCM机制有效提升模型对噪声标签的鲁棒性，KANs在噪声场景下表现优于MLPs。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [102] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出GraphTreeGen（GTG），一种基于子树的生成框架，用于高效、准确地合成脑连接组，解决了现有模型在局部结构、节点属性依赖、边权重预测和计算效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 脑连接组的获取成本高且耗时，现有生成模型在局部结构、节点属性依赖、边权重预测和计算效率上存在不足，需要一种更高效的生成方法。

Method: GTG将连接组分解为熵引导的k跳子树，通过共享GCN编码，结合全局节点特征，使用双分支解码器联合预测边存在和权重。

Result: GTG在自监督任务中优于现有基线，在监督任务中表现竞争性，具有更高的结构保真度和更精确的权重预测，且内存需求更低。

Conclusion: GTG是一种高效的脑连接组生成框架，其模块化设计支持扩展到超分辨率和跨模态合成。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [103] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: GenCO框架通过生成建模与多实例奖励学习结合，优化电商广告创意组合选择，显著提升广告收入。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独评估创意元素，难以处理组合搜索空间大的问题。

Method: 两阶段架构：生成模型生成创意组合，强化学习优化；多实例学习模型分配奖励。

Result: 显著提升广告收入，并发布工业数据集。

Conclusion: GenCO框架有效解决创意组合选择问题，具有实际应用价值。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [104] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 提出了一种名为Hereditary Knowledge Transfer (HKT)的生物启发框架，通过模块化和选择性特征转移优化小型可部署模型。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络因增加容量和深度而导致的集成性和效率问题，同时提升小型模型的性能。

Method: 采用生物启发机制（如记忆RNA转移），通过Extraction、Transfer和Mixture (ETM)三阶段实现特征转移，并引入Genetic Attention (GA)机制整合继承和原生表示。

Result: 在多种视觉任务（如光流、图像分类、语义分割）中，HKT显著优于传统蒸馏方法，同时保持模型紧凑性。

Conclusion: HKT为资源受限环境提供了一种通用、可解释且可扩展的高性能神经网络部署方案。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [105] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 该研究开发了一种机器学习模型，通过动态捕捉生物标志物的变化率，显著提高了预测年龄的准确性，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 预测个体衰老轨迹是预防医学和生物信息学的核心挑战，现有模型难以捕捉衰老的动态纵向特征。

Method: 利用纵向队列数据（2019-2022），设计动态特征（生物标志物变化率），采用LightGBM模型进行预测。

Result: 模型在后续时间点的预测中表现优异（男性R²=0.515，女性R²=0.498），动态特征成为关键预测因子。

Conclusion: 动态健康轨迹比静态数据更能反映生物年龄，为临床动态跟踪和个性化干预提供了新工具。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [106] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: 本文探讨了如何将μTransfer技术应用于MoE架构，提出了μP方法，并验证了其在模型宽度和专家数量扩展中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管μTransfer和MoE分别在超参数调优和大模型架构中表现突出，但二者的结合尚未被研究。

Method: 推导了适用于MoE的μP方法，并进行了实验验证。

Result: μP方法在理论和实验上均表现出色，同时研究了专家数量和粒度对学习率的影响。

Conclusion: μP为MoE架构提供了理论支持，并展示了其在扩展性方面的潜力。

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [107] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为TriForecaster的新框架，用于解决多区域电力负荷预测（MRELF）问题，通过混合专家（MoE）和多任务学习（MTL）方法，显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 智能电网和电表的普及提供了更详细的负荷数据，但多区域负荷预测面临区域、上下文和时间变化的挑战。

Method: 提出TriForecaster框架，结合MoE和MTL，引入RegionMixer和CTSpecializer层，动态协调专家模型。

Result: 在四个真实数据集上，TriForecaster平均预测误差降低22.4%，并在中国东部17个城市成功部署。

Conclusion: TriForecaster在多区域电力负荷预测中表现出色，具有灵活性和广泛适用性。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [108] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出一种优化Matérn核时间高斯过程的方法，基于递归贝叶斯估计，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程是概率数值领域的重要模型，但传统优化方法（如边际似然最大化或哈密尔顿蒙特卡洛采样）在运行时和精度上存在不足。

Method: 将优化问题转化为自回归模型参数的递归贝叶斯估计过程。

Result: 在运行时和均方根误差上优于边际似然最大化和哈密尔顿蒙特卡洛采样。

Conclusion: 该方法为高斯过程超参数优化提供了更高效的解决方案。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [109] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 该研究利用机器学习分析跳远比赛中的生物力学特征，发现男运动员的支撑腿膝盖角度和女运动员的落地姿势及助跑技术是关键因素。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以明确分析生物力学特征与运动员表现的关系，现代数据科学方法为此提供了新途径。

Method: 使用分位数回归建模生物力学特征与跳远成绩的关系，结合SHAP、PDP和ICE图进行解释。

Result: 男运动员支撑腿膝盖角度大于169度、女运动员的落地姿势和助跑技术对顶尖表现至关重要。

Conclusion: 研究为分析运动表现中的关键特征提供了框架，特别关注顶尖赛事。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [110] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: 提出RankList，一种新颖的列表式偏好学习框架，扩展了RankNet，通过全局和局部约束提升排序一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有成对偏好学习框架（如RankNet）在全局排序一致性上的局限性。

Method: 引入列表级监督，结合概率框架建模局部和非局部排序约束，使用log-sum-exp近似提升训练效率，并扩展跳步比较以增强全局排序。

Result: 在多个基准数据集（如MSP-Podcast、IEMOCAP等）上，RankList在Kendall's Tau和排序准确率上优于基线方法，并在跨域任务中表现优异。

Conclusion: RankList为主观学习任务提供了一种统一且可扩展的偏好建模方法，具有广泛适用性。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [111] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: FedShard是一种联邦学习遗忘算法，旨在同时保证效率和性能公平性，解决了现有研究中未充分探索的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决联邦学习中遗忘算法的效率和性能公平性问题，填补现有研究的空白。

Method: 提出FedShard算法，自适应解决收敛、遗忘效率和公平性之间的权衡，并引入两种新指标量化公平性。

Result: FedShard在遗忘性能和效率上均表现出公平性，比从头训练快1.3-6.2倍，比现有最优方法快4.9倍。

Conclusion: FedShard有效解决了联邦遗忘中的公平性问题，显著提升了效率和性能。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [112] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 现代人工神经网络（ANN）在土壤属性预测任务中表现优于传统机器学习方法，尤其是TabPFN模型，成为新的默认选择。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证现代ANN在土壤属性预测中的适用性，挑战传统机器学习方法的主导地位。

Method: 通过31个数据集评估多种ANN架构（如TabM、RealMLP、FT-Transformer等）与传统方法（如随机森林、偏最小二乘回归）的性能。

Result: 现代ANN在多数任务中表现更优，TabPFN模型表现最稳健。

Conclusion: 推荐将现代ANN（尤其是TabPFN）作为土壤属性预测的新标准工具。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [113] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: PDM提出了一种无需外部存储的原型扩散模型，通过对比学习构建动态视觉原型，提高生成效率并保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算成本高，检索增强方法依赖外部存储和静态模型，缺乏适应性。

Method: PDM利用对比学习从干净图像特征构建动态视觉原型，指导去噪过程。

Result: PDM在减少计算和存储开销的同时保持了高质量生成。

Conclusion: PDM为扩散模型提供了一种高效且可扩展的替代方案。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [114] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出了一种通过噪声超网络替代奖励引导的测试时噪声优化的方法，以减少计算开销，同时保留测试时扩展的优势。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展虽然提升了模型性能，但计算时间大幅增加，限制了实际应用。作者希望保留其优势，同时避免推理开销。

Method: 提出噪声超网络，调制初始输入噪声，并通过理论框架学习奖励倾斜的分布，以优化生成器。

Result: 方法在显著降低计算成本的同时，恢复了大部分测试时优化的质量提升。

Conclusion: 噪声超网络是一种高效的方法，能够在减少计算开销的同时保持测试时扩展的性能优势。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [115] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 论文提出了一种动态混合专家（DyMoE）方法，用于解决图增量学习中的灾难性遗忘问题，通过定制正则化损失和稀疏MoE设计，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法在增量学习场景中会因灾难性遗忘而失效，且现有方法未考虑不同时间点获取的知识对新任务的不同贡献。

Method: 提出动态混合专家（DyMoE）方法，新增专家网络处理新数据块，并设计定制正则化损失以平衡新旧任务学习。采用稀疏MoE减少计算成本。

Result: 模型在类增量学习任务中相对基线模型提升了4.92%的准确率。

Conclusion: DyMoE方法有效解决了图增量学习中的灾难性遗忘问题，并通过稀疏设计提升了计算效率。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化（VFI）扩展到深度强化学习（DRL）的方法，通过重用先前任务的紧凑表格Q值作为可转移知识库，提升早期学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在DRL中扩展VFI面临状态-动作空间连续性、神经网络噪声近似和存储所有过去模型的不可行性等挑战，DQInit旨在解决这些问题。

Method: DQInit利用已知性机制软性整合转移值到未探索区域，并逐步转向代理学习估计，避免固定时间衰减的限制。

Result: 实验表明，DQInit在多个连续控制任务中显著提高了早期学习效率、稳定性和整体性能。

Conclusion: DQInit为DRL中的知识转移提供了新视角，仅依赖值估计而非策略或演示，结合了跳启动RL和策略蒸馏的优势。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [117] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 论文介绍了Othello AI Arena，一个评估AI系统在有限时间内适应新环境能力的基准框架，旨在填补现有AI评测在灵活性和泛化能力上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI评测主要关注固定环境下的性能优化，缺乏对系统在规则或结构变化时适应能力的评估，因此需要新的评测方法。

Method: 通过Othello AI Arena平台，要求参与者在60秒内分析新Othello棋盘配置并生成定制策略，以评估元学习能力。

Result: 初步测试和学生参与显示出不同的适应策略，如快速参数调整和模拟学习环境模型。

Conclusion: Othello AI Arena是一个独特的教育工具和研究基准，有助于培养和评估AI系统的快速智能适应能力。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [118] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型和多智能体协作的自动化多模态评估框架，解决了当前评估方法的高成本、标准不一致和主观偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动智能助手技术的快速发展，多模态AI助手成为日常用户交互的重要接口，但现有评估方法存在高人工成本、标准不一致和主观偏见等挑战。

Method: 采用基于Qwen3-8B模型的监督微调，构建三层智能体架构（交互评估、语义验证和体验决策智能体）。

Result: 在八大智能助手上实验，框架显著提升了评估匹配准确率，能有效预测用户满意度和识别生成缺陷。

Conclusion: 该框架为多模态AI助手的自动化评估提供了高效、一致的解决方案。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [119] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种名为EvoCurr的自进化框架，通过动态调整问题难度来提升大语言模型（LLM）在复杂决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多个领域表现出色，但在需要深度推理的复杂问题上性能下降，缺乏结构化中间指导导致效率低下或失败。

Method: EvoCurr框架中，一个专门的课程生成LLM根据求解LLM的学习进度动态生成难度递增的问题序列，优化学习轨迹。求解LLM通过生成Python决策树脚本来逐步提升复杂决策能力。

Result: 实验结果表明，相比直接求解方法，EvoCurr显著提高了任务成功率和解决方案效率。

Conclusion: LLM驱动的课程学习在提升复杂领域自动推理能力方面具有巨大潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [120] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 本文提出了一种方法，将SHAP值中的不确定性分解为偶然性、认知性和纠缠性成分，结合Dempster-Shafer证据理论和Dirichlet过程采样，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: SHAP值通常被视为点估计，忽略了预测模型和数据中的不确定性，这在高风险领域（如医疗分析）中尤为重要。

Method: 结合Dempster-Shafer证据理论和Dirichlet过程采样，分解SHAP值中的不确定性。

Result: 实验表明，SHAP值最高的特征不一定最稳定，认知性不确定性可通过更好的数据和模型开发技术减少。

Conclusion: 树模型（尤其是装袋法）能有效量化认知性不确定性，为高风险应用提供更可靠的决策支持。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [121] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO通过多专家互学机制和多样化提示，解决了RLVR中的奖励稀疏问题，显著提升了LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法在奖励稀疏时无法提供学习信号，尤其在复杂任务中表现不佳。

Method: 提出MEML-GRPO框架，利用多样化专家提示生成更广范围的响应，并通过专家间互学机制促进知识共享。

Result: 在多个推理基准测试中，MEML-GRPO平均性能提升4.89%（Qwen）和11.33%（Llama）。

Conclusion: MEML-GRPO有效克服了传统RLVR的核心限制，显著提升了模型性能。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [122] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文提出UDA框架，通过动态调整Elo评分系统减少评估中的偏好偏差，显著降低评委间分歧。


<details>
  <summary>Details</summary>
Motivation: LLM的成对评估存在偏好偏差，导致评委间排名不一致，需要一种无监督方法减少偏差。

Method: 提出UDA框架，通过动态调整Elo评分系统的K因子和胜率，最小化评委间的评分差异。

Result: UDA将评委间评分标准差降低63.4%，与人类判断相关性提升24.7%。

Conclusion: UDA有效减少评估偏差，提升低质量评委表现，构建更可靠的评估系统。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [123] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 论文提出PacifAIst基准测试，用于评估大型语言模型在目标冲突场景中的行为对齐，发现不同模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在关键社会功能中的自主性增强，AI安全的重点需从内容转向行为对齐。现有基准未能系统测试模型在目标冲突下的决策。

Method: 引入PacifAIst基准，包含700个场景，围绕生存优先（EP）分类测试模型行为。评估了8个主流LLMs。

Result: Gemini 2.5 Flash表现最佳（P-Score 90.31%），GPT-5最低（79.49%）。不同模型在子类别中表现差异显著。

Conclusion: 需标准化工具（如PacifAIst）以测量和缓解目标冲突风险，确保AI行为优先符合人类安全。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [124] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL是一种用于推理基于公共观察的知识更新的逻辑，其可满足性问题被证明是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中基于观察的知识更新逻辑，特别是在认知规划中的应用。

Method: 提出并分析公共观察逻辑（POL），利用克里普克模型的状态演化来匹配观察期望与实际观察。

Result: 证明了POL的可满足性问题是2EXPTIME完全的。

Conclusion: POL为多智能体系统中的知识更新提供了一种有效的逻辑框架，但其计算复杂度较高。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [125] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种结合文本、关卡和草图的多模态强化学习框架，通过对比学习和嵌入对齐提升人机协作内容生成的人类相似性。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL系统在人类中心行为上表现不足，限制了AI工具在实际设计工作流中的实用性。

Method: 提出VIPCGRL框架，结合文本、关卡和草图三种模态，通过四重对比学习和嵌入相似性奖励对齐策略。

Result: 实验表明VIPCGRL在人类相似性上优于基线，定量指标和人工评估均验证其有效性。

Conclusion: VIPCGRL通过多模态和嵌入对齐显著提升了人机协作内容生成的人类相似性和实用性。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [126] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种动态监督与调控机制，构建了多智能体系统（MAS）架构，通过引入Guard Agent验证和修正推理过程，显著提升了系统的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）智能代理在依赖多工具时面临上下文扩展和噪声输出的挑战，需要增强系统稳定性。

Method: 在AWorld框架中构建动态MAS，通过Execution Agent调用Guard Agent验证和修正推理过程。

Result: 在GAIA测试数据集上，动态调控机制显著提升了解决方案的有效性和稳定性，优于单智能体系统和标准工具增强系统，并在GAIA排行榜上名列第一。

Conclusion: 协作智能体角色在开发更可靠、可信的智能系统中具有重要实践价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [127] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和检索增强生成的多智能体框架，用于解决监管合规问答中的挑战。


<details>
  <summary>Details</summary>
Motivation: 监管合规问答需要精确、可验证的信息和领域专业知识，这对大型语言模型提出了挑战。

Method: 通过构建和维护无本体知识图谱，提取并处理监管文档中的三元组，嵌入到向量数据库中，结合检索增强生成进行问答。

Result: 系统在复杂监管查询中优于传统方法，确保事实正确性、可追溯性，并通过子图可视化增强理解。

Conclusion: 该框架为合规驱动和审计应用提供了坚实基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [128] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型（LLMs）在解决数学问题时的准确性，发现推理增强的OpenAI o1模型表现最佳，双代理配置显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数学教育中的准确性，以提供可靠反馈和改进AI驱动的教学实践。

Method: 构建具有挑战性的数学任务，分析LLMs的答案准确性和步骤错误，测试单代理和双代理配置。

Result: OpenAI o1模型表现最佳，双代理配置显著提升性能，程序性错误最常见。

Conclusion: 研究为提升LLMs性能和数学教育应用提供了实用策略。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [129] [Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation](https://arxiv.org/abs/2508.09460)
*Xujie Yuan,Shimin Di,Jielong Tang,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: MetaKGRAG提出了一种基于元认知的闭环知识图谱检索增强生成框架，解决了现有KG-RAG的认知盲区和路径依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有KG-RAG框架存在认知盲区，导致相关性漂移和不完整证据，传统自优化方法无法有效解决。

Method: 引入感知-评估-调整循环，实现路径感知的闭环优化。

Result: 在多个领域的数据集上，MetaKGRAG表现优于现有KG-RAG和自优化基线方法。

Conclusion: 路径感知优化在结构化知识检索中至关重要，MetaKGRAG展示了其优越性。

Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) significantly
enhances the reasoning capabilities of LargeLanguage Models by leveraging
structured knowledge. However, existing KG-RAG frameworks typically operate as
open-loop systems, suffering from cognitive blindness, an inability to
recognize their exploration deficiencies. This leads to relevance drift and
incomplete evidence, which existing self-refinement methods, designed for
unstructured text-based RAG, cannot effectively resolve due to the
path-dependent nature of graph exploration. To address this challenge, we
propose Metacognitive Knowledge Graph Retrieval Augmented Generation
(MetaKGRAG), a novel framework inspired by the human metacognition process,
which introduces a Perceive-Evaluate-Adjust cycle to enable path-aware,
closed-loop refinement. This cycle empowers the system to self-assess
exploration quality, identify deficiencies in coverage or relevance, and
perform trajectory-connected corrections from precise pivot points. Extensive
experiments across five datasets in the medical, legal, and commonsense
reasoning domains demonstrate that MetaKGRAG consistently outperforms strong
KG-RAG and self-refinement baselines. Our results validate the superiority of
our approach and highlight the critical need for path-aware refinement in
structured knowledge retrieval.

</details>


### [130] [Improving Dense Passage Retrieval with Multiple Positive Passages](https://arxiv.org/abs/2508.09534)
*Shuai Chang*

Main category: cs.IR

TL;DR: DPR通过双编码器架构在段落检索中优于BM25，但训练时每个问题仅配对一个正例段落。本文研究加入多个正例段落的效果，实验表明这能提升检索精度，且支持单GPU训练。


<details>
  <summary>Details</summary>
Motivation: 传统DPR训练中每个问题仅配对一个正例段落，未探讨多正例段落的效果。本文旨在填补这一空白。

Method: 在DPR训练中为每个问题引入多个正例段落，并测试其对检索精度的影响。

Result: 实验显示，多正例段落能显著提升检索精度，且支持更小的批量训练（单GPU可行）。

Conclusion: 多正例段落训练是DPR性能提升的有效策略，同时降低了硬件需求。

Abstract: By leveraging a dual encoder architecture, Dense Passage Retrieval (DPR) has
outperformed traditional sparse retrieval algorithms such as BM25 in terms of
passage retrieval accuracy. Recently proposed methods have further enhanced
DPR's performance. However, these models typically pair each question with only
one positive passage during training, and the effect of associating multiple
positive passages has not been examined. In this paper, we explore the
performance of DPR when additional positive passages are incorporated during
training. Experimental results show that equipping each question with multiple
positive passages consistently improves retrieval accuracy, even when using a
significantly smaller batch size, which enables training on a single GPU.

</details>


### [131] [TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking](https://arxiv.org/abs/2508.09539)
*Yongqi Fan,Xiaoyang Chen,Dezhi Ye,Jie Liu,Haijin Liang,Jin Ma,Ben He,Yingfei Sun,Tong Ruan*

Main category: cs.IR

TL;DR: TFRank是一种基于小型LLM的高效点式推理排序模型，通过整合CoT数据、细粒度评分监督和多任务训练提升性能，同时通过“思维模式切换”和点式格式约束实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型LLM的推理密集型排序模型计算成本高、延迟大，限制了实际应用。

Method: TFRank结合CoT数据、细粒度评分监督和多任务训练，并通过“思维模式切换”和点式格式约束实现高效推理。

Result: TFRank（1.7B）在BRIGHT基准上性能接近参数四倍的模型，在BEIR基准上表现优异。

Conclusion: TFRank在性能与效率间取得平衡，为实际系统集成高级推理提供了实用方案。

Abstract: Reasoning-intensive ranking models built on Large Language Models (LLMs) have
made notable progress, but existing approaches often rely on large-scale LLMs
and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational
cost and latency that limit real-world use. To address this, we propose
\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale
LLMs. To improve ranking performance, TFRank effectively integrates CoT data,
fine-grained score supervision, and multi-task training. Furthermore, it
achieves an efficient ``\textbf{T}hink-\textbf{F}ree" reasoning capability by
employing a ``think-mode switch'' and pointwise format constraints.
Specifically, this allows the model to leverage explicit reasoning during
training while delivering precise relevance scores for complex queries at
inference without generating any reasoning chains. Experiments show that TFRank
(e.g., 1.7B) achieves performance comparable to models with four times more
parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on
the BEIR benchmark. Further analysis shows that TFRank achieves an effective
balance between performance and efficiency, providing a practical solution for
integrating advanced reasoning into real-world systems. Our code and data are
released in the repository: https://github.com/JOHNNY-fans/TFRank.

</details>


### [132] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出了一种基于多任务学习框架的个性化产品搜索排序优化模型，结合表格与非表格数据，使用TinyBERT和新型采样技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理混合数据类型和个性化排序时效果有限，需要更高效的模型架构和数据整合方法。

Method: 整合表格与非表格数据，利用预训练TinyBERT生成语义嵌入，采用新型采样技术捕捉用户行为多样性，并提出基于点击率和语义相似性的可扩展相关性标注机制。

Result: 实验表明，多任务学习框架结合非表格数据和高级嵌入技术显著提升模型性能，消融研究验证了相关性标注和TinyBERT微调的重要性。

Conclusion: 该模型在个性化产品搜索排序中表现优异，证明了多任务学习和数据整合的有效性。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


### [133] [On Negative-aware Preference Optimization for Recommendation](https://arxiv.org/abs/2508.09653)
*Chenlu Ding,Daoxuan Liu,Jiancan Wu,Xingyu Hu,Junkang Wu,Haitao Wang,Yongkang Wang,Xingxing Wang,Xiang Wang*

Main category: cs.IR

TL;DR: NAPO是一种针对基于LLM的推荐系统的优化框架，通过负样本共享和动态奖励调整提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用负样本时存在计算开销大和样本信息利用不足的问题。

Method: NAPO采用批内负样本共享和动态奖励边际调整策略。

Result: 在三个公开数据集上，NAPO在推荐准确性和减少流行度偏差方面优于现有方法。

Conclusion: NAPO为LLM推荐系统提供了一种高效且性能优越的优化方案。

Abstract: Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

</details>


### [134] [Multimodal Fusion And Sparse Attention-based Alignment Model for Long Sequential Recommendation](https://arxiv.org/abs/2508.09664)
*Yongrui Fu,Jian Liu,Tao Li,Zonggang Wu,Shouke Qin,Hanmeng Liu*

Main category: cs.IR

TL;DR: MUFASA是一个多模态融合与稀疏注意力对齐模型，用于长序列推荐，通过多模态融合和分层注意力机制提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态项目序列和多粒度用户兴趣的有效利用问题，以缩小内容理解与推荐之间的差距。

Method: 模型包含多模态融合层（MFL）和稀疏注意力对齐层（SAL），分别处理多模态融合和长序列用户兴趣建模。

Result: 在真实基准测试中表现优于现有方法，并在在线A/B测试中验证了其有效性。

Conclusion: MUFASA通过多模态融合和分层注意力机制，显著提升了推荐系统的性能。

Abstract: Recent advances in multimodal recommendation enable richer item
understanding, while modeling users' multi-scale interests across temporal
horizons has attracted growing attention. However, effectively exploiting
multimodal item sequences and mining multi-grained user interests to
substantially bridge the gap between content comprehension and recommendation
remain challenging. To address these issues, we propose MUFASA, a MUltimodal
Fusion And Sparse Attention-based Alignment model for long sequential
recommendation. Our model comprises two core components. First, the Multimodal
Fusion Layer (MFL) leverages item titles as a cross-genre semantic anchor and
is trained with a joint objective of four tailored losses that promote: (i)
cross-genre semantic alignment, (ii) alignment to the collaborative space for
recommendation, (iii) preserving the similarity structure defined by titles and
preventing modality representation collapse, and (iv) distributional
regularization of the fusion space. This yields high-quality fused item
representations for further preference alignment. Second, the Sparse
Attention-guided Alignment Layer (SAL) scales to long user-behavior sequences
via a multi-granularity sparse attention mechanism, which incorporates windowed
attention, block-level attention, and selective attention, to capture user
interests hierarchically and across temporal horizons. SAL explicitly models
both the evolution of coherent interest blocks and fine-grained intra-block
variations, producing robust user and item representations. Extensive
experiments on real-world benchmarks show that MUFASA consistently surpasses
state-of-the-art baselines. Moreover, online A/B tests demonstrate significant
gains in production, confirming MUFASA's effectiveness in leveraging multimodal
cues and accurately capturing diverse user preferences.

</details>


### [135] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 论文提出了一种利用多模态大语言模型（MLLM）生成视频自然语言描述的方法，以提升视频推荐系统的语义理解能力，优于传统低层特征。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统依赖低层特征或用户定义元数据，无法捕捉深层语义（如意图、幽默等），影响个性化推荐效果。

Method: 通过MLLM生成视频的丰富自然语言描述，结合文本编码器，输入到多种推荐模型中。

Result: 在MicroLens-100K数据集上，该方法在五种模型中均优于传统特征。

Conclusion: MLLM可作为动态知识提取器，提升视频推荐系统的意图感知能力。

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>
