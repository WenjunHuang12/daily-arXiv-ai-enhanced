<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.DS](#cs.DS) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Risk-Aware GPU-Assisted Cardinality Estimation for Cost-Based Query Optimizers](https://arxiv.org/abs/2512.19750)
*Ilsun Chang*

Main category: cs.DB

TL;DR: 提出GACE架构，利用GPU辅助选择性测量来增强传统优化器的基数估计，解决统计信息过时等问题，在保持低开销的同时提升计划稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实工作负载经常违反静态统计信息的假设，导致决策不稳定和计划翻转率增加。传统基数估计面临统计信息过时、数据倾斜、连接相关性、绑定变量隐藏分布和采样偏差等问题。

Method: 提出GACE（GPU辅助基数估计）混合架构，包含风险门控检测估计不确定性，GPU测量引擎进行高速探测并显式计算测量成本。只在风险区间选择性调用GPU测量，而非完全替换优化器。

Result: 在稳定区域保持低开销，在问题场景中改善计划稳定性并减少尾延迟（P99）。通过硬件加速测量量化开销和盈亏平衡点。

Conclusion: GACE架构通过选择性GPU测量有效增强传统优化器，平衡了测量开销和估计准确性，为成本优化器提供了实用的增强方案。

Abstract: Cardinality estimation is a cornerstone of cost-based optimizers (CBOs), yet real-world workloads often violate the assumptions behind static statistics, degrading decision stability and increasing plan flip rates. We empirically characterize failures caused by stale statistics, skew, join correlations, hidden distributions in bind variables, and sampling bias, and quantify the overhead and break-even points of hardware-accelerated measurement.
  We propose GACE (GPU-Assisted Cardinality Estimation), a hybrid auxiliary architecture that augments rather than replaces the optimizer. GACE selectively invokes GPU-based measurement only in risky intervals via a Risky Gate that detects estimation uncertainty, and a GPU Measurement Engine that performs high-speed probing with explicit cost accounting for the measurement itself. This design preserves low overhead in stable regions while improving plan stability and reducing tail latency (P99) in problematic scenarios.

</details>


### [2] [Automated Training of Learned Database Components with Generative AI](https://arxiv.org/abs/2512.20271)
*Angjela Davitkova,Sebastian Michel*

Main category: cs.DB

TL;DR: 使用生成模型（如GPT）为学习型数据库组件合成训练数据，解决高质量训练数据获取难题


<details>
  <summary>Details</summary>
Motivation: 深度学习在数据库优化中应用广泛，但获取高质量训练数据仍然是一个重大挑战，需要探索新的数据获取方法

Method: 使用生成模型（如GPT）合成训练数据，进行可行性研究，评估其生成真实查询分布和执行计划的能力

Result: 初步结果表明生成模型能有效增强训练数据集，提高学习型数据库技术的适应性

Conclusion: 生成模型在合成数据库训练数据方面具有潜力，但面临数据可扩展性和标注等挑战，需要进一步研究解决方案

Abstract: The use of deep learning for database optimization has gained significant traction, offering improvements in indexing, cardinality estimation, and query optimization. However, acquiring high-quality training data remains a significant challenge. This paper explores the possibility of using generative models, such as GPT, to synthesize training data for learned database components. We present an initial feasibility study investigating their ability to produce realistic query distributions and execution plans for database workloads. Additionally, we discuss key challenges, such as data scalability and labeling, along with potential solutions. The initial results suggest that generative models can effectively augment training datasets, improving the adaptability of learned database techniques.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [3] [Visual Event Detection over AI-Edge LEO Satellites with AoI Awareness](https://arxiv.org/abs/2512.19764)
*Chathuranga M. Wijerathna Basnayaka,Haeyoung Lee,Pandelis Kourtessis,John M. Senior,Vishalya P. Sooriarachchi,Dushantha Nalin K. Jayakody,Marko Beko,Seokjoo Shin*

Main category: cs.IT

TL;DR: 提出基于深度联合信源信道编码的AI原生LEO卫星下行方案，用于目标导向的视觉推理，相比传统分离编码具有更高推理精度和更低信息老化


<details>
  <summary>Details</summary>
Motivation: 非地面网络（特别是LEO卫星）对任务关键应用（如灾害救援）至关重要，但有限的链路预算、严重路径损耗和衰落限制了可靠下行传输。AI原生通信使LEO卫星能作为智能边缘节点进行机上学习和任务导向推理。

Method: 提出基于深度联合信源信道编码的下行方案，仅传输语义相关特征而非原始图像数据。引入误分类信息年龄（AoMI）指标和基于阈值的AoI分析，评估信息新鲜度和视觉事件检测性能。

Result: 仿真结果表明，所提DJSCC方案相比传统SSCC基线具有更高推理精度、更低平均AoMI和更好的阈值符合率，支持6G及以后的AI原生LEO卫星网络语义通信。

Conclusion: DJSCC方案能有效支持AI原生LEO卫星网络中的语义通信，为目标导向视觉推理提供可靠下行传输，满足任务关键应用对时效性和准确性的要求。

Abstract: Non terrestrial networks (NTNs), particularly low Earth orbit (LEO) satellite systems, play a vital role in supporting future mission critical applications such as disaster relief. Recent advances in artificial intelligence (AI)-native communications enable LEO satellites to act as intelligent edge nodes capable of on board learning and task oriented inference. However, the limited link budget, coupled with severe path loss and fading, significantly constrains reliable downlink transmission. This paper proposes a deep joint source-channel coding (DJSCC)-based downlink scheme for AI-native LEO networks, optimized for goal-oriented visual inference. In the DJSCC approach, only semantically meaningful features are extracted and transmitted, whereas conventional separate source-channel coding (SSCC) transmits the original image data. To evaluate information freshness and visual event detection performance, this work introduces the age of misclassified information (AoMI) metric and a threshold based AoI analysis that measures the proportion of users meeting application specific timeliness requirements. Simulation results show that the proposed DJSCC scheme provides higher inference accuracy, lower average AoMI, and greater threshold compliance than the conventional SSCC baseline, enabling semantic communication in AI native LEO satellite networks for 6G and beyond.

</details>


### [4] [Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning](https://arxiv.org/abs/2512.19777)
*Antonio Tarizzo,Mohammad Kazemi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出一种学习型数字OTA框架，通过结合无源随机访问码本、向量量化和AMP-DA-Net解码器，在保持相同上行开销的同时，将可靠数字OTA操作扩展到低SNR区域超过10dB。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习中，重复上传模型更新导致通信成为主要瓶颈。现有数字OTA方案在低信噪比(SNR)条件下性能有限，需要提高恢复精度、收敛行为和鲁棒性。

Method: 集成无源随机访问(URA)码本与向量量化，使用AMP-DA-Net（基于近似消息传递的解码器）进行端到端训练，结合参数服务器本地训练统计信息，支持包括修剪均值和多数规则在内的对称函数类。

Result: 在高度异构设备数据集和不同活跃设备数量下，所提设计将可靠数字OTA操作扩展到低SNR区域超过10dB，在整个SNR范围内匹配或改进性能，在消息损坏和非线性聚合下仍保持有效。

Conclusion: 学习型数字OTA框架显著提升了联邦边缘学习中的通信效率，端到端学习设计在数字OTA通信中展现出更广泛的潜力，特别是在低SNR条件下的鲁棒性。

Abstract: Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.

</details>


### [5] [Generative Bayesian Spectrum Cartography: Unified Reconstruction and Active Sensing via Diffusion Models](https://arxiv.org/abs/2512.20108)
*Yuntong Gu,Xiangming meng,Zhiyuan Lin,Sheng Wu,Linling Kuang*

Main category: cs.IT

TL;DR: 提出统一的扩散贝叶斯框架，联合解决频谱地图重建和主动感知问题，通过条件生成过程和不确定性感知主动采样策略，显著提升重建精度和采样效率。


<details>
  <summary>Details</summary>
Motivation: 高保真频谱地图对频谱管理和无线态势感知至关重要，但由于观测稀疏且不规则，这是一个具有挑战性的不适定逆问题。现有方法通常将重建与感知解耦，缺乏信息采样的原则性机制。

Method: 提出统一的扩散贝叶斯框架，将重建任务建模为由学习扩散先验驱动的条件生成过程。推导了反向扩散过程的可处理闭式后验转移核，支持线性和非线性量化测量。基于扩散模型的内在概率特性，开发了不确定性感知主动采样策略。

Result: 广泛实验表明，该框架在重建精度、采样效率和低比特量化鲁棒性方面显著优于最先进的插值、稀疏性和深度学习基线方法。

Conclusion: 提出的统一扩散贝叶斯框架成功解决了频谱地图重建和主动感知的联合问题，通过条件生成和不确定性感知采样实现了高精度重建和高效频谱利用。

Abstract: High-fidelity spectrum cartography is pivotal for spectrum management and wireless situational awareness, yet it remains a challenging ill-posed inverse problem due to the sparsity and irregularity of observations. Furthermore, existing approaches often decouple reconstruction from sensing, lacking a principled mechanism for informative sampling. To address these limitations, this paper proposes a unified diffusion-based Bayesian framework that jointly addresses spectrum reconstruction and active sensing. We formulate the reconstruction task as a conditional generation process driven by a learned diffusion prior. Specifically, we derive tractable, closed-form posterior transition kernels for the reverse diffusion process, which enforce consistency with both linear Gaussian and non-linear quantized measurements. Leveraging the intrinsic probabilistic nature of diffusion models, we further develop an uncertainty-aware active sampling strategy. This strategy quantifies reconstruction uncertainty to adaptively guide sensing agents toward the most informative locations, thereby maximizing spectral efficiency. Extensive experiments demonstrate that the proposed framework significantly outperforms state-of-the-art interpolation, sparsity-based, and deep learning baselines in terms of reconstruction accuracy, sampling efficiency, and robustness to low-bit quantization.

</details>


### [6] [RIS-Empowered OTFS Modulation With Faster-than-Nyquist Signaling in High-Mobility Wireless Communications](https://arxiv.org/abs/2512.20332)
*Chaorong Zhang,Benjamin K. Ng,Hui Xu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: 提出RIS赋能的OTFS调制与FTN信令方案，在高移动性无线通信中提升可靠性和频谱效率


<details>
  <summary>Details</summary>
Motivation: 高移动性无线通信系统面临严重多普勒扩展和多径延迟，传统调制方案可靠性和频谱效率下降。OTFS调制在延迟-多普勒域具有强鲁棒性，FTN信令通过符号压缩提升频谱效率，RIS通过被动波束成形改善链路质量

Method: 提出RIS-OTFS-FTN方案：建立统一的延迟-多普勒域输入输出关系模型，联合考虑RIS被动波束成形、FTN引起的符号间干扰和DD域信道特性；设计实用的RIS相位调整策略，采用量化相位选择最大化有效信道增益

Result: 在标准化扩展车载A信道模型下进行大量蒙特卡洛仿真验证理论结果，获得频谱效率、PAPR、输入回退和误码性能之间的权衡关系。方案在可靠性和频谱效率方面均表现出显著性能增益

Conclusion: RIS-OTFS-FTN方案为未来高移动性和频谱受限的无线系统提供了可行的解决方案，在可靠性和频谱效率方面均有显著提升

Abstract: High-mobility wireless communication systems suffer from severe Doppler spread and multi-path delay, which degrade the reliability and spectral efficiency of conventional modulation schemes. Orthogonal time frequency space (OTFS) modulation offers strong robustness in such environments by representing symbols in the delay-Doppler (DD) domain, while faster-than-Nyquist (FTN) signaling can further enhance spectral efficiency through intentional symbol packing. Meanwhile, reconfigurable intelligent surfaces (RIS) provide a promising means to improve link quality via passive beamforming. Motivated by these advantages, we propose a novel RIS-empowered OTFS modulation with FTN signaling (RIS-OTFS-FTN) scheme. First, we establish a unified DD-domain input-output relationship that jointly accounts for RIS passive beamforming, FTN-induced inter-symbol interference, and DD-domain channel characteristics. Based on this model, we provide comprehensive analytical performance for the frame error rate, spectral efficiency, and peak-to-average power ratio (PAPR), etc. Furthermore, a practical RIS phase adjustment strategy with quantized phase selection is designed to maximize the effective channel gain. Extensive Monte Carlo simulations under a standardized extended vehicular A (EVA) channel model validate the theoretical results and provide key insights into the trade-offs among spectral efficiency, PAPR, input back-off (IBO), and error performance, with some interesting insights.The proposed RIS-OTFS-FTN scheme demonstrates notable performance gains in both reliability and spectral efficiency, offering a viable solution for future high-mobility and spectrum-constrained wireless systems.

</details>


### [7] [Viterbi State Selection for Discrete Pinching Antenna Systems](https://arxiv.org/abs/2512.20389)
*Victoria E. Galanopoulou,Thrassos K. Oikonomou,Odysseas G. Karagiannidis,Sotiris A. Tegos,Panagiotis D. Diamantoulakis*

Main category: cs.IT

TL;DR: 提出基于Viterbi状态选择算法解决波导馈电夹持天线阵列的天线子集选择问题，将计算复杂度从指数级降低到多项式级，同时保持与穷举搜索相同的性能。


<details>
  <summary>Details</summary>
Motivation: 夹持天线能够通过可重构辐射结构动态控制电磁波传播，但天线子集选择是一个具有指数复杂度的组合优化问题。在波导馈电夹持天线阵列服务地面用户的场景下，需要高效解决天线子集选择问题。

Method: 提出Viterbi状态选择算法，利用组合接收信号的相位结构。将网格状态定义为累积复增益相位的量化表示，使用基于Viterbi的幸存规则在不同阶段剪枝被支配的天线子集。

Result: 数值结果表明，该方法实现了与穷举搜索相同的天线选择和速率性能，同时将计算复杂度从可用天线数量的指数级降低到多项式级。

Conclusion: Viterbi状态选择算法有效解决了夹持天线阵列的天线子集选择问题，在保持最优性能的同时显著降低了计算复杂度，为实际系统部署提供了可行方案。

Abstract: Pinching antennas enable dynamic control of electromagnetic wave propagation through reconfigurable radiating structures, but selecting an optimal subset of antennas remains a combinatorial problem with exponential complexity. This letter considers antenna subset selection for a waveguide-fed pinching antenna array serving ground users under a time-division access scheme. The achievable rate depends on the coherent superposition of the effective complex channel gains and is therefore highly sensitive to the relative phase alignment of the activated antennas. To address the prohibitive complexity of exhaustive search, we propose a Viterbi state selection (VSS) algorithm that exploits the phase structure of the combined received signal. The trellis state is defined by a quantized representation of the phase of the accumulated complex gain, and a Viterbi-based survivor rule is used to prune dominated antenna subsets across stages. Numerical results demonstrate that the proposed method achieves the same antenna selection and rate as exhaustive search, while reducing the computational complexity from exponential to polynomial in the number of available antennas.

</details>


### [8] [Information-theoretic signatures of causality in Bayesian networks and hypergraphs](https://arxiv.org/abs/2512.20552)
*Sung En Chiang,Zhaolu Liu,Robert L. Peach,Mauricio Barahona*

Main category: cs.IT

TL;DR: 该论文建立了部分信息分解(PID)组件与因果结构之间的理论对应关系，证明PID可以用于推断贝叶斯网络和超图中的因果结构，无需全局搜索。


<details>
  <summary>Details</summary>
Motivation: 目前高阶信息论度量与因果结构之间的数学联系尚未建立。需要工具来捕捉超越成对关系的高阶相互作用，以分析多元系统中的因果关系。

Method: 使用部分信息分解(PID)框架，分析PID组件（冗余、独特、协同信息）与因果结构的关系。在贝叶斯网络和超图中建立理论对应关系，证明独特信息表征直接因果邻居，协同信息识别碰撞关系。

Result: 1. 在贝叶斯网络中，独特信息精确表征直接因果邻居，协同信息识别碰撞关系
2. 在超图中，PID特征能区分父节点、子节点、共头节点和共尾节点
3. 发现了多尾超边特有的高阶碰撞效应
4. 提出了系统表征因果结构的程序

Conclusion: PID为推断成对和高阶因果结构提供了严谨、模型无关的基础，引入了基于局部信息论的因果发现新视角，消除了对图空间全局搜索的需求。

Abstract: Analyzing causality in multivariate systems involves establishing how information is generated, distributed and combined, and thus requires tools that capture interactions beyond pairwise relations. Higher-order information theory provides such tools. In particular, Partial Information Decomposition (PID) allows the decomposition of the information that a set of sources provides about a target into redundant, unique, and synergistic components. Yet the mathematical connection between such higher-order information-theoretic measures and causal structure remains undeveloped. Here we establish the first theoretical correspondence between PID components and causal structure in both Bayesian networks and hypergraphs. We first show that in Bayesian networks unique information precisely characterizes direct causal neighbors, while synergy identifies collider relationships. This establishes a localist causal discovery paradigm in which the structure surrounding each variable can be recovered from its immediate informational footprint, eliminating the need for global search over graph space. Extending these results to higher-order systems, we prove that PID signatures in Bayesian hypergraphs differentiate parents, children, co-heads, and co-tails, revealing a higher-order collider effect unique to multi-tail hyperedges. We also present procedures by which our results can be used to characterize systematically the causal structure of Bayesian networks and hypergraphs. Our results position PID as a rigorous, model-agnostic foundation for inferring both pairwise and higher-order causal structure, and introduce a fundamentally local information-theoretic viewpoint on causal discovery.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [9] [DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion](https://arxiv.org/abs/2512.20059)
*Ziyang Fan,Li Tao,Yi Wang,Jingwei Qu,Ying Wang,Fei Jiang*

Main category: cs.MM

TL;DR: 提出基于超图卷积网络的双流多特征融合模型(DS-HGCN)，通过建模学生参与度的社会传染机制来准确预测学生参与状态。


<details>
  <summary>Details</summary>
Motivation: 学生参与度是影响学业成功的关键因素，但现有方法大多关注单维特征分析和个体因素，忽略了学生间的社会传染效应。

Method: 构建超图结构编码学生间参与度传染，通过多频信号捕捉情感和行为差异与共性，引入超图注意力机制动态加权每个学生的影响。

Result: 在公共基准数据集上的实验表明，该方法取得了优越性能，显著优于现有最先进方法。

Conclusion: DS-HGCN通过建模多维特征及其在学生间的传播机制，能够准确预测学生参与状态，为优化教学策略和个性化干预提供了有效工具。

Abstract: Student engagement is a critical factor influencing academic success and learning outcomes. Accurately predicting student engagement is essential for optimizing teaching strategies and providing personalized interventions. However, most approaches focus on single-dimensional feature analysis and assessing engagement based on individual student factors. In this work, we propose a dual-stream multi-feature fusion model based on hypergraph convolutional networks (DS-HGCN), incorporating social contagion of student engagement. DS-HGCN enables accurate prediction of student engagement states by modeling multi-dimensional features and their propagation mechanisms between students. The framework constructs a hypergraph structure to encode engagement contagion among students and captures the emotional and behavioral differences and commonalities by multi-frequency signals. Furthermore, we introduce a hypergraph attention mechanism to dynamically weigh the influence of each student, accounting for individual differences in the propagation process. Extensive experiments on public benchmark datasets demonstrate that our proposed method achieves superior performance and significantly outperforms existing state-of-the-art approaches.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [10] [Maximizing the Egalitarian Welfare in Friends and Enemies Games](https://arxiv.org/abs/2512.20261)
*Edith Elkind,Michele Flammini,Giovanna Varricchio*

Main category: cs.GT

TL;DR: 研究朋友与敌人游戏中最大化平等福利的复杂性，针对FA和EA两种场景，分别给出近似算法和硬度结果。


<details>
  <summary>Details</summary>
Motivation: 朋友与敌人游戏是享乐游戏的一个子类，每个代理将其他代理分为朋友和敌人。研究最大化平等福利的复杂性，特别是FA（朋友优先）和EA（敌人厌恶）两种经典场景。

Method: 针对EA场景，证明了O(n^{1-ε})的近似难度，并提供了(n-1)近似算法。针对FA场景，证明了NP难度，并设计了近似算法：当每个代理至少有2个朋友时达到2-Θ(1/n)近似比；当有代理最多只有1个朋友时近似比降为n/2。还研究了随机化和对称关系的变体。

Result: EA场景：存在O(n^{1-ε})近似难度下界，提供(n-1)多项式时间近似。FA场景：NP难，提供2-Θ(1/n)近似算法（当每个代理至少有2个朋友时），否则n/2近似。在随机化和对称关系变体中恢复2-Θ(1/n)近似比。识别出两种场景下可多项式时间计算最优解的特殊情况。

Conclusion: 朋友与敌人游戏中最大化平等福利在一般情况下是计算困难的，但可以通过近似算法处理，且在某些特殊情况下可多项式时间求解。FA场景的近似性能优于EA场景。

Abstract: We consider the complexity of maximizing egalitarian welfare in Friends and Enemies Games -- a subclass of hedonic games in which every agent partitions other agents into friends and enemies. We investigate two classic scenarios proposed in the literature, namely, Friends Appreciation ($\mathsf{FA}$) and Enemies Aversion ($\mathsf{EA}$): in the former, each agent primarily cares about the number of friends in her coalition, breaking ties based on the number of enemies, while in the latter, the opposite is true. For $\mathsf{EA}$, we show that our objective is hard to approximate within $O(n^{1-ε})$, for any fixed $ε>0$, and provide a polynomial-time $(n-1)$-approximation. For $\mathsf{FA}$, we obtain an NP-hardness result and a polynomial-time approximation algorithm. Our algorithm achieves a ratio of $2-Θ(\frac{1}{n})$ when every agent has at least two friends; however, if some agent has at most one friend, its approximation ratio deteriorates to $n/2$. We recover the $2-Θ(\frac{1}{n})$ approximation ratio for two important variants: when randomization is allowed and when the friendship relationship is symmetric. Additionally, for both $\mathsf{EA}$ and $\mathsf{FA}$ we identify special cases where the optimal egalitarian partition can be computed in polynomial time.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [11] [Towards Analysing Invoices and Receipts with Amazon Textract](https://arxiv.org/abs/2512.19958)
*Sneha Oommen,Gabby Sanchez,Cassandra T. Britto,Di Wang,Jordan Chiou,Maria Spichkova*

Main category: cs.IR

TL;DR: 评估AWS Textract在收据数据提取中的表现，分析其在不同格式和条件下的收据处理能力，发现总额检测准确但存在图像质量和布局相关的问题，并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 评估AWS Textract在真实场景中处理收据数据提取的能力，了解其在不同格式和条件下的表现，为实际应用提供参考依据。

Method: 使用包含多种格式和条件的收据数据集，对AWS Textract功能进行系统性分析，评估其在不同场景下的表现。

Result: Textract在收据总额检测方面表现一致准确，但存在图像质量和布局影响导致的典型问题和异常，这些问题会影响整体提取效果。

Conclusion: AWS Textract在收据数据提取中具有实用价值，但需要针对图像质量和布局问题进行优化，提出的缓解策略有助于改善实际应用效果。

Abstract: This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.

</details>


### [12] [IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation](https://arxiv.org/abs/2512.19983)
*Ziyuan Guo,Jie Guo,Zhenghao Chen,Bin Song,Fei Richard Yu*

Main category: cs.IR

TL;DR: IGDMRec提出了一种基于扩散模型的图去噪方法，通过整合用户行为信息来净化多模态推荐系统中的语义物品图，从而提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结构的MRS通过构建语义物品图取得了SOTA性能，但这类图存在噪声问题：1) 多模态信息本身有噪声；2) 物品语义与用户-物品共现关系之间的不对齐，导致虚假链接和次优推荐。

Method: 提出IGDMRec方法：1) 行为条件图扩散(BGD)模块，将交互数据作为条件信息指导语义物品图的去噪；2) 条件去噪网络(CD-Net)，以可管理的复杂度实现去噪过程；3) 对比表示增强方案，利用去噪图和原始图增强物品表示。

Result: 在四个真实世界数据集上的实验表明IGDMRec优于竞争基线，鲁棒性分析验证了其去噪能力，消融研究确认了关键组件的有效性。

Conclusion: IGDMRec通过扩散模型和分类器自由指导，成功整合用户行为信息来去噪语义物品图，提高了多模态推荐系统的性能。

Abstract: Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.

</details>


### [13] [LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews](https://arxiv.org/abs/2512.20022)
*Kian Godhwani,David Benrimoh*

Main category: cs.IR

TL;DR: OLIVER系统使用LLM加速文献筛选，评估发现单模型性能受综述特性影响且校准差，而actor-critic框架能提升分类质量和置信度可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文献筛选研究主要关注早期模型、标准化Cochrane综述、单模型设置和准确率指标，缺乏对泛化性、配置效果和校准的全面评估。

Method: 开发OLIVER开源流水线，评估多个当代LLM在两个非Cochrane系统综述中的表现，使用准确率、AUC和校准指标，并测试结合两个轻量模型的actor-critic筛选框架。

Result: 单模型性能差异大：小综述中敏感性高但假阳性多、校准差；大综述中特异性高但召回率低。单模型配置校准普遍弱。Actor-critic框架显著提升区分度和校准，降低校准误差。

Conclusion: LLM可能加速文献筛选，但单模型性能受综述特性和提示设计影响且校准有限。Actor-critic框架能提升分类质量和置信度可靠性，同时保持计算效率，实现低成本大规模筛选。

Abstract: Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.
  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.
  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.
  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.

</details>


### [14] [VSA:Visual-Structural Alignment for UI-to-Code](https://arxiv.org/abs/2512.20034)
*Xian Wu,Ming Zhang,Zhiyu Fang,Fei Li,Bin Wang,Yong Jiang,Hao Zhou*

Main category: cs.IR

TL;DR: VSA是一个多阶段视觉-结构对齐范式，用于从视觉设计生成结构化前端代码，相比现有方法能产生更模块化、可维护的组件化代码。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型的设计到代码转换方法主要生成非结构化、扁平的代码库，缺乏与React或Angular等组件化库的兼容性，导致代码低内聚、高耦合，难以长期维护。

Method: VSA采用多阶段方法：1) 使用空间感知transformer将视觉输入重构为层次树表示；2) 集成算法模式匹配层识别重复UI模式并封装为模块化模板；3) 通过模式驱动合成引擎处理模板，确保LLM生成类型安全、支持属性传递的生产级组件。

Result: 实验结果表明，该框架在代码模块化和架构一致性方面相比最先进基准有显著改进，有效弥合了原始像素与可扩展软件工程之间的差距。

Conclusion: VSA通过视觉-结构对齐范式，能够从视觉设计生成组织良好的前端资产，解决了现有设计到代码转换方法在代码结构、可维护性和组件化兼容性方面的不足。

Abstract: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.

</details>


### [15] [Collaborative Group-Aware Hashing for Fast Recommender Systems](https://arxiv.org/abs/2512.20172)
*Yan Zhang,Li Deng,Lixin Duan,Ivor W. Tsang,Guowu Yang*

Main category: cs.IR

TL;DR: 提出CGAH方法，通过整合用户和物品的固有群组信息来缓解稀疏性问题，提升哈希推荐系统的准确率


<details>
  <summary>Details</summary>
Motivation: 大规模数据库的快速在线推荐至关重要，但稀疏场景下准确推荐具有挑战性。现有哈希推荐方法在稀疏设置下准确率低，主要因为比特表示能力有限且忽略了用户和物品间的固有关系

Method: 提出协同群组感知哈希(CGAH)方法：1) 将用户和物品的潜在向量分类到不同群组以提取固有群组亲和性；2) 将偏好建模为群组亲和性与哈希码相似度的内积；3) 通过固有群组信息学习哈希码

Result: 在三个公共数据集上的大量实验表明，CGAH和CGAH-CF在不同稀疏设置下优于最先进的离散协同过滤方法和离散内容感知推荐方法

Conclusion: 通过整合固有群组信息，CGAH方法在稀疏交互数据下能获得比其他离散方法更有效的哈希码，显著提升了哈希推荐系统的性能

Abstract: The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.

</details>


### [16] [Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register](https://arxiv.org/abs/2512.20458)
*Shuting Wang,Qiaolin Xia,Hao Wang,Yu Lu,Bobsimons,Zhicheng Dou*

Main category: cs.IR

TL;DR: Laser是一个用于稳定和扩展智能体搜索的通用框架，通过符号化动作协议将智能体行为组织为规划、任务解决和反思三个空间，使用结构化推理过程提升多跳查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型和大推理模型的智能体搜索系统主要依赖非结构化的自然语言推理，导致推理轨迹不稳定、上下文溢出以及在复杂多跳查询上性能下降。

Method: Laser定义了一个符号化动作协议，将智能体行为组织为规划、任务解决和反思三个空间，每个动作都有明确的语义和确定性执行格式。同时维护紧凑的上下文寄存器，只存储推理过程的关键状态。

Result: 在Qwen2.5/3系列模型上的实验表明，Laser在具有挑战性的多跳QA数据集上始终优于现有的智能体搜索基线方法，无论是在仅提示还是微调设置下。

Conclusion: Laser为稳健、可扩展的智能体搜索提供了一个原则性和有效的基础框架，通过结构化推理和紧凑上下文管理解决了现有方法的局限性。

Abstract: Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.

</details>


### [17] [Making Large Language Models Efficient Dense Retrievers](https://arxiv.org/abs/2512.20612)
*Yibin Lei,Shwai He,Ang Li,Andrew Yates*

Main category: cs.IR

TL;DR: EffiR框架通过分析LLM在检索任务中的层冗余，发现MLP层可大幅剪枝而注意力层关键，采用粗到细策略压缩MLP并保持检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示直接微调LLM进行稠密检索效果好但计算效率低，虽然生成任务中LLM存在层冗余，但检索任务需要将整个序列编码为固定表示而非迭代生成，这种冗余特性尚不明确。

Method: 提出EffiR框架：1）分析LLM检索器层冗余，发现MLP层高度可剪枝而注意力层关键；2）采用粗到细策略大规模压缩MLP：先粗粒度深度减少，再细粒度宽度减少；3）结合检索特定微调。

Result: 在多种BEIR数据集和LLM骨干网络上，EffiR显著减少模型大小和推理成本，同时保持全尺寸模型的性能。

Conclusion: 检索任务中LLM的层冗余模式与生成任务不同，MLP层可大幅压缩而注意力层关键，EffiR框架能有效开发高效检索器。

Abstract: Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [18] [Approximation and parameterized algorithms for covering disjointness-compliable set families](https://arxiv.org/abs/2512.20180)
*Zeev Nutov,Anael Vaknin*

Main category: cs.DS

TL;DR: 该论文研究了一类特殊的集合族覆盖问题，提出了针对非对称但满足"不相交可补性"集合族的近似算法框架，获得了O(αlogτ)的近似比，并应用于多个经典问题。


<details>
  <summary>Details</summary>
Motivation: 许多著名算法问题（如k-MST、G-P2P、Group Steiner等）的集合族满足"不相交可补性"但不对称，而经典的Goemans-Williamson算法只适用于对称的proper集合族。需要为这类更广泛的非对称情况设计近似算法。

Method: 提出一个通用框架：对于任何满足不相交可补性的集合族覆盖问题，如果τ=1时已有α近似算法，则可以构造O(αlogτ)近似算法。其中τ是集合族中极小集合的数量。还研究了参数化复杂度，对proper集合族给出了O*(3^τ)时间的FPT算法。

Result: 1. 首次为G-P2P问题提供了确定性多项式时间O(logn)近似算法；2. 为多根版本的Covering Steiner问题提供了O(log⁴n)近似比；3. 证明了proper集合族覆盖问题是FPT的，可在O*(3^τ)时间内求解；4. 对非对称情况给出了时间O*(3^τ)内α到α+1的近似比。

Conclusion: 该工作为一大类满足不相交可补性但不对称的集合族覆盖问题建立了统一的近似算法框架，解决了多个长期存在的开放问题，并揭示了这类问题的参数化复杂度特性。

Abstract: A set-family ${\cal F}$ is disjointness-compliable if $A' \subseteq A \in {\cal F}$ implies $A' \in {\cal F}$ or $A \setminus A' \in {\cal F}$; if ${\cal F}$ is also symmetric then ${\cal F}$ is proper. A classic result of Goemans and Williamson [SODA 92:307-316] states that the problem of covering a proper set-family by a min-cost edge set admits approximation ratio $2$, by a classic primal-dual algorithm. However, there are several famous algorithmic problems whose set-family ${\cal F}$ is disjointness-compliable but not symmetric -- among them $k$-Minimum Spanning Tree ($k$-MST), Generalized Point-to-Point Connection (G-P2P), Group Steiner, Covering Steiner, multiroot versions of these problems, and others. We will show that any such problem admits approximation ratio $O(α\log τ)$, where $τ$ is the number of inclusion-minimal sets in the family ${\cal F}$ that models the problem and $α$ is the best known approximation ratio for the case when $τ=1$. This immediately implies several results, among them the following two. (i) The first deterministic polynomial time $O(\log n)$-approximation algorithm for the G-P2P problem. Here the $τ=1$ case is the $k$-MST problem. (ii) Approximation ratio $O(\log^4 n)$ for the multiroot version of the Covering Steiner problem, where each root has its own set of groups. Here the $τ=1$ case is the Covering Steiner problem.
  We also discuss the parameterized complexity of covering a disjointness-compliable family ${\cal F}$, when parametrized by $τ$. We will show that if ${\cal F}$ is proper then the problem is fixed parameter tractable and can be solved in time $O^*(3^τ)$. For the non-symmetric case we will show that the problem admits approximation ratio between $α$ and $α+1$ in time $O^*(3^τ)$, which is essentially the best possible.

</details>


### [19] [On the near-tightness of $χ\leq 2r$: a general $σ$-ary construction and a binary case via LFSRs](https://arxiv.org/abs/2512.20598)
*Vinicius T. V. Date,Leandro M. Zatesko*

Main category: cs.DS

TL;DR: 该论文研究压缩字符串索引中的重复性度量χ与BWT运行数r之间的关系，特别关注χ≤2r界限的紧致性，提出了两种渐近紧致的情况：一般σ值构造和二进制字母表的de Bruijn序列。


<details>
  <summary>Details</summary>
Motivation: 虽然Navarro等人证明了χ≤2r，但实证研究表明这个界限较松（实际χ约在1.13r到1.33r之间）。为了理解理论与实证之间的差距，需要研究这个界限的渐近紧致性。

Method: 提出两种构造来证明χ≤2r界限的渐近紧致性：1) 针对任意σ值的一般构造；2) 针对二进制字母表的de Bruijn序列，这些序列通过原始多项式在𝔽₂上的线性反馈移位寄存器构造。特别分析了哪些de Bruijn序列能达到文献中的循环BWT运行最小模式。

Result: 证明了χ≤2r界限在渐近意义上是紧致的，提供了两种达到该界限的构造。同时发现de Bruijn序列在σ≥3时无法缩小理论与实证之间的差距。

Conclusion: 该研究深化了对χ与r之间关系的理解，证明了χ≤2r界限的渐近紧致性，并揭示了de Bruijn序列在缩小理论与实证差距方面的局限性，为未来研究提供了理论基础。

Abstract: In the field of compressed string indexes, recent work has introduced suffixient sets and their corresponding repetitiveness measure $χ$. In particular, researchers have explored its relationship to other repetitiveness measures, notably $r$, the number of runs in the Burrows--Wheeler Transform (BWT) of a string. Navarro et al. (2025) proved that $χ\leq 2r$, although empirical results by Cenzato et al. (2024) suggest that this bound is loose, with real data bounding $χ$ by around $1.13r$ to $1.33r$ when the size of the alphabet is $σ= 4$. To better understand this gap, we present two cases for the asymptotic tightness of the $χ\leq 2r$ bound: a general construction for arbitrary $σ$ values, and a binary alphabet case, consisting of de Bruijn sequences constructed by linear-feedback shift registers (LFSRs) from primitive polynomials over $\mathbb{F}_2$. The second is a novel characterization of which de Bruijn sequences achieve the literature run-minimal pattern for the cyclic BWT. Moreover, we show that de Bruijn sequences fail to close the gap for $σ\geq 3$.

</details>
