<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Impartial Selection with Predictions](https://arxiv.org/abs/2510.19002)
*Javier Cembrano,Felix Fischer,Max Klimm*

Main category: cs.GT

TL;DR: 本文研究了基于相互提名的代理人选择问题，提出了结合预测信息的公正机制，在保证公正性的同时显著提升了选择性能。


<details>
  <summary>Details</summary>
Motivation: 解决在相互提名系统中代理人可能为了自身利益而歪曲对他人资格评价的问题，同时探索如何利用预测信息来提升公正机制的性能。

Method: 设计了一种结合预测集的公正机制，分别针对一般情况（选择k个代理人）和特殊情况（选择单个代理人）提出了不同的机制设计。

Result: 对于一般情况，机制达到1-O(1/k)的一致性和1-1/e-O(1/k)的鲁棒性；对于特殊情况，实现了1一致性和1/2鲁棒性。

Conclusion: 通过引入预测信息，可以在几乎不牺牲鲁棒性的前提下实现渐进最优的一致性，显著改进了现有公正机制的性能界限。

Abstract: We study the selection of agents based on mutual nominations, a theoretical
problem with many applications from committee selection to AI alignment. As
agents both select and are selected, they may be incentivized to misrepresent
their true opinion about the eligibility of others to influence their own
chances of selection. Impartial mechanisms circumvent this issue by
guaranteeing that the selection of an agent is independent of the nominations
cast by that agent. Previous research has established strong bounds on the
performance of impartial mechanisms, measured by their ability to approximate
the number of nominations for the most highly nominated agents. We study to
what extent the performance of impartial mechanisms can be improved if they are
given a prediction of a set of agents receiving a maximum number of
nominations. Specifically, we provide bounds on the consistency and robustness
of such mechanisms, where consistency measures the performance of the
mechanisms when the prediction is accurate and robustness its performance when
the prediction is inaccurate. For the general setting where up to $k$ agents
are to be selected and agents nominate any number of other agents, we give a
mechanism with consistency $1-O\big(\frac{1}{k}\big)$ and robustness
$1-\frac{1}{e}-O\big(\frac{1}{k}\big)$. For the special case of selecting a
single agent based on a single nomination per agent, we prove that
$1$-consistency can be achieved while guaranteeing $\frac{1}{2}$-robustness. A
close comparison with previous results shows that (asymptotically) optimal
consistency can be achieved with little to no sacrifice in terms of robustness.

</details>


### [2] [Desirable Effort Fairness and Optimality Trade-offs in Strategic Learning](https://arxiv.org/abs/2510.19098)
*Valia Efthymiou,Ekaterina Fedorova,Chara Podimata*

Main category: cs.GT

TL;DR: 该论文提出了一个统一的主从代理交互模型，研究在战略学习环境中决策者如何在最大化目标（如准确率）与限制代理激励差异之间权衡，考虑了特征因果依赖、异质操纵成本和同伴学习三个额外因素。


<details>
  <summary>Details</summary>
Motivation: 现实决策系统的目标不仅限于产生好的预测，还需要考虑诱导特定激励的外部效应，即某些特征变化对决策者更有利，同时需要公平地激励异质代理进行期望的特征改变。

Method: 提出了一个统一的主从代理交互模型，包含三个关键组件：(1)特征间的因果依赖关系；(2)代理间的异质操纵成本；(3)同伴学习机制。提供了在特定期望公平容忍度约束下决策者最优性损失的理论保证。

Result: 通过理论分析为多类公平度量提供了关于决策者在特定期望公平容忍度约束下的最优性损失的理论保证，并在真实数据集上通过实验展示了最大化准确率与期望努力公平性之间的明确权衡。

Conclusion: 该研究为战略学习环境中的公平激励设计提供了理论框架和实证证据，揭示了在追求准确率与确保公平激励之间的固有权衡关系。

Abstract: Strategic learning studies how decision rules interact with agents who may
strategically change their inputs/features to achieve better outcomes. In
standard settings, models assume that the decision-maker's sole scope is to
learn a classifier that maximizes an objective (e.g., accuracy) assuming that
agents best respond. However, real decision-making systems' goals do not align
exclusively with producing good predictions. They may consider the external
effects of inducing certain incentives, which translates to the change of
certain features being more desirable for the decision maker. Further, the
principal may also need to incentivize desirable feature changes fairly across
heterogeneous agents. How much does this constrained optimization (i.e.,
maximize the objective, but restrict agents' incentive disparity) cost the
principal? We propose a unified model of principal-agent interaction that
captures this trade-off under three additional components: (1) causal
dependencies between features, such that changes in one feature affect others;
(2) heterogeneous manipulation costs between agents; and (3) peer learning,
through which agents infer the principal's rule. We provide theoretical
guarantees on the principal's optimality loss constrained to a particular
desirability fairness tolerance for multiple broad classes of fairness
measures. Finally, through experiments on real datasets, we show the explicit
tradeoff between maximizing accuracy and fairness in desirability effort.

</details>


### [3] [Autobidding Arena: unified evaluation of the classical and RL-based autobidding algorithms](https://arxiv.org/abs/2510.19357)
*Andrey Pudovikov,Alexandra Khirianova,Ekaterina Solodneva,Aleksandr Katrutsa,Egor Samosvat,Yuriy Dorn*

Main category: cs.GT

TL;DR: 提出了一个标准化、透明的自动竞价算法评估协议，比较了控制类、强化学习类、最优公式类等不同类别的自动竞价算法，并提供了多维度评估结果。


<details>
  <summary>Details</summary>
Motivation: 广告拍卖对电商公司收入至关重要，需要可扩展的自动竞价算法。目前缺乏公平、可复现的自动竞价算法评估方法。

Method: 使用行业最新开源竞价环境，准确模拟竞价过程，比较不同类别最高效的自动竞价算法，采用多维度评估指标。

Result: 展示了不同自动竞价算法最有前景的应用场景，揭示了它们的意外缺点，并根据多个指标进行了评估。

Conclusion: 该比较结果帮助从业者从不同角度评估候选自动竞价算法，并根据公司目标选择高效算法。

Abstract: Advertisement auctions play a crucial role in revenue generation for
e-commerce companies. To make the bidding procedure scalable to thousands of
auctions, the automatic bidding (autobidding) algorithms are actively developed
in the industry. Therefore, the fair and reproducible evaluation of autobidding
algorithms is an important problem. We present a standardized and transparent
evaluation protocol for comparing classical and reinforcement learning (RL)
autobidding algorithms. We consider the most efficient autobidding algorithms
from different classes, e.g., ones based on the controllers, RL, optimal
formulas, etc., and benchmark them in the bidding environment. We utilize the
most recent open-source environment developed in the industry, which accurately
emulates the bidding process. Our work demonstrates the most promising use
cases for the considered autobidding algorithms, highlights their surprising
drawbacks, and evaluates them according to multiple metrics. We select the
evaluation metrics that illustrate the performance of the autobidding
algorithms, the corresponding costs, and track the budget pacing. Such a choice
of metrics makes our results applicable to the broad range of platforms where
autobidding is effective. The presented comparison results help practitioners
to evaluate the candidate autobidding algorithms from different perspectives
and select ones that are efficient according to their companies' targets.

</details>


### [4] [Comparing Uniform Price and Discriminatory Multi-Unit Auctions through Regret Minimization](https://arxiv.org/abs/2510.19591)
*Marius Potfer,Vianney Perchet*

Main category: cs.GT

TL;DR: 比较统一价格拍卖和歧视性拍卖在多单位重复拍卖中的学习难度，发现两者在最坏情况遗憾下具有相似的标度，但统一价格拍卖在某些情境下可能实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 研究在多单位重复拍卖中，投标人面对随机对手时的学习策略，比较两种主要拍卖格式（统一价格和歧视性拍卖）的学习难度差异。

Method: 通过理论分析比较两种拍卖格式在完全信息和强盗反馈下的遗憾标度，并分析超越最坏情况遗憾的结构性差异。

Result: 在最坏情况下，两种拍卖格式的遗憾标度相似（完全信息：$\tilde{\Theta} ( \sqrt{T} )$，强盗反馈：$\tilde{\Theta} ( T^{2/3} )$），但统一价格拍卖在某些情境下可实现更快的$\tilde{\Theta} ( \sqrt{T} )$遗憾标度。

Conclusion: 虽然两种拍卖格式在最坏情况下的学习难度相似，但统一价格拍卖具有结构性优势，在特定情境下可实现更快的收敛速度，这一差异在对称单位需求参与者情境下仍然存在。

Abstract: Repeated multi-unit auctions, where a seller allocates multiple identical
items over many rounds, are common mechanisms in electricity markets and
treasury auctions. We compare the two predominant formats: uniform-price and
discriminatory auctions, focusing on the perspective of a single bidder
learning to bid against stochastic adversaries. We characterize the learning
difficulty in each format, showing that the regret scales similarly for both
auction formats under both full-information and bandit feedback, as
$\tilde{\Theta} ( \sqrt{T} )$ and $\tilde{\Theta} ( T^{2/3} )$, respectively.
However, analysis beyond worst-case regret reveals structural differences:
uniform-price auctions may admit faster learning rates, with regret scaling as
$\tilde{\Theta} ( \sqrt{T} )$ in settings where discriminatory auctions remain
at $\tilde{\Theta} ( T^{2/3} )$. Finally, we provide a specific analysis for
auctions in which the other participants are symmetric and have unit-demand,
and show that in these instances, a similar regret rate separation appears.

</details>


### [5] [On Minimal Achievable Quotas in Multiwinner Voting](https://arxiv.org/abs/2510.19620)
*Patrick Becker,Fabian Frank*

Main category: cs.GT

TL;DR: 该论文提出了基于实例相关配额的比例性概念，突破了传统固定配额的限制，证明了常见投票规则与最优值的距离界限，并研究了计算最优α值的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统比例性公理（JR和EJR）使用固定配额（如Hare或Droop），但Droop配额是最小保证存在的配额。作者希望超越固定配额范式，引入实例相关的配额概念。

Method: 引入基于实例相关配额的比例性概念，分析常见投票规则与最优值的距离，证明计算最优α值的NP完全性，提出整数线性规划（ILP）方法来计算满足α-JR的委员会。

Result: 所有常见投票规则与最优值存在加性距离界限$rac{k^2}{(k+1)^2}$；确定满足α-JR的最优α值是NP完全的；在选民区间（VI）和候选人区间（CI）领域获得积极结果。

Conclusion: 实例相关配额提供了更灵活的比例性概念，虽然计算最优值具有挑战性，但通过ILP方法和特定领域分析可以取得进展。

Abstract: Justified representation (JR) and extended justified representation (EJR) are
well-established proportionality axioms in approval-based multiwinner voting.
Both axioms are always satisfiable, but they rely on a fixed quota (typically
Hare or Droop), with the Droop quota being the smallest one that guarantees
existence across all instances. With this observation in mind, we take a first
step beyond the fixed-quota paradigm and introduce proportionality notions
where the quota is instance-dependent. We demonstrate that all commonly studied
voting rules can have an additive distance to the optimum of
$\frac{k^2}{(k+1)^2}$. Moreover, we look into the computational aspects of our
instance-dependent quota and prove that determining the optimal value of
$\alpha$ for a given approval profile satisfying $\alpha$-JR is NP-complete. To
address this, we introduce an integer linear programming (ILP) formulation for
computing committees that satisfy $\alpha$-JR, and we provide positive results
in the voter interval (VI) and candidate interval (CI) domains.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [6] [FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains](https://arxiv.org/abs/2510.19025)
*Hamed Jelodar,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.DB

TL;DR: FlexiDataGen是一个基于大型语言模型的动态语义数据集生成框架，旨在解决敏感领域中数据稀缺、获取成本高和隐私限制等数据集挑战。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健、生物医学研究和网络安全等领域，数据获取成本高、标注数据有限以及关键事件的稀有性和敏感性严重阻碍了准确且可泛化的机器学习模型的发展。

Method: FlexiDataGen集成了四个核心组件：(1)句法语义分析，(2)检索增强生成，(3)动态元素注入，(4)带语义验证的迭代释义，确保生成高质量、领域相关的数据。

Result: 实验结果表明，FlexiDataGen有效缓解了数据短缺和标注瓶颈，实现了可扩展且准确的机器学习模型开发。

Conclusion: FlexiDataGen框架为敏感领域提供了一种有效的动态语义数据集生成解决方案，能够自主合成丰富、语义连贯且语言多样的数据集。

Abstract: Dataset availability and quality remain critical challenges in machine
learning, especially in domains where data are scarce, expensive to acquire, or
constrained by privacy regulations. Fields such as healthcare, biomedical
research, and cybersecurity frequently encounter high data acquisition costs,
limited access to annotated data, and the rarity or sensitivity of key events.
These issues-collectively referred to as the dataset challenge-hinder the
development of accurate and generalizable machine learning models in such
high-stakes domains. To address this, we introduce FlexiDataGen, an adaptive
large language model (LLM) framework designed for dynamic semantic dataset
generation in sensitive domains. FlexiDataGen autonomously synthesizes rich,
semantically coherent, and linguistically diverse datasets tailored to
specialized fields. The framework integrates four core components: (1)
syntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic
element injection, and (4) iterative paraphrasing with semantic validation.
Together, these components ensure the generation of high-quality,
domain-relevant data. Experimental results show that FlexiDataGen effectively
alleviates data shortages and annotation bottlenecks, enabling scalable and
accurate machine learning model development.

</details>


### [7] [Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum](https://arxiv.org/abs/2510.19197)
*Nofar Carmeli,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 该论文研究了在最小/最大值排序下直接访问合取查询(CQ)答案的细粒度复杂度，并探讨了相关的计数、枚举、直接访问和谓词消除等任务。


<details>
  <summary>Details</summary>
Motivation: 研究合取查询在最小/最大值排序下的直接访问复杂度，以及相关任务的效率边界，旨在建立完整的复杂度分类。

Method: 使用细粒度复杂度分析工具，针对无自连接的合取查询，分析各种任务在最小/最大值排序下的计算复杂度。

Result: 为无自连接合取查询建立了完整的复杂度二分法，精确识别了能够在接近理想时间内（准线性预处理时间，常数或对数时间输出）解决的问题情况。

Conclusion: 该研究为合取查询在最小/最大值排序下的各种任务提供了完整的复杂度分类，明确了哪些情况可以实现高效计算。

Abstract: We investigate the fine-grained complexity of direct access to Conjunctive
Query (CQ) answers according to their position, ordered by the minimum (or
maximum) value between attributes. We further use the tools we develop to
explore a wealth of related tasks. We consider the task of ranked enumeration
under min/max orders, as well as tasks concerning CQs with predicates of the
form x <= min X , where X is a set of variables and x is a single variable:
counting, enumeration, direct access, and predicate elimination (i.e.,
transforming the pair of query and database to an equivalent pair without
min-predicates). For each task, we establish a complete dichotomy for
self-join-free CQs, precisely identifying the cases that are solvable in
near-ideal time, i.e., (quasi)linear preprocessing time followed by constant or
logarithmic time per output.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [From Unweighted to Weighted Dynamic Matching in Non-Bipartite Graphs: A Low-Loss Reduction](https://arxiv.org/abs/2510.19049)
*Aaron Bernstein,Jiale Chen*

Main category: cs.DS

TL;DR: 提出了首个低损失约简方法，将二分图上的完全动态(1-ε)-近似最大基数匹配算法转化为一般图上的完全动态(1-ε)-近似最大权重匹配算法，仅需poly(log n/ε)的更新时间开销。


<details>
  <summary>Details</summary>
Motivation: 解决一般图上完全动态最大权重匹配问题，填补二分图与一般图之间的性能差距，之前的方法在非二分图上存在1/2的近似损失。

Method: 使用新的原始-对偶框架，将一般图上的近似最大权重匹配计算约简为辅助二分图扩展上的近似诱导匹配查询序列。

Result: 显著缩小了加权与未加权问题之间的差距，实现了仅poly(log n/ε)更新时间开销的低损失约简。

Conclusion: 该工作为一般图上的完全动态最大权重匹配问题提供了有效的解决方案，并给出了首个关于近似部分动态匹配最坏情况更新时间的条件下限。

Abstract: We study the approximate maximum weight matching (MWM) problem in a fully
dynamic graph subject to edge insertions and deletions. We design
meta-algorithms that reduce the problem to the unweighted approximate maximum
cardinality matching (MCM) problem. Despite recent progress on bipartite graphs
-- Bernstein-Dudeja-Langley (STOC 2021) and
Bernstein-Chen-Dudeja-Langley-Sidford-Tu (SODA 2025) -- the only previous
meta-algorithm that applied to non-bipartite graphs suffered a $\frac{1}{2}$
approximation loss (Stubbs-Williams, ITCS 2017). We significantly close the
weighted-and-unweighted gap by showing the first low-loss reduction that
transforms any fully dynamic $(1-\varepsilon)$-approximate MCM algorithm on
bipartite graphs into a fully dynamic $(1-\varepsilon)$-approximate MWM
algorithm on general (not necessarily bipartite) graphs, with only a
$\mathrm{poly}(\log n/\varepsilon)$ overhead in the update time. Central to our
approach is a new primal-dual framework that reduces the computation of an
approximate MWM in general graphs to a sequence of approximate induced matching
queries on an auxiliary bipartite extension. In addition, we give the first
conditional lower bound on approximate partially dynamic matching with
worst-case update time.

</details>


### [9] [Succinct Dynamic Rank/Select: Bypassing the Tree-Structure Bottleneck](https://arxiv.org/abs/2510.19175)
*William Kuszmaul,Jingxun Liang,Renfei Zhou*

Main category: cs.DS

TL;DR: 构建了一个动态有序字典，支持插入/删除/排名/选择操作，在U=poly(n)时达到最优时间复杂度和近乎最优的空间消耗，解决了Pibiri和Venturini提出的关于o(n)位冗余是否可能的问题。


<details>
  <summary>Details</summary>
Motivation: 解决动态数据结构中空间冗余的问题，特别是突破所谓的树结构瓶颈，该瓶颈导致动态树结构需要Ω(n)位的冗余空间。

Method: 使用压缩表加权树堆作为主要技术构建块，这是一种动态平衡二叉搜索树，支持多对数时间操作，需要静态查找表，但能实现极低的空间冗余。

Result: 实现了O(1+log n/log log U)的最优摊销期望时间复杂度，空间消耗为log(U选n)+n/2^(log n)^Ω(1)+polylog U位，在U=poly(n)时冗余仅为O(log U)位。

Conclusion: 这是第一个绕过树结构瓶颈的动态解决方案，证明了o(n)位冗余是可能的，为动态数据结构设计提供了新的可能性。

Abstract: We show how to construct a dynamic ordered dictionary, supporting
insert/delete/rank/select on a set of $n$ elements from a universe of size $U$,
that achieves the optimal amortized expected time complexity of $O(1 + \log n /
\log \log U)$, while achieving a nearly optimal space consumption of $\log
\binom{U}{n} + n / 2^{(\log n)^{\Omega(1)}} + \text{polylog}\, U$ bits in the
regime where $U = \text{poly}(n)$. This resolves an open question by Pibiri and
Venturini as to whether a redundancy (a.k.a. space overhead) of $o(n)$ bits is
possible, and is the first dynamic solution to bypass the so-called
tree-structure bottleneck, in which the bits needed to encode some dynamic tree
structure are themselves enough to force a redundancy of
$\widetilde{\Omega}(n)$ bits. Our main technical building block is a dynamic
balanced binary search tree, which we call the compressed tabulation-weighted
treap, that itself achieves a surprising time/space tradeoff. The tree supports
$\text{polylog}\, n$-time operations and requires a static lookup table of size
$\text{poly}(n) + \text{polylog}\, U$ -- but, in exchange for these, the tree
is able to achieve a remarkable space guarantee. Its total space redundancy is
$O(\log U)$ bits. In fact, if the tree is given $n$ and $U$ for free, then the
redundancy further drops to $O(1)$ bits.

</details>


### [10] [Online Two-Stage Submodular Maximization](https://arxiv.org/abs/2510.19480)
*Iasonas Nikolaou,Miltiadis Stouras,Stratis Ioannidis,Evimaria Terzi*

Main category: cs.DS

TL;DR: 提出了在线两阶段子模最大化问题(O2SSM)，针对加权阈值势函数这一重要子类，设计了在一般拟阵约束下具有次线性遗憾的算法。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段子模最大化(2SSM)假设所有子模函数已知，但在线设置中这些函数是逐步揭示的，需要设计能够适应动态环境的算法。

Method: 设计在线算法处理逐步揭示的子模目标函数，针对加权阈值势函数这一子类，在一般拟阵约束下实现次线性遗憾。

Result: 算法在一般拟阵约束下达到(1-1/e)^2次线性遗憾，在秩为k的均匀拟阵下达到(1-1/e)(1-e^{-k}k^k/k!)遗憾，后者也改进了离线2SSM问题的现有最优界。

Conclusion: 提出的在线算法在理论和实验上都表现良好，为在线子模优化问题提供了有效的解决方案。

Abstract: Given a collection of monotone submodular functions, the goal of Two-Stage
Submodular Maximization (2SSM) [Balkanski et al., 2016] is to restrict the
ground set so an objective selected u.a.r. from the collection attains a high
maximal value, on average, when optimized over the restricted ground set. We
introduce the Online Two-Stage Submodular Maximization (O2SSM) problem, in
which the submodular objectives are revealed in an online fashion. We study
this problem for weighted threshold potential functions, a large and important
subclass of monotone submodular functions that includes influence maximization,
data summarization, and facility location, to name a few. We design an
algorithm that achieves sublinear $(1 - 1/e)^2$-regret under general matroid
constraints and $(1 - 1/e)(1-e^{-k}k^k/k!)$-regret in the case of uniform
matroids of rank $k$; the latter also yields a state-of-the-art bound for the
(offline) 2SSM problem. We empirically validate the performance of our online
algorithm with experiments on real datasets.

</details>


### [11] [Optimal Random Access and Conditional Lower Bounds for 2D Compressed Strings](https://arxiv.org/abs/2510.19750)
*Rajat De,Dominik Kempa*

Main category: cs.DS

TL;DR: 该论文提出了二维压缩索引的理论进展，包括首个支持二维语法压缩字符串随机访问的最优时间数据结构、二维语法压缩模式匹配的条件下界证明，以及将二维压缩索引复杂度与一维开放问题连接的证据。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多数据集（如图像、地图、邻接矩阵）本质上是二维且高度可压缩的。将一维技术直接应用于二维数据会导致次优结果，因为基本的结构重复在线性化过程中丢失，这促使开发原生二维压缩索引方案。

Method: 设计了支持二维语法压缩字符串随机访问的数据结构；证明了二维语法压缩模式匹配的条件下界；展示了多种二维基础查询与一维语法压缩字符串中秩和符号出现查询的硬度假设之间的联系。

Result: 实现了O(log n/log log n)查询时间和O(|G|log^{2+ε}n)空间的二维语法压缩随机访问；证明了在正交向量猜想下，二维语法压缩模式匹配无法在O(|G|^{2-ε}·|P|^{O(1)})时间内解决；连接了二维压缩索引复杂度与一维开放问题。

Conclusion: 这是二维压缩索引理论的重要进展，首次提供了原生二维压缩索引方案，揭示了二维与一维情况在计算复杂度上的本质差异，并为未来研究建立了理论基础。

Abstract: Compressed indexing is a powerful technique that enables efficient querying
over data stored in compressed form, significantly reducing memory usage and
often accelerating computation. While extensive progress has been made for
one-dimensional strings, many real-world datasets (such as images, maps, and
adjacency matrices) are inherently two-dimensional and highly compressible.
Unfortunately, naively applying 1D techniques to 2D data leads to suboptimal
results, as fundamental structural repetition is lost during linearization.
This motivates the development of native 2D compressed indexing schemes that
preserve both compression and query efficiency.
  We present three main contributions that advance the theory of compressed
indexing for 2D strings: (1) We design the first data structure that supports
optimal-time random access to a 2D string compressed by a 2D grammar.
Specifically, for a 2D string $T\in\Sigma^{r\times c}$ compressed by a 2D
grammar $G$ and any constant $\epsilon>0$, we achieve $O(\log n/\log \log n)$
query time and $O(|G|\log^{2+\epsilon}n)$ space, where $n=\max(r,c)$. (2) We
prove conditional lower bounds for pattern matching over 2D-grammar compressed
strings. Assuming the Orthogonal Vectors Conjecture, no algorithm can solve
this problem in time $O(|G|^{2-\epsilon}\cdot |P|^{O(1)})$ for any
$\epsilon>0$, demonstrating a separation from the 1D case, where optimal
solutions exist. (3) We show that several fundamental 2D queries, such as the
2D longest common extension, rectangle sum, and equality, cannot be supported
efficiently under hardness assumptions for rank and symbol occurrence queries
on 1D grammar-compressed strings. This is the first evidence connecting the
complexity of 2D compressed indexing to long-standing open problems in the 1D
setting.

</details>


### [12] [Strongly Polynomial Parallel Work-Depth Tradeoffs for Directed SSSP](https://arxiv.org/abs/2510.19780)
*Adam Karczmarz,Wojciech Nadara,Marek Sokołowski*

Main category: cs.DS

TL;DR: 本文提出了在非负权重有向图中并行计算单源最短路径(SSSP)的新强多项式工作-深度权衡算法，实现了近乎工作最优且深度亚线性的突破。


<details>
  <summary>Details</summary>
Motivation: 针对稠密图中SSSP并行计算的效率问题，寻求在强多项式时间内实现工作最优和深度亚线性的平衡，以改进最小成本流、分配问题等相关算法的性能。

Method: 开发了新的并行算法框架，在非负权重有向图中实现SSSP计算，达到$\tilde{O}(m+n^{2-\epsilon})$工作和$\tilde{O}(n^{1-\epsilon})$深度，其中$\epsilon>0$。

Result: 首次为稠密图提供了近乎工作最优且深度亚线性的强多项式SSSP算法，并成功应用于最小成本流、分配问题和最小平均环的动态算法改进。

Conclusion: 该工作-深度权衡算法为并行图算法设计提供了重要突破，显著提升了多个经典图问题的计算效率，特别是在Word RAM模型下处理大权重图时表现优异。

Abstract: In this paper, we show new strongly polynomial work-depth tradeoffs for
computing single-source shortest paths (SSSP) in non-negatively weighted
directed graphs in parallel. Most importantly, we prove that directed SSSP can
be solved within $\tilde{O}(m+n^{2-\epsilon})$ work and
$\tilde{O}(n^{1-\epsilon})$ depth for some positive $\epsilon>0$. In
particular, for dense graphs with non-negative real weights, we provide the
first nearly work-efficient strongly polynomial algorithm with sublinear depth.
  Our result immediately yields improved strongly polynomial parallel
algorithms for min-cost flow and the assignment problem. It also leads to the
first non-trivial strongly polynomial dynamic algorithm for minimum mean cycle.
Moreover, we develop efficient parallel algorithms in the Word RAM model for
several variants of SSSP in graphs with exponentially large edge weights.

</details>


### [13] [A Logic-based Algorithmic Meta-Theorem for Treedepth: Single Exponential FPT Time and Polynomial Space](https://arxiv.org/abs/2510.19793)
*Benjamin Bergougnoux,Vera Chekan,Giannos Stamoulis*

Main category: cs.DS

TL;DR: 该论文提出了一个新的逻辑系统NEO₂[FRec]+ACK，能够捕获在树深有界图上具有2^O(k)n^O(1)时间复杂度的NP难问题，并提供了相应的模型检查算法。


<details>
  <summary>Details</summary>
Motivation: 为了统一和扩展已知在树深有界图上具有高效算法的NP难问题（如独立集、哈密顿环等）的理论框架，需要开发一个能够捕获这类问题的逻辑系统。

Method: 引入邻域操作符逻辑NEO₂[FRec]+ACK，扩展了全存在MSO₂逻辑，增加了邻域查询、连通性和无环性验证、团检测等谓词，并设计了相应的模型检查算法。

Result: 为NEO₂[FRec]+ACK提供了2^O(k)n^O(1)时间和n^O(1)空间的模型检查算法，统一了多个已知结果。对于不含连通性和无环性约束的片段NEO₂[FRec]+k，空间复杂度可降至O(k log(n))。

Conclusion: 该逻辑系统能够捕获多个先前未知高效算法的问题，包括CNF-SAT和模计数问题，为树深有界图上的算法设计提供了统一的逻辑框架。

Abstract: For a graph $G$, the parameter treedepth measures the minimum depth among all
forests $F$, called elimination forests, such that $G$ is a subgraph of the
ancestor-descendant closure of $F$. We introduce a logic, called neighborhood
operator logic with acyclicity, connectivity and clique constraints
($\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{ACK}$ for short), that captures all
NP-hard problems$\unicode{x2013}$like Independent Set or Hamiltonian
Cycle$\unicode{x2013}$that are known to be tractable in time
$2^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ and space $n^{\mathcal{O}(1)}$ on
$n$-vertex graphs provided with elimination forests of depth $k$. We provide a
model checking algorithm for $\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{ACK}$ with
such complexity that unifies and extends these results. For
$\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{k}$, the fragment of the above logic
that does not use acyclicity and connectivity constraints, we get a
strengthening of this result, where the space complexity is reduced to
$\mathcal{O}(k\log(n))$.
  With a similar mechanism as the distance neighborhood logic introduced in
[Bergougnoux, Dreier and Jaffke, SODA 2023], the logic
$\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{ACK}$ is an extension of the
fully-existential $\mathsf{MSO}_2$ with predicates for (1) querying
generalizations of the neighborhoods of vertex sets, (2) verifying the
connectivity and acyclicity of vertex and edge sets, and (3) verifying that a
vertex set induces a clique. Our results provide
$2^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ time and $n^{\mathcal{O}(1)}$ space
algorithms for problems for which the existence of such algorithms was
previously unknown. In particular, $\mathsf{NEO}_2[\mathsf{FRec}]$ captures
CNF-SAT via the incidence graphs associated to CNF formulas, and it also
captures several modulo counting problems like Odd Dominating Set.

</details>


### [14] [Explaining the Inherent Tradeoffs for Suffix Array Functionality: Equivalences between String Problems and Prefix Range Queries](https://arxiv.org/abs/2510.19815)
*Dominik Kempa,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: 本文提出了首个双向归约，证明后缀数组查询与前缀选择查询在除查询时间中的O(log log n)项外所有参数上等价，统一了先前方法并建立了字符串处理与前缀查询模型之间的核心问题对连接。


<details>
  <summary>Details</summary>
Motivation: 研究当后缀数组无法显式存储时如何高效访问其条目，解决之前单向归约方法的局限性，探索是否可以通过不同技术改进现有边界。

Method: 提出双向归约方法，证明后缀数组查询与前缀选择查询的等价性，并扩展到逆后缀数组查询、模式排名、字典序范围和SA区间查询等六个核心问题对。

Result: 建立了后缀数组查询与前缀选择查询的等价关系（查询时间差O(log log n)），统一了先前方法，为通过前缀查询分析改进字符串处理问题提供了统一框架。

Conclusion: 后缀数组表示本质上可以通过前缀选择结构表达，该框架为分析和改进基础字符串处理问题的效率提供了统一基础。

Abstract: We study the fundamental question of how efficiently suffix array entries can
be accessed when the array cannot be stored explicitly. The suffix array
$SA_T[1..n]$ of a text $T$ of length $n$ encodes the lexicographic order of its
suffixes and underlies numerous applications in pattern matching, data
compression, and bioinformatics. Previous work established one-way reductions
showing how suffix array queries can be answered using, for example, rank
queries on the Burrows-Wheeler Transform. More recently, a new class of prefix
queries was introduced, together with reductions that, among others, transform
a simple tradeoff for prefix-select queries into a suffix array tradeoff
matching state-of-the-art space and query-time bounds, while achieving
sublinear construction time. For binary texts, the resulting data structure
achieves space $O(n)$ bits, preprocessing time $O(n / \sqrt{\log n})$,
preprocessing space of $O(n)$ bits, and query time $O(\log^{\epsilon} n)$ for
any constant $\epsilon > 0$. However, whether these bounds could be improved
using different techniques has remained open.
  We resolve this question by presenting the first bidirectional reduction
showing that suffix array queries are, up to an additive $O(\log\log n)$ term
in query time, equivalent to prefix-select queries in all parameters. This
result unifies prior approaches and shows that essentially all efficient suffix
array representations can be expressed via prefix-select structures. Moreover,
we prove analogous equivalences for inverse suffix array queries, pattern
ranking, lexicographic range, and SA-interval queries, identifying six core
problem pairs that connect string and prefix query models. Our framework thus
provides a unified foundation for analyzing and improving the efficiency of
fundamental string-processing problems through the lens of prefix queries.

</details>


### [15] [Tight Lower Bounds for Central String Queries in Compressed Space](https://arxiv.org/abs/2510.19820)
*Dominik Kempa,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: 本文研究了压缩数据结构中各种字符串查询的时间复杂度下界，证明了在O(δ(T)log^O(1)n)空间内，SA、SA^{-1}、LCP、LCE查询需要Ω(log n/log log n)时间，而BWT、PLCP、LF等查询需要Ω(log log n)时间，建立了查询时间复杂度的清晰二分法。


<details>
  <summary>Details</summary>
Motivation: 当前压缩数据结构中，虽然大多数基本查询都能在O(δ(T)log^O(1)n)空间内高效支持，但除了随机访问外，其他查询的最优时间复杂度尚未完全确定。本文旨在填补这一空白，为压缩索引建立完整的理论基础。

Method: 通过理论分析证明下界，针对不同类型的字符串查询，分别证明其在压缩空间下的最小时间复杂度要求。

Result: 证明了在O(δ(T)log^O(1)n)空间内：SA、SA^{-1}、LCP、LCE查询需要Ω(log n/log log n)时间；BWT、PLCP、LF等查询需要Ω(log log n)时间。这些下界与已知上界匹配，形成了紧确界限。

Conclusion: 本文建立了压缩索引中字符串查询时间复杂度的完整理论框架，揭示了查询时间要么是Θ(log n/log log n)要么是Θ(log log n)的二分性质，为未来数据结构设计提供了明确目标。

Abstract: In this work, we study the limits of compressed data structures, i.e.,
structures that support various queries on an input text $T\in\Sigma^n$ using
space proportional to the size of $T$ in compressed form. Nearly all
fundamental queries can currently be efficiently supported in
$O(\delta(T)\log^{O(1)}n)$ space, where $\delta(T)$ is the substring
complexity, a strong compressibility measure that lower-bounds the optimal
space to represent the text [Kociumaka, Navarro, Prezza, IEEE Trans. Inf.
Theory 2023]. However, optimal query time has been characterized only for
random access.
  We address this gap by developing tight lower bounds for nearly all other
fundamental queries: (1) We prove that suffix array (SA), inverse suffix array
(SA$^{-1}$), longest common prefix (LCP) array, and longest common extension
(LCE) queries all require $\Omega(\log n/\log\log n)$ time within
$O(\delta(T)\log^{O(1)}n)$ space, matching known upper bounds. (2) We further
show that other common queries, currently supported in $O(\log\log n)$ time and
$O(\delta(T)\log^{O(1)}n)$ space, including the Burrows-Wheeler Transform
(BWT), permuted longest common prefix (PLCP) array, Last-to-First (LF), inverse
LF, lexicographic predecessor ($\Phi$), and inverse $\Phi$ queries, all require
$\Omega(\log\log n)$ time, yielding another set of tight bounds.
  Our lower bounds hold even for texts over a binary alphabet. This work
establishes a clean dichotomy: the optimal time complexity to support central
string queries in compressed space is either $\Theta(\log n/\log\log n)$ or
$\Theta(\log\log n)$. This completes the theoretical foundation of compressed
indexing, closing a crucial gap between upper and lower bounds and providing a
clear target for future data structures: seeking either the optimal time in the
smallest space or the fastest time in the optimal space, both of which are now
known for central string queries.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [16] [Recursive decoding of binary rank Reed-Muller codes and Plotkin construction for matrix codes](https://arxiv.org/abs/2510.19095)
*Alain Couvreur,Rakhi Pratihar*

Main category: cs.IT

TL;DR: 提出了一种针对特定类型秩度量Reed-Muller码的递归解码算法，其渐近复杂度优于基于Dickson矩阵的通用解码算法，并引入了类似Plotkin结构的秩度量码构造方法。


<details>
  <summary>Details</summary>
Motivation: 研究2021年提出的新型秩度量码的解码问题，这些码可视为秩度量版本的Reed-Muller码，特别关注二进制情况下的高效解码。

Method: 采用递归解码方法，灵感来自汉明度量二进制Reed-Muller码的Plotkin "(u|u+v)"结构，并提出了通用的类似Plotkin的秩度量码构造和解码器。

Result: 对于特定子类的秩度量Reed-Muller码，递归解码算法的渐近复杂度优于基于Dickson矩阵的通用解码算法，且解码器性质完全不同。

Conclusion: 成功开发了高效的递归解码算法和通用的Plotkin-like构造方法，为秩度量码的解码提供了新思路。

Abstract: In 2021, Augot, Couvreur, Lavauzelle and Neri introduced a new class of rank
metric codes which can be regarded as rank metric counterparts of Reed-Muller
codes. Given a finite Galois extension $\mathbb{L} / \mathbb{K}$, these codes
are defined as some specific $\mathbb{L}$-subspaces of the twisted group
algebra $\mathbb{L} [\textrm{G}]$. We investigate the decoding of such codes in
the "binary" case, \emph{i.e.,} when $\textrm{G} = (\mathbb{Z}/2\mathbb{Z})^m$.
Our approach takes its inspiration from the decoding of Hamming metric binary
Reed-Muller codes using their recursive Plotkin "$(u ~|~ u+v)$" structure. If
our recursive algorithm restricts to a specific subclass of rank metric
Reed-Muller codes, its asymptotic complexity beats that of the recently
proposed decoding algorithm for arbitrary rank metric Reed-Muller codes based
on Dickson matrices. Also, this decoder is of completely different nature and
leads a natural rank metric counterpart of the Plotkin construction. To
illustrate this, we also propose a generic Plotkin-like construction for matrix
rank metric codes with an associate decoder, which can be applied to any pair
of codes equipped with an efficient decoder.

</details>


### [17] [Weighted Sum Rate Optimization for Movable Antenna Enabled Near-Field ISAC](https://arxiv.org/abs/2510.19759)
*Nemanja Stefan Perović,Keshav Singh,Chih-Peng Li,Mark F. Flanagan*

Main category: cs.IT

TL;DR: 本文研究了在近场集成感知与通信系统中使用可移动天线来最大化加权和速率，同时满足感知需求。


<details>
  <summary>Details</summary>
Motivation: 可移动天线有潜力提升ISAC系统性能，但在近场球形波传播下实现这些增益面临挑战。

Method: 提出交替优化算法，联合优化感知接收组合器、通信预编码矩阵、感知发射波束形成器和用户可移动天线位置。

Result: 仿真表明，在近场ISAC系统中使用可移动天线相比固定天线带来显著性能优势；靠近基站的用户获得更高加权和速率；感知性能对最小SINR阈值更敏感。

Conclusion: 可移动天线能有效提升近场ISAC系统性能，且系统设计需考虑用户位置权重分配和感知性能的敏感性。

Abstract: Integrated sensing and communication (ISAC) has been recognized as one of the
key technologies capable of simultaneously improving communication and sensing
services in future wireless networks. Moreover, the introduction of recently
developed movable antennas (MAs) has the potential to further increase the
performance gains of ISAC systems. Achieving these gains can pose a significant
challenge for MA-enabled ISAC systems operating in the near-field due to the
corresponding spherical wave propagation. Motivated by this, in this paper we
maximize the weighted sum rate (WSR) for communication users while maintaining
a minimal sensing requirement in an MA-enabled near-field ISAC system. To
achieve this goal, we propose an algorithm that optimizes the sensing receive
combiner, the communication precoding matrices, the sensing transmit beamformer
and the positions of the users' MAs in an alternating manner. Simulation
results show that using MAs in near-field ISAC systems provides a substantial
performance advantage compared to near-field ISAC systems with only fixed
antennas. Additionally, we demonstrate that the highest WSR is obtained when
larger weights are allocated to the users placed closer to the BS, and that the
sensing performance is significantly more affected by the minimum sensing
signal-to-interference-plus-noise ratio (SINR) threshold compared to the
communication performance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [Step-Aware Residual-Guided Diffusion for EEG Spatial Super-Resolution](https://arxiv.org/abs/2510.19166)
*Hongjun Liu,Leyu Zhou,Zijianghao Yang,Chao Yao*

Main category: cs.MM

TL;DR: SRGDiff是一个用于EEG空间超分辨率的步感知残差引导扩散模型，通过动态条件生成从低密度EEG信号恢复高密度信号，在多个数据集和上采样尺度上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界BCI应用中，轻量级EEG系统虽然成本效益好，但空间稀疏性限制了空间保真度，影响学习效果并引入偏差。现有EEG空间超分辨率方法存在分布偏移和信号失真问题，降低了EEG分析和可视化的可用性。

Method: 提出SRGDiff模型，将EEG空间超分辨率建模为动态条件生成。核心思想是从低密度输入学习动态残差条件，预测步级时空细节，并通过逐步演化的线索引导去噪过程实现高密度重建。在每个去噪步骤中，残差条件与先前去噪器特征图相加融合，然后通过步相关仿射调制缩放和移动激活以产生当前特征。

Result: 在SEED、SEED-IV和Localize-MI数据集上，采用信号级、特征级和下游级指标的全面评估协议。SRGDiff在多个上采样尺度上相比强基线实现了高达40%的持续增益，证明了其在EEG空间超分辨率任务中的优越性。

Conclusion: SRGDiff通过动态残差引导的扩散模型有效解决了EEG空间超分辨率中的分布偏移和信号失真问题，在保持保真度-一致性平衡的同时，显著提升了重建质量，为轻量级EEG系统的实际应用提供了有力支持。

Abstract: For real-world BCI applications, lightweight Electroencephalography (EEG)
systems offer the best cost-deployment balance. However, such spatial sparsity
of EEG limits spatial fidelity, hurting learning and introducing bias. EEG
spatial super-resolution methods aim to recover high-density EEG signals from
sparse measurements, yet is often hindered by distribution shift and signal
distortion and thus reducing fidelity and usability for EEG analysis and
visualization. To overcome these challenges, we introduce SRGDiff, a step-aware
residual-guided diffusion model that formulates EEG spatial super-resolution as
dynamic conditional generation. Our key idea is to learn a dynamic residual
condition from the low-density input that predicts the step-wise temporal and
spatial details to add and uses the evolving cue to steer the denoising process
toward high-density reconstructions. At each denoising step, the proposed
residual condition is additively fused with the previous denoiser feature maps,
then a step-dependent affine modulation scales and shifts the activation to
produce the current features. This iterative procedure dynamically extracts
step-wise temporal rhythms and spatial-topographic cues to steer high-density
recovery and maintain a fidelity-consistency balance. We adopt a comprehensive
evaluation protocol spanning signal-, feature-, and downstream-level metrics
across SEED, SEED-IV, and Localize-MI and multiple upsampling scales. SRGDiff
achieves consistent gains of up to 40% over strong baselines, proving its
superiority in the task of EEG spatial super-resolution. Moreover, topographic
visualizations comparison and substantial EEG-FID gains jointly indicate that
our SR EEG mitigates the spatial-spectral shift between low- and high-density
recordings.

</details>


### [19] [CDI-DTI: A Strong Cross-domain Interpretable Drug-Target Interaction Prediction Framework Based on Multi-Strategy Fusion](https://arxiv.org/abs/2510.19520)
*Xiangyu Li,Haojie Yang,Kaimiao Hu,Runzhi Wu,Liangliang Liu,Ran Su*

Main category: cs.MM

TL;DR: 提出CDI-DTI框架，通过多模态特征融合和跨域注意力机制，解决药物-靶点相互作用预测中的跨域泛化、冷启动和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 现有DTI预测方法在跨域泛化、冷启动预测和可解释性方面存在不足，需要开发更鲁棒和可解释的框架。

Method: 集成文本、结构和功能多模态特征，采用多策略融合方法，引入多源跨注意力机制进行特征对齐和融合，使用双向跨注意力层捕获细粒度相互作用，结合Gram Loss和深度正交融合模块提升可解释性。

Result: 在多个基准数据集上显著优于现有方法，特别是在跨域和冷启动任务中表现优异，同时保持高可解释性。

Conclusion: CDI-DTI框架有效解决了DTI预测中的关键挑战，为药物发现提供了鲁棒且可解释的解决方案。

Abstract: Accurate prediction of drug-target interactions (DTI) is pivotal for drug
discovery, yet existing methods often fail to address challenges like
cross-domain generalization, cold-start prediction, and interpretability. In
this work, we propose CDI-DTI, a novel cross-domain interpretable framework for
DTI prediction, designed to overcome these limitations. By integrating
multi-modal features-textual, structural, and functional-through a
multi-strategy fusion approach, CDI-DTI ensures robust performance across
different domains and in cold-start scenarios. A multi-source cross-attention
mechanism is introduced to align and fuse features early, while a bidirectional
cross-attention layer captures fine-grained intra-modal drug-target
interactions. To enhance model interpretability, we incorporate Gram Loss for
feature alignment and a deep orthogonal fusion module to eliminate redundancy.
Experimental results on several benchmark datasets demonstrate that CDI-DTI
significantly outperforms existing methods, particularly in cross-domain and
cold-start tasks, while maintaining high interpretability for practical
applications in drug-target interaction prediction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [20] [SBAN: A Framework \& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining](https://arxiv.org/abs/2510.18936)
*Hamed Jelodar,Mohammad Meymani,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: SBAN是一个大规模多维度数据集，包含300多万个样本，涵盖二进制代码、汇编指令、自然语言描述和源代码四个层次，用于提升大语言模型在软件代码分析中的预训练和评估能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决软件代码分析中缺乏大规模、多维度数据集的问题，支持跨表示学习、软件语义理解和自动化恶意软件检测等研究。

Method: 构建包含300多万个样本的数据集，每个样本包含二进制代码、汇编指令、自然语言描述和源代码四个互补层次的表示。

Result: 创建了SBAN数据集，包含290万个良性样本和67.2万个恶意软件样本，支持代码翻译、代码解释等软件挖掘任务。

Conclusion: SBAN通过连接低级机器表示和高级人类语义，为构建智能代码推理系统提供了坚实基础，为软件行为挖掘和安全分析开辟了新机会。

Abstract: This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.

</details>


### [21] [XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security](https://arxiv.org/abs/2510.19006)
*Hamed Jelodar,Mohammad Meymani,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: XGen-Q是基于Qwen-Coder架构的领域适应大语言模型，专门用于恶意软件检测和分析，通过多阶段提示策略和检索增强生成技术，在复杂代码混淆情况下仍能提供可靠的恶意软件识别和详细取证报告。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件检测系统难以泛化到混淆或未见过的威胁，需要更适应性强且可解释的模型。生成式AI和大语言模型在代码理解方面表现出色，但在网络安全领域的应用仍然有限。

Method: 基于Qwen-Coder架构构建领域适应LLM，在超过100万个恶意软件样本的大规模语料库上进行预训练，涵盖源代码和汇编代码。采用多阶段提示策略结合检索增强生成(RAG)，设计训练管道系统性地暴露模型于多样化混淆模式。

Result: 实验结果显示XGen-Q相比竞争基线实现了显著更低的困惑度，并在新型恶意软件样本上表现出强大性能。

Conclusion: 基于LLM的方法在可解释和鲁棒的恶意软件分析方面具有良好前景，XGen-Q展示了这种方法的有效性。

Abstract: Generative AI and large language models (LLMs) have shown strong capabilities
in code understanding, but their use in cybersecurity, particularly for malware
detection and analysis, remains limited. Existing detection systems often fail
to generalize to obfuscated or previously unseen threats, underscoring the need
for more adaptable and explainable models. To address this challenge, we
introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and
pretrained on a large-scale corpus of over one million malware samples,
spanning both source and assembly code. XGen-Q uses a multi-stage prompt
strategy combined with retrieval-augmented generation (RAG) to deliver reliable
malware identification and detailed forensic reporting, even in the presence of
complex code obfuscation. To further enhance generalization, we design a
training pipeline that systematically exposes the model to diverse obfuscation
patterns. Experimental results show that XGen-Q achieves significantly lower
perplexity than competitive baselines and exhibits strong performance on novel
malware samples, demonstrating the promise of LLM-based approaches for
interpretable and robust malware analysis.

</details>


### [22] [C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search](https://arxiv.org/abs/2510.19221)
*Yingchen Zhang,Ruqing Zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: C2T-ID是一种新的文档标识符设计方法，通过将语义数值docid转换为关键词docid，在保持语义表达力的同时控制搜索空间大小。


<details>
  <summary>Details</summary>
Motivation: 解决生成式检索中语义表达性与搜索空间可控性之间的权衡问题：数值docid缺乏语义信息，纯文本docid搜索空间过大且对早期错误敏感。

Method: 分三步：(1)通过层次聚类构建语义数值docid；(2)提取高频元数据关键词，用每个簇的top-K关键词迭代替换数值标签；(3)可选的两级语义平滑步骤增强C2T-ID的流畅性。

Result: 在Natural Questions和淘宝产品搜索上的实验表明，C2T-ID显著优于原子docid、语义码本和纯文本docid基线方法。

Conclusion: C2T-ID在语义表达性和搜索空间约束之间取得了良好平衡，是生成式检索中有效的文档标识符设计方法。

Abstract: Designing document identifiers (docids) that carry rich semantic information
while maintaining tractable search spaces is a important challenge in
generative retrieval (GR). Popular codebook methods address this by building a
hierarchical semantic tree and constraining generation to its child nodes, yet
their numeric identifiers cannot leverage the large language model's pretrained
natural language understanding. Conversely, using text as docid provides more
semantic expressivity but inflates the decoding space, making the system
brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)
first construct semantic numerical docid via hierarchical clustering; (ii) then
extract high-frequency metadata keywords and iteratively replace each numeric
label with its cluster's top-K keywords; and (iii) an optional two-level
semantic smoothing step further enhances the fluency of C2T-ID. Experiments on
Natural Questions and Taobao's product search demonstrate that C2T-ID
significantly outperforms atomic, semantic codebook, and pure-text docid
baselines, demonstrating its effectiveness in balancing semantic expressiveness
with search space constraints.

</details>


### [23] [CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale](https://arxiv.org/abs/2510.19340)
*L. Caspari,M. Dinzinger,K. Gosh Dastidar,C. Fellicious,J. Mitrović,M. Granitzer*

Main category: cs.IR

TL;DR: CoRECT框架用于大规模评估嵌入压缩方法，发现非学习压缩方法能在100M段落上显著减小索引大小且性能损失统计不显著，但最优压缩方法选择仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽视语料库复杂度对密集检索性能的影响，需要系统评估嵌入压缩方法在不同条件下的表现。

Method: 提出CoRECT框架，构建新数据集集合，对8种代表性压缩方法进行基准测试。

Result: 非学习压缩方法在100M段落上能大幅减少索引大小，性能损失统计不显著，但不同模型间性能差异明显。

Conclusion: 压缩方法选择具有挑战性，需要CoRECT框架来确保一致比较和明智选择，所有代码和数据已开源。

Abstract: Dense retrieval systems have proven to be effective across various
benchmarks, but require substantial memory to store large search indices.
Recent advances in embedding compression show that index sizes can be greatly
reduced with minimal loss in ranking quality. However, existing studies often
overlook the role of corpus complexity -- a critical factor, as recent work
shows that both corpus size and document length strongly affect dense retrieval
performance. In this paper, we introduce CoRECT (Controlled Retrieval
Evaluation of Compression Techniques), a framework for large-scale evaluation
of embedding compression methods, supported by a newly curated dataset
collection. To demonstrate its utility, we benchmark eight representative types
of compression methods. Notably, we show that non-learned compression achieves
substantial index size reduction, even on up to 100M passages, with
statistically insignificant performance loss. However, selecting the optimal
compression method remains challenging, as performance varies across models.
Such variability highlights the necessity of CoRECT to enable consistent
comparison and informed selection of compression methods. All code, data, and
results are available on GitHub and HuggingFace.

</details>


### [24] [Top-P Masking for Cross Language Information Retrieval](https://arxiv.org/abs/2510.19758)
*Joseph Casale,Andrew Silverschotz,Joseph DeSimone*

Main category: cs.IR

TL;DR: 提出使用Top-P动态掩码（类似于大型语言模型中的核采样）替代Top-K掩码方案，在跨语言信息检索任务中表现更好


<details>
  <summary>Details</summary>
Motivation: Top-K掩码方案被提出作为促进信息检索任务中稀疏表示的简单方法，是浮点运算正则化的替代方案。现有算法如BLADE将其作为后处理阶段使用

Method: 采用Top-P动态掩码方法，类似于大型语言模型中的核采样技术，与传统的Top-K掩码方案进行对比

Result: 在跨语言信息检索领域评估表明，Top-P动态掩码比Top-K掩码具有更好的性能表现

Conclusion: Top-P动态掩码在促进稀疏表示方面优于Top-K掩码方案，为信息检索任务提供了更有效的后处理方法

Abstract: Top-K masking schemes have been proposed as a method to promote sparse
representations in Information Retrieval (IR) tasks, as a simple alternative to
Floating Point Operations per Second (FLOPS) regularization. Algorithms such as
Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as
a post-processing stage. We propose using Top-P Dynamic Masking similar to
Nucleus Sampling in Large Language Models, and demonstrate better performance
than Top-K masking. Specifically, we evaluate our methods in the domain of
Cross Language Information Retrieval (CLIR)

</details>
