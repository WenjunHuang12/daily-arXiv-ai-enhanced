<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 3]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Mechanism Design for Facility Location using Predictions](https://arxiv.org/abs/2508.03818)
*Toby Walsh*

Main category: cs.GT

TL;DR: 研究了基于预测的设施选址机制，通过考虑最大距离和最小效用，提出新的机制设计，平衡一致性和鲁棒性，并扩展到双设施问题。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用预测优化设施选址问题，同时关注一致性和鲁棒性，以提升机制性能。

Method: 设计新机制，调整参数以平衡一致性和鲁棒性，并扩展到双设施问题，使用两个预测点。

Result: 提出了更鲁棒的新机制，能够通过参数调整在一致性和鲁棒性之间权衡，并成功应用于双设施选址。

Conclusion: 通过考虑预测和多种性能指标，设计出更优的设施选址机制，适用于单设施和双设施问题。

Abstract: We study mechanisms for the facility location problem augmented with
predictions of the optimal facility location. We demonstrate that an
egalitarian viewpoint which considers both the maximum distance of any agent
from the facility and the minimum utility of any agent provides important new
insights compared to a viewpoint that just considers the maximum distance. As
in previous studies, we consider performance in terms of consistency (worst
case when predictions are accurate) and robustness (worst case irrespective of
the accuracy of predictions). By considering how mechanisms with predictions
can perform poorly, we design new mechanisms that are more robust. Indeed, by
adjusting parameters, we demonstrate how to trade robustness for consistency.
We go beyond the single facility problem by designing novel strategy proof
mechanisms for locating two facilities with bounded consistency and robustness
that use two predictions for where to locate the two facilities.

</details>


### [2] [What Do Agents Think Others Would Do? Level-2 Inverse Games for Inferring Agents' Estimates of Others' Objectives](https://arxiv.org/abs/2508.03824)
*Hamzah I. Khan,Jingqi Li,David Fridovich-Keil*

Main category: cs.GT

TL;DR: 论文提出了一种解决多智能体战略交互中目标推断问题的框架，通过引入“level-2”推断方法，解决了现有“level-1”方法在现实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设智能体完全了解彼此目标，但在现实场景（如城市驾驶和谈判）中，这一假设不成立。论文旨在解决这一问题。

Method: 提出“level-2”推断框架，通过梯度优化方法解决非凸问题，并在线性二次博弈中验证其有效性。

Result: 实验表明，新方法能发现“level-1”方法忽略的细微目标不一致。

Conclusion: “level-2”推断方法在多智能体交互中更准确地捕捉了目标异质性，具有实际应用价值。

Abstract: Effectively interpreting strategic interactions among multiple agents
requires us to infer each agent's objective from limited information. Existing
inverse game-theoretic approaches frame this challenge in terms of a "level-1"
inference problem, in which we take the perspective of a third-party observer
and assume that individual agents share complete knowledge of one another's
objectives. However, this assumption breaks down in decentralized, real-world
decision scenarios like urban driving and bargaining, in which agents may act
based on conflicting views of one another's objectives. We demonstrate the
necessity of inferring agents' heterogeneous estimates of each other's
objectives through empirical examples, and by theoretically characterizing the
prediction error of level-1 inference on fictitious gameplay data from
linear-quadratic games. To address this fundamental issue, we propose a
framework for level-2 inference to address the question: "What does each agent
believe about all agents' objectives?" We prove that the level-2 inference
problem is non-convex even in benign settings like linear-quadratic games, and
we develop an efficient gradient-based approach for identifying local
solutions. Experiments on a synthetic urban driving example show that our
approach uncovers nuanced misalignments that level-1 methods miss.

</details>


### [3] [Inequality in the Age of Pseudonymity](https://arxiv.org/abs/2508.04668)
*Aviv Yaish,Nir Chemaya,Lin William Cong,Dahlia Malkhi*

Main category: cs.GT

TL;DR: 论文分析了在匿名环境中（如互联网或区块链平台）不平等度量（如基尼系数）的应用，揭示了“Sybils”（伪造身份）如何扭曲不平等度量，并提出了一些抗Sybil的度量方法。


<details>
  <summary>Details</summary>
Motivation: 研究不平等度量在匿名环境中的有效性，特别是Sybils（伪造身份）对度量结果的影响，为政策制定和平台设计提供理论支持。

Method: 分析了满足文献中理想属性的不平等度量在Sybils存在时的局限性，提出并分类了抗Sybil的度量方法，并证明了其局限性。

Result: 证明传统不平等度量（如基尼系数）易受Sybil操纵，抗Sybil度量方法在细粒度评估不平等时能力受限。

Conclusion: 在匿名环境中，Sybils的存在使得传统不平等度量失效，抗Sybil度量方法虽可行但牺牲了部分评估能力。

Abstract: Inequality measures such as the Gini coefficient are used to inform and
motivate policymaking, and are increasingly applied to digital platforms. We
analyze how measures fare in pseudonymous settings, as common to internet-based
or blockchain-based platforms. One key challenge that arises is the ability of
actors to create multiple fake identities under fictitious false names, also
known as ``Sybils.'' While some actors may do so to preserve their privacy, we
show that this can inadvertently distort inequality metrics. As we show, when
using inequality measures that satisfy literature's canonical set of desired
properties, the presence of Sybils in an economy implies that it is impossible
to properly measure the economy's inequality. Then, we present several classes
of Sybil-proof measures that satisfy relaxed versions of the aforementioned
desired properties, and, by fully characterizing them, we prove that the
structure imposed restricts their ability to assess inequality at a
fine-grained level. In addition, we prove that popular inequality metrics,
including the famous Gini coefficient, are vulnerable to Sybil manipulations,
and examine the dynamics that result in the creation of Sybils, whether in
pseudonymous settings or traditional ones.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [4] [A 60-Addition, Rank-23 Scheme for Exact 3x3 Matrix Multiplication](https://arxiv.org/abs/2508.03857)
*Joshua Stapleton*

Main category: cs.DS

TL;DR: 将3x3非交换矩阵乘法的加法成本从61和62降至60，无需基变换，创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 降低矩阵乘法的计算成本，提高效率。

Method: 优化算法，减少加法操作，无需改变基。

Result: 加法成本从61和62降至60，达到最新技术水平。

Conclusion: 成功降低了3x3矩阵乘法的加法成本，为未来研究提供了新方向。

Abstract: We reduce the additive cost of general (non-commutative) 3x3 matrix
multiplication from the previous records of 61 (Schwartz-Vaknin, 2023) and 62
(Martensson-Wagner, 2025) to 60 without a change of basis. To our knowledge,
this represents a new state-of-the-art.

</details>


### [5] [Counting Distinct Square Substrings in Sublinear Time](https://arxiv.org/abs/2508.03930)
*Panagiotis Charalampopoulos,Manal Mohamed,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 提出了一种在压缩字符串中计算不同平方数的次线性时间算法，时间复杂度为O(n/logσn)。


<details>
  <summary>Details</summary>
Motivation: 解决压缩字符串中高效计算不同平方数的问题，填补了次线性时间算法的空白。

Method: 利用长周期run的隐式表示和稀疏Lyndon根技术，结合字符串同步器的思想。

Result: 首次实现了压缩字符串中平方数的次线性时间计算。

Conclusion: 通过新颖的技术解决了压缩字符串中平方数计算的难题，为相关领域提供了新思路。

Abstract: We show that the number of distinct squares in a packed string of length $n$
over an alphabet of size $\sigma$ can be computed in $O(n/\log_\sigma n)$ time
in the word-RAM model. This paper is the first to introduce a sublinear-time
algorithm for counting squares in the packed setting. The packed representation
of a string of length $n$ over an alphabet of size $\sigma$ is given as a
sequence of $O(n/\log_\sigma n)$ machine words in the word-RAM model (a machine
word consists of $\omega \ge \log_2 n$ bits). Previously, it was known how to
count distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for
a string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al.,
CPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for
extracting squares from runs described by Crochemore et al. [TCS 2014].
However, the packed model requires novel approaches.
  We need an $O(n/\log_\sigma n)$-sized representation of all long-period runs
(runs with period $\Omega(\log_\sigma n)$) which allows for a sublinear-time
counting of the -- potentially linearly-many -- implied squares. The
long-period runs with a string period that is periodic itself (called layer
runs) are an obstacle, since their number can be $\Omega(n)$. The number of all
other long-period runs is $O(n/\log_\sigma n)$ and we can construct an implicit
representation of all long-period runs in $O(n/\log_\sigma n)$ time by
leveraging the insights of Amir et al. [ESA 2019]. We count squares in layer
runs by exploiting combinatorial properties of pyramidally-shaped groups of
layer runs. Another difficulty lies in computing the locations of Lyndon roots
of runs in packed strings, which is needed for grouping runs that may generate
equal squares. To overcome this difficulty, we introduce sparse-Lyndon roots
which are based on string synchronizers [Kempa and Kociumaka, STOC 2019].

</details>


### [6] [Exactly simulating stochastic chemical reaction networks in sub-constant time per reaction](https://arxiv.org/abs/2508.04079)
*Joshua Petrack,David Doty*

Main category: cs.DS

TL;DR: 本文提出了一种新的化学反应网络随机模拟算法，其运行时间在理论上低于线性，同时保持与Gillespie算法相同的随机动态。


<details>
  <summary>Details</summary>
Motivation: 传统的Gillespie算法模拟ℓ次反应需要Ω(ℓ)时间，效率较低。本文旨在开发一种更高效的算法，能够在保持精确随机动态的同时，显著减少运行时间。

Method: 本文提出了一种基于Berenbrink等人算法的改进方法，将其扩展到更通用的化学反应网络场景。算法在ℓ≥n^{5/4}时运行时间为O(ℓ/√n)，在n≤ℓ≤n^{5/4}时为O(ℓ/n^{2/5})。

Result: 新算法在理论上实现了运行时间的亚线性优化，并在实践中通过Python和Rust实现表现出卓越的性能。

Conclusion: 本文提出的算法为化学反应网络的随机模拟提供了一种高效且精确的解决方案，显著提升了计算效率。

Abstract: The model of chemical reaction networks is among the oldest and most widely
studied and used in natural science. The model describes reactions among
abstract chemical species, for instance $A + B \to C$, which indicates that if
a molecule of type $A$ interacts with a molecule of type $B$ (the reactants),
they may stick together to form a molecule of type $C$ (the product). The
standard algorithm for simulating (discrete, stochastic) chemical reaction
networks is the Gillespie algorithm [JPC 1977], which stochastically simulates
one reaction at a time, so to simulate $\ell$ consecutive reactions, it
requires total running time $\Omega(\ell)$.
  We give the first chemical reaction network stochastic simulation algorithm
that can simulate $\ell$ reactions, provably preserving the exact stochastic
dynamics (sampling from precisely the same distribution as the Gillespie
algorithm), yet using time provably sublinear in $\ell$. Under reasonable
assumptions, our algorithm can simulate $\ell$ reactions among $n$ total
molecules in time $O(\ell/\sqrt n)$ when $\ell \ge n^{5/4}$, and in time
$O(\ell/n^{2/5})$ when $n \le \ell \le n^{5/4}$. Our work adapts an algorithm
of Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020] for
simulating the distributed computing model known as population protocols,
extending it (in a very nontrivial way) to the more general chemical reaction
network setting.
  We provide an implementation of our algorithm as a Python package, with the
core logic implemented in Rust, with remarkably fast performance in practice.

</details>


### [7] [Exact Matching in Matrix Multiplication Time](https://arxiv.org/abs/2508.04081)
*Ryotaro Sato,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文回顾了代数匹配算法的基础知识，并探讨了通过快速计算矩阵特征多项式改进的可能性，特别是解决了精确匹配问题，并扩展到线性拟阵配对问题。


<details>
  <summary>Details</summary>
Motivation: 研究代数算法在匹配及相关问题中的应用，探索通过矩阵特征多项式快速计算改进现有方法。

Method: 利用矩阵特征多项式的快速计算技术，解决精确匹配问题，并扩展到线性拟阵配对问题。

Result: 证明精确匹配问题可以在与矩阵乘法相同的时间复杂度内以高概率解决。

Conclusion: 通过代数方法改进匹配问题的算法效率，并展示了扩展到更复杂问题的潜力。

Abstract: Initiated by Mulmuley, Vazirani, and Vazirani (1987), many algebraic
algorithms have been developed for matching and related problems. In this
paper, we review basic facts and discuss possible improvements with the aid of
fast computation of the characteristic polynomial of a matrix. In particular,
we show that the so-called exact matching problem can be solved with high
probability in asymptotically the same time order as matrix multiplication. We
also discuss its extension to the linear matroid parity problem.

</details>


### [8] [Approximation Algorithms for Scheduling Crowdsourcing Tasks in Mobile Social Networks](https://arxiv.org/abs/2508.04159)
*Chi-Yeh Chen*

Main category: cs.DS

TL;DR: 本文纠正了Zhang等人关于移动社交网络中调度问题的近似比分析错误，并提出了新的随机和确定性算法以优化任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 解决移动社交网络中任务调度的近似比分析错误，并提出更高效的调度算法。

Method: 1. 纠正Zhang等人的近似比分析；2. 提出随机近似算法（1.5 + ε）；3. 提出确定性近似算法（max{2.5,1+ε}）。

Result: 随机算法实现1.5 + ε的近似比；确定性算法在特定条件下可达1.5 + ε。

Conclusion: 本文改进了移动社交网络中的任务调度算法，提供了更优的近似比结果。

Abstract: This paper addresses the scheduling problem in mobile social networks. We
begin by proving that the approximation ratio analysis presented in the paper
by Zhang \textit{et al.} (IEEE Transactions on Mobile Computing, 2025) is
incorrect, and we provide the correct analysis results. Furthermore, when the
required service time for a task exceeds the total contact time between the
requester and the crowd worker, we demonstrate that the approximation ratio of
the Largest-Ratio-First task scheduling algorithm can reach $2 - \frac{1}{m}$.
Next, we introduce a randomized approximation algorithm to minimize mobile
social networks' total weighted completion time. This algorithm achieves an
expected approximation ratio of $1.5 + \epsilon$ for $\epsilon>0$. Finally, we
present a deterministic approximation algorithm that minimizes mobile social
networks' total weighted completion time. This deterministic algorithm achieves
an approximation ratio of $\max\left\{2.5,1+\epsilon\right\}$ for $\epsilon>0$.
Additionally, when the task's required service time or the total contact time
between the requester and the crowd worker is sufficiently large, this
algorithm can reach an approximation ratio of $1.5+\epsilon$ for $\epsilon>0$.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [9] [LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content](https://arxiv.org/abs/2508.04353)
*Anderson de Lima Luiz*

Main category: cs.MM

TL;DR: LUST框架通过多模态分析视频内容，结合视觉和音频文本信息，利用两阶段评分机制量化视频片段与用户定义主题的相关性。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种能够根据用户定义的文本描述，量化视频内容主题相关性的方法，以支持更精准的视频分析。

Method: 采用多模态分析管道，结合视觉和ASR提取的文本信息，通过两阶段（直接相关性和上下文相关性）评分机制，利用LLM进行评分。

Result: 输出带注释的视频，包含可视化相关性评分和详细分析日志。

Conclusion: LUST框架能够提供时间感知的用户定义主题相关性度量，适用于复杂视频内容分析。

Abstract: This paper introduces the Learned User Significance Tracker (LUST), a
framework designed to analyze video content and quantify the thematic relevance
of its segments in relation to a user-provided textual description of
significance. LUST leverages a multi-modal analytical pipeline, integrating
visual cues from video frames with textual information extracted via Automatic
Speech Recognition (ASR) from the audio track. The core innovation lies in a
hierarchical, two-stage relevance scoring mechanism employing Large Language
Models (LLMs). An initial "direct relevance" score, $S_{d,i}$, assesses
individual segments based on immediate visual and auditory content against the
theme. This is followed by a "contextual relevance" score, $S_{c,i}$, that
refines the assessment by incorporating the temporal progression of preceding
thematic scores, allowing the model to understand evolving narratives. The LUST
framework aims to provide a nuanced, temporally-aware measure of user-defined
significance, outputting an annotated video with visualized relevance scores
and comprehensive analytical logs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是一个专为代理AI系统设计的运行时治理框架，通过六个组件实时控制风险，填补传统治理方法的不足。


<details>
  <summary>Details</summary>
Motivation: 代理AI系统在运行时表现出不可预测的行为，传统预部署治理无法完全应对，需要新的治理方法。

Method: MI9框架包括六个组件：代理风险指数、代理语义遥测捕获、持续授权监控、基于有限状态机的符合性引擎、目标条件漂移检测和分级遏制策略。

Result: MI9在多样场景中展示了系统性治理能力，解决了现有方法无法覆盖的问题。

Conclusion: MI9为代理AI系统的安全部署提供了技术基础，实现了全面的运行时治理。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [11] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL是一种多智能体强化学习框架，通过联合训练任务智能体获得防御能力，避免依赖外部安全模块，提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型构建的多智能体系统（MAS）面临安全风险（如越狱和对抗攻击），现有防御方法依赖外部安全模块，存在单点故障和成本问题。

Method: 提出Evo-MARL框架，通过多智能体强化学习（MARL）训练每个智能体同时执行主任务和防御任务，结合进化搜索和参数共享强化学习进行对抗训练。

Result: 实验表明，Evo-MARL将攻击成功率降低22%，推理任务准确率提升5%，证明安全性与性能可同时提升。

Conclusion: Evo-MARL通过内部化安全机制和对抗训练，有效提升MAS的安全性和性能，避免系统开销和单点故障。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [12] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MOTIF框架通过多策略优化和基于蒙特卡洛树搜索的交互式框架，联合改进多个组件，显著提升组合优化问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常仅优化单一组件（如启发式评分函数），限制了创新潜力。MOTIF旨在通过多策略优化和交互式框架，探索更广泛的解决方案。

Method: 提出MOTIF框架，基于蒙特卡洛树搜索，通过两个LLM代理的轮换优化，促进竞争与合作，共同改进多个组件。

Result: 在多个组合优化问题领域，MOTIF表现优于现有最先进方法。

Conclusion: MOTIF展示了基于轮换的多代理提示在自动化求解器设计中的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [13] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自进化框架，通过自主学习和任务生成，帮助计算机使用代理（CUAs）掌握新软件环境，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）在缺乏人工标注的新软件环境中表现不佳，需要自主进化能力。

Method: SEAgent通过世界状态模型和课程生成器实现自主学习和任务生成，结合对抗模仿和GRPO优化策略。

Result: 在五个新软件环境中，SEAgent的成功率从11.3%提升至34.5%，超越现有方法。

Conclusion: SEAgent通过自主学习和任务生成，显著提升了CUAs在新软件环境中的表现。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


### [14] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: SymbolBench是一个评估符号推理能力的基准，结合LLMs与遗传编程，揭示其在科学发现中的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs从时间序列数据中推断可解释符号结构的能力，填补现有研究的空白。

Method: 引入SymbolBench基准，结合LLMs与遗传编程，形成闭环符号推理系统。

Result: 揭示了当前模型的优势与局限，强调领域知识、上下文对齐和推理结构的重要性。

Conclusion: 结合领域知识与推理结构可提升LLMs在自动科学发现中的表现。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [15] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 论文提出EmoAgent框架，通过情感提示劫持推理路径，揭示MLRMs在高情感强度下的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 发现面向人类服务的MLRMs在深度思考阶段易受用户情感影响，可能绕过安全协议。

Method: 提出EmoAgent框架，利用夸张情感提示干扰模型推理，并设计三个指标量化风险。

Result: 实验证明EmoAgent有效，揭示了模型安全行为中的情感认知偏差。

Conclusion: MLRMs存在情感认知与安全行为的不一致，现有安全措施可能失效。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [16] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为Cognition Forest的语义结构，用于将认知建模与系统设计统一为一个自增强循环，并基于此开发了Galaxy框架，支持多维交互和个性化能力生成。实验表明Galaxy优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 智能个人助手（IPA）的主动行为研究不足，设计兼具主动性、隐私保护和自我进化能力的IPA是一个重要挑战。

Method: 提出Cognition Forest语义结构，将认知架构与系统设计统一为自增强循环，并开发Galaxy框架，实现多维交互和个性化能力生成。

Result: Galaxy在实验中优于多个先进基准，消融研究和实际交互案例验证了其有效性。

Conclusion: Cognition Forest和Galaxy框架为IPA的主动行为和自我进化提供了新思路，实验证明了其优越性。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [17] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一个不确定性感知的GUI代理，通过自适应感知解决输入冗余和决策模糊问题，包含组件推荐和交互模块，并在复杂场景中验证有效性。


<details>
  <summary>Details</summary>
Motivation: GUI代理在自动化移动任务中存在输入冗余和决策模糊的问题，需要更高效的解决方案。

Method: RecAgent采用组件推荐机制减少感知不确定性，交互模块处理决策不确定性，结合人机协同优化。

Result: 实验验证了RecAgent的有效性，并提出了ComplexAction数据集用于评估。

Conclusion: RecAgent通过自适应感知和人机协同显著提升了GUI代理的性能，适用于复杂场景。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [18] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出了一种名为SEA（Self-Evolution Agent）的计算机使用代理，通过创新的数据生成、强化学习和模型增强方法，显著提升了代理的性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理的性能不足以实际应用，因此需要开发更高效的代理。

Method: 提出自动生成可验证轨迹的管道、高效的逐步强化学习策略，以及无需额外训练的模型增强方法。

Result: SEA仅需7B参数，性能优于同规模模型，并与更大模型相当。

Conclusion: SEA通过创新的数据生成、训练策略和增强方法，显著提升了计算机使用代理的性能，并将开源模型和代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [19] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究探讨了基于生成式AI的职业目标个性化学习内容如何提升学习者的参与度、满意度和效率。实验表明，个性化内容显著提高了学习时长和满意度，并略微缩短了学习时间。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在数字学习环境中的普及，个性化学习内容以匹配学习者的职业目标，有望提升学习者的参与度和长期动力。

Method: 采用混合方法实验，涉及4000多名学习者，分为接受职业目标个性化内容组和对照组。

Result: 定量结果显示个性化内容组的学习时长增加、满意度提高、学习时间略微减少；定性分析表明学习者认为个性化内容更具激励性和实用性。

Conclusion: 研究表明，将教育内容与学习者的职业目标对齐具有重要价值，AI个性化可有效连接学术知识与职场应用。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [20] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT框架通过知识图谱和可执行代码提升复杂推理任务表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务（如数学推理和代码生成）中表现不佳，需要改进。

Method: 提出KGA-ECoT框架，结合知识图谱（GraphRAG）和可执行代码，分解问题为结构化任务图并生成可验证代码。

Result: 在多个数学推理基准测试中显著优于现有方法，准确率提升数个百分点至超过十个百分点。

Conclusion: KGA-ECoT是一个强大且泛化性强的框架，适用于复杂数学推理任务。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [21] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR是一个自优化的地理推理框架，通过嵌入地理原则（如Tobler第一地理定律）和多智能体协作，提升大语言模型在地理问题中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在地理问题中的空间一致性、多跳推理和地理偏差等挑战。

Method: 提出GeoSR框架，包含变量选择、点选择和优化三个智能体，通过迭代优化结合空间依赖性和变量关系。

Result: 实验表明GeoSR在物理和社会经济预测任务中优于标准提示策略。

Conclusion: GeoSR通过整合地理统计先验和空间结构化推理，提高了地理预测的准确性和公平性。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [22] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 论文提出了一种名为语义熵的指标，用于衡量AI评分系统对同一学生回答生成的不同解释的多样性，以此作为人类评分者分歧的代理。实验表明语义熵与评分者分歧相关，并能反映学科差异和任务结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统无法有效表达评分决策的不确定性或争议性，因此需要一种透明且可信的指标来支持AI辅助评分。

Method: 通过聚类GPT-4生成的解释，基于蕴含相似性计算语义熵，量化解释多样性，而不依赖最终评分。

Result: 实验证明语义熵与人类评分者分歧相关，能区分不同学科，并对需要解释性推理的任务更敏感。

Conclusion: 语义熵是一种可解释的不确定性信号，有助于提升AI辅助评分的透明度和可信度。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [23] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出了一种组合式即时合成框架，结合了DFA构建和游戏求解的优势，适用于大型LTLf公式。


<details>
  <summary>Details</summary>
Motivation: 现有技术在DFA构建和游戏求解之间缺乏优势，无法高效处理大型LTLf公式。

Method: 开发了一种组合式即时合成框架，在游戏求解过程中进行组合，支持两种变体：组合前剪枝和组合中剪枝。

Result: 该框架能够解决其他求解器无法处理的实例，两种变体各有优势。

Conclusion: 组合式即时合成框架显著提升了处理大型LTLf公式的能力。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [24] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE是一种基于代理的框架，通过迭代检索和多步推理动态构建知识图谱三元组，无需训练即可显著优于现有方法，尤其适用于新兴实体。


<details>
  <summary>Details</summary>
Motivation: 解决开放域知识图谱补全中新兴实体信息不足的问题，现有方法依赖预训练模型或单步检索，难以捕捉最新信息。

Method: 结合迭代检索和多步推理的动态框架AgREE，无需训练数据。

Result: AgREE在构建知识图谱三元组上优于现有方法13.7%，尤其适用于未见过的实体。

Conclusion: AgREE展示了代理推理与信息检索结合在动态环境中维护知识图谱的有效性。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [25] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识驱动和数据驱动的方法，用于提升AI代理在临时团队协作中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法需要大量标注数据、缺乏透明度且难以快速更新知识，导致临时团队协作效率低下。

Method: 通过非单调逻辑推理，结合领域常识知识、快速学习的行为预测模型和基于基础模型的未来目标预测。

Result: 在VirtualHome仿真环境中验证了该架构的有效性。

Conclusion: 结合知识驱动和数据驱动的方法能显著提升临时团队协作的效率和适应性。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [26] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD是一种新型电路感知SAT求解框架，利用GNN计算电路级条件概率，显著提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统CSAT求解方法将电路转换为CNF并丢弃结构信息，导致性能不佳。

Method: CASCAD通过GNN建模门级条件概率，动态指导CDCL启发式策略。

Result: 在LEC基准测试中，CASCAD比CNF方法快10倍，并通过概率引导子句过滤进一步减少23.5%运行时间。

Conclusion: 保留电路结构信息对提升SAT求解效率和EDA工具设计至关重要。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [27] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio是一个理论支持的参数高效生物医学推理框架，通过正交梯度空间防止能力干扰，实现多能力集成。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域AI对齐中多能力集成的挑战，确保安全部署。

Method: 1. MKGSG（医学知识基础合成生成）扩展Source2Synth，结合临床工作流约束和医学本体验证；2. 能力感知组相对策略优化，通过混合奖励权重保持强化学习中的正交性。

Result: 在参数类别中取得最佳结果：领域专业知识（80.95% BIOMED-MMLU）、推理（61.94%）、指令遵循（67.95%）和集成（86.7%）。实际部署减少78%成本，提高23%诊断准确率。

Conclusion: BalancedBio为生物医学AI对齐提供了理论支持的方法，实现高效、安全、可靠的推理。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [28] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出了一种合成POMDP的理论框架和方法，用于详细评估记忆增强强化学习算法，并提供了可定制难度的环境。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对记忆模型挑战程度的可控性，而合成环境能更精细地评估记忆增强RL。

Method: 基于Memory Demand Structure（MDS）和线性过程动力学等方法，构建具有预定义属性的POMDP。

Result: 开发了一系列难度递增的POMDP环境，并验证了其有效性。

Conclusion: 研究为分析、设计POMDP环境提供了指导，并支持RL任务中记忆模型的选择。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [29] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 论文提出了一种名为DRN的新方法，通过将逻辑推理从概率最大化转为不确定性最小化，解决了大语言模型在语义启发式与决定性证据冲突时的逻辑推理问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逻辑推理中容易陷入认知陷阱，即语义启发式与决定性证据冲突时表现不佳。

Method: DRN通过追踪信念状态和量化竞争假设的认知不确定性，采用迭代证据合成过程，实现了内在可解释性。验证包括定制判别模型和轻量级验证模块。

Result: 在LCR-1000基准测试中，DRN比基线提升15.2%；与Mistral-7B集成后，在最具挑战性问题上的准确率从20%提升至80%。

Conclusion: DRN展示了零样本泛化能力，可作为可信AI系统的基础组件。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [30] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态基准测试，旨在评估动态交互世界中智能体的跨模态推理能力，揭示了现有模型在高保真记忆任务中表现优异，但在需要推理和规划的任务中存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法全面测试多模态模型在动态交互环境中的智能表现，尤其是忽略了听觉和时间线索。

Method: OmniPlay包含五个游戏环境，系统地设计协同和冲突场景，迫使智能体进行真正的跨模态推理。

Result: 评估显示，领先的多模态模型在高保真记忆任务中表现超人类，但在推理和规划任务中存在严重缺陷，且移除部分感官信息可能意外提升性能。

Conclusion: 研究指出，实现稳健的通用人工智能需要超越规模扩展，专注于协同融合机制的研究。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [31] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本文提出SLP测试框架，通过三个标准评估AI系统是否具备类似意识的接口表示。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统是否具备意识这一争议性问题，将其转化为可实证测试的形式。

Method: 引入SLP测试（主观-语言、潜在-涌现、现象学-结构），利用范畴论建模接口表示。

Result: SLP测试将主观体验操作化为功能接口，而非物理系统的固有属性。

Conclusion: 该框架为AI意识研究提供了实证路径。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [32] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: 论文提出了一种基于强化学习的GUI视觉定位方法GuirlVG，通过系统实证研究和新型稳定技术，显著优于传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统GUI视觉定位依赖监督微调，需大量数据和训练成本；随着多模态大模型的进步，其必要性受到质疑。强化微调虽具潜力，但应用方式未明确。

Method: 提出GuirlVG方法，分解强化微调核心组件并优化，引入动态稳定的Adversarial KL Factor，探索训练配置。

Result: 仅用5.2K样本，GuirlVG在多个数据集上超越需10M样本的监督微调方法，性能提升显著。

Conclusion: GuirlVG展示了强化微调在GUI视觉定位中的高效性，为未来研究提供了新方向。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [33] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: D2Snap是一种新型DOM降采样算法，通过GPT-4o后端评估，其性能接近或优于基于GUI快照的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于GUI快照的Web代理存在视觉输入限制，DOM快照虽结构更优但输入令牌量过大，因此需要一种高效的DOM降采样方法。

Method: 提出D2Snap算法，对DOM快照进行降采样，并在Online-Mind2Web数据集上通过GPT-4o评估其性能。

Result: D2Snap降采样后的DOM快照成功率（67%）与GUI快照基线（65%）相当，且在更高令牌量配置下性能提升8%。

Conclusion: DOM层次结构对LLM具有强特征表示能力，D2Snap为高效Web代理提供了可行方案。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [34] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct工具通过模拟新手与专家对话，解决了高质量教学对话数据稀缺的问题，并展示了LLM在生成教学对话中的潜力。


<details>
  <summary>Details</summary>
Motivation: 高质量的多轮教学对话对AI教学系统至关重要，但数据稀缺。SimInstruct旨在通过模拟新手与专家对话解决这一问题。

Method: SimInstruct利用LLM模拟新手教师，结合人类专家的多轮反馈，生成教学对话。

Result: 生成的对话在教学质量上与真实对话相当，专家反馈积极。LLaMA模型在指令质量上优于GPT-4o。

Conclusion: SimInstruct为教学对话数据生成提供了可扩展方案，并揭示了GPT-4o在教学支持中的不足。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [35] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 论文提出Meta-cognitive Reasoning Framework (MERA)，通过分离推理与控制组件，优化大型推理模型的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中缺乏内在调控机制，导致过度推理和计算资源浪费，限制了实际应用。

Method: MERA框架将推理过程分解为推理与控制组件，采用接管式数据构建机制、结构化推理-控制分离和监督微调，并结合Control-Segment Policy Optimization (CSPO)优化控制行为学习。

Result: 实验表明，MERA训练的模型在多个推理基准测试中提高了推理效率和准确性。

Conclusion: MERA通过显式分离推理与控制，有效解决了LRMs的过度推理问题，提升了模型的实用性和性能。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [36] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型的OS Agents，探讨其组成、能力、构建方法、评估标准及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实现类似J.A.R.V.I.S的AI助手，通过OS Agents在操作系统环境中自动化任务。

Method: 分析OS Agents的关键组件（环境、观察空间、动作空间）、能力（理解、规划、落地）及构建方法（领域专用基础模型和代理框架）。

Result: 总结了OS Agents的评估协议和基准测试，并提出了当前挑战和未来方向（如安全、隐私、个性化）。

Conclusion: 本文为OS Agents研究提供了全面综述，指导学术与工业发展，并维护开源资源促进创新。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [37] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的可解释、可解释的偏见检测方法，基于保护特征值在个体及其邻域中的辩论，结合形式化和计算论证技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的广泛应用，防止数据或模型中的偏见对特定群体造成系统性不利影响变得至关重要。现有方法多忽略透明度，而可解释性和可解释性对算法公平性尤为重要。

Method: 提出一种基于辩论的偏见检测方法，利用保护特征值在个体及其邻域中的辩论，结合形式化和计算论证技术。

Result: 通过形式化、定量和定性评估，该方法在性能、可解释性和可解释性方面优于基线。

Conclusion: 该方法为算法公平性提供了一种透明且有效的解决方案，强调了可解释性和可解释性的重要性。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [38] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 论文介绍了SID基准，用于评估LLM在多轮跨学科苏格拉底对话中的高阶指导能力，发现当前LLM在引导学生知识整合与迁移方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现代教育核心目标是培养学生复杂问题解决中的知识整合与迁移能力，跨学科STEM是关键途径，但需要难以规模化的专家指导。LLM虽有潜力，但缺乏有效评估基准。

Method: 提出SID基准，包括大规模数据集（10,000对话轮次）、新注释框架和评估指标（如X-SRG），并通过基线实验验证LLM表现。

Result: 实验表明，即使最先进的LLM也难以有效引导学生实现知识整合与迁移。

Conclusion: SID基准对推动更具教学意识的LLM发展具有重要价值。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [39] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 论文提出了ConfProBench，首个全面评估多模态大语言模型（MLLMs）在推理步骤中置信度可靠性的基准，通过三种对抗性扰动和三个新指标进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注推理步骤的正确性分类和搜索，而忽略了置信度可靠性这一关键问题，因此需要填补这一空白。

Method: 构建三种对抗性扰动（同义词替换、句法变换、图像扰动）和三个新指标（CRS、CSS、CCS）来评估MPJs的置信度可靠性。

Result: 实验揭示了当前MPJs在置信度表现上的局限性，并提供了未来研究的竞争基线。

Conclusion: ConfProBench为评估和改进MPJs的置信度可靠性提供了系统工具，推动了多模态推理能力的发展。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [40] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体协作优化方法MAGRPO，用于解决LLM在协作任务中的问题，实验表明其能高效生成高质量响应。


<details>
  <summary>Details</summary>
Motivation: 现有LLM独立预训练且未针对协作优化，个体奖励设计复杂，难以促进多智能体协作。

Method: 将LLM协作建模为合作型多智能体强化学习问题，提出MAGRPO算法。

Result: 实验证明MAGRPO能高效生成高质量协作响应。

Conclusion: MAGRPO为LLM协作提供了新思路，并揭示了相关挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [41] [Suggest, Complement, Inspire: Story of Two Tower Recommendations at Allegro.com](https://arxiv.org/abs/2508.03702)
*Aleksandra Osowska-Kurczab,Klaudia Nazarko,Mateusz Marzec,Lidia Wojciechowska,Eliška Kremeňová*

Main category: cs.IR

TL;DR: 论文提出了一种统一的内容推荐系统，解决了大规模电商推荐系统中的三个关键技术挑战：通用架构设计、降低维护成本和管理动态产品目录。


<details>
  <summary>Details</summary>
Motivation: 解决电商推荐系统中的通用性、维护成本和动态产品目录管理问题。

Method: 基于双塔检索框架，利用文本和结构化属性表示产品，并通过近似最近邻搜索实现高效检索。

Result: 通过A/B测试验证了在桌面和移动应用渠道中显著提升了用户参与度和盈利指标。

Conclusion: 灵活的架构设计能够以最小维护成本满足多样化的用户需求。

Abstract: Building large-scale e-commerce recommendation systems requires addressing
three key technical challenges: (1) designing a universal recommendation
architecture across dozens of placements, (2) decreasing excessive maintenance
costs, and (3) managing a highly dynamic product catalogue. This paper presents
a unified content-based recommendation system deployed at Allegro.com, the
largest e-commerce platform of European origin. The system is built on a
prevalent Two Tower retrieval framework, representing products using textual
and structured attributes, which enables efficient retrieval via Approximate
Nearest Neighbour search. We demonstrate how the same model architecture can be
adapted to serve three distinct recommendation tasks: similarity search,
complementary product suggestions, and inspirational content discovery, by
modifying only a handful of components in either the model or the serving
logic. Extensive A/B testing over two years confirms significant gains in
engagement and profit-based metrics across desktop and mobile app channels. Our
results show that a flexible, scalable architecture can serve diverse user
intents with minimal maintenance overhead.

</details>


### [42] [Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective](https://arxiv.org/abs/2508.03703)
*Yubo Wang,Min Tang,Nuo Shen,Shujie Cui,Weiqing Wang*

Main category: cs.IR

TL;DR: 研究发现基于大语言模型（LLM）的推荐系统易受重建攻击，攻击者可通过模型输出重构包含用户隐私的原始提示。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统难以处理冷启动用户或新物品，LLM推荐系统虽有效但存在隐私漏洞。

Method: 采用vec2text框架并优化为相似性引导细化方法，从模型输出重构文本提示。

Result: 实验显示能恢复65%用户交互物品，87%情况下准确推断年龄和性别。

Conclusion: LLM推荐系统存在严重隐私漏洞，需进一步研究防护措施。

Abstract: The large language model (LLM) powered recommendation paradigm has been
proposed to address the limitations of traditional recommender systems, which
often struggle to handle cold start users or items with new IDs. Despite its
effectiveness, this study uncovers that LLM empowered recommender systems are
vulnerable to reconstruction attacks that can expose both system and user
privacy. To examine this threat, we present the first systematic study on
inversion attacks targeting LLM empowered recommender systems, where
adversaries attempt to reconstruct original prompts that contain personal
preferences, interaction histories, and demographic attributes by exploiting
the output logits of recommendation models. We reproduce the vec2text framework
and optimize it using our proposed method called Similarity Guided Refinement,
enabling more accurate reconstruction of textual prompts from model generated
logits. Extensive experiments across two domains (movies and books) and two
representative LLM based recommendation models demonstrate that our method
achieves high fidelity reconstructions. Specifically, we can recover nearly 65
percent of the user interacted items and correctly infer age and gender in 87
percent of the cases. The experiments also reveal that privacy leakage is
largely insensitive to the victim model's performance but highly dependent on
domain consistency and prompt complexity. These findings expose critical
privacy vulnerabilities in LLM empowered recommender systems.

</details>


### [43] [I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation](https://arxiv.org/abs/2508.04247)
*Huilin Chen,Miaomiao Cai,Fan Liu,Zhiyong Cheng,Richang Hong,Meng Wang*

Main category: cs.IR

TL;DR: 论文提出了一种名为I³-MRec的新方法，通过不变学习和信息瓶颈原则处理多模态推荐系统中的缺失模态问题，提升了模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统（MRS）在实际应用中常因模态缺失（如图像、描述不完整）导致性能下降，现有模型对此缺乏鲁棒性。

Method: I³-MRec结合不变风险最小化（IRM）和信息瓶颈（IB）原则，确保跨模态偏好一致性和紧凑有效的模态表示。

Result: 在三个真实数据集上的实验表明，I³-MRec在多种模态缺失场景下均优于现有方法。

Conclusion: I³-MRec有效解决了模态缺失问题，提升了推荐系统的实用性和鲁棒性。

Abstract: Multimodal recommender systems (MRS) improve recommendation performance by
integrating diverse semantic information from multiple modalities. However, the
assumption of the availability of all modalities rarely holds in practice due
to missing images, incomplete descriptions, or inconsistent user content. These
challenges significantly degrade the robustness and generalization capabilities
of current models. To address these challenges, we introduce a novel method
called \textbf{I$^3$-MRec}, which uses \textbf{I}nvariant learning with
\textbf{I}nformation bottleneck principle for \textbf{I}ncomplete
\textbf{M}odality \textbf{Rec}ommendation. To achieve robust performance in
missing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)
cross-modal preference invariance, which ensures consistent user preference
modeling across varying modality environments, and (ii) compact yet effective
modality representation, which filters out task-irrelevant modality information
while maximally preserving essential features relevant to recommendation. By
treating each modality as a distinct semantic environment, I$^3$-MRec employs
invariant risk minimization (IRM) to learn modality-specific item
representations. In parallel, a missing-aware fusion module grounded in the
Information Bottleneck (IB) principle extracts compact and effective item
embeddings by suppressing modality noise and preserving core user preference
signals. Extensive experiments conducted on three real-world datasets
demonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art
MRS methods across various modality-missing scenarios, highlighting its
effectiveness and robustness in practical applications. The code and processed
datasets are released at https://github.com/HuilinChenJN/I3-MRec.

</details>


### [44] [Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study](https://arxiv.org/abs/2508.03710)
*Rafael Salinas-Buestan,Otto Parra,Nelly Condori-Fernandez,Maria Fernanda Granda*

Main category: cs.IR

TL;DR: 研究评估了五种生成式AI工具在推荐非数字活动时的表现和用户满意度，重点关注精准度、召回率、F1分数、MCC分数及用户满意度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在健康行为干预中的应用效果尚未充分探索，尤其是减少技术使用的场景。

Method: 采用GQM范式，通过生成式AI工具基于预定义用户画像和干预场景推荐离线活动，评估定量性能（如精准度）和定性方面（如用户满意度）。

Result: 研究回答了RQ1（哪种工具推荐最准确）和RQ2（工具选择如何影响用户满意度）。

Conclusion: 研究填补了生成式AI在健康行为干预中的空白，为工具选择和优化提供了依据。

Abstract: Background: Generative AI tools have become increasingly relevant in
supporting personalized recommendations across various domains. However, their
effectiveness in health-related behavioral interventions, especially those
aiming to reduce the use of technology, remains underexplored. Aims: This study
evaluates the performance and user satisfaction of the five most widely used
generative AI tools when recommending non-digital activities tailored to
individuals at risk of repetitive strain injury. Method: Following the
Goal/Question/Metric (GQM) paradigm, this proposed experiment involves
generative AI tools that suggest offline activities based on predefined user
profiles and intervention scenarios. The evaluation is focused on quantitative
performance (precision, recall, F1-score and MCC-score) and qualitative aspects
(user satisfaction and perceived recommendation relevance). Two research
questions were defined: RQ1 assessed which tool delivers the most accurate
recommendations, and RQ2 evaluated how tool choice influences user
satisfaction.

</details>


### [45] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 提出了一种基于语言模型的系统，用于从社交媒体内容中检测和分类房地产相关事件，支持城市管理的及时数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台如Twitter和Facebook已成为日常生活的重要组成部分，为识别房地产相关问题提供了宝贵资源，尤其是在城市人口增长的背景下。

Method: 采用分层分类框架，先筛选相关帖子，再将其分类为可操作的房地产相关主题；对于缺乏明确地理标签的帖子，使用基于Transformer的地理定位模块推断位置。

Result: 系统能够提供及时的数据驱动见解，支持城市管理、操作响应和情境感知。

Conclusion: 该集成方法为城市管理提供了有效的工具，能够从社交媒体中提取有价值的房地产相关信息。

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [46] [Measuring the stability and plasticity of recommender systems](https://arxiv.org/abs/2508.03941)
*Maria João Lavoura,Robert Jungnickel,João Vinagre*

Main category: cs.IR

TL;DR: 论文提出了一种评估推荐模型在重新训练时的稳定性和可塑性（适应变化能力）的方法，并通过实验展示了不同算法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的离线评估协议仅能反映模型在特定时间点的性能，无法捕捉模型随时间演变的动态行为。

Method: 设计了一种离线评估协议，用于分析模型在重新训练时的稳定性和可塑性，并应用于GoodReads数据集。

Result: 初步结果显示不同算法在稳定性和可塑性上表现不同，且两者可能存在权衡关系。

Conclusion: 提出的框架有助于理解推荐模型的长期动态行为，但需进一步实验验证。

Abstract: The typical offline protocol to evaluate recommendation algorithms is to
collect a dataset of user-item interactions and then use a part of this dataset
to train a model, and the remaining data to measure how closely the model
recommendations match the observed user interactions. This protocol is
straightforward, useful and practical, but it only captures performance of a
particular model trained at some point in the past. We know, however, that
online systems evolve over time. In general, it is a good idea that models
reflect such changes, so models are frequently retrained with recent data. But
if this is the case, to what extent can we trust previous evaluations? How will
a model perform when a different pattern (re)emerges? In this paper we propose
a methodology to study how recommendation models behave when they are
retrained. The idea is to profile algorithms according to their ability to, on
the one hand, retain past patterns -- stability -- and, on the other hand,
(quickly) adapt to changes -- plasticity. We devise an offline evaluation
protocol that provides detail on the long-term behavior of models, and that is
agnostic to datasets, algorithms and metrics. To illustrate the potential of
this framework, we present preliminary results of three different types of
algorithms on the GoodReads dataset that suggest different stability and
plasticity profiles depending on the algorithmic technique, and a possible
trade-off between stability and plasticity.Although additional experiments will
be necessary to confirm these observations, they already illustrate the
usefulness of the proposed framework to gain insights on the long term dynamics
of recommendation models.

</details>


### [47] [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
*Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Zhichao Xu,Zhan Su,Jian-Yun Nie*

Main category: cs.IR

TL;DR: ConvMix是一个混合标准框架，用于增强对话密集检索，通过大型语言模型扩展数据标注，解决数据稀缺问题，并在多个基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决对话搜索中因上下文依赖查询导致的数据稀缺问题，提升检索性能。

Method: 提出ConvMix框架，结合两阶段相关性判断增强和多样化样本生成，利用大型语言模型进行数据扩展。

Result: 在五个基准测试中，ConvMix训练的检索器表现优于现有基线方法。

Conclusion: ConvMix通过数据增强和质量控制机制，显著提升了对话密集检索的效果。

Abstract: Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

</details>


### [48] [Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04032)
*Qian Yong,Yanhui Li,Jialiang Shi,Yaguang Dou,Tian Qi*

Main category: cs.IR

TL;DR: 提出了一种利用大语言模型（LLM）动态构建用户知识图谱的方法，以提升推荐系统的意外性，并通过两阶段框架（兴趣推理和近线适应）在工业推荐系统中实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中的反馈循环导致内容同质化、过滤气泡效应和用户满意度下降，而LLM在意外性推荐中展现出潜力，但其推理过程的合理性、结果的有用性以及延迟问题仍需解决。

Method: 采用两阶段框架：1）两跳兴趣推理，利用LLM动态构建用户知识图谱并进行推理；2）近线适应，低成本部署模型，结合u2i和i2i检索能力。

Result: 在Dewu应用上的实验表明，该方法显著提升了曝光新颖率、点击新颖率、平均观看时长等指标，改善了用户体验。

Conclusion: 该方法有效解决了LLM在推荐系统中的挑战，提升了推荐系统的意外性和用户满意度。

Abstract: The feedback loop in industrial recommendation systems reinforces homogeneous
content, creates filter bubble effects, and diminishes user satisfaction.
Recently, large language models(LLMs) have demonstrated potential in
serendipity recommendation, thanks to their extensive world knowledge and
superior reasoning capabilities. However, these models still face challenges in
ensuring the rationality of the reasoning process, the usefulness of the
reasoning results, and meeting the latency requirements of industrial
recommendation systems (RSs). To address these challenges, we propose a method
that leverages llm to dynamically construct user knowledge graphs, thereby
enhancing the serendipity of recommendation systems. This method comprises a
two stage framework:(1) two-hop interest reasoning, where user static profiles
and historical behaviors are utilized to dynamically construct user knowledge
graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy
of LLM reasoning results, is then performed on the constructed graphs to
identify users' potential interests; and(2) Near-line adaptation, a
cost-effective approach to deploying the aforementioned models in industrial
recommendation systems. We propose a u2i (user-to-item) retrieval model that
also incorporates i2i (item-to-item) retrieval capabilities, the retrieved
items not only exhibit strong relevance to users' newly emerged interests but
also retain the high conversion rate of traditional u2i retrieval. Our online
experiments on the Dewu app, which has tens of millions of users, indicate that
the method increased the exposure novelty rate by 4.62%, the click novelty rate
by 4.85%, the average view duration per person by 0.15%, unique visitor click
through rate by 0.07%, and unique visitor interaction penetration by 0.30%,
enhancing user experience.

</details>


### [49] [Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation](https://arxiv.org/abs/2508.04145)
*Teng Shi,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: GSERec利用用户-代码图和消息传递解决搜索增强推荐中的数据稀疏问题，通过LLMs生成离散代码连接相似用户，提升稀疏搜索行为用户的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖丰富的搜索交互，仅对少数搜索行为频繁的用户有效，而对大多数搜索行为稀疏的用户效果有限。

Method: 提出GSERec，基于用户-代码图的消息传递机制，利用LLMs生成离散代码连接相似用户，并引入对比损失优化用户相似性建模。

Result: 在三个真实数据集上，GSERec表现优于基线方法，尤其对搜索行为稀疏的用户效果显著。

Conclusion: GSERec通过消息传递和对比损失有效缓解数据稀疏问题，提升搜索增强推荐的性能。

Abstract: In modern online platforms, search and recommendation (S&R) often coexist,
offering opportunities for performance improvement through search-enhanced
approaches. Existing studies show that incorporating search signals boosts
recommendation performance. However, the effectiveness of these methods relies
heavily on rich search interactions. They primarily benefit a small subset of
users with abundant search behavior, while offering limited improvements for
the majority of users who exhibit only sparse search activity. To address the
problem of sparse search data in search-enhanced recommendation, we face two
key challenges: (1) how to learn useful search features for users with sparse
search interactions, and (2) how to design effective training objectives under
sparse conditions. Our idea is to leverage the features of users with rich
search interactions to enhance those of users with sparse search interactions.
Based on this idea, we propose GSERec, a method that utilizes message passing
on the User-Code Graphs to alleviate data sparsity in Search-Enhanced
Recommendation. Specifically, we utilize Large Language Models (LLMs) with
vector quantization to generate discrete codes, which connect similar users and
thereby construct the graph. Through message passing on this graph, embeddings
of users with rich search data are propagated to enhance the embeddings of
users with sparse interactions. To further ensure that the message passing
captures meaningful information from truly similar users, we introduce a
contrastive loss to better model user similarities. The enhanced user
representations are then integrated into downstream search-enhanced
recommendation models. Experiments on three real-world datasets show that
GSERec consistently outperforms baselines, especially for users with sparse
search behaviors.

</details>


### [50] [Bridging Search and Recommendation through Latent Cross Reasoning](https://arxiv.org/abs/2508.04152)
*Teng Shi,Weicong Qin,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: 提出了一种潜在交叉推理框架，通过结合搜索和推荐历史来提升推荐性能，利用对比学习和强化学习优化结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能明确区分有用的搜索行为，导致推荐性能下降，因此需要一种更有效的方法来利用搜索历史。

Method: 设计潜在交叉推理框架，先编码用户搜索和推荐历史以捕捉全局兴趣，再迭代推理搜索行为提取有用信号，结合对比学习和强化学习优化。

Result: 在公开基准测试中表现优于基线，验证了推理对提升搜索感知推荐的重要性。

Conclusion: 潜在交叉推理框架能有效利用搜索行为提升推荐性能，为搜索感知推荐提供了新思路。

Abstract: Search and recommendation (S&R) are fundamental components of modern online
platforms, yet effectively leveraging search behaviors to improve
recommendation remains a challenging problem. User search histories often
contain noisy or irrelevant signals that can even degrade recommendation
performance, while existing approaches typically encode S&R histories either
jointly or separately without explicitly identifying which search behaviors are
truly useful. Inspired by the human decision-making process, where one first
identifies recommendation intent and then reasons about relevant evidence, we
design a latent cross reasoning framework that first encodes user S&R histories
to capture global interests and then iteratively reasons over search behaviors
to extract signals beneficial for recommendation. Contrastive learning is
employed to align latent reasoning states with target items, and reinforcement
learning is further introduced to directly optimize ranking performance.
Extensive experiments on public benchmarks demonstrate consistent improvements
over strong baselines, validating the importance of reasoning in enhancing
search-aware recommendation.

</details>


### [51] [SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval](https://arxiv.org/abs/2508.04162)
*Ruyin Li,Xiaoyu Chen*

Main category: cs.IR

TL;DR: SSEmb是一种新型嵌入框架，通过结合结构和语义特征提升数学公式检索性能，在ARQMath-3任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数学信息检索中公式检索是关键，现有方法未能充分结合结构和语义特征。

Method: 使用图对比学习编码公式的结构特征，引入图数据增强策略；结合Sentence-BERT编码公式的语义特征；通过加权融合结构和语义相似度。

Result: 在ARQMath-3任务中，SSEmb在P'@10和nDCG'@10上优于现有方法5个百分点以上。

Conclusion: SSEmb通过结合结构和语义特征，显著提升了公式检索性能，并与其他方法结合达到最优效果。

Abstract: Formula retrieval is an important topic in Mathematical Information
Retrieval. We propose SSEmb, a novel embedding framework capable of capturing
both structural and semantic features of mathematical formulas. Structurally,
we employ Graph Contrastive Learning to encode formulas represented as Operator
Graphs. To enhance structural diversity while preserving mathematical validity
of these formula graphs, we introduce a novel graph data augmentation approach
through a substitution strategy. Semantically, we utilize Sentence-BERT to
encode the surrounding text of formulas. Finally, for each query and its
candidates, structural and semantic similarities are calculated separately and
then fused through a weighted scheme. In the ARQMath-3 formula retrieval task,
SSEmb outperforms existing embedding-based methods by over 5 percentage points
on P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs
of other methods and achieves state-of-the-art results when combined with
Approach0.

</details>


### [52] [ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation](https://arxiv.org/abs/2508.04206)
*Fatemeh Nazary,Ali Tourani,Yashar Deldjoo,Tommaso Di Noia*

Main category: cs.IR

TL;DR: ViLLA-MMBench是一个可扩展的多模态电影推荐基准，结合视觉、音频和文本模态，利用LLM增强数据，支持多种融合方法和评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注单一模态或简单融合，缺乏对多模态联合建模的支持，尤其是在冷启动和覆盖率方面表现不足。

Method: 基于MovieLens和MMTF-14K数据集，通过LLM（如OpenAI Ada）自动生成高质量电影摘要，支持多种嵌入编码器和融合策略（早期、中期、晚期融合）。

Result: 实验表明，LLM增强和强文本嵌入显著提升了冷启动和覆盖率，尤其是与音频-视觉特征融合时。

Conclusion: ViLLA-MMBench为可复现的多模态推荐研究提供了开源工具，推动了生成式AI在大规模推荐中的集成。

Abstract: Recommending long-form video content demands joint modeling of visual, audio,
and textual modalities, yet most benchmarks address only raw features or narrow
fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for
LLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,
it aligns dense item embeddings from three modalities: audio (block-level,
i-vector), visual (CNN, AVF), and text. Missing or sparse metadata is
automatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),
generating high-quality synopses for thousands of movies. All text (raw or
augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),
producing multiple ready-to-use sets. The pipeline supports interchangeable
early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and
multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are
fully declarative via a single YAML file. Evaluation spans accuracy (Recall,
nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,
diversity, fairness. Results show LLM-based augmentation and strong text
embeddings boost cold-start and coverage, especially when fused with
audio-visual features. Systematic benchmarking reveals universal versus
backbone- or metric-specific combinations. Open-source code, embeddings, and
configs enable reproducible, fair multimodal RS research and advance principled
generative AI integration in large-scale recommendation. Code:
https://recsys-lab.github.io/ViLLA-MMBench

</details>


### [53] [Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains](https://arxiv.org/abs/2508.04221)
*Joey De Pauw,Bart Goethals*

Main category: cs.IR

TL;DR: 论文探讨了如何在因子分解推荐模型中显式编码时间，提出了一种连续时间编码机制，并通过实验验证了时间建模的有效性，但发现简单的后处理流行度调整通常足以达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统通常忽视时间维度或仅简单处理，而显式建模时间可以更好地捕捉用户偏好和物品感知的变化。

Method: 提出了一种完全连续的时间编码机制，通过多项式拟合避免离散化，并在三个真实数据集上进行了比较研究。

Result: 实验表明显式建模时间能有效捕捉时间信号，但简单的后处理流行度调整通常表现最佳。

Conclusion: 对于推荐任务，预测未来比捕捉过去趋势更重要，需要专门机制进行未来数据的外推。

Abstract: Recommender systems learn from past user behavior to predict future user
preferences. Intuitively, it has been established that the most recent
interactions are more indicative of future preferences than older interactions.
Many recommendation algorithms use this notion to either drop older
interactions or to assign them a lower weight, so the model can focus on the
more informative, recent information. However, very few approaches model the
flow of time explicitly.
  This paper analyzes how time can be encoded in factorization-style
recommendation models. By including absolute time as a feature, our models can
learn varying user preferences and changing item perception over time. In
addition to simple binning approaches, we also propose a novel, fully
continuous time encoding mechanism. Through the use of a polynomial fit inside
the loss function, our models completely avoid the need for discretization, and
they are able to capture the time dimension in arbitrary resolution.
  We perform a comparative study on three real-world datasets that span
multiple years, where long user histories are present, and items stay relevant
for a longer time. Empirical results show that, by explicitly modeling time,
our models are very effective at capturing temporal signals, such as varying
item popularities over time. Despite this however, our experiments also
indicate that a simple post-hoc popularity adjustment is often sufficient to
achieve the best performance on the unseen test set. This teaches us that, for
the recommendation task, predicting the future is more important than capturing
past trends. As such, we argue that specialized mechanisms are needed for
extrapolation to future data.

</details>


### [54] [Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum](https://arxiv.org/abs/2508.04293)
*Nirmal Gaud,Surej Mouli,Preeti Katiyar,Vaduguru Venkata Ramya*

Main category: cs.IR

TL;DR: NIRMAL是一种新型优化算法，结合了梯度下降、动量、随机扰动、自适应学习率和非线性变换等多种策略，在图像分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提出一种更高效、稳健的优化算法，以提升深度学习模型的性能。

Method: 结合多种策略（如梯度下降、动量等），在多个基准数据集（MNIST、FashionMNIST等）上测试。

Result: 在CIFAR-100上表现优于Adam，接近SGD with Momentum，具有稳健的收敛性和泛化能力。

Conclusion: NIRMAL是一种多功能且有效的优化器，适用于各种深度学习任务。

Abstract: This study proposes NIRMAL (Novel Integrated Robust Multi-Adaptation
Learning), a novel optimization algorithm that combines multiple strategies
inspired by the movements of the chess piece. These strategies include gradient
descent, momentum, stochastic perturbations, adaptive learning rates, and
non-linear transformations. We carefully evaluated NIRMAL against two widely
used and successful optimizers, Adam and SGD with Momentum, on four benchmark
image classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100.
The custom convolutional neural network (CNN) architecture is applied on each
dataset. The experimental results show that NIRMAL achieves competitive
performance, particularly on the more challenging CIFAR-100 dataset, where it
achieved a test accuracy of 45.32\%and a weighted F1-score of 0.4328. This
performance surpasses Adam (41.79\% accuracy, 0.3964 F1-score) and closely
matches SGD with Momentum (46.97\% accuracy, 0.4531 F1-score). Also, NIRMAL
exhibits robust convergence and strong generalization capabilities, especially
on complex datasets, as evidenced by stable training results in loss and
accuracy curves. These findings underscore NIRMAL's significant ability as a
versatile and effective optimizer for various deep learning tasks.

</details>


### [55] [Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics](https://arxiv.org/abs/2508.04419)
*Jarne Mathi Decker,Joeran Beel*

Main category: cs.IR

TL;DR: 论文提出了一种结合用户元特征和算法特征的元学习方法，用于推荐系统中的算法选择问题，实验表明该方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中算法选择问题的挑战，传统方法忽略算法内在特性，而新方法通过显式特征化算法提升性能。

Method: 提出了一种基于用户元特征和从源代码自动提取的算法特征的元学习方法。

Result: 实验显示，加入算法特征后，NDCG@10性能提升8.83%，优于单一最佳算法基线，并缩小了与理论最优选择器的差距。

Conclusion: 静态源代码指标提供了有价值的预测信号，为构建更鲁棒和智能的推荐系统提供了新方向。

Abstract: The Algorithm Selection Problem for recommender systems-choosing the best
algorithm for a given user or context-remains a significant challenge.
Traditional meta-learning approaches often treat algorithms as categorical
choices, ignoring their intrinsic properties. Recent work has shown that
explicitly characterizing algorithms with features can improve model
performance in other domains. Building on this, we propose a per-user
meta-learning approach for recommender system selection that leverages both
user meta-features and automatically extracted algorithm features from source
code. Our preliminary results, averaged over six diverse datasets, show that
augmenting a meta-learner with algorithm features improves its average NDCG@10
performance by 8.83% from 0.135 (user features only) to 0.147. This enhanced
model outperforms the Single Best Algorithm baseline (0.131) and successfully
closes 10.5% of the performance gap to a theoretical oracle selector. These
findings show that even static source code metrics provide a valuable
predictive signal, presenting a promising direction for building more robust
and intelligent recommender systems.

</details>


### [56] [TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04474)
*Xinkui Zhao,Haode Li,Yifan Zhang,Guanjie Cheng,Yueshen Xu*

Main category: cs.IR

TL;DR: TRAIL是一个统一框架，结合推理和动态知识图谱更新，提升大语言模型的适应性和实时知识更新能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）依赖静态参数记忆，限制了其在知识密集型场景中的适应性和准确性。知识图谱（KGs）作为结构化知识库，可以补充LLMs的外部记忆，但现有方法将推理和知识更新分离，导致信息利用不足。

Method: 提出TRAIL框架，通过联合推理和动态知识图谱优化，结合置信度驱动机制生成、验证和修剪新事实，实现知识迭代更新。

Result: 实验表明，TRAIL在多个基准测试中优于现有方法3%至13%，支持持续适应而无需重新训练。

Conclusion: TRAIL为开发具有持续学习和透明推理能力的自适应语言模型迈出了重要一步。

Abstract: Recent advances in large language models (LLMs) have unlocked powerful
reasoning and decision-making capabilities. However, their inherent dependence
on static parametric memory fundamentally limits their adaptability, factual
accuracy, and interpretability in knowledge-intensive scenarios. Knowledge
graphs (KGs), as structured repositories of explicit relational knowledge,
offer a promising approach for augmenting LLMs with external, interpretable
memory. Nevertheless, most existing methods that combine LLMs with KGs treat
reasoning and knowledge updating as separate processes, resulting in suboptimal
utilization of new information and hindering real-time updates. In this work,
we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And
Incremental Learning that couples joint inference and dynamic KG refinement
with large language models. TRAIL enables LLM agents to iteratively explore,
update, and refine knowledge graphs during the reasoning process, employing a
confidence-driven mechanism for the generation, validation, and pruning of new
facts. This plug-and-play architecture facilitates seamless integration with
various LLMs, supporting continual adaptation without the need for retraining.
Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms
existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More
importantly, these results represent a significant step toward developing
adaptive, memory-augmented language models capable of continual learning and
reliable, transparent reasoning.

</details>


### [57] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 论文研究了多模态推荐系统中嵌入表示的作用，提出使用大型视觉语言模型（LVLMs）生成语义对齐的多模态嵌入，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态推荐系统的性能提升是否源于真正的多模态理解还是模型复杂性，并强调嵌入表示的语义信息性。

Method: 利用大型视觉语言模型（LVLMs）通过结构化提示生成多模态嵌入，避免传统方法中模态特定编码器和融合策略的局限性。

Result: 实验表明，LVLMs生成的嵌入显著提升了推荐性能，并能解码为结构化文本描述，验证了其语义深度和对齐性。

Conclusion: 研究强调了语义丰富表示的重要性，并将LVLMs定位为构建多模态推荐系统中鲁棒且有意义的表示的基础。

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [58] [A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature](https://arxiv.org/abs/2508.04612)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.IR

TL;DR: 提出了一种开源、可复现的自动化流程，用于文献检索、筛选、元数据提取、实验复现等任务，验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着自回归生成模型研究的快速发展，手动文献综述和实验复现变得不切实际，需要自动化工具支持。

Method: 开发了一个开源流程，包括文献检索、筛选、元数据提取、主题聚类、摘要生成和实验脚本容器化。

Result: 在50篇标注论文上的评估显示，相关分类、超参数提取和引用识别的F1分数超过0.85；1000篇论文的测试显示近线性扩展性；三个案例研究验证了复现的准确性。

Conclusion: 该流程高效且准确，支持大规模文献分析和实验复现。

Abstract: The accelerating pace of research on autoregressive generative models has
produced thousands of papers, making manual literature surveys and reproduction
studies increasingly impractical. We present a fully open-source, reproducible
pipeline that automatically retrieves candidate documents from public
repositories, filters them for relevance, extracts metadata, hyper-parameters
and reported results, clusters topics, produces retrieval-augmented summaries
and generates containerised scripts for re-running selected experiments.
Quantitative evaluation on 50 manually-annotated papers shows F1 scores above
0.85 for relevance classification, hyper-parameter extraction and citation
identification. Experiments on corpora of up to 1000 papers demonstrate
near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM
on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model
on the Lakh MIDI dataset -- confirm that the extracted settings support
faithful reproduction, achieving test perplexities within 1--3% of the original
reports.

</details>


### [59] [HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs](https://arxiv.org/abs/2508.04618)
*Dengzhao Fang,Jingtong Gao,Chengcheng Zhu,Yu Li,Xiangyu Zhao,Yi Chang*

Main category: cs.IR

TL;DR: HiD-VAE提出了一种分层解缠的生成推荐框架，通过分层监督量化和唯一性损失解决现有方法语义扁平化和表示纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐方法因无监督标记化导致语义ID缺乏层次性和易纠缠，影响推荐准确性和多样性。

Method: HiD-VAE采用分层监督量化对齐多级标签，并引入唯一性损失减少表示重叠。

Result: 在三个公开基准测试中，HiD-VAE表现优于现有方法。

Conclusion: HiD-VAE通过分层解缠表示提升了生成推荐的性能和可解释性。

Abstract: Recommender systems are indispensable for helping users navigate the immense
item catalogs of modern online platforms. Recently, generative recommendation
has emerged as a promising paradigm, unifying the conventional
retrieve-and-rank pipeline into an end-to-end model capable of dynamic
generation. However, existing generative methods are fundamentally constrained
by their unsupervised tokenization, which generates semantic IDs suffering from
two critical flaws: (1) they are semantically flat and uninterpretable, lacking
a coherent hierarchy, and (2) they are prone to representation entanglement
(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.
To overcome these limitations, we propose HiD-VAE, a novel framework that
learns hierarchically disentangled item representations through two core
innovations. First, HiD-VAE pioneers a hierarchically-supervised quantization
process that aligns discrete codes with multi-level item tags, yielding more
uniform and disentangled IDs. Crucially, the trained codebooks can predict
hierarchical tags, providing a traceable and interpretable semantic path for
each recommendation. Second, to combat representation entanglement, HiD-VAE
incorporates a novel uniqueness loss that directly penalizes latent space
overlap. This mechanism not only resolves the critical ID collision problem but
also promotes recommendation diversity by ensuring a more comprehensive
utilization of the item representation space. These high-quality, disentangled
IDs provide a powerful foundation for downstream generative models. Extensive
experiments on three public benchmarks validate HiD-VAE's superior performance
against state-of-the-art methods. The code is available at
https://anonymous.4open.science/r/HiD-VAE-84B2.

</details>


### [60] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: QAM是一种混合框架，通过将开放文本查询分解为结构化元数据标签和语义元素，提升搜索精度和相关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统搜索方法在自由文本查询中的局限性，通过自动提取元数据过滤器减少噪声并聚焦检索相关项。

Method: QAM框架将查询分解为元数据标签和语义元素，结合实验评估使用Amazon Toys Reviews数据集。

Result: QAM在mAP@5上达到52.99%，显著优于BM25、语义搜索、交叉编码器重排及混合搜索方法。

Conclusion: QAM是电子商务等企业搜索应用的强大解决方案。

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


### [61] [Recommendation with Generative Models](https://arxiv.org/abs/2409.15173)
*Yashar Deldjoo,Zhankui He,Julian McAuley,Anton Korikov,Scott Sanner,Arnau Ramisa,Rene Vidal,Maheswaran Sathiamoorthy,Atoosa Kasrizadeh,Silvia Milano,Francesco Ricci*

Main category: cs.IR

TL;DR: 论文探讨了生成模型（如GANs、VAEs和GPT）及其在推荐系统（Gen-RecSys）中的应用，提出了一种分类法，并讨论了其潜在风险和评估框架。


<details>
  <summary>Details</summary>
Motivation: 生成模型在机器学习中日益重要，但其在推荐系统中的应用和分类尚未系统化，需要更全面的理解和评估。

Method: 提出了一种分类法，将深度生成模型（DGMs）分为ID驱动模型、大型语言模型（LLMs）和多模态模型，并分析了各类别的技术进展。

Result: 分类法帮助研究者更好地理解Gen-RecSys的发展，并强调了评估框架的重要性。

Conclusion: 生成模型在推荐系统中具有广阔前景，但需关注潜在风险并建立稳健的评估机制。

Abstract: Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过结合监督对比学习和特权信息学习，提升了情感计算模型从实验室到真实环境的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从实验室环境（in-vitro）到真实环境（in-vivo）迁移的可靠性问题。

Method: 提出Privileged Contrastive Pretraining (PriCon)框架，结合监督对比学习（SCL）和特权信息学习（LUPI）。

Result: 在RECOLA和AGAIN数据集上，PriCon模型表现优于LUPI和端到端模型，接近全模态训练模型的性能。

Conclusion: PriCon为缩小实验室与真实环境情感建模差距提供了可扩展的实用解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [63] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种新型轨迹压缩框架，结合频域物理建模和误差有界优化，支持任意维度轨迹压缩，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有线简化方法通常仅适用于2D轨迹，且忽略时间同步和运动连续性，无法满足高效压缩需求。

Method: PILOT-C通过独立压缩每个空间轴，结合频域物理建模和误差有界优化，支持任意维度轨迹。

Result: 在四个真实数据集上，PILOT-C在压缩比和轨迹保真度上均优于现有方法，3D轨迹压缩比提升49%。

Conclusion: PILOT-C是一种高效、通用的轨迹压缩框架，适用于多维轨迹，性能显著优于现有技术。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [64] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种基于课程强化学习和可验证过程奖励的生成模型，用于CXR任务的多模态推理，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有CXR诊断模型缺乏对推理过程的监督，导致多任务诊断中的推理冗长、奖励稀疏和幻觉问题。

Method: 通过课程强化学习和可验证过程奖励（CuRL-VPR）构建CX-Set数据集，分两阶段优化模型。

Result: CX-Mind在视觉理解、文本生成和时空对齐方面优于现有模型，性能提升25.1%。

Conclusion: CX-Mind在真实临床数据集上表现优异，专家评估证实其临床实用性。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [65] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为Latent Knowledge Scalpel（LKS）的方法，通过轻量级超网络编辑大语言模型（LLMs）中的潜在知识，实现大规模精确编辑，同时保留模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在预训练中可能保留不准确或过时的信息，导致推理时出现错误或偏见。现有编辑方法难以同时编辑大量事实信息，且可能损害模型的通用能力。

Method: 通过轻量级超网络操纵特定实体的潜在知识，实现类似自然语言输入的实体替换。

Result: 在Llama-2和Mistral上的实验表明，即使同时编辑10,000条信息，LKS仍能有效进行知识编辑，同时保持模型的通用能力。

Conclusion: LKS是一种可行的大规模知识编辑方法，能够在不损害模型通用能力的情况下精确编辑LLMs中的知识。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [66] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，结合临床特征、眼底图像和文本描述，用于青光眼风险预测，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测青光眼至关重要，但现有方法依赖单模态数据且缺乏可解释性，限制了临床实用性。

Method: GlaBoost整合临床特征、眼底图像嵌入和专家文本描述，通过增强的XGBoost模型进行分类。

Result: 在真实数据集上验证，GlaBoost准确率达98.71%，特征重要性分析显示临床一致性。

Conclusion: GlaBoost为可解释的青光眼诊断提供了透明且可扩展的解决方案，适用于其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [67] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种新的低秩Tucker表示模型（LRTuckerRep），结合全局低秩和局部平滑先验，通过自适应加权核范数和无参数拉普拉斯正则化，实现了高效的多维数据补全。


<details>
  <summary>Details</summary>
Motivation: 现有方法（全局低秩或局部平滑）存在计算成本高、参数调优复杂或泛化能力差的问题，需要一种统一且高效的方法。

Method: LRTuckerRep模型通过Tucker分解结合自适应加权核范数和无参数拉普拉斯正则化，开发了两种迭代算法。

Result: 实验表明，LRTuckerRep在高缺失率下优于基线方法，具有更高的补全精度和鲁棒性。

Conclusion: LRTuckerRep是一种高效且统一的多维数据补全方法，适用于图像修复和交通数据填补等任务。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [68] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一种利用大语言模型（LLMs）自动化和规模化先验分布设定的框架LLMPrior，并通过联邦算法Fed-LLMPrior实现多智能体系统中的先验聚合。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断中先验分布的设定是一个手动、主观且难以规模化的问题，亟需自动化解决方案。

Method: 通过将LLMs与显式生成模型（如高斯混合模型）结合，设计LLMPrior算子，将非结构化上下文转化为有效概率分布，并扩展至多智能体系统。

Result: 提出了LLMPrior和Fed-LLMPrior算法，能够自动化生成和聚合先验分布，降低贝叶斯建模的门槛。

Conclusion: 该框架为贝叶斯建模提供了一种新的自动化工具，有望推动其广泛应用。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [69] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 论文提出了一种在线分布鲁棒强化学习方法，解决了现有方法依赖生成模型或离线数据集的限制，通过优化最坏情况性能在未知环境中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实部署中因模拟与实际的差距（sim-to-real gap）表现不佳，现有分布鲁棒RL方法依赖生成模型或广泛覆盖的离线数据集，实用性受限。

Method: 研究在线分布鲁棒RL，提出一种基于$f$-散度的计算高效算法，适用于未知训练环境，并具有次线性遗憾保证。

Result: 算法在多种环境中验证了鲁棒性和效率，理论分析显示其接近最优。

Conclusion: 该方法为未知环境中的分布鲁棒RL提供了实用且高效的解决方案。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [70] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 论文分析了GRPO的两大局限性，并提出GTPO作为改进方案，通过跳过冲突令牌的负更新和过滤高熵补全，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在语言模型训练中存在冲突梯度更新和输出分布扁平化的问题，影响模型性能。

Method: 提出GTPO，识别冲突令牌并跳过其负更新，同时过滤高熵补全，避免KL散度正则化。

Result: 在GSM8K、MATH和AIME 2024基准测试中验证了GTPO的稳定性和性能提升。

Conclusion: GTPO解决了GRPO的局限性，提供了一种更稳定有效的策略优化方法。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [71] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的物理信息网络（U-PINet），用于高效且物理一致的电磁散射建模。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，纯数据驱动的深度学习方法缺乏物理约束且需要大量标注数据。

Method: U-PINet采用多尺度处理神经网络架构和物理启发的稀疏图表示，建模近场和远场相互作用。

Result: U-PINet准确预测表面电流分布，显著减少计算时间，并在雷达截面预测任务中表现优异。

Conclusion: U-PINet在计算效率、泛化能力和物理一致性方面优于传统方法和纯数据驱动方法。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [72] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于物理信息的神经网络（PINN）方法，用于加速核聚变装置EAST中的热通量估计，相比传统有限元方法（FEM）实现了40倍的加速，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法（FEM）在热通量估计中计算效率低，难以实现实时模拟，因此需要一种更高效的方法。

Method: 使用物理信息神经网络（PINN），结合空间坐标和时间戳输入，计算边界损失、初始条件损失和物理损失，并通过少量数据点采样提升模型预测能力。

Result: 实验表明，PINN在均匀和非均匀加热条件下均能达到与FEM相当的精度，同时计算效率提升40倍。

Conclusion: PINN是一种高效且准确的热通量估计方法，适用于核聚变装置的实时模拟。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [73] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 提出了一种名为SoilNet的多模态多任务模型，用于解决土壤层次分类的复杂问题，结合图像数据和地理时间元数据，通过模块化流程实现准确分类。


<details>
  <summary>Details</summary>
Motivation: 土壤层次分类对土壤健康监测至关重要，但现有基础模型难以应对其多模态、多任务和复杂层次标签的挑战。

Method: 采用多模态多任务模型，整合图像和元数据，先预测深度标记分割土壤剖面，再提取形态特征，最后基于图表示预测层次标签。

Result: 在真实土壤剖面数据集上验证了方法的有效性。

Conclusion: SoilNet能够有效处理复杂层次分类问题，为土壤健康监测提供了新工具。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [74] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文提出了Bernoulli-LoRA，一种新的理论框架，统一并扩展了现有的LoRA方法，通过概率伯努利机制选择更新的矩阵，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模的指数增长，参数高效微调（PEFT）变得至关重要。尽管LoRA方法在实践中表现出色，但其理论理解有限，需要进一步研究。

Method: 提出了Bernoulli-LoRA框架，引入伯努利机制选择更新的矩阵，并分析了多种变体的收敛性，包括GD、SGD、PAGE等。

Result: 在理论和实验中验证了Bernoulli-LoRA的有效性，提供了收敛保证，并展示了其实际应用效果。

Conclusion: Bernoulli-LoRA为开发理论扎实且实际有效的PEFT方法迈出了重要一步。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [75] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种新的基于神经网络的贝叶斯优化方法，解决了高维和大规模评估时的计算复杂度问题，无需依赖模型不确定性估计，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在高维和大规模评估时面临计算复杂度问题，而神经网络虽具扩展性，但不确定性估计复杂。

Method: 提出SNBO方法，通过独立探索和开发标准添加样本，并自适应控制采样区域。

Result: 在10至102维的优化问题中，SNBO表现优于基线算法，减少40-60%评估次数，运行时间降低一个数量级。

Conclusion: SNBO是一种高效且可扩展的黑盒优化方法，适用于高维问题。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [76] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了一种名为DP-NCB的新算法框架，同时实现差分隐私和公平性，填补了多臂老虎机算法中隐私与公平性兼顾的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机算法中，隐私保护和公平性通常被独立研究，但实际应用中需要同时满足两者。本文旨在填补这一空白。

Method: 提出DP-NCB框架，结合差分隐私和纳什遗憾最小化，适用于全局和局部隐私模型，且无需预知时间范围。

Result: 理论证明DP-NCB在隐私和公平性上均达到最优，仿真实验显示其纳什遗憾显著低于现有基线。

Conclusion: DP-NCB为高影响力应用提供了兼具隐私保护和公平性的多臂老虎机算法设计基础。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [77] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种可部分训练的替代模型VAE-DNN，用于解决参数化非线性偏微分方程的正向和逆向问题，相比FNO和DeepONet更高效且准确。


<details>
  <summary>Details</summary>
Motivation: 现有替代模型和算子学习模型（如FNO和DeepONet）在训练时间和能耗上较高，需要一种更高效的训练方法。

Method: VAE-DNN模型通过独立训练编码器、全连接神经网络和解码器三部分，编码器和解码器分别作为VAE的一部分，降低训练复杂度。

Result: VAE-DNN在非线性扩散方程的正向和逆向解中，比FNO和DeepONet更高效且准确。

Conclusion: VAE-DNN是一种高效且准确的替代模型，适用于参数化非线性偏微分方程的求解。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [78] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 提出了一种基于时空预测框架的频谱需求预测方法，利用用户侧KPI和监管数据，通过特征工程、相关性分析和迁移学习提高预测准确性和跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 频谱需求预测对频谱分配、监管规划和无线通信网络可持续发展至关重要，支持ITU制定公平政策和满足5G、6G及IoT需求。

Method: 结合用户侧KPI和监管数据，采用特征工程、相关性分析和迁移学习构建时空预测框架。

Result: 相比传统ITU模型，该方法预测更准确且具有跨区域泛化能力，实验结果验证了其有效性。

Conclusion: 该方法为政策制定者和监管机构提供了更现实的频谱管理工具，具有实际应用潜力。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [79] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论的智能数据子采样方法，用于离线学习，以减少下游预测的不确定性，并在实验中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 数据流中持续生成新观测值，如何在保持计算成本可控的同时捕捉相关信息是学习模型的关键挑战。

Method: 采用信息论方法，专注于减少下游预测的不确定性，进行智能数据子采样。

Result: 实验表明，这种预测导向的方法在两个广泛研究的问题上优于之前提出的信息论技术。

Conclusion: 实际应用中，可靠的高性能需要精心设计的模型。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [80] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE框架通过智能子采样减少数据量，提升模型精度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放的终结，高效训练需要重新思考数据量。

Method: 提出SICKLE框架，采用最大熵采样方法，并与随机和相空间采样对比。

Result: 在Frontier上验证，子采样可提升模型精度，能耗降低高达38倍。

Conclusion: 智能子采样是高效训练的有效方法。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [81] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架，结合脉冲控制和切换控制，用于1型糖尿病治疗决策，显著降低了血糖水平异常。


<details>
  <summary>Details</summary>
Motivation: 解决1型糖尿病治疗中干预效果的延迟和异质性问题，提升个性化治疗的效果。

Method: 结合脉冲控制（快速干预）和切换控制（长期干预）的强化学习框架，通过约束马尔可夫决策过程实现安全策略学习。

Result: 在模拟任务中，血糖水平异常率从22.4%降至10.8%。

Conclusion: 为未来安全且时间敏感的医疗强化学习奠定了基础，但尚不适用于临床部署。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [82] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习和循环神经网络的混合建模方法，用于提高葡萄物候预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统生物物理模型在精细化管理中精度不足，而深度学习方法因数据稀疏性受限。

Method: 采用多任务学习和循环神经网络参数化可微分生物物理模型，实现跨品种共享学习。

Result: 在真实和合成数据集上，该方法显著优于传统生物物理模型和基线深度学习方法。

Conclusion: 混合方法在预测物候阶段及其他作物状态变量上表现优异，适用于精细化管理。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [83] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 论文揭示了基于距离的分类器中隐藏的神经网络结构，使得可解释AI技术（如LRP）得以应用，并通过实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 基于距离的分类器（如k近邻和支持向量机）在机器学习中广泛应用，但其预测的可解释性仍需提升。

Method: 发现基于距离的分类器中隐藏的神经网络结构（线性检测单元与非线性池化层结合），并应用可解释AI技术（如LRP）。

Result: 定量评估表明，新解释方法优于多个基线，并通过实际用例展示了其价值。

Conclusion: 通过揭示隐藏结构，为基于距离的分类器提供了有效的可解释性方法。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [84] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 论文研究了主动学习与迁移学习结合在跨域时间序列数据异常检测中的效果，发现聚类与主动学习存在交互作用，最佳性能出现在未应用聚类时。主动学习能提升模型性能，但提升速度较慢，且迁移学习与主动学习的组合性能最终趋于平缓。


<details>
  <summary>Details</summary>
Motivation: 探索主动学习与迁移学习结合在跨域时间序列异常检测中的有效性，以优化模型性能和数据标注效率。

Method: 结合主动学习和迁移学习，分析聚类的作用，并通过实验设计区分采样池和测试池的数据样本。

Result: 主动学习能提升性能，但速度较慢；迁移学习与主动学习组合的性能初期提升后趋于平缓。

Conclusion: 主动学习有效但性能提升呈线性平缓趋势，聚类未显著提升性能，数据点选择顺序影响最终表现。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [85] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 论文提出了一种结合流形学习和机器学习的方法，从高保真代理模拟中学习潜在空间中的离散演化算子，用于桥接微观和宏观建模尺度的人群动力学。


<details>
  <summary>Details</summary>
Motivation: 解决微观和宏观建模尺度之间的桥梁问题，以支持系统的数值分析、优化和控制。

Method: 四阶段方法：1) 从微观数据推导宏观场；2) 基于流形学习构建潜在空间映射；3) 使用机器学习（LSTM和MVAR）学习降阶代理模型；4) 重构高维空间中的密度分布。

Result: 数值结果表明该方法具有高精度、鲁棒性和泛化能力，能够快速准确模拟人群动力学。

Conclusion: 通过潜在空间学习和重构，成功构建了有效的宏观PDE解算子，为人群动力学建模提供了新工具。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [86] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 研究了Transformer仅通过下一词预测训练学习涉及上下文的算法的能力，发现模型在达到一定规模和训练集大小后能学习估计转移概率而非记忆训练模式。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在仅通过下一词预测训练时是否能学习上下文相关的算法，而非简单记忆。

Method: 使用随机转移矩阵的马尔可夫链训练Transformer预测下一词，测试时使用不同矩阵。

Result: 模型在达到一定规模和训练集大小后能学习估计转移概率，且更复杂的状态编码能提升对未见结构的预测鲁棒性。

Conclusion: Transformer能通过下一词预测学习上下文相关的算法，且模型规模和编码方式影响学习效果。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [87] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT是一种新颖的后处理框架，通过最优传输选择性调整风险评分分布，平衡公平性和AUC性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗、金融、刑事司法）中，公平性常基于风险评分评估，但严格公平性要求会显著降低AUC性能。

Method: 提出FairPOT，利用最优传输选择性调整弱势群体的评分分布，通过参数λ调节公平性与AUC性能的权衡。

Result: 实验表明，FairPOT在全局和部分AUC场景中优于现有方法，能在轻微AUC损失下提升公平性。

Conclusion: FairPOT计算高效且实用，适合实际部署。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [88] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet是一种基于物理信息的深度算子网络，用于从压力剖面映射到气泡半径响应，结合了Rowdy自适应激活函数以改善高频特征表示，并在多种气泡动力学场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，需要一种高效且物理准确的替代模型来模拟气泡动力学。

Method: 基于PI-DeepONet框架，结合Rowdy自适应激活函数，评估了单步和两步训练技术，并在不同气泡动力学场景中测试。

Result: BubbleONet在多种气泡动力学场景中表现出色，可作为传统数值求解器的高效替代方案。

Conclusion: BubbleONet是一种高效且物理准确的替代模型，适用于气泡动力学模拟。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [89] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP是一个动态、用户可控的隐私保护框架，通过多模态对比学习实现少量样本的敏感活动检测，并生成隐私合规数据。


<details>
  <summary>Details</summary>
Motivation: 现代传感系统中，用户隐私需求因人而异且动态变化，现有方法多为静态或需要大量训练数据，缺乏灵活性和用户控制。

Method: PrivCLIP利用多模态对比学习将IMU数据与自然语言描述对齐，通过语言引导的数据净化模块（IMU-GPT）生成隐私合规数据。

Result: 在多个活动识别数据集上，PrivCLIP在隐私保护和数据效用方面显著优于基线方法。

Conclusion: PrivCLIP提供了一种灵活、用户可控的隐私保护方案，适用于动态隐私需求的传感系统。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [90] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA通过文本级和参数级的方法解决多任务中LoRA适配器的任务干扰问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在多任务设置中，合并来自不同源的LoRA适配器常导致任务干扰，影响下游性能。

Method: 文本级：聚类训练样本并训练专用LoRA适配器；参数级：引入联合CP分解，分离任务特定和共享因素。

Result: 在Phi-3和Mistral-7B上分别提升1.4%和2.3%的准确率。

Conclusion: TC-LoRA有效减少任务干扰，提升LLM适应性能。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [91] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 论文提出了一种名为DCFL的新框架，通过解耦对比学习的损失函数，解决了联邦学习中数据异质性导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异质性导致性能下降，而现有对比学习方法在有限样本下无法满足其渐进假设。

Method: 提出DCFL框架，将对比损失解耦为对齐和均匀性两个目标，独立校准吸引和排斥力。

Result: DCFL在正样本对齐和负样本均匀性上优于现有方法，并在多个基准测试中表现优异。

Conclusion: DCFL为联邦学习提供了一种有效的对比学习方法，显著提升了性能。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [92] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文对TensorFlow和PyTorch两大深度学习框架进行了全面比较，涵盖易用性、性能和部署灵活性，指出PyTorch适合研究，TensorFlow更适合生产环境。


<details>
  <summary>Details</summary>
Motivation: 比较TensorFlow和PyTorch的优缺点，帮助开发者根据需求选择合适的框架。

Method: 通过编程范式、训练速度、推理性能、部署工具、生态系统和社区支持等多维度对比分析。

Result: PyTorch在研究领域更受欢迎，而TensorFlow在生产环境中更具优势。

Conclusion: 选择框架需权衡研究灵活性与生产成熟度，未来需进一步优化执行模式和跨框架兼容性。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [93] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FeDaL的联邦学习方法，用于解决时间序列基础模型中的数据集异质性问题，通过消除局部和全局偏差提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据集的异质性导致显著的领域偏差，降低了时间序列基础模型的泛化能力，这一问题尚未充分研究。

Method: 采用联邦学习范式，提出FeDaL方法，通过域偏差消除（DBE）和全局偏差消除（GBE）机制学习数据集无关的时间表示。

Result: 在8个任务和54个基线模型上验证了FeDaL的跨数据集泛化能力，并分析了联邦学习的扩展行为。

Conclusion: FeDaL有效解决了时间序列数据集的异质性问题，提升了模型的泛化性能。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [94] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 量子时间融合变换器（QTFT）是一种量子增强的混合量子-经典架构，基于经典TFT框架，在时间序列预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩展经典TFT的能力，利用量子计算提升时间序列预测性能。

Method: 提出QTFT，基于变分量子算法，适用于当前NISQ设备。

Result: 在某些测试案例中，QTFT表现优于经典TFT，其余案例中性能相当。

Conclusion: QTFT是一种有前景的量子增强方法，适用于实际NISQ设备。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [95] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 论文探讨了自动短答案评分（ASAG）的改进方法，比较了基于大语言模型（LLMs）的微调与少样本提示的优劣，发现微调方法在特定条件下优于少样本提示。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何利用有限资源（如少量数据或消费级GPU）高效改进ASAG，同时比较封闭模型与开源模型的性能差异。

Method: 方法包括评估两种微调方法（OpenAI的微调服务和QLORA）与少样本提示的结合效果，并使用结构化（JSON）输出进行ASAG。

Result: 结果显示，少量数据微调对开源模型效果有限，但对封闭模型效果显著；合成数据可显著提升开源模型性能。

Conclusion: 结论是微调方法在特定场景下优于少样本提示，且性能可能受领域影响；合成数据是提升开源模型的有效策略。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [96] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT是一种新型联邦学习后门攻击方法，通过潜在驱动条件自编码器生成多样化、目标特定的触发器，提高攻击灵活性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法受限于固定模式或单一目标触发器，灵活性差且易被检测，亟需更隐蔽、多样化的攻击手段。

Method: 利用潜在驱动条件自编码器生成多样化触发器，支持任意目标选择且无需重新训练。

Result: FLAT攻击成功率高，能有效规避现有防御机制，实验验证其鲁棒性。

Conclusion: FLAT展示了潜在驱动多目标后门攻击的威胁，呼吁开发新的防御策略。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [97] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种对抗性公平多视图聚类（AFMVC）框架，通过对抗训练去除学习特征中的敏感属性信息，确保聚类分配不受其影响。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法主要关注性能，而公平性在人类中心应用中至关重要，但常被忽视。

Method: 采用对抗训练从学习特征中去除敏感属性信息，并通过KL散度对齐视图特定聚类分配与公平不变共识分布。

Result: AFMVC在公平性和聚类性能上优于现有方法。

Conclusion: AFMVC在保证公平性的同时实现了竞争性的聚类性能，并提供了理论支持。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [98] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 研究了视觉语言模型（VLMs）在模型反转攻击中的隐私风险，提出了新的基于令牌和序列的反转方法，实验证明VLMs易受训练数据泄露影响。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在模型反转攻击中的隐私漏洞，填补现有研究空白。

Method: 提出了四种反转策略：TMI、TMI-C、SMI和SMI-AW，结合词汇表示的对数最大化损失进行实验。

Result: 序列方法（特别是SMI-AW）在攻击准确性和视觉相似性上优于令牌方法，人类评估攻击准确率达75.31%。

Conclusion: VLMs存在显著的隐私风险，需引起重视，尤其是在医疗和金融等领域的广泛应用中。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [99] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种一致性感知的策略优化框架，通过全局奖励和熵基软混合机制解决强化学习中梯度消失问题，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多个响应结果一致时，梯度消失导致学习信号无效，限制了训练效率和性能。

Method: 提出一致性感知策略优化框架，引入基于结果一致性的全局奖励和熵基软混合机制。

Result: 在多个数学推理基准测试中表现显著提升，验证了方法的鲁棒性和通用性。

Conclusion: 该方法通过创新奖励设计和优化策略，有效解决了梯度消失问题，提升了模型性能。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [100] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 该论文提出了一种半监督深度域适应框架，用于解决太阳能发电预测中的域偏移问题，仅需目标域20%的标注数据即可显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 不同地理位置的天气特征差异导致域偏移，使得机器学习模型在跨区域预测时性能下降，同时缺乏标注数据和存储问题增加了挑战。

Method: 采用半监督深度域适应框架，基于源位置数据训练深度卷积神经网络，并通过无源数据的教师-学生模型配置进行目标域适应，利用一致性和交叉熵损失进行半监督学习。

Result: 在仅标注目标域20%数据的情况下，模型在加州、佛罗里达和纽约的预测准确性分别提升了11.36%、6.65%和4.92%。

Conclusion: 该方法有效解决了域偏移问题，显著提升了跨区域太阳能发电预测的准确性，且对标注数据需求较低。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [101] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练的两阶段分子生成方法，结合MIST编码器和MolForge解码器，显著提升了从质谱数据生成分子结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决从质谱数据直接生成分子结构的挑战，通过预训练和优化解码器性能，提高生成准确性。

Method: 采用MIST编码器将质谱转化为分子指纹，再通过预训练的MolForge解码器将指纹解码为分子结构，优化了阈值处理以聚焦子结构。

Result: 相比现有方法，该方法在top-1和top-10准确率上分别达到28%和36%，提升了十倍性能。

Conclusion: 该方法为质谱数据分子解析提供了强基线，未来研究可在此基础上进一步优化。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [102] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出了一种名为SAMT的新方法，通过交替更新网络参数块和自适应步长策略，提高了深度神经网络训练的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中的非凸优化问题，减少计算成本并提高收敛稳定性。

Method: 采用块交替最小化策略，结合基于元学习的自适应步长选择方法。

Result: 实验证明SAMT在多个基准测试中表现优于现有方法，泛化性能更好且参数更新更少。

Conclusion: SAMT是一种高效且稳定的神经网络优化方法，具有理论和实践上的优势。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [103] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 论文提出Causal Reward Adjustment (CRA)方法，通过因果推断解决外部推理系统中的奖励欺骗问题，提高数学问题求解的准确性。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统结合语言模型和过程奖励模型（PRMs）时，容易出现奖励欺骗现象，即高评分但逻辑错误的推理路径被PRMs错误评分，导致答案错误。

Method: 提出CRA方法，通过稀疏自编码器从PRM内部激活中提取可解释特征，并利用后门调整校正混淆因素，估计推理路径的真实奖励。

Result: 在数学问题求解数据集上的实验表明，CRA有效缓解了奖励欺骗，提高了最终准确性，且无需修改策略模型或重新训练PRM。

Conclusion: CRA通过因果推断方法解决了奖励欺骗问题，为复杂任务推理系统的改进提供了有效途径。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [104] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 提出了一种基于对称散度的行为正则化策略优化（BRPO）框架，解决了对称散度在离线强化学习中的数值问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用非对称散度（如KL散度），但对称散度在正则化和损失函数中存在数值问题，需要解决。

Method: 通过泰勒展开$f$-散度，证明有限级数可得到解析策略，并分解对称散度为不对称和条件对称项以缓解数值问题。

Result: 提出的S$f$-AC算法在分布近似和MuJoCo实验中表现优异。

Conclusion: S$f$-AC是首个实用的基于对称散度的BRPO算法，验证了其竞争力。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [105] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS是一种数据为中心的AutoML代理，通过优化时间序列数据质量提升预测性能，平均减少6%误差。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML方法多关注特征工程和模型架构，而轻量级模型在时间序列预测中表现优异，因此探索数据质量优化方向。

Method: 提出DCATS，利用时间序列元数据清理数据并优化预测性能。

Result: 在交通流量预测数据集上测试四种模型，DCATS平均减少6%误差。

Conclusion: 数据为中心的方法在时间序列AutoML中具有潜力。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [106] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的自动化多普勒角度估计方法，通过预训练模型和浅层网络实现，误差低于临床阈值。


<details>
  <summary>Details</summary>
Motivation: 多普勒角度估计错误是血流速度测量的主要误差来源，需自动化解决方案。

Method: 使用2100张颈动脉超声图像，结合图像增强和预训练模型提取特征，通过浅层网络估计角度。

Result: 最佳模型的平均绝对误差低于临床阈值，避免误诊。

Conclusion: 深度学习技术在多普勒角度自动化估计中具有潜力，可集成到商业超声设备中。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [107] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time是一个新颖的三模态框架，通过时间、频谱和提示分支捕捉多变量时间序列的复杂关系，动态调整特征优先级和跨模态对齐，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉多变量时间序列的细粒度、特定于预测范围的关系时存在局限性，如依赖固定归纳偏置或静态融合策略。

Method: 提出T3Time框架，包含时间、频谱和提示分支，通过门控机制动态调整特征优先级，并自适应聚合跨模态对齐头。

Result: 在基准数据集上，T3Time平均减少MSE 3.28%和MAE 2.29%，在少样本学习中也表现优异。

Conclusion: T3Time通过动态特征融合和跨模态对齐，显著提升了多变量时间序列预测的性能和泛化能力。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [108] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个基于Python的工具，通过集成局部和全局敏感性分析，帮助用户可视化和理解机器学习模型的行为。


<details>
  <summary>Details</summary>
Motivation: 为AI研究人员和领域专家提供一个无需编程的交互式图形界面，支持配置、训练、评估和解释模型。

Method: 自动化模型训练和选择，提供基于方差的全局特征归因，以及通过LIME和SHAP的实例解释。

Result: 在泰坦尼克数据集上的分类任务中展示了系统如何通过敏感性信息指导特征选择和数据优化。

Conclusion: SAInT通过交互式工具简化了模型解释和优化流程，适用于非技术用户。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [109] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 论文提出框架Mockingbird，通过让大语言模型（LLM）扮演功能角色并自我反思，将其应用于通用机器学习任务。评估表明，该方法效果尚可，但仅靠自我反思无法超越领域特定文档和专家反馈。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLM）在通用机器学习任务中的潜力，超越聊天机器人应用。

Method: 提出Mockingbird框架，通过指令让LLM扮演功能角色并自我反思错误以改进。

Result: 在多个通用机器学习任务中，LLM驱动的方法（如Mockingbird）表现尚可，但仅靠自我反思无法超越领域特定文档和专家反馈的效果。

Conclusion: LLM在通用机器学习任务中具有一定潜力，但需结合领域特定知识和专家反馈以进一步提升性能。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [110] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: VL-DAC是一种轻量级、无超参数的RL算法，通过解耦动作和价值的更新，提升了VLMs在多种任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLMs在多模态交互任务中缺乏语言条件动作生成能力的问题，并避免传统RL方法的超参数敏感性和泛化不足。

Method: 提出VL-DAC算法，将PPO更新应用于动作标记，同时在环境步骤级别学习价值函数，实现解耦训练。

Result: 在多个模拟器和任务中（如MiniWorld、ALFWorld等）训练后，VL-DAC显著提升了泛化性能（如BALROG任务提升50%）。

Conclusion: VL-DAC首次证明简单RL算法可在廉价合成环境中训练VLMs，并在真实任务中取得显著性能提升。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [111] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出了一种基于权重显著性的两阶段高效机器遗忘方法（WSS-CL），通过对比学习在特征空间中最大化遗忘与保留数据的距离，显著提升了遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘方法在精确性、稳定性和跨领域适用性方面存在挑战，需要更高效的解决方案。

Method: 两阶段方法：1) 遗忘阶段通过最大化KL散度实现高效遗忘；2) 对抗微调阶段利用对比学习在特征空间中分离遗忘与保留数据。

Result: 实验表明，该方法在遗忘效果和性能损失方面优于现有技术，适用于监督和自监督场景。

Conclusion: WSS-CL方法显著提升了机器遗忘的效率和精确性，具有广泛的应用潜力。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [112] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种通过将语料库中的令牌分为正负两类来优化监督微调（SFT）的方法，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）对预训练大语言模型（LLMs）至关重要，但其效果依赖于数据质量和数量。为减少这种依赖，作者提出分类令牌的方法。

Method: 将语料库中的令牌分为正负两类：正令牌按常规训练，负令牌则通过遗忘机制处理，以减少无效信息的影响。

Result: 实验表明，该方法不仅提升了模型整体性能，还促进了更丰富的模型响应。

Conclusion: 通过令牌分类和遗忘机制，可以更精确地指导模型学习，提升监督微调的效果。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [113] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS是一种新的隐私推理范式，通过分布式特征共享平衡隐私与效率，显著降低客户端计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决基于云的机器学习服务（MLaaS）中隐私与效率的权衡问题，避免现有方法（如加密或拆分推理）的缺陷。

Method: 将输入特征划分为多个平衡份额，分发到非共谋服务器进行部分推理，客户端安全聚合输出；扩展方法包括对抗训练（PrivDFS-AT）和用户特定密钥（PrivDFS-KD）。

Result: 在CIFAR-10和CelebA上，PrivDFS在保持隐私的同时减少客户端计算100倍，且无精度损失；扩展方法对攻击具有鲁棒性。

Conclusion: PrivDFS为隐私推理提供了高效且安全的解决方案，适用于敏感数据场景。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [114] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出了一种名为MMSFM的新方法，用于处理高维系统在不规则时间点观测数据的建模问题，避免了传统降维方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法在处理高维系统动态时可能过度简化，无法捕捉非平衡系统中的关键瞬态行为。

Method: MMSFM扩展了无模拟分数和流匹配方法，支持多边际设置，利用测度值样条增强对不规则时间点的鲁棒性，并通过分数匹配防止高维空间过拟合。

Result: 在合成和基准数据集（包括基因表达数据和图像进展任务）上验证了方法的有效性。

Conclusion: MMSFM是一种多功能方法，适用于高维数据在不规则时间点的建模。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [115] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种针对多实例学习（MIL）的持续学习方法，通过选择实例和样本存储，避免灾难性遗忘，并在白血病检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床环境中数据流动态变化，需要持续更新模型以保持性能，但现有方法在多实例学习中效果不佳。

Method: 基于实例注意力分数和距离选择样本存储，保留数据多样性，适用于MIL的持续学习。

Result: 在白血病实验室数据上，该方法显著优于现有持续学习方法。

Conclusion: 该方法首次为MIL提供持续学习解决方案，适应数据分布变化，如疾病发生或基因改变。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [116] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种新型的INT6量化框架，通过算法创新和系统优化，在保持高精度的同时提升推理效率和内存节省。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高内存和计算成本限制了实际部署，现有INT4/INT8量化方法在精度或效率上存在不足。

Method: FlexQ采用统一的6位权重量化，并通过层敏感分析保留8位激活，开发了支持W6A6和W6A8的高性能GPU内核。

Result: 在LLaMA模型上，FlexQ保持接近FP16的精度，推理速度提升1.33倍，内存节省1.21倍。

Conclusion: FlexQ在INT6量化中实现了精度与效率的平衡，为LLMs的实际部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [117] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 系统综述分析了2020年至2024年初关于多模态模型可解释性的研究，发现注意力技术是主要解释方法，但存在模态交互不足和评估方法不系统的问题。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习和可解释AI（XAI）的发展，需要系统分析多模态模型的可解释性研究现状。

Method: 通过文献综述，从模型架构、模态、解释算法和评估方法等多维度分析研究。

Result: 研究发现注意力技术主导解释方法，但模态交互和评估方法存在不足。

Conclusion: 建议未来研究采用更严谨、透明和标准化的评估方法，以提升多模态XAI的可解释性。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [118] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了基于矩阵向量乘法的随机算法，用于估计矩阵的两种范数，并应用于深度神经网络训练和推荐系统对抗攻击缓解。


<details>
  <summary>Details</summary>
Motivation: 解决在矩阵自由设置中高效估计矩阵范数的问题，并探索其在深度学习和推荐系统中的实际应用。

Method: 改进Hutchinson对角估计器及其Hutch++版本，提供算法复杂度分析。

Result: 算法在图像分类任务中有效用于Jacobian正则化，并能缓解推荐系统中的对抗攻击。

Conclusion: 新算法在理论和实践中均表现出色，具有广泛的应用潜力。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [119] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: CMCFAE是一种新型生成模型，将云模型与Wasserstein自编码器结合，通过云模型特征函数正则化潜在空间，提升复杂数据分布的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖标准高斯先验和传统散度度量，导致重构样本同质化，CMCFAE旨在通过云模型先验提供更灵活的潜在空间表示。

Method: 提出云模型特征函数，并将其作为正则化项集成到WAE框架中。

Result: 在MNIST、FashionMNIST、CIFAR-10和CelebA数据集上的实验表明，CMCFAE在重构质量、潜在空间结构和样本多样性上优于现有模型。

Conclusion: CMCFAE不仅实现了云模型理论与MMD正则化的创新结合，还为自编码器生成模型提供了新的优化方向。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [120] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的多轮对抗对话方法，用于更有效地发现大型语言模型的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法依赖脆弱的提示模板或单轮攻击，无法捕捉真实对抗对话的复杂性。

Method: 将红队测试建模为马尔可夫决策过程，采用分层强化学习框架，通过细粒度的令牌级奖励训练生成代理。

Result: 该方法能够发现现有基准测试遗漏的细微漏洞，成为新的最佳实践。

Conclusion: 将红队测试重新定义为动态、基于轨迹的过程，对AI的稳健部署至关重要。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [121] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 论文探讨了小规模应用中基于注意力的架构是否优于传统方法（如多层感知机或循环网络），通过任务切换框架测试，发现标准Transformer无法解决基本任务，而改进后的模型（如cisformer和extensive attention结合）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索在小规模应用中，基于注意力的架构是否能超越传统方法，尤其是在任务切换的背景下。

Method: 方法包括测试标准Transformer、LSTM、MLP以及改进的cisformer和extensive attention在任务切换框架（IARC任务）中的表现。

Result: 结果显示标准Transformer表现不佳，而结合cisformer和extensive attention的模型能达到约95%的准确率。

Conclusion: 结论表明，通过比较不同注意力机制在任务切换中的表现，可以更好地理解和改进注意力机制。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [122] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出了一种基于缓存的并行推测解码框架CARD，通过'查询-纠正'范式解耦起草和验证，显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法需顺序执行起草和验证，效率低且限制起草模型规模，单次拒绝导致后续候选令牌丢弃。

Method: CARD框架中，起草模型生成候选令牌存入共享缓存，目标模型并行纠正起草方向，实现接近起草模型的推理速度。

Result: 相比原始解码，CARD实现最高4.83倍加速，无需微调起草或目标模型。

Conclusion: CARD通过并行化起草和验证，显著提升推理效率，为LLM加速提供新思路。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [123] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种基于Transformer的神经算子方法，通过同时学习全局和局部特征，解决了多尺度问题，提升了物理一致性和数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了局部物理细节与全局特征之间的协同学习，这对多尺度问题和长期预测至关重要。

Method: GFocal通过Nyström注意力机制和切片机制分别捕获全局和局部特征，并通过卷积门控块动态融合多尺度信息。

Result: GFocal在六个基准测试中的五个表现最佳，平均相对增益15.2%，并在工业规模模拟中表现出色。

Conclusion: GFocal通过全局和局部特征的协同学习，显著提升了物理建模和预测的准确性。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [124] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP提出了一种基于解析解（闭式解）的个性化联邦学习方案，避免了梯度更新的依赖，解决了非独立同分布数据（non-IID）带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法因依赖梯度更新而对非独立同分布数据敏感，导致性能下降。FedHiP旨在通过解析解从根本上解决这一问题。

Method: 利用自监督预训练的基础模型作为冻结特征提取器，开发解析分类器，实现无梯度训练。方案分为解析本地训练、全局聚合和本地个性化三个阶段。

Result: 在基准数据集上，FedHiP的准确率比现有最佳方法高出5.79%-20.97%。

Conclusion: FedHiP通过解析解实现了对数据异质性的不变性，显著提升了性能。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [125] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 研究发现，墙体保温措施平均减少19%的燃气需求，但效果因家庭能源负担而异：低负担家庭节约显著，高负担家庭几乎无变化。


<details>
  <summary>Details</summary>
Motivation: 探讨能源效率干预措施对燃气需求的异质性影响，特别是不同能源负担群体的分布效应。

Method: 使用因果机器学习模型分析英国代表性住房数据，评估墙体保温的平均和条件处理效应。

Result: 低能源负担家庭燃气需求显著下降，高负担家庭因将节约用于改善舒适度而几乎无变化。

Conclusion: 需综合考虑气候影响和能源政策的公平性，建立更广泛的评估框架。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [126] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 论文提出了一种基于层次结构的评分指标，用于更细粒度地评估机器学习模型的分类和物体检测性能，考虑了类别间的层次关系。


<details>
  <summary>Details</summary>
Motivation: 传统的分类评估方法将所有错误分类视为等同，忽略了类别间的层次关系。本文旨在通过层次评分指标提供更细致的错误分析。

Method: 开发了多种复杂度的层次评分指标，利用评分树编码类别关系，并通过抽象用例和三种加权策略展示其效果。

Result: 结果表明，这些指标能更细粒度地捕捉错误，且评分树支持灵活调整，从而更全面地评估模型性能。

Conclusion: 层次评分指标提供了一种更细致的模型评估方法，不仅关注错误数量，还关注错误的类型和影响。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [127] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的情绪检测方法，使用条件生成对抗网络（cGANs），通过多模态框架整合文本、音频和面部表情，显著提升了情绪识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情绪检测方法依赖单一数据类型，限制了情绪理解的全面性。本文旨在通过多模态数据整合和cGANs生成合成数据，提升情绪识别的准确性和鲁棒性。

Method: 采用条件生成对抗网络（cGANs）构建多模态框架，整合文本、音频和面部表情数据，生成合成情绪数据以增强分类性能。

Result: 实验结果表明，该方法在情绪识别性能上显著优于基线模型。

Conclusion: 本研究展示了cGANs在多模态情绪检测中的潜力，为提升人机交互系统的情感理解能力提供了新思路。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [128] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM是一种基于卷积的特征提取器，通过多分辨率、每通道设计，显著减少模型复杂性和参数数量，同时在多元时间序列分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer和CNN模型在多元时间序列分类中计算量大、频率多样性有限和参数需求高的问题。

Method: 使用对称有限脉冲响应（FIR）滤波器在多个时间尺度上独立处理每个通道，避免通道间卷积，减少模型复杂性。

Result: 在人类活动、睡眠阶段和生物医学基准测试中，PRISM性能优于或匹配主流CNN和Transformer模型，同时参数和计算量大幅减少。

Conclusion: PRISM结合经典信号处理和现代深度学习，提供了一种高效且准确的多元时间序列分类解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [129] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: GraphProp是一种图基础模型（GFM），通过强调结构泛化能力，显著提升了图分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GFM主要关注节点特征的跨域泛化，但缺乏结构信息的跨域一致性。GraphProp旨在通过结构信息提升GFM的泛化能力。

Method: GraphProp分为两阶段训练：1）通过预测图不变量训练结构GFM；2）利用结构GFM的表示作为位置编码，结合节点属性和标签训练综合GFM。

Result: 实验表明，GraphProp在监督学习和少样本学习中表现优异，尤其在无节点属性的图上。

Conclusion: GraphProp通过结构泛化显著提升了GFM的性能，尤其在跨域任务中表现突出。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [130] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 论文提出Causal Reflection框架，通过动态建模因果关系，帮助智能体进行因果推理，并结合LLMs生成自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs和传统强化学习智能体在因果推理上的不足，如依赖虚假相关性和缺乏对因果关系的建模。

Method: 引入Causal Reflection框架，动态建模状态、动作、时间和扰动之间的因果关系，并定义Reflect机制修正模型。

Result: 框架为智能体提供了适应、自我修正和表达因果理解的能力。

Conclusion: Causal Reflection为智能体在动态环境中的因果推理奠定了理论基础。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [131] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的分层架构，用于终身网络入侵检测系统（NIDS），结合静态和动态SNN，通过生物启发的学习机制实现高效适应和低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受大脑分层处理和能量效率的启发，旨在开发一种能够持续学习新威胁并保持现有知识的网络入侵检测系统。

Method: 使用静态SNN初步识别潜在入侵，动态SNN分类具体攻击类型，结合GWR结构可塑性和新型Ad-STDP学习规则。

Result: 在UNSW-NB15基准测试中，系统表现出强适应性和低灾难性遗忘，总体准确率达85.3%，并在神经形态硬件上实现高操作稀疏性。

Conclusion: 该架构在终身学习和低功耗部署方面具有潜力，为网络入侵检测提供了生物启发的解决方案。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [132] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 论文提出了一种新的联邦交通预测方法Fed-CI，通过独立通道建模范式（CIP）减少通信开销，提高预测效率，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 交通数据分散且隐私受限，现有联邦学习方法通信开销大，训练速度慢。

Method: 提出CIP范式，避免客户端间通信，开发Fed-CI框架，利用本地数据进行高效预测。

Result: Fed-CI在多个数据集上表现优异，RMSE、MAE和MAPE分别提升8%、14%和16%，通信成本显著降低。

Conclusion: Fed-CI在减少通信开销的同时，实现了高性能的联邦交通预测，符合隐私法规。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [133] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练的链接预测方法，通过融合节点和边信息，并引入Mixture-of-Experts框架和参数高效调优策略，显著提升了稀疏数据和分布变化下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在链接预测任务中面临监督稀疏、初始化敏感和泛化能力差的问题，预训练被视为解决这些挑战的潜在方案。

Method: 采用节点和边信息的后期融合策略，结合Mixture-of-Experts框架处理数据多样性，并设计参数高效调优策略以快速适应新数据集。

Result: 在16个数据集上的实验表明，该方法在低资源链接预测中达到最优性能，计算开销降低10,000倍以上。

Conclusion: 预训练结合模块化设计和高效调优策略，显著提升了链接预测的性能和泛化能力。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [134] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 研究通过分析5000多起身份盗窃和欺诈案例，构建了一个基于图的身份生态系统模型，用于预测个人隐私风险。


<details>
  <summary>Details</summary>
Motivation: 缺乏对隐私风险的基本理解使得个人和组织难以保护个人信息。

Method: 构建身份生态系统图，节点代表个人身份信息属性，边代表属性间的披露关系；利用图论和图神经网络开发隐私风险预测框架。

Result: 模型能有效预测某一身份属性泄露是否可能导致另一属性泄露。

Conclusion: 该研究为隐私风险评估提供了有效工具，有助于更好地保护个人信息。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [135] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 论文研究了基于物理信息的神经网络在铝点焊中的应用，提出了两种新的训练策略，实现了非破坏性的质量评估。


<details>
  <summary>Details</summary>
Motivation: 电阻点焊是汽车工业中主要的连接工艺，但焊点直径的测量需要破坏性测试，限制了高效质量控制的可能性。

Method: 采用物理信息神经网络重建内部过程状态，引入渐进的实验损失函数和条件更新温度相关材料参数的方法，并选择轴对称二维模型以提高计算效率。

Result: 二维网络预测的动态位移和焊点生长在实验置信区间内，支持从钢到铝的焊接阶段转移，展示了工业应用中快速模型质量控制的潜力。

Conclusion: 提出的训练策略和模型组件在非破坏性质量评估中表现出色，具有工业应用的潜力。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [136] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出了一种广义的概率分布映射学习框架，扩展了流和扩散模型的时间动态。


<details>
  <summary>Details</summary>
Motivation: 通过广义化随机插值，将标量时间变量替换为向量、矩阵或线性算子，以桥接多维空间中的概率分布。

Method: 采用基于算子的插值方法，构建多功能生成模型，无需任务特定训练。

Result: 数值实验展示了该方法在条件生成、修复、微调和后验采样等多任务中的零样本有效性。

Conclusion: 该方法为现有生成模型提供了统一的理论视角，并扩展了其能力，有望成为通用任务无关的替代方案。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [137] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果关系的框架CaPulse，用于时间序列异常检测，通过结构因果模型和周期性归一化流解决数据稀缺性和复杂周期性问题，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉时间序列异常生成的机制，且面临标签稀缺、数据不平衡和复杂周期性等挑战。

Method: 构建结构因果模型解析异常生成过程，提出带掩码机制的周期性归一化流和周期性学习器，实现周期性感知的密度异常检测。

Result: 在七个真实数据集上，CaPulse性能优于现有方法，AUROC提升3%至17%，且具有更好的可解释性。

Conclusion: CaPulse通过因果工具和周期性建模有效解决了时间序列异常检测的挑战，性能显著提升。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [138] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一种高性能的生物声学预训练模型，通过多类群数据集训练，结合自蒸馏和原型学习分类器，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩展Perch模型的应用范围，从仅针对鸟类扩展到多类群生物声学分类，并探索细粒度物种分类作为预训练任务的鲁棒性。

Method: 使用自蒸馏和原型学习分类器进行训练，引入新的源预测训练标准。

Result: 在BirdSet和BEANS基准测试中达到最优性能，同时在海洋迁移学习任务中表现优于专用模型。

Conclusion: 细粒度物种分类是生物声学预训练的有效任务，Perch 2.0展示了其广泛适用性和强大性能。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [139] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出了一种高效算法，用于在高斯分布下学习单指数模型，即使存在对抗性标签噪声，也能实现常数因子近似。


<details>
  <summary>Details</summary>
Motivation: 解决单指数模型在对抗性标签噪声下的学习问题，填补现有方法在激活函数适用范围和近似精度上的不足。

Method: 开发了一种优化框架，通过直接利用问题结构、高斯空间性质和单调函数规律性，识别有用的向量场来指导算法更新。

Result: 算法适用于所有具有有界2+ζ阶矩的单调激活函数，包括单调Lipschitz函数和半空间等不连续函数。

Conclusion: 该算法首次实现了对广泛单调激活函数的高效学习，为对抗性噪声下的单指数模型学习提供了新思路。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [140] [A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution](https://arxiv.org/abs/2508.03767)
*Sandeepa Kannangara,Arman Abrahamyan,Daniel Elias,Thomas Kilby,Nadav Dar,Luiz Pizzato,Anna Leontjeva,Dan Jermyn*

Main category: cs.DB

TL;DR: MERAI是一种高效的大规模实体解析（ER）管道，专为解决企业级高容量数据集中的记录去重和链接问题而设计，性能优于Dedupe和Splink。


<details>
  <summary>Details</summary>
Motivation: 实体解析在处理大规模数据集时面临挑战，需要一种可扩展且可靠的解决方案。

Method: MERAI通过AI技术构建了一个高效的管道，支持大规模记录去重和链接。

Result: MERAI成功处理了1570万条记录，匹配准确率和F1分数均优于Dedupe和Splink。

Conclusion: MERAI为企业级大规模实体解析提供了可扩展且可靠的解决方案。

Abstract: Entity resolution (ER) remains a significant challenge in data management,
especially when dealing with large datasets. This paper introduces MERAI
(Massive Entity Resolution using AI), a robust and efficient pipeline designed
to address record deduplication and linkage issues in high-volume datasets at
an enterprise level. The pipeline's resilience and accuracy have been validated
through various large-scale record deduplication and linkage projects. To
evaluate MERAI's performance, we compared it with two well-known entity
resolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2
million records due to memory constraints, MERAI successfully processed
datasets of up to 15.7 million records and produced accurate results across all
experiments. Experimental data demonstrates that MERAI outperforms both
baseline systems in terms of matching accuracy, with consistently higher F1
scores in both deduplication and record linkage tasks. MERAI offers a scalable
and reliable solution for enterprise-level large-scale entity resolution,
ensuring data integrity and consistency in real-world applications.

</details>


### [141] [Raqlet: Cross-Paradigm Compilation for Recursive Queries](https://arxiv.org/abs/2508.03978)
*Amir Shaikhha,Youning Xia,Meisam Tarabkhah,Jazal Saleem,Anna Herlihy*

Main category: cs.DB

TL;DR: Raqlet是一个源到源的编译框架，旨在解决递归查询引擎在关系型、图型和演绎型系统中的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前SQL:2023的SQL/PGQ和GQL标准为图数据查询提供了共同基础，但实际支持不一致。Raqlet通过中间表示（IRs）填补了这一空白。

Method: Raqlet通过PGIR、DLIR和SQIR三种中间表示，将递归查询在不同范式间转换。

Result: Raqlet提供了一个共享语义基础，支持静态分析和性能优化，如magic-set转换。

Conclusion: Raqlet旨在成为一个强大的平台，支持跨范式原型设计、可移植递归查询和形式化推理。

Abstract: We introduce Raqlet, a source-to-source compilation framework that addresses
the fragmentation of recursive querying engines spanning relational (recursive
SQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards
such as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for
querying graph data within relational and graph databases; however, real-world
support remains inconsistent across systems. Raqlet bridges this gap by
translating recursive queries across paradigms through leveraging intermediate
representations (IRs) grounded in well-defined semantics; it translates Cypher
or SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),
and finally to SQIR (inspired by recursive SQL). Raqlet provides a shared
semantic basis that can serve as a golden reference implementation for language
standards, while supporting static analysis and transformations (e.g.,
magic-set transformation) for performance tuning. Our vision is to make Raqlet
a robust platform that enables rapid cross-paradigm prototyping, portable
recursive queries, and formal reasoning about recursion even when targeting
diverse query execution engines.

</details>


### [142] [BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases](https://arxiv.org/abs/2508.04031)
*Lianggui Weng,Dandan Liu,Rong Zhu,Bolin Ding,Jingren Zhou*

Main category: cs.DB

TL;DR: BridgeScope是一个连接LLM和数据库的通用工具包，通过模块化SQL操作、权限管理和代理机制解决了现有交互设计中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与数据库的交互存在可用性、安全性和效率问题，需要一种更优的解决方案。

Method: BridgeScope通过模块化SQL工具、权限对齐和代理机制实现高效、安全的数据库操作。

Result: 评估显示BridgeScope显著提升LLM代理的数据库操作效率，减少80%的token使用，并支持数据密集型任务。

Conclusion: BridgeScope为下一代智能数据自动化提供了坚实基础。

Abstract: As large language models (LLMs) demonstrate increasingly powerful reasoning
and orchestration capabilities, LLM-based agents are rapidly proliferating for
complex data-related tasks. Despite this progress, the current design of how
LLMs interact with databases exhibits critical limitations in usability,
security, privilege management, and data transmission efficiency. To resolve
these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs
and databases through three key innovations. First, it modularizes SQL
operations into fine-grained tools for context retrieval, CRUD execution, and
ACID-compliant transaction management, enabling more precise and LLM-friendly
functionality controls. Second, it aligns tool implementations with both
database privileges and user security policies to steer LLMs away from unsafe
or unauthorized operations, improving task execution efficiency while
safeguarding database security. Third, it introduces a proxy mechanism for
seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All
of these designs are database-agnostic and can be transparently integrated with
existing agent architectures. We also release an open-source implementation of
BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate
that BridgeScope enables LLM agents to operate databases more effectively,
reduces token usage by up to 80% through improved security awareness, and
uniquely supports data-intensive workflows beyond existing toolkits,
establishing BridgeScope as a robust foundation for next-generation intelligent
data automation.

</details>


### [143] [Rethinking Analytical Processing in the GPU Era](https://arxiv.org/abs/2508.04701)
*Bobbi Yogatama,Yifei Yang,Kevin Kristensen,Devesh Sarda,Abigale Kim,Adrian Cockcroft,Yu Teng,Joshua Patterson,Gregory Kimball,Wes McKinney,Weiwei Gong,Xiangyao Yu*

Main category: cs.DB

TL;DR: GPU数据分析时代已至，硬件和软件的进步消除了GPU数据分析广泛采用的主要障碍。Sirius是一个开源GPU原生SQL引擎，提供对现有数据库的无缝加速。


<details>
  <summary>Details</summary>
Motivation: 近年来硬件（如更大的GPU内存、更快的互连和IO、成本下降）和软件（如可组合数据系统和成熟库）的进步，为GPU数据分析的广泛应用铺平了道路。

Method: Sirius是一个GPU原生SQL引擎，利用libcudf等库实现高性能关系操作，并通过Substrait查询表示无缝替换现有数据库的CPU引擎。

Result: 在TPC-H测试中，Sirius与DuckDB集成时单节点性能提升7倍，与Apache Doris集成时分布式性能提升12.5倍。

Conclusion: Sirius展示了GPU数据分析的潜力，为现有数据库提供了高性能的无缝加速方案。

Abstract: The era of GPU-powered data analytics has arrived. In this paper, we argue
that recent advances in hardware (e.g., larger GPU memory, faster interconnect
and IO, and declining cost) and software (e.g., composable data systems and
mature libraries) have removed the key barriers that have limited the wider
adoption of GPU data analytics. We present Sirius, a prototype open-source
GPU-native SQL engine that offers drop-in acceleration for diverse data
systems. Sirius treats GPU as the primary engine and leverages libraries like
libcudf for high-performance relational operators. It provides drop-in
acceleration for existing databases by leveraging the standard Substrait query
representation, replacing the CPU engine without changing the user-facing
interface. On TPC-H, Sirius achieves 7x speedup when integrated with DuckDB in
a single node at the same hardware rental cost, and up to 12.5x speedup when
integrated with Apache Doris in a distributed setting.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [144] [Single Fragment Forensic Coding from Discrepancy Theory](https://arxiv.org/abs/2508.03938)
*Junsheng Liu,Netanel Raviv*

Main category: cs.IT

TL;DR: 该论文研究了在3D打印中嵌入唯一标识符以确保可追溯性，提出了一种基于纠错码的方法，能够从物体部分或损坏的片段中解码信息。


<details>
  <summary>Details</summary>
Motivation: 3D打印的普及带来了安全风险，如非法制造无法追踪的武器和违禁品。嵌入唯一标识符有助于追踪和调查非法使用。

Method: 利用纠错码原理，提出了一维、二维和三维信息的编码方案，支持从足够大的片段中解码信息，并引入能纠正比特翻转错误的编码。

Result: 新方法能够从单一足够大的片段中解码信息，并支持二维和三维数据的恢复，同时具备纠错能力。

Conclusion: 该研究为3D打印物体的可追溯性提供了高效解决方案，扩展了数据嵌入和恢复的能力。

Abstract: Three-dimensional (3D) printing's accessibility enables rapid manufacturing
but also poses security risks, such as the unauthorized production of
untraceable firearms and prohibited items. To ensure traceability and
accountability, embedding unique identifiers within printed objects is
essential, in order to assist forensic investigation of illicit use. This paper
models data embedding in 3D printing using principles from error-correcting
codes, aiming to recover embedded information from partial or altered fragments
of the object. Previous works embedded one-dimensional data (i.e., a vector)
inside the object, and required almost all fragments of the object for
successful decoding. In this work, we study a problem setting in which only one
sufficiently large fragment of the object is available for decoding. We first
show that for one-dimensional embedded information the problem can be easily
solved using existing tools. Then, we introduce novel encoding schemes for
two-dimensional information (i.e., a matrix), and three-dimensional information
(i.e., a cube) which enable the information to be decoded from any sufficiently
large rectangle-shaped or cuboid-shaped fragment. Lastly, we introduce a code
that is also capable of correcting bit-flip errors, using techniques from
recently proposed codes for DNA storage. Our codes operate at non-vanishing
rates, and involve concepts from discrepancy theory called Van der Corput sets
and Halton-Hammersely sets in novel ways.

</details>


### [145] [One-weight codes in the sum-rank metric](https://arxiv.org/abs/2508.04262)
*Usman Mushrraf,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 本文研究了在sum-rank度量下的一权码的几何特性，分类了恒定秩列表码，探索了恒定秩轮廓码，并研究了同时为MSRD码的一权码。


<details>
  <summary>Details</summary>
Motivation: sum-rank度量下的一权码分类复杂，本文旨在填补这一领域的空白，提供新的构造和分类结果。

Method: 分类恒定秩列表码，探索恒定秩轮廓码的结构，并通过几何方法构造维度为二和三的MSRD一权码。

Result: 提出了恒定秩列表码的完整分类，给出了恒定秩轮廓码的首批例子和部分结构，并针对维度二和三的MSRD一权码提出了新构造和不存在性结果。

Conclusion: 本文扩展了对sum-rank度量下一权码的理解，为未来研究提供了基础。

Abstract: One-weight codes, in which all nonzero codewords share the same weight, form
a highly structured class of linear codes with deep connections to finite
geometry. While their classification is well understood in the Hamming and rank
metrics - being equivalent to (direct sums of) simplex codes - the sum-rank
metric presents a far more intricate landscape. In this work, we explore the
geometry of one-weight sum-rank metric codes, focusing on three distinct
classes. First, we introduce and classify \emph{constant rank-list} sum-rank
codes, where each nonzero codeword has the same tuple of ranks, extending
results from the rank-metric setting. Next, we investigate the more general
\emph{constant rank-profile} codes, where, up to reordering, each nonzero
codeword has the same tuple of ranks. Although a complete classification
remains elusive, we present the first examples and partial structural results
for this class. Finally, we consider one-weight codes that are also MSRD
(Maximum Sum-Rank Distance) codes. For dimension two, constructions arise from
partitions of scattered linear sets on projective lines. For dimension three,
we connect their existence to that of special $2$-fold blocking sets in the
projective plane, leading to new bounds and nonexistence results over certain
fields.

</details>


### [146] [Is Lattice Reduction Necessary for Vector Perturbation Precoding?](https://arxiv.org/abs/2508.04313)
*Dominik Semmler,Wolfgang Utschick,Michael Joham*

Main category: cs.IT

TL;DR: 研究表明，基于互信息而非未编码SER/BER时，LR辅助的VP预编码未必优于THP，且速率分配矩阵对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨在互信息作为评价指标时，LR辅助的VP预编码与THP的性能差异，以及速率分配矩阵的作用。

Method: 分析互信息表达式中的速率分配矩阵，推导其最优选择，并比较不同算法的性能。

Result: 优化后的速率分配矩阵显示，LR辅助算法无法超越THP的速率，THP方法更有效。

Conclusion: 速率分配矩阵对性能至关重要，尤其在信道条件不佳时；THP在互信息指标下表现更优。

Abstract: Vector perturbation (VP) precoding is an effective nonlinear precoding
technique in the downlink (DL) with modulo channels. Especially, when combined
with Lattice reduction (LR), low-complexity algorithms achieve very promising
performances, outperforming other popular nonlinear precoding techniques like
Tomlinson-Harashima precoding (THP). However, these results are based on the
uncoded symbol error rate (SER) or uncoded bit error rate (BER). We show that
when using the mutual information as the figure of merit, the observation is
fundamentally different and that these algorithms generally do not outperform
THP. Within the expression of the mutual information, a rate allocation matrix
can be incorporated, which has not received much attention so far. In this
article, we derive the optimal choice of this matrix for different algorithms,
and we show that this matrix is indeed crucial for the performance, especially
for ill-conditioned channels. Furthermore, when using an optimized choice of
this matrix, we show that the classical LR-aided algorithms cannot exceed the
rate of THP, highlighting the effectiveness of the THP method. This concept can
be generalized to a whole class of algorithms for which LR yields no
improvement. We derive the corresponding properties and categorize various
algorithms accordingly.

</details>


### [147] [Bases of Riemann-Roch spaces associated with arbitrary elliptic curve divisors and their application in constructing various elliptic Codes families](https://arxiv.org/abs/2508.04340)
*Artyom Kuninets,Ekaterina Malygina*

Main category: cs.IT

TL;DR: 论文提出了为椭圆码的Riemann-Roch空间构建显式基的方法，并应用于准循环椭圆码及其子域子码，以及Goppa类椭圆码。


<details>
  <summary>Details</summary>
Motivation: 为代数几何码应用提供任意除数的Riemann-Roch空间基的显式描述，以支持高效编码和揭示密码学应用中的结构特性。

Method: 确定椭圆码的Riemann-Roch空间基，开发精确算法以构建任意除数对应的基。

Result: 成功构建了准循环椭圆码、子域子码和Goppa类椭圆码的基，并揭示了其在密码学中的新分析方法。

Conclusion: 显式基的构建不仅优化了编码效率，还为密码学应用提供了新的结构分析工具。

Abstract: In this paper, we determine explicit bases for Riemann--Roch spaces
associated with various families of elliptic codes. We establish the
feasibility and provide exact algorithms for constructing bases of
Riemann--Roch spaces corresponding to arbitrary divisors on elliptic curves.
These results are subsequently applied to derive bases for quasi-cyclic
elliptic codes and their subfield subcodes as well as for the class of
Goppa-like elliptic codes. For algebraic geometry code applications, having an
explicit description of Riemann--Roch space bases for arbitrary divisors is
particularly valuable as it simultaneously enables efficient code construction
and reveals structural properties of the codes leading to the new cryptanalysis
methods when these codes are employed in cryptographic schemes

</details>


### [148] [Grid-like Error-Correcting Codes for Matrix Multiplication with Better Correcting Capability](https://arxiv.org/abs/2508.04355)
*Hao Shi,Zhengyi Jiang,Zhongyi Huang,Bo Bai,Gong Zhang,Hanxu Hou*

Main category: cs.IT

TL;DR: 提出了一种针对矩阵乘法的错误校正编码框架，用于检测和纠正分布式深度学习训练中的静默数据损坏（SDC），显著提高了计算的容错能力。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式训练中，静默数据损坏（SDC）在矩阵乘法过程中可能导致模型性能严重下降，而现有方法难以检测和纠正这些错误。

Method: 采用基于网格的结构编码方案，增强错误定位和纠正能力，适用于多矩阵参与的计算。

Result: 实验表明，该方法能以100%的可靠性纠正分布在三个矩阵中的最多两个错误符号，计算时间仅增加24%。

Conclusion: 提出的编码框架在理论和实验上均验证了其正确性和鲁棒性，显著提升了矩阵乘法在分布式训练中的容错性。

Abstract: Matrix multiplication over the real field constitutes a foundational
operation in the training of deep learning models, serving as a computational
cornerstone for both forward and backward propagation processes. However, the
presence of silent data corruption (SDC) in large-scale distributed training
environments poses a significant threat to model convergence and predictive
accuracy, particularly when such errors manifest during matrix multiplication.
Due to their transient and non-intrusive nature, these errors often evade
detection, allowing them to propagate and accumulate over time, ultimately
leading to substantial degradation in model performance. In this paper, we
introduce a novel error-correcting coding framework specifically tailored for
matrix multiplication operations. Our proposed framework is designed to detect
and correct multiple computational errors that may arise during the execution
of matrix products. By leveraging a grid-based structural encoding scheme, our
approach enhances error localization and correction capabilities across all
participating matrices, thereby significantly improving the fault tolerance of
the computation. Experimental results demonstrate that our method achieves
deterministic correction of up to two erroneous symbols distributed across
three matrices with 100\% reliability, while incurring only a 24\% overhead in
computational time on GPU architectures. Furthermore, we provide a rigorous
theoretical analysis of the error-correction properties inherent to our coding
scheme, establishing its correctness and robustness under well-defined fault
models.

</details>


### [149] [Tradeoff Between the Number of Transmitted Molecules and the BER Performance in Molecular Communication between Bionanosensors](https://arxiv.org/abs/2508.04466)
*Dongliang Jing,Linjuan Li,Lin Lin,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 本文研究了分子通信中传输分子数量与误码率（BER）之间的平衡问题，提出了一种基于梯度下降算法的优化方法，以实现最佳通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 分子通信中，发射器的存储容量限制了可传输分子数量，影响了通信可靠性。本文旨在找到传输分子数量与BER之间的最佳平衡点。

Method: 首先分析传输分子数量与BER的关系，引入考虑两者权重的平衡函数，并进行归一化处理。随后使用梯度下降算法确定最优传输分子数量。

Result: 理论和仿真结果表明，所提方法能在分子数量与BER之间实现理想平衡。

Conclusion: 通过优化传输分子数量，可以有效提升分子通信系统的可靠性。

Abstract: In the domain of molecular communication (MC), information is conveyed
through the characteristics of molecules transmitted between the transmitter
and the receiver bionanosensors via propagation. The constrained size of the
transmitter imposes limitations on its storage capacity, constraining the
number of available molecules for transmission, with a resulting effect on
communication reliability. This paper primarily focuses on achieving an
equilibrium between the number of transmitted molecules and the bit error rate
(BER) performance. To this end, we first analyze the relationship between the
number of transmitted molecules and the BER performance. Subsequently, a
balancing function that considers both the number of transmitted molecules and
the BER performance is introduced, taking into account the molecules'
respective weights. Given the difference in magnitude between the number of
transmitted molecules and the BER, these parameters are normalized to
facilitate analysis. Subsequently, a Gradient Descent Algorithm is employed to
determine the optimal number of transmitted molecules, aiming to achieve the
optimal equilibrium in the analyzed MC system. Theoretical and simulation
results are provided, substantiating that the optimal outcome indeed
establishes an ideal balance between the number of transmitted molecules and
the BER.

</details>


### [150] [Energy-Efficient Hybrid Beamfocusing for Near-Field Integrated Sensing and Communication](https://arxiv.org/abs/2508.04627)
*Wenhao Hu,Zhenyao He,Wei Xu,Yongming Huang,Derrick Wing Kwan Ng,Naofal Al-Dhahir*

Main category: cs.IT

TL;DR: 论文研究了6G网络中集成感知与通信（ISAC）的能量高效混合波束聚焦设计，针对近场效应和硬件成本问题，提出了优化方法并揭示了系统能效与目标估计精度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的ISAC技术面临近场效应、高硬件成本和功耗问题，混合架构设计成为硬件和能源高效的解决方案。

Method: 推导了点目标和扩展目标的CRB/BCRB，通过优化发射波束聚焦最小化CRB/BCRB，并利用惩罚连续凸逼近和交替优化算法设计混合波束形成器。

Result: 仿真结果表明，近场区域可实现联合距离和角度估计，但混合架构会降低距离估计精度，且系统能效提升会牺牲目标估计精度。

Conclusion: 研究揭示了系统能效与目标估计精度之间的权衡，为6G ISAC系统的设计提供了重要参考。

Abstract: Integrated sensing and communication (ISAC) is a pivotal component of
sixth-generation (6G) wireless networks, leveraging high-frequency bands and
massive multiple-input multiple-output (M-MIMO) to deliver both high-capacity
communication and high-precision sensing. However, these technological
advancements lead to significant near-field effects, while the implementation
of M-MIMO \mbox{is associated with considerable} hardware costs and escalated
power consumption. In this context, hybrid architecture designs emerge as both
hardware-efficient and energy-efficient solutions. Motivated by these
considerations, we investigate the design of energy-efficient hybrid
beamfocusing for near-field ISAC under two distinct target scenarios, i.e., a
point target and an extended target. Specifically, we first derive the
closed-form Cram\'{e}r-Rao bound (CRB) of joint angle-and-distance estimation
for the point target and the Bayesian CRB (BCRB) of the target response matrix
for the extended target. Building on these derived results, we minimize the
CRB/BCRB by optimizing the transmit beamfocusing, while ensuring the energy
efficiency (EE) of the system and the quality-of-service (QoS) for
communication users. To address the resulting \mbox{nonconvex problems}, we
first utilize a penalty-based successive convex approximation technique with a
fully-digital beamformer to obtain a suboptimal solution. Then, we propose an
efficient alternating \mbox{optimization} algorithm to design the
analog-and-digital beamformer. \mbox{Simulation} results indicate that joint
distance-and-angle estimation is feasible in the near-field region. However,
the adopted hybrid architectures inevitably degrade the accuracy of distance
estimation, compared with their fully-digital counterparts. Furthermore,
enhancements in system EE would compromise the accuracy of target estimation,
unveiling a nontrivial tradeoff.

</details>
