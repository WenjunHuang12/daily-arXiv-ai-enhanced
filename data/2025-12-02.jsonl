{"id": "2512.00105", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00105", "abs": "https://arxiv.org/abs/2512.00105", "authors": ["Djawad Bekkoucha", "Lamine Diop", "Abdelkader Ouali", "Bruno Crémilleux", "Patrice Boizumault"], "title": "Efficiently Sampling Interval Patterns from Numerical Databases", "comment": null, "summary": "Pattern sampling has emerged as a promising approach for information discovery in large databases, allowing analysts to focus on a manageable subset of patterns. In this approach, patterns are randomly drawn based on an interestingness measure, such as frequency or hyper-volume. This paper presents the first sampling approach designed to handle interval patterns in numerical databases. This approach, named Fips, samples interval patterns proportionally to their frequency. It uses a multi-step sampling procedure and addresses a key challenge in numerical data: accurately determining the number of interval patterns that cover each object. We extend this work with HFips, which samples interval patterns proportionally to both their frequency and hyper-volume. These methods efficiently tackle the well-known long-tail phenomenon in pattern sampling. We formally prove that Fips and HFips sample interval patterns in proportion to their frequency and the product of hyper-volume and frequency, respectively. Through experiments on several databases, we demonstrate the quality of the obtained patterns and their robustness against the long-tail phenomenon."}
{"id": "2512.00662", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.00662", "abs": "https://arxiv.org/abs/2512.00662", "authors": ["Christian Mancas", "Diana Christina Mancas"], "title": "MatBase algorithm for translating (E)MDM schemes into E-R data models", "comment": "Submitted on 11/27/2025 to the Journal of Data Science and Intelligent Systems, BON VIEW PUB. PTE. LTD, Singapore", "summary": "This paper presents a pseudocode algorithm for translating (Elementary) Mathematical Data Model schemes into Entity-Relationship data models. We prove that this algorithm is linear, sound, complete, and semi-optimal. As an example, we apply this algorithm to an (Elementary) Mathematical Data Model scheme for a genealogical tree sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models."}
{"id": "2512.01092", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01092", "abs": "https://arxiv.org/abs/2512.01092", "authors": ["Sofia Sideri", "Georgia Troullinou", "Elisjana Ymeralli", "Vasilis Efthymiou", "Dimitris Plexousakis", "Haridimos Kondylakis"], "title": "PG-HIVE: Hybrid Incremental Schema Discovery for Property Graphs", "comment": null, "summary": "Property graphs have rapidly become the de facto standard for representing and managing complex, interconnected data, powering applications across domains from knowledge graphs to social networks. Despite the advantages, their schema-free nature poses major challenges for integration, exploration, visualization, and efficient querying. To bridge this gap, we present PG-HIVE, a novel framework for automatic schema discovery in property graphs. PG-HIVE goes beyond existing approaches by uncovering latent node and edge types, inferring property datatypes, constraints, and cardinalities, and doing so even in the absence of explicit labeling information. Leveraging a unique combination of Locality-Sensitive Hashing with property- and label-based clustering, PG-HIVE identifies structural similarities at scale. Moreover, it introduces incremental schema discovery, eliminating costly recomputation as new data arrives. Through extensive experimentation, we demonstrate that PG-HIVE consistently outperforms state-of-the-art solutions, in both accuracy (by up to 65% for nodes and 40% for edges), and efficiency (up to 1.95x faster execution), unlocking the full potential of schema-aware property graph management."}
{"id": "2512.01490", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01490", "abs": "https://arxiv.org/abs/2512.01490", "authors": ["Marius Ottosen", "Magnus Keinicke Parlo", "Philippe Bonnet"], "title": "DuckDB on xNVMe", "comment": null, "summary": "DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs."}
{"id": "2512.00883", "categories": ["cs.MM", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.00883", "abs": "https://arxiv.org/abs/2512.00883", "authors": ["Jiahua Wang", "Shannan Yan", "Leqi Zheng", "Jialong Wu", "Yaoxin Mao"], "title": "Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound", "comment": null, "summary": "World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance."}
{"id": "2512.00371", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00371", "abs": "https://arxiv.org/abs/2512.00371", "authors": ["Swadesh Sistla", "Max Kleiman-Weiner"], "title": "Evaluating LLMs in Open-Source Games", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas."}
{"id": "2512.00004", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00004", "abs": "https://arxiv.org/abs/2512.00004", "authors": ["Jihang Li", "Bing Xu", "Zulong Chen", "Chuanfei Xu", "Minping Chen", "Suyu Liu", "Ying Zhou", "Zeyi Wen"], "title": "Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions", "comment": null, "summary": "Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY."}
{"id": "2512.00111", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.00111", "abs": "https://arxiv.org/abs/2512.00111", "authors": ["Dexin Chen"], "title": "An O(1) Space Algorithm for N-Dimensional Tensor Rotation: A Generalization of the Reversal Method", "comment": "8 pages", "summary": "The rotation of multi-dimensional arrays, or tensors, is a fundamental operation in computer science with applications ranging from data processing to scientific computing. While various methods exist, achieving this rotation in-place (i.e., with O(1) auxiliary space) presents a significant algorithmic challenge. The elegant three-reversal algorithm provides a well-known O(1) space solution for one-dimensional arrays. This paper introduces a generalization of this method to N dimensions, resulting in the \"$2^n+1$ reversal algorithm\". This algorithm achieves in-place tensor rotation with O(1) auxiliary space and a time complexity linear in the number of elements. We provide a formal definition for N-dimensional tensor reversal, present the algorithm with detailed pseudocode, and offer a rigorous proof of its correctness, demonstrating that the pattern observed in one dimension ($2^1+1=3$ reversals) and two dimensions ($2^2+1=5$ reversals) holds for any arbitrary number of dimensions."}
{"id": "2512.00135", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.00135", "abs": "https://arxiv.org/abs/2512.00135", "authors": ["Amirreza Zamani", "Ayfer Özgür", "Mikael Skoglund"], "title": "An Information Geometric Approach to Fairness With Equalized Odds Constraint", "comment": null, "summary": "We study the statistical design of a fair mechanism that attains equalized odds, where an agent uses some useful data (database) $X$ to solve a task $T$. Since both $X$ and $T$ are correlated with some latent sensitive attribute $S$, the agent designs a representation $Y$ that satisfies an equalized odds, that is, such that $I(Y;S|T) =0$. In contrast to our previous work, we assume here that the agent has no direct access to $S$ and $T$; hence, the Markov chains $S - X - Y$ and $T - X - Y$ hold. Furthermore, we impose a geometric structure on the conditional distribution $P_{S|Y}$, allowing $Y$ and $S$ to have a small correlation, bounded by a threshold. When the threshold is small, concepts from information geometry allow us to approximate mutual information and reformulate the fair mechanism design problem as a quadratic program with closed-form solutions under certain constraints. For other cases, we derive simple, low-complexity lower bounds based on the maximum singular value and vector of a matrix. Finally, we compare our designs with the optimal solution in a numerical example."}
{"id": "2512.01693", "categories": ["cs.DB", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.01693", "abs": "https://arxiv.org/abs/2512.01693", "authors": ["Honghui Kim", "Dohoon Kim", "Jihan Kim"], "title": "LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database", "comment": null, "summary": "Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science."}
{"id": "2512.00928", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2512.00928", "abs": "https://arxiv.org/abs/2512.00928", "authors": ["Jiajun Cao", "Qinggang Zhang", "Yunbo Tang", "Zhishang Xiang", "Chang Yang", "Jinsong Su"], "title": "Augmenting Intra-Modal Understanding in MLLMs for Robust Multimodal Keyphrase Generation", "comment": null, "summary": "Multimodal keyphrase generation (MKP) aims to extract a concise set of keyphrases that capture the essential meaning of paired image-text inputs, enabling structured understanding, indexing, and retrieval of multimedia data across the web and social platforms. Success in this task demands effectively bridging the semantic gap between heterogeneous modalities. While multimodal large language models (MLLMs) achieve superior cross-modal understanding by leveraging massive pretraining on image-text corpora, we observe that they often struggle with modality bias and fine-grained intra-modal feature extraction. This oversight leads to a lack of robustness in real-world scenarios where multimedia data is noisy, along with incomplete or misaligned modalities. To address this problem, we propose AimKP, a novel framework that explicitly reinforces intra-modal semantic learning in MLLMs while preserving cross-modal alignment. AimKP incorporates two core innovations: (i) Progressive Modality Masking, which forces fine-grained feature extraction from corrupted inputs by progressively masking modality information during training; (ii) Gradient-based Filtering, that identifies and discards noisy samples, preventing them from corrupting the model's core cross-modal learning. Extensive experiments validate AimKP's effectiveness in multimodal keyphrase generation and its robustness across different scenarios."}
{"id": "2512.00513", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00513", "abs": "https://arxiv.org/abs/2512.00513", "authors": ["Xun Shao", "Ryuuto Shimizu"], "title": "Truthful Double Auctions under Approximate VCG: Immediate-Penalty Enforcement in P2P Energy Trading", "comment": null, "summary": "This paper examines truthful double auctions when exact VCG allocation is computationally infeasible and repeated-game punishments are impractical. We analyze an $α$-approximate VCG mechanism and show that truthful reporting becomes a subgame-perfect equilibrium when the immediate penalty exceeds the incentive gap created by approximation, scaled by monitoring accuracy. To validate this result, we construct a PPO-based multi-agent reinforcement learning environment for P2P smart-grid trading, where prosumers incur penalties for bidding far from their true valuations. Across systematic experiments varying approximation accuracy, tolerance, penalty magnitude, and discounting, the learned behavior closely matches theoretical predictions. The findings demonstrate that immediate-penalty approximate VCG mechanisms provide a practical and transparent approach to sustaining truthful behavior in distributed market settings."}
{"id": "2512.00007", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00007", "abs": "https://arxiv.org/abs/2512.00007", "authors": ["Jingyi Huang", "Yuyi Yang", "Mengmeng Ji", "Charles Alba", "Sheng Zhang", "Ruopeng An"], "title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking", "comment": null, "summary": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation."}
{"id": "2512.00153", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.00153", "abs": "https://arxiv.org/abs/2512.00153", "authors": ["Mridul Ahi", "Keerti Choudhary", "Shlok Pande", "Pushpraj", "Lakshay Saggi"], "title": "Maximum-Flow and Minimum-Cut Sensitivity Oracles for Directed Graphs", "comment": null, "summary": "Given a digraph $G = (V, E)$ with a designated source $s$, sink $t$, and an $(s,t)$-max-flow of value $λ$, we present constructions for max-flow and min-cut sensitivity oracles, and introduce the concept of a fault-tolerant flow family, which may be of independent interest. Our main contributions are as follows.\n  1. Fault-Tolerant Flow Family: For any graph $G$ with $(s,t)$-max-flow value $λ$, we construct a family $B$ of $2λ+1$ $(s,t)$-flows such that for every edge $e$, $B$ contains an $(s,t)$-max-flow of $G-e$.\n  2. Max-Flow Sensitivity Oracle: We construct a single as well as dual-edge sensitivity oracle for $(s,t)$-max-flow that requires only $O(λn)$ space. Given any set $F$ of up to two failing edges, the oracle reports the updated max-flow value in $G-F$ in $O(n)$ time. Additionally, for the single-failure case, the oracle can determine in constant time whether the flow through an edge $x$ changes when another edge $e$ fails.\n  3. Min-Cut Sensitivity Oracle for Dual Failures: Recently, Baswana et al. (ICALP'22) designed an $O(n^2)$-sized oracle for answering $(s,t)$-min-cut size queries under dual edge failures in constant time. We extend this by focusing on graphs with small min-cut values $λ$, and present a more compact oracle of size $O(λn)$ that answers such min-cut size queries in constant time and reports the corresponding $(s,t)$-min-cut partition in $O(n)$ time.\n  4. Min-Cut Sensitivity Oracle for Multiple Failures: We extend our results to the general case of $k$ edge failures. For any graph with $(s,t)$-min-cut of size $λ$, we construct a $k$-fault-tolerant min-cut oracle with space complexity $O_{λ,k}(n \\log n)$ that answers min-cut size queries in $O_{λ,k}(\\log n)$ time."}
{"id": "2512.00248", "categories": ["cs.IT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.00248", "abs": "https://arxiv.org/abs/2512.00248", "authors": ["Rohan Goyal", "Prahladh Harsha", "Mrinal Kumar", "Ashutosh Shankar"], "title": "Fast list recovery of univariate multiplicity and folded Reed-Solomon codes", "comment": null, "summary": "A recent work of Goyal, Harsha, Kumar and Shankar gave nearly linear time algorithms for the list decoding of Folded Reed-Solomon codes (FRS) and univariate multiplicity codes up to list decoding capacity in their natural setting of parameters. A curious aspect of this work was that unlike most list decoding algorithms for codes that also naturally extend to the problem of list recovery, the algorithm in the work of Goyal et al. seemed to be crucially tied to the problem of list decoding. In particular, it wasn't clear if their algorithm could be generalized to solve the problem of list recovery FRS and univariate multiplicity codes in near linear time.\n  In this work, we address this question and design $\\tilde{O}(n)$-time algorithms for list recovery of Folded Reed-Solomon codes and univariate Multiplicity codes up to capacity, where $n$ is the blocklength of the code. For our proof, we build upon the lattice based ideas crucially used by Goyal et al. with one additional technical ingredient - we show the construction of appropriately structured lattices over the univariate polynomial ring that \\emph{capture} the list recovery problem for these codes."}
{"id": "2512.01733", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01733", "abs": "https://arxiv.org/abs/2512.01733", "authors": ["Heyang Li", "Anthony Widjaja Lin", "Domagoj Vrgoč"], "title": "Answering Constraint Path Queries over Graphs", "comment": null, "summary": "Constraints are powerful declarative constructs that allow users to\n  conveniently restrict variable values that potentially range over an\n  infinite domain. In this paper, we propose a constraint path query language\n  over property graphs,\n  which extends Regular Path Queries (RPQs) with SMT constraints on data\n  attributes in the form of equality constraints and Linear\n  Real Arithmetic (LRA) constraints. We provide efficient algorithms\n  for evaluating such path queries over property graphs, which exploits\n  optimization of macro-states (among others, using theory-specific\n  techniques).\n  In particular, we demonstrate how such an algorithm may effectively utilize\n  highly optimized SMT solvers for resolving such constraints over paths.\n  We implement our algorithm in MillenniumDB, an open-source graph engine\n  supporting property graph queries and GQL. Our extensive empirical\n  evaluation in a real-world setting demonstrates the viability of our\n  approach."}
{"id": "2512.01267", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.01267", "abs": "https://arxiv.org/abs/2512.01267", "authors": ["Yuezhang Peng", "Yuxin Liu", "Yao Li", "Sheng Wang", "Fei Wen", "Xie Chen"], "title": "ZO-ASR: Zeroth-Order Fine-Tuning of Speech Foundation Models without Back-Propagation", "comment": "2025 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)", "summary": "Fine-tuning pre-trained speech foundation models for Automatic Speech Recognition (ASR) is prevalent, yet constrained by substantial GPU memory requirements. We introduce ZO-ASR, a memory-efficient Zeroth-Order (ZO) method that avoids Back-Propagation (BP) and activation memory by estimating gradients via forward passes. When combined with SGD optimizer, ZO-ASR-SGD fine-tunes ASR models using only inference memory. Our evaluation spans supervised and unsupervised tasks. For Supervised Domain Adaptation on Whisper-Large-V3, ZO-ASR's multiple query mechanism enhances robustness and achieves up to an 18.9\\% relative Word Error Rate reduction over zero-shot baselines, outperforming existing ZO methods. For unsupervised Test-Time Adaptation on Wav2Vec2-Base, ZO-ASR exhibits moderately lower performance compared to first-order optimizer Adam. Our BP-free approach provides a viable solution for fine-tuning ASR models in computationally resource-constrained or gradient-inaccessible scenarios."}
{"id": "2512.00616", "categories": ["cs.GT", "cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.00616", "abs": "https://arxiv.org/abs/2512.00616", "authors": ["Wesley H. Holliday", "Milan Mossé", "Chase Norman", "Eric Pacuit", "Cynthia Wang"], "title": "Stable Voting and the Splitting of Cycles", "comment": "Forthcoming in Proceedings of the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Algorithms for resolving majority cycles in preference aggregation have been studied extensively in computational social choice. Several sophisticated cycle-resolving methods, including Tideman's Ranked Pairs, Schulze's Beat Path, and Heitzig's River, are refinements of the Split Cycle (SC) method that resolves majority cycles by discarding the weakest majority victories in each cycle. Recently, Holliday and Pacuit proposed a new refinement of Split Cycle, dubbed Stable Voting, and a simplification thereof, called Simple Stable Voting (SSV). They conjectured that SSV is a refinement of SC whenever no two majority victories are of the same size. In this paper, we prove the conjecture up to 6 alternatives and refute it for more than 6 alternatives. While our proof of the conjecture for up to 5 alternatives uses traditional mathematical reasoning, our 6-alternative proof and 7-alternative counterexample were obtained with the use of SAT solving. The SAT encoding underlying this proof and counterexample is applicable far beyond SC and SSV: it can be used to test properties of any voting method whose choice of winners depends only on the ordering of margins of victory by size."}
{"id": "2512.00313", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00313", "abs": "https://arxiv.org/abs/2512.00313", "authors": ["Zhitong Guan", "Yi Wang"], "title": "Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System", "comment": null, "summary": "Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning."}
{"id": "2512.00176", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.00176", "abs": "https://arxiv.org/abs/2512.00176", "authors": ["Kent Quanrud"], "title": "Approximating Directed Connectivity in Almost-Linear Time", "comment": null, "summary": "We present randomized algorithms that compute $(1+ε)$-approximate minimum global edge and vertex cuts in weighted directed graphs in $O(\\log^4(n) / ε)$ and $O(\\log^5(n)/ε)$ single-commodity flows, respectively. With the almost-linear time flow algorithm of [CKL+22], this gives almost linear time approximation schemes for edge and vertex connectivity. By setting $ε$ appropriately, this also gives faster exact algorithms for small vertex connectivity.\n  At the heart of these algorithms is a divide-and-conquer technique called \"shrink-wrapping\" for a certain well-conditioned rooted Steiner connectivity problem. Loosely speaking, for a root $r$ and a set of terminals, shrink-wrapping uses flow to certify the connectivity from a root $r$ to some of the terminals, and for the remaining uncertified terminals, generates an $r$-cut where the sink component both (a) contains the sink component of the minimum $(r,t)$-cut for each uncertified terminal $t$ and (b) has size proportional to the number of uncertified terminals. This yields a divide-and-conquer scheme over the terminals where we can divide the set of terminals and compute their respective minimum $r$-cuts in smaller, contracted subgraphs."}
{"id": "2512.00347", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.00347", "abs": "https://arxiv.org/abs/2512.00347", "authors": ["Zhuang Li", "Wenyi Zhang"], "title": "ORBGRAND Is Exactly Capacity-achieving via Rank Companding", "comment": null, "summary": "Among guessing random additive noise decoding (GRAND) algorithms, ordered reliability bits GRAND (ORBGRAND) has attracted considerable attention due to its efficient use of soft information and suitability for hardware implementation. It has also been shown that ORBGRAND achieves a rate very close to the capacity over additive white Gaussian noise channels with antipodal inputs. In this work, it is further established that, via suitably companding the ranks in ORBGRAND according to the inverse cumulative distribution function (CDF) of channel reliability, the resulting CDF-ORBGRAND algorithm exactly achieves the mutual information of general binary-input memoryless channels under symmetric input distribution, i.e., the symmetric capacity. This result is then applied to bit-interleaved coded modulation (BICM) systems to handle high-order input constellations. Via considering the effects of mismatched decoding due to both BICM and ORBGRAND, it is shown that CDF-ORBGRAND is capable of achieving the BICM capacity, which was initially derived by treating BICM as a set of independent parallel channels."}
{"id": "2512.01442", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01442", "abs": "https://arxiv.org/abs/2512.01442", "authors": ["Heng Xie", "Kang Zhu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu", "Ruibo Fu", "Changsheng Li"], "title": "PSA-MF: Personality-Sentiment Aligned Multi-Level Fusion for Multimodal Sentiment Analysis", "comment": "AAAI 2026 accepted", "summary": "Multimodal sentiment analysis (MSA) is a research field that recognizes human sentiments by combining textual, visual, and audio modalities. The main challenge lies in integrating sentiment-related information from different modalities, which typically arises during the unimodal feature extraction phase and the multimodal feature fusion phase. Existing methods extract only shallow information from unimodal features during the extraction phase, neglecting sentimental differences across different personalities. During the fusion phase, they directly merge the feature information from each modality without considering differences at the feature level. This ultimately affects the model's recognition performance. To address this problem, we propose a personality-sentiment aligned multi-level fusion framework. We introduce personality traits during the feature extraction phase and propose a novel personality-sentiment alignment method to obtain personalized sentiment embeddings from the textual modality for the first time. In the fusion phase, we introduce a novel multi-level fusion method. This method gradually integrates sentimental information from textual, visual, and audio modalities through multimodal pre-fusion and a multi-level enhanced fusion strategy. Our method has been evaluated through multiple experiments on two commonly used datasets, achieving state-of-the-art results."}
{"id": "2512.00733", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.00733", "abs": "https://arxiv.org/abs/2512.00733", "authors": ["Ho-Lin Chen", "Pin-Ju Huang"], "title": "Price of Anarchy of Multi-Stage Machine Scheduling Games", "comment": "15 pages, 4 figures, 3 tables", "summary": "In this paper, we extend the discussion of the price of anarchy of machine scheduling games to a multi-stage machine setting. The multi-stage setting arises naturally in manufacturing pipelines and distributed computing workflows, when each job must traverse a fixed sequence of processing stages. While the classical makespan price of anarchy of $2 - \\frac{1}{m}$ has been established for sequential scheduling on identical machines, the efficiency loss in multi-stage scheduling has, to the best of our knowledge, not been previously analyzed. We assume that each task follows a greedy strategy and gets assigned to the least-loaded machine upon arrival at each stage. Notably, we observe that in multi-stage environments, greedy behavior generally does not coincide with a subgame perfect Nash equilibrium. We continue with analyzing the equilibrium under greedy choices, since it is logical for modeling selfish agents with limited computational power, and may also model a central scheduler performing the common least-load scheduling heuristics. Under this model, we first show that in single-stage scheduling, greedy choice again yields an exact price of anarchy of $2 - \\frac{1}{m}$. In multi-stage scheduling, we show that the completion time from one stage to the next increases by at most two times the maximum job execution time. Using this relationship, we derived the price of anarchy of multistage scheduling under greedy choices to lie within $[2 - \\frac{1}{m}, 3 - \\frac{1}{m}]$, where $m$ denote the maximum number of machines in one stage."}
{"id": "2512.00367", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00367", "abs": "https://arxiv.org/abs/2512.00367", "authors": ["Aparajitha Allamraju", "Maitreya Prafulla Chitale", "Hiranmai Sri Adibhatla", "Rahul Mishra", "Manish Shrivastava"], "title": "Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation", "comment": null, "summary": "Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance."}
{"id": "2512.00506", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.00506", "abs": "https://arxiv.org/abs/2512.00506", "authors": ["Md Rawha Siddiqi Riad", "Md Manzurul Hasan"], "title": "Expected Cost Analysis of Online Facility Assignment on Regular Polygons", "comment": null, "summary": "This paper analyzes the online facility assignment problem in a geometric setting where facilities with unit capacity are positioned at the vertices of a regular $n$-gon. Customers arrive sequentially at uniformly random positions along the edges. They must be assigned immediately to the nearest available facility, with ties broken by coin toss. The sequential nature and unknown future arrivals require a probabilistic analysis of the expected assignment cost. Our main contribution is a recursive characterization of the expected cost: for any occupancy state $S$, the expected remaining cost $V(S)$ equals the average over all edge positions of the immediate assignment cost plus the expected future cost after assignment. We prove that this integral equation can calculate a solution and provide the expected value for small $n$ ($n = 3, 4, 5$). For larger values of $n$ and expected cost, we develop efficient numerical methods, including a discretized dynamic programming approach and Monte Carlo simulation. The work establishes a fundamental probabilistic approach for online assignment in polygonal environments."}
{"id": "2512.00378", "categories": ["cs.IT", "cs.DS", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00378", "abs": "https://arxiv.org/abs/2512.00378", "authors": ["Nikit Phadke"], "title": "The Information Theory of Similarity", "comment": null, "summary": "We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(Δ^{-2} \\log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem."}
{"id": "2512.01021", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.01021", "abs": "https://arxiv.org/abs/2512.01021", "authors": ["Aditya Aradhye", "David Lagziel", "Eilon Solan"], "title": "Mechanism Design with Spiteful Agents", "comment": null, "summary": "We study a mechanism-design problem in which spiteful agents strive to not only maximize their rewards but also, contingent upon their own payoff levels, seek to lower the opponents' rewards. We characterize all individually rational (IR) and incentive-compatible (IC) mechanisms that are immune to such spiteful behavior, showing that they take the form of threshold mechanisms with an ordering of the agents. Building on this characterization, we prove two impossibility results: under either anonymity or efficiency, any such IR and IC mechanism collapses to the null mechanism, which never allocates the item to any agent. Leveraging these findings, we partially extend our analysis to a multi-item setup. These results illuminate the challenges of auctioning items in the natural presence of other-regarding preferences."}
{"id": "2512.00439", "categories": ["cs.IR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00439", "abs": "https://arxiv.org/abs/2512.00439", "authors": ["Xiaoshan Yu", "Ziwei Huang", "Shangshang Yang", "Ziwen Wang", "Haiping Ma", "Xingyi Zhang"], "title": "PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing", "comment": "AAAI-2026, 9 pages", "summary": "With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights."}
{"id": "2512.00632", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.00632", "abs": "https://arxiv.org/abs/2512.00632", "authors": ["William Swartworth", "David P. Woodruff", "Samson Zhou"], "title": "Perfect $L_p$ Sampling with Polylogarithmic Update Time", "comment": "FOCS 2025", "summary": "Perfect $L_p$ sampling in a stream was introduced by Jayaram and Woodruff (FOCS 2018) as a streaming primitive which, given turnstile updates to a vector $x \\in \\{-\\text{poly}(n), \\ldots, \\text{poly}(n)\\}^n$, outputs an index $i^* \\in \\{1, 2, \\ldots, n\\}$ such that the probability of returning index $i$ is exactly \\[\\Pr[i^* = i] = \\frac{|x_i|^p}{\\|x\\|_p^p} \\pm \\frac{1}{n^C},\\] where $C > 0$ is an arbitrarily large constant. Jayaram and Woodruff achieved the optimal $\\tilde{O}(\\log^2 n)$ bits of memory for $0 < p < 2$, but their update time is at least $n^C$ per stream update. Thus an important open question is to achieve efficient update time while maintaining optimal space. For $0 < p < 2$, we give the first perfect $L_p$-sampler with the same optimal amount of memory but with only $\\text{poly}(\\log n)$ update time. Crucial to our result is an efficient simulation of a sum of reciprocals of powers of truncated exponential random variables by approximating its characteristic function, using the Gil-Pelaez inversion formula, and applying variants of the trapezoid formula to quickly approximate it."}
{"id": "2512.00531", "categories": ["cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00531", "abs": "https://arxiv.org/abs/2512.00531", "authors": ["Saeed Mashdour", "André R. Flores", "Rodrigo C. de Lamare"], "title": "Robust Precoding for Resilient Cell-Free Networks", "comment": "2 figures, 6 pages", "summary": "This paper presents a robust precoder design for resilient cell-free massive MIMO (CF-mMIMO) systems that minimizes the weighted sum of desired signal mean square error (MSE) and residual interference leakage power under a total transmit power constraint. The proposed robust precoder incorporates channel state information (CSI) error statistics to enhance resilience against CSI imperfections. We employ an alternating optimization algorithm initialized with a minimum MSE-type solution, which iteratively refines the precoder while maintaining low computational complexity and ensuring fast convergence. Numerical results show that the proposed method significantly outperforms conventional linear precoders, providing an effective balance between performance and computational efficiency."}
{"id": "2512.01112", "categories": ["cs.GT", "q-fin.RM", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2512.01112", "abs": "https://arxiv.org/abs/2512.01112", "authors": ["Tarun Chitra"], "title": "Autodeleveraging: Impossibilities and Optimization", "comment": null, "summary": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \\$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \\emph{trilemma}: no policy can simultaneously satisfy exchange \\emph{solvency}, \\emph{revenue}, and \\emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \\emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \\$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by approximately $8\\times$ relative to our optimal policy, imposing roughly \\$630 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency."}
{"id": "2512.00596", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00596", "abs": "https://arxiv.org/abs/2512.00596", "authors": ["Jiahao Tian", "Zhenkai Wang"], "title": "DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems", "comment": null, "summary": "Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems."}
{"id": "2512.01049", "categories": ["cs.DS", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2512.01049", "abs": "https://arxiv.org/abs/2512.01049", "authors": ["Heman Shakeri", "Torben Amtoft", "Behnaz Moradi-Jamei", "Nathan Albin", "Pietro Poggi-Corradini"], "title": "A Fast Algorithm for Finding Minimum Weight Cycles in Mining Cyclic Graph Topologies", "comment": "16 pages, 4 figures", "summary": "Cyclic structures are fundamental topological features in graphs, playing critical roles in network robustness, information flow, community structure, and various dynamic processes. Algorithmic tools that can efficiently probe and analyze these cyclic topologies are increasingly vital for tasks in graph mining, network optimization, bioinformatics, and social network analysis. A core primitive for quantitative analysis of cycles is finding the Minimum Weight Cycle (MWC), representing the shortest cyclic path in a weighted graph. However, computing the MWC efficiently remains a challenge, particularly compared to shortest path computations. This paper introduces a novel deterministic algorithm for finding the MWC in general weighted graphs. Our approach adapts the structure of Dijkstra's algorithm by introducing and minimizing a \\textit{composite distance} metric, effectively translating the global cycle search into an iterative node-centric optimization. We provide a rigorous proof of correctness based on loop invariants. We detail two mechanisms for accelerating the search: a provable node discarding technique based on intermediate results, and a highly effective graph pruning heuristic. This heuristic dynamically restricts the search to relevant subgraphs, leveraging the principle of locality often present in complex networks to achieve significant empirical speedups, while periodic resets ensure global optimality is maintained. The efficiency of the proposed MWC algorithm enables its use as a core component in more complex analyses focused on cyclic properties. We illustrate this through a detailed application case study: accelerating the computation of the Loop Modulus, a measure of cycle richness used in advanced network characterization. Our algorithm dramatically reduces the runtime of the iterative constraint-finding bottleneck in this computation."}
{"id": "2512.00711", "categories": ["cs.IT", "cs.DC", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00711", "abs": "https://arxiv.org/abs/2512.00711", "authors": ["Loc X. Nguyen", "Ji Su Yoon", "Huy Q. Le", "Yu Qiao", "Avi Deb Raha", "Eui-Nam Huh", "Walid Saad", "Dusit Niyato", "Zhu Han", "Choong Seon Hong"], "title": "Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation", "comment": "13 pages, 7 figures, 6 tables", "summary": "Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves."}
{"id": "2512.00679", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00679", "abs": "https://arxiv.org/abs/2512.00679", "authors": ["Yi Zhang", "Yiwen Zhang", "Yu Wang", "Tong Chen", "Hongzhi Yin"], "title": "ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation", "comment": "Accepted by KDD 2026 (First Cycle)", "summary": "The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models."}
{"id": "2512.01064", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2512.01064", "abs": "https://arxiv.org/abs/2512.01064", "authors": ["Francisco J. Soulignac"], "title": "Beware of the Classical Benchmark Instances for the Traveling Salesman Problem with Time Windows", "comment": null, "summary": "We propose a simple and exact informed search method for the Traveling Salesman Problem with Time Windows and Makespan objective (TSPTW-M) that solves all instances of the classical benchmark with 50 or more customers in less than ten seconds each. Applying this algorithm as an off-the-shelf method, we also solve all but one of these instances for the Duration objective. Our main conclusion is that these instances should no longer be employed for evaluating the TSPTW-M and its Duration variant: they can be ``hacked'' to yield results that seem outstanding at first sight. Additionally, caution is advised when designing hard training sets for machine learning algorithms."}
{"id": "2512.00758", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.00758", "abs": "https://arxiv.org/abs/2512.00758", "authors": ["Yushen Wang", "Weidong Mei", "Xin Wei", "Zhi Chen", "Boyu Ning"], "title": "Movable Antenna Empowered Near-Field Sensing via Antenna Position Optimization", "comment": null, "summary": "Movable antenna (MA) technology exhibits great promise for enhancing the sensing capabilities of future sixth-generation (6G) networks due to its capability to alter antenna array geometry. With the growing prevalence of near-field propagation at ultra-high frequencies, this paper focuses on the application of one-dimensional (1D) and two-dimensional (2D) MA arrays for near-field sensing to jointly estimate the angle and distance information about a target. First, for the 1D MA array scenario, to gain insights into MA-enhanced near-field sensing, we investigate two simplified cases with only angle-of-arrival (AoA) or distance estimation, respectively, assuming that the other information is already known. The worst-case Cramer-Rao bounds (CRBs) on the mean square errors (MSEs) of the AoA estimation and the distance estimation are derived in these two cases. Then, we jointly optimize the positions of the MAs within the 1D array to minimize these CRBs and derive their closed-form solutions, which yield an identical array geometry to MA-enhanced far-field sensing. For the more challenging joint AoA and distance estimation, since the associated worst-case CRB is a highly complex and non-convex function with respect to the MA positions, a discrete sampling-based approach is proposed to sequentially update the MA positions and obtain an efficient suboptimal solution. Furthermore, we investigate the worst-case CRB minimization problems for a 2D MA array under various conditions and extend our proposed algorithms to solve them efficiently. Numerical results demonstrate that the proposed MA-enhanced near-field sensing scheme dramatically outperforms conventional fixed-position antennas (FPAs). Moreover, the joint angle and distance estimation results in a different array geometry from that in the individual estimation of angle/distance or far-field sensing."}
{"id": "2512.00772", "categories": ["cs.IR", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.00772", "abs": "https://arxiv.org/abs/2512.00772", "authors": ["Hyunseok Ryu", "Wonjune Shin", "Hyun Park"], "title": "SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG", "comment": "10 pages, 4 figures, 1 table, 1 algorithm, 3 prompts", "summary": "Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language\n  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to\n  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing\n  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.\n  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of\n  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a\n  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries\n  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process\n  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual\n  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual\n  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,\n  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and\n  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,\n  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries."}
{"id": "2512.01121", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.01121", "abs": "https://arxiv.org/abs/2512.01121", "authors": ["Christine Awofeso", "Patrick Greaves", "Oded Lachish", "Felix Reidl"], "title": "A practical algorithm for 3-admissibility", "comment": null, "summary": "The $3$-admissibility of a graph is a promising measure to identify real-world networks that have an algorithmically favourable structure.\n  We design an algorithm that decides whether the $3$-admissibility of an input graph~$G$ is at most~$p$ in time~\\runtime and space~\\memory, where $m$ is the number of edges in $G$ and $n$ the number of vertices. To the best of our knowledge, this is the first explicit algorithm to compute the $3$-admissibility.\n  The linear dependence on the input size in both time and space complexity, coupled with an `optimistic' design philosophy for the algorithm itself, makes this algorithm practicable, as we demonstrate with an experimental evaluation on a corpus of \\corpussize real-world networks.\n  Our experimental results show, surprisingly, that the $3$-admissibility of most real-world networks is not much larger than the $2$-admissibility, despite the fact that the former has better algorithmic properties than the latter."}
{"id": "2512.00770", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.00770", "abs": "https://arxiv.org/abs/2512.00770", "authors": ["Jiasi Zhou", "Chintha Tellambura", "Geoffrey Ye Li"], "title": "Rate-Splitting Multiple Access for Secure Near-Field Integrated Sensing and Communication", "comment": null, "summary": "Near-field integrated sensing and communication (ISAC) leverages distance-dependent channel variations for joint distance and angle estimation. However, full-digital architectures have prohibitive hardware costs, making hybrid analog-digital (HAD) designs the primary alternative. Nevertheless, such architectures compromise beamfocusing precision and lead to energy leakage, which exacerbates inter-user interference and increases eavesdropping risks. To address these challenges, this paper proposes a rate-splitting multiple access (RSMA)-enhanced secure transmit scheme for near-field ISAC. For the first time, it exploits the common stream in RSMA to concurrently (i) flexibly manage interference, (ii) act as artificial noise to suppress eavesdropping, and (iii) serve as sensing sequences. The objective is to maximize the minimum secrecy rate while satisfying the angle and distance Cramer-Rao Bound (CRB) constraints. This results in a hard, non-convex optimization problem, and we employ block coordinate descent to decompose it into three sub-problems with lower computational complexity. In the first stage of optimizing fully digital beamfocusers, we develop an iterative solution using weighted minimum mean-squared error (WMMSE), quadratic transform, and Taylor expansion methods, thus avoiding conventional semidefinite relaxation. In the second and third stages, the analog and digital beamfocusers are optimized in closed form. Simulation results show that the proposed scheme (1) achieves near full-digital beamfocusing performance with a 16-fold reduction in RF chains, (2) provides superior secrecy performance compared to conventional beamfocusing-only and far-field security schemes, and (3) enables high-accuracy sensing with negligible loss in secrecy performance."}
{"id": "2512.00968", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00968", "abs": "https://arxiv.org/abs/2512.00968", "authors": ["Ziyang Zeng", "Heming Jing", "Jindong Chen", "Xiangli Li", "Hongyu Liu", "Yixuan He", "Zhengyu Li", "Yige Sun", "Zheyong Xie", "Yuqing Yang", "Shaosheng Cao", "Jun Fan", "Yi Wu", "Yao Hu"], "title": "Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search", "comment": "Accepted by KDD 2026 ADS Track", "summary": "Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach."}
{"id": "2512.01240", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.01240", "abs": "https://arxiv.org/abs/2512.01240", "authors": ["Shaddin Dughmi", "Yusuf Hakan Kalayci", "Xinyu Liu"], "title": "Near-Optimal Sparsifiers for Stochastic Knapsack and Assignment Problems", "comment": "51 pages, 8 figures. Accepted to ITCS 2026", "summary": "When uncertainty meets costly information gathering, a fundamental question emerges: which data points should we probe to unlock near-optimal solutions? Sparsification of stochastic packing problems addresses this trade-off. The existing notions of sparsification measure the level of sparsity, called degree, as the ratio of queried items to the optimal solution size. While effective for matching and matroid-type problems with uniform structures, this cardinality-based approach fails for knapsack-type constraints where feasible sets exhibit dramatic structural variation. We introduce a polyhedral sparsification framework that measures the degree as the smallest scalar needed to embed the query set within a scaled feasibility polytope, naturally capturing redundancy without relying on cardinality.\n  Our main contribution establishes that knapsack, multiple knapsack, and generalized assignment problems admit (1 - epsilon)-approximate sparsifiers with degree polynomial in 1/p and 1/epsilon -- where p denotes the independent activation probability of each element -- remarkably independent of problem dimensions. The key insight involves grouping items with similar weights and deploying a charging argument: when our query set misses an optimal item, we either substitute it with a queried item from the same group or leverage that group's excess contribution to compensate for the loss. This reveals an intriguing complexity-theoretic separation -- while the multiple knapsack problem lacks an FPTAS and generalized assignment is APX-hard, their sparsification counterparts admit efficient (1 - epsilon)-approximation algorithms that identify polynomial-degree query sets. Finally, we raise an open question: can such sparsification extend to general integer linear programs with degree independent of problem dimensions?"}
{"id": "2512.00774", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.00774", "abs": "https://arxiv.org/abs/2512.00774", "authors": ["Jiasi Zhou", "Huiyun Xia", "Chuan Wu", "Chintha Tellambura"], "title": "Hybrid Beamfocusing Design for RSMA-Enhanced Near-Field Secure Communications", "comment": "10 pages and 6 figures", "summary": "Near-field spherical wavefronts enable spotlight-like beam focusing to mitigate unintended energy leakage, creating new opportunities for physical-layer security (PLS). However, under hybrid analog-digital (HAD) antenna architectures, beamfocusing alone may not provide foolproof privacy protection due to reduced focusing precision. To address this issue, this paper proposes a rate-splitting multiple access (RSMA)-enhanced secure transmit scheme for near-field communications with fully-connected or sub-connected HAD architectures. In the proposed scheme, the common stream is designed for dual purposes, delivering the desired message for legitimate users while acting as artificial noise to disrupt eavesdropping. The primary objective is to maximize the minimum secrecy rate by jointly optimizing the analog beamfocuser, digital beamfocuser, and common secrecy rate allocation. To solve the formulated non-convex problem, we develop a penalty-based alternating optimization algorithm. Specifically, the variables are partitioned into three blocks, where one block is solved via a surrogate optimization method, while the others are updated in closed form. Simulation results reveal that our transmit scheme: (1) approaches fully digital beamfocusing with substantially fewer radio frequency chains, (2) outperforms conventional beamfocusing-only and far-field security schemes, and (3) preserves secrecy without significantly compromising communication rates."}
{"id": "2512.01171", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01171", "abs": "https://arxiv.org/abs/2512.01171", "authors": ["Tao Xue", "Yanwu Yang", "Panyu Zhai"], "title": "Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions", "comment": "99 pages, 15 figures, 7 tables", "summary": "Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area."}
{"id": "2512.01587", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.01587", "abs": "https://arxiv.org/abs/2512.01587", "authors": ["Édouard Bonnet", "Tuukka Korhonen", "Hung Le", "Jason Li", "Tomáš Masařík"], "title": "Separator Theorem for Minor-Free Graphs in Linear Time", "comment": "21 pages", "summary": "The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that any minor-free graph admits a balanced separator of size $O(\\sqrt{n})$ that can be found in $O(n^{3/2})$ time. The superlinear running time in their separator theorem is a key bottleneck for generalizing algorithmic results from planar to minor-free graphs. Despite extensive research for more than two decades, finding a balanced separator of size $O(\\sqrt{n})$ in (linear) $O(n)$ time for minor-free graphs remains a major open problem. Known algorithms either give a separator of size much larger than $O(\\sqrt{n})$ or have superlinear running time, or both.\n  In this paper, we answer the open problem affirmatively. Our algorithm is very simple: it runs a vertex-weighted variant of breadth-first search (BFS) a constant number of times on the input graph. Our key technical contribution is a weighting scheme on the vertices to guide the search for a balanced separator, offering a new connection between the size of a balanced separator and the existence of a clique-minor model. We believe that our weighting scheme may be of independent interest."}
{"id": "2512.00985", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.00985", "abs": "https://arxiv.org/abs/2512.00985", "authors": ["Adem Utku Atasayar", "Aimin Li", "Çağrı Arı", "Elif Uysal"], "title": "Age Optimal Sampling and Routing under Intermittent Links and Energy Constraints", "comment": null, "summary": "Links in practical systems, such as satellite-terrestrial integrated networks, exhibit distinct delay distributions, intermittent availability, and heterogeneous energy costs. These characteristics pose significant challenges to maintaining timely and energy-efficient status updates. While link availability restricts feasible transmission routes, routing decisions determine the actual delay and energy expenditure. This paper tackles these challenges by jointly optimizing sampling and routing decisions to minimize monotonic, nonlinear Age of Information (AoI). The proposed formulation incorporates key system features, including multiple routes with correlated random delays, stochastic link availability, and route-dependent energy consumption. We model the problem as an infinite-horizon constrained semi-Markov decision process (CSMDP) with a hybrid state-action space and develop an efficient nested algorithm, termed Bisec-ReaVI, to solve this problem. We reveal a well-defined jointly optimal policy structure: (i) the optimal routing policy is a monotonic handover policy that adapts to the availability of routes and their mean delays; and (ii) the optimal sampling policy is a piecewise linear waiting policy, with at most \"N choose 2 + N\" breakpoints given N routes. Numerical experiments in a satellite-terrestrial integrated routing scenario demonstrate that the proposed scheme efficiently balances energy usage and information freshness, and reveal a counter-intuitive insight: even routes with higher average delay, higher delay variance, or lower availability can still play a critical role in minimizing monotonic functions of AoI."}
{"id": "2512.01179", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01179", "abs": "https://arxiv.org/abs/2512.01179", "authors": ["Shan Gao", "Yanwu Yang"], "title": "Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives", "comment": "64 pages, 8 figures, 11 tables", "summary": "This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR."}
{"id": "2512.01802", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.01802", "abs": "https://arxiv.org/abs/2512.01802", "authors": ["Xin Wang", "Xi Chen"], "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford", "comment": "with editor,22 pages", "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."}
{"id": "2512.01454", "categories": ["cs.IT", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.01454", "abs": "https://arxiv.org/abs/2512.01454", "authors": ["Jiping Luo", "Bowen Li", "Nikolaos Pappas"], "title": "Value of Communication in Goal-Oriented Semantic Communications: A Pareto Analysis", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Emerging cyber-physical systems increasingly operate under stringent communication constraints that preclude the reliable transmission of their extensive machine-type data streams. Since raw measurements often contain correlated or redundant components, effective operation depends not on transmitting all available data but on selecting the information that contributes to achieving the objectives of the system. Beyond accuracy, goal-oriented semantic communication assesses the \\emph{value of information} and aims to generate and transmit only what is relevant and at the right time. Motivated by this perspective, this work studies the \\emph{value of communication} through the canonical setting of remote estimation of Markov sources, where a value-of-information measure quantifies the relevance of information. We investigate how optimal estimation performance varies with the available communication budget and determine the marginal performance gain attributable to additional communication. Our approach is based on a \\emph{Pareto analysis} that characterizes the complete set of policies that achieve optimal trade-offs between estimation performance and communication cost. The value of communication is defined as the absolute slope of the resulting Pareto frontier. Although computing this frontier is non-trivial, we demonstrate that in our setting it admits a notably tractable structure: it is strictly decreasing, convex, and piecewise linear, and its slope is governed by a finite collection of constants. Moreover, each Pareto-optimal operating point is realizable as a convex combination of two stationary deterministic policies, enabling practical implementation. Leveraging these structural insights, we introduce SPLIT, an efficient and provably optimal algorithm for constructing the complete Pareto frontier."}
{"id": "2512.01372", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01372", "abs": "https://arxiv.org/abs/2512.01372", "authors": ["Wei Yang", "Rui Zhong", "Yiqun Chen", "Chi Lu", "Peng Jiang"], "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation", "comment": null, "summary": "Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance."}
{"id": "2512.01900", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.01900", "abs": "https://arxiv.org/abs/2512.01900", "authors": ["Narek Bojikian", "Stefan Kratsch"], "title": "Tight Bounds for Feedback Vertex Set Parameterized by Clique-width", "comment": null, "summary": "We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the decision problem in the same time. We complement these by a matching lower bound under the Strong Exponential-Time Hypothesis (SETH). This closes an open question that appeared multiple times in the literature [ESA 23, ICALP 24, IPEC 25].\n  We also present an algorithm that, given a graph $G$ and a tree decomposition of width $k$ of $G$, in time $O(3^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. This matches the known SETH-tight bound for the decision version, which was obtained using the celebrated cut-and-count technique [FOCS 11, TALG 22]. Unlike other applications of cut-and-count, which use the isolation lemma to reduce a decision problem to counting solutions modulo $2$, this bound was obtained via counting other objects, leaving the complexity of counting solutions modulo $2$ open.\n  Finally, we present a one-sided error Monte-Carlo algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(18^kn^c)$ decides the existence of a connected feedback vertex set of size $b$ in $G$. We provide a matching lower bound under SETH."}
{"id": "2512.01505", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.01505", "abs": "https://arxiv.org/abs/2512.01505", "authors": ["Geoffrey Deperle", "Philippe Jacquet"], "title": "Generating Random Hyperfractal Cities", "comment": null, "summary": "This paper focuses on the challenge of interactively modeling street networks. In this work, we extend the simple fractal model, which is particularly useful for describing small cities or individual districts, by constructing random cities based on a tiling structure over which hyperfractals are distributed. This approach enables the connection of multiple hyperfractal districts, providing a more comprehensive urban representation. Furthermore, we demonstrate how this decomposition can be used to segment a city into distinct districts through fractal analysis. Finally, we present tools for the numerical generation of random cities following this model."}
{"id": "2512.00378", "categories": ["cs.IT", "cs.DS", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00378", "abs": "https://arxiv.org/abs/2512.00378", "authors": ["Nikit Phadke"], "title": "The Information Theory of Similarity", "comment": null, "summary": "We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(Δ^{-2} \\log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem."}
{"id": "2512.02003", "categories": ["cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.02003", "abs": "https://arxiv.org/abs/2512.02003", "authors": ["Yang P. Liu", "Richard Peng", "Colin Tang", "Albert Weng", "Junzhao Yang"], "title": "Adaptive Matrix Sparsification and Applications to Empirical Risk Minimization", "comment": null, "summary": "Consider the empirical risk minimization (ERM) problem, which is stated as follows. Let $K_1, \\dots, K_m$ be compact convex sets with $K_i \\subseteq \\mathbb{R}^{n_i}$ for $i \\in [m]$, $n = \\sum_{i=1}^m n_i$, and $n_i\\le C_K$ for some absolute constant $C_K$. Also, consider a matrix $A \\in \\mathbb{R}^{n \\times d}$ and vectors $b \\in \\mathbb{R}^d$ and $c \\in \\mathbb{R}^n$. Then the ERM problem asks to find \\[ \\min_{\\substack{x \\in K_1 \\times \\dots \\times K_m\\\\ A^\\top x = b}}\n  c^\\top x. \\] We give an algorithm to solve this to high accuracy in time $\\widetilde{O}(nd + d^6\\sqrt{n}) \\le \\widetilde{O} (nd + d^{11})$, which is nearly-linear time in the input size when $A$ is dense and $n \\ge d^{10}$.\n  Our result is achieved by implementing an $\\widetilde{O}(\\sqrt{n})$-iteration interior point method (IPM) efficiently using dynamic data structures. In this direction, our key technical advance is a new algorithm for maintaining leverage score overestimates of matrices undergoing row updates. Formally, given a matrix $A \\in \\mathbb{R}^{n \\times d}$ undergoing $T$ batches of row updates of total size $n$ we give an algorithm which can maintain leverage score overestimates of the rows of $A$ summing to $\\widetilde{O}(d)$ in total time $\\widetilde{O}(nd + Td^6)$. This data structure is used to sample a spectral sparsifier within a robust IPM framework to establish the main result."}
{"id": "2512.01861", "categories": ["cs.IT", "cond-mat.dis-nn", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01861", "abs": "https://arxiv.org/abs/2512.01861", "authors": ["Yingying Xu", "Masayuki Ohzeki", "Yoshiyuki Kabashima"], "title": "Storage capacity of perceptron with variable selection", "comment": "21 pages, 3 figures", "summary": "A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $α$ and the variable selection ratio $ρ$ for which a simple perceptron can perfectly classify $P = αN$ random patterns by optimally selecting $M = ρN$ variables out of $N$ variables. While the Cover--Gardner theory establishes that a random subset of $ρN$ dimensions can separate $αN$ random patterns if and only if $α< 2ρ$, we demonstrate that optimal variable selection can surpass this bound by developing a method, based on the replica method from statistical mechanics, for enumerating the combinations of variables that enable perfect pattern classification. This not only provides a quantitative criterion for distinguishing true structure in the data from spurious regularities, but also yields the storage capacity of associative memory models with sparse asymmetric couplings."}
{"id": "2512.00711", "categories": ["cs.IT", "cs.DC", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00711", "abs": "https://arxiv.org/abs/2512.00711", "authors": ["Loc X. Nguyen", "Ji Su Yoon", "Huy Q. Le", "Yu Qiao", "Avi Deb Raha", "Eui-Nam Huh", "Walid Saad", "Dusit Niyato", "Zhu Han", "Choong Seon Hong"], "title": "Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation", "comment": "13 pages, 7 figures, 6 tables", "summary": "Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves."}
{"id": "2512.00378", "categories": ["cs.IT", "cs.DS", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00378", "abs": "https://arxiv.org/abs/2512.00378", "authors": ["Nikit Phadke"], "title": "The Information Theory of Similarity", "comment": null, "summary": "We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(Δ^{-2} \\log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem."}
{"id": "2512.01985", "categories": ["cs.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.01985", "abs": "https://arxiv.org/abs/2512.01985", "authors": ["Daniel T. Larsson", "Dipankar Maity", "Panagiotis Tsiotras"], "title": "A Dual Approach for Hierarchical Information-Theoretic Tree Abstractions", "comment": null, "summary": "In this paper, we consider establishing a formal connection between two distinct tree-abstraction problems inspired by the information-bottleneck (IB) method. Specifically, we consider the hard- and soft-constrained formulations that have recently appeared in the literature to determine the conditions for which the two approaches are equivalent. Our analysis leverages concepts from Lagrangian relaxation and duality theory to relate the dual function of the hard-constrained problem to the Q-function employed in Q-tree search and shows the connection between tree phase transitions and solutions to the dual problem obtained by exploiting the problem structure. An algorithm is proposed that employs knowledge of the tree phase transitions to find a setting of the dual variable that solves the dual problem. Furthermore, we present an alternative approach to select the dual variable that leverages the integer programming formulation of the hard-constrained problem and the strong duality of linear programming. To obtain a linear program, we establish that a relaxation of the integer programming formulation of the hard-constrained tree-search problem has the integrality property by showing that the program constraint matrix is totally unimodular. Empirical results that corroborate the theoretical developments are presented and discussed throughout."}
