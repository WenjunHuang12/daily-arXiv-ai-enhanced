<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 55]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [The NIAID Discovery Portal: A Unified Search Engine for Infectious and Immune-Mediated Disease Datasets](https://arxiv.org/abs/2509.13524)
*Ginger Tsueng,Emily Bullen,Candice Czech,Dylan Welzel,Leandro Collares,Jason Lin,Everaldo Rodolpho,Zubair Qazi,Nichollette Acosta,Lisa M. Mayer,Sudha Venkatachari,Zorana Mitrović Vučičević,Poromendro N. Burman,Deepti Jain,Jack DiGiovanna,Maria Giovanni,Asiyah Lin,Wilbert Van Panhuis,Laura D. Hughes,Andrew I. Su,Chunlei Wu*

Main category: cs.DB

TL;DR: NIAID数据生态系统发现门户是一个统一的搜索平台，整合了400多万个传染病和免疫介导疾病研究相关数据集，提供用户友好的搜索过滤器和API接口，降低数据访问门槛。


<details>
  <summary>Details</summary>
Motivation: 解决有价值生物医学数据集难以发现和访问的问题，为研究人员提供集中化的搜索界面，促进数据的重复利用和科学进步。

Method: 整合领域特定和通用存储库的元数据，通过标准化关键元数据字段和协调异构格式，提供过滤器、预构建查询和数据集集合，支持程序化API访问。

Result: 建立了包含400多万个数据集的统一搜索门户，支持流行病学、临床和多组学等多种资源类型的发现，提高了数据的可发现性、可访问性和可重用性。

Conclusion: 该门户作为传染病和免疫介导疾病研究的入口点，通过降低数据访问障碍，支持假设生成、比较分析和公共数据的二次使用，最大化公共研究投资的影响力。

Abstract: The NIAID Data Ecosystem Discovery Portal (https://data.niaid.nih.gov)
provides a unified search interface for over 4 million datasets relevant to
infectious and immune-mediated disease (IID) research. Integrating metadata
from domain-specific and generalist repositories, the Portal enables
researchers to identify and access datasets using user-friendly filters or
advanced queries, without requiring technical expertise. The Portal supports
discovery of a wide range of resources, including epidemiological, clinical,
and multi-omic datasets, and is designed to accommodate exploratory browsing
and precise searches. The Portal provides filters, prebuilt queries, and
dataset collections to simplify the discovery process for users. The Portal
additionally provides documentation and an API for programmatic access to
harmonized metadata. By easing access barriers to important biomedical
datasets, the NIAID Data Ecosystem Discovery Portal serves as an entry point
for researchers working to understand, diagnose, or treat IID.
  Valuable datasets are often overlooked because they are difficult to locate.
The NIAID Data Ecosystem Discovery Portal fills this gap by providing a
centralized, searchable interface that empowers users with varying levels of
technical expertise to find and reuse data. By standardizing key metadata
fields and harmonizing heterogeneous formats, the Portal improves data
findability, accessibility, and reusability. This resource supports hypothesis
generation, comparative analysis, and secondary use of public data by the IID
research community, including those funded by NIAID. The Portal supports data
sharing by standardizing metadata and linking to source repositories, and
maximizes the impact of public investment in research data by supporting
scientific advancement via secondary use.

</details>


### [2] [Tractability Frontiers of the Shapley Value for Aggregate Conjunctive Queries](https://arxiv.org/abs/2509.13565)
*Christoph Standke,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 本文研究了在聚合连接查询中计算元组Shapley值的复杂性，针对不同聚合函数（min、max、count-distinct、average、quantile）确定了可计算Shapley值的层次化查询类，并证明这些类对于每个聚合函数都是最大的。


<details>
  <summary>Details</summary>
Motivation: Shapley值作为衡量元组对数据库查询结果贡献的重要指标，先前研究已证明对于非层次化查询计算是#P难的。但对于常见聚合函数如min、max、count-distinct等的复杂性仍为开放问题，需要系统研究。

Method: 通过识别每个聚合函数对应的层次化查询类（如all-hierarchical、q-hierarchical），分析在这些类中Shapley值的可计算性，并证明这些类的最大性——即超出这些类的查询存在使计算#P难的局部值函数。

Result: 发现不同聚合函数对应不同的层次化查询类：max、min、count-distinct对应all-hierarchical类，average和quantile对应更严格的q-hierarchical类。每个类都是最大的，确保了在这些类内Shapley值可高效计算。

Conclusion: 研究为不同聚合函数下的Shapley值计算提供了完整的复杂性特征，揭示了聚合函数与查询结构层次性之间的对应关系，解决了先前提出的开放问题，并为数据库查询贡献度分析提供了理论基础。

Abstract: In recent years, the Shapley value has emerged as a general game-theoretic
measure for assessing the contribution of a tuple to the result of a database
query. We study the complexity of calculating the Shapley value of a tuple for
an aggregate conjunctive query, which applies an aggregation function to the
result of a conjunctive query (CQ) based on a value function that assigns a
number to each query answer. Prior work by Livshits, Bertossi, Kimelfeld, and
Sebag (2020) established that this task is #P-hard for every nontrivial
aggregation function when the query is non-hierarchical with respect to its
existential variables, assuming the absence of self-joins. They further showed
that this condition precisely characterizes the class of intractable CQs when
the aggregate function is sum or count. In addition, they posed as open
problems the complexity of other common aggregate functions such as min, max,
count-distinct, average, and quantile (including median). Towards the
resolution of these problems, we identify for each aggregate function a class
of hierarchical CQs where the Shapley value is tractable with every value
function, as long as it is local (i.e., determined by the tuples of one
relation). We further show that each such class is maximal: for every CQ
outside of this class, there is a local (easy-to-compute) value function that
makes the Shapley value #P-hard. Interestingly, our results reveal that each
aggregate function corresponds to a different generalization of the class of
hierarchical CQs from Boolean to non-Boolean queries. In particular, max, min,
and count-distinct match the class of CQs that are all-hierarchical (i.e.,
hierarchical with respect to all variables), and average and quantile match the
narrower class of q-hierarchical CQs introduced by Berkholz, Keppeler, and
Schweikardt (2017) in the context of the fine-grained complexity of query
answering.

</details>


### [3] [XASDB -- Design and Implementation of an Open-Access Spectral Database](https://arxiv.org/abs/2509.13566)
*Denis Spasyuk*

Main category: cs.DB

TL;DR: XASDB是一个基于网络的X射线吸收光谱数据库平台，包含1000多个参考光谱，支持浏览器端数据处理和可视化，促进FAIR数据原则和协作研究。


<details>
  <summary>Details</summary>
Motivation: 随着同步辐射设施产生的XAS数据量和复杂性不断增加，需要强大的数据管理、共享和分析基础设施来处理多样化的数据格式并促进科学研究。

Method: 采用Node.js/MongoDB架构开发网络平台，集成XASproc JavaScript库进行浏览器端数据处理，包含XASVue光谱查看器实现跨设备可视化分析。

Result: 建立了包含40种元素、324种化合物的1000多个参考光谱数据库，实现了浏览器端的标准化数据处理和分析功能，支持多种应用场景。

Conclusion: XASDB展示了以网络为中心的方法在XAS数据分析中的潜力，为材料科学、环境研究、化学和生物学领域的进展提供了宝贵资源。

Abstract: The increasing volume and complexity of X-ray absorption spectroscopy (XAS)
data generated at synchrotron facilities worldwide require robust
infrastructure for data management, sharing, and analysis. This paper
introduces the XAS Database (XASDB), a comprehensive web-based platform
developed and hosted by the Canadian Light Source (CLS). The database houses
more than 1000 reference spectra spanning 40 elements and 324 chemical
compounds. The platform employs a Node.js/MongoDB architecture designed to
handle diverse data formats from multiple beamlines and synchrotron facilities.
A key innovation is the XASproc JavaScript library, which enables browser-based
XAS data processing including normalization, background sub- traction, extended
X-ray absorption fine structure (EXAFS) extraction, and preliminary analysis
traditionally limited to desktop applications. The integrated XASVue spectral
viewer provides installation-free data visualization and analysis with broad
accessibility across devices and operating systems. By offering standardized
data output, comprehensive metadata, and integrated analytical ca- pabilities,
XASDB facilitates collaborative research and promotes FAIR (Findable,
Accessible, In- teroperable, and Reusable) data principles. The platform serves
as a valuable resource for linear combination fitting (LCF) analysis, machine
learning applications, and educational purposes. This initiative demonstrates
the potential for web-centric approaches in XAS data analysis, accelerating
advances in materials science, environmental research, chemistry, and biology.

</details>


### [4] [Algorithms for Optimizing Acyclic Queries](https://arxiv.org/abs/2509.14144)
*Zheng Luo,Wim Van den Broeck,Guy Van den Broeck,Yisu Remy Wang*

Main category: cs.DB

TL;DR: 本文提出了三种构建无环查询连接树的方法：枚举α-无环查询的所有连接树、构建Berge-无环查询的最浅连接树、将γ-无环查询的线性计划转换为连接树。


<details>
  <summary>Details</summary>
Motivation: 传统查询优化主要关注二元连接算法（如哈希连接和排序合并连接），但近年来理论最优算法（如Yannakakis算法）受到关注，这些算法依赖连接树而非二元连接的操作树，需要新的优化技术。

Method: 1. 开发了枚举α-无环查询所有连接树的算法，具有摊销常数延迟；2. 使用Tarjan和Yannakakis的最大基数搜索算法构建Berge-无环查询的最浅连接树；3. 提出将γ-无环查询的线性计划转换为连接树的简单算法。

Result: 为无环连接查询提供了基于成本的优化器基础；实现了大型连接查询的并行执行；允许重用为二元连接开发的优化基础设施。

Conclusion: 这些方法为不同类别的无环查询提供了有效的连接树构建技术，填补了理论最优算法在实际优化应用中的空白，为查询优化器设计提供了新的思路。

Abstract: Most research on query optimization has centered on binary join algorithms
like hash join and sort-merge join. However, recent years have seen growing
interest in theoretically optimal algorithms, notably Yannakakis' algorithm.
These algorithms rely on join trees, which differ from the operator trees for
binary joins and require new optimization techniques. We propose three
approaches to constructing join trees for acyclic queries. First, we give an
algorithm to enumerate all join trees of an alpha-acyclic query by edits with
amortized constant delay, which forms the basis of a cost-based optimizer for
acyclic joins. Second, we show that the Maximum Cardinality Search algorithm by
Tarjan and Yannakakis constructs a unique shallowest join tree, rooted at any
relation, for a Berge-acyclic query; this tree enables parallel execution of
large join queries. Finally, we prove that any connected left-deep linear plan
for a gamma-acyclic query can be converted into a join tree by a simple
algorithm, allowing reuse of optimization infrastructure developed for binary
joins.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [5] [MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval](https://arxiv.org/abs/2509.13562)
*Yifan Liu,Qianfeng Wen,Mark Zhao,Jiazhou Liang,Scott Sanner*

Main category: cs.IR

TL;DR: 提出了MA-DPR方法，通过构建最近邻图来建模段落嵌入的非线性流形结构，在OOD检索中比欧氏距离和余弦距离提升26%


<details>
  <summary>Details</summary>
Motivation: 传统DPR方法依赖欧氏距离或余弦距离，但实验发现嵌入向量往往位于低维非线性流形上，特别是在OOD场景下，传统距离度量无法有效捕捉语义相似性

Method: 使用最近邻图建模段落的内在流形结构，基于图中最短路径来度量查询-段落距离

Result: 在OOD段落检索中比欧氏距离和余弦距离提升达26%，在分布内性能相当，查询推理时间仅轻微增加

Conclusion: 流形感知距离允许DPR利用相关邻域段落的上下文信息，即使在没有直接语义重叠的情况下也有效，可广泛应用于各种密集嵌入和检索任务

Abstract: Dense Passage Retrieval (DPR) typically relies on Euclidean or cosine
distance to measure query-passage relevance in embedding space, which is
effective when embeddings lie on a linear manifold. However, our experiments
across DPR benchmarks suggest that embeddings often lie on lower-dimensional,
non-linear manifolds, especially in out-of-distribution (OOD) settings, where
cosine and Euclidean distance fail to capture semantic similarity. To address
this limitation, we propose a manifold-aware distance metric for DPR (MA-DPR)
that models the intrinsic manifold structure of passages using a nearest
neighbor graph and measures query-passage distance based on their shortest path
in this graph. We show that MA-DPR outperforms Euclidean and cosine distances
by up to 26% on OOD passage retrieval with comparable in-distribution
performance across various embedding models while incurring a minimal increase
in query inference time. Empirical evidence suggests that manifold-aware
distance allows DPR to leverage context from related neighboring passages,
making it effective even in the absence of direct semantic overlap. MADPR can
be applied to a wide range of dense embedding and retrieval tasks, offering
potential benefits across a wide spectrum of domains.

</details>


### [6] [Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation](https://arxiv.org/abs/2509.13603)
*Yongye Su,Zeya Zhang,Jane Kou,Cheng Ju,Shubhojeet Sarkar,Yamin Wang,Ji Liu,Shengbo Guo*

Main category: cs.IR

TL;DR: Facebook提出混合关键词检索和嵌入检索的社交网络搜索框架，通过LLM评估显示能显著提升搜索相关性和用户参与度


<details>
  <summary>Details</summary>
Motivation: 传统社交网络搜索主要依赖关键词匹配，难以理解语义上下文，需要改进搜索相关性和多样性以更好地服务用户在社交环境中的信息检索需求

Method: 开发混合检索系统，结合传统关键词检索和嵌入检索(EBR)，引入基于大语言模型(LLM)的离线相关性评估框架进行质量基准测试

Result: 混合检索系统显著提升了用户参与度和搜索质量，在线指标和LLM评估均验证了系统效果

Conclusion: 该工作为大规模社交平台部署和评估先进检索系统提供了实用见解，证明了混合检索方法在提升社交搜索效果方面的价值

Abstract: Beyond general web-scale search, social network search uniquely enables users
to retrieve information and discover potential connections within their social
context. We introduce a framework of modernized Facebook Group Scoped Search by
blending traditional keyword-based retrieval with embedding-based retrieval
(EBR) to improve the search relevance and diversity of search results. Our
system integrates semantic retrieval into the existing keyword search pipeline,
enabling users to discover more contextually relevant group posts. To
rigorously assess the impact of this blended approach, we introduce a novel
evaluation framework that leverages large language models (LLMs) to perform
offline relevance assessments, providing scalable and consistent quality
benchmarks. Our results demonstrate that the blended retrieval system
significantly enhances user engagement and search quality, as validated by both
online metrics and LLM-based evaluation. This work offers practical insights
for deploying and evaluating advanced retrieval systems in large-scale,
real-world social platforms.

</details>


### [7] [Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval](https://arxiv.org/abs/2509.13626)
*Amanda Chan,James Jiayu Liu,He Kai,Onno P. Kampman*

Main category: cs.IR

TL;DR: 提出基于AI的gap-informed框架，通过分析用户自然语言数据识别知识库中的缺失主题，实现定向内容扩充，显著提升心理健康信息检索系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康知识库建设资源密集且常与用户需求脱节，导致检索系统在面对未覆盖主题或非正式表达时性能不佳，需要更有效的知识库扩充方法。

Method: 开发gap-informed框架，通过叠加论坛帖子等自然用户数据识别代表性不足的主题，进行定向内容扩充，并与随机扩充方法进行对比评估。

Result: 定向扩充仅需42%-318%的内容增长即可达到参考语料库95%的性能，而随机扩充需要232%-763%的增长，效率显著更高。

Conclusion: 战略性定向知识库增长能减少内容创建需求，维持高质量检索性能，为构建可信健康信息库和生成式AI应用提供可扩展方案。

Abstract: Access to reliable mental health information is vital for early help-seeking,
yet expanding knowledge bases is resource-intensive and often misaligned with
user needs. This results in poor performance of retrieval systems when
presented concerns are not covered or expressed in informal or contextualized
language. We present an AI-based gap-informed framework for corpus augmentation
that authentically identifies underrepresented topics (gaps) by overlaying
naturalistic user data such as forum posts in order to prioritize expansions
based on coverage and usefulness. In a case study, we compare Directed
(gap-informed augmentations) with Non-Directed augmentation (random additions),
evaluating the relevance and usefulness of retrieved information across four
retrieval-augmented generation (RAG) pipelines. Directed augmentation achieved
near-optimal performance with modest expansions--requiring only a 42% increase
for Query Transformation, 74% for Reranking and Hierarchical, and 318% for
Baseline--to reach ~95% of the performance of an exhaustive reference corpus.
In contrast, Non-Directed augmentation required substantially larger and thus
practically infeasible expansions to achieve comparable performance (232%,
318%, 403%, and 763%, respectively). These results show that strategically
targeted corpus growth can reduce content creation demands while sustaining
high retrieval and provision quality, offering a scalable approach for building
trusted health information repositories and supporting generative AI
applications in high-stakes domains.

</details>


### [8] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRUT是一个生成式推荐模型，通过时间感知提示和趋势感知推理来捕捉用户偏好随时间的变化，在四个基准数据集上Recall@5和NDCG@5指标提升高达15.4%和14.3%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐研究主要关注物品序列顺序，但忽略了物品间的时间动态性，而时间动态可以反映用户偏好的演变过程。

Method: 提出GRUT模型，包含时间感知提示（用户级时间上下文和物品级转移上下文）和训练免费的趋势感知推理方法，通过时间信号捕捉隐藏的用户偏好。

Result: 在四个基准数据集上的实验表明，GRUT在Recall@5和NDCG@5指标上分别实现了最高15.4%和14.3%的性能提升，优于最先进模型。

Conclusion: GRUT通过有效利用时间动态信息，显著提升了生成式推荐的性能，证明了时间感知在推荐系统中的重要性。

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [9] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: GEM-Bench是首个针对生成式引擎营销(GEM)中广告注入响应生成的综合基准，包含三个数据集、多维度评估指标和基线方法，发现现有方法在用户满意度和参与度之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基准不专门针对GEM中的广告注入响应生成，限制了相关研究发展，需要专门的评估框架来推动这一新兴领域的研究。

Method: 提出了GEM-Bench基准，包含三个精心策划的数据集（覆盖聊天机器人和搜索场景）、捕捉用户满意度和参与度的多维度指标本体，以及在可扩展多智能体框架中实现的多个基线解决方案。

Result: 初步结果显示，基于提示的简单方法能获得合理的参与度（如点击率），但往往降低用户满意度；而基于预生成无广告响应插入广告的方法能缓解此问题，但引入额外开销。

Conclusion: 需要未来研究设计更有效和高效的GEM广告注入响应生成解决方案，以平衡用户满意度和商业目标。

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [GTA -- An ATSP Method: Shifting the Bottleneck from Algorithm to RAM](https://arxiv.org/abs/2509.13327)
*Wissam Nakhle*

Main category: cs.DS

TL;DR: 提出了一种可扩展的高性能算法GTA，能够在商用计算机上确定性地解决大规模非对称旅行商问题(ATSP)，最高可处理5000个节点(约2500万个二进制变量)，收敛速度与高性能计算框架相当。


<details>
  <summary>Details</summary>
Motivation: 传统TSP求解器依赖超级计算硬件，算法复杂度高。需要开发一种能够在普通商用硬件上高效解决大规模TSP问题的确定性算法，将求解瓶颈从算法复杂度转移到硬件资源(RAM和系统内存)。

Method: 结合高效的启发式预热启动(能在秒级内达到接近最优解)和子回路消除策略(无需传统MTZ约束)，使用Gurobi Tabu算法(GTA)，提供实时迭代跟踪和自适应接口。

Result: 在8逻辑处理器的普通计算机上，能够一致地解决最多5000个节点的大规模ATSP实例，收敛速度与高性能计算框架相当，在物流、生物信息学和天文学等领域具有实际应用价值。

Conclusion: GTA算法实现了TSP求解瓶颈从算法复杂度向硬件资源的根本性转变，为跨学科的大规模TSP问题提供了确定性、资源高效的替代方案，无需依赖超级计算硬件。

Abstract: We present a scalable, high-performance algorithm that deterministically
solves large-scale instances of the Traveling Salesman problem (in its
asymmetric version, ATSP) to optimality using commercially available computing
hardware. By combining an efficient heuristic warm start, capable of achieving
near-optimality within seconds in some cases, with a subtour elimination
strategy that removes the need for traditional MTZ constraints, our approach
consistently resolves instances up to 5,000 nodes (approximately 25 million
binary variables) in record time on widely accessible computers, with eight
logical processors. We demonstrate reproducible results with convergence rates
comparable to those of high-performance computing frameworks. Real-time
iteration tracking and an adaptable interface allow seamless integration into
scheduling workflows in logistics, bioinformatics, and astronomy. Designed to
streamline solutions to large-scale TSP problems across disciplines, our
approach is benchmarked against widely used public datasets, offering a
deterministic, resource-efficient alternative to conventional solvers that rely
on supercomputing hardware. Our GTA (Gurobi Tabu Algorithm) algorithm is a
fundamental shift of TSP solution bottleneck from algorithmic complexity to the
underlying hardware (RAM and system memory), which is a highly desirable
characteristic.

</details>


### [11] [Hardness of Dynamic Core and Truss Decompositions](https://arxiv.org/abs/2509.13584)
*Yan S. Couto,Cristina G. Fernandes*

Main category: cs.DS

TL;DR: 本文证明了在动态图中计算k-core及其近似值的困难性，基于OMv和SETH猜想，表明不存在高效的动态算法，并解释了为什么现有研究主要关注有界算法。


<details>
  <summary>Details</summary>
Motivation: k-core作为一种重要的凝聚子图模型，在动态图分析中受到广泛关注。然而，现有的动态算法效率有限，本文旨在探究其根本原因并回答相关开放性问题。

Method: 基于OMv（在线矩阵向量乘法）和SETH（强指数时间假说）等复杂性理论猜想，通过归约方法证明k-core动态计算的下界，同时提出了2-core的多对数动态算法。

Result: 证明了除非能改进矩阵乘法等基础算法的长期最优结果，否则不存在高效的动态k-core算法；同时证明了有界算法也不存在；但提出了2-core的有效动态算法。

Conclusion: k-core的动态计算本质上困难，这解释了为什么现有研究主要关注特殊情况；虽然一般情况困难，但特定情况（如2-core）仍可能存在高效算法。

Abstract: The k-core of a graph is its maximal subgraph with minimum degree at least k,
and the core value of a vertex u is the largest k for which u is contained in
the k-core of the graph. Among cohesive subgraphs, k-core and its variants have
received a lot of attention recently, particularly on dynamic graphs, as
reported by Hanauer, Henzinger, and Schulz in their recent survey on dynamic
graph algorithms. We answer questions on k-core stated in the survey, proving
that there is no efficient dynamic algorithm for k-core or to find (2 -
{\epsilon})-approximations for the core values, unless we can improve
decade-long state-of-the-art algorithms in many areas including matrix
multiplication and satisfiability, based on the established OMv and SETH
conjectures. Some of our results show that there is no dynamic algorithm for
k-core asymptotically faster than the trivial ones. This explains why most
recent research papers in this area focus not on a generic efficient dynamic
algorithm, but on finding a bounded algorithm, which is fast when few core
values change per update. However, we also prove that such bounded algorithms
do not exist, based on the OMv conjecture. We present lower bounds also for a
directed version of the problem, and for the edge variant of the problem, known
as k-truss. On the positive side, we present a polylogarithmic dynamic
algorithm for 2-core.

</details>


### [12] [On Solving Asymmetric Diagonally Dominant Linear Systems in Sublinear Time](https://arxiv.org/abs/2509.13891)
*Tsz Chiu Kwok,Zhewei Wei,Mingji Yang*

Main category: cs.DS

TL;DR: 本文研究了在亚线性时间内求解行/列对角占优线性系统Mx=b的问题，目标是估计给定向量t与特定解x*的点积t⊤x*。提出了最大p-范数间隙的新概念，并开发了基于随机游走采样和局部推送技术的算法框架。


<details>
  <summary>Details</summary>
Motivation: 将对称对角占优系统的亚线性时间求解器推广到非对称情况，为局部图算法和有向谱图理论提供统一的理论框架。

Method: 通过诺依曼级数表达解，证明其收敛性，并利用最大p-范数间隙控制截断误差。采用随机游走采样、局部推送及其双向组合等技术开发算法。

Result: 建立了问题数学结构的特征化，为有界最大p-范数间隙系统开发了多种算法结果，统一理解了Forward Push和Backward Push方法。

Conclusion: 该框架为亚线性求解器、局部图算法和有向谱图理论的进一步研究奠定了基础，继承了SDD求解器和局部PageRank计算的硬度结果。

Abstract: We initiate a study of solving a row/column diagonally dominant (RDD/CDD)
linear system $Mx=b$ in sublinear time, with the goal of estimating
$t^{\top}x^*$ for a given vector $t\in R^n$ and a specific solution $x^*$. This
setting naturally generalizes the study of sublinear-time solvers for symmetric
diagonally dominant (SDD) systems [AKP19] to the asymmetric case.
  Our first contributions are characterizations of the problem's mathematical
structure. We express a solution $x^*$ via a Neumann series, prove its
convergence, and upper bound the truncation error on this series through a
novel quantity of $M$, termed the maximum $p$-norm gap. This quantity
generalizes the spectral gap of symmetric matrices and captures how the
structure of $M$ governs the problem's computational difficulty.
  For systems with bounded maximum $p$-norm gap, we develop a collection of
algorithmic results for locally approximating $t^{\top}x^*$ under various
scenarios and error measures. We derive these results by adapting the
techniques of random-walk sampling, local push, and their bidirectional
combination, which have proved powerful for special cases of solving RDD/CDD
systems, particularly estimating PageRank and effective resistance on graphs.
Our general framework yields deeper insights, extended results, and improved
complexity bounds for these problems. Notably, our perspective provides a
unified understanding of Forward Push and Backward Push, two fundamental
approaches for estimating random-walk probabilities on graphs.
  Our framework also inherits the hardness results for sublinear-time SDD
solvers and local PageRank computation, establishing lower bounds on the
maximum $p$-norm gap or the accuracy parameter. We hope that our work opens the
door for further study into sublinear solvers, local graph algorithms, and
directed spectral graph theory.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [13] [Delta Matters: An Analytically Tractable Model for $β$-$δ$ Discounting Agents](https://arxiv.org/abs/2509.13637)
*Yasunori Akagi,Takeshi Kurashima*

Main category: cs.GT

TL;DR: 本文扩展了时间不一致行为模型，从δ=1扩展到0<δ≤1的一般情况，证明了闭式解的存在性，并推导了任务放弃条件和最优干预计算方法。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注δ=1的特殊情况，限制了模型对现实世界决策过程的适用性。本文旨在放松这一约束，研究一般δ值下的时间不一致行为。

Method: 使用β-δ贴现（准双曲线贴现）模型，分析0<δ≤1范围内的时间不一致行为，推导闭式解并建立任务放弃条件和最优干预计算框架。

Result: 证明了在一般δ值下仍可获得闭式解，发现了代理行为放弃任务的条件，并开发了高效的最优干预计算方法。

Conclusion: δ值对代理行为和最优干预策略具有关键影响，固定δ=1会过度简化现实决策过程，本文的扩展模型能更好地捕捉真实世界的时间不一致行为。

Abstract: Humans exhibit time-inconsistent behavior, in which planned actions diverge
from executed actions. Understanding time inconsistency and designing
appropriate interventions is a key research challenge in computer science and
behavioral economics. Previous work focuses on progress-based tasks and derives
a closed-form description of agent behavior, from which they obtain optimal
intervention strategies. They model time-inconsistency using the
$\beta$-$\delta$ discounting (quasi-hyperbolic discounting), but the analysis
is limited to the case $\delta = 1$. In this paper, we relax that constraint
and show that a closed-form description of agent behavior remains possible for
the general case $0 < \delta \le 1$. Based on this result, we derive the
conditions under which agents abandon tasks and develop efficient methods for
computing optimal interventions. Our analysis reveals that agent behavior and
optimal interventions depend critically on the value of $\delta$, suggesting
that fixing $\delta = 1$ in many prior studies may unduly simplify real-world
decision-making processes.

</details>


### [14] [Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation](https://arxiv.org/abs/2509.13653)
*Hang Ren,Yulin Wu,Shuhan Qi,Jiajia Zhang,Xiaozhen Sun,Tianzi Ma,Xuan Wang*

Main category: cs.GT

TL;DR: 本文提出了一种自适应技术来解决奖励变换框架中的参数敏感性问题，通过动态调整参数来改善RTRM和RTCFR算法在NFG和EFG中的收敛性能


<details>
  <summary>Details</summary>
Motivation: 传统的遗憾最小化方法只能保证平均策略收敛，计算成本高且存在误差。奖励变换框架虽然实现了最终迭代收敛，但对人工调参敏感，容易导致收敛缓慢、振荡或陷入局部最优

Method: 提出自适应参数调整技术，动态平衡探索与利用，改进遗憾积累过程，增强RTRM和RTCFR及其变体在NFG和EFG中的收敛性能

Result: 实验结果表明，该方法显著加速收敛速度，在线性收敛方面表现优异，超越了现有最先进算法

Conclusion: 自适应参数调整技术有效解决了奖励变换框架的实践挑战，实现了理论保证与实际性能的更好一致性，为NFG和EFG求解提供了更有效的解决方案

Abstract: Regret minimization is a powerful method for finding Nash equilibria in
Normal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically
guarantees convergence only for the average strategy. However, computing the
average strategy requires significant computational resources or introduces
additional errors, limiting its practical applicability. The Reward
Transformation (RT) framework was introduced to regret minimization to achieve
last-iterate convergence through reward function regularization. However, it
faces practical challenges: its performance is highly sensitive to manually
tuned parameters, which often deviate from theoretical convergence conditions,
leading to slow convergence, oscillations, or stagnation in local optima.
  Inspired by previous work, we propose an adaptive technique to address these
issues, ensuring better consistency between theoretical guarantees and
practical performance for RT Regret Matching (RTRM), RT Counterfactual Regret
Minimization (RTCFR), and their variants in solving NFGs and EFGs more
effectively. Our adaptive methods dynamically adjust parameters, balancing
exploration and exploitation while improving regret accumulation, ultimately
enhancing asymptotic last-iterate convergence and achieving linear convergence.
Experimental results demonstrate that our methods significantly accelerate
convergence, outperforming state-of-the-art algorithms.

</details>


### [15] [Nash Equilibria in Games with Playerwise Concave Coupling Constraints: Existence and Computation](https://arxiv.org/abs/2509.14032)
*Philip Jordan,Maryam Kamgarpour*

Main category: cs.GT

TL;DR: 该论文研究了具有共享耦合约束的连续静态博弈中纳什均衡的存在性和计算方法，在玩家凹效用和凹约束条件下建立了存在性证明，并提出了基于对数障碍正则化的梯度上升算法来计算近似纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 研究具有共享耦合约束的博弈中纳什均衡的存在性和计算问题，因为现有结果依赖于联合凸性等强假设，无法适用于玩家凹约束的情况。

Method: 使用拓扑不动点理论和可行集可收缩性的结构洞察来证明纳什均衡存在性；对于计算，采用带有自适应步长的对数障碍正则化梯度上升方法，在存在势函数的假设下进行独立梯度计算。

Result: 在较弱的条件下证明了纳什均衡的存在性；提出的算法从初始可行策略出发，在精确梯度反馈下，可以在O(ε⁻³)次迭代内收敛到ε-近似约束纳什均衡。

Conclusion: 该工作为具有玩家凹约束的博弈提供了纳什均衡存在性的理论保证和有效的计算方法，放宽了传统方法对联合凸性的强假设要求。

Abstract: We study the existence and computation of Nash equilibria in continuous
static games where the players' admissible strategies are subject to shared
coupling constraints, i.e., constraints that depend on their \emph{joint}
strategies. Specifically, we focus on a class of games characterized by
playerwise concave utilities and playerwise concave constraints. Prior results
on the existence of Nash equilibria are not applicable to this class, as they
rely on strong assumptions such as joint convexity of the feasible set. By
leveraging topological fixed point theory and novel structural insights into
the contractibility of feasible sets under playerwise concave constraints, we
give an existence proof for Nash equilibria under weaker conditions. Having
established existence, we then focus on the computation of Nash equilibria via
independent gradient methods under the additional assumption that the utilities
admit a potential function. To account for the possibly nonconvex feasible
region, we employ a log barrier regularized gradient ascent with adaptive
stepsizes. Starting from an initial feasible strategy profile and under exact
gradient feedback, the proposed method converges to an $\epsilon$-approximate
constrained Nash equilibrium within $\mathcal{O}(\epsilon^{-3})$ iterations.

</details>


### [16] [Generalised Reachability Games Revisited](https://arxiv.org/abs/2509.14091)
*Sougata Bose,Daniel Hausmann,Soumyajit Paul,Sven Schewe,Tansholpan Zhanabekova*

Main category: cs.GT

TL;DR: 本文研究了图上的广义可达性游戏，改进了复杂度分析，并探讨了优化变体的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 经典可达性游戏是零和游戏，Eve的目标是访问目标集中的顶点，Adam的目标是阻止。广义可达性游戏扩展了这一概念，Eve需要访问多个目标集中的所有顶点。本文旨在深入研究这类游戏的复杂度特性。

Method: 通过理论分析和复杂度证明，研究了广义可达性游戏的求解复杂度，包括改进已知的可处理类，并分析优化变体（如最大化访问的单例目标集数量）的计算复杂性。

Result: 1）将可处理类从所有目标集都是单例扩展到允许对数数量的任意大小目标集；2）优化变体在大多数情况下是难解的，特别是当Eve试图最大化访问的单例目标集数量时是NP难的；3）当所有目标集都是单例时，通过要求Eve承诺她能保证访问的最大目标集子集可以恢复可处理性。

Conclusion: 广义可达性游戏在复杂度方面具有丰富的结构，优化变体通常更难处理，但在特定约束下可以找到可处理的解决方案。

Abstract: Classic reachability games on graphs are zero-sum games, where the goal of
one player, Eve, is to visit a vertex from a given target set, and that of
other player, Adam, is to prevent this. Generalised reachability games, studied
by Fijalkow and Horn, are a generalisation of reachability objectives, where
instead of a single target set, there is a family of target sets and Eve must
visit all of them in any order. In this work, we further study the complexity
of solving two-player games on graphs with generalised reachability objectives.
Our results are twofold: first, we provide an improved complexity picture for
generalised reachability games, expanding the known tractable class from games
in which all target sets are singleton to additionally allowing a logarithmic
number of target sets of arbitrary size. Second, we study optimisation variants
of generalised reachability with a focus on the size of the target sets. For
these problems, we show intractability for most interesting cases.
Particularly, in contrast to the tractability in the classic variant for
singleton target sets, the optimisation problem is NP-hard when Eve tries to
maximise the number of singleton target sets that are visited. Tractability can
be recovered in the optimisation setting when all target sets are singleton by
requiring that Eve pledges a maximum sized subset of target sets that she can
guarantee to visit.

</details>


### [17] [Sound Value Iteration for Simple Stochastic Games](https://arxiv.org/abs/2509.14112)
*Muqsit Azeem,Jan Kretinsky,Maximilian Weininger*

Main category: cs.GT

TL;DR: 扩展了声音值迭代(SVI)算法，使其能够处理随机游戏(SG)和具有结束组件的MDP，提供了精度保证和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 基本值迭代算法无法提供结果精度保证，现有的SVI算法虽然能提供精确的上下界且在概率循环中收敛更快，但不能处理随机游戏和具有结束组件的MDP。

Method: 扩展SVI算法，改进对结束组件的处理方式，并提供多种优化技术。

Result: 开发了原型实现，实验证明在具有概率循环的系统上具有潜在优势。

Conclusion: 成功扩展了SVI算法，使其能够处理更广泛的MDP和SG问题，特别是在存在结束组件的情况下。

Abstract: Algorithmic analysis of Markov decision processes (MDP) and stochastic games
(SG) in practice relies on value-iteration (VI) algorithms. Since basic VI does
not provide guarantees on the precision of the result, variants of VI have been
proposed that offer such guarantees. In particular, sound value iteration (SVI)
not only provides precise lower and upper bounds on the result, but also
converges faster in the presence of probabilistic cycles. Unfortunately, it is
neither applicable to SG, nor to MDP with end components. In this paper, we
extend SVI and cover both cases. The technical challenge consists mainly in
proper treatment of end components, which require different handling than in
the literature. Moreover, we provide several optimizations of SVI. Finally, we
evaluate our prototype implementation experimentally to demonstrate its
potential on systems with probabilistic cycles.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [18] [Efficient STAR-RIS Mode for Energy Minimization in WPT-FL Networks with NOMA](https://arxiv.org/abs/2509.13441)
*MohammadHossien Alishahi,Ming Zeng,Paul Fortier,Omer Waqar,Muhammad Hanif,Dinh Thai Hoang,Diep N. Nguyen,Quoc-Viet Pham*

Main category: cs.IT

TL;DR: 本文研究在6G物联网网络中，通过STAR-RIS技术结合无线能量传输和联邦学习，优化上行和下行传输阶段的能量消耗，采用能量分割和时间切换两种模式进行联合优化。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络中物联网设备的大规模部署，面临着通信开销大、覆盖范围有限和电池寿命短等挑战。需要可持续、低延迟和节能的通信解决方案。

Method: 提出非凸能量最小化问题，针对STAR-RIS的ES和TS模式进行联合优化。采用块坐标下降法将问题分解为两个子问题：优化STAR-RIS相移矢量和波束成形矩阵，以及通过一维搜索或二分算法分配时间、功率和计算频率。

Result: 通过联合优化STAR-RIS相移、波束成形、时间功率分配和计算频率，实现了能量消耗的最小化。

Conclusion: STAR-RIS技术在WPT-FL多天线AP网络中能够有效扩展覆盖范围并提高能效，为大规模物联网网络提供了可持续的通信解决方案。

Abstract: With the massive deployment of IoT devices in 6G networks, several critical
challenges have emerged, such as large communication overhead, coverage
limitations, and limited battery lifespan. FL, WPT, multi-antenna AP, and RIS
can mitigate these challenges by reducing the need for large data
transmissions, enabling sustainable energy harvesting, and optimizing the
propagation environment. Compared to conventional RIS, STAR-RIS not only
extends coverage from half-space to full-space but also improves energy saving
through appropriate mode selection. Motivated by the need for sustainable,
low-latency, and energy-efficient communication in large-scale IoT networks,
this paper investigates the efficient STAR-RIS mode in the uplink and downlink
phases of a WPT-FL multi-antenna AP network with non-orthogonal multiple access
to minimize energy consumption, a joint optimization that remains largely
unexplored in existing works on RIS or STAR-RIS. We formulate a non-convex
energy minimization problem for different STAR-RIS modes, i.e., energy
splitting (ES) and time switching (TS), in both uplink and downlink
transmission phases, where STAR-RIS phase shift vectors, beamforming matrices,
time and power for harvesting, uplink transmission, and downlink transmission,
local processing time, and computation frequency for each user are jointly
optimized. To tackle the non-convexity, the problem is decoupled into two
subproblems: the first subproblem optimizes STAR-RIS phase shift vectors and
beamforming matrices across all WPT-FL phases using block coordinate descent
over either semi-definite programming or Rayleigh quotient problems, while the
second one allocates time, power, and computation frequency via the
one-dimensional search algorithms or the bisection algorithm.

</details>


### [19] [Uplink-Downlink Duality for Beamforming in Integrated Sensing and Communications](https://arxiv.org/abs/2509.13661)
*Kareem M. Attiah,Wei Yu*

Main category: cs.IT

TL;DR: 本文提出了一种用于集成感知与通信(ISAC)系统的波束赋形和功率优化框架，通过最小化贝叶斯克拉美罗下界(BCRB)来优化参数估计精度，同时满足通信用户的信干噪比约束。


<details>
  <summary>Details</summary>
Motivation: 随着ISAC技术的发展，需要同时优化感知性能和通信质量，但现有的优化方法在处理这种联合优化问题时存在效率不足的问题。

Method: 提出了两个关键创新：1) 证明BCRB最小化问题对应于在特定感知方向上最大化波束赋形功率；2) 将经典MIMO通信中的上下行对偶性扩展到ISAC场景，建立了包含负噪声功率和额外条件的对偶理论。

Result: 基于新的对偶理论，开发了高效的迭代算法来联合优化ISAC系统的功率和波束赋形器。

Conclusion: 该研究为ISAC系统的波束赋形和功率优化提供了有效的理论框架和算法解决方案，解决了感知与通信性能的联合优化问题。

Abstract: This paper considers the beamforming and power optimization problem for a
class of integrated sensing and communications (ISAC) problems that utilize the
communication signals simultaneously for sensing. We formulate the problem of
minimizing the Bayesian Cram\'er-Rao bound (BCRB) on the mean-squared error of
estimating a vector of parameters, while satisfying downlink
signal-to-interference-and-noise-ratio constraints for a set of communication
users at the same time. The proposed optimization framework comprises two key
new ingredients. First, we show that the BCRB minimization problem corresponds
to maximizing beamforming power along certain sensing directions of interest.
Second, the classical uplink-downlink duality for multiple-input
multiple-output communications can be extended to the ISAC setting, but unlike
the classical communication problem, the dual uplink problem for ISAC may
entail negative noise power and needs to include an extra condition on the
uplink beamformers. This new duality theory opens doors for an efficient
iterative algorithm for optimizing power and beamformers for ISAC.

</details>


### [20] [Clustering Strategies in Satellite-Aided Communications](https://arxiv.org/abs/2509.13701)
*Tam Ninh Thi-Thanh,Nguyen Minh Quan,Do Son Tung,Trinh Van Chien,Hung Tran*

Main category: cs.IT

TL;DR: 本文综述了基于机器学习和启发式算法的现代聚类方法在下一代卫星网络中的应用，实验表明改进的机器学习技术和图论方法比传统聚类算法性能更好，特别是在大规模卫星网络场景中。


<details>
  <summary>Details</summary>
Motivation: 随着下一代卫星网络的快速发展，需要解决聚类任务、用户分组和高效链路管理问题，以优化网络性能并减少干扰。

Method: 对基于机器学习和启发式算法的现代聚类方法进行全面综述，包括改进的机器学习技术和图论方法。

Result: 实验结果表明改进的机器学习技术和图论方法比传统聚类算法（如先前研究中的纯聚类算法）具有显著更好的性能和可扩展性，在大规模卫星网络场景中优势尤为明显。

Conclusion: 论文概述了潜在的研究方向，并讨论了集成多维解决方案以提高未来卫星通信的适应性和效率。

Abstract: With the rapid advancement of next-generation satellite networks, addressing
clustering tasks, user grouping, and efficient link management has become
increasingly critical to optimize network performance and reduce interference.
In this paper, we provide a comprehensive overview of modern clustering
approaches based on machine learning and heuristic algorithms. The experimental
results indicate that improved machine learning techniques and graph
theory-based methods deliver significantly better performance and scalability
than conventional clustering methods, such as the pure clustering algorithm
examined in previous research. These advantages are especially evident in
large-scale satellite network scenarios. Furthermore, the paper outlines
potential research directions and discusses integrated, multi-dimensional
solutions to enhance adaptability and efficiency in future satellite
communication.

</details>


### [21] [Asymptotic Analysis of Nonlinear One-Bit Precoding in Massive MIMO Systems via Approximate Message Passing](https://arxiv.org/abs/2509.13955)
*Zheyu Wu,Junjie Ma,Ya-Feng Liu,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文分析了采用1位数模转换器的大规模MIMO系统中的非线性符号级预编码问题，提出了基于近似消息传递的分析框架来推导高维渐近性能，并证明了ℓ∞²正则化器在混合正则化函数中的最优性。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统使用1位数模转换器可以提供硬件效率，但1位约束使预编码设计变为离散非凸优化问题，需要分析广泛采用的"凸松弛后量化"方法的性能。

Method: 首先求解离散最小均方误差预编码问题的凸松弛，然后量化解以满足1位约束。开发基于近似消息传递(AMP)的分析框架，推导大系统极限下的符号错误概率闭式表达式。

Result: 经验结果表明，ℓ∞²正则化器在最优正则化参数下，在广泛凸正则化函数类中实现最优SEP性能。理论证明了ℓ∞²正则化器在混合ℓ∞²-ℓ₂²正则化函数中的最优性。

Conclusion: 提出的AMP分析框架能够定量表征模型和系统参数对SEP性能的影响，ℓ∞²正则化器在1位预编码中表现出优越性能，为硬件高效的大规模MIMO系统设计提供了理论指导。

Abstract: Massive multiple-input multiple-output (MIMO) systems employing one-bit
digital-to-analog converters offer a hardware-efficient solution for wireless
communications. However, the one-bit constraint poses significant challenges
for precoding design, as it transforms the problem into a discrete and
nonconvex optimization task. In this paper, we investigate a widely adopted
``convex-relaxation-then-quantization" approach for nonlinear symbol-level
one-bit precoding. Specifically, we first solve a convex relaxation of the
discrete minimum mean square error precoding problem, and then quantize the
solution to satisfy the one-bit constraint. To analyze the high-dimensional
asymptotic performance of this scheme, we develop a novel analytical framework
based on approximate message passing (AMP). This framework enables us to derive
a closed-form expression for the symbol error probability (SEP) at the receiver
side in the large-system limit, which provides a quantitative characterization
of how model and system parameters affect the SEP performance. Our empirical
results suggest that the $\ell_\infty^2$ regularizer, when paired with an
optimally chosen regularization parameter, achieves optimal SEP performance
within a broad class of convex regularization functions. As a first step
towards a theoretical justification, we prove the optimality of the
$\ell_\infty^2$ regularizer within the mixed $\ell_\infty^2$-$\ell_2^2$
regularization functions.

</details>


### [22] [Deriving Moments in the Age of Gossip Process from Percolation](https://arxiv.org/abs/2509.13981)
*Thomas Jacob Maranzatto,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文基于信息年龄与首次通过渗流理论的联系，重新推导了gossip网络中任意k阶矩的递归恒等式，提供了比现有方法更简洁的概率论证明


<details>
  <summary>Details</summary>
Motivation: 研究信息年龄(AoI)在gossip网络中的基本恒等式，利用与首次通过渗流理论的新联系来简化证明过程

Method: 基于信息年龄与首次通过渗流理论的联系，使用概率论基本事实推导k阶矩的递归恒等式，推广统计物理学中的Eden模型技术

Result: 成功恢复了任意k阶矩的递归恒等式，证明过程比现有方法更简洁易懂，仅需概率论基础知识

Conclusion: 该方法不仅建立了与渗流理论的联系，还提供了更简洁的证明框架，对统计物理学中的Eden模型技术进行了推广

Abstract: This paper concerns fundamental identities in the study of age of information
(AoI) in gossip networks. We recover known recursive identities for arbitrary
kth moments of the age process based on the recent connection between AoI and
first passage percolation. Apart from the connection to percolation, our proofs
are more concise and can be followed using only elementary facts from
probability. Our argument generalizes some techniques known in the statistical
physics community, and we remark on connections to the Eden model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 系统比较了思考型和非思考型LLM在作为评判者时的表现，发现思考型模型在准确率、计算效率和鲁棒性方面均优于非思考型模型，即使经过多种增强策略改进后


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作自动评判工具，确保其可靠性、效率和鲁棒性变得至关重要，需要系统比较不同LLM在评判任务中的表现

Method: 使用开源Qwen 3模型（0.6B、1.7B和4B参数），在RewardBench任务上评估准确性和计算效率，并测试了上下文学习、规则引导评判、基于参考的评估和n-best聚合等增强策略

Result: 思考型模型准确率高出约10个百分点，计算开销仅增加不到2倍，而增强策略如少样本学习成本高（>8倍）但收益有限。在多种偏见条件下，思考型模型保持更高的稳定性（平均高6%）

Conclusion: 显式推理在LLM作为评判者的范式中具有明显优势，不仅在准确性和效率方面，在鲁棒性方面也是如此，这一优势在多语言环境中也得到了验证

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [24] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究发现大型语言模型存在评估意识，即模型能区分评估和部署环境，这种能力随模型规模呈幂律增长，可用于预测未来更大模型的欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能在评估时隐藏危险能力，先前研究仅在单个70B模型中发现此现象，但不同规模模型的评估意识变化规律尚不清楚。

Method: 使用线性探测方法分析15个不同规模模型（0.27B到70B参数）的激活向量，研究评估意识的缩放规律。

Result: 发现评估意识随模型规模呈幂律增长，这种缩放规律可用于预测未来更大模型的欺骗行为。

Conclusion: 评估意识的幂律缩放规律为AI安全评估提供了重要指导，有助于设计针对不同规模模型的评估策略。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [25] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练提升大语言模型推理忠实性的对齐方法，通过生成忠实/不忠实推理对来训练模型偏好因果一致的推理路径，在多个推理任务上显著提升了忠实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法存在推理步骤与最终答案缺乏因果关联的问题，导致输出脆弱且不可信。虽然已有方法主要关注测量忠实性，但系统性提升忠实性的方法仍然有限。

Method: 提出FRIT方法：1）通过干预模型生成的思维链中的推理步骤来生成合成训练数据，创建忠实/不忠实推理对；2）应用直接偏好优化训练模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上，FRIT在GSM8K任务上将Mistral的忠实推理提升了3.4个百分点，准确性提升了7.6个百分点。

Conclusion: FRIT提供了首个可扩展、无监督的方法来训练语言模型产生更可靠和可解释的推理，解决了推理性能与可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [26] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 本文主张AI安全研究应采用抗脆弱性视角，使系统处理罕见和分布外事件的能力随时间增强，而非依赖静态基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试和一次性鲁棒性测试无法应对环境演变和模型漂移问题（如奖励黑客攻击、过度优化等），需要新的方法来保证AI系统的长期安全性。

Method: 提出抗脆弱性方法，利用现有不确定性来更好地准备应对未来更大的不可预测性，包括重新校准AI安全的测量、基准测试和持续改进方法。

Result: 识别了静态测试的关键局限性（场景多样性不足、奖励黑客问题、过度对齐等），并探讨了抗脆弱性解决方案管理罕见事件的潜力。

Conclusion: 抗脆弱性方法对于开放端机器学习系统的长期可靠性至关重要，需要建立伦理和实践指南来培养抗脆弱性AI安全社区，作为现有鲁棒性方法的补充。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [27] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: IMAC方法利用世界模型生成想象环境，通过无监督环境设计自动生成课程，在有限数据下训练出能很好泛化到新任务变体的鲁棒智能体


<details>
  <summary>Details</summary>
Motivation: 解决在具身环境中训练智能体需要大量训练数据或精确仿真的问题，利用离线被动收集数据构建世界模型来生成多样化的训练环境

Method: 提出IMAC（Imagined Autocurricula）方法，结合无监督环境设计（UED）在世界模型生成的想象环境中自动生成课程训练智能体

Result: 在具有挑战性的程序生成环境中，仅使用较窄数据集学习的世界模型进行训练，就能在保留环境中实现强大的迁移性能

Conclusion: 该方法为利用更大规模的基础世界模型训练通用能力智能体开辟了道路

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [28] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 本文系统比较了不同动作空间在Minecraft环境中的表现，发现最优动作空间高度依赖具体任务。为此提出了Chain of Action (CoA)框架，将高层规划和底层控制统一在单一VLA模型中，通过混合动作空间训练实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决动作空间选择这一关键但未解决的挑战，因为研究发现没有单一动作空间在所有任务中都最优，这给构建通用智能体带来了困境。

Method: 提出了Chain of Action (CoA)框架，将抽象动作视为中间推理步骤而非单独策略的命令，在单一VLA模型中统一高层规划和底层控制。使用混合动作空间训练All-in-One智能体。

Result: CoA框架训练的智能体实现了新的最先进性能，相比专门的基线模型提高了整体任务成功率，学习到了更鲁棒和可泛化的策略。

Conclusion: CoA框架成功解决了动作空间选择的困境，通过统一规划和控制的端到端方法实现了更好的通用性。同时发布了OpenHA套件促进可重复研究。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [29] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA是一个用于智能手机AI任务指令推荐的创新框架，通过长按图像或文本来提供上下文相关的AI任务建议，使用多模态大语言模型和结构化推理来提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，智能手机集成了多种AI服务，但用户访问这些预定义AI服务的方式需要更加直观和简化。

Method: 采用多模态大语言模型(MLLM)推荐流水线进行结构化推理，结合模板增强推理机制和前缀树约束解码策略，确保输出与预定义指令候选集一致。

Result: 通过真实标注数据集和用户研究评估，MIRA在指令推荐准确性方面表现出显著提升。

Conclusion: MIRA有潜力彻底改变用户与智能手机AI服务的交互方式，提供更无缝和高效的体验。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [30] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct指令调优框架，通过逻辑思维链推理增强大语言模型在符号规划任务中的能力，在标准基准测试中达到94%的规划准确率，相比基线模型提升66%


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多种任务上表现出色，但在需要形式化表示（如PDDL）的结构化符号规划方面能力有限，需要弥合通用推理能力与自动规划所需的逻辑精确性之间的差距

Method: 开发指令提示引导模型通过精确的逻辑推理步骤，严格推理动作适用性、状态转换和计划有效性，将规划过程分解为关于前提条件满足、效果应用和不变性保持的显式推理链

Result: 在多个规划领域的实验结果显示，基于思维链推理的指令调优模型显著提升了规划能力，在标准基准测试中达到94%的规划准确率，相比基线模型绝对提升66%

Conclusion: 该工作为大语言模型与自动规划系统之间的能力差距提供了桥梁，为开发更好的AI规划系统指明了有前景的方向

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [31] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 提出了Agentic UAVs框架，通过五层架构增强无人机的大语言模型驱动推理能力，在搜救模拟中显著提升了检测性能和自主决策能力


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统主要依赖基于规则的控制和窄AI，缺乏情境感知推理、自主决策和生态系统集成能力，无法利用大语言模型进行实时知识访问

Method: 开发了包含感知、推理、行动、集成、学习五层架构的框架，集成YOLOv11目标检测、GPT-4推理和本地Gemma-3部署，基于ROS2和Gazebo构建原型系统

Result: 在模拟搜救场景中，检测置信度从0.72提升到0.79，人员检测率从75%提升到91%，行动推荐率从4.5%大幅提升到92%

Conclusion: 适度的计算开销能够实现质的自主性提升和生态系统集成，为大语言模型驱动的无人机自主系统提供了有效解决方案

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [32] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 提出语义融合方案，通过并行模糊成员特征通道增强Transformer语言模型，实现可解释的语义特征编码和可控生成


<details>
  <summary>Details</summary>
Motivation: 增强语言模型的语义理解和可控生成能力，同时保持模型轻量化和可解释性

Method: 为每个token创建包含词性、语义角色、边界标志、情感极性等可解释特征的向量，通过门控适配器将语义矩阵融合到LM中，使用标准下一词预测、辅助重建损失和轻量级正则化器进行训练

Result: 在合成双子句语料库上，语义融合提高了困惑度，实现了精确的用户可控极性和标点生成，同时保持模型简洁性

Conclusion: 该方法以较小开销实现了可解释的条件自然语言生成路径，完全兼容绑定输入输出嵌入

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [33] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出了星号算子（∗-operator），这是一个基于邻接结构并行传播（ASPP）的统一抽象推理框架，将结构化推理任务形式化为由隐式关系图指导的局部并行状态演化过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决抽象推理问题，需要一种既能保持局部计算约束又能实现全局推理能力的统一框架，以提供高效且收敛的计算范式。

Method: 基于邻接结构并行传播（ASPP）的星号算子，将推理任务建模为局部并行状态演化过程，并提出了创新的Embedding-Asterisk蒸馏方法。

Result: 在ARC2挑战和康威生命游戏中验证了算子的通用性、收敛性和优越性能，使用仅6M参数就在ARC2验证集上达到100%准确率。

Conclusion: 星号算子代表了神经符号推理领域的重大突破，提供了一个高效、收敛且通用的抽象推理计算框架。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [34] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2是一个完全自动化的强化学习代理生成框架，通过LLM驱动将自然语言任务描述转换为高性能RL解决方案，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理开发需要大量专业知识和迭代，失败率高且可访问性有限。需要实现完全自动化的RL代理设计。

Method: 采用双代理架构：生成器代理分析任务并生成可执行RL代理，目标代理是自动生成的RL代理。框架将RL开发分解为MDP建模和算法优化两个阶段，基于模型上下文协议构建。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等多个基准测试中，Agent^2始终优于手动设计的解决方案，性能提升高达55%，平均表现显著提升。

Conclusion: 这项工作建立了智能代理设计和优化其他代理的新范式，实现了真正端到端的闭环自动化，是自动化AI系统的根本性突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [35] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 对16个先进视觉语言模型在6个多模态数据集上的不确定性量化进行综合基准测试，发现大模型具有更好的不确定性量化能力，数学和推理任务的不确定性表现较差


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在复杂视觉理解方面取得了显著进展，但不确定性量化这一关键维度尚未得到足够关注，需要对此进行全面的基准测试研究

Method: 评估16个最先进的视觉语言模型（开源和闭源），在6个多模态数据集上使用3种不同的评分函数进行不确定性基准测试

Result: 较大模型始终表现出更好的不确定性量化能力；确定性更高的模型获得更高的准确率；数学和推理任务在所有模型中相比其他领域表现出较差的不确定性性能

Conclusion: 这项工作为多模态系统中可靠的不确定性评估奠定了基础，表明"知道更多的模型也更好地知道它们不知道什么"

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [36] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 使用Transformer架构从动作序列中学习命题STRIPS世界模型，通过监督式的下一个动作预测任务来推断隐藏的动作前提和效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要完整的动作前提和效果信息，而本文旨在仅从动作序列中学习世界模型，无需额外的状态信息，这更符合实际应用场景。

Method: 将任务构建为监督式的下一个动作预测问题，使用Transformer架构处理动作序列，通过正负样本（有效和无效动作序列）训练模型来推断动作的前提条件和效果。

Result: 实验表明，合适的Transformer架构能够准确表示命题STRIPS世界模型，并且仅通过随机生成的有效和无效动作序列就能成功学习这些模型。

Conclusion: 深度学习方法（特别是Transformer）能够从动作序列中有效学习世界模型，为从观察数据中自动获取领域知识提供了有前景的途径。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [37] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为（如谄媚和常识道德）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作通常只关注真实性或推理能力来展示表示引导的副作用，但许多权衡关系尚未被系统性地理解和探索。

Method: 构建了一个模块化的引导框架，基于五个流行引导方法的独特组件，收集了安全相关的主要和次要行为数据集来评估引导效果和行为纠缠。

Result: 在Qwen-2.5-7B和Llama-3.1-8B模型上的实验发现，强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠。

Conclusion: 表示引导的效果具有高度情境依赖性，需要系统性的评估框架来理解不同对齐目标之间的权衡关系，为此发布了SteeringControl基准和代码。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [38] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 为LLM代理配备类似人类的协作工具（社交媒体和日志工具）可以显著提升其在最困难编程问题上的表现，降低成本、减少交互轮次并加快完成速度，但效果因问题难度而异。


<details>
  <summary>Details</summary>
Motivation: 研究是否通过赋予LLM代理人类自然使用的协作工具和自主权，能够改善其在问题解决中的表现。

Method: 为Claude Code代理配备基于MCP的社交媒体和日志工具，允许它们自主使用这些工具，在34个Aider Polyglot Python编程挑战中进行测试。

Result: 协作工具在最困难问题上显著提升性能：成本降低15-40%，交互轮次减少12-27%，完成速度加快12-38%。不同模型自然采用不同的协作策略，代理更倾向于写作而非阅读（2-9倍）。

Conclusion: AI代理在其能力边界处可以系统性地受益于人类启发的协作工具，这表明自适应协作界面可以作为推理增强器，而非通用的效率提升工具。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [39] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 本研究调查了本科生在证明数学课程中使用生成式AI的情况、学生认知及其对教学的影响


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在高等教育中的快速兴起和现有AI检测工具的不可靠性，制定能够鼓励学生学习和批判性思维的政策变得日益重要

Method: 通过调查问卷和学生访谈，分析三个证明数学课程（抽象代数、拓扑学）中学生使用AI工具的方式、对AI有用性和局限性的认知

Result: 研究发现学生在允许使用AI的课程政策下如何与AI工具互动，以及这些认知对证明数学教学的影响

Conclusion: 讨论了将生成式AI整合到证明数学教学中的未来考虑因素

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [40] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA是一个用于在基于LLM的社会模拟中系统化指定智能体行为的工具包，通过显式编程认知偏见来解决传统自然语言描述方法的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用隐式自然语言描述来指定智能体行为，但这种方法无法在不同模型间产生一致的行为，且无法准确捕捉描述的细微差别。

Method: CoBRA包含两个组件：认知偏见指数（通过经典社会科学实验量化智能体反应来测量认知偏见）和行为调节引擎（将智能体行为与受控认知偏见对齐）。该方法基于经典社会科学实验来显式编程智能体的认知偏见。

Result: 评估结果表明，CoBRA能够以模型无关的方式精确编程社交智能体中展示的认知偏见。

Conclusion: CoBRA提供了一个有效的工具包，用于在LLM-based社会模拟中系统化和精确地控制智能体的认知偏见行为。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [41] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文提出了State-aware Reasoning (StaR)方法，通过训练多模态代理感知当前切换状态并相应执行指令，解决了GUI控制中切换指令执行不可靠的问题，在多个基准测试中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态代理在图形用户界面(GUI)控制中，特别是在执行切换控制指令时表现不可靠，尤其是在当前状态与期望状态已经匹配的情况下，这成为了一个关键瓶颈。

Method: 提出了State-aware Reasoning (StaR)训练方法，教导代理感知当前切换状态，从指令中分析期望状态，并相应地执行动作。

Result: 在三个多模态代理上的实验表明，StaR可以将切换指令执行准确率提高30%以上。在三个公共基准测试上的进一步评估显示，StaR还能提升一般任务性能。动态环境评估突显了StaR在现实应用中的潜力。

Conclusion: StaR方法有效解决了GUI控制中切换指令执行的可靠性问题，显著提升了多模态代理的性能，具有很好的实际应用前景。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [42] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专门为工业管理系统设计的探索式GUI代理框架，通过五个创新模块解决LLM-based GUI代理在工业管理中的五大挑战，显著提升了任务成功率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战。现有RPA自动化方案灵活性有限且维护成本高，而通用LLM-based GUI代理在工业管理中存在元素理解、精度效率、状态定位、部署约束和安全需求五大问题。

Method: 提出InfraMind框架，包含五个核心模块：(1)基于系统搜索探索和虚拟机快照的自主GUI理解；(2)内存驱动规划确保高精度高效任务执行；(3)高级状态识别用于分层界面中的鲁棒定位；(4)结构化知识蒸馏实现轻量模型高效部署；(5)多层安全机制保护敏感操作。

Result: 在开源和商业DCIM平台上的广泛实验表明，该方法在任务成功率和操作效率方面持续优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了LLM-based GUI代理在工业环境中的关键挑战。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [43] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习进行工具集成层次优化的框架，解决了LLM在数学推理中高精度任务的问题，包括数据构建、细粒度优化和推理增强三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得了显著进展，但在数值计算和形式符号操作等高精度任务上仍然存在困难。集成外部工具是弥补这一差距的有前景方法，但现有方法在构建工具集成推理数据、进行细粒度优化和增强推理方面面临挑战。

Method: 提出THOR框架：1) TIRGen - 基于多智能体actor-critic的流水线构建高质量工具集成推理路径数据集；2) 分层强化学习策略，联合优化轨迹级问题解决和步骤级代码生成；3) 自校正机制，利用工具反馈动态修正推理错误。

Result: 该方法在不同模型上表现出强大的泛化能力，在推理和非推理模型中都有效。在多个数学基准测试中实现了同类规模模型的最先进性能，同时在代码基准测试上也带来了一致的改进。

Conclusion: THOR通过工具集成和分层优化有效提升了LLM在高精度数学任务上的性能，为解决数学推理中的精度问题提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [44] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 本文提出了一种基于DPLL架构的精确方法来解决整数线性约束模型计数问题，通过集成混合整数规划中的简化技术显著提升了效率


<details>
  <summary>Details</summary>
Motivation: 线性约束是计算机科学、运筹学和优化等领域中最基本的约束之一，许多应用问题都归结为整数线性约束模型计数任务，需要高效的解决方案

Method: 基于穷举DPLL架构设计精确方法，并整合混合整数规划中的多种有效简化技术来提升效率

Result: 在2840个随机基准测试和4131个应用基准测试中，该方法解决了1718个随机实例（最先进方法仅解决1470个），并且是唯一能解决所有4131个应用实例的方法

Conclusion: 该方法在整数线性约束模型计数问题上显著优于所有现有精确方法，特别是在应用实例上表现出色

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [45] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究使用人工神经网络模型探讨信息流结构变化是否能带来认知性能的过渡性转变，发现递归网络相比前馈网络在处理复杂语法时具有质的性能提升，并观察到训练难度形成的过渡障碍。


<details>
  <summary>Details</summary>
Motivation: 研究动机是基于进化过渡理论，探讨认知能力是否通过一系列主要转变来演化，这些转变改变了生物神经网络的信息流结构。

Method: 使用理想化的信息流模型和人工神经网络，比较前馈、递归和分层拓扑结构的网络性能，控制网络大小和资源，测试学习不同复杂度人工语法的能力。

Result: 递归网络相比前馈网络在处理输入类型上有质的扩展，在最复杂语法学习上表现显著提升；分层网络在语法学习任务中并未优于非分层网络；递归网络的训练难度形成了过渡障碍和偶然不可逆性。

Conclusion: 某些信息流结构的变化确实能够产生认知性能的过渡性转变，这支持了认知能力通过主要过渡演化的理论，但并非所有网络拓扑变化都能带来性能优势。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [46] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent是一个多智能体系统，通过整合任务分配、数据标注和质量/成本管理，为LLM、SLM和人类专家提供端到端的协同标注流程控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注标注步骤本身，缺乏对多样化标注源（LLM、SLM、人类专家）的动态管理和复杂调度需求，需要统一的质-成本权衡解决方案。

Method: 采用多智能体系统架构，实现任务分配、数据标注和质量管理的一体化控制，让不同标注源在协作工作流中协同推进。

Result: 在六个多样化多模态分类任务上的广泛实验证明了CrowdAgent的有效性。

Conclusion: CrowdAgent通过端到端流程控制成功解决了多源标注的动态调度和质-成本权衡问题，为高质量标注数据生产提供了有效解决方案。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [47] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过层次架构（GCN作为一阶学习器，MLP作为二阶学习器）实证验证了二阶学习促进环境-认知同构性形成的假设，在迷宫导航任务中展示了显著性能提升和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索心理表征（结构化内部模型与外部环境同构）如何通过二阶学习机制（调整一阶学习）来促进高级认知，现有理论缺乏实证验证。

Method: 提出分层架构：使用图卷积网络（GCN）作为一阶学习器直接预测最优导航路径，MLP控制器作为二阶学习器动态调整GCN参数以适应结构新颖的迷宫环境。

Result: 实验显示当认知系统发展出与环境结构同构的内部心理地图时，二阶学习特别有效，在未见过的迷宫任务上实现了显著性能提升和鲁棒泛化。

Conclusion: 研究为结构化心理表征在最大化二阶学习效果中的关键作用提供了实证支持，验证了环境-认知同构性假说。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: 提出了USPIL框架，将物理信息神经网络与守恒定律结合，统一建模捕食者-猎物系统的时空动力学，在保持物理一致性的同时实现高效计算和机制解释。


<details>
  <summary>Details</summary>
Motivation: 生态系统的多尺度复杂动力学挑战传统建模方法，需要既能捕捉时空振荡和涌现模式，又能遵守守恒原理的新方法。

Method: USPIL框架整合物理信息神经网络(PINNs)和守恒定律，使用自动微分强制物理约束和自适应损失加权，统一处理ODE和PDE系统。

Result: 在Lotka-Volterra系统中，1D时间动力学达到98.9%相关性(loss:0.0219)，2D系统捕捉复杂螺旋波(pattern相关性0.94)，计算速度比数值求解器快10-50倍，守恒定律遵守度在0.5%以内。

Conclusion: USPIL为多尺度生态建模开辟了新途径，是生态预测、保护规划和生态系统韧性理解的变革性工具，确立了物理信息深度学习作为科学严谨范式的重要地位。

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [49] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 本文通过360次实验发现优化器选择对神经网络训练能耗有显著影响，AdamW和NAdam在能效方面表现最佳，而SGD在复杂数据集上性能优越但碳排放更高


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型日益复杂和计算需求增加，理解训练决策对环境的影响对于可持续AI发展变得至关重要

Method: 在三个基准数据集(MNIST, CIFAR-10, CIFAR-100)上使用8种流行优化器进行360次控制实验，每个优化器使用15个随机种子，通过CodeCarbon在Apple M1 Pro硬件上精确追踪能耗

Result: 发现训练速度、准确性和环境影响之间存在显著权衡，这些权衡因数据集和模型复杂度而异。AdamW和NAdam表现出一致的效率，而SGD在复杂数据集上性能优越但排放更高

Conclusion: 研究结果为从业者在机器学习工作流中平衡性能和可持续性提供了可行的见解

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [50] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet-Transolver框架，用于解决PET瓶屈曲分析问题，能够同时预测节点位移场和时间相关的反作用力，在几何变化域上实现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有神经代理和算子网络方法在处理非参数化几何域变化时泛化能力有限，而传统有限元分析计算成本高昂，需要开发高效的计算力学替代模型。

Method: 采用混合DeepONet-Transolver框架，结合深度算子网络和Transformer求解器，在参数化瓶几何上进行训练，使用Abaqus非线性有限元模拟生成254个独特设计的数据。

Result: 在四参数瓶几何族上，位移场的平均相对L2误差为2.5-13%，时间相关反作用力误差约2.4%，点位移误差在10^-4-10^-3量级，成功捕捉屈曲等关键物理现象。

Conclusion: 该框架展示了作为可扩展计算高效替代模型的潜力，特别适用于计算力学中的多任务预测和需要快速设计评估的应用场景。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [51] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS是一个10-800亿参数的像素级Swin扩散变换器，通过SWiPe并行化技术在Aurora超级计算机上实现高效扩展，在天气预测中超越IFS ENS并保持90天季节尺度的稳定性。


<details>
  <summary>Details</summary>
Motivation: 生成式机器学习为理解复杂地球系统动力学提供了新机会，但现有扩散方法在高分辨率下难以稳定扩展，需要解决这一技术瓶颈。

Method: 提出AERIS（1.3-80B参数像素级Swin扩散变换器）和SWiPe技术（结合窗口并行、序列并行和流水线并行，在不增加通信成本或全局批大小的情况下分片基于窗口的变换器）。

Result: 在Aurora（10,080节点）上实现10.21 ExaFLOPS混合精度性能，峰值达11.21 ExaFLOPS，弱扩展效率95.5%，强扩展效率81.6%。AERIS超越IFS ENS并在90天季节尺度保持稳定。

Conclusion: 十亿参数扩散模型在天气和气候预测方面具有巨大潜力，AERIS展示了在高分辨率地球系统建模中的可扩展性和性能优势。

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [52] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL是一种线性元学习算法，在保持可解释性的同时提高多个化学性质的预测准确性，性能比标准岭回归提升1.1-25倍。


<details>
  <summary>Details</summary>
Motivation: 化学研究中高质量数据集有限，机器学习方法虽然提升了预测能力但增加了数据需求，同时需要平衡预测准确性和人类可理解性。

Method: 采用元学习框架，识别相关任务间的共享模型参数，学习共同的功能流形作为新任务的更明智起点，即使任务不共享数据。

Result: 在不同数据集领域上，性能比标准岭回归提升1.1-25倍，始终优于或匹配传统线性方法。

Conclusion: LAMeL是化学性质预测中既准确又可解释的可靠工具，特别适合需要平衡准确性和可解释性的场景。

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [53] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了GenPAS框架，系统性地分析了生成式推荐中数据增强策略对模型性能的影响，通过三个偏差控制步骤统一现有方法并实现更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统中的数据增强策略往往被简化处理，缺乏系统性和原则性的理解，而不同的增强策略会导致显著的性能差异。

Method: 提出GenPAS框架，将数据增强建模为包含三个偏差控制步骤的随机采样过程：序列采样、目标采样和输入采样，统一现有策略并灵活控制训练分布。

Result: 在基准和工业数据集上的实验表明，GenPAS在准确性、数据效率和参数效率方面均优于现有策略。

Conclusion: GenPAS为生成式推荐中的训练数据构建提供了原则性指导，证明了系统化数据增强策略的重要性。

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [54] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: 研究发现GPT-4o mini存在"单模态瓶颈"安全架构缺陷，多模态推理被上下文无关的安全过滤器系统性地阻断，导致良性表情包被错误拦截


<details>
  <summary>Details</summary>
Motivation: 随着大型多模态模型在日常数字生活中的广泛应用，理解其安全架构对AI对齐至关重要，需要系统分析全球部署模型在多模态仇恨言论检测任务中的表现

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段调查，分析模型的推理过程和失败模式，并对144个内容策略拒绝案例进行定量验证

Result: 实验识别出"单模态瓶颈"架构缺陷，50%的拒绝由视觉内容触发，50%由文本内容触发，安全系统脆弱，不仅拦截高风险图像还错误拦截良性表情包

Conclusion: 研究揭示了最先进多模态模型中能力与安全之间的根本冲突，强调需要更集成化、上下文感知的对齐策略，以确保AI系统既能安全又能有效部署

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [55] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 基于自然语言处理和序列感知神经网络的自动化故障分析框架，通过语义嵌入技术处理EPICS控制系统实时事件日志，实现异常检测和故障预警


<details>
  <summary>Details</summary>
Motivation: 为了解决先进光源(ALS)控制系统中复杂故障的快速识别问题，需要开发能够实时分析事件日志并检测异常模式的自动化工具

Method: 将事件日志作为自然语言处理，使用语义嵌入技术生成上下文向量表示，然后利用在正常操作数据上训练的序列感知神经网络为每个事件分配实时异常分数

Result: 该方法能够标记与基线行为的偏差，使操作员能够快速识别导致复杂系统故障的关键事件序列

Conclusion: 该框架为大型控制系统提供了一种有效的实时故障分析和预警解决方案，通过自然语言处理技术提升了故障检测的准确性和效率

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [56] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私的私有预测框架，用于生成高质量合成文本，在保证强隐私保护的同时维持高实用性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在隐私泄露风险，攻击者可能从提示中提取敏感信息，需要在不微调模型的情况下提供理论上的隐私保护

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token的输出分布，结合私有和公共推理的混合操作来增强实用性

Result: 在上下文学习任务上超越了现有最先进方法，能够生成长且连贯的合成文本

Conclusion: 该方法为隐私保护文本生成提供了一个有前景的方向，在保持高实用性的同时提供强隐私保证

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [57] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: 提出DeepLogit模型，通过序列约束方法结合深度学习与离散选择模型，在保持参数可解释性的同时提升预测精度


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在规划和政策领域的应用受限于其黑盒特性，需要开发既能保持可解释性又能提高准确性的方法

Method: 采用序列约束方法：先估计仅含线性项的CNN模型（等价于线性参数多项式logit模型），然后约束需要解释的参数值，引入高阶项或Transformer等先进架构

Result: 在真实世界的新加坡公交智能卡数据上验证，该方法在保持选定参数可解释性的同时，显著提高了模型准确性

Conclusion: 展示了理论驱动的离散选择模型与数据驱动的AI模型相结合的统一方法潜力，可在保持规划政策应用适用性的同时实现更准确建模

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [58] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种结合数字孪生和零知识联邦学习的新型框架，用于优化无人机辅助联邦学习系统的能源效率、通信安全和资源管理


<details>
  <summary>Details</summary>
Motivation: 解决无人机辅助联邦学习系统中存在的能源消耗过高、通信效率低下和安全漏洞等问题，确保系统的可靠运行

Method: 集成数字孪生技术实现实时系统监控和预测性维护，采用零知识证明技术增强安全性，引入动态分配策略优化无人机飞行路径、传输功率和处理速率

Result: 系统能耗相比传统联邦学习方法降低29.6%，学习性能、安全性和可扩展性均得到显著提升

Conclusion: 该框架为下一代无人机智能网络提供了一个有前景的解决方案，在能源效率、安全性和性能方面都表现出色

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [59] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 将多模态生理信号（PPG、GSR、ACC）转换为2D图像矩阵，利用CNN进行压力检测的新方法，通过信号融合和图像化表示提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理生理信号或依赖固定编码，无法有效捕捉时间跨度和信号间依赖关系，需要一种能更好利用CNN优势的多模态信号处理方法。

Method: 将多模态生理信号融合成结构化图像表示，系统性地重新组织信号为多种格式，采用多阶段训练流程，利用图像化转换作为数据增强手段。

Result: 该方法显著提升了分类性能，改善了模型泛化能力和鲁棒性，同时增强了结果的可解释性。

Conclusion: 提出的图像化转换方法不仅适用于压力检测，可广泛应用于任何涉及多模态生理信号的领域，为通过可穿戴技术实现更准确、个性化和实时的健康监测铺平道路。

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [60] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个将交错图像-文本生成重构为工具使用问题的灵活框架，通过强化学习训练LLM智能协调多种视觉工具，在多个基准测试中大幅超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决当前统一模型受限于合成图像、难以处理需要事实基础或程序化精度的任务的"单一工具"瓶颈问题

Method: 设计强化学习框架训练中心LLM/MLLM代理智能协调专业视觉工具（在线图像搜索、扩散生成、代码执行、图像编辑），采用结合规则逻辑和LLM/MLLM评估的混合奖励系统

Result: 在四个基准测试中大幅超越现有方法，达到最先进性能，并引入新的测试时扩展策略获得进一步性能提升

Conclusion: LLM-I框架通过工具使用范式有效解决了交错图像-文本生成的局限性，展示了在多工具协调方面的强大能力

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [61] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 该论文提出了可控帕累托权衡(CPT)方法，通过多目标优化实现公平性与准确性之间的可控权衡，使用移动平均梯度稳定性和关键参数梯度剪枝技术，在仇恨言论检测和职业分类任务中取得了优于基线方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前NLP任务中的公平性-准确性权衡研究主要寻找单一"最优"解决方案，但帕累托前沿存在多种可能解。本文旨在根据用户偏好提供可控的权衡方案。

Method: 采用多目标优化(MOO)方法，提出CPT框架：1)使用随机梯度的移动平均来稳定公平性更新方向；2)通过仅保留关键参数的梯度进行梯度剪枝。

Result: 在仇恨言论检测和职业分类任务上的实验表明，CPT能够在帕累托前沿获得比基线方法更高质量的解决方案集合，并展现出更好的可控性，能够精确遵循人工定义的参考向量。

Conclusion: CPT方法有效解决了公平性-准确性权衡的可控性问题，通过梯度稳定和剪枝技术实现了对用户偏好的精确响应，为多目标优化在NLP公平性研究中的应用提供了新思路。

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [62] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM是一个新颖的无线信道建模框架，通过辐射场联合表示大尺度信号衰减和多径分量，解决了传统局部统计信道建模在多频段、多小区场景下的局限性，显著提升了信道预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统局部统计信道建模方法局限于单小区、单网格和单载频分析，无法捕捉复杂的跨域交互，限制了蜂窝网络优化的效果。

Method: 提出RF-LSCM框架，采用物理信息频率相关衰减模型实现跨频段泛化，结合点云辅助环境增强方法实现多小区多网格建模，并利用低秩张量表示和分层张量角度建模算法提高计算效率。

Result: 在真实多小区数据集上的实验表明，RF-LSCM显著优于现有方法，覆盖预测的平均绝对误差降低30%，通过有效融合多频数据实现22%的MAE改进。

Conclusion: RF-LSCM通过创新的辐射场建模方法，成功解决了传统信道建模的局限性，为蜂窝网络优化提供了更准确和高效的信道建模解决方案。

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [63] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文提出了一个基于共形预测的分布无关不确定性量化框架，为物理信息神经网络(PINNs)提供严格的统计保证和空间自适应不确定性区间


<details>
  <summary>Details</summary>
Motivation: 现有的PINNs不确定性量化方法缺乏严格的统计保证，需要一种能够提供有限样本覆盖保证的分布无关方法

Method: 引入分布无关的共形预测框架，通过在校准集上构建非共形性分数来校准预测区间，并进一步提出局部共形分位数估计来处理空间异方差性

Result: 在典型PDE系统上的系统评估显示，该框架实现了可靠的校准和局部自适应不确定性区间，在多个不确定性指标上一致优于启发式UQ方法

Conclusion: 这项工作通过将PINNs与分布无关UQ相结合，不仅提高了校准和可靠性，还为复杂PDE系统的不确定性感知建模开辟了新途径

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [64] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 通过智能手表系统和心率数据预训练，组合特质测量的元学习方法能够在社交焦虑患者中达到60.4%的平衡准确率来预测时刻焦虑波动，超过以往方法至7%。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑导致学术、社交和职业功能障碍，但之前少有研究测量或预测日常生活中的时刻焦虑波动。捐捕这些动态变化对设计实时个性化干预措施至关重要。

Method: 使用自定制智能手表系统收集91名社交焦虑大学生的数据，每天进行7次生态石石评估。基于外部10,000天心率数据建立基础模型，转移表征后细调生成概率预测，与特质测量组合构建元学习器。

Result: 在本研究数据集上达到60.4%的平衡准确率，在TILES-18数据集的10,095个日常评估中达到59.1%的平衡准确率，超过以往方法至7%。

Conclusion: 这种结合可执行数据流、跨数据集预训练和特质测量的元学习方法，能够有效预测社交焦虑患者的时刻焦虑波动，为实时个性化干预提供了技术支持。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [65] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了DirGraphSSM，首个将状态空间模型扩展到有向图学习的创新方法，通过k-hop ego图序列化和消息传递机制，在保持高效训练的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN和图Transformer在处理有向图时面临两个主要挑战：难以有效捕捉长距离因果依赖关系，以及在处理大规模图数据时难以平衡准确性和训练效率。现有的图状态空间模型仅适用于无向图，限制了其性能。

Method: 提出DirEgo2Token方法通过k-hop ego图将有向图序列化，并在此基础上开发DirGraphSSM架构，通过消息传递机制在有向图上实现状态空间模型。

Result: 在三个代表性有向图学习任务上达到最先进性能，在另外两个任务上获得竞争性性能，训练速度比现有最先进模型快1.5到2倍。

Conclusion: 这是状态空间模型在有向图学习领域的首次系统性扩展，证明了该方法在保持高效训练的同时能够显著提升有向图学习的性能。

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [66] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis是一个并行保护框架，通过模型分区策略在联邦学习中实现隐私-效用-效率的灵活平衡，使用差分隐私保护低重要性部分，同态加密保护关键部分，并通过分布式投票机制达成共识。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习保护机制（如差分隐私和同态加密）存在刚性权衡，必须在模型效用和计算效率之间做出选择，缺乏灵活性，阻碍了实际应用。

Method: 提出ParaAegis框架，采用策略性模型分区方案：对模型中重要性较低的低范数部分应用轻量级差分隐私，对剩余部分使用同态加密保护，并通过分布式投票机制实现分区共识。

Result: 理论分析确认了在相同隐私保护水平下效率与效用之间的可调节性。实验结果表明，通过调整超参数，该方法能够灵活地在模型准确性和训练时间之间进行优先排序。

Conclusion: ParaAegis为解决联邦学习中隐私保护与性能平衡的困境提供了灵活可调的解决方案，使实践者能够根据具体需求在隐私、效用和效率之间做出最优权衡。

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [67] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK是一个增强大语言模型捕捉时空依赖性的新框架，通过空间增强注意力和记忆检索前馈网络解决LLM在交通预测中的空间建模限制


<details>
  <summary>Details</summary>
Motivation: 大语言模型在交通预测中表现出潜力，但其主要为序列标记处理设计，难以有效捕捉空间依赖关系，特别是在处理图结构空间数据方面存在架构不兼容问题

Method: 提出ST-LINK框架，包含两个关键组件：空间增强注意力（SE-Attention）将空间相关性作为旋转变换集成到注意力机制中；记忆检索前馈网络（MRFFN）动态检索和利用历史模式来捕捉复杂时间依赖性

Result: 在基准数据集上的综合实验表明，ST-LINK超越了传统的深度学习和LLM方法，能够有效捕捉常规交通模式和突变变化

Conclusion: ST-LINK成功解决了LLM在交通预测中的空间建模限制，通过创新的空间增强和动态记忆检索机制，显著提升了时空依赖性的捕捉能力

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [68] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 本文从因果视角分析多视图无监督特征选择(MUFS)，提出CAUSA方法通过因果正则化模块分离混淆变量并平衡分布，以消除伪相关，选择因果信息特征。


<details>
  <summary>Details</summary>
Motivation: 现有MUFS方法通过特征与聚类标签的相关性来选择判别特征，但忽略了混淆变量导致的伪相关性可能选择不相关特征的问题。

Method: 提出CAUSA方法：1)使用广义无监督谱回归模型捕获特征与共识聚类标签的依赖关系；2)引入因果正则化模块自适应分离多视图数据中的混淆变量，并学习视图共享样本权重来平衡混淆变量分布。

Result: 综合实验表明CAUSA优于多个最先进方法，是首个在无监督设置下深入研究因果多视图特征选择的工作。

Conclusion: 从因果视角分析MUFS问题，提出的CAUSA方法能有效缓解伪相关性，选择更具因果信息的特征，为无监督多视图特征选择提供了新的因果分析框架。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [69] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: 提出了FHNN框架，通过物理结构化的神经网络预测可解释的水动力参数，结合解析运动方程，显著提升流体-结构相互作用建模的精度和稳定性


<details>
  <summary>Details</summary>
Motivation: 传统黑盒神经网络模型在流体-结构相互作用建模中存在可解释性差和长期预测不稳定的问题，需要一种既能保持物理一致性又能处理耗散动力学的方法

Method: 开发Floating-Body Hydrodynamic Neural Networks (FHNN)，预测方向性附加质量、阻力系数和基于流函数的流场等可解释水动力参数，并与解析运动方程耦合

Result: 在合成涡流数据集上，FHNN比Neural ODEs误差降低一个数量级，恢复物理一致的流场，相比哈密顿和拉格朗日神经网络能更有效处理耗散动力学

Conclusion: FHNN填补了黑盒学习与透明系统识别之间的空白，通过物理结构化设计约束假设空间，增强可解释性并稳定积分过程

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [70] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: 提出了通用物理变换器(GPhyT)，这是一个基于Transformer的物理基础模型，能够在多个物理领域实现零样本泛化和稳定长期预测，无需重新训练即可适应不同物理系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的机器学习方法局限于单一狭窄领域，需要为每个新系统重新训练。物理基础模型(PFM)可以 democratize 高保真模拟访问，加速科学发现，消除专业求解器开发需求。

Method: 使用Transformer架构，在1.8TB多样化模拟数据上训练，通过上下文学习从数据中推断控制动力学，无需告知底层方程。

Result: GPhyT在多个物理领域表现优异，比专业架构性能提升高达29倍；通过上下文学习实现零样本泛化到全新物理系统；能够进行50个时间步的稳定长期预测。

Conclusion: 这项工作证明单个模型可以从数据中学习可泛化的物理原理，为通向可能改变计算科学与工程的通用物理基础模型开辟了道路。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [71] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: 该研究提出了一种混合量子-经典工作流，用于解决普惠金融中少样本信用风险评估问题，通过量子神经网络在数据稀缺场景下取得了优于经典方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决普惠金融中由于数据稀缺和不平衡导致的信用风险评估难题，传统方法在此类场景下效果有限，而量子机器学习为复杂金融问题提供了新的解决范式。

Method: 设计混合量子-经典工作流：首先使用经典机器学习模型（逻辑回归、随机森林、XGBoost）进行智能特征工程和降维，然后使用通过参数偏移规则训练的量子神经网络作为核心分类器。

Result: 在279个真实信用数据样本上，量子神经网络在模拟中达到0.852±0.027的平均AUC，在Quafu量子云平台的ScQ-P21超导处理器上硬件实验获得0.88的AUC，性能超越了一系列经典基准方法。

Conclusion: 该研究为NISQ时代量子计算在数据受限金融场景中的应用提供了实用蓝图，并为量子计算在高风险应用（如普惠金融）中的潜力提供了有价值的实证证据。

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [72] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出了一种端到端可微分的混合框架，将图神经网络嵌入孔隙网络模型中，用于多孔介质渗透率预测，避免了传统方法的理想化几何假设，同时保持了物理基础。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动模型缺乏跨尺度泛化能力且不包含物理约束，而传统孔隙网络模型依赖理想化几何假设限制了在复杂结构中的准确性，需要结合两者优势的新方法。

Method: 用图神经网络替代孔隙网络模型中的解析公式进行水力传导度预测，通过自动微分和离散伴随方法实现端到端训练，仅需单一渗透率标量作为训练目标。

Result: 该模型实现了高精度和良好的跨尺度泛化能力，优于纯数据驱动和传统孔隙网络方法，梯度敏感性分析显示物理一致的特征影响。

Conclusion: 该方法为复杂多孔介质渗透率预测提供了可扩展且物理信息丰富的框架，降低了模型不确定性并提高了准确性。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [73] [Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets](https://arxiv.org/abs/2010.01052)
*Jaume Banus,Maxime Sermesant,Oscar Camara,Marco Lorenzi*

Main category: cs.LG

TL;DR: 提出概率框架联合心脏数据插补和心血管机制模型个性化，用于脑研究中不完整心脏数据的处理


<details>
  <summary>Details</summary>
Motivation: 临床研究中机制模型应用受限，因为缺乏多模态患者数据。神经影像数据集无法充分代表心脏特征来建模脑疾病中的心血管因素

Method: 基于变分框架联合推断：从可用特征插补心脏信息的模型 + 高斯过程模拟器（能忠实再现个性化心血管动力学）

Result: 在UK Biobank上实验显示，模型能准确插补仅含收缩压和舒张压等最少心脏信息数据集中的缺失心脏特征，同时估计集总模型的模拟参数

Conclusion: 通过模拟不同脑解剖条件下的真实心脏动力学，为探索心脑联合关系提供了新途径

Abstract: The use of mechanistic models in clinical studies is limited by the lack of
multi-modal patients data representing different anatomical and physiological
processes. For example, neuroimaging datasets do not provide a sufficient
representation of heart features for the modeling of cardiovascular factors in
brain disorders. To tackle this problem we introduce a probabilistic framework
for joint cardiac data imputation and personalisation of cardiovascular
mechanistic models, with application to brain studies with incomplete heart
data. Our approach is based on a variational framework for the joint inference
of an imputation model of cardiac information from the available features,
along with a Gaussian Process emulator that can faithfully reproduce
personalised cardiovascular dynamics. Experimental results on UK Biobank show
that our model allows accurate imputation of missing cardiac features in
datasets containing minimal heart information, e.g. systolic and diastolic
blood pressures only, while jointly estimating the emulated parameters of the
lumped model. This allows a novel exploration of the heart-brain joint
relationship through simulation of realistic cardiac dynamics corresponding to
different conditions of brain anatomy.

</details>


### [74] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 提出了一种在分布式环境下基于图正则化的高斯混合模型学习方法，利用相似性图指导节点间参数共享，避免原始数据传输，在异构小样本场景中优于集中式和本地训练方法


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中数据异构且样本有限的情况下，如何有效学习高斯混合模型而不需要传输原始数据的问题

Method: 使用图正则化方法，通过提供的相似性图来指导节点间的参数共享，实现灵活的邻居参数聚合

Result: 该方法在异构、小样本场景下表现优于集中式训练和本地训练的GMM模型

Conclusion: 图正则化的分布式GMM学习方法能够有效处理数据异构性和样本限制问题，提供了一种隐私保护的参数共享方案

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [75] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，将掩码扩散模型解释为离散最优传输中的能量最小化问题，证明了三种能量公式的数学等价性，并通过Beta分布参数化插值调度，实现了高效的采样改进。


<details>
  <summary>Details</summary>
Motivation: 统一掩码扩散模型的理论基础，澄清其数学本质，并为实际采样提供理论指导，特别是在低步数采样设置中提升性能。

Method: 通过数学证明三种能量公式（动能、条件动能和测地能量）在MDMs结构下的等价性；使用Beta分布参数化插值调度，将调度设计空间简化为2D搜索；进行后训练调优而不修改模型。

Result: 实验证明，基于能量启发的调度在合成和真实世界基准测试中优于手工设计的基线，特别是在低步数采样设置中表现突出。

Conclusion: 该框架不仅统一了MDMs的理论基础，还提供了实用的调度优化方法，显著提升了采样效率，特别是在资源受限的低步数场景中。

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [76] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG是一种基于随机采样的历史感知漂移对齐方法，通过维护每个客户端的漂移记忆和基于参与率的门控机制，有效解决联邦学习中的非IID数据和部分参与问题，提升收敛速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非IID数据和部分客户端参与会导致客户端漂移和局部最优不一致，造成收敛不稳定和准确率下降。需要一种能够有效对齐本地和全局模型的方法。

Method: FedSSG维护每个客户端的漂移记忆，积累本地模型差异作为历史梯度的轻量级草图。通过基于观察/预期参与比的门控函数来控制记忆更新和本地对齐项，在采样噪声主导时保持弱平滑，在统计稳定后增强对齐。

Result: 在CIFAR-10/100数据集上，100/500个客户端，2-15%参与率的情况下，FedSSG相比基线方法提升测试准确率约0.9点(CIFAR-10)和2.7点(CIFAR-100)，收敛速度提升约4.5倍。

Conclusion: FedSSG证明采样统计可以转化为原则性的历史感知相位控制，稳定并加速联邦训练，仅需O(d)客户端内存和常数时间门控，在近IID或均匀采样情况下优雅退化为温和正则化器。

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [77] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter是一个轻量级适配器，无需微调即可为时间序列基础模型添加协变量信息，通过两阶段方法结合伪预测和TSFM预测，在真实数据集上比基础模型提升24-27%


<details>
  <summary>Details</summary>
Motivation: 大多数时间序列基础模型无法利用协变量信息，因为协变量具有领域特定性且缺乏相关归纳偏置，这限制了其在许多实际应用中的准确性

Method: 使用两阶段方法：1) 用简单回归模型生成伪预测；2) 训练高斯过程回归器，结合伪预测、TSFM预测和协变量来优化预测结果

Result: 在真实数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，相比基础基础模型提升24-27%，且数据和计算开销最小

Conclusion: 轻量级适配器有潜力弥合通用基础模型与领域特定预测需求之间的差距

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [78] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx是首个显式建模交叉公平性的框架，通过多目标优化处理多个敏感属性的组合偏差，在保持准确性的同时显著减少公平性违规


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法只处理单一敏感属性，无法捕捉交叉子群体面临的复杂、乘性偏差，需要解决交叉公平性这一关键问题

Method: APFEx框架包含三个创新：自适应多目标优化器动态切换策略、可微交叉公平性指标支持梯度优化、理论保证收敛到帕累托最优解

Result: 在四个真实数据集上的实验表明，APFEx在保持竞争力的准确性的同时，显著减少了公平性违规，优于现有方法

Conclusion: APFEx填补了公平机器学习的重要空白，为交叉公平性提供了可扩展、模型无关的解决方案

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [79] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: 通过简单的置信度加权平均方法，无需重新训练即可组合多个最先进的深度学习模型，在车辆轨迹预测任务中实现了10%的性能提升，特别是在长尾指标上。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶领域不断涌现更大更强的预测模型，一个重要挑战是如何在不进行昂贵重新训练的情况下组合这些大型模型的优势。

Method: 使用置信度加权平均方法，直接组合现成的最先进深度学习模型（无需重新训练或微调）。

Result: 在NuScenes和Argoverse数据集上，该方法比最佳单一模型性能提升10%，且在整个数据分布范围内都有改善，特别是在长尾指标上表现突出。

Conclusion: 简单的模型组合方法可以显著提升轨迹预测性能，证明了集成学习在自动驾驶预测任务中的有效性，且代码已开源。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [80] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: 提出WILF-Q方法解决无线联邦学习中的客户端选择问题，通过Q学习自适应学习Whittle指数来选择最优客户端，显著提升学习效率


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中客户端动态状态变化影响计算和通信效率，传统方法需要显式了解客户端状态转移或数据分布，不适用于实际部署

Method: 将客户端选择建模为多臂老虎机问题，使用Q学习自适应学习和更新每个客户端的近似Whittle指数，选择指数最高的客户端

Result: 实验结果表明WILF-Q在学学效率方面显著优于现有基线策略

Conclusion: WILF-Q为无线联邦学习中的客户端选择提供了鲁棒且高效的方法，无需客户端状态转移或数据分布的显式知识

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [81] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN是一种扩展物理信息神经网络框架，用于处理多裂纹断裂力学问题，通过能量损失函数、定制积分方案和域分解方法，结合XFEM思想在神经网络解空间中引入特殊函数来捕捉裂纹不连续性和奇异性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多裂纹断裂力学问题时面临挑战，需要开发一个能够有效捕捉裂纹不连续性和奇异性的稳健框架。

Method: 提出基于能量的损失函数、定制积分方案和域分解程序，受XFEM启发在神经网络解空间中用特殊函数丰富解空间，使用不同神经网络分别建模标准和增强解分量。

Result: 数值实验验证了该方法在1D和2D多裂纹问题中的有效性和稳健性，并具有良好的3D问题扩展性。

Conclusion: X-PINN框架为复杂多裂纹断裂力学问题提供了灵活有效的模拟解决方案，具有处理裂纹不连续性和奇异性的强大能力。

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [82] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART是一个用于癫痫发作检测的持续学习框架，通过选择性保留高熵和癫痫预测样本，在最小内存和计算需求下实现个性化适应，在CHB-MIT数据集上F1分数提升21%


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断依赖专家分析脑电图，过程耗时且需要专业知识。现有静态深度学习模型存在灾难性遗忘问题，无法适应患者脑电图信号的动态变化

Method: 提出EpiSMART持续学习框架，使用大小受限的重放缓冲区和信息样本选择策略，选择性保留高熵和癫痫预测样本，逐步适应患者特定的脑电图信号

Result: 在CHB-MIT数据集验证显示，相比无更新的基线模型，F1分数提升21%。平均每天仅需6.46分钟标记数据和6.28次更新，适合可穿戴系统实时部署

Conclusion: EpiSMART能够在资源受限的现实条件下，有效整合新数据而不破坏过去知识，实现稳健的个性化癫痫发作检测，推动可穿戴医疗系统的实际应用

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [83] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 提出基于环境传感器融合的轻量级蜂王检测系统，使用温度、湿度和压力差数据，在STM32微控制器上实现实时低功耗边缘计算，准确率超过99%


<details>
  <summary>Details</summary>
Motivation: 传统蜂王监测方法依赖人工检查，劳动密集且干扰蜂群；现有音频方法功耗高、预处理复杂且易受环境噪声影响，需要更可持续的解决方案

Method: 采用环境传感器融合技术（温度、湿度、内外压力差），在商用STM32微控制器上实现量化决策树推理，进行实时低功耗边缘计算

Result: 系统仅使用环境输入就实现了超过99%的蜂王检测准确率，音频特征未带来显著性能提升

Conclusion: 该工作提供了一个可扩展、可持续的非侵入式蜂巢监测解决方案，为使用现成节能硬件实现自主精准养蜂铺平了道路

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [84] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 提出基于动态图回归的GNSS干扰抑制方法，使用异构图卷积LSTM网络实时预测并校正接收机水平偏差，在多种干扰场景下显著优于传统时间序列基线模型


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统(GNSS)日益受到故意干扰的影响，在需要精确定位和授时的关键时刻导致系统失效，需要开发实时干扰抑制技术

Method: 将卫星接收环境建模为异构星形图（接收机为中心，卫星为叶节点），使用单层异构图卷积LSTM网络聚合空间上下文和时间动态信息，实时预测2D偏差向量进行校正

Result: 在-45dBm强干扰下达到3.64-7.74cm的MAE，在-60至-70dBm时改善至1.65-2.08cm，混合模式下MAE为3.78-4.25cm，数据效率优异（仅10%训练数据仍优于基线20cm vs 36-42cm）

Conclusion: 所提出的接收机中心深度时序图网络能有效抑制GNSS干扰，在各种干扰类型和功率水平下均表现出优越性能，且具有出色的数据效率

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [85] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO是一个离线强化学习框架，通过树形轨迹表示和过程奖励模型解决Web Agent训练中的信用分配、标注成本和奖励稀疏问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，Web Agent自动化交互变得重要，但强化学习训练面临信用分配不当、标注成本高和奖励稀疏等挑战。

Method: 提出树形轨迹表示合并语义相同状态消除标签冲突，包含过程奖励模型通过子目标进度、冗余检测和动作验证自动生成细粒度奖励，采用动态权重机制优先处理高影响力决策点。

Result: 在Online-Mind2Web和自建C-WebShop数据集上的实验表明，TGPO显著优于现有方法，以更少冗余步骤实现更高成功率。

Conclusion: TGPO框架有效解决了Web Agent训练的关键问题，为自动化网页交互提供了高效的离线强化学习解决方案。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [86] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 提出基于联邦学习和差分隐私的本地化流行病预测方法，在保护隐私的同时实现有效的县级COVID-19病例预测


<details>
  <summary>Details</summary>
Motivation: 在流行病爆发时需要快速反应，但本地训练机器学习模型数据不足，集中数据又面临隐私问题，需要找到隐私保护与数据可用性的平衡

Method: 使用联邦学习框架，以县/社区为客户端，训练多层感知机预测病例数，采用客户端级差分隐私，只交换经过范数裁剪和噪声添加的模型更新

Result: 在适度隐私保护水平下，差分隐私模型接近非隐私模型性能：2020年11月R²=0.94（vs 0.95），MAPE=26%；2022年3月R²=0.88（vs 0.93），MAPE=21%

Conclusion: 客户端级差分隐私联邦学习能够在强隐私保证下提供有用的县级预测，可行的隐私预算取决于流行病阶段，支持卫生当局进行隐私合规的本地预测协作

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [87] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign是一个轻量级的即插即用框架，通过简单的重构任务学习辅助特征，解决时间序列预测中输入历史与未来目标之间的分布差异问题，显著提升各种基础预测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习等表示学习方法在时间序列预测中表现不佳，作者认为显式的表示对齐可以提供关键信息来弥合输入历史与未来目标之间的分布差距。

Method: 提出TimeAlign框架，通过重构任务学习辅助特征，并将这些特征反馈给任何基础预测器。该方法架构无关且计算开销极小。

Result: 在8个基准测试中验证了其优越性能，研究表明性能提升主要来自纠正历史输入与未来输出之间的频率不匹配问题，并提供了理论证明。

Conclusion: TimeAlign可以作为现代深度学习时间序列预测系统的通用对齐模块，具有架构无关性和可忽略的计算开销优势。

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [88] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: 本文提出了一种使用小波变换将纳米孔电流信号转换为尺度图图像，然后利用机器学习算法进行蛋白质分类的新方法，在42种肽的分类中达到了约81%的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发能够在临床环境中实时分类蛋白质的设备，实现廉价快速的疾病诊断。纳米孔设备通过测量蛋白质进入纳米孔时产生的电流信号来进行识别，但现有方法的信号复杂性限制了准确性。

Method: 将电流信号通过小波变换转换为尺度图图像，捕捉振幅、频率和时间信息，这种模态更适合机器学习算法。同时展示了模型迁移技术，为在实际硬件中部署模型铺平道路。

Result: 在42种肽的测试中，分类准确率达到约81%，创造了该领域的新最先进水平。

Conclusion: 该方法为在护理点实现实用的肽/蛋白质诊断迈出了重要一步，为实时疾病诊断开辟了新途径。

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [89] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一个开创性的Banach-Bregman随机优化框架，突破了传统Hilbert空间限制，在非欧几里得空间中实现了更快的收敛速度和更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化理论主要局限于Hilbert空间，无法有效处理镜像下降、Bregman近端方法、自然梯度下降等非欧几里得设置，需要建立更一般的Banach空间优化框架。

Method: 提出了统一的Banach-Bregman框架，通过Bregman投影和Bregman-Fejer单调性模板，涵盖随机近似、镜像下降、自然梯度、自适应方法和mirror-prox等多种方法，并建立了超松弛技术。

Result: 在机器学习、深度学习、强化学习和大型语言模型训练中实现了高达20%的收敛加速，降低了方差并提高了准确性，在合成和真实任务中验证了收敛定理。

Conclusion: Banach-Bregman几何成为统一优化理论和实践的核心基石，为下一代人工智能优化提供了理论基础和实用工具。

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [90] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 语言模型的激活值线性编码了信息在训练过程中被学习的时间顺序，模型能够区分不同时期学习的信息


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否以及如何编码信息被学习的时间顺序，这对于理解模型如何处理冲突数据和知识修改具有重要意义

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的命名实体数据集上，分析激活值的线性编码特性，使用线性探测和微调方法验证时间信号的提取

Result: 发现激活值中心点在2D子空间中按训练顺序直线排列，线性探测能准确区分早期和晚期实体（约90%准确率），微调后模型能报告未见实体的训练阶段（约80%准确率）

Conclusion: 语言模型确实能够按获取时间区分信息，这一发现对模型处理冲突数据和知识修改机制有重要启示

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [91] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究强化学习中的贝叶斯风险规避方法，通过BRMDP处理模型参数不确定性，推导了贝叶斯风险价值函数与真实价值函数之间的渐近正态性关系，并提出了基于后验采样的在线RL和CMAB算法，获得了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中由于数据不足导致的认知不确定性问题，通过贝叶斯风险规避方法来处理模型参数的不确定性，提高决策的鲁棒性。

Method: 采用贝叶斯风险马尔可夫决策过程(BRMDP)，推导价值函数的渐近正态性，提出基于后验采样的在线强化学习和上下文多臂老虎机算法。

Result: 贝叶斯风险规避方法会悲观地低估原始价值函数，这种差异随风险规避强度增加而增大，随数据量增加而减小。算法在在线RL和CMAB设置下都获得了次线性遗憾界。

Conclusion: 贝叶斯风险规避方法能有效处理认知不确定性，数值实验验证了理论性质的有效性，为风险敏感的强化学习提供了理论保证和实践方案。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [92] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 本研究比较了不同优化器和神经网络架构在EEG频段分类中的性能，发现Adagrad和RMSprop优化器表现最佳，CNN在空间特征提取方面表现出色，SHAP分析揭示了EEG频段对模型准确性的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同优化器和神经网络架构在EEG频段分类任务中的性能差异，以及如何有效预测左右半球的分类，为神经影像分类任务提供优化策略。

Method: 使用TensorFlow和PyTorch框架实现三种神经网络架构（深度密集网络、浅层三层网络和CNN），比较多种优化器（Adagrad、RMSprop、Adadelta、SGD、FTRL）在不同EEG频段的性能，并采用SHAP进行特征重要性分析。

Result: Adagrad在beta频段表现最佳，RMSprop在gamma频段表现最优；CNN获得第二高准确率，擅长捕捉EEG空间特征；深度密集网络在学习复杂模式方面具有竞争力；浅层网络计算效率高但准确率较低。

Conclusion: 优化器选择、模型架构和EEG频段分析对提升分类器性能至关重要，研究为神经影像分类任务的特征重要性和模型优化提供了重要见解。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [93] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 提出了Quantile Neural Basis Model，将分位数广义可加模型的解释性原理融入神经网络框架，在保持预测性能的同时提供模型行为的可解释性洞察。


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络在多水平概率预测中取得了高精度，但理解特征条件输出的底层机制仍然是一个重大挑战，需要提高模型的可解释性。

Method: 采用共享基分解和权重分解技术，结合分位数广义可加模型的解释性原则，构建端到端的神经网络训练框架，避免参数化分布假设。

Result: 在日前电价预测任务中验证了方法的有效性，预测性能与分布回归和分位数回归神经网络相当，同时通过学习到的输入特征到输出预测的非线性映射提供了有价值的模型行为洞察。

Conclusion: Quantile Neural Basis Model成功地将可解释性原理融入神经网络框架，在保持预测性能的同时提供了对模型行为的深入理解，为解决概率预测中的可解释性问题提供了有效途径。

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [94] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 该研究通过预测建模和实地试验，对高风险再监禁人群进行针对性心理健康干预，发现对最高风险群体的干预效果最显著。


<details>
  <summary>Details</summary>
Motivation: 监狱系统难以有效处理囚犯的心理健康、药物依赖和无家可归等复杂问题，导致再犯罪和监禁循环，特别是加剧了种族不平等。需要创新方法来打破这一循环。

Method: 采用预测建模方法识别高风险再监禁人群，并通过实地试验进行针对性心理健康外展干预，评估干预效果和最优风险阈值。

Result: 模型预测准确性高，最高风险群体中超过一半在一年内再次入狱。干预对最高风险人群效果最显著，改善了心理健康服务使用、急救调度和司法参与情况。

Conclusion: 针对性心理健康外展干预对最高再监禁风险人群最为有效，为打破监禁循环提供了数据支持的有效策略。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [95] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: 该论文研究了一种组合式核岭回归方法，通过坐标重加权进行特征学习，证明了在噪声变量为高斯分布时，全局最小值和驻点都能有效消除噪声坐标，并发现拉普拉斯核等ℓ₁型核能恢复非线性特征，而高斯核只能恢复线性特征。


<details>
  <summary>Details</summary>
Motivation: 研究组合式核岭回归作为特征学习的简单测试平台，探索如何通过坐标重加权从输入中恢复相关变量并消除噪声变量。

Method: 采用变分问题框架的组合式核岭回归方法，分析全局最小值和驻点的性质，比较ℓ₁型核（如拉普拉斯核）和高斯核在特征恢复方面的表现。

Result: 证明了当噪声变量为高斯分布时，全局最小值和驻点都能成功消除噪声坐标；发现ℓ₁型核能恢复非线性效应的特征，而高斯核只能恢复线性特征。

Conclusion: 组合式核岭回归为特征学习提供了有效的测试平台，ℓ₁型核在恢复非线性特征方面优于高斯核，这为特征选择和非线性建模提供了重要见解。

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [96] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 提出了一个端到端框架，通过AI架构和新型数据生成策略PCDS，从稀疏的常规数据中非侵入性地估计青光眼治疗中无法测量的关键参数（如小梁网渗透性），解决了缺乏真实数据和计算成本高的逆问题。


<details>
  <summary>Details</summary>
Motivation: 青光眼治疗中关键参数（如眼内压的主要决定因素小梁网渗透性）无法在体内测量，临床医生只能依赖间接替代指标，同时缺乏真实数据和计算成本高昂阻碍了预测模型的发展。

Method: 结合多阶段AI架构功能分离问题、新型PCDS数据生成策略（将计算时间从数年缩短到数小时）、贝叶斯引擎量化预测不确定性，从常规输入中解构单个IOP测量值。

Result: 非侵入性估计的流出设施与最先进的眼压测量法高度一致，精度堪比直接物理仪器；新推导的渗透性生物标志物在按疾病风险分层临床队列方面表现出高准确性。

Conclusion: 该框架为其他数据稀缺、计算密集型领域的类似逆问题提供了可推广的解决方案蓝图，具有重要的诊断潜力。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [97] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing是一个端到端框架，通过图算法和LLM代理实现电路理解，并将知识整合到贝叶斯优化中，提高模拟电路设计的效率和可行性。


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号电路设计面临高质量数据短缺和领域知识难以融入自动化流程的挑战。传统黑盒优化缺乏电路理解，而学习型方法成本高且需要重新训练。

Method: 首先使用图算法将电路组织为层次化设备-模块-阶段表示，然后LLM代理执行假设-验证-精炼循环进行标注，最后将验证的见解通过LLM引导的初始采样和信任区域更新整合到贝叶斯优化中。

Result: 该方法提高了优化效率，同时保持了可行性，减少了低价值设计空间的评估浪费。

Conclusion: TopoSizing框架通过结合图算法和LLM代理，实现了从原始网表到优化增益的稳健电路理解，为模拟电路设计提供了更高效和透明的自动化解决方案。

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [98] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出了一个统一的变分框架来形式化基于残差的自适应策略，通过整合残差的凸变换将离散化选择与误差度量直接关联，系统化设计自适应方案并提升性能


<details>
  <summary>Details</summary>
Motivation: 基于残差的自适应策略在科学机器学习中广泛应用但缺乏理论依据，需要建立统一的理论框架来形式化这些方法

Method: 引入变分框架，通过残差的凸变换（指数权重对应最小化均匀误差，线性权重对应最小化二次误差）将自适应加权等价于优化原始目标的采样分布选择

Result: 框架实现了三个好处：系统化设计跨范数的自适应方案、通过损失估计器方差减少降低离散化误差、通过改善梯度信噪比增强学习动态，在算子学习中展示了显著的性能提升

Conclusion: 为基于残差的自适应性提供了理论依据，为有原则的离散化和训练策略奠定了基础

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [99] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: RKTV-INR是一个基于龙格-库塔积分和全变分的隐式神经表示去噪框架，用于处理非线性动力系统中的测量噪声问题，通过约束确保重构状态接近原始数据并提供准确的导数估计，最终支持SINDy方法恢复系统控制方程。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的数据驱动建模经常受到测量噪声的影响，传统方法难以有效处理噪声问题并准确估计系统导数，这限制了系统辨识的精度。

Method: 提出RKTV-INR框架，使用隐式神经表示直接拟合噪声观测数据，通过龙格-库塔积分和全变分约束确保重构状态是动力系统的轨迹并保持接近原始数据，利用自动微分获得准确的一阶导数，最后结合SINDy方法恢复控制方程。

Result: 实验证明该方法能有效抑制噪声、精确估计导数，并实现可靠的系统辨识。

Conclusion: RKTV-INR框架成功解决了非线性动力系统建模中的噪声问题，提供了连续清洁的轨迹和准确的导数估计，为系统辨识提供了有效工具。

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [100] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 本文提出使用临界阻尼高阶朗之万动力学来防御扩散模型中的成员推理攻击，通过引入辅助变量和联合扩散过程来混合外部随机性，从而在扩散过程早期破坏敏感输入数据。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用的快速发展带来了新的数据安全问题，扩散模型虽然比其他生成模型对成员推理攻击更具抵抗力，但仍然存在脆弱性，需要有效的防御机制。

Method: 采用临界阻尼高阶朗之万动力学，引入多个辅助变量和联合扩散过程，利用辅助变量混合外部随机性来在扩散过程早期破坏敏感输入数据。

Result: 在玩具数据集和语音数据集上进行了理论分析和实验验证，使用AUROC曲线和FID指标证明了防御方法的有效性。

Conclusion: 提出的基于高阶朗之万动力学的防御方法能够有效保护扩散模型免受成员推理攻击，为生成式AI的数据安全提供了新的解决方案。

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [101] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA是一种新颖的大语言模型结构化剪枝方法，通过神经正切核理论指导的显著性准则、自适应稀疏度分配机制和基于KL散度的校准数据选择策略，在保持零样本准确性的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型结构化剪枝方法存在显著的性能下降问题，特别是在零样本设置下，且需要昂贵的恢复技术如监督微调或适配器插入。

Method: 使用基于Adam优化动态下神经正切核的一阶显著性准则，结合跨层和模块的自适应稀疏度分配机制，以及基于KL散度的校准数据选择策略。

Result: 在Llama3、Qwen和T5模型上的综合实验表明，NIRVANA在同等稀疏度约束下优于现有结构化剪枝方法。

Conclusion: NIRVANA提供了一个理论上有依据且实用的LLM压缩方法，平衡了零样本准确性保持和微调能力。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [102] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: Compute as Teacher (CaT) 通过将模型在推理时的探索转化为无参考监督，利用并行rollout合成参考信号来优化模型性能，在可验证和不可验证任务中均取得显著提升


<details>
  <summary>Details</summary>
Motivation: 解决后训练阶段缺乏真实监督信号的问题，探索如何将推理时的计算资源转化为有效的学习信号

Method: 通过当前策略生成一组并行rollout，使用冻结的初始策略（anchor）来调和冲突和遗漏，合成单一参考信号；在可验证任务中使用程序等价性，在不可验证任务中使用自提议的评分标准并由独立LLM评判

Result: 在Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B上取得显著提升（MATH-500上最高+27%，HealthBench上+12%）；结合强化学习（CaT-RL）后进一步提升（最高+33%和+30%）

Conclusion: CaT方法成功将推理时计算转化为有效的教师信号，性能随rollout数量扩展，训练后的策略甚至能超越初始教师信号，为无监督学习提供了新思路

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>
