<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 6]
- [cs.IT](#cs.IT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Dynamic Cooperative Strategies in Search Engine Advertising Market: With and Without Retail Competition](https://arxiv.org/abs/2512.21501)
*Huiran Li,Qiucheng Li,Baozhu Feng*

Main category: cs.GT

TL;DR: 该研究为搜索引擎广告(SEA)市场中的合作广告提供管理指南，通过建模两种合作广告决策场景，分析制造商与零售商之间的渠道协调问题。


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的搜索引擎广告市场中，零售商与制造商之间的渠道协调对广告策略效果至关重要。研究旨在为SEA环境下的合作广告提供管理指导。

Method: 提出新颖的合作广告优化模型，考虑动态质量评分和有限时间范围。建模两种场景：场景I为单一制造商与单一零售商的简单合作渠道；场景II增加独立零售商与制造商-零售商联盟竞争。提供各场景的可行均衡解，并进行数值实验和敏感性分析。

Result: 为两种场景提供最优策略的可行均衡解，通过数值实验分析质量评分和毛利率的敏感性，探讨竞争零售商初始市场份额的影响，研究零售竞争对合作联盟最优策略和渠道绩效的影响。

Conclusion: 从均衡和数值分析中得出的特性为SEA市场中参与合作广告的各方提供了关键见解，有助于优化广告策略和渠道协调。

Abstract: In search engine advertising (SEA) market, where competition among retailers is intense and multifaceted, channel coordination between retailers and manufacturers emerges as a critical factor, which significantly influences the effectiveness of advertising strategies. This research attempts to provide managerial guidelines for cooperative advertising in the SEA context by modeling two cooperative advertising decision scenarios. Scenario I defines a simple cooperative channel consisting of one manufacturer and one retailer. In Scenario II, we consider a more general setting where there is an independent retailer who competes with the Manufacturer-Retailer alliance in Scenario I. We propose a novel cooperative advertising optimization model, wherein a manufacturer can advertise product directly through SEA campaigns and indirectly by subsidizing its retailer. To highlight the distinctive features of SEA, our model incorporates dynamic quality scores and focuses on a finite time horizon. In each scenario, we provide a feasible equilibrium solution of optimal policies for all members. Subsequently, we conduct numerical experiments to perform sensitivity analysis for both the quality score and gross margin. Additionally, we explore the impact of the initial market share of the competing retailer in Scenario II. Finally, we investigate how retail competition affects the cooperative alliance's optimal strategy and channel performance. Our identified properties derived from the equilibrium and numerical analyses offer crucial insights for participants engaged in cooperative advertising within the SEA market.

</details>


### [2] [EFX Allocations Exist on Triangle-Free Multi-Graphs](https://arxiv.org/abs/2512.21644)
*Mahyar Afshinmehr,Arash Ashuri,Pouria Mahmoudkhan,Kurt Mehlhorn*

Main category: cs.GT

TL;DR: 该论文证明了在多图估值中，当图不包含长度为3的环时，EFX分配总是存在，并提供了伪多项式时间算法来计算这种分配。


<details>
  <summary>Details</summary>
Motivation: EFX（消除任何单一物品后的无嫉妒）分配的存在性一直是公平分配领域的核心开放问题。虽然Christodoulou等人证明了简单图估值下的存在性，但对于多图估值（即一对代理之间可能有多个双方都重视的物品）的情况仍然开放。Sgouritsa和Sotiriou以及Afshinmehr等人的工作分别对特定图结构证明了存在性，但需要更强的条件。

Method: 作者研究了多图估值场景，其中物品对应图中的边，代理对应节点，每个代理只重视与其相邻的边。通过分析图结构特性，特别是环的存在性，证明了当图不包含长度为3的环时，EFX分配总是存在。对于单调估值，提供了伪多项式时间算法来计算这种分配，当代理具有可取消估值时，算法在多项式时间内运行。

Result: 1. 证明了在多图估值中，当图不包含长度为3的环时，EFX分配总是存在；2. 对于单调估值，提供了伪多项式时间算法来计算EFX分配；3. 当代理具有可取消估值时，算法在多项式时间内运行；4. 这是少数几个对于任意数量代理都能保证EFX分配存在的情况之一。

Conclusion: 该研究显著推进了EFX分配存在性的理论边界，将存在性条件从"无奇数环"或"最短非平行边环长度至少为6"放宽到"无长度为3的环"。同时提供了实际计算算法，使得理论结果具有实际应用价值，为公平分配领域做出了重要贡献。

Abstract: We study the fair allocation of indivisible goods among agents, with a focus on limiting envy. A central open question in this area is the existence of EFX allocations-allocations in which any envy of any agent i towards any agent j vanishes upon the removal of any single good from j's bundle. Establishing the existence of such allocations has proven notoriously difficult in general, but progress has been made for restricted valuation classes. Christodoulou et al. [2023] proved existence for graphical valuations, where goods correspond to edges in a graph, agents to nodes, and each agent values only incident edges. The graph was required to be simple, i.e., for any pair of agents, there could be at most one good that both agents value. The problem remained open, however, for multi-graph valuations, where for a pair of agents several goods may have value to both. In this setting, Sgouritsa and Sotiriou [2025] established existence whenever the shortest cycle with non-parallel edges has length at least six, while Afshinmehr et al. [2025] proved existence when the graph contains no odd cycles. In this paper, we strengthen these results by proving that EFX allocations always exist in multi-graphs that contain no cycle of length three. Assuming monotone valuations, we further provide a pseudo-polynomial time algorithm for computing such an allocation, which runs in polynomial time when agents have cancelable valuations, a strict superclass of additive valuation functions. Accordingly, our results stand as one of the only cases where EFX allocations exist for an arbitrary number of agents.

</details>


### [3] [Near-Optimal Coalition Structures in Polynomial Time](https://arxiv.org/abs/2512.21657)
*Angshul Majumdar*

Main category: cs.GT

TL;DR: 比较三种联盟结构生成算法范式：动态规划、MILP分支定界和稀疏松弛方法，在随机稀疏协同模型下，稀疏松弛方法能在多项式时间内以高概率获得接近最优解，而其他方法需要指数时间


<details>
  <summary>Details</summary>
Motivation: 研究经典联盟结构生成问题，比较不同算法范式的"随时"行为表现，探索在随机稀疏协同模型下哪种方法能更快获得高质量解

Method: 在随机稀疏协同模型下，分析三种算法范式：动态规划、MILP分支定界、基于贪婪或l1型方法的稀疏松弛方法，理论证明稀疏松弛方法的性能优势

Result: 稀疏松弛方法能在多项式时间内以高概率恢复接近最优的联盟结构，而动态规划和MILP算法需要指数时间才能达到类似解质量，建立了稀疏松弛方法的概率性随时性能优势

Conclusion: 尽管精确方法最终能达到最优，但在随机稀疏协同模型下，稀疏松弛方法在随时性能方面具有显著优势，能在多项式时间内获得接近最优解，为联盟结构生成问题提供了更实用的解决方案

Abstract: We study the classical coalition structure generation (CSG) problem and compare the anytime behavior of three algorithmic paradigms: dynamic programming (DP), MILP branch-and-bound, and sparse relaxations based on greedy or $l_1$-type methods. Under a simple random "sparse synergy" model for coalition values, we prove that sparse relaxations recover coalition structures whose welfare is arbitrarily close to optimal in polynomial time with high probability. In contrast, broad classes of DP and MILP algorithms require exponential time before attaining comparable solution quality. This establishes a rigorous probabilistic anytime separation in favor of sparse relaxations, even though exact methods remain ultimately optimal.

</details>


### [4] [Multi-agent Adaptive Mechanism Design](https://arxiv.org/abs/2512.21794)
*Qiushi Han,David Simchi-Levi,Renfei Tan,Zishuo Zhao*

Main category: cs.GT

TL;DR: 提出DRAM框架，结合机制设计和在线学习，在未知代理人信念下实现真实报告和成本最优，达到$\tilde{O}(\sqrt{T})$后悔率并证明其最优性。


<details>
  <summary>Details</summary>
Motivation: 在机制设计中，当委托人没有代理人信念的先验知识时，如何同时保证真实报告和成本最优是一个挑战。现有方法通常假设已知激励约束，但在实际中这些约束需要学习。

Method: 提出分布鲁棒自适应机制（DRAM）框架，结合机制设计和在线学习。通过估计代理人信念，迭代更新具有收缩模糊集的分布鲁棒线性规划，在保持真实性的同时减少支付。

Result: DRAM以高概率保证真实报告，实现$\tilde{O}(\sqrt{T})$累积后悔率。建立了匹配的下界证明没有真实自适应机制能渐近做得更好。框架支持插件估计器、结构化先验和延迟反馈。

Conclusion: 这是第一个在一般设置下保持真实性并实现最优后悔的自适应机制，解决了激励约束未知且需要学习的问题，为机制设计和在线学习的交叉领域提供了新框架。

Abstract: We study a sequential mechanism design problem in which a principal seeks to elicit truthful reports from multiple rational agents while starting with no prior knowledge of agents' beliefs. We introduce Distributionally Robust Adaptive Mechanism (DRAM), a general framework combining insights from both mechanism design and online learning to jointly address truthfulness and cost-optimality. Throughout the sequential game, the mechanism estimates agents' beliefs and iteratively updates a distributionally robust linear program with shrinking ambiguity sets to reduce payments while preserving truthfulness. Our mechanism guarantees truthful reporting with high probability while achieving $\tilde{O}(\sqrt{T})$ cumulative regret, and we establish a matching lower bound showing that no truthful adaptive mechanism can asymptotically do better. The framework generalizes to plug-in estimators, supporting structured priors and delayed feedback. To our knowledge, this is the first adaptive mechanism under general settings that maintains truthfulness and achieves optimal regret when incentive constraints are unknown and must be learned.

</details>


### [5] [Determining Blockchain Transaction Timing and Fee with Observable Mempools](https://arxiv.org/abs/2512.21923)
*Qianlan Bai,Yuedong Xu,Zhijian Zhou,Xin Wang*

Main category: cs.GT

TL;DR: 研究区块链中战略用户的交易策略，包括广播时间和手续费设置，考虑普通用户和半战略用户两种场景，分析不同共识机制下的最优策略。


<details>
  <summary>Details</summary>
Motivation: 区块链交易手续费影响交易处理优先级，战略用户可以通过观察内存池延迟广播和设置低手续费，但随机挖矿间隔可能导致交易错过区块。同时，手续费提升功能使费用设置更加复杂，需要研究最优交易策略。

Method: 从单个战略用户角度研究交易策略，考虑两种场景：1）普通用户（内存池不可知）按分布设置手续费；2）半战略用户按泊松速率检查内存池并更新手续费。前者计算适应任意挖矿间隔分布的最优广播时间和手续费；后者构建连续时间马尔可夫链描述内存池状态动态，推导最优手续费调整频率。

Result: 在比特币类PoW系统中（指数分布区块间隔），战略用户应在交易创建后立即广播；在以太坊类PoS系统中（固定区块间隔），用户应等到区块生成前最后一刻广播。理论分析和仿真表明，战略用户应在手续费落后于最低包含费用时立即提高手续费。

Conclusion: 区块链中战略用户的交易策略需要根据共识机制和用户类型动态调整。在PoW系统中立即广播是最优策略，而在PoS系统中延迟广播更有利。战略用户应实时监控内存池状态，及时调整手续费以确保交易被包含。

Abstract: Transaction fee plays an important role in determining the priority of transaction processing in public blockchain systems. Owing to the observability of unconfirmed transactions, a strategic user can postpone his transaction broadcasting time and set a fee as low as possible by prying into his mempool that stores them. However, the stochastic mining interval may cause the delayed transaction to miss the next valid block. Meanwhile, a new feature (i.e. fee bumping) emerges that allows each user to increase his transaction fee before confirmation, making the fee setting more challenging. In this paper, we investigate a novel transaction policy from the perspective of a single strategic user that determines the broadcasting time and the transaction fee simultaneously. Two representative scenarios are considered, in which a number of coexisting ordinary users are mempool-oblivious that set their fees according to certain distribution, and are semi-strategic that check their mempools at a Poisson rate and update their fees. In the former, we compute the optimal broadcasting time and transaction fee that adapts to the arbitrary distribution of mining interval. When the block interval is exponentially distributed in Bitcoin-like PoW systems, the strategic user needs to broadcast his transaction immediately after its creation. And when the block interval is fixed in Ethereum-like PoS systems, he finds it profitable to wait until the last moment before block generation. In the latter, we formulate a continuous-time Markov chain to characterize the dynamics of mempool states, and derive the optimal fee adjusting frequency of the strategic user when the block interval is exponentially distributed. In both theory and simulations, we show that this strategic user should immediately increase his fee whenever it falls behind the minimum fee of being included.

</details>


### [6] [Deep Learning Based Auction Design for Selling Agricultural Produce through Farmer Collectives to Maximize Nash Social Welfare](https://arxiv.org/abs/2512.22039)
*Mayank Ratan Bhardwaj,Vishisht Srihari Rao,Bazil Ahmed,Kartik Sagar,Y. Narahari*

Main category: cs.GT

TL;DR: 提出一个基于深度学习的农产品拍卖机制VDA-SAP，通过农民集体作为销售代理，采用数量折扣拍卖，旨在最大化纳什社会福利，同时满足激励相容、个体理性等性质。


<details>
  <summary>Details</summary>
Motivation: 设计一个稳健的市场机制，既惠及农民（农产品生产者），也惠及农产品购买者（消费者）。农民集体作为合作社可以发挥规模经济优势，但需要满足激励相容、个体理性等基本性质，以及农业场景中特别相关的纳什社会福利最大化。

Method: 提出VDA-SAP（农产品销售数量折扣拍卖）机制，农民集体作为销售代理，高销量或零售消费者作为购买代理。由于同时满足所有理想性质在理论上是不可行的，采用深度学习网络来学习拍卖机制，最小化对期望性质的违反。

Result: VDA-SAP在满足性质丰富性方面优于经典的VCG机制，并且在其他基准拍卖中也表现更优。在农民集体销售易腐蔬菜给潜在买家的现实场景中验证了结果。

Conclusion: 提出的深度学习驱动的拍卖机制VDA-SAP能够有效平衡农民和消费者的利益，满足农业市场的实际业务约束，在理论性质和实际应用方面都表现出优越性。

Abstract: This paper is motivated by the need to design a robust market mechanism to benefit farmers (producers of agricultural produce) as well as buyers of agricultural produce (consumers). Our proposal is a volume discount auction with a Farmer Collective (FC) as the selling agent and high volume or retail consumers as buying agents. An FC is a cooperative of farmers coming together to harness the power of aggregation and economies of scale. Our auction mechanism seeks to satisfy fundamental properties such as incentive compatibility and individual rationality, and an extremely relevant property for the agriculture setting, namely, Nash social welfare maximization. Besides satisfying these properties, our proposed auction mechanism also ensures that certain practical business constraints are met. Since an auction satisfying all of these properties exactly is a theoretical impossibility, we invoke the idea of designing deep learning networks that learn such an auction with minimal violation of the desired properties. The proposed auction, which we call VDA-SAP (Volume Discount Auction for Selling Agricultural Produce), is superior in many ways to the classical VCG (Vickrey-Clarke-Groves) mechanism in terms of richness of properties satisfied and further outperforms other baseline auctions as well. We demonstrate our results for a realistic setting of an FC selling perishable vegetables to potential buyers.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [7] [Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme](https://arxiv.org/abs/2512.21396)
*Doğukan Özbayrak,Ahmed Hareedy*

Main category: cs.IT

TL;DR: 提出基于设备状态的离线与在线学习方法，通过多项式拟合和线性规划优化，为TDMR系统实现自适应编码方案切换，以最大化存储容量并最小化解码复杂度。


<details>
  <summary>Details</summary>
Motivation: 现代存储系统在不同阶段需要不同级别的数据保护，传统基于时间戳的编码方案切换忽略了设备实际状态。需要基于设备状态的自适应编码重配置方法来延长设备寿命。

Method: 利用LOCO码的自然可重配置特性，提出离线学习和在线学习方法：1）离线学习使用整个时间跨度的训练数据；2）在线学习仅在特定时间间隔使用训练数据。通过多项式方程拟合比特错误率与TD密度的关系，设计线性规划优化问题来确定最优编码方案切换决策。

Result: 该方法可简化为线性规划问题，证明其解为全局最优。实验结果表明该方法在TDMR系统中能有效最大化存储容量和/或最小化解码复杂度。

Conclusion: 基于设备状态的自适应编码重配置方法优于传统基于时间戳的方法，能有效延长TDMR设备寿命，提高系统性能。

Abstract: In the age of data revolution, a modern storage~or transmission system typically requires different levels of protection. For example, the coding technique used to fortify data in a modern storage system when the device is fresh cannot be the same as that used when the device ages. Therefore, providing reconfigurable coding schemes and devising an effective way to perform this reconfiguration are key to extending the device lifetime. We focus on constrained coding schemes for the emerging two-dimensional magnetic recording (TDMR) technology. Recently, we have designed efficient lexicographically-ordered constrained (LOCO) coding schemes for various stages of the TDMR device lifetime, focusing on the elimination of isolation patterns, and demonstrated remarkable gains by using them. LOCO codes are naturally reconfigurable, and we exploit this feature in our work. Reconfiguration based on predetermined time stamps, which is what the industry adopts, neglects the actual device status. Instead, we propose offline and online learning methods to perform this task based on the device status. In offline learning, training data is assumed to be available throughout the time span of interest, while in online learning, we only use training data at specific time intervals to make consequential decisions. We fit the training data to polynomial equations that give the bit error rate in terms of TD density, then design an optimization problem in order to reach the optimal reconfiguration decisions to switch from a coding scheme to another. The objective is to maximize the storage capacity and/or minimize the decoding complexity. The problem reduces to a linear programming problem. We show that our solution is the global optimal based on problem characteristics, and we offer various experimental results that demonstrate the effectiveness of our approach in TDMR systems.

</details>


### [8] [Near-Field Communication with Massive Movable Antennas: An Electrostatic Equilibrium Perspective](https://arxiv.org/abs/2512.21660)
*Shicong Liu,Xianghao Yu,Shenghui Song,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 提出一种针对近场大规模MIMO系统的天线布局优化方法，将天线布局问题转化为加权Fekete问题，通过ODE框架和特征值分解高效求解，显著提升频谱效率。


<details>
  <summary>Details</summary>
Motivation: 现有天线布局方案在大规模天线系统中面临可扩展性有限和忽略近场效应的问题，限制了空间自由度的有效利用。

Method: 将天线布局问题在角度域重新表述为加权Fekete问题，揭示其本质为静电平衡问题；提出基于ODE的框架，通过多项式解的根表征最优天线位置，采用两步特征值分解高效求解。

Result: 仿真结果表明，该方案能有效利用近场信道的空间自由度，显著提升频谱效率，对系统参数失配具有鲁棒性；渐近闭式解在多种实际场景下接近理论最优。

Conclusion: 提出的天线布局方法为近场大规模MIMO系统提供了一种高效、可扩展的解决方案，能够充分利用空间自由度提升系统性能。

Abstract: Recent advancements in large-scale position-reconfigurable antennas have opened up new dimensions to effectively utilize the spatial degrees of freedom (DoFs) of wireless channels. However, the deployment of existing antenna placement schemes is primarily hindered by their limited scalability and frequently overlooked near-field effects in large-scale antenna systems. In this paper, we propose a novel antenna placement approach tailored for near-field massive multiple-input multiple-output systems, which effectively exploits the spatial DoFs to enhance spectral efficiency. For that purpose, we first reformulate the antenna placement problem in the angular domain, resulting in a weighted Fekete problem. We then derive the optimality condition and reveal that the {optimal} antenna placement is in principle an electrostatic equilibrium problem. To further reduce the computational complexity of numerical optimization, we propose an ordinary differential equation (ODE)-based framework to efficiently solve the equilibrium problem. In particular, the optimal antenna positions are characterized by the roots of the polynomial solutions to specific ODEs in the normalized angular domain. By simply adopting a two-step eigenvalue decomposition (EVD) approach, the optimal antenna positions can be efficiently obtained. Furthermore, we perform an asymptotic analysis when the antenna size tends to infinity, which yields a closed-form solution. Simulation results demonstrate that the proposed scheme efficiently harnesses the spatial DoFs of near-field channels with prominent gains in spectral efficiency and maintains robustness against system parameter mismatches. In addition, the derived asymptotic closed-form {solution} closely approaches the theoretical optimum across a wide range of practical scenarios.

</details>


### [9] [Latency-Optimal Cache-aided Multicast Streaming via Forward-Backward Reinforcement Learning](https://arxiv.org/abs/2512.21954)
*Mohsen Amidzadeh*

Main category: cs.IT

TL;DR: 提出基于前向-后向马尔可夫决策过程（FB-MDP）的缓存策略优化框架，使用前向-后向多目标强化学习（FB-MORL）算法优化时延、中断概率和资源消耗


<details>
  <summary>Details</summary>
Motivation: 在缓存使能的基站网络中，用户因多播流中断而保持兴趣导致前向动态，而时延优化需要后向动态建模，需要结合两种动态的优化框架

Method: 建立前向-后向马尔可夫决策过程（FB-MDP）模型，结合用户偏好的前向动态和时延的后向动态，使用前向-后向多目标强化学习（FB-MORL）算法进行优化

Result: 仿真结果表明提出的FB-MORL算法能够找到有前景的动态缓存策略，优化时延、中断概率和资源消耗等多目标性能

Conclusion: 通过FB-MDP框架和FB-MORL算法，能够有效建模和优化缓存网络的时延动态，实现多目标性能优化

Abstract: We consider a cellular network equipped with cache-enabled base-stations (BSs) leveraging an orthogonal multipoint multicast (OMPMC) streaming scheme. The network operates in a time-slotted fashion to serve content-requesting users by streaming cached files. The users being unsatisfied by the multicat streaming face a delivery outage, implying that they will remain interested in their preference at the next time-slot, which leads to a forward dynamics on the user preference. To design a latency-optimal streaming policy, the dynamics of latency is properly modeled and included in the learning procedure. We show that this dynamics surprisingly represents a backward dynamics. The combination of problem's forward and backward dynamics then develops a forward-backward Markov decision process (FB-MDP) that fully captures the network evolution across time. This FB-MDP necessitates usage of a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize the expected latency as well as other performance metrics of interest including the overall outage probability and total resource consumption. Simulation results show the merit of proposed FB-MORL algorithm in finding a promising dynamic cache policy.

</details>


### [10] [On the Ergodic Capacity for SIM-Aided Holographic MIMO Communications](https://arxiv.org/abs/2512.22068)
*Anastasios Papazafeiropoulos,Ioannis Bartsiokas,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: 本文推导了在瑞利衰落条件下，由堆叠智能超表面增强的全息MIMO系统容量下界的闭式表达式，该表达式适用于有限天线和SIM元件的系统，并在整个SNR范围内具有紧致性。


<details>
  <summary>Details</summary>
Motivation: 全息MIMO系统结合堆叠智能超表面技术具有巨大潜力，但现有研究缺乏对其容量性能的精确分析，特别是在有限系统参数和整个SNR范围内的闭式表达式。

Method: 推导了HMIMO-SIM系统在瑞利衰落条件下的闭式容量下界表达式，进行了全面的低SNR分析，研究了关键系统参数对容量的影响。

Result: 获得了适用于有限天线和SIM元件系统的紧致容量下界表达式，该表达式在整个SNR范围内都保持紧致性，并通过低SNR分析揭示了系统参数对容量的影响规律。

Conclusion: 本文提出的闭式容量下界表达式为HMIMO-SIM系统的性能分析和优化提供了有效的理论工具，有助于指导实际系统设计和参数配置。

Abstract: We derive a novel closed-form lower bound on the ergodic capacity of holographic multiple-input multiple-output (HMIMO) systems enhanced by stacked intelligent metasurfaces (SIMs) under Rayleigh fading conditions. The proposed expression is valid for systems with a finite number of antennas and SIM elements and exhibits tightness throughout the whole signal-to-noise ratio (SNR) range. Furthermore, we conduct a comprehensive low-SNR analysis, offering meaningful observations on how key system parameters influence the capacity performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [11] [Selective LLM-Guided Regularization for Enhancing Recommendation Models](https://arxiv.org/abs/2512.21526)
*Shanglin Yang,Zhan Shi*

Main category: cs.IR

TL;DR: 提出选择性LLM引导正则化框架，通过可训练门控机制仅在LLM可靠时激活其成对排序监督，提升推荐系统性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法存在成本高、有偏见、不可靠等问题，全局知识蒸馏会强制下游模型模仿不准确的LLM预测。研究发现LLM在重排序和挑战性场景中表现优异，但并非在所有情境下都可靠

Method: 选择性LLM引导正则化框架：使用可训练门控机制（基于用户历史长度、物品流行度和模型不确定性）预测LLM可靠性，仅在可靠时激活LLM的成对排序监督。所有LLM评分离线进行，不增加推理成本

Result: 在多个数据集上的实验表明，该选择性策略能持续提升整体准确性，在冷启动和长尾场景中带来显著增益，优于全局蒸馏基线方法

Conclusion: 选择性激活LLM监督比全局知识蒸馏更有效，能够针对性地利用LLM在特定场景下的优势，同时避免其不可靠预测带来的负面影响

Abstract: Large language models provide rich semantic priors and strong reasoning capabilities, making them promising auxiliary signals for recommendation. However, prevailing approaches either deploy LLMs as standalone recommender or apply global knowledge distillation, both of which suffer from inherent drawbacks. Standalone LLM recommender are costly, biased, and unreliable across large regions of the user item space, while global distillation forces the downstream model to imitate LLM predictions even when such guidance is inaccurate. Meanwhile, recent studies show that LLMs excel particularly in re-ranking and challenging scenarios, rather than uniformly across all contexts.We introduce Selective LLM Guided Regularization, a model-agnostic and computation efficient framework that activates LLM based pairwise ranking supervision only when a trainable gating mechanism informing by user history length, item popularity, and model uncertainty predicts the LLM to be reliable. All LLM scoring is performed offline, transferring knowledge without increasing inference cost. Experiments across multiple datasets show that this selective strategy consistently improves overall accuracy and yields substantial gains in cold start and long tail regimes, outperforming global distillation baselines.

</details>


### [12] [CEMG: Collaborative-Enhanced Multimodal Generative Recommendation](https://arxiv.org/abs/2512.21543)
*Yuzhen Lin,Hongyi Chen,Xuanjing Chen,Shaowen Wang,Ivonne Xu,Dongming Jiang*

Main category: cs.IR

TL;DR: CEMG是一个新颖的协作增强多模态生成推荐框架，通过动态融合视觉和文本特征，并使用残差量化VAE转换为离散语义代码，最后用大语言模型生成推荐，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐模型存在两个关键问题：1）协作信号的浅层集成；2）多模态特征的解耦融合。这些限制阻碍了创建真正全面的物品表示。

Method: 提出CEMG框架，包含三个主要阶段：1）多模态融合层，在协作信号指导下动态整合视觉和文本特征；2）统一模态标记化阶段，使用残差量化VAE将融合表示转换为离散语义代码；3）端到端生成推荐阶段，微调大语言模型自回归生成这些物品代码。

Result: 大量实验表明，CEMG显著优于最先进的基线方法。

Conclusion: CEMG通过协作增强的多模态融合和离散语义编码，有效解决了生成推荐中的关键挑战，实现了更全面的物品表示和更好的推荐性能。

Abstract: Generative recommendation models often struggle with two key challenges: (1) the superficial integration of collaborative signals, and (2) the decoupled fusion of multimodal features. These limitations hinder the creation of a truly holistic item representation. To overcome this, we propose CEMG, a novel Collaborative-Enhaned Multimodal Generative Recommendation framework. Our approach features a Multimodal Fusion Layer that dynamically integrates visual and textual features under the guidance of collaborative signals. Subsequently, a Unified Modality Tokenization stage employs a Residual Quantization VAE (RQ-VAE) to convert this fused representation into discrete semantic codes. Finally, in the End-to-End Generative Recommendation stage, a large language model is fine-tuned to autoregressively generate these item codes. Extensive experiments demonstrate that CEMG significantly outperforms state-of-the-art baselines.

</details>


### [13] [LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model](https://arxiv.org/abs/2512.21595)
*Yinfu Feng,Yanjing Wu,Rong Xiao,Xiaoyi Zen*

Main category: cs.IR

TL;DR: LLM-I2I是一个利用大语言模型改进Item-to-Item推荐的数据中心化框架，通过生成合成交互缓解数据稀疏性，并通过判别器过滤噪声数据，在不改变模型架构的情况下提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有I2I推荐系统面临两个方向的挑战：模型中心化方法采用更深架构但增加计算成本和部署复杂度；数据中心化方法优化训练数据但受限于数据稀疏性和噪声问题。需要一种既能提升性能又保持部署简便性的解决方案。

Method: 提出LLM-I2I框架，包含两个核心组件：1）基于LLM的生成器，为长尾物品合成用户-物品交互数据，缓解数据稀疏性；2）基于LLM的判别器，从真实和合成数据中过滤噪声交互。将精炼后的数据融合用于训练I2I模型。

Result: 在工业（AEDS）和学术（ARD）数据集上评估，LLM-I2I持续提升推荐准确性，特别是对长尾物品。在大型跨境电商平台部署后，相比现有I2I模型，召回数提升6.02%，商品交易总额提升1.22%。

Conclusion: 这项工作展示了LLMs在增强数据中心化推荐系统中的潜力，无需修改模型架构即可显著提升推荐性能，为实际工业应用提供了有效解决方案。

Abstract: Item-to-Item (I2I) recommendation models are widely used in real-world systems due to their scalability, real-time capabilities, and high recommendation quality. Research to enhance I2I performance focuses on two directions: 1) model-centric approaches, which adopt deeper architectures but risk increased computational costs and deployment complexity, and 2) data-centric methods, which refine training data without altering models, offering cost-effectiveness but struggling with data sparsity and noise. To address these challenges, we propose LLM-I2I, a data-centric framework leveraging Large Language Models (LLMs) to mitigate data quality issues. LLM-I2I includes (1) an LLM-based generator that synthesizes user-item interactions for long-tail items, alleviating data sparsity, and (2) an LLM-based discriminator that filters noisy interactions from real and synthetic data. The refined data is then fused to train I2I models. Evaluated on industry (AEDS) and academic (ARD) datasets, LLM-I2I consistently improves recommendation accuracy, particularly for long-tail items. Deployed on a large-scale cross-border e-commerce platform, it boosts recall number (RN) by 6.02% and gross merchandise value (GMV) by 1.22% over existing I2I models. This work highlights the potential of LLMs in enhancing data-centric recommendation systems without modifying model architectures.

</details>


### [14] [KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering](https://arxiv.org/abs/2512.21799)
*Hung-Nghiep Tran,Atsuhiro Takasu*

Main category: cs.IR

TL;DR: KG20C和KG20C-QA是两个用于学术数据问答研究的精选数据集，包括高质量学术知识图谱和基于该图谱构建的问答基准


<details>
  <summary>Details</summary>
Motivation: 为学术领域提供可重用、可扩展的问答研究资源，支持知识图谱推理和知识驱动应用的发展

Method: 从Microsoft Academic Graph中通过场地选择、质量过滤和模式定义构建KG20C知识图谱，然后定义问答模板将图三元组转换为自然语言问答对，创建KG20C-QA基准

Result: 创建了两个高质量数据集：KG20C学术知识图谱和KG20C-QA问答基准，提供了标准知识图谱嵌入方法的基准测试结果，并分析了不同关系类型的性能表现

Conclusion: 通过正式发布这些经过充分文档化的数据集，为研究社区贡献了可重用、可扩展的资源，支持学术领域未来的问答、推理和知识驱动应用研究

Abstract: In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.

</details>


### [15] [Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion](https://arxiv.org/abs/2512.21863)
*Huatuan Sun,Yunshan Ma,Changguang Wu,Yanxin Zhang,Pengfei Wang,Xiaoyu Du*

Main category: cs.IR

TL;DR: 本文首次系统评估了冻结大型视频语言模型在微视频推荐中的集成策略，提出了双特征融合框架，通过自适应融合多层表示与ID嵌入实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前冻结大型视频语言模型在微视频推荐中缺乏系统评估，通常被用作固定黑盒特征提取器，没有系统比较不同的表示策略，需要填补这一研究空白。

Method: 提出双特征融合框架，系统研究两个关键设计维度：1) 与ID嵌入的集成策略（替换vs融合）；2) 特征提取范式（LVLM生成字幕vs中间解码器隐藏状态）。

Result: 实验发现三个关键原则：1) 中间隐藏状态优于字幕表示；2) ID嵌入捕获不可替代的协同信号，融合优于替换；3) 中间解码器特征在不同层间效果差异显著。

Conclusion: 提出的DFF框架在两个真实微视频推荐基准上实现了最先进的性能，为将现成的大型视觉语言模型集成到微视频推荐系统提供了原则性方法。

Abstract: Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [16] [Weighted Fourier Factorizations: Optimal Gaussian Noise for Differentially Private Marginal and Product Queries](https://arxiv.org/abs/2512.21499)
*Christian Janos Lebeda,Aleksandar Nikolov,Haohua Tang*

Main category: cs.DS

TL;DR: 提出一种基于傅里叶基和独立高斯噪声的差分隐私边际查询释放机制，该机制在因子化机制类中达到最优噪声方差，并扩展到乘积查询和扩展边际查询。


<details>
  <summary>Details</summary>
Motivation: 重新审视在差分隐私下释放边际查询的任务，目标是设计更高效、更优的算法，改进现有基于半定规划的因子化机制，降低计算复杂度。

Method: 在傅里叶基中释放查询，使用独立高斯噪声并精心校准方差，通过逆傅里叶变换重建边际查询答案。该方法是一种因子化机制，扩展到乘积查询和扩展边际查询。

Result: 该算法在因子化机制类中达到精确最优：最小化加权噪声方差和与最大噪声方差。相比基于半定规划的算法，运行时间更优（多项式时间）。扩展版本在乘积查询中保持精确最优，在扩展边际查询中达到近似最优。

Conclusion: 提出的傅里叶基方法提供了一种更简单、更高效的差分隐私边际查询释放机制，在因子化机制类中达到最优性能，并成功扩展到更广泛的查询类型。

Abstract: We revisit the task of releasing marginal queries under differential privacy with additive (correlated) Gaussian noise. We first give a construction for answering arbitrary workloads of weighted marginal queries, over arbitrary domains. Our technique is based on releasing queries in the Fourier basis with independent noise with carefully calibrated variances, and reconstructing the marginal query answers using the inverse Fourier transform. We show that our algorithm, which is a factorization mechanism, is exactly optimal among all factorization mechanisms, both for minimizing the sum of weighted noise variances, and for minimizing the maximum noise variance. Unlike algorithms based on optimizing over all factorization mechanisms via semidefinite programming, our mechanism runs in time polynomial in the dataset and the output size. This construction recovers results of Xiao et al. [Neurips 2023] with a simpler algorithm and optimality proof, and a better running time.
  We then extend our approach to a generalization of marginals which we refer to as product queries. We show that our algorithm is still exactly optimal for this more general class of queries. Finally, we show how to embed extended marginal queries, which allow using a threshold predicate on numerical attributes, into product queries. We show that our mechanism is almost optimal among all factorization mechanisms for extended marginals, in the sense that it achieves the optimal (maximum or average) noise variance up to lower order terms.

</details>


### [17] [Fully Dynamic Spectral Sparsification for Directed Hypergraphs](https://arxiv.org/abs/2512.21671)
*Sebastian Forster,Gramoz Goranci,Ali Momeni*

Main category: cs.DS

TL;DR: 提出了一种针对有向超图的完全动态谱稀疏化算法，具有近最优的稀疏化大小和高效的更新复杂度。


<details>
  <summary>Details</summary>
Motivation: 谱超图稀疏化是图谱稀疏化的自然推广，近年来受到广泛关注。现有研究主要关注静态或增量设置，缺乏高效的完全动态算法来处理有向超图的实时更新。

Method: 设计了一种简单的完全动态算法，用于维护有向超图的谱稀疏化。算法支持超边的插入和删除操作，并扩展到并行批处理动态设置。

Result: 算法达到近最优的稀疏化大小 O(n²/ε² log⁷ m) 和摊销更新时间 O(r² log³ m)。在并行批处理动态设置中，处理k个超边更新的摊销工作量为 O(kr² log³ m)，深度为 O(log² m)。

Conclusion: 这是首个在并行批处理动态设置中基于谱的稀疏化算法，为有向超图的动态谱稀疏化提供了高效解决方案。

Abstract: There has been a surge of interest in spectral hypergraph sparsification, a natural generalization of spectral sparsification for graphs. In this paper, we present a simple fully dynamic algorithm for maintaining spectral hypergraph sparsifiers of \textit{directed} hypergraphs. Our algorithm achieves a near-optimal size of $O(n^2 / \varepsilon ^2 \log ^7 m)$ and amortized update time of $O(r^2 \log ^3 m)$, where $n$ is the number of vertices, and $m$ and $r$ respectively upper bound the number of hyperedges and the rank of the hypergraph at any time.
  We also extend our approach to the parallel batch-dynamic setting, where a batch of any $k$ hyperedge insertions or deletions can be processed with $O(kr^2 \log ^3 m)$ amortized work and $O(\log ^2 m)$ depth. This constitutes the first spectral-based sparsification algorithm in this setting.

</details>


### [18] [A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication](https://arxiv.org/abs/2512.21980)
*A. I. Perminov*

Main category: cs.DS

TL;DR: 提出了一种新的3×3矩阵乘法算法，在非交换环上达到秩23方案，仅需58次标量加法，将总标量操作数从83降至81


<details>
  <summary>Details</summary>
Motivation: 改进3×3矩阵乘法的计算效率，降低标量加法次数，同时保持系数简单（仅-1,0,1）以确保算法在不同域上的可移植性

Method: 结合三元限制翻转图探索和贪心交集约简进行公共子表达式消除的自动化搜索方法

Result: 实现了秩23方案，仅需58次标量加法（之前最佳为60次），总标量操作数从83降至81，系数仅使用{-1,0,1}

Conclusion: 该算法在非交换环上的3×3矩阵乘法中达到了新的最优性能，通过自动化搜索方法发现了更高效的实现方案

Abstract: This paper presents a new state-of-the-art algorithm for exact $3\times3$ matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from $\{-1, 0, 1\}$, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [19] [Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks](https://arxiv.org/abs/2512.21345)
*Jasmin Saxer,Isabella Maria Aigner,Luise Linzmeier,Andreas Weiler,Kurt Stockinger*

Main category: cs.DB

TL;DR: 提出Query Carefully管道，结合LLM生成SQL与不可回答输入的检测处理，在生物医学文本到SQL系统中提高可靠性


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL系统倾向于为模糊、超出范围或不可回答的查询生成可执行SQL，存在被误解为正确的风险，这在需要精确性的生物医学领域尤其严重

Method: 基于ScienceBenchmark的OncoMX组件构建OncoMX-NAQ数据集（80个不可回答问题，8个类别）；使用llama3.3:70b模型，结合模式感知提示、明确的不可回答规则(NAR)以及来自可回答和不可回答问题的少样本示例

Result: 在OncoMX开发集上，使用可回答示例的少样本提示提高了结果准确性，添加不可回答示例不会降低性能；在OncoMX-NAQ上，平衡提示实现了最高的不可回答检测准确率(0.8)，结构定义类别表现优异但缺失值查询(0.5)和列模糊性(0.3)仍具挑战

Conclusion: Query Carefully管道通过透明地展示中间SQL、执行结果和弃权，支持生物医学应用中可靠且透明的文本到SQL转换，有效降低了误判风险

Abstract: Text-to-SQL systems allow non-SQL experts to interact with relational databases using natural language. However, their tendency to generate executable SQL for ambiguous, out-of-scope, or unanswerable queries introduces a hidden risk, as outputs may be misinterpreted as correct. This risk is especially serious in biomedical contexts, where precision is critical. We therefore present Query Carefully, a pipeline that integrates LLM-based SQL generation with explicit detection and handling of unanswerable inputs. Building on the OncoMX component of ScienceBenchmark, we construct OncoMX-NAQ (No-Answer Questions), a set of 80 no-answer questions spanning 8 categories (non-SQL, out-of-schema/domain, and multiple ambiguity types). Our approach employs llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples drawn from both answerable and unanswerable questions. We evaluate SQL exact match, result accuracy, and unanswerable-detection accuracy. On the OncoMX dev split, few-shot prompting with answerable examples increases result accuracy, and adding unanswerable examples does not degrade performance. On OncoMX-NAQ, balanced prompting achieves the highest unanswerable-detection accuracy (0.8), with near-perfect results for structurally defined categories (non-SQL, missing columns, out-of-domain) but persistent challenges for missing-value queries (0.5) and column ambiguity (0.3). A lightweight user interface surfaces interim SQL, execution results, and abstentions, supporting transparent and reliable text-to-SQL in biomedical applications.

</details>
