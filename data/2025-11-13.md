<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Design and Performance Analysis of Hybrid FSO/THz Relay with Aerial RIS for Future NTN-Integrated 6G Wireless Communications](https://arxiv.org/abs/2511.08756)
*Al Nahian Mugdho,Md. Ibrahim,A. S. M. Badrudduza,Md. Abdur Rakib,Imran Shafique Ansari*

Main category: cs.IT

TL;DR: 提出了一种新型双跳无线网络架构，结合FSO/THz混合链路和空中RIS辅助的RF链路，通过硬切换和软切换策略比较，性能相比传统RF-FSO框架提升52.54%，空中RIS集成带来41.39%额外性能增益。


<details>
  <summary>Details</summary>
Motivation: 在6G无线网络背景下，利用RIS智能控制电磁波传播的能力来增强回程通信性能，解决传统无线通信的性能瓶颈问题。

Method: 构建双跳无线网络：第一跳为FSO/THz混合链路，第二跳为空中RIS辅助的RF链路；比较硬切换和软切换策略；推导关键性能指标的闭式表达式；进行渐近分析和蒙特卡洛仿真验证。

Result: 提出的混合模型比传统RF-FSO框架性能提升52.54%；空中RIS集成带来41.39%性能提升；将空中RIS放置在较低高度且与通信端点等距较短距离时系统性能最佳。

Conclusion: 所提出的混合网络架构结合FSO/THz和空中RIS技术，通过优化切换策略和RIS部署位置，显著提升了系统性能，为6G回程通信提供了有效的解决方案。

Abstract: In the context of emerging sixth-generation (6G) wireless networks, reconfigurable intelligent surfaces (RISs) are gaining prominence for their ability to intelligently control electromagnetic wave propagation and enhance backhaul communication performance. In this paper, we propose a novel dual-hop wireless network, where the first hop consists of a hybrid free-space optics (FSO) / terahertz (THz) link, and the second hop incorporates an aerial RIS-based radio frequency (RF) link. To provide a comprehensive performance evaluation, a comparative analysis of two switching strategies is conducted: (1) hard switching and (2) soft switching. Novel closed-form expressions are derived for key performance metrics, including outage probability and bit error rate. These expressions are then utilized to investigate the impact of various system parameters. Our proposed hybrid model demonstrates a 52.54% performance improvement over the traditional RF-FSO framework. Moreover, the integration of an aerial RIS in the second hop enhances system performance by 41.39%. Numerical findings suggest that strategically placing the aerial RIS at a lower altitude and maintaining an equal, shorter distance from both communication endpoints significantly improves overall system performance. To analyze the response under high signal-to-noise ratio (SNR) conditions, asymptotic analysis is performed, and the diversity order of the system is determined. Finally, the analytical results are validated through a Monte Carlo simulation.

</details>


### [2] [Tracing AG Codes: Toward Meeting the Gilbert-Varshamov Bound](https://arxiv.org/abs/2511.08788)
*Gil Cohen,Dean Doron,Noam Goldgraber,Tomer Manket*

Main category: cs.IT

TL;DR: 该论文通过追踪代数几何码来尝试匹配Gilbert-Varshamov界，提出了一种新的TAG码分析方法，并建立了适用于分析TAG码的Hasse-Weil型定理。


<details>
  <summary>Details</summary>
Motivation: 解决编码理论中最古老的问题之一——用显式二进制码匹配Gilbert-Varshamov界。传统方法如级联码在字母表缩减时会损失参数，而代数几何码在较大常数域上已知能超过GV界，作者希望通过追踪AG码来利用这一优势。

Method: 采用追踪代数几何码的方法，从常数域扩展追踪到二进制域。与传统的级联方法不同，该方法在字母表缩减步骤中持续利用AG码的代数结构。主要技术贡献是建立了适用于分析TAG码的Hasse-Weil型定理。

Result: 虽然未获得改进的构造，但证明了常数因子加强的界就足够了。在远距离机制下，TAG码不如码级联。新的Hasse-Weil型定理比分析TAG码所需的范围更广泛，还推导出了指数和的新估计。

Conclusion: 追踪代数几何码方法为匹配GV界提供了新的分析框架，虽然在高距离机制下不如级联码，但建立的Hasse-Weil型定理具有更广泛的适用性，为指数和估计提供了新工具。

Abstract: One of the oldest problems in coding theory is to match the Gilbert-Varshamov bound with explicit binary codes. Over larger-yet still constant-sized-fields, algebraic-geometry codes are known to beat the GV bound. In this work, we leverage this phenomenon by taking traces of AG codes. Our hope is that the margin by which AG codes exceed the GV bound will withstand the parameter loss incurred by taking the trace from a constant field extension to the binary field. In contrast to concatenation, the usual alphabet-reduction method, our analysis of trace-of-AG (TAG) codes uses the AG codes' algebraic structure throughout - including in the alphabet-reduction step.
  Our main technical contribution is a Hasse-Weil-type theorem that is well-suited for the analysis of TAG codes. The classical theorem (and its Grothendieck trace-formula extension) are inadequate in this setting. Although we do not obtain improved constructions, we show that a constant-factor strengthening of our bound would suffice. We also analyze the limitations of TAG codes under our bound and prove that, in the high-distance regime, they are inferior to code concatenation. Our Hasse-Weil-type theorem holds in far greater generality than is needed for analyzing TAG codes. In particular, we derive new estimates for exponential sums.

</details>


### [3] [Policy-Guided MCTS for near Maximum-Likelihood Decoding of Short Codes](https://arxiv.org/abs/2511.09054)
*Y. Tian,C. Yue,P. Cheng,G. Pang,B. Vucetic,Y. Li*

Main category: cs.IT

TL;DR: 提出一种基于策略引导的蒙特卡洛树搜索解码器，用于短块码的近最大似然解码，无需高斯消元，搜索复杂度降低95%，在高信噪比下解码延迟更低。


<details>
  <summary>Details</summary>
Motivation: 传统有序统计解码(OSD)需要高斯消元，计算复杂度高，希望开发一种无需高斯消元且能实现近最大似然解码性能的方法。

Method: 使用蒙特卡洛树搜索在接收信息位中搜索测试错误模式，通过神经网络策略引导搜索过程，训练策略以最小步数找到正确测试错误模式。

Result: 相比非高斯消元OSD，搜索复杂度降低95%，在高信噪比下解码延迟低于OSD和非高斯消元OSD，实现近最大似然解码性能。

Conclusion: 该方法为短块码提供了一种高效的无高斯消元解码方案，在保持近最优性能的同时显著降低了计算复杂度。

Abstract: In this paper, we propose a policy-guided Monte Carlo Tree Search (MCTS) decoder that achieves near maximum-likelihood decoding (MLD) performance for short block codes. The MCTS decoder searches for test error patterns (TEPs) in the received information bits and obtains codeword candidates through re-encoding. The TEP search is executed on a tree structure, guided by a neural network policy trained via MCTS-based learning. The trained policy guides the decoder to find the correct TEPs with minimal steps from the root node (all-zero TEP). The decoder outputs the codeword with maximum likelihood when the early stopping criterion is satisfied. The proposed method requires no Gaussian elimination (GE) compared to ordered statistics decoding (OSD) and can reduce search complexity by 95\% compared to non-GE OSD. It achieves lower decoding latency than both OSD and non-GE OSD at high SNRs.

</details>


### [4] [Color Multiset Codes based on Sunmao Construction](https://arxiv.org/abs/2511.09070)
*Wing Shing Wong,Chung Shue Chen,Yuan-Hsun Lo*

Main category: cs.IT

TL;DR: 该论文提出了一种使用多重集而非有序序列的编码方案，适用于符号顺序无法保持或观察的场景，如传感器网络中的移动目标跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于传感器网络中移动目标跟踪问题，以及在符号顺序无法维持或观察的场景下需要编码方案的需求。

Method: 提出了多重集编码方案，将原始源数据网格分解为子网格，使用称为'榫卯构造'的方法将子网格解决方案拼接成完整解决方案，并定义了具体的编织码。

Result: 编织码对于给定码集大小和多重集基数，在编码效率方面具有与最优方案相同的渐近阶数，并展现出有趣的固有纠错特性。

Conclusion: 多重集编码方案通过网格分解和榫卯构造方法，为顺序不可维护的场景提供了有效的编码解决方案，编织码在效率和纠错方面表现优异。

Abstract: We present results on coding using multisets instead of ordered sequences. The study is motivated by a moving object tracking problem in a sensor network and can find applications in settings where the order of the symbols in a codeword cannot be maintained or observed. In this paper a multiset coding scheme is proposed on source data that can be organized as a flat or cyclic multi-dimensional integer lattice (grid). A fundamental idea in the solution approach is to decompose the original source data grid into sub-grids. The original multiset coding problem can then be restricted to each of the sub-grid. Solutions for the sub-grids are subsequently piece together to form the desired solution. We name this circle of idea as sunmao construction in reference to woodwork construction method with ancient origin. Braid codes are specific solutions defined using the sunmao construction. They are easy to define for multi-dimensional grids. Moreover for a code of a given code set size and multiset cardinality, if we measure coding efficiency by the number of distinct symbols required, then braid codes have asymptotic order equal to those that are optimal. We also show that braid codes have interesting inherent error correction properties.

</details>


### [5] [Learning Binary Autoencoder-Based Codes with Progressive Training](https://arxiv.org/abs/2511.09221)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: 提出了一种简化的两阶段训练方法，用于学习二进制纠错码的自动编码器架构，无需梯度近似技术即可实现稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 解决在可微分自动编码器架构中强制二进制码字的困难，因为离散化会破坏梯度流并导致不稳定收敛。

Method: 采用两阶段训练：连续预训练阶段，然后直接二值化和微调，不使用梯度近似技术。

Result: 在(7,4)块配置和二进制对称信道下，学习到的编码器-解码器对自然地恢复了最优汉明码的旋转版本（陪集码），具有相同的线性特性和距离特性，在使用最大似然解码时达到相同的块错误率。

Conclusion: 紧凑的自动编码器架构可以通过稳定且简单的训练有效地学习结构化的、代数最优的二进制码。

Abstract: Error correcting codes play a central role in digital communication, ensuring that transmitted information can be accurately reconstructed despite channel impairments. Recently, autoencoder (AE) based approaches have gained attention for the end-to-end design of communication systems, offering a data driven alternative to conventional coding schemes. However, enforcing binary codewords within differentiable AE architectures remains difficult, as discretization breaks gradient flow and often leads to unstable convergence. To overcome this limitation, a simplified two stage training procedure is proposed, consisting of a continuous pretraining phase followed by direct binarization and fine tuning without gradient approximation techniques. For the (7,4) block configuration over a binary symmetric channel (BSC), the learned encoder-decoder pair learns a rotated version (coset code) of the optimal Hamming code, naturally recovering its linear and distance properties and thereby achieving the same block error rate (BLER) with maximum likelihood (ML) decoding. These results indicate that compact AE architectures can effectively learn structured, algebraically optimal binary codes through stable and straightforward training.

</details>


### [6] [Generic Construction of Optimal-Access Binary MDS Array Codes with Smaller Sub-packetization](https://arxiv.org/abs/2511.09251)
*Lan Ma,Qifu Tyler Sun,Shaoteng Liu,Liyang Zhou*

Main category: cs.IT

TL;DR: 提出了两种通用构造方法（通用构造I和II）来构建具有最优访问带宽或修复带宽的二进制MDS阵列码，用于分布式存储系统中的单节点故障修复。


<details>
  <summary>Details</summary>
Motivation: 二进制MDS阵列码的修复问题，特别是单节点故障的修复，引起了广泛关注。现有方法在子分组大小和修复带宽方面存在限制，需要更灵活的构造方法。

Method: 通用构造I：给定任意基码，构造(k+r,k,ms^⌈(k+r)/s⌉)码，修复失败节点需连接d=k+s-1个辅助节点。通用构造II：当r为偶数且s+1整除k+r时，构造(k+r,k,ms^{(k+r)/(s+1)})码，其中部分节点具有最优访问特性。

Result: 构造I实现了更小的子分组大小和更大的系数矩阵选择灵活性。构造II在现有具有最优修复带宽的二进制MDS阵列码中实现了最小的子分组大小。

Conclusion: 提出的两种通用构造方法为二进制MDS阵列码提供了灵活的最优修复方案，在子分组大小和修复性能方面优于现有方法。

Abstract: A $(k+r,k,l)$ binary array code of length $k+r$, dimension $k$, and sub-packetization $l$ is composed of $l\times(k+r)$ matrices over $\mathbb{F}_2$, with every column of the matrix stored on a separate node in the distributed storage system and viewed as a coordinate of the codeword. It is said to be maximum distance separable (MDS) if any $k$ out of $k+r$ coordinates suffice to reconstruct the whole codeword. The repair problem of binary MDS array codes has drawn much attention, particularly for single-node failures. In this paper, given an arbitrary binary MDS array code with sub-packetization $m$ as the base code, we propose two generic approaches (Generic Construction I and II) for constructing binary MDS array codes with optimal access (or repair) bandwidth for single-node failures. For every $s\leq r$, a $(k+r,k,ms^{\lceil \frac{k+r}{s}\rceil})$ code $\mathcal{C}_1$ with optimal access bandwidth can be constructed by Generic Construction I. Repairing a failed node of $\mathcal{C}_1$ requires connecting to $d = k+s-1$ helper nodes, in which $s-1$ helper nodes are designated and $k$ are free to select. $\mathcal{C}_1$ generally achieves smaller sub-packetization and provides greater flexibility in the selection of its coefficient matrices. For even $r\geq4$ and $s=\frac{r}{2}$ such that $s+1$ divides $k+r$, a $(k+r, k,ms^{\frac{k+r}{s+1}})$ code $\mathcal{C}_2$ with optimal repair bandwidth can be constructed by Generic Construction II, with $\frac{s}{s+1}(k+r)$ out of $k+r$ nodes having the optimal access property. To the best of our knowledge, $\mathcal{C}_2$ possesses the smallest sub-packetization among existing binary MDS array codes with optimal repair bandwidth known to date.

</details>


### [7] [Enabling Smart Radio Environments in the Frequency Domain With Movable Signals](https://arxiv.org/abs/2511.09384)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出了一种新的智能无线电环境实现方法——通过可移动信号在频域实现，相比传统的可重构智能表面和灵活天线，避免了电子可重构组件的实现挑战。


<details>
  <summary>Details</summary>
Motivation: 传统智能无线电环境技术依赖电子可重构或可移动组件，存在实现挑战，可能阻碍商业化。需要寻找替代方案来克服这些挑战。

Method: 提出在频域实现智能无线电环境的概念——可移动信号，即信号频谱可以沿频率轴动态移动。分析了在LOS和NLOS条件下的MISO系统性能。

Result: 在LOS条件下，可移动信号比量化等增益传输获得更高的平均接收功率。在NLOS条件下，通过固定智能表面辅助，可移动信号系统可达到使用固定频率信号的RIS辅助系统四倍的接收功率。

Conclusion: 频域中的可移动信号是实现智能无线电环境的有前景的替代方法，能够克服传统技术的实现挑战，同时提供优越的性能。

Abstract: Smart radio environments (SREs) enhance wireless communications by allowing control over the channel. They have been enabled through surfaces with reconfigurable electromagnetic (EM) properties, known as reconfigurable intelligent surfaces (RISs), and through flexible antennas, which can be viewed as realizations of SREs in the EM domain and space domain, respectively. However, these technologies rely on electronically reconfigurable or movable components, introducing implementation challenges that could hinder commercialization. To overcome these challenges, we propose a new domain to enable SREs, the frequency domain, through the concept of movable signals, where the signal spectrum can be dynamically moved along the frequency axis. We first analyze movable signals in multiple-input single-output (MISO) systems under line-of-sight (LoS) conditions, showing that they can achieve higher average received power than quantized equal gain transmission (EGT). We then study movable signals under non-line-of-sight (NLoS) conditions, showing that they remain effective by leveraging reflections from surfaces made of uniformly spaced elements with fixed EM properties, denoted as fixed intelligent surfaces (FISs). Analytical results reveal that a FIS-aided system using movable signals can achieve up to four times the received power of a RIS-aided system using fixed-frequency signals.

</details>


### [8] [Computability of the Optimizer for Rate Distortion Functions](https://arxiv.org/abs/2511.09412)
*Jonathan E. W. Huffmann,Holger Boche*

Main category: cs.IT

TL;DR: 本文研究了率失真函数中优化测试信道的可计算性问题，发现虽然率失真函数通常是可计算的，但其优化器在一般情况下是不可计算的，即使对于简单的失真度量也是如此。


<details>
  <summary>Details</summary>
Motivation: 率失真理论在通信和信息理论中具有重要理论和实践价值，研究其优化器的可计算性有助于理解信息理论问题的计算复杂性。

Method: 通过分析率失真函数优化问题的计算特性，与已知的其他信息理论问题优化器结果进行比较研究。

Result: 研究发现率失真函数本身通常是可计算的，但用于码本构造的优化测试信道在一般情况下是不可计算的。

Conclusion: 率失真函数的优化器存在不可计算性，这与信息理论中其他问题的优化器特性相似，揭示了信息理论优化问题的计算复杂性。

Abstract: Rate distortion theory treats the problem of encoding a source with minimum codebook size while at the same time allowing for a certain amount of errors in the reconstruction measured by a fidelity criterion and distortion level. Similar to the channel coding problem the optimal rate of the codebook with respect to the blocklength is given by a convex optimization problem involving information theoretic quantities like mutual information. The value of the rate in dependence of the distortion level as well as the optimizer used in the codebook construction are of theoretical and practical importance in communication and information theory. In this paper the behavior of the rate distortion function regarding the computability of the optimizing test channel is investigated. We find that comparable with known results about the optimizer for other information theoretic problems a similar result is found to be true also regarding the computability of the optimizer for rate distortion functions.
  It turns out that while the rate distortion function is usually computable the optimizer for this problem is in general non-computable even for simple distortion measures.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [9] [Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding](https://arxiv.org/abs/2511.08978)
*Jingtian Ma,Jingyuan Wang,Wayne Xin Zhao,Guoping Liu,Xiang Wen*

Main category: cs.MM

TL;DR: 提出了ST-CLIP模型，通过时空上下文感知多角度提示学习方法将时空信息整合到CLIP模型中，用于交通场景理解任务


<details>
  <summary>Details</summary>
Motivation: 现有研究往往将交通场景理解视为普通图像理解任务，忽略了时空信息以及交通场景不同方面之间的相互关系

Method: 基于CLIP模型，设计了时空上下文感知多角度提示学习方法，包括动态时空上下文表示模块和双层时空感知多角度提示学习模块

Result: 在两个真实世界数据集上的实验表明，在复杂场景理解场景中具有优越性能，并采用少样本学习策略

Conclusion: 这是首次尝试将时空信息整合到视觉语言模型中以促进交通场景理解任务

Abstract: Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy.

</details>


### [10] [MCAD: Multimodal Context-Aware Audio Description Generation For Soccer](https://arxiv.org/abs/2511.09448)
*Lipisha Chaudhary,Trisha Mittal,Subhadra Gopalakrishnan,Ifeoma Nwogu,Jaclyn Pytlarz*

Main category: cs.MM

TL;DR: MCAD是一个端到端的音频描述生成系统，专门针对足球比赛，无需依赖真实音频描述数据，通过多模态上下文线索和微调的视频大语言模型生成完整的音频描述文本。


<details>
  <summary>Details</summary>
Motivation: 现有音频描述自动化方法仅限于高质量电影内容且需要人工标注的真实音频描述数据，无法扩展到体育领域特别是足球比赛。

Method: 在公开电影音频描述数据集上微调视频大语言模型学习叙述结构，推理时结合球员身份、足球事件动作、比赛解说等多模态上下文线索生成音频描述。

Result: 开发了ARGE-AD评估指标来准确评估生成音频描述的质量，包含五个特征：人名使用、动作事件提及、适当长度、无代词、与解说字幕重叠度。

Conclusion: MCAD成功将音频描述生成扩展到体育领域，提供了100个足球比赛片段的专家标注音频描述数据集，并验证了新评估指标的有效性。

Abstract: Audio Descriptions (AD) are essential for making visual content accessible to individuals with visual impairments. Recent works have shown a promising step towards automating AD, but they have been limited to describing high-quality movie content using human-annotated ground truth AD in the process. In this work, we present an end-to-end pipeline, MCAD, that extends AD generation beyond movies to the domain of sports, with a focus on soccer games, without relying on ground truth AD. To address the absence of domain-specific AD datasets, we fine-tune a Video Large Language Model on publicly available movie AD datasets so that it learns the narrative structure and conventions of AD. During inference, MCAD incorporates multimodal contextual cues such as player identities, soccer events and actions, and commentary from the game. These cues, combined with input prompts to the fine-tuned VideoLLM, allow the system to produce complete AD text for each video segment. We further introduce a new evaluation metric, ARGE-AD, designed to accurately assess the quality of generated AD. ARGE-AD evaluates the generated AD for the presence of five characteristics: (i) usage of people's names, (ii) mention of actions and events, (iii) appropriate length of AD, (iv) absence of pronouns, and (v) overlap from commentary or subtitles. We present an in-depth analysis of our approach on both movie and soccer datasets. We also validate the use of this metric to quantitatively comment on the quality of generated AD using our metric across domains. Additionally, we contribute audio descriptions for 100 soccer game clips annotated by two AD experts.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [A Simple Analysis of Ranking in General Graphs](https://arxiv.org/abs/2511.08801)
*Mahsa Derakhshan,Mohammad Roghani,Mohammad Saneian,Tao Yu*

Main category: cs.DS

TL;DR: 该论文对Karp、Vazirani和Vazirani提出的Ranking算法进行了简单组合分析，证明该算法在一般图上能够实现(1/2 + c)近似匹配，其中c ≥ 0.005。


<details>
  <summary>Details</summary>
Motivation: 重新分析经典的Ranking算法，证明其在一般图上的近似比优于1/2，这是对原始工作的改进。

Method: 使用组合分析方法对Ranking算法进行理论分析。

Result: 证明了Ranking算法在一般图上能够实现(1/2 + 0.005)近似匹配。

Conclusion: Ranking算法在一般图上的近似性能比之前认为的要好，至少达到1/2 + 0.005的近似比。

Abstract: We provide a simple combinatorial analysis of the Ranking algorithm, originally introduced in the seminal work by Karp, Vazirani, and Vazirani [KVV90], demonstrating that it achieves a $(1/2 + c)$-approximate matching for general graphs for $c \geq 0.005$.

</details>


### [12] [Space-Efficient and Output-Sensitive Algorithms for the Longest Common Bitonic Subsequence](https://arxiv.org/abs/2511.08958)
*Md. Tanzeem Rahat,Md. Manzurul Hasan*

Main category: cs.DS

TL;DR: 该论文提出了两种改进的最长公共双调子序列(LCBS)算法：一种使用θ(min(n,m))额外内存的滚动行实现，另一种基于稀疏DAG的最长路径方法，时间复杂度为O((n+m)logn + MlogM)，空间复杂度为O(M)，在匹配符号数M远小于nm时显著优于传统二次算法。


<details>
  <summary>Details</summary>
Motivation: LCBS自然建模生物信息学、金融和信号分析中的上升-下降模式，但之前唯一的解决方案是二次动态规划，需要θ(nm)时间和空间，限制了实际应用。

Method: 1. 改进的滚动行实现：在θ(nm)时间内使用θ(min(n,m))额外内存评估相同递推关系；2. 稀疏DAG方法：通过隔离M个符号匹配及其C个双调兼容对，将LCBS建模为稀疏有向无环图中的最长路径问题。

Result: 新算法使精确LCBS计算对于之前无法处理的输入变得可行，暴露了新的细粒度复杂度景观，当M << nm时渐进快于二次基线。

Conclusion: LCBS的空间障碍并非固有，通过创新算法设计可以显著降低计算复杂度，为实际应用打开了可能性，并为进一步探索提供了新的复杂度研究视角。

Abstract: The longest common bitonic subsequence (LCBS) of two sequences A and B is the longest subsequence that increases to a single peak and then decreases while appearing, in order, in both inputs. Although LCBS naturally models rise-fall patterns in bioinformatics, finance, and signal analysis, the only previously documented solution was a quadratic dynamic program that needs θ(nm) time and space. We show that this space barrier is not inherent: a refined rolling-row implementation evaluates the same recurrence in θ(nm) time with only θ(min(n, m)) additional memory. By isolating the M symbol matches and their C bitonic-compatible pairs, we cast LCBS as a longest-path problem in a sparse DAG and solve it in O((n + m) log n + M log M) time and O(M) space, which is asymptotically faster than the quadratic baseline whenever M << n m. These results make exact LCBS computation practical for inputs that were previously out of reach and expose a new fine-grained complexity landscape that invites further exploration.

</details>


### [13] [Prophet and Secretary at the Same Time](https://arxiv.org/abs/2511.09531)
*Gregory Kehne,Thomas Kesselheim*

Main category: cs.DS

TL;DR: 该论文研究了在线最优停止问题中算法对分布错误指定的鲁棒性，探讨了能否同时为已知分布（先知不等式）和未知分布（秘书问题）提供竞争性保证。


<details>
  <summary>Details</summary>
Motivation: 研究在线算法对输入分布错误指定的鲁棒性，探索算法能否在分布符合预期和不符合预期时都保持良好的竞争比。

Method: 引入了一系列算法族，这些算法能够提供非平凡的同时保证，并在极端的i.i.d.先知和秘书问题中达到最优。同时证明了不可能性结果，识别了任何自适应停止规则都无法达到的竞争比组合。

Result: 确定了停止规则可达到的帕累托前沿，并提供了在泊松过程和固定到达数设置下的最优算法。

Conclusion: 该研究为在线最优停止问题中分布鲁棒性提供了理论框架，识别了可达到和不可达到的竞争比组合，并提供了在两种设置下都有效的算法。

Abstract: Many online problems are studied in stochastic settings for which inputs are samples from a known distribution, given in advance, or from an unknown distribution. Such distributions model both beyond-worst-case inputs and, when given, partial foreknowledge for the online algorithm. But how robust can such algorithms be to misspecification of the given distribution? When is this detectable, and when does it matter? When can algorithms give good competitive ratios both when the input distribution is as specified, and when it is not?
  We consider these questions in the setting of optimal stopping, where the cases of known and unknown distributions correspond to the well-known prophet inequality and to the secretary problem, respectively. Here we ask: Can a stopping rule be competitive for the i.i.d. prophet inequality problem and the secretary problem at the same time? We constrain the Pareto frontier of simultaneous approximation ratios $(α, β)$ that a stopping rule can attain.
  We introduce a family of algorithms that give nontrivial joint guarantees and are optimal for the extremal i.i.d. prophet and secretary problems. We also prove impossibilities, identifying $(α, β)$ unattainable by any adaptive stopping rule. Our results hold for both $n$ fixed arrivals and for arrivals from a Poisson process with rate $n$. We work primarily in the Poisson setting, and provide reductions between the Poisson and $n$-arrival settings that may be of broader interest.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [14] [Efficient Model-Agnostic Continual Learning for Next POI Recommendation](https://arxiv.org/abs/2511.08941)
*Chenhao Wang,Shanshan Feng,Lisi Chen,Fan Li,Shuo Shang*

Main category: cs.IR

TL;DR: 提出了GIRAM框架，用于持续的下一个兴趣点推荐，通过整合上下文感知的持续兴趣和近期兴趣来动态适应用户行为变化。


<details>
  <summary>Details</summary>
Motivation: 现有下一个兴趣点推荐方法依赖静态数据集和固定模型，无法适应随时间变化的用户行为，因此需要能够持续学习和更新的方法。

Method: GIRAM框架包含四个组件：兴趣记忆库、上下文感知键编码模块、基于生成键的检索模块、自适应兴趣更新和融合模块，可与现有推荐模型无缝集成。

Result: 在三个真实世界数据集上的实验表明，GIRAM在更新时间和内存消耗方面保持高效率的同时，持续优于最先进的方法。

Conclusion: GIRAM为持续下一个兴趣点推荐提供了一个高效、模型无关的解决方案，能够有效平衡持续兴趣和近期兴趣，适应动态变化的用户行为。

Abstract: Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.

</details>


### [15] [NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning](https://arxiv.org/abs/2511.09250)
*Jiyuan Wang,Li Zhang,Haipeng Lin,Qile Liu,Gan Huang,Ziyu Li,Zhen Liang,Xia Wu*

Main category: cs.IR

TL;DR: NeuroCLIP是一个针对EEG-图像对比学习的提示调优框架，通过双流视觉嵌入、视觉提示令牌和改进的对比损失，在THINGS-EEG2数据集上实现了63.2%的Top-1零样本图像检索准确率，比之前最佳方法提升了12.3%。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将CLIP视为静态特征提取器，忽视了其对神经表征的适应性以及EEG-图像对齐中固有的生理-符号鸿沟。

Method: 1) 双流视觉嵌入管道结合动态滤波和令牌级融合生成实例级自适应提示；2) 首次在EEG-图像对齐中引入视觉提示令牌作为全局模态级提示；3) 基于神经科学原理提出改进的对比损失函数。

Result: 在THINGS-EEG2数据集上，NeuroCLIP实现了63.2%的Top-1零样本图像检索准确率，比之前最佳方法提升12.3%，在跨被试条件下也表现出强泛化能力（+4.6% Top-1）。

Conclusion: NeuroCLIP展示了生理感知提示调优在连接脑信号和视觉语义方面的潜力，为脑启发人工智能提供了新思路。

Abstract: Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.

</details>


### [16] [Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction](https://arxiv.org/abs/2511.09329)
*Andreas Konstantin Kruff,Christin Katharina Kreutz,Timo Breuer,Philipp Schaer,Krisztian Balog*

Main category: cs.IR

TL;DR: 提出了Sim4IA-Bench，这是IR社区首个用于评估用户模拟器的基准套件，包含160个真实搜索会话和模拟器预测结果，支持下一查询预测和交互感知检索评估。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏成熟的衡量标准和基准，验证用户模拟器是否准确反映真实用户行为具有挑战性。

Method: 构建包含160个真实搜索会话的数据集，其中70个会话包含最多62次模拟器运行，分为任务A（预测下一搜索查询）和任务B（预测下一话语）。

Result: 创建了首个公开可用的基准套件，将真实搜索会话与模拟的下一查询预测联系起来，并引入了新的下一查询预测评估指标。

Conclusion: Sim4IA-Bench为评估和比较用户模拟方法提供了基础，促进了信息访问中现实和可解释用户模拟的可重复研究。

Abstract: Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR community. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simulation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query predictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.

</details>


### [17] [Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs](https://arxiv.org/abs/2511.09545)
*Etienne Dallaire*

Main category: cs.IR

TL;DR: 本文针对生产环境RAG系统的评估问题，提出了RA-nWG@K评估指标、rag-gs黄金集构建流程、端到端基准测试和针对性诊断方法，为实践者提供可复现的决策指导。


<details>
  <summary>Details</summary>
Motivation: 传统IR指标不适用于RAG场景，缺乏标准化的黄金集构建方法，现有排行榜无法反映生产环境的权衡，嵌入模型处理专有名词和对话噪声的能力不透明。

Method: 提出RA-nWG@K评估指标、PROC和%PROC操作上限、rag-gs黄金集构建流程、端到端基准测试框架，以及针对专有名词和对话噪声的诊断方法。

Result: 在科学论文语料库上进行了全面基准测试，涵盖密集检索、混合检索、嵌入模型、重排序器等多种技术，提供了实践者的帕累托指导。

Conclusion: 这些组件共同为实践者提供了可复现、预算/SLA感知的决策支持，建立了可审计的防护机制。

Abstract: This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build and audit golden sets. Third, leaderboards exist but lack end-to-end, on-corpus benchmarking that reflects production trade-offs. Fourth, how state-of-the-art embedding models handle proper-name identity signals and conversational noise remains opaque. To address these, we contribute: (1) RA-nWG@K, a rarity-aware, per-query-normalized set score, and operational ceilings via the pool-restricted oracle ceiling (PROC) and the percentage of PROC (%PROC) to separate retrieval from ordering headroom within a Cost-Latency-Quality (CLQ) lens; (2) rag-gs (MIT), a lean golden-set pipeline with Plackett-Luce listwise refinement whose iterative updates outperform single-shot LLM ranking; (3) a comprehensive benchmark on a production RAG (scientific-papers corpus) spanning dense retrieval, hybrid dense+BM25, embedding models and dimensions, cross-encoder rerankers, ANN (HNSW), and quantization; and (4) targeted diagnostics that quantify proper-name identity signal and conversational-noise sensitivity via identity-destroying and formatting ablations. Together, these components provide practitioner Pareto guidance and auditable guardrails to support reproducible, budget/SLA-aware decisions.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [18] [Formal Verification of Diffusion Auctions](https://arxiv.org/abs/2511.08765)
*Rustam Galimullin,Munyque Mittelmann,Laurent Perrussel*

Main category: cs.GT

TL;DR: 该论文提出了一个逻辑形式化框架来建模扩散拍卖中的动态和策略维度，提供了模型检查程序来验证纳什均衡等性质，并建立了计算复杂性结果。


<details>
  <summary>Details</summary>
Motivation: 扩散拍卖中卖家可以利用社交网络扩大参与度增加收入，但现有研究缺乏对形式化验证和策略推理的深入探讨。

Method: 引入逻辑形式化框架捕捉扩散动态和策略维度，提供模型检查程序验证纳什均衡等性质，建立计算复杂性分析。

Result: 开发了能够验证扩散拍卖中策略性质的形式化方法和算法，并确定了其计算复杂性。

Conclusion: 该研究填补了扩散拍卖中形式化验证和策略推理的空白，为卖家策略存在性检查提供了理论基础和实用工具。

Abstract: In diffusion auctions, sellers can leverage an underlying social network to broaden participation, thereby increasing their potential revenue. Specifically, sellers can incentivise participants in their auction to diffuse information about the auction through the network. While numerous variants of such auctions have been recently studied in the literature, the formal verification and strategic reasoning perspectives have not been investigated yet.
  Our contribution is threefold. First, we introduce a logical formalism that captures the dynamics of diffusion and its strategic dimension. Second, for such a logic, we provide model-checking procedures that allow one to verify properties as the Nash equilibrium, and that pave the way towards checking the existence of sellers' strategies. Third, we establish computational complexity results for the presented algorithms.

</details>


### [19] [Minimal Regret Walras Equilibria for Combinatorial Markets](https://arxiv.org/abs/2511.09021)
*Aloïs Duguet,Tobias Harks,Martin Schmidt,Julian Schwarz*

Main category: cs.GT

TL;DR: 提出了Δ-遗憾瓦尔拉斯均衡的概念，用于组合多物品市场，通过价格和分配实现市场出清、容量可行，且玩家策略总遗憾为Δ。建立了与对偶间隙和整数性间隙的联系，并给出了存在性条件和近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统瓦尔拉斯均衡在组合市场中可能不存在，需要研究近似均衡概念来平衡市场效率和计算可行性。

Method: 定义Δ-遗憾均衡，建立与对偶间隙和整数性间隙的理论联系，利用线性规划的敏感性理论分析可实现的遗憾界限。

Result: 为单调估值提供了完整的Δ-遗憾均衡特征化，对一般估值建立了敏感性间隙与遗憾界限的关系，并将社会福利优化近似算法转化为低遗憾均衡算法。

Conclusion: Δ-遗憾均衡为组合市场提供了实用的近似均衡概念，建立了与优化理论的内在联系，并提供了有效的计算方法和下界分析。

Abstract: We consider combinatorial multi-item markets and propose the notion of a $Δ$-regret Walras equilibrium, which is an allocation of items to players and a set of item prices that achieve the following goals: prices clear the market, the allocation is capacity-feasible, and the players' strategies lead to a total regret of $Δ$. The regret is defined as the sum of individual player regrets measured by the utility gap with respect to the optimal item bundle given the prices. We derive necessary and sufficient conditions for the existence of $Δ$-regret equilibria, where we establish a connection to the duality gap and the integrality gap of the social welfare problem. For the special case of monotone valuations, the derived necessary and sufficient optimality conditions coincide and lead to a complete characterization of achievable $Δ$-regret equilibria. For general valuations, we establish an interesting connection to the area of sensitivity theory in linear optimization. We show that the sensitivity gap of the optimal-value function of two (configuration) linear programs with changed right-hand side can be used to establish a bound on the achievable regret. Finally, we use these general structural results to translate known approximation algorithms for the social welfare optimization problem into algorithms computing low-regret Walras equilibria. We also demonstrate how to derive strong lower bounds based on integrality and duality gaps but also based on NP-complexity theory.

</details>


### [20] [Pricing Online LLM Services with Data-Calibrated Stackelberg Routing Game](https://arxiv.org/abs/2511.09062)
*Zhendong Guo,Wenchao Bai,Jiahui Jin*

Main category: cs.GT

TL;DR: PriLLM是一个用于LLM路由平台实时动态定价的实用可扩展解决方案，通过Stackelberg博弈建模服务市场，使用深度聚合网络学习提供商抽象，在保持95%以上最优利润的同时仅需5%的计算时间。


<details>
  <summary>Details</summary>
Motivation: LLM路由已成为标准服务交付机制，但最优定价需要精确建模动态服务市场，而实时大规模解决此问题在计算上不可行。

Method: 将服务市场建模为Stackelberg博弈，提供商设定价格，用户基于多标准选择服务；使用深度聚合网络学习提供商抽象以保持可扩展性；结合客观因素（成本、QoS）和主观用户偏好。

Result: 在真实数据上的实证评估显示，PriLLM实现了超过95%的最优利润，同时仅需最优解计算时间的不到5%。

Conclusion: PriLLM提供了一个实用且可扩展的实时动态定价解决方案，在竞争性LLM路由中有效平衡了计算效率和利润优化，并具有可解释性。

Abstract: The proliferation of Large Language Models (LLMs) has established LLM routing as a standard service delivery mechanism, where users select models based on cost, Quality of Service (QoS), among other things. However, optimal pricing in LLM routing platforms requires precise modeling for dynamic service markets, and solving this problem in real time at scale is computationally intractable. In this paper, we propose \PriLLM, a novel practical and scalable solution for real-time dynamic pricing in competitive LLM routing. \PriLLM models the service market as a Stackelberg game, where providers set prices and users select services based on multiple criteria. To capture real-world market dynamics, we incorporate both objective factors (\eg~cost, QoS) and subjective user preferences into the model. For scalability, we employ a deep aggregation network to learn provider abstraction that preserve user-side equilibrium behavior across pricing strategies. Moreover, \PriLLM offers interpretability by explaining its pricing decisions. Empirical evaluation on real-world data shows that \PriLLM achieves over 95\% of the optimal profit while only requiring less than 5\% of the optimal solution's computation time.

</details>


### [21] [Steering Noncooperative Games Through Conjecture Design](https://arxiv.org/abs/2511.09435)
*Francesco Morri,Hélène Le Cadre,David Salas,Didier Aussel*

Main category: cs.GT

TL;DR: 提出一个基于推测变分博弈的激励设计框架，通过协调者计算并传达目标均衡，引导玩家达到期望结果，在集中式和分散式设置下都能保证均衡存在。


<details>
  <summary>Details</summary>
Motivation: 解决动态非合作博弈中均衡多重性和不良结果的问题，传统方法如对手建模和激励设计通常是分开处理的。

Method: 协调者首先通过优化预定义目标函数计算均衡，然后将其作为目标传达给玩家。在集中式设置中优化推测，在分散式设置中玩家独立计算推测并基于个体目标更新策略。

Result: 为集中式和分散式设置提供了均衡存在性保证，能够将博弈解耦为独立优化问题，支持大规模环境下的高效计算和并行化。

Conclusion: 该框架不仅通过推测引导系统达到期望结果，还能解耦博弈问题，在经典非合作博弈中展示了应用潜力。

Abstract: In dynamic noncooperative games, each player makes conjectures about other players' reactions before choosing a strategy. However, resulting equilibria may be multiple and do not always lead to desirable outcomes. These issues are typically addressed separately, for example, through opponent modelling and incentive design. Drawing inspiration from conjectural variations games, we propose an incentive design framework in which a coordinator first computes an equilibrium by optimizing a predefined objective function, then communicates this equilibrium as a target for the players to reach. In a centralized setting, the coordinator also optimizes the conjectures to steer the players towards the target. In decentralized settings, players independently compute conjectures and update their strategies based on individual targets. We provide a guarantee of equilibrium existence in both cases. This framework uses conjectures not only to guide the system towards desirable outcomes but also to decouple the game into independent optimization problems, enabling efficient computation and parallelization in large-scale settings. We illustrate our theoretical results on classical representative noncooperative games, demonstrating its application potential.

</details>


### [22] [Understanding the Impact of Proportionality in Approval-Based Multiwinner Elections](https://arxiv.org/abs/2511.09479)
*Niclas Boehmer,Lara Glessen,Jannik Peters*

Main category: cs.GT

TL;DR: 本文通过计算复杂性分析和实验研究，揭示了比例性公理在实际选举中的限制性影响，并提出了衡量候选人对实现比例结果重要性的新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管在基于批准的多赢家投票中进行了广泛的理论研究，但比例性公理在实践中对委员会和候选人选择的影响仍知之甚少。

Method: 通过(i)分析比例性公理相关问题的计算复杂性，以及(ii)在真实世界和合成选举中进行广泛实验研究。

Result: 发现在不同实例中比例性的限制性存在显著差异，包括在部分真实案例中观察到之前未发现的高度限制性。新提出的候选重要性衡量方法与基于批准得分的评估方法明显不同。

Conclusion: 比例性公理在实际选举中具有重要且多变的影响，需要新的方法来准确评估候选人对实现比例结果的重要性。

Abstract: Despite extensive theoretical research on proportionality in approval-based multiwinner voting, its impact on which committees and candidates can be selected in practice remains poorly understood. We address this gap by (i) analyzing the computational complexity of several natural problems related to the behavior of proportionality axioms, and (ii) conducting an extensive experimental study on both real-world and synthetic elections. Our findings reveal substantial variation in the restrictiveness of proportionality across instances, including previously unobserved high levels of restrictiveness in some real-world cases. We also introduce and evaluate new measures for quantifying a candidate's importance for achieving proportional outcomes, which differ clearly from assessing candidate strength by approval score.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [23] [FlashMap: A Flash Optimized Key-Value Store](https://arxiv.org/abs/2511.08826)
*Zonglin Guo,Tony Givargis*

Main category: cs.DB

TL;DR: FlashMap是一个针对闪存固态硬盘优化的高性能键值存储系统，在数据中心级服务器上实现了出色的吞吐量性能。


<details>
  <summary>Details</summary>
Motivation: 随着现代计算对响应性和可扩展性需求的增长，键值存储已成为数据基础设施的关键组件。闪存固态硬盘的普及为优化键值存储性能提供了新的机会。

Method: 开发了FlashMap，一个专门针对闪存固态硬盘优化的键值存储系统，充分利用SSD的特性来提升性能。

Result: 实验结果显示，FlashMap在单个数据中心级服务器上实现了平均1980万次插入和2380万次随机查找每秒的吞吐量（使用100字节负载）。

Conclusion: FlashMap证明了针对闪存固态硬盘专门优化的键值存储系统能够实现卓越的性能，为高性能应用提供了有效的解决方案。

Abstract: Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.

</details>


### [24] [Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration](https://arxiv.org/abs/2511.09001)
*Yuka Haruki,Shigeru Ishikura,Kazuya Demachi,Teruaki Hayashi*

Main category: cs.DB

TL;DR: 提出了一种结合表格结构信息和上下文元素（如列描述和外部知识）的上下文图嵌入技术，用于数据集成中的模式匹配和实体解析任务，在多种数据集特性下表现优于现有图方法。


<details>
  <summary>Details</summary>
Motivation: 当前数据集成任务（如模式匹配和实体解析）需要大量人工努力，现有自动化方法对数据集特性的影响研究不足，且不同方法的组合有限。

Method: 使用上下文图嵌入技术，整合表格数据的结构细节以及列描述、外部知识等上下文元素。

Result: 在具有不同特性（领域特异性、数据大小、缺失率、重叠率）的数据集上测试，该方法持续超越现有图方法，特别是在数值比例高或缺失数据多的困难场景中。但存在语义相似但不同的列难以区分的失败案例。

Conclusion: 上下文嵌入提高了匹配可靠性，数据集特性显著影响集成结果，这些贡献可推动实用数据集成系统的发展以支持企业应用。

Abstract: As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.

</details>


### [25] [Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking](https://arxiv.org/abs/2511.09052)
*Yu Wang,Hui Wang,Jiake Ge,Xin Wang*

Main category: cs.DB

TL;DR: 提出了三个核心创新将GNN-PE扩展到分布式系统：动态负载均衡、多GPU协作缓存和查询计划优化，显著提升分布式子图匹配效率。


<details>
  <summary>Details</summary>
Motivation: 现有GNN-PE框架在单机上高效，但缺乏对分布式环境的扩展性和优化，无法满足大规模图上的精确子图匹配需求。

Method: 1) 轻量级动态负载均衡与热迁移机制；2) 在线增量学习的多GPU协作动态缓存策略；3) 基于PE-score的查询计划排序方法。结合METIS分区和并行预处理。

Result: 在数十台机器的分布式场景中实现了"最小边割+负载均衡+不间断查询"，显著提升了分布式子图匹配的效率和稳定性。

Conclusion: 提出的分布式GNN-PE扩展方案有效解决了大规模图精确子图匹配的分布式系统挑战，为分布式图分析提供了高效稳定的解决方案。

Abstract: Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves "minimum edge cut + load balancing + non-interruptible queries" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.

</details>


### [26] [CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System](https://arxiv.org/abs/2511.09262)
*Jiaping Cao,Ting Sun,Man Lung Yiu,Xiao Yan,Bo Tang*

Main category: cs.DB

TL;DR: CheetahGIS是一个基于Apache Flink Stateful Functions构建的可扩展高效系统，用于处理海量移动对象上的流式空间查询。


<details>
  <summary>Details</summary>
Motivation: 现有空间数据分析系统在处理大量移动对象和实时空间查询时存在局限性。

Method: 采用模块化架构，基于Apache Flink Stateful Functions构建，并设计了轻量级全局网格索引、元数据同步策略和负载均衡机制等优化方案。

Result: 系统具有良好的可扩展性，能够高效处理三种代表性流式查询（对象查询、范围计数查询和k近邻查询）。

Conclusion: CheetahGIS是一个可扩展且高效的系统，能够有效处理海量移动对象上的流式空间查询。

Abstract: Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS.

</details>
