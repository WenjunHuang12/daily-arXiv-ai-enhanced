<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IT](#cs.IT) [Total: 9]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Efficiently Computing Equilibria in Budget-Aggregation Games](https://arxiv.org/abs/2509.08767)
*Patrick Becker,Alexander Fries,Matthias Greger,Erel Segal-Halevi*

Main category: cs.GT

TL;DR: 本文研究预算聚合博弈，分析不同偏好模型下纳什均衡的结构和计算效率，特别是解决了Leontief效用下多项式时间求解纳什均衡的开放问题


<details>
  <summary>Details</summary>
Motivation: 预算聚合处理在给定预算下如何根据代理人偏好分配公共项目资金的问题，本文从博弈论角度研究代理人拥有部分预算决策权时的博弈场景

Method: 采用博弈论框架，分析预算聚合博弈的结构特性，研究不同偏好模型（包括Leontief效用）下纳什均衡的计算方法

Result: 证明了在Leontief效用函数下，纳什均衡可以在多项式时间内计算得到，解决了Brandt等人2023年提出的开放问题

Conclusion: 预算聚合博弈具有可计算性良好的纳什均衡，特别是在Leontief效用模型下，为实际应用提供了理论基础

Abstract: Budget aggregation deals with the social choice problem of distributing an
exogenously given budget among a set of public projects, given agents'
preferences. Taking a game-theoretic perspective, we initialize the study of
\emph{budget-aggregation games} where each agent has virtual decision power
over some fraction of the budget. This paper investigates the structure and
shows efficient computability of Nash equilibria in this setting for various
preference models. In particular, we show that Nash equilibria for Leontief
utilities can be found in polynomial time, solving an open problem from Brandt
et al. [2023].

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [Learning-Based Planning for Improving Science Return of Earth Observation Satellites](https://arxiv.org/abs/2509.07997)
*Abigail Breitfeld,Alberto Candela,Juan Delfa,Akseli Kangaslahti,Itai Zilberstein,Steve Chien,David Wettergreen*

Main category: cs.AI

TL;DR: 这篇论文提出了两种基于学习的动态目标指向方法（强化学习和模仿学习），用于最大化地球观测卫星的科学数据收集效果。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星在数据收集方面存在限制：无法轻易偏离轨道轨迹，传感器视野有限，指向和操作消耗资源。需要优化数据收集，只包含最重要或信息量最大的测量。

Method: 使用强化学习和模仿学习两种学习方法，基于动态规划解决方案来规划采样位置序列。这些方法利用卫星资源和预先仪器的数据来智能地重新配置和指向主仪器。

Result: 模仿学习比最佳惯例方法平均提高10.0%，强化学习平均提高13.7%。两种学习方法都能在较少数据量下有效训练。

Conclusion: 学习方法在动态目标指向应用中表现优异，能够显著提高地球观测卫星的科学信息收集效果。

Abstract: Earth observing satellites are powerful tools for collecting scientific
information about our planet, however they have limitations: they cannot easily
deviate from their orbital trajectories, their sensors have a limited field of
view, and pointing and operating these sensors can take a large amount of the
spacecraft's resources. It is important for these satellites to optimize the
data they collect and include only the most important or informative
measurements. Dynamic targeting is an emerging concept in which satellite
resources and data from a lookahead instrument are used to intelligently
reconfigure and point a primary instrument. Simulation studies have shown that
dynamic targeting increases the amount of scientific information gathered
versus conventional sampling strategies. In this work, we present two different
learning-based approaches to dynamic targeting, using reinforcement and
imitation learning, respectively. These learning methods build on a dynamic
programming solution to plan a sequence of sampling locations. We evaluate our
approaches against existing heuristic methods for dynamic targeting, showing
the benefits of using learning for this application. Imitation learning
performs on average 10.0\% better than the best heuristic method, while
reinforcement learning performs on average 13.7\% better. We also show that
both learning methods can be trained effectively with relatively small amounts
of data.

</details>


### [3] [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)
*Linyao Chen,Zimian Peng,Yingxuan Yang,Yikun Wang,Wenzheng Tom Tang,Hiroki H. Kobayashi,Weinan Zhang*

Main category: cs.AI

TL;DR: EnvX是一个利用Agentic AI将GitHub仓库转化为智能代理的框架，通过自然语言交互和代理间协作，自动化软件复用过程，在GitTaskBench基准测试中表现优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 开源软件仓库数量庞大但利用率低，开发者需要手动查阅文档、理解API和编写集成代码，存在效率低下和易出错的问题。

Method: 采用三阶段方法：1)TODO引导的环境初始化；2)人机对齐的代理自动化；3)代理间(A2A)协作协议，结合大语言模型能力和结构化工具集成。

Result: 在GitTaskBench基准测试的18个跨领域仓库上，EnvX达到74.07%的执行完成率和51.85%的任务通过率，优于现有框架。

Conclusion: EnvX将仓库从被动代码资源转变为智能交互代理，提升了开源生态系统的可访问性和协作性。

Abstract: The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

</details>


### [4] [Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI](https://arxiv.org/abs/2509.08151)
*Botao Zhu,Jeslyn Wang,Dusit Niyato,Xianbin Wang*

Main category: cs.AI

TL;DR: 提出基于大AI模型的师生代理架构2TSD模型，通过服务器端教师代理提取任务特定信任语义，减少设备端评估开销，提高协作设备选择效率和准确性


<details>
  <summary>Details</summary>
Motivation: 传统方法中每个任务所有者独立评估所有协作设备的可信度会导致频繁数据交换、复杂推理和动态情境变化，产生显著开销和信任评估质量下降

Method: 基于大AI模型的师生代理架构，教师代理部署在服务器端负责多维信任数据收集、任务特定信任语义提取和任务-协作设备匹配分析，学生代理接收信任语义进行快速协作选择

Result: 实验结果表明2TSD模型能够减少协作设备评估时间、降低设备资源消耗，并提高协作设备选择的准确性

Conclusion: 2TSD模型通过任务特定信任语义蒸馏有效解决了分布式信任评估中的开销和准确性问题，为复杂计算任务的协作设备选择提供了高效解决方案

Abstract: Accurate trustworthiness evaluation of potential collaborating devices is
essential for the effective execution of complex computing tasks. This
evaluation process involves collecting diverse trust-related data from
potential collaborators, including historical performance and available
resources, for collaborator selection. However, when each task owner
independently assesses all collaborators' trustworthiness, frequent data
exchange, complex reasoning, and dynamic situation changes can result in
significant overhead and deteriorated trust evaluation. To overcome these
challenges, we propose a task-specific trust semantics distillation (2TSD)
model based on a large AI model (LAM)-driven teacher-student agent
architecture. The teacher agent is deployed on a server with powerful
computational capabilities and an augmented memory module dedicated to
multidimensional trust-related data collection, task-specific trust semantics
extraction, and task-collaborator matching analysis. Upon receiving
task-specific requests from device-side student agents, the teacher agent
transfers the trust semantics of potential collaborators to the student agents,
enabling rapid and accurate collaborator selection. Experimental results
demonstrate that the proposed 2TSD model can reduce collaborator evaluation
time, decrease device resource consumption, and improve the accuracy of
collaborator selection.

</details>


### [5] [Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following](https://arxiv.org/abs/2509.08222)
*Minjong Yoo,Jinwoo Jang,Wei-jin Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了Exploratory Retrieval-Augmented Planning (ExRAP)框架，通过环境探索和上下文记忆增强LLM在动态环境中的持续指令跟随能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体在动态非平稳环境中执行持续指令跟随任务时，LLM需要有效探索物理环境并建立环境上下文记忆的问题。

Method: 采用探索增强的任务规划方案，将基于信息的探索整合到LLM规划过程中，结合记忆增强的查询评估和时间一致性精化方案。

Result: 在VirtualHome、ALFRED和CARLA等基准测试中，该方法在各种指令规模和类型、非平稳度不同的场景下表现出鲁棒性，在目标成功率和执行效率方面均优于其他最先进的LLM任务规划方法。

Conclusion: ExRAP框架通过有效平衡环境上下文记忆的有效性和环境探索负载，显著提升了具身智能体在动态环境中的持续指令跟随性能。

Abstract: This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)
framework, designed to tackle continual instruction following tasks of embodied
agents in dynamic, non-stationary environments. The framework enhances Large
Language Models' (LLMs) embodied reasoning capabilities by efficiently
exploring the physical environment and establishing the environmental context
memory, thereby effectively grounding the task planning process in time-varying
environment contexts. In ExRAP, given multiple continual instruction following
tasks, each instruction is decomposed into queries on the environmental context
memory and task executions conditioned on the query results. To efficiently
handle these multiple tasks that are performed continuously and simultaneously,
we implement an exploration-integrated task planning scheme by incorporating
the {information-based exploration} into the LLM-based planning process.
Combined with memory-augmented query evaluation, this integrated scheme not
only allows for a better balance between the validity of the environmental
context memory and the load of environment exploration, but also improves
overall task performance. Furthermore, we devise a {temporal consistency
refinement} scheme for query evaluation to address the inherent decay of
knowledge in the memory. Through experiments with VirtualHome, ALFRED, and
CARLA, our approach demonstrates robustness against a variety of embodied
instruction following scenarios involving different instruction scales and
types, and non-stationarity degrees, and it consistently outperforms other
state-of-the-art LLM-based task planning approaches in terms of both goal
success rate and execution efficiency.

</details>


### [6] [Real-world Music Plagiarism Detection With Music Segment Transcription System](https://arxiv.org/abs/2509.08282)
*Seonghyeon Go*

Main category: cs.AI

TL;DR: 通过结合多种音乐信息检索技术，提出了一种能够识别不同音乐格式中音乐剥窃的系统，并创建了公开的相似音乐对数据集。


<details>
  <summary>Details</summary>
Motivation: 随着音乐信息检索技术的发展，音乐生产和分发更加多样化和普及，保护音乐智能财产权利的需求日益增长。

Method: 开发了音乐段落转记系统，从音频录音中提取有音乐意义的段落，并基于多种音乐特征计算相似度分数。

Result: 在音乐剥窃检测实验中展现出有前景的结果，方法可应用于实际音乐场景，同时创建了公开的SMP数据集。

Conclusion: 该研究提供了一种有效的音乐剥窃检测方法，通过结合多种MIR技术实现了跨格式的音乐相似性分析，为音乐知识产权保护提供了技术支持。

Abstract: As a result of continuous advances in Music Information Retrieval (MIR)
technology, generating and distributing music has become more diverse and
accessible. In this context, interest in music intellectual property protection
is increasing to safeguard individual music copyrights. In this work, we
propose a system for detecting music plagiarism by combining various MIR
technologies. We developed a music segment transcription system that extracts
musically meaningful segments from audio recordings to detect plagiarism across
different musical formats. With this system, we compute similarity scores based
on multiple musical features that can be evaluated through comprehensive
musical analysis. Our approach demonstrated promising results in music
plagiarism detection experiments, and the proposed method can be applied to
real-world music scenarios. We also collected a Similar Music Pair (SMP)
dataset for musical similarity research using real-world cases. The dataset are
publicly available.

</details>


### [7] [Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies](https://arxiv.org/abs/2509.08312)
*Binghan Wu,Shoufeng Wang,Yunxin Liu,Ya-Qin Zhang,Joseph Sifakis,Ye Ouyang*

Main category: cs.AI

TL;DR: 本文通过实现Sifakis的自主网络代理架构，在5G RAN链路自适应中验证了认知系统的可行性，实现了6%吞吐量提升和67%误块率降低


<details>
  <summary>Details</summary>
Motivation: 填补自主网络架构理论与实际运营之间的差距，实现TM Forum提出的自配置、自修复、自优化系统愿景，达到零等待、零接触、零故障服务

Method: 采用Joseph Sifakis的AN Agent参考架构，部署协调的主动-被动运行时系统，基于混合知识表示驱动，通过RAN链路自适应代理进行实证案例研究

Result: 在5G NR sub-6 GHz中实现亚10毫秒实时控制，相比OLLA算法下行吞吐量提高6%，超可靠服务的误块率降低67%，通过动态MCS优化实现性能提升

Conclusion: 该架构证实了克服传统自主性障碍的可行性，推进了关键L4能力的发展，为下一代网络目标奠定了基础

Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a
strategic inflection point in telecommunications, where networks must transcend
reactive automation to achieve genuine cognitive capabilities--fulfilling TM
Forum's vision of self-configuring, self-healing, and self-optimizing systems
that deliver zero-wait, zero-touch, and zero-fault services. This work bridges
the gap between architectural theory and operational reality by implementing
Joseph Sifakis's AN Agent reference architecture in a functional cognitive
system, deploying coordinated proactive-reactive runtimes driven by hybrid
knowledge representation. Through an empirical case study of a Radio Access
Network (RAN) Link Adaptation (LA) Agent, we validate this framework's
transformative potential: demonstrating sub-10 ms real-time control in 5G NR
sub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link
Adaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for
ultra-reliable services through dynamic Modulation and Coding Scheme (MCS)
optimization. These improvements confirm the architecture's viability in
overcoming traditional autonomy barriers and advancing critical L4-enabling
capabilities toward next-generation objectives.

</details>


### [8] [Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives](https://arxiv.org/abs/2509.08380)
*Prathamesh Vasudeo Naik,Naresh Kumar Dintakurthi,Zhanghao Hu,Yue Wang,Robby Qiu*

Main category: cs.AI

TL;DR: Co-Investigator AI是一个基于多智能体框架的系统，专门用于生成符合监管要求的可疑活动报告(SAR)，相比传统方法显著提高了生成速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前可疑活动报告生成成本高、可扩展性差，传统大语言模型存在事实幻觉、犯罪类型对齐差和可解释性差等问题，在合规关键领域存在不可接受的风险。

Method: 采用自主智能体架构，集成了规划、犯罪类型检测、外部情报收集和合规验证等专门智能体，具有动态内存管理、AI隐私保护层和实时验证智能体等功能。

Result: 系统在多种复杂金融犯罪场景中表现出色，能够简化SAR起草流程，使报告叙述符合监管预期，让合规团队专注于更高层次的分析工作。

Conclusion: 该方法标志着合规报告新时代的开始，将AI智能体的变革性优势引入监管流程核心，为可扩展、可靠且透明的SAR生成铺平了道路。

Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

</details>


### [9] [TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making](https://arxiv.org/abs/2509.08500)
*Kechen Jiao,Zhirui Fang,Jiahao Liu,Bei Li,Qifan Wang,Xinyu Liu,Junhao Ruan,Zhongjian Qiao,Yifan Zhu,Yaxin Xu,Jingang Wang,Xiu Li*

Main category: cs.AI

TL;DR: TCPO方法通过逐步偏好优化和推理过程对齐，解决了视觉语言模型在具身决策中的模型退化问题，在ALFWorld环境中实现了26.67%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习和思维链的后SFT方法存在稀疏奖励、动作优化限制，导致样本效率低、一致性差和模型退化问题，需要更有效的具身决策对齐方法。

Method: 提出Thought-Centric Preference Optimization (TCPO)：1）逐步偏好优化将稀疏奖励转为丰富样本对；2）强调中间推理过程对齐；3）引入动作策略一致性约束(APC)确保输出一致性。

Result: 在ALFWorld环境中平均成功率26.67%，相比RL4VLM提升6%，有效缓解了微调后的模型退化问题。

Conclusion: 将基于偏好的学习技术与思维链过程结合，能有效提升视觉语言模型在具身智能体中的决策能力，TCPO方法为解决模型退化问题提供了有效方案。

Abstract: Using effective generalization capabilities of vision language models (VLMs)
in context-specific dynamic tasks for embodied artificial intelligence remains
a significant challenge. Although supervised fine-tuned models can better align
with the real physical world, they still exhibit sluggish responses and
hallucination issues in dynamically changing environments, necessitating
further alignment. Existing post-SFT methods, reliant on reinforcement learning
and chain-of-thought (CoT) approaches, are constrained by sparse rewards and
action-only optimization, resulting in low sample efficiency, poor consistency,
and model degradation. To address these issues, this paper proposes
Thought-Centric Preference Optimization (TCPO) for effective embodied
decision-making. Specifically, TCPO introduces a stepwise preference-based
optimization approach, transforming sparse reward signals into richer step
sample pairs. It emphasizes the alignment of the model's intermediate reasoning
process, mitigating the problem of model degradation. Moreover, by
incorporating Action Policy Consistency Constraint (APC), it further imposes
consistency constraints on the model output. Experiments in the ALFWorld
environment demonstrate an average success rate of 26.67%, achieving a 6%
improvement over RL4VLM and validating the effectiveness of our approach in
mitigating model degradation after fine-tuning. These results highlight the
potential of integrating preference-based learning techniques with CoT
processes to enhance the decision-making capabilities of vision-language models
in embodied agents.

</details>


### [10] [No-Knowledge Alarms for Misaligned LLMs-as-Judges](https://arxiv.org/abs/2509.08593)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 通过观察LLM判官之间的逻辑一致性，使用线性规划计算判官能力，开发无偏差告警检测错误判断


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为判官评估其他LLM时的监督问题，避免无限监控链条和不可信任专家判断的困境

Method: 利用不同LLM判官之间的逻辑一致性，将其形式化为线性规划问题，在有限测试的整数响应计数空间中进行计算

Result: 开发出无偏差告警系统，可以在满足用户指定评分要求的前提下，以零偏差检测出至少一个判官成员的错误判断

Conclusion: 通过逻辑一致性分析可以有效监控LLM判官的评估质量，为构建可靠的自动化评估系统提供了新的解决方案

Abstract: If we use LLMs as judges to evaluate the complex decisions of other LLMs, who
or what monitors the judges? Infinite monitoring chains are inevitable whenever
we do not know the ground truth of the decisions by experts and we do not want
to trust them. One way to ameliorate our evaluation uncertainty is to exploit
the use of logical consistency between disagreeing experts. By observing how
LLM judges agree and disagree while grading other LLMs, we can compute the only
possible evaluations of their grading ability. For example, if two LLM judges
disagree on which tasks a third one completed correctly, they cannot both be
100\% correct in their judgments. This logic can be formalized as a Linear
Programming problem in the space of integer response counts for any finite
test. We use it here to develop no-knowledge alarms for misaligned LLM judges.
The alarms can detect, with no false positives, that at least one member or
more of an ensemble of judges are violating a user specified grading ability
requirement.

</details>


### [11] [Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference](https://arxiv.org/abs/2509.08682)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Jiawei Shen,Jingjiang Liu,Yidan Liang*

Main category: cs.AI

TL;DR: 提出了首个基于多粒度因果推理的多智能体系统故障归因框架，通过性能因果反转原理和因果发现算法CDC-MAS，显著提高了故障定位准确率，并实现了自动化优化循环。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在实际部署中面临故障归因的挑战，现有基于统计相关性的诊断工具效果不佳，在Who&When基准上准确率不足15%，亟需更有效的解决方案。

Method: 1) 性能因果反转原理：通过反转执行日志中的数据流正确建模性能依赖关系，结合Shapley值精确分配智能体级责任；2) CDC-MAS因果发现算法：针对MAS交互数据的非平稳特性，鲁棒地识别关键故障步骤。

Result: 在Who&When和TRAIL基准测试中表现显著提升，步骤级准确率达到36.2%，生成的优化方案平均提升任务成功率22.4%。

Conclusion: 该工作为调试复杂智能体交互提供了原则性和有效的解决方案，为更可靠和可解释的多智能体系统铺平了道路。

Abstract: Multi-agent systems (MAS) are critical for automating complex tasks, yet
their practical deployment is severely hampered by the challenge of failure
attribution. Current diagnostic tools, which rely on statistical correlations,
are fundamentally inadequate; on challenging benchmarks like Who\&When,
state-of-the-art methods achieve less than 15\% accuracy in locating the
root-cause step of a failure. To address this critical gap, we introduce the
first failure attribution framework for MAS grounded in multi-granularity
causal inference. Our approach makes two key technical contributions: (1) a
performance causal inversion principle, which correctly models performance
dependencies by reversing the data flow in execution logs, combined with
Shapley values to accurately assign agent-level blame; (2) a novel causal
discovery algorithm, CDC-MAS, that robustly identifies critical failure steps
by tackling the non-stationary nature of MAS interaction data. The framework's
attribution results directly fuel an automated optimization loop, generating
targeted suggestions whose efficacy is validated via counterfactual
simulations. Evaluations on the Who\&When and TRAIL benchmarks demonstrate a
significant leap in performance. Our method achieves up to 36.2\% step-level
accuracy. Crucially, the generated optimizations boost overall task success
rates by an average of 22.4\%. This work provides a principled and effective
solution for debugging complex agent interactions, paving the way for more
reliable and interpretable multi-agent systems.

</details>


### [12] [One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases](https://arxiv.org/abs/2509.08705)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 基于双系统理论的心理理论框架，结合图卷积网络和元学习技术，实现了直觉性和思考性的动态平衡理解。


<details>
  <summary>Details</summary>
Motivation: 为了建立一种能够模仿人类双重过程思维的心理理论框架，并解释认知偏见的机制，以推动人巧化社交认知的AI系统发展。

Method: 使用图卷积网络(GCNs)实现快速习惯性图基理解系络(System 1)，通过元学习技术实现慢速上下文敏感的元适应学习系络(System 2)，通过学习的上下文门机制动态平衡两种理解。

Result: 在标准错误信念任务中验证框架，能够复现与双系统理论相关的典型认知偏见，包括错头效应、认知负荷疲劳、框架效应和启动效应，并在未见上下文中实现健壮的泛化。

Conclusion: 该双系统方法能够精确模仿人类适应性行为，为理解认知偏见的机制提供了见解，在人巧化社交认矣和适应性决策AI系统方面有重要意义。

Abstract: We introduce a novel Theory of Mind (ToM) framework inspired by dual-process
theories from cognitive science, integrating a fast, habitual graph-based
reasoning system (System 1), implemented via graph convolutional networks
(GCNs), and a slower, context-sensitive meta-adaptive learning system (System
2), driven by meta-learning techniques. Our model dynamically balances
intuitive and deliberative reasoning through a learned context gate mechanism.
We validate our architecture on canonical false-belief tasks and systematically
explore its capacity to replicate hallmark cognitive biases associated with
dual-process theory, including anchoring, cognitive-load fatigue, framing
effects, and priming effects. Experimental results demonstrate that our
dual-process approach closely mirrors human adaptive behavior, achieves robust
generalization to unseen contexts, and elucidates cognitive mechanisms
underlying reasoning biases. This work bridges artificial intelligence and
cognitive theory, paving the way for AI systems exhibiting nuanced, human-like
social cognition and adaptive decision-making capabilities.

</details>


### [13] [The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems](https://arxiv.org/abs/2509.08713)
*Ziming Luo,Atoosa Kasirzadeh,Nihar B. Shah*

Main category: cs.AI

TL;DR: 研究发现当代AI科学家系统存在四种漏洞模式：基准选择不当、数据泄漏、指标欠使用和后验偏差，通过实验验证了这些风险，建议提交跟踪日志和代码以确保透明性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 自主AI科学家系统有助于加速科学发现，但其内部工作流程缺乏详细检查，可能引入缺陷影响研究输出的整体性、可靠性和可信过性。

Method: 设计控制实验来隔离四种漏洞模式，对两个主流开源AI科学家系统进行评估，分析完整自动化工作流程的跟踪日志和代码。

Result: 在实际系统中发现了多种严重程度不一的失败模式，这些问题在实践中容易被忽视。通过跟踪日志和代码识别失败的效果远超过仅检查最终论文。

Conclusion: 建议期刊和会议要求在提交AI生成研究时同时提供完整工作流程的跟踪日志和代码，以确保透明性、负责性和可复现性。

Abstract: AI scientist systems, capable of autonomously executing the full research
workflow from hypothesis generation and experimentation to paper writing, hold
significant potential for accelerating scientific discovery. However, the
internal workflow of these systems have not been closely examined. This lack of
scrutiny poses a risk of introducing flaws that could undermine the integrity,
reliability, and trustworthiness of their research outputs. In this paper, we
identify four potential failure modes in contemporary AI scientist systems:
inappropriate benchmark selection, data leakage, metric misuse, and post-hoc
selection bias. To examine these risks, we design controlled experiments that
isolate each failure mode while addressing challenges unique to evaluating AI
scientist systems. Our assessment of two prominent open-source AI scientist
systems reveals the presence of several failures, across a spectrum of
severity, which can be easily overlooked in practice. Finally, we demonstrate
that access to trace logs and code from the full automated workflow enables far
more effective detection of such failures than examining the final paper alone.
We thus recommend journals and conferences evaluating AI-generated research to
mandate submission of these artifacts alongside the paper to ensure
transparency, accountability, and reproducibility.

</details>


### [14] [Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making](https://arxiv.org/abs/2509.08785)
*Anup Tuladhar,Araz Minhas,Adam Kirton,Eli Kinney-Lang*

Main category: cs.AI

TL;DR: 一个基于双系统架构的实验平台，通过结合强化学习和语言模型推理，探索故事元素对AI决策的影响


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统现在既能做决策又能进行故事推理，但这两种能力主要是分开研究的，需要架设平台来探索故事框架如何影响奖励基于学习的决策

Method: 采用双系统架构：一个强化学习策略根据历史经验建议行动，另一个语言模型通过不同故事框架处理这些建议来指导决策，在可配置的gridworld环境中实现

Result: 开发了一个模块化设计的实验平台，能够控制测试环境复杂性、故事参数以及强化学习与故事基于决策的交互作用，并通过日志系统捕获基本决策指标

Conclusion: 这个预初实现为研究不同故事框架如何影响奖励基于决策提供了基础，同时也为探索优化基于学习和符号推理在AI系统中的潜在交互作用创造了条件

Abstract: We present a preliminary experimental platform that explores how narrative
elements might shape AI decision-making by combining reinforcement learning
(RL) with language model reasoning. While AI systems can now both make
decisions and engage in narrative reasoning, these capabilities have mostly
been studied separately. Our platform attempts to bridge this gap using a
dual-system architecture to examine how narrative frameworks could influence
reward-based learning. The system comprises a reinforcement learning policy
that suggests actions based on past experience, and a language model that
processes these suggestions through different narrative frameworks to guide
decisions. This setup enables initial experimentation with narrative elements
while maintaining consistent environment and reward structures. We implement
this architecture in a configurable gridworld environment, where agents receive
both policy suggestions and information about their surroundings. The
platform's modular design facilitates controlled testing of environmental
complexity, narrative parameters, and the interaction between reinforcement
learning and narrative-based decisions. Our logging system captures basic
decision metrics, from RL policy values to language model reasoning to action
selection patterns. While preliminary, this implementation provides a
foundation for studying how different narrative frameworks might affect
reward-based decisions and exploring potential interactions between
optimization-based learning and symbolic reasoning in AI systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [15] [A Dynamic, Self-balancing k-d Tree](https://arxiv.org/abs/2509.08148)
*Russell A. Brown*

Main category: cs.DS

TL;DR: 本文提出了动态k-d树的自平衡插入和删除算法，解决了传统k-d树无法使用旋转平衡技术的问题


<details>
  <summary>Details</summary>
Motivation: 传统的k-d树无法使用AVL树或红黑树等旋转平衡技术，因为旋转会破坏k-d树的排序顺序，因此通常需要批量构建静态k-d树。本文旨在开发动态k-d树的自平衡机制

Method: 设计了动态k-d树的插入和删除算法，使树在每次插入或删除操作后能够自我平衡

Result: 实现了动态k-d树的自平衡功能，并对其性能进行了测量评估

Conclusion: 成功开发了动态k-d树的自平衡算法，解决了传统k-d树无法动态平衡的问题，为k-d树的实际应用提供了更好的灵活性

Abstract: The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree, because these techniques involve cyclic exchange (aka
rotation) of tree nodes, which destroys the sorted order of the k-d tree. For
this reason, a static k-d tree is often built from all of the k-dimensional
data en masse. However, it is possible to build a dynamic k-d tree that
self-balances when necessary after insertion or deletion of each individual
k-dimensional datum. This article describes insertion and deletion algorithms
for a dynamic k-d tree, and measures their performance.

</details>


### [16] [Enumeration kernels for Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2509.08475)
*Marin Bougeret,Guilherme C. M. Gomes,Vinicius F. dos Santos,Ignasi Sau*

Main category: cs.DS

TL;DR: 本文提出了枚举核化的新方法，为Enum Vertex Cover问题开发了2k顶点的多项式延迟枚举核，为Enum Feedback Vertex Set问题开发了O(k³)顶点和边的枚举核，改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 枚举核化是参数化复杂性和枚举算法交叉的新兴领域，但在Vertex Cover和Feedback Vertex Set等经典问题的枚举版本中进展有限，需要填补这一空白。

Method: 对于Enum Vertex Cover，开发了基于crown分解归约规则的非平凡提升算法；对于Enum Feedback Vertex Set，受Thomassé思想的启发，应用了q-expansion技术。

Result: 获得了Enum Vertex Cover的2k顶点多项式延迟枚举核（改进了之前的O(k²)结果），以及Enum Feedback Vertex Set的O(k³)顶点和边的枚举核。

Conclusion: 成功解决了枚举核化领域在经典问题上的短板，为Vertex Cover和Feedback Vertex Set提供了高效的枚举核化算法，推动了该领域的发展。

Abstract: Enumerative kernelization is a recent and promising area sitting at the
intersection of parameterized complexity and enumeration algorithms. Its study
began with the paper of Creignou et al. [Theory Comput. Syst., 2017], and
development in the area has started to accelerate with the work of Golovach et
al. [J. Comput. Syst. Sci., 2022]. The latter introduced polynomial-delay
enumeration kernels and applied them in the study of structural
parameterizations of the \textsc{Matching Cut} problem and some variants. Few
other results, mostly on \textsc{Longest Path} and some generalizations of
\textsc{Matching Cut}, have also been developed. However, little success has
been seen in enumeration versions of \textsc{Vertex Cover} and \textsc{Feedback
Vertex Set}, some of the most studied problems in kernelization. In this paper,
we address this shortcoming. Our first result is a polynomial-delay enumeration
kernel with $2k$ vertices for \textsc{Enum Vertex Cover}, where we wish to list
all solutions with at most $k$ vertices. This is obtained by developing a
non-trivial lifting algorithm for the classical crown decomposition reduction
rule, and directly improves upon the kernel with $\mathcal{O}(k^2)$ vertices
derived from the work of Creignou et al. Our other result is a polynomial-delay
enumeration kernel with $\mathcal{O}(k^3)$ vertices and edges for \textsc{Enum
Feedback Vertex Set}; the proof is inspired by some ideas of Thomass\'e [TALG,
2010], but with a weaker bound on the kernel size due to difficulties in
applying the $q$-expansion technique.

</details>


### [17] [Checking and producing word attractors](https://arxiv.org/abs/2509.08503)
*Marie-Pierre Béal,Maxime Crochemore,Giuseppe Romana*

Main category: cs.DS

TL;DR: 本文提出了两个基于后缀自动机或有向无环词图的组合算法，用于判断和生成词吸引子，解决文本压缩效率相关的问题


<details>
  <summary>Details</summary>
Motivation: 词吸引子与文本压缩效率密切相关，但相关算法研究有限，需要高效的方法来判断和生成吸引子

Method: 提出了两个组合算法：1）线性时间判断位置集是否为词吸引子的算法；2）贪心方式生成词吸引子的算法

Result: 第一个算法能在线性时间内完成判断，第二个算法虽然问题是NP难的，但效率高且能为多个知名词族生成很小的吸引子

Conclusion: 基于后缀自动机和有向无环词图的组合算法能有效解决词吸引子的判断和生成问题，为文本压缩提供了实用工具

Abstract: The article focuses on word (or string) attractors, which are sets of
positions related to the text compression efficiency of the underlying word.
The article presents two combinatorial algorithms based on Suffix automata or
Directed Acyclic Word Graphs. The first algorithm decides in linear time
whether a set of positions on the word is an attractor of the word. The second
algorithm generates an attractor for a given word in a greedy manner. Although
this problem is NP-hard, the algorithm is efficient and produces very small
attractors for several well-known families of words.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization](https://arxiv.org/abs/2509.07993)
*Federico Fontana,Anxhelo Diko,Romeo Lanzino,Marco Raoul Marini,Bachir Kaddar,Gian Luca Foresti,Luigi Cinque*

Main category: cs.LG

TL;DR: 该论文将深度伪造检测重新定义为持续学习问题，提出了一个高效框架来增量适应新兴视觉操纵技术，同时保留对过去生成器的知识。


<details>
  <summary>Details</summary>
Motivation: 深度伪造生成技术的快速演进对检测系统提出了严峻挑战，非持续学习方法需要频繁且昂贵的重新训练。

Method: 提出了一个持续学习框架，模拟真实世界中深度伪造技术的时间演化过程，基于轻量级视觉骨干网络实现实时性能，并引入了两个新评估指标。

Result: 实验表明框架能够实现高效适应（比完全重新训练快155倍）和稳健的历史知识保留，但对未来生成器的泛化能力接近随机水平（FWT-AUC≈0.5）。

Conclusion: 提出了非通用深度伪造分布假说，指出现有方法对未来生成器的泛化能力有限，每个生成器都有独特的特征印记。

Abstract: The rapid evolution of deepfake generation technologies poses critical
challenges for detection systems, as non-continual learning methods demand
frequent and expensive retraining. We reframe deepfake detection (DFD) as a
Continual Learning (CL) problem, proposing an efficient framework that
incrementally adapts to emerging visual manipulation techniques while retaining
knowledge of past generators. Our framework, unlike prior approaches that rely
on unreal simulation sequences, simulates the real-world chronological
evolution of deepfake technologies in extended periods across 7 years.
Simultaneously, our framework builds upon lightweight visual backbones to allow
for the real-time performance of DFD systems. Additionally, we contribute two
novel metrics: Continual AUC (C-AUC) for historical performance and Forward
Transfer AUC (FWT-AUC) for future generalization. Through extensive
experimentation (over 600 simulations), we empirically demonstrate that while
efficient adaptation (+155 times faster than full retraining) and robust
retention of historical knowledge is possible, the generalization of current
approaches to future generators without additional training remains near-random
(FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing
generator. Such observations are the foundation of our newly proposed
Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}

</details>


### [19] [How Far Are We from True Unlearnability?](https://arxiv.org/abs/2509.08058)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.LG

TL;DR: 本文研究了解学习不可能示例(UEs)的跨任务性能力，发现现有方法在多任务场景下无法完全阻止模型学习，并从损失函数角度提出了量化评估方法。


<details>
  <summary>Details</summary>
Motivation: 资料所有者需要保护自己的数据不被未经授权用于模型训练，但现有的解学习不可能示例方法在跨任务场景下效果不佳。

Method: 从模型优化角度分析，通过观察清洁和毒化模型的收敛过程和损失函数形状，提出了效果度边界感知(SAL)来量化参数的解学习性，并基于此构建了解学习距离(UD)来评估数据的解学习性。

Result: 在Taskonomy多任务数据集上进行测试，发现现有解学习不可能方法在语义分割等任务上效果不佳，并通过UD指标对主流方法进行了基准测试。

Conclusion: 现有解学习不可能方法还未达到真正的跨任务解学习性，需要更深入研究损失函数形状与解学习性的关系，本文提出的SAL和UD方法为评估解学习性提供了新的视角。

Abstract: High-quality data plays an indispensable role in the era of large models, but
the use of unauthorized data for model training greatly damages the interests
of data owners. To overcome this threat, several unlearnable methods have been
proposed, which generate unlearnable examples (UEs) by compromising the
training availability of data. Clearly, due to unknown training purposes and
the powerful representation learning capabilities of existing models, these
data are expected to be unlearnable for models across multiple tasks, i.e.,
they will not help improve the model's performance. However, unexpectedly, we
find that on the multi-task dataset Taskonomy, UEs still perform well in tasks
such as semantic segmentation, failing to exhibit cross-task unlearnability.
This phenomenon leads us to question: How far are we from attaining truly
unlearnable examples? We attempt to answer this question from the perspective
of model optimization. To this end, we observe the difference in the
convergence process between clean and poisoned models using a simple model
architecture. Subsequently, from the loss landscape we find that only a part of
the critical parameter optimization paths show significant differences,
implying a close relationship between the loss landscape and unlearnability.
Consequently, we employ the loss landscape to explain the underlying reasons
for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the
unlearnability of parameters based on this explanation. Furthermore, we propose
an Unlearnable Distance (UD) to measure the unlearnability of data based on the
SAL distribution of parameters in clean and poisoned models. Finally, we
conduct benchmark tests on mainstream unlearnable methods using the proposed
UD, aiming to promote community awareness of the capability boundaries of
existing unlearnable methods.

</details>


### [20] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

TL;DR: JEL是一个新颖的计算高效端到端多神经网络实体链接模型，在性能上超越了当前最先进模型


<details>
  <summary>Details</summary>
Motivation: 知识图谱在整合多源异构数据和捕获实体关系方面具有重要价值，但需要将文本中的提及正确链接到知识图谱中的实体。实体链接是自然语言处理的基础任务，在新闻分析等应用场景中具有重要商业价值，摩根大通每年花费超过200万美元用于外部供应商成本

Method: 提出JEL模型，采用多神经网络架构，实现端到端的计算高效实体链接

Result: JEL模型在性能上超越了当前最先进的实体链接模型

Conclusion: JEL模型为实体链接任务提供了一个高效且性能优越的解决方案，特别适用于新闻分析等需要将非结构化文本与知识图谱连接的应用场景

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>


### [21] [Performance Assessment Strategies for Generative AI Applications in Healthcare](https://arxiv.org/abs/2509.08087)
*Victor Garcia,Mariia Sidulova,Aldo Badano*

Main category: cs.LG

TL;DR: 这篇论文讨论了医疗健康领域中生成式AI应用的评估方法，指出传统量化指标的局限性，并提出结合人类专业知识和计算模型的评估策略。


<details>
  <summary>Details</summary>
Motivation: 因为生成式AI在医疗领域应用日益普及，需要全面评估其在真实临床环境中的性能表现，而传统的量化指标存在过拟合和普适性不足的问题。

Method: 论文讨论了当前最先进的评估方法，包括利用人类专业知识和使用成本效益高的计算模型作为评估器的策略。

Result: 文章展现了现有评估方法的局限性，并提出了更全面的评估框架，以确保生成式AI在医疗应用中的可靠性和普适性。

Conclusion: 生成式AI在医疗领域的评估需要结合专业知识和计算模型，避免传统量化指标的局限性，以便在真实临床环境中进行更有效的性能评估。

Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm
within artificial intelligence, with applications throughout the medical
enterprise. Assessing GenAI applications necessitates a comprehensive
understanding of the clinical task and awareness of the variability in
performance when implemented in actual clinical environments. Presently, a
prevalent method for evaluating the performance of generative models relies on
quantitative benchmarks. Such benchmarks have limitations and may suffer from
train-to-the-test overfitting, optimizing performance for a specified test set
at the cost of generalizability across other task and data distributions.
Evaluation strategies leveraging human expertise and utilizing cost-effective
computational models as evaluators are gaining interest. We discuss current
state-of-the-art methodologies for assessing the performance of GenAI
applications in healthcare and medical devices.

</details>


### [22] [Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning](https://arxiv.org/abs/2509.08089)
*Lucas Fenaux,Zheng Wang,Jacob Yan,Nathan Chung,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习后门攻击防御方法Hammer and Anvil，通过结合两种正交防御机制来对抗自适应攻击者，其中Krum+防御在实验中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的分布式特性使得恶意客户端能够发起后门攻击，现有防御方法在面对自适应攻击者时效果有限，特别是对抗这种强大但尚未充分探索的攻击类别。

Method: 首先设计了一种超越现有攻击能力的新型自适应攻击者，然后提出了Hammer and Anvil防御框架，将两种基于不同原理的防御方法相结合，形成组合防御策略。

Result: 新型攻击仅需1-2个恶意客户端（共20个）就能突破现有最先进防御；而提出的Krum+组合防御能够成功抵御新型自适应攻击和现有最先进攻击。

Conclusion: 通过正交防御原理的组合，Hammer and Anvil框架能够提供有效的后门攻击防御，特别是Krum+防御在面对强大自适应攻击时表现出色。

Abstract: Federated Learning is a distributed learning technique in which multiple
clients cooperate to train a machine learning model. Distributed settings
facilitate backdoor attacks by malicious clients, who can embed malicious
behaviors into the model during their participation in the training process.
These malicious behaviors are activated during inference by a specific trigger.
No defense against backdoor attacks has stood the test of time, especially
against adaptive attackers, a powerful but not fully explored category of
attackers. In this work, we first devise a new adaptive adversary that
surpasses existing adversaries in capabilities, yielding attacks that only
require one or two malicious clients out of 20 to break existing
state-of-the-art defenses. Then, we present Hammer and Anvil, a principled
defense approach that combines two defenses orthogonal in their underlying
principle to produce a combined defense that, given the right set of
parameters, must succeed against any attack. We show that our best combined
defense, Krum+, is successful against our new adaptive adversary and
state-of-the-art attacks.

</details>


### [23] [Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography](https://arxiv.org/abs/2509.08116)
*Nooshin Maghsoodi,Sarah Nassar,Paul F R Wilson,Minh Nguyen Nhat To,Sophia Mannina,Shamel Addas,Stephanie Sibley,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: PhysioCLR是一种生理学感知的对比学习框架，通过整合ECG生理相似性线索和特定增强技术，显著提升心电图心律失常分类的泛化能力和临床相关性


<details>
  <summary>Details</summary>
Motivation: 解决AI心电图分析中标注数据有限的问题，通过自监督学习利用大规模未标注数据，同时增强模型的临床相关性和泛化能力

Method: 提出PhysioCLR框架，在对比学习中整合ECG生理相似性线索，使用ECG特异性增强技术保持类别不变性，并采用混合损失函数优化表示质量

Result: 在三个数据集上测试，PhysioCLR相比最强基线平均AUROC提升12%，展现出强大的跨数据集泛化能力

Conclusion: 通过将生理学知识嵌入对比学习，PhysioCLR能够学习具有临床意义且可迁移的ECG特征，为更有效和标签高效的心电图诊断提供了有前景的路径

Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart
conditions; however, the effectiveness of artificial intelligence (AI)-based
ECG analysis is often hindered by the limited availability of labeled data.
Self-supervised learning (SSL) can address this by leveraging large-scale
unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning
Representation for ECG), a physiology-aware contrastive learning framework that
incorporates domain-specific priors to enhance the generalizability and
clinical relevance of ECG-based arrhythmia classification. Methods: During
pretraining, PhysioCLR learns to bring together embeddings of samples that
share similar clinically relevant features while pushing apart those that are
dissimilar. Unlike existing methods, our method integrates ECG physiological
similarity cues into contrastive learning, promoting the learning of clinically
meaningful representations. Additionally, we introduce ECG- specific
augmentations that preserve the ECG category post augmentation and propose a
hybrid loss function to further refine the quality of learned representations.
Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,
for multilabel ECG diagnoses, as well as a private ICU dataset labeled for
binary classification. Across the Chapman, Georgia, and private cohorts,
PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,
underscoring its robust cross-dataset generalization. Conclusion: By embedding
physiological knowledge into contrastive learning, PhysioCLR enables the model
to learn clinically meaningful and transferable ECG eatures. Significance:
PhysioCLR demonstrates the potential of physiology-informed SSL to offer a
promising path toward more effective and label-efficient ECG diagnostics.

</details>


### [24] [Optimization Methods and Software for Federated Learning](https://arxiv.org/abs/2509.08120)
*Konstantin Burlachenko*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习的五大关键挑战（数据设备异构性、通信问题、隐私保护等），提出了新颖解决方案，并建立了理论与实践之间的桥梁


<details>
  <summary>Details</summary>
Motivation: 联邦学习在去中心化环境中面临独特挑战，现有理论研究需要多样化的实践实现来提升现实世界适用性

Method: 识别联邦学习中的五个关键挑战，并提出新的算法和系统方法来解决这些挑战，同时注重理论到实践的转化

Result: 提出了先进的联邦学习算法和系统，成功连接了理论进展与实际应用，为研究者提供了理论方法向高效现实世界实现的指导

Conclusion: 该研究不仅推动了联邦学习技术的发展，更重要的是建立了理论与实践之间的双向桥梁，通过实践视角揭示了算法的新维度

Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning
paradigm where multiple clients, such as mobile devices, collaborate to solve
machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al.
(2016a,b); McMahan et al. (2017), FL has gained further attention through its
inclusion in the National AI Research and Development Strategic Plan (2023
Update) of the United States (Science and on Artificial Intelligence, 2023).
The FL training process is inherently decentralized and often takes place in
less controlled settings compared to data centers, posing unique challenges
distinct from those in fully controlled environments. In this thesis, we
identify five key challenges in Federated Learning and propose novel approaches
to address them. These challenges arise from the heterogeneity of data and
devices, communication issues, and privacy concerns for clients in FL training.
Moreover, even well-established theoretical advances in FL require diverse
forms of practical implementation to enhance their real-world applicability.
Our contributions advance FL algorithms and systems, bridging theoretical
advancements and practical implementations. More broadly, our work serves as a
guide for researchers navigating the complexities of translating theoretical
methods into efficient real-world implementations and software. Additionally,
it offers insights into the reverse process of adapting practical
implementation aspects back into theoretical algorithm design. This reverse
process is particularly intriguing, as the practical perspective compels us to
examine the underlying mechanics and flexibilities of algorithms more deeply,
often uncovering new dimensions of the algorithms under study.

</details>


### [25] [In-Context Learning Enhanced Credibility Transformer](https://arxiv.org/abs/2509.08122)
*Kishan Padayachy,Ronald Richman,Salvatore Scognamiglio,Mario V. Wüthrich*

Main category: cs.LG

TL;DR: 本文提出了一种基于Credibility Transformer的新范式，通过添加上下文学习机制来增强模型性能，能够处理训练时未见过的新类别特征。


<details>
  <summary>Details</summary>
Motivation: 扩展传统Transformer架构，通过可信度机制和上下文学习来提高模型的学习能力和预测性能，特别是处理训练数据中未出现的新类别特征。

Method: 在Credibility Transformer基础上增加上下文学习机制，使用相似实例组成的上下文批次来增强CLS令牌表示，通过额外上下文信息和微调来改进模型。

Result: 经验验证表明，上下文学习通过适应相似风险模式提高了预测准确性，并且能够泛化到训练时未见过的新类别特征实例。

Conclusion: 提出的上下文学习范式有效增强了Credibility Transformer的性能，使其能够处理新出现的类别特征，具有很好的实用价值。

Abstract: The starting point of our network architecture is the Credibility Transformer
which extends the classical Transformer architecture by a credibility mechanism
to improve model learning and predictive performance. This Credibility
Transformer learns credibilitized CLS tokens that serve as learned
representations of the original input features. In this paper we present a new
paradigm that augments this architecture by an in-context learning mechanism,
i.e., we increase the information set by a context batch consisting of similar
instances. This allows the model to enhance the CLS token representations of
the instances by additional in-context information and fine-tuning. We
empirically verify that this in-context learning enhances predictive accuracy
by adapting to similar risk patterns. Moreover, this in-context learning also
allows the model to generalize to new instances which, e.g., have feature
levels in the categorical covariates that have not been present when the model
was trained -- for a relevant example, think of a new vehicle model which has
just been developed by a car manufacturer.

</details>


### [26] [torchmil: A PyTorch-based library for deep Multiple Instance Learning](https://arxiv.org/abs/2509.08129)
*Francisco M. Castro-Macías,Francisco J. Sáez-Maldonado,Pablo Morales-Álvarez,Rafael Molina*

Main category: cs.LG

TL;DR: torchmil是一个基于PyTorch的开源Python库，为多示例学习(MIL)提供统一的模块化框架，包含标准数据格式、基准数据集和模型，旨在促进MIL领域的可重复性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度MIL方法日益受到关注，但该领域缺乏标准化的模型开发、评估和比较工具，这阻碍了研究的可重复性和可访问性。

Method: 开发基于PyTorch的torchmil库，提供MIL模型的基本构建模块、标准化数据格式、精选的基准数据集和模型集合，以及全面的文档和教程。

Result: 成功创建了一个统一、模块化且可扩展的MIL框架，支持从业者和研究人员快速开发和应用MIL模型。

Conclusion: torchmil将加速MIL领域的进展，降低新用户的入门门槛，促进该领域的标准化和可重复性研究。

Abstract: Multiple Instance Learning (MIL) is a powerful framework for weakly
supervised learning, particularly useful when fine-grained annotations are
unavailable. Despite growing interest in deep MIL methods, the field lacks
standardized tools for model development, evaluation, and comparison, which
hinders reproducibility and accessibility. To address this, we present
torchmil, an open-source Python library built on PyTorch. torchmil offers a
unified, modular, and extensible framework, featuring basic building blocks for
MIL models, a standardized data format, and a curated collection of benchmark
datasets and models. The library includes comprehensive documentation and
tutorials to support both practitioners and researchers. torchmil aims to
accelerate progress in MIL and lower the entry barrier for new users. Available
at https://torchmil.readthedocs.io.

</details>


### [27] [Perfectly-Private Analog Secure Aggregation in Federated Learning](https://arxiv.org/abs/2509.08683)
*Delio Jaramillo-Velez,Charul Rajput,Ragnar Freij-Hollanti,Camilla Hollanti,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于托秘的新题安全聚合方法，解决了有限域方法的精度复杂度交换问题，在保持完美隐私的同时提高了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中安全聚合的隐私泄漏问题，有限域方法存在精度和复杂度的交换问题，需要找到新的解决方案。

Method: 使用托秘而非有限域进行安全参数聚合，利用托秘上的均匀分布保证完美隐私，避免固定点模算算术导致的精度损失。

Result: 实验结果显示，新协议在保持完美隐私的同时，性能与无安全聚合模型相似，在某些情况下在模型准确性和余弦相似度方面显著超过有限域安全聚合。

Conclusion: 基于托秘的安全聚合协议是更安全的选择，能够在保证完美隐私的前提下提高模型准确性，解决了有限域方法的精度复杂度交换问题。

Abstract: In federated learning, multiple parties train models locally and share their
parameters with a central server, which aggregates them to update a global
model. To address the risk of exposing sensitive data through local models,
secure aggregation via secure multiparty computation has been proposed to
enhance privacy. At the same time, perfect privacy can only be achieved by a
uniform distribution of the masked local models to be aggregated. This raises a
problem when working with real valued data, as there is no measure on the reals
that is invariant under the masking operation, and hence information leakage is
bound to occur. Shifting the data to a finite field circumvents this problem,
but as a downside runs into an inherent accuracy complexity tradeoff issue due
to fixed point modular arithmetic as opposed to floating point numbers that can
simultaneously handle numbers of varying magnitudes. In this paper, a novel
secure parameter aggregation method is proposed that employs the torus rather
than a finite field. This approach guarantees perfect privacy for each party's
data by utilizing the uniform distribution on the torus, while avoiding
accuracy losses. Experimental results show that the new protocol performs
similarly to the model without secure aggregation while maintaining perfect
privacy. Compared to the finite field secure aggregation, the torus-based
protocol can in some cases significantly outperform it in terms of model
accuracy and cosine similarity, hence making it a safer choice.

</details>


### [28] [From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital](https://arxiv.org/abs/2509.08140)
*Mihir Kumar,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Afriyie Kwesi Samuel,Fuat Alican,Yigit Ihlamur*

Main category: cs.LG

TL;DR: 提出一个结合大语言模型和多模型机器学习架构的框架，用于预测罕见高影响事件，在风险投资领域实现高精度预测和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决在有限且嘈杂的早期数据中预测罕见高影响事件（如初创公司成功）的挑战，需要同时兼顾预测准确性和决策可解释性

Method: 使用LLM进行特征工程从非结构化数据提取复杂信号，构建包含XGBoost、随机森林和线性回归的分层集成模型，首先生成连续成功概率估计，然后通过阈值处理生成二元罕见事件预测

Result: 在三个独立测试子集中，模型精度达到随机分类器基线的9.8倍至11.1倍；特征敏感性分析显示创业类别列表占预测影响力的15.6%，创始人数量次之，教育水平和领域专业知识也有稳定贡献

Conclusion: 该框架成功整合了黑盒模型的预测能力和可解释性要求，为风险投资等高风险决策领域提供了有效的罕见事件预测解决方案

Abstract: This paper presents a framework for predicting rare, high-impact outcomes by
integrating large language models (LLMs) with a multi-model machine learning
(ML) architecture. The approach combines the predictive strength of black-box
models with the interpretability required for reliable decision-making. We use
LLM-powered feature engineering to extract and synthesize complex signals from
unstructured data, which are then processed within a layered ensemble of models
including XGBoost, Random Forest, and Linear Regression. The ensemble first
produces a continuous estimate of success likelihood, which is then thresholded
to produce a binary rare-event prediction. We apply this framework to the
domain of Venture Capital (VC), where investors must evaluate startups with
limited and noisy early-stage data. The empirical results show strong
performance: the model achieves precision between 9.8X and 11.1X the random
classifier baseline in three independent test subsets. Feature sensitivity
analysis further reveals interpretable success drivers: the startup's category
list accounts for 15.6% of predictive influence, followed by the number of
founders, while education level and domain expertise contribute smaller yet
consistent effects.

</details>


### [29] [MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs](https://arxiv.org/abs/2509.08156)
*Swati Swati,Arjun Roy,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: mmm-fair是一个开源工具包，使用基于boosting的集成方法动态优化模型权重，同时最小化分类错误和多种公平性违规，支持多目标优化和交叉偏见检测。


<details>
  <summary>Details</summary>
Motivation: 解决分类任务中性能和公平性的平衡问题，特别是交叉偏见和冲突的公平性定义带来的挑战，现有工具包对多维公平性和相关权衡的支持有限。

Method: 基于boosting的集成方法，动态优化模型权重，联合最小化分类错误和多样公平性违规，支持多目标优化、无代码聊天界面、LLM驱动的解释等功能。

Result: 能够部署符合用户特定上下文需求的模型，可靠地发现最先进方法经常遗漏的交叉偏见，提供交互式帕累托探索和自定义公平性约束定义。

Conclusion: mmm-fair独特地结合了深度多属性公平性、多目标优化、无代码界面等多种功能，是现有公平性工具中罕见的综合解决方案。

Abstract: Fairness-aware classification requires balancing performance and fairness,
often intensified by intersectional biases. Conflicting fairness definitions
further complicate the task, making it difficult to identify universally fair
solutions. Despite growing regulatory and societal demands for equitable AI,
popular toolkits offer limited support for exploring multi-dimensional fairness
and related trade-offs. To address this, we present mmm-fair, an open-source
toolkit leveraging boosting-based ensemble approaches that dynamically
optimizes model weights to jointly minimize classification errors and diverse
fairness violations, enabling flexible multi-objective optimization. The system
empowers users to deploy models that align with their context-specific needs
while reliably uncovering intersectional biases often missed by
state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth
multi-attribute fairness, multi-objective optimization, a no-code, chat-based
interface, LLM-powered explanations, interactive Pareto exploration for model
selection, custom fairness constraint definition, and deployment-ready models
in a single open-source toolkit, a combination rarely found in existing
fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.

</details>


### [30] [Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation](https://arxiv.org/abs/2509.08163)
*Ho Ming Lee,Katrien Antonio,Benjamin Avanzi,Lorenzo Marchi,Rui Zhou*

Main category: cs.LG

TL;DR: 提出基于距离协方差的正则化框架，解决回归任务中的多属性公平性问题，防止公平性划分，适用于连续和分类保护属性


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法主要针对二分类问题，而回归任务（如保险定价、招聘评分）同样重要。现有方法无法处理连续属性（如年龄）和多属性同时存在时的公平性划分问题

Method: 距离协方差正则化框架，减弱模型预测与保护属性之间的关联性。扩展了两种多元依赖度量：联合距离协方差(JdCov)和新提出的连接距离协方差(CCdCov)，有效处理多属性公平性问题

Result: 框架在COMPAS累犯数据集和大型机动车保险索赔数据集上应用，能够捕获线性和非线性依赖关系，解决公平性划分问题

Conclusion: 提出的距离协方差正则化框架为回归和分类任务中的多属性公平性问题提供了有效解决方案，特别是针对连续属性和多属性同时存在的情况

Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as
gender or ethnicity) is a critical issue in machine learning. Most existing
literature focuses on binary classification, but achieving fairness in
regression tasks-such as insurance pricing or hiring score assessments-is
equally important. Moreover, anti-discrimination laws also apply to continuous
attributes, such as age, for which many existing methods are not applicable. In
practice, multiple protected attributes can exist simultaneously; however,
methods targeting fairness across several attributes often overlook so-called
"fairness gerrymandering", thereby ignoring disparities among intersectional
subgroups (e.g., African-American women or Hispanic men). In this paper, we
propose a distance covariance regularisation framework that mitigates the
association between model predictions and protected attributes, in line with
the fairness definition of demographic parity, and that captures both linear
and nonlinear dependencies. To enhance applicability in the presence of
multiple protected attributes, we extend our framework by incorporating two
multivariate dependence measures based on distance covariance: the previously
proposed joint distance covariance (JdCov) and our novel concatenated distance
covariance (CCdCov), which effectively address fairness gerrymandering in both
regression and classification tasks involving protected attributes of various
types. We discuss and illustrate how to calibrate regularisation strength,
including a method based on Jensen-Shannon divergence, which quantifies
dissimilarities in prediction distributions across groups. We apply our
framework to the COMPAS recidivism dataset and a large motor insurance claims
dataset.

</details>


### [31] [MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments](https://arxiv.org/abs/2509.08176)
*Honghui Du,Leandro Minku,Huiyu Zhou*

Main category: cs.LG

TL;DR: MARLINE是一种新颖的多源迁移学习方法，用于处理非平稳环境中的概念漂移问题，即使源域和目标域概念不匹配也能有效利用多源知识


<details>
  <summary>Details</summary>
Motivation: 现有方法假设至少有一个源模型与目标概念相似，这在现实场景中往往不成立。需要一种能够在源域和目标域概念不匹配时仍能有效利用多源知识的方法

Method: 通过将目标概念投影到每个源概念的空间中，使多个源子分类器能够作为集成的一部分为目标概念的预测做出贡献

Result: 在多个合成和真实数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确

Conclusion: MARLINE能够有效处理非平稳环境中的概念漂移问题，在源域和目标域概念不匹配的情况下仍能充分利用多源知识，显著提升预测性能

Abstract: Concept drift is a major problem in online learning due to its impact on the
predictive performance of data stream mining systems. Recent studies have
started exploring data streams from different sources as a strategy to tackle
concept drift in a given target domain. These approaches make the assumption
that at least one of the source models represents a concept similar to the
target concept, which may not hold in many real-world scenarios. In this paper,
we propose a novel approach called Multi-source mApping with tRansfer LearnIng
for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge
from multiple data sources in non-stationary environments even when source and
target concepts do not match. This is achieved by projecting the target concept
to the space of each source concept, enabling multiple source sub-classifiers
to contribute towards the prediction of the target concept as part of an
ensemble. Experiments on several synthetic and real-world datasets show that
MARLINE was more accurate than several state-of-the-art data stream learning
approaches.

</details>


### [32] [The Domain Mixed Unit: A New Neural Arithmetic Layer](https://arxiv.org/abs/2509.08180)
*Paul Curry*

Main category: cs.LG

TL;DR: DMU是一种新的神经算术单元，通过单一参数门在log空间和线性空间表示之间混合，支持加法和减法运算，在NALM基准测试中实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经网络在算术运算泛化方面的挑战，特别是需要同时处理加法和乘法、减法和除法等不同运算模式的问题。

Method: 提出Domain Mixed Unit (DMU)，使用单一参数门在log空间和线性空间表示之间进行混合，支持DMU加法（加法和乘法）和DMU减法（减法和除法）两种初始化方式。

Result: 在NALM基准测试中实现了最先进的性能，特别是在乘法和除法运算上获得了所有种子中最高的解决百分比。

Conclusion: DMU是一个有效的神经算术单元，能够很好地泛化算术运算，代码已开源并在GitHub上提供。

Abstract: The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a
single parameter gate that mixes between log-space and linear-space
representations while performing either addition (DMU add) or subtraction (DMU
sub). Two initializations are proposed for the DMU: one covering addition and
multiplication, and another covering subtraction and division. The DMU achieves
state-of-the-art performance on the NALM Benchmark, a dataset designed to test
the ability of neural arithmetic units to generalize arithmetic operations,
specifically performing with the highest percentage solved over all seeds on
multiplication and division. The DMU will be submitted as a pull request to the
open-source NALM benchmark, and its code is available on GitHub at
https://github.com/marict?tab=repositories

</details>


### [33] [Multi-Label Transfer Learning in Non-Stationary Data Streams](https://arxiv.org/abs/2509.08181)
*Honghui Du,Leandro Minku,Aonghus Lawlor,Huiyu Zhou*

Main category: cs.LG

TL;DR: 提出了两种多标签数据流迁移学习方法，通过标签间知识转移提升非平稳环境下的分类性能


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中标签概念经常漂移，标签间可能存在依赖关系，但现有研究对多标签迁移学习在数据流中的应用有限

Method: BR-MARLENE利用源流和目标流中不同标签的知识进行多标签分类；BRPW-MARLENE进一步显式建模和迁移成对标签依赖关系

Result: 综合实验表明两种方法在非平稳环境中都优于最先进的多标签流方法

Conclusion: 标签间知识转移能有效提升多标签数据流分类的预测性能

Abstract: Label concepts in multi-label data streams often experience drift in
non-stationary environments, either independently or in relation to other
labels. Transferring knowledge between related labels can accelerate
adaptation, yet research on multi-label transfer learning for data streams
remains limited. To address this, we propose two novel transfer learning
methods: BR-MARLENE leverages knowledge from different labels in both source
and target streams for multi-label classification; BRPW-MARLENE builds on this
by explicitly modelling and transferring pairwise label dependencies to enhance
learning performance. Comprehensive experiments show that both methods
outperform state-of-the-art multi-label stream approaches in non-stationary
environments, demonstrating the effectiveness of inter-label knowledge transfer
for improved predictive performance.

</details>


### [34] [Selective Induction Heads: How Transformers Select Causal Structures In Context](https://arxiv.org/abs/2509.08184)
*Francesco D'Angelo,Francesco Croce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文提出了一个新颖框架，展示Transformer动态处理因果结构的能力，通过交错不同滞后的马尔可夫链来揭示选择性归纳头的形成机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖固定因果结构的马尔可夫链来研究归纳头，但无法捕捉自然语言中随上下文动态变化的token关系复杂性。

Method: 使用具有不同滞后的交错马尔可夫链来变化因果结构，同时保持转移概率固定，从而研究选择性归纳头的形成。

Result: Transformer学会了通过识别正确滞后并从过去复制相应token来预测下一个token，构建了3层Transformer实现选择性归纳头。

Conclusion: 该机制渐近收敛于最大似然解，推进了对Transformer如何选择因果结构的理解，为其功能和可解释性提供了新见解。

Abstract: Transformers have exhibited exceptional capabilities in sequence modeling
tasks, leveraging self-attention and in-context learning. Critical to this
success are induction heads, attention circuits that enable copying tokens
based on their previous occurrences. In this work, we introduce a novel
framework that showcases transformers' ability to dynamically handle causal
structures. Existing works rely on Markov Chains to study the formation of
induction heads, revealing how transformers capture causal dependencies and
learn transition probabilities in-context. However, they rely on a fixed causal
structure that fails to capture the complexity of natural languages, where the
relationship between tokens dynamically changes with context. To this end, our
framework varies the causal structure through interleaved Markov chains with
different lags while keeping the transition probabilities fixed. This setting
unveils the formation of Selective Induction Heads, a new circuit that endows
transformers with the ability to select the correct causal structure
in-context. We empirically demonstrate that transformers learn this mechanism
to predict the next token by identifying the correct lag and copying the
corresponding token from the past. We provide a detailed construction of a
3-layer transformer to implement the selective induction head, and a
theoretical analysis proving that this mechanism asymptotically converges to
the maximum likelihood solution. Our findings advance the understanding of how
transformers select causal structures, providing new insights into their
functioning and interpretability.

</details>


### [35] [ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis](https://arxiv.org/abs/2509.08188)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 这篇论文研究使用生成对抗网络(WGAN-GP)和去噪漏渏模型来生成真实的电脑电图巧合物数据，以解决巧合物标注成本高的问题。评估显示WGAN-GP在谱对齐方面更优，但两种模型的类别条件恢复能力都较弱，限制了数据增帽效果。


<details>
  <summary>Details</summary>
Motivation: 电脑电图(EEG)中的巧合物(肌肉、眼球运动、电极、咬喃、发抖等)会干扰自动化分析，但大规模标注成本高。研究主要动机是探索现代生成模型能否生成真实的、带标签的巧合物数据段以用于数据增帽和压力测试。

Method: 使用TUH EEG Artifact (TUAR)数据集，简化为主体分割和固定长度多通道窗口。比较两种模型：条件WGAN-GP(带抗损减项的Wasserstein生成对抗网络)与投影判别器，以及1D去噪漏渏模型与分类器免指导。通过三个方面评估：(i)保真度(Welch谱带功率差异、通道协方差范数、自相关L2距离、分布指标)；(ii)特异性(类别条件恢复能力)；(iii)实用性(数据增帽对巧合物识别的效果)。

Result: 在该设置下，WGAN-GP实现了更接近的谱对齐和低MMD(最大均差)距离，表明与真实数据更为接近。但两种模型都显示出较弱的类别条件恢复能力，这限制了立即的数据增帽收益，也为更强的条件控制和覆盖范围提供了改进机会。

Conclusion: 论文提供了可复现的流程包括数据清单、训练配置和评估脚本，为EEG巧合物合成预书了基准线，并显示了可行动的失败模式以便未来工作改进。研究结果指出虽然WGAN-GP在谱保真方面表现更好，但现有模型在类别条件控制方面仍有提升空间，需要进一步优化条件机制来提高生成数据的特异性和实用性。

Abstract: Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,
chewing, and shiver -- confound automated analysis yet are costly to label at
scale. We study whether modern generative models can synthesize realistic,
label-aware artifact segments suitable for augmentation and stress-testing.
Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and
fixed-length multi-channel windows (e.g., 250 samples) with preprocessing
tailored to each model (per-window min-max for adversarial training;
per-recording/channel $z$-score for diffusion). We compare a conditional
WGAN-GP with a projection discriminator to a 1D denoising diffusion model with
classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch
band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$),
channel-covariance Frobenius distance, autocorrelation $L_2$, and
distributional metrics (MMD/PRD); (ii) specificity via class-conditional
recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation
effects on artifact recognition. In our setting, WGAN-GP achieves closer
spectral alignment and lower MMD to real data, while both models exhibit weak
class-conditional recovery, limiting immediate augmentation gains and revealing
opportunities for stronger conditioning and coverage. We release a reproducible
pipeline -- data manifests, training configurations, and evaluation scripts --
to establish a baseline for EEG artifact synthesis and to surface actionable
failure modes for future work.

</details>


### [36] [Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics](https://arxiv.org/abs/2509.08191)
*Robert Stephany,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种结合灵活高阶有限差分格式和Rollout损失函数的降阶模型方法，用于解决长时间预测精度下降问题，并在2D Burgers方程上验证有效性


<details>
  <summary>Details</summary>
Motivation: 复杂偏微分方程求解计算成本高，现有降阶模型在长时间预测时精度下降严重，需要改进长时间预测能力

Method: 1) 引入灵活、高阶且计算成本低的有限差分格式；2) 提出Rollout损失函数来训练降阶模型，使其能够在任意时间范围内做出准确预测

Result: 在2D Burgers方程上验证了所提方法的有效性，展示了改进的长时间预测性能

Conclusion: 该方法通过新的数值格式和训练策略，有效提升了降阶模型在长时间预测中的精度和稳定性

Abstract: Solving complex partial differential equations is vital in the physical
sciences, but often requires computationally expensive numerical methods.
Reduced-order models (ROMs) address this by exploiting dimensionality reduction
to create fast approximations. While modern ROMs can solve parameterized
families of PDEs, their predictive power degrades over long time horizons. We
address this by (1) introducing a flexible, high-order, yet inexpensive
finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to
make accurate predictions over arbitrary time horizons. We demonstrate our
approach on the 2D Burgers equation.

</details>


### [37] [Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization](https://arxiv.org/abs/2509.08194)
*Caio de Prospero Iglesias,Kimberly Villalobos Carballo,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出了Prescribe-then-Select框架，通过构建候选策略库并学习元策略来选择最优策略，在异质性协变量空间中优于单一策略


<details>
  <summary>Details</summary>
Motivation: 解决上下文随机优化中的策略选择问题，当多个候选策略在不同协变量区域表现各异且没有单一主导策略时

Method: 模块化框架：先构建可行候选策略库，然后学习元策略选择最优策略，使用交叉验证训练的最优策略树集成实现

Result: 在报童问题和运输规划两个基准问题上，PS在异质性协变量区域持续优于最佳单一策略，在无异质性时收敛到主导策略

Conclusion: PS框架能够有效处理上下文随机优化中的策略选择问题，在异质性环境中表现优异

Abstract: We address the problem of policy selection in contextual stochastic
optimization (CSO), where covariates are available as contextual information
and decisions must satisfy hard feasibility constraints. In many CSO settings,
multiple candidate policies--arising from different modeling paradigms--exhibit
heterogeneous performance across the covariate space, with no single policy
uniformly dominating. We propose Prescribe-then-Select (PS), a modular
framework that first constructs a library of feasible candidate policies and
then learns a meta-policy to select the best policy for the observed
covariates. We implement the meta-policy using ensembles of Optimal Policy
Trees trained via cross-validation on the training set, making policy choice
entirely data-driven. Across two benchmark CSO problems--single-stage
newsvendor and two-stage shipment planning--PS consistently outperforms the
best single policy in heterogeneous regimes of the covariate space and
converges to the dominant policy when such heterogeneity is absent. All the
code to reproduce the results can be found at
https://anonymous.4open.science/r/Prescribe-then-Select-TMLR.

</details>


### [38] [Sketched Gaussian Mechanism for Private Federated Learning](https://arxiv.org/abs/2509.08195)
*Qiaobo Li,Zhijie Chen,Arindam Banerjee*

Main category: cs.LG

TL;DR: 这篇论文提出了简测高斯机制(SGM)，将梯度压缩与高斯噪声直接结合，在联邦学习中同时优化通信成本和隐私保护。通过联合分析，SGM在同样的噪声预算下能提供比传统高斯机制更强的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的通信成本和隐私保护是两个重要考虑。现有研究对梯度压缩和高斯机制的隐私分析是孤立的，需要一种更灵活和更严格的联合分析方法。

Method: 提出简测高斯机制(SGM)，直接结合梯度压缩(简测)和高斯噪声。使用Rényi-DP工具进行联合隐私分析，并将SGM应用于使用梯度下降或适配服务器优化器的联邦学习。

Result: 证明SGM在固定噪声强度下的隐私水平与1/√b成正比(b为简测维度)，在同样噪声预算下能提供比原始高斯机制更强的隐私保护。实验结果显示SGM在同样隐私水平下至少与非简测私有联邦学习变体竞争，并在某些设置中更优。

Conclusion: SGM通过联合分析梯度压缩和高斯噪声，能够在联邦学习中同时优化通信效率和隐私保护。使用适配服务器优化器可以改善实践性能同时保持隐私保证。

Abstract: Communication cost and privacy are two major considerations in federated
learning (FL). For communication cost, gradient compression by sketching the
clients' transmitted model updates is often used for reducing per-round
communication. For privacy, the Gaussian mechanism (GM), which consists of
clipping updates and adding Gaussian noise, is commonly used to guarantee
client-level differential privacy. Existing literature on private FL analyzes
privacy of sketching and GM in an isolated manner, illustrating that sketching
provides privacy determined by the sketching dimension and that GM has to
supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which
directly combines sketching and the Gaussian mechanism for privacy. Using
R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy
guarantee, which is significantly more flexible and sharper compared to
isolated analysis of sketching and GM privacy. In particular, we prove that the
privacy level of SGM for a fixed noise magnitude is proportional to
$1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for
moderate $b$) SGM can provide much stronger privacy guarantees than the
original GM under the same noise budget. We demonstrate the application of SGM
to FL with either gradient descent or adaptive server optimizers, and establish
theoretical results on optimization convergence, which exhibits only a
logarithmic dependence on the number of parameters $d$. Experimental results
confirm that at the same privacy level, SGM based FL is at least competitive
with non-sketching private FL variants and outperforms them in some settings.
Moreover, using adaptive optimization at the server improves empirical
performance while maintaining the privacy guarantees.

</details>


### [39] [Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition](https://arxiv.org/abs/2509.08225)
*Matthew Nolan,Lina Yao,Robert Davidson*

Main category: cs.LG

TL;DR: 这篇论文探索了集成分布蓬荟(EDD)在自监督学习框架中的新应用，用于人类活动识别(HAR)，以解决数据需求、可靠性和稳健性挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习技术在HAR领域取得了显著进步，但仍面临着数据需求大、可靠性不足和稳健性挑战的问题。

Method: 提出了在自监督学习框架下的集成分布蓬荟(EDD)方法，利用未标签数据和部分监督训练策略，包括为HAR设计的创新数据增帽技术。

Result: 方法在多个公开数据集上进行了评估，实现了预测准确性提升、稳健的不确定性估计，以及在对抗性扰动下显著提高稳健性。

Conclusion: 该方法在不增加推理计算复杂度的情况下，显著提高了实际应用场景中的可靠性，为HAR领域提供了一种有效的解决方案。

Abstract: Human Activity Recognition (HAR) has seen significant advancements with the
adoption of deep learning techniques, yet challenges remain in terms of data
requirements, reliability and robustness. This paper explores a novel
application of Ensemble Distribution Distillation (EDD) within a
self-supervised learning framework for HAR aimed at overcoming these
challenges. By leveraging unlabeled data and a partially supervised training
strategy, our approach yields an increase in predictive accuracy, robust
estimates of uncertainty, and substantial increases in robustness against
adversarial perturbation; thereby significantly improving reliability in
real-world scenarios without increasing computational complexity at inference.
We demonstrate this with an evaluation on several publicly available datasets.
The contributions of this work include the development of a self-supervised EDD
framework, an innovative data augmentation technique designed for HAR, and
empirical validation of the proposed method's effectiveness in increasing
robustness and reliability.

</details>


### [40] [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)
*Kai Yi*

Main category: cs.LG

TL;DR: 这篇论文探索分布式和联邦学习中的通信效率优化策略，包括模型压缩、本地训练和个性化方法，提出了多种方案来减少通信开销并保持性能。


<details>
  <summary>Details</summary>
Motivation: 分布式和联邦学习在保护隐私的同时面临着通信开销过大的挑战，需要提高通信效率来支持可扩展的分布式学习。

Method: 建立了偏置和无偏置压缩运算符的统一框架，提出了包含个性化的适应性本地训练策略，设计了Scafflix、Cohort-Squeeze和SymWanda等方法来优化通信效率和保持性能。

Result: 在标准数据集和大规模语言模型上进行了广泛实验，证明了方法在准确性、收敛性和通信效率之间取得了良好的平衡，在IID和非IID情况下都取得了优异性能。

Conclusion: 该研究为可扩展、高效的分布式学习提供了理论和实践见解，通过通信效率优化策略有效解决了分布式学习中的通信瓶颈问题。

Abstract: Distributed and federated learning are essential paradigms for training
models across decentralized data sources while preserving privacy, yet
communication overhead remains a major bottleneck. This dissertation explores
strategies to improve communication efficiency, focusing on model compression,
local training, and personalization. We establish a unified framework for
biased and unbiased compression operators with convergence guarantees, then
propose adaptive local training strategies that incorporate personalization to
accelerate convergence and mitigate client drift. In particular, Scafflix
balances global and personalized objectives, achieving superior performance
under both IID and non-IID settings. We further introduce privacy-preserving
pruning frameworks that optimize sparsity while minimizing communication costs,
with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device
overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances
robustness under high sparsity and maintains accuracy without retraining.
Extensive experiments on benchmarks and large-scale language models demonstrate
favorable trade-offs among accuracy, convergence, and communication, offering
theoretical and practical insights for scalable, efficient distributed
learning.

</details>


### [41] [The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data](https://arxiv.org/abs/2509.08247)
*Xiaolong Luo,Michael Lingzhi Li*

Main category: cs.LG

TL;DR: CRITICAL数据集提供了大规模多机构重症监护数据，CRISP预处理管道将其转化为ML就绪格式，简化研究流程


<details>
  <summary>Details</summary>
Motivation: 现有重症监护数据集规模有限且缺乏多样性，CRITICAL数据集虽规模庞大但数据异质性强，需要高效预处理方法

Method: 开发CRISP预处理管道，包括数据质量管理、医学术语映射标准化、模块化并行处理架构、基准模型评估

Result: CRISP能在标准硬件上1天内完成整个数据集处理，提供可重复性能基准，大幅减少研究人员预处理时间

Conclusion: CRISP成功解决了多机构医疗数据整合的复杂性，使研究人员能专注于临床AI算法开发而非数据预处理

Abstract: While existing critical care EHR datasets such as MIMIC and eICU have enabled
significant advances in clinical AI research, the CRITICAL dataset opens new
frontiers by providing extensive scale and diversity -- containing 1.95 billion
records from 371,365 patients across four geographically diverse CTSA
institutions. CRITICAL's unique strength lies in capturing full-spectrum
patient journeys, including pre-ICU, ICU, and post-ICU encounters across both
inpatient and outpatient settings. This multi-institutional, longitudinal
perspective creates transformative opportunities for developing generalizable
predictive models and advancing health equity research. However, the richness
of this multi-site resource introduces substantial complexity in data
harmonization, with heterogeneous collection practices and diverse vocabulary
usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource.
CRISP systematically transforms raw Observational Medical Outcomes Partnership
Common Data Model data into ML-ready datasets through: (1) transparent data
quality management with comprehensive audit trails, (2) cross-vocabulary
mapping of heterogeneous medical terminologies to unified SNOMED-CT standards,
with deduplication and unit standardization, (3) modular architecture with
parallel optimization enabling complete dataset processing in $<$1 day even on
standard computing hardware, and (4) comprehensive baseline model benchmarks
spanning multiple clinical prediction tasks to establish reproducible
performance standards. By providing processing pipeline, baseline
implementations, and detailed transformation documentation, CRISP saves
researchers months of preprocessing effort and democratizes access to
large-scale multi-institutional critical care data, enabling them to focus on
advancing clinical AI.

</details>


### [42] [Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models](https://arxiv.org/abs/2509.08270)
*Pranav Pawar,Kavish Shah,Akshat Bhalani,Komal Kasat,Dev Mittal,Hadi Gala,Deepali Patil,Nikita Raichada,Monali Deshmukh*

Main category: cs.LG

TL;DR: 这篇论文提出了一个新的框架用于评估视觉-语言模型的二维物理理解能力，发现模型规模与理解能力强相关，但在需要空间折理的领域表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型的发展，对其理解基础科学原理的能力需要进一步评估，物理理解是一个未充分探索的前沿领域。

Method: 设计了一个实用的场景生成器，创建了400多个问题的多样化测试库，涵盖投射运动、碰撞动力学、力学和流体动力学四个核心领域。

Result: 对4个最先进的VLM进行了全面评估，发现模型规模与理解能力存在强相关关系，最佳模型Qwen2.5-VL-7B达到0.815的总体分数。模型在公式化问题上表现优异，但在需要抽象空间理解的领域遇到明显困难。

Conclusion: 通过设计这个框架，目标是促进VLM科学理解能力的研究流行化，并为深入了解模型的能力和局限性提供见解。

Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to
perform reasoning is coming under increasing supervision. While they excel at
many tasks, their grasp of fundamental scientific principles, such as physics,
remains an underexplored frontier. To reflect the advancements in these
capabilities, we introduce a novel and accessible framework designed to
rigorously evaluate VLMs on their understanding of 2D physics. Our framework
features a pragmatic scenario generator that creates a diverse testbed of over
400 problems across four core domains: Projectile Motion, Collision Dynamics,
Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four
state-of-the-art VLMs, we demonstrate a strong correlation between model scale
and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving
an overall score of 0.815. We find that while models excel at formulaic
problems, they struggle significantly with domains requiring abstract spatial
reasoning. By designing this framework, we aim to democratize the study of
scientific reasoning in VLMs and foster deeper insights into their capabilities
and limitations.

</details>


### [43] [Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning](https://arxiv.org/abs/2509.08255)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.LG

TL;DR: 提出FAPM遗忘感知剪枝指标，通过任务向量与预训练参数的重叠度来量化灾难性遗忘，在保持下游任务99.67%准确率的同时将遗忘率降至0.25%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调时面临灾难性遗忘问题，需要平衡下游任务性能和预训练知识保留

Method: 基于任务向量与预训练参数的重叠度设计剪枝指标FAPM，无需修改训练过程或模型架构，也不需要辅助数据

Result: 在8个数据集上的实验表明，FAPM将灾难性遗忘限制在0.25%，同时保持下游任务99.67%的准确率

Conclusion: FAPM是一种有效的灾难性遗忘缓解方法，通过剪枝策略成功平衡了微调性能和预训练知识保留

Abstract: Recent advancements in large language models (LLMs) have shown impressive
capabilities in various downstream tasks but typically face Catastrophic
Forgetting (CF) during fine-tuning. In this paper, we propose the
Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to
balance CF and downstream task performance. Our investigation reveals that the
degree to which task vectors (i.e., the subtraction of pre-trained weights from
the weights fine-tuned on downstream tasks) overlap with pre-trained model
parameters is a critical factor for CF. Based on this finding, FAPM employs the
ratio of the task vector to pre-trained model parameters as a metric to
quantify CF, integrating this measure into the pruning criteria. Importantly,
FAPM does not necessitate modifications to the training process or model
architecture, nor does it require any auxiliary data. We conducted extensive
experiments across eight datasets, covering natural language inference, General
Q&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results
demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\%
accuracy on downstream tasks. We provide the code to reproduce our results.

</details>


### [44] [\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2509.08300)
*Yao Lu,Chunfeng Sun,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: FoQuS通过从原始数据集中选择核心集来近似完整训练效果，显著减少训练开销，在仅使用1%-30%数据的情况下保持高识别精度和良好的跨架构泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习自动调制识别模型在开发新模型或超参数调优时，使用海量数据进行重复训练所带来的时间和能耗不可承受的问题。

Method: FoQuS记录完整数据集训练过程中每个样本的预测轨迹，基于训练动态构建三个重要性指标来选择核心集。

Result: 在多个AMR数据集上，仅使用1%-30%的原始数据就能维持高识别准确率，并展现出良好的跨架构泛化性能。

Conclusion: FoQuS方法有效降低了自动调制识别模型的训练成本，在保持性能的同时显著减少了数据需求，具有实际应用价值。

Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made
significant progress with the support of large-scale labeled data. However,
when developing new models or performing hyperparameter tuning, the time and
energy consumption associated with repeated training using massive amounts of
data are often unbearable. To address the above challenges, we propose
\emph{FoQuS}, which approximates the effect of full training by selecting a
coreset from the original dataset, thereby significantly reducing training
overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each
sample during full-dataset training and constructs three importance metrics
based on training dynamics. Experiments show that \emph{FoQuS} can maintain
high recognition accuracy and good cross-architecture generalization on
multiple AMR datasets using only 1\%-30\% of the original data.

</details>


### [45] [Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing](https://arxiv.org/abs/2509.08329)
*Lukas Toral,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 使用预训练大语言模型作为导师在强化学习中加速训练过程，通过LLM生成的指导和建议重用机制显著缩短收敛时间


<details>
  <summary>Details</summary>
Motivation: 解决强化学习算法在复杂环境中训练过慢的问题，现有加速技术如奖励塑造和课程学习需要专业知识和预置工作

Method: 采用学生-教师架构，使用预训练LLM（Llama、Vicuna、DeepSeek）作为导师为RL算法（DQN、PPO、A2C）提供指导，在黑标杂、蛇、连连看等环境中进行54种配置实验

Result: LLM导师机制显著加速了RL收敛速度，同时保持相当的最优性能；建议重用进一步缩短训练时间但导致收敛不稳定

Conclusion: LLM导师通常能够改善收敛效果，其有效性受到具体任务、RL算法和LLM模型组合的影响

Abstract: Reinforcement Learning (RL) algorithms often require long training to become
useful, especially in complex environments with sparse rewards. While
techniques like reward shaping and curriculum learning exist to accelerate
training, these are often extremely specific and require the developer's
professionalism and dedicated expertise in the problem's domain. Tackling this
challenge, in this study, we explore the effectiveness of pre-trained Large
Language Models (LLMs) as tutors in a student-teacher architecture with RL
algorithms, hypothesizing that LLM-generated guidance allows for faster
convergence. In particular, we explore the effectiveness of reusing the LLM's
advice on the RL's convergence dynamics. Through an extensive empirical
examination, which included 54 configurations, varying the RL algorithm (DQN,
PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,
Snake, Connect Four), our results demonstrate that LLM tutoring significantly
accelerates RL convergence while maintaining comparable optimal performance.
Furthermore, the advice reuse mechanism shows a further improvement in training
duration but also results in less stable convergence dynamics. Our findings
suggest that LLM tutoring generally improves convergence, and its effectiveness
is sensitive to the specific task, RL algorithm, and LLM model combination.

</details>


### [46] [Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning](https://arxiv.org/abs/2509.08277)
*Dung T. Tran,Huyen Ngoc Huyen,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.LG

TL;DR: 提出了基于矩阵轮廓的加权集成框架MPWE，用于越南降雨预测，通过动态捕捉地理模型间的协变依赖关系和冗余感知加权，在多个流域和时间尺度上实现了更高的预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 越南降雨预测面临气候多样性和地理变异性的挑战，但准确的预测对洪水管理、水电运营和灾害预防至关重要。

Method: MPWE框架采用制度切换机制，动态捕捉多个地理模型预测间的协变依赖关系，并引入冗余感知加权来平衡各模型的贡献。

Result: 在越南8个主要流域的5个预测时间尺度上，MPWE相比地理模型和集成基线方法，预测误差的均值和标准差均显著降低。

Conclusion: MPWE框架能够有效提升降雨预测的准确性和稳定性，适用于不同流域和预测时间尺度，具有重要的实际应用价值。

Abstract: Rainfall forecasting in Vietnam is highly challenging due to its diverse
climatic conditions and strong geographical variability across river basins,
yet accurate and reliable forecasts are vital for flood management, hydropower
operation, and disaster preparedness. In this work, we propose a Matrix
Profile-based Weighted Ensemble (MPWE), a regime-switching framework that
dynamically captures covariant dependencies among multiple geographical model
forecasts while incorporating redundancy-aware weighting to balance
contributions across models. We evaluate MPWE using rainfall forecasts from
eight major basins in Vietnam, spanning five forecast horizons (1 hour and
accumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results
show that MPWE consistently achieves lower mean and standard deviation of
prediction errors compared to geographical models and ensemble baselines,
demonstrating both improved accuracy and stability across basins and horizons.

</details>


### [47] [Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism](https://arxiv.org/abs/2509.08342)
*Jiaming Yan,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.LG

TL;DR: MoEpic是一个高效的MoE推理系统，通过专家分割机制和预测预取策略，在减少GPU内存占用的同时显著提升推理速度


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型参数庞大，GPU内存需求高，专家卸载到CPU RAM的方案因缓存命中率低和加载延迟导致推理速度显著下降

Method: 提出专家垂直分割机制（top-bottom分段），缓存热点专家的top段以提高缓存命中率；预测下一层激活专家并预取；基于定点迭代的自适应缓存配置算法

Result: 实验表明MoEpic可节省约一半GPU成本，相比基线降低推理延迟37.51%-65.73%

Conclusion: MoEpic通过创新的专家分割和智能预取策略，有效解决了MoE模型推理中的内存和延迟问题，实现了高效的推理性能

Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern
large language models (LLMs). However, massive parameters impose heavy GPU
memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.
Offloading the expert parameters to CPU RAM offers an effective way to
alleviate the VRAM requirements for MoE inference. Existing approaches
typically cache a small subset of experts in VRAM and dynamically prefetch
experts from RAM during inference, leading to significant degradation in
inference speed due to the poor cache hit rate and substantial expert loading
latency. In this work, we propose MoEpic, an efficient MoE inference system
with a novel expert split mechanism. Specifically, each expert is vertically
divided into two segments: top and bottom. MoEpic caches the top segment of hot
experts, so that more experts will be stored under the limited VRAM budget,
thereby improving the cache hit rate. During each layer's inference, MoEpic
predicts and prefetches the activated experts for the next layer. Since the top
segments of cached experts are exempt from fetching, the loading time is
reduced, which allows efficient transfer-computation overlap. Nevertheless, the
performance of MoEpic critically depends on the cache configuration (i.e., each
layer's VRAM budget and expert split ratio). To this end, we propose a
divide-and-conquer algorithm based on fixed-point iteration for adaptive cache
configuration. Extensive experiments on popular MoE LLMs demonstrate that
MoEpic can save about half of the GPU cost, while lowering the inference
latency by about 37.51%-65.73% compared to the baselines.

</details>


### [48] [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)
*Matan Avitan,Moran Baruch,Nir Drucker,Itamar Zimerman,Yoav Goldberg*

Main category: cs.LG

TL;DR: 这篇论文提出了两种同态加密友好的解码方法：cutmax算法和核采样方法，解决了在加密数据上进行神经文本生成的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理敏感数据时存在隐私泄漏风险，而传统的argmax和采样解码方法在同态加密环境下计算成本极高。

Method: 提出cutmax算法，减少密文操作数量；设计了首个HE兼容的核采样方法，利用cutmax实现高效随机解码。所有方法都是多项式的，支持高效推理。

Result: 在实际LLM输出上实现了24x-35x的延迟降低，显著提升了安全文本生成的性能。

Conclusion: 该研究为隐私保护设置中的安全文本生成提供了实用的解决方案，具有强烈的理论保证和显著的性能改善。

Abstract: Large language models (LLMs) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires decoding methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy decoding under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic decoding with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-preserving
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic LLM outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.

</details>


### [49] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: EvolKV是一个自适应的KV缓存压缩框架，通过进化搜索动态配置各层缓存预算，在保持任务性能的同时显著提升内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法依赖启发式策略，忽略了层间特征模式与任务性能的关键交互，导致泛化性能下降。

Method: 将缓存分配重新表述为多目标优化问题，利用进化搜索动态配置各层预算，直接最大化下游任务性能。

Result: 在11个任务上的实验表明，EvolKV在所有基线方法中表现最优，在长上下文任务上广泛超越，在GSM8K上比启发式基线高出7个百分点。仅使用1.5%原始预算就在代码补全任务上超越完整KV缓存设置。

Conclusion: EvolKV展示了学习式压缩策略在KV缓存预算分配中的巨大潜力，实现了内存效率和任务性能的联合优化。

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [50] [Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics](https://arxiv.org/abs/2509.08461)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 使用微调版LLaMa 3.2视觉语言模型处理高能物理实验中的中微子相互作用识别任务，性能超越传统CNN方法，同时提供更好的可解释性


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在处理像素化探测器数据方面的应用潜力，特别是在高能物理实验中的中微子相互作用识别任务

Method: 使用微调的LLaMa 3.2视觉语言模型，与NOvA和DUNE实验中使用的先进卷积神经网络架构进行对比评估

Result: 视觉语言模型在分类性能上超越CNN，同时提供更好的可解释性和灵活性，能够整合辅助文本或语义信息

Conclusion: 视觉语言模型有潜力成为物理事件分类的通用骨干网络，因其高性能、可解释性和泛化能力，为实验性中微子物理中的多模态推理开辟了新途径

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their
remarkable capacity to process and reason over structured and unstructured data
modalities beyond natural language. In this work, we explore the applications
of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa
3.2, to the task of identifying neutrino interactions in pixelated detector
data from high-energy physics (HEP) experiments. We benchmark this model
against a state-of-the-art convolutional neural network (CNN) architecture,
similar to those used in the NOvA and DUNE experiments, which have achieved
high efficiency and purity in classifying electron and muon neutrino events.
Our evaluation considers both the classification performance and
interpretability of the model predictions. We find that VLMs can outperform
CNNs, while also providing greater flexibility in integrating auxiliary textual
or semantic information and offering more interpretable, reasoning-based
predictions. This work highlights the potential of VLMs as a general-purpose
backbone for physics event classification, due to their high performance,
interpretability, and generalizability, which opens new avenues for integrating
multimodal reasoning in experimental neutrino physics.

</details>


### [51] [Variational Rank Reduction Autoencoders for Generative](https://arxiv.org/abs/2509.08515)
*Alicia Tierz,Jad Mounayer,Beatriz Moya,Francisco Chinesta*

Main category: cs.LG

TL;DR: 提出了一种结合变分秩降自编码器(VRRAE)和深度算子网络(DeepONet)的混合框架，用于复杂几何形状的生成式热设计，解决了传统方法的高计算成本和潜在空间不连续性问题。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型如自编码器和变分自编码器存在潜在空间不连续、后验坍塌等问题，限制了设计探索和物理一致性解决方案的生成能力，且高保真仿真计算成本高昂。

Method: 使用VRRAE在潜在空间中引入截断SVD，获得连续、可解释的结构化表示；然后利用DeepONet，在其分支网络中使用紧凑潜在编码，在主干网络中使用空间坐标，高效准确地预测温度梯度。

Result: 该方法不仅提高了生成几何形状的质量和梯度预测的准确性，相比传统数值求解器在推理效率上具有显著优势。

Conclusion: 研究强调了结构化潜在表示对算子学习的重要性，展示了生成模型与算子网络结合在热设计及其他工程应用中的潜力。

Abstract: Generative thermal design for complex geometries is fundamental in many areas
of engineering, yet it faces two main challenges: the high computational cost
of high-fidelity simulations and the limitations of conventional generative
models. Approaches such as autoencoders (AEs) and variational autoencoders
(VAEs) often produce unstructured latent spaces with discontinuities, which
restricts their capacity to explore designs and generate physically consistent
solutions.
  To address these limitations, we propose a hybrid framework that combines
Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks
(DeepONets). The VRRAE introduces a truncated SVD within the latent space,
leading to continuous, interpretable, and well-structured representations that
mitigate posterior collapse and improve geometric reconstruction. The DeepONet
then exploits this compact latent encoding in its branch network, together with
spatial coordinates in the trunk network, to predict temperature gradients
efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries
and the accuracy of gradient prediction, but also provides a substantial
advantage in inference efficiency compared to traditional numerical solvers.
Overall, the study underscores the importance of structured latent
representations for operator learning and highlights the potential of combining
generative models and operator networks in thermal design and broader
engineering applications.

</details>


### [52] [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
*Aadit Sengupta,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 论文主张将可解释性特别是机制性方法作为AI对齐的核心设计原则，而非辅助诊断工具，认为这是实现安全可信AI的关键路径


<details>
  <summary>Details</summary>
Motivation: 大型神经模型在高风险场景部署时存在与人类价值观对齐的可靠性问题，需要内部透明性来确保系统行为符合人类意图

Method: 对比事后解释方法（如LIME、SHAP）和机制性技术（如电路追踪、激活修补），后者能提供因果洞察并识别行为方法可能忽略的内部故障

Result: 机制性可解释性方法能够揭示包括欺骗性或未对齐推理在内的内部故障，为AI对齐提供更可靠的保障

Conclusion: 安全可信AI的进展取决于将可解释性作为AI研发的一等目标，确保系统不仅有效，而且可审计、透明并与人类意图对齐

Abstract: Large neural models are increasingly deployed in high-stakes settings,
raising concerns about whether their behavior reliably aligns with human
values. Interpretability provides a route to internal transparency by revealing
the computations that drive outputs. We argue that interpretability especially
mechanistic approaches should be treated as a design principle for alignment,
not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer
intuitive but correlational explanations, while mechanistic techniques like
circuit tracing or activation patching yield causal insight into internal
failures, including deceptive or misaligned reasoning that behavioral methods
like RLHF, red teaming, or Constitutional AI may overlook. Despite these
advantages, interpretability faces challenges of scalability, epistemic
uncertainty, and mismatches between learned representations and human concepts.
Our position is that progress on safe and trustworthy AI will depend on making
interpretability a first-class objective of AI research and development,
ensuring that systems are not only effective but also auditable, transparent,
and aligned with human intent.

</details>


### [53] [Prediction Loss Guided Decision-Focused Learning](https://arxiv.org/abs/2509.08359)
*Haeun Jeon,Hyunglip Bae,Chanyeong Kim,Yongjae Lee,Woo Chang Kim*

Main category: cs.LG

TL;DR: 提出了一种通过扰动决策损失梯度来改善决策聚焦学习收敛性的方法，结合预测损失梯度来构建更稳定的更新方向。


<details>
  <summary>Details</summary>
Motivation: 传统决策聚焦学习(DFL)虽然能端到端优化决策质量，但存在收敛不稳定问题；而预测聚焦学习(PFL)虽然优化稳定但忽略了决策质量。需要一种方法能兼顾两者的优势。

Method: 使用预测损失梯度扰动决策损失梯度来构建更新方向，采用sigmoid类衰减参数让预测损失梯度引导决策损失梯度，无需额外训练即可与任何DFL求解器集成。

Result: 在三个随机优化问题上验证了方法的有效性，相比基线方法获得了更低的遗憾值和更稳定的训练效果，在PFL或DFL单独表现不佳的情况下也能取得良好效果。

Conclusion: 该方法成功解决了DFL收敛不稳定的问题，同时保持了决策质量的优化，提供了理论收敛保证，是一种简单有效的决策聚焦学习改进方法。

Abstract: Decision-making under uncertainty is often considered in two stages:
predicting the unknown parameters, and then optimizing decisions based on
predictions. While traditional prediction-focused learning (PFL) treats these
two stages separately, decision-focused learning (DFL) trains the predictive
model by directly optimizing the decision quality in an end-to-end manner.
However, despite using exact or well-approximated gradients, vanilla DFL often
suffers from unstable convergence due to its flat-and-sharp loss landscapes. In
contrast, PFL yields more stable optimization, but overlooks the downstream
decision quality. To address this, we propose a simple yet effective approach:
perturbing the decision loss gradient using the prediction loss gradient to
construct an update direction. Our method requires no additional training and
can be integrated with any DFL solvers. Using the sigmoid-like decaying
parameter, we let the prediction loss gradient guide the decision loss gradient
to train a predictive model that optimizes decision quality. Also, we provide a
theoretical convergence guarantee to Pareto stationary point under mild
assumptions. Empirically, we demonstrate our method across three stochastic
optimization problems, showing promising results compared to other baselines.
We validate that our approach achieves lower regret with more stable training,
even in situations where either PFL or DFL struggles.

</details>


### [54] [Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques](https://arxiv.org/abs/2509.08606)
*Alireza Sameh,Mehrdad Rostami,Mourad Oussalah,Vahid Farrahi*

Main category: cs.LG

TL;DR: 深度学习算法使用原始加速度信号在24小时运动行为分类中表现略好于传统机器学习算法，准确度约85%，但差异不大


<details>
  <summary>Details</summary>
Motivation: 比较深度学习和传统机器学习算法在分类24小时运动行为（睡眠、坐着、轻度运动、中高强度运动）方面的性能差异

Method: 使用151名成年人的手腕加速度计数据，提取104个手工特征，训练4种DL算法（LSTM、BiLSTM、GRU、1D-CNN）和6种传统ML算法（RF、SVM、XGBoost、LR、ANN、DT）

Result: DL算法使用原始信号准确度约85%，传统ML和DL使用特征提取准确度70-81%，中高强度运动分类错误较高

Conclusion: 深度学习使用原始加速度信号在24小时运动行为分类中仅略好于传统机器学习方法，性能差异不显著

Abstract: Purpose: We compared the performance of deep learning (DL) and classical
machine learning (ML) algorithms for the classification of 24-hour movement
behavior into sleep, sedentary, light intensity physical activity (LPA), and
moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access
data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.
Participants were randomly divided into training, validation, and test sets
(121, 15, and 15 participants each). Raw acceleration signals were segmented
into non-overlapping 10-second windows, and then a total of 104 handcrafted
features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),
Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and
One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw
acceleration signals and with handcrafted features extracted from these signals
to predict 24-hour movement behavior categories. The handcrafted features were
also used to train classical ML algorithms, namely Random Forest (RF), Support
Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression
(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying
24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an
overall accuracy of approximately 85% when trained with raw acceleration
signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on
handcrafted features, the overall accuracy for both DL and classical ML
algorithms ranged from 70% to 81%. Overall, there was a higher confusion in
classification of MVPA and LPA, compared to sleep and sedentary categories.
Conclusion: DL methods with raw acceleration signals had only slightly better
performance in predicting 24-hour movement behavior intensities, compared to
when DL and classical ML were trained with handcrafted features.

</details>


### [55] [Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models](https://arxiv.org/abs/2509.08372)
*Kosuke Kihara,Junki Mori,Taiki Miyagawa,Akinori F. Ebihara*

Main category: cs.LG

TL;DR: 本文提出了CI-FFREEDA框架，通过使用冻结的视觉基础模型(VFM)替代传统骨干网络，有效解决了联邦学习中类别不平衡、域差距和非IID数据分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦自由源域适应(FFREEDA)方法在处理源域和目标域类别不平衡、标签偏移以及目标客户端间非IID分布时表现不佳，需要更有效的解决方案。

Method: 使用冻结的视觉基础模型(VFM)作为特征提取器，替代传统的可训练骨干网络，无需大量参数调优，同时降低计算和通信成本。

Result: 实验结果表明，VFM能有效缓解域差距、类别不平衡和目标客户端间非IID性的影响，显著提高了整体准确率。

Conclusion: 强大的特征提取器而非复杂的适应或联邦学习方法，是现实世界联邦学习成功的关键，VFM为此提供了有效的解决方案。

Abstract: Federated Learning (FL) offers a framework for training models
collaboratively while preserving data privacy of each client. Recently,
research has focused on Federated Source-Free Domain Adaptation (FFREEDA), a
more realistic scenario wherein client-held target domain data remains
unlabeled, and the server can access source domain data only during
pre-training. We extend this framework to a more complex and realistic setting:
Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account class
imbalances in both the source and target domains, as well as label shifts
between source and target and among target clients. The replication of existing
methods in our experimental setup lead us to rethink the focus from enhancing
aggregation and domain adaptation methods to improving the feature extractors
within the network itself. We propose replacing the FFREEDA backbone with a
frozen vision foundation model (VFM), thereby improving overall accuracy
without extensive parameter tuning and reducing computational and communication
costs in federated learning. Our experimental results demonstrate that VFMs
effectively mitigate the effects of domain gaps, class imbalances, and even
non-IID-ness among target clients, suggesting that strong feature extractors,
not complex adaptation or FL methods, are key to success in the real-world FL.

</details>


### [56] [Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective](https://arxiv.org/abs/2509.08697)
*James Gong,Raymond Luo,Emma Wang,Leon Ge,Bruce Li,Felix Marattukalam,Waleed Abdulla*

Main category: cs.LG

TL;DR: FAUST算法通过将Forward-Forward算法与相似性学习框架结合，消除了推理时多次前向传播的需求，在保持生物合理性的同时显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统反向传播算法的生物不合理性（反向锁定和全局误差传播）以及Forward-Forward算法准确率低、推理效率差的问题。

Method: 将Forward-Forward算法与基于相似性的Tuplet损失函数框架相结合，在训练过程中使用两个前向传播，但在推理时只需要单次前向传播。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上显著提升准确率，CIFAR-10上达到56.22%的准确率，接近反向传播的57.63%基准。

Conclusion: FAUST算法成功解决了Forward-Forward算法的效率问题，在保持生物合理性的同时大幅缩小了与反向传播算法的性能差距。

Abstract: Backpropagation is the pivotal algorithm underpinning the success of
artificial neural networks, yet it has critical limitations such as
biologically implausible backward locking and global error propagation. To
circumvent these constraints, the Forward-Forward algorithm was proposed as a
more biologically plausible method that replaces the backward pass with an
additional forward pass. Despite this advantage, the Forward-Forward algorithm
significantly trails backpropagation in accuracy, and its optimal form exhibits
low inference efficiency due to multiple forward passes required. In this work,
the Forward-Forward algorithm is reshaped through its integration with
similarity learning frameworks, eliminating the need for multiple forward
passes during inference. This proposed algorithm is named Forward-Forward
Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical
evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST
substantially improves accuracy, narrowing the gap with backpropagation. On
CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron
architecture, approaching the backpropagation benchmark of 57.63\% accuracy.

</details>


### [57] [A layered architecture for log analysis in complex IT systems](https://arxiv.org/abs/2509.08698)
*Thorsten Wittkopp*

Main category: cs.LG

TL;DR: 提出三层架构支持DevOps故障解决：日志调查层实现自动日志标注和异常分类，异常检测层提供灵活检测方法（F1-score 0.98-1.0），根因分析层能识别90-98%的根因日志行


<details>
  <summary>Details</summary>
Motivation: IT系统复杂性增加给DevOps团队带来实施和维护挑战，日志分析作为AIOps核心要素，需要为故障解决提供有效支持

Method: 三层架构：1) 日志调查层 - 自动日志标注和异常分类；2) 异常检测层 - 灵活适应无监督、弱监督和监督训练的异常检测方法；3) 根因分析层 - 识别最小日志集、故障来源和事件序列

Result: 异常检测在公开和工业数据集上F1-score达到0.98-1.0；根因分析能在前10候选者中检测到90-98%的根因日志行

Conclusion: 通过集成三层架构，为团队提供增强IT系统可靠性的稳健方法，有效帮助DevOps高效解决故障

Abstract: In the evolving IT landscape, stability and reliability of systems are
essential, yet their growing complexity challenges DevOps teams in
implementation and maintenance. Log analysis, a core element of AIOps, provides
critical insights into complex behaviors and failures. This dissertation
introduces a three-layered architecture to support DevOps in failure
resolution. The first layer, Log Investigation, performs autonomous log
labeling and anomaly classification. We propose a method that labels log data
without manual effort, enabling supervised training and precise evaluation of
anomaly detection. Additionally, we define a taxonomy that groups anomalies
into three categories, ensuring appropriate method selection. The second layer,
Anomaly Detection, detects behaviors deviating from the norm. We propose a
flexible Anomaly Detection method adaptable to unsupervised, weakly supervised,
and supervised training. Evaluations on public and industry datasets show
F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third
layer, Root Cause Analysis, identifies minimal log sets describing failures,
their origin, and event sequences. By balancing training data and identifying
key services, our Root Cause Analysis method consistently detects 90-98% of
root cause log lines within the top 10 candidates, providing actionable
insights for mitigation. Our research addresses how log analysis methods can be
designed and optimized to help DevOps resolve failures efficiently. By
integrating these three layers, the architecture equips teams with robust
methods to enhance IT system reliability.

</details>


### [58] [Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models](https://arxiv.org/abs/2509.08401)
*Xunkai Li,Daohan Su,Sicheng Liu,Ru Zhang,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 论文提出了MoT方法来解决图基础模型中的领域泛化冲突问题，通过信息调整和正则化调整来改善编码器退化与表示坍塌两个核心问题，在多个领域数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 图基础模型在多领域图数据预训练中存在领域泛化冲突，具体表现为模型退化（编码器无法捕捉输入多样性）和表示坍塌（隐藏表示失去语义可分性），这些问题源于信息瓶颈和正则化缺失。

Method: 提出MoT框架：1）信息调整器-使用边级别语义融合策略和混合码本进行领域感知路由；2）正则化调整器-增加两个正则化项来改善梯度监督。该方法遵循GFM的缩放定律，提供可控模型规模。

Result: 在6个领域的22个数据集上的实验表明，MoT在全监督、少样本和零样本场景下均显著优于现有最优基线方法。

Conclusion: MoT通过同时解决信息瓶颈和正则化缺失问题，有效缓解了图基础模型预训练中的优化困境，为多领域图表示学习提供了有效的解决方案。

Abstract: Graph foundation models, inspired by the success of LLMs, are designed to
learn the optimal embedding from multi-domain TAGs for the downstream
cross-task generalization capability. During our investigation, graph VQ-MAE
stands out among the increasingly diverse landscape of GFM architectures. This
is attributed to its ability to jointly encode topology and textual attributes
from multiple domains into discrete embedding spaces with clear semantic
boundaries. Despite its potential, domain generalization conflicts cause
imperceptible pitfalls. In this paper, we instantiate two of them, and they are
just like two sides of the same GFM optimization coin - Side 1 Model
Degradation: The encoder and codebook fail to capture the diversity of inputs;
Side 2 Representation Collapse: The hidden embedding and codebook vector fail
to preserve semantic separability due to constraints from narrow representation
subspaces. These two pitfalls (sides) collectively impair the decoder and
generate the low-quality reconstructed supervision, causing the GFM
optimization dilemma during pre-training (coin). Through empirical
investigation, we attribute the above challenges to Information Bottleneck and
Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -
(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic
fusion strategy and a mixture-of-codebooks with domain-aware routing to improve
information capacity. (2) Regularization Tinker for Optimization Coin, which
utilizes two additional regularizations to further improve gradient supervision
in our proposed Information Tinker. Notably, as a flexible architecture, MoT
adheres to the scaling laws of GFM, offering a controllable model scale.
Compared to SOTA baselines, experiments on 22 datasets across 6 domains
demonstrate that MoT achieves significant improvements in supervised, few-shot,
and zero-shot scenarios.

</details>


### [59] [DEQuify your force field: More efficient simulations using deep equilibrium models](https://arxiv.org/abs/2509.08734)
*Andreas Burger,Luca Thiede,Alán Aspuru-Guzik,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: 将等变力场模型重构为深度平衡模型，利用分子动力学模拟的连续性特征，通过重用先前时间步的中间特征，在精度和速度上提升10-20%


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟具有连续性特征，连续状态极其相似，这一先验信息尚未被充分利用

Method: 将最先进的等变基础模型重构为深度平衡模型，重用先前时间步的中间神经网络特征

Result: 在MD17、MD22和OC20 200k数据集上，相比非DEQ基础模型，精度和速度均提升10-20%，训练内存效率更高，能够训练更大系统的更表达性模型

Conclusion: 利用分子动力学模拟的连续性特征，通过深度平衡模型重用中间特征，能够显著提升机器学习力场的性能和效率

Abstract: Machine learning force fields show great promise in enabling more accurate
molecular dynamics simulations compared to manually derived ones. Much of the
progress in recent years was driven by exploiting prior knowledge about
physical systems, in particular symmetries under rotation, translation, and
reflections. In this paper, we argue that there is another important piece of
prior information that, thus fa,r hasn't been explored: Simulating a molecular
system is necessarily continuous, and successive states are therefore extremely
similar. Our contribution is to show that we can exploit this information by
recasting a state-of-the-art equivariant base model as a deep equilibrium
model. This allows us to recycle intermediate neural network features from
previous time steps, enabling us to improve both accuracy and speed by
$10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ
base model. The training is also much more memory efficient, allowing us to
train more expressive models on larger systems.

</details>


### [60] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出了AgentGym-RL框架和ScalingInter-RL训练方法，通过强化学习训练LLM智能体进行多轮交互决策，在27个任务上达到或超过商业模型性能


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的交互式强化学习框架来训练LLM智能体从零开始解决复杂现实任务，而无需依赖监督微调

Method: 开发了模块化、解耦的AgentGym-RL框架，支持主流RL算法；提出ScalingInter-RL训练方法，在早期阶段限制交互次数强调利用，后期逐步转向探索以鼓励多样化策略

Result: 在多样化环境的27个任务上，训练的智能体达到或超过商业模型性能，证明了框架的稳定性和有效性

Conclusion: 该框架为开发下一代智能体提供了关键洞见，将开源完整框架（包括代码和数据集）以赋能研究社区

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [61] [An Interpretable Deep Learning Model for General Insurance Pricing](https://arxiv.org/abs/2509.08467)
*Patrick J. Laub,Tu Pho,Bernard Wong*

Main category: cs.LG

TL;DR: 提出了一种用于保险定价的可解释深度学习模型ANAM，通过为每个协变量和交互项分配专用神经网络，在保持神经网络强大预测能力的同时提供完全透明的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统精算方法预测能力有限，而现代机器学习方法缺乏可解释性。保险定价需要既准确又完全透明的模型以满足监管要求和业务理解。

Method: 为每个协变量和成对交互项分配专用神经网络或子网络，通过架构约束实现稀疏性、平滑性和单调性等保险应用必需的特性。

Result: 在合成和真实保险数据集上的实验表明，该模型在大多数情况下预测精度优于传统精算方法和先进机器学习方法，同时保持完全透明。

Conclusion: ANAM模型成功实现了预测精度和可解释性的平衡，为保险定价提供了既强大又透明的深度学习解决方案。

Abstract: This paper introduces the Actuarial Neural Additive Model, an inherently
interpretable deep learning model for general insurance pricing that offers
fully transparent and interpretable results while retaining the strong
predictive power of neural networks. This model assigns a dedicated neural
network (or subnetwork) to each individual covariate and pairwise interaction
term to independently learn its impact on the modeled output while implementing
various architectural constraints to allow for essential interpretability (e.g.
sparsity) and practical requirements (e.g. smoothness, monotonicity) in
insurance applications. The development of our model is grounded in a solid
foundation, where we establish a concrete definition of interpretability within
the insurance context, complemented by a rigorous mathematical framework.
Comparisons in terms of prediction accuracy are made with traditional actuarial
and state-of-the-art machine learning methods using both synthetic and real
insurance datasets. The results show that the proposed model outperforms other
methods in most cases while offering complete transparency in its internal
logic, underscoring the strong interpretability and predictive capability.

</details>


### [62] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 深度强化学习AI系统在模拟大规模伤交事件中优化患者转运决策，使非专家能达到专家水平，显著提升决策质量和一致性


<details>
  <summary>Details</summary>
Motivation: 大规模伤交事件给医疗系统带来巨大压力，需要在极端压力下做出快速、准确的患者-医院分配决策

Method: 开发深度强化学习决策支持AI系统，统筹患者疼痛程度、特殊照护需求、医院容量和运输逻辑，并建立MasTER网页命令控制板进行模拟

Result: 在30名参与者（6名伤害专家和24名非专家）的用户研究中，AI参与度增加显著提高决策质量和一致性。AI系统表现超过伤害外科医生（p < 0.001），并使非专家在协作中达到专家水平

Conclusion: 这些发现证明了我们的AI驱动决策支持系统在提高大规模伤交事件预防训练和真实应急响应管理方面的潜力

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


### [63] [SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms](https://arxiv.org/abs/2509.08482)
*Andrea Maldonado,Christian M. M. Frey,Sai Anirudh Aryasomayajula,Ludwig Zellner,Stephan A. Fahrenkrog-Petersen,Thomas Seidl*

Main category: cs.LG

TL;DR: SHAining方法首次量化事件日志特征对流程挖掘算法性能的边际贡献，通过分析22,000多个事件日志，揭示了哪些特征对算法指标影响最大，并评估算法鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在固定的事件日志集上评估算法，缺乏对事件日志特征如何单独影响算法的系统性分析，且忽视了特征之间的共现关系。

Method: 提出SHAining方法，使用流程发现作为下游任务，分析超过22,000个覆盖广泛特征的事件日志，量化事件日志特征对算法指标的边际贡献。

Result: 揭示了哪些事件日志特征对算法指标（如拟合度、精确度、复杂度）影响最大，并提供了关于特征值与贡献影响相关性的新见解。

Conclusion: SHAining方法能够系统性地评估事件日志特征对流程挖掘算法性能的影响，为算法选择和评估提供了更深入的见解。

Abstract: Process mining aims to extract and analyze insights from event logs, yet
algorithm metric results vary widely depending on structural event log
characteristics. Existing work often evaluates algorithms on a fixed set of
real-world event logs but lacks a systematic analysis of how event log
characteristics impact algorithms individually. Moreover, since event logs are
generated from processes, where characteristics co-occur, we focus on
associational rather than causal effects to assess how strong the overlapping
individual characteristic affects evaluation metrics without assuming isolated
causal effects, a factor often neglected by prior work. We introduce SHAining,
the first approach to quantify the marginal contribution of varying event log
characteristics to process mining algorithms' metrics. Using process discovery
as a downstream task, we analyze over 22,000 event logs covering a wide span of
characteristics to uncover which affect algorithms across metrics (e.g.,
fitness, precision, complexity) the most. Furthermore, we offer novel insights
about how the value of event log characteristics correlates with their
contributed impact, assessing the algorithm's robustness.

</details>


### [64] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: MoT是一个轻量级框架，通过交替进行教师特定监督微调和权重空间合并，将多个教师的推理能力统一到学生模型中，在数学竞赛基准上仅用200个高质量CoT样本就超越了多个强大模型。


<details>
  <summary>Details</summary>
Motivation: 传统推理蒸馏假设单一最优教师，但实践中存在多个候选教师和不断增长的CoT语料库。研究发现不同学生有不同的"最佳教师"，甚至同一学生在不同数据集上的最佳教师也不同。

Method: 提出Merge-of-Thought Distillation (MoT)框架，交替进行教师特定的监督微调分支和结果学生变体的权重空间合并，以统一多个教师的推理能力并克服监督冲突。

Result: 在数学竞赛基准上，仅使用约200个高质量CoT样本，MoT应用于Qwen3-14B学生模型超越了DEEPSEEK-R1、QWEN3-30B-A3B等多个强大模型，性能显著提升。

Conclusion: MoT持续优于最佳单教师蒸馏和朴素多教师联合方法，提高了性能上限同时减轻过拟合，对分布偏移和同级教师表现出鲁棒性，还能减少灾难性遗忘并提升数学之外的通用推理能力。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


### [65] [Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis](https://arxiv.org/abs/2509.08483)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 该论文分析了带Polyak重球动量的梯度下降算法，证明了在指数吸引不变流形上，该算法等价于带修正损失函数的普通梯度下降，并提供了任意精度的全局近似边界和连续修正方程。


<details>
  <summary>Details</summary>
Motivation: 研究带重球动量的梯度下降算法的内在机制，通过数学分析揭示其与修正损失函数梯度下降的等价性，为理解动量方法的优化特性提供理论依据。

Method: 基于Kovachki和Stuart(2021)的工作，在指数吸引不变流形上分析算法，使用组合数学方法分析无记忆近似，推导任意阶的连续修正方程和主流动量近似。

Result: 证明了算法在小步长下等价于带修正损失函数的梯度下降，获得了O(h^R)的全局近似边界，发现了包含Eulerian和Narayana多项式的丰富多项式族。

Conclusion: 研究结果深入揭示了重球动量梯度下降的主要特性，为分析其他优化算法提供了路线图，理论结果适用于全批次和小批次HB算法。

Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed
momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory.
Building on Kovachki and Stuart (2021), we prove that on an exponentially
attractive invariant manifold the algorithm is exactly plain gradient descent
with a modified loss, provided that the step size $h$ is small enough. Although
the modified loss does not admit a closed-form expression, we describe it with
arbitrary precision and prove global (finite "time" horizon) approximation
bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a
fine-grained analysis of the combinatorics underlying the memoryless
approximations of HB, in particular, finding a rich family of polynomials in
$\beta$ hidden inside which contains Eulerian and Narayana polynomials. We
derive continuous modified equations of arbitrary approximation order (with
rigorous bounds) and the principal flow that approximates the HB dynamics,
generalizing Rosca et al. (2023). Approximation theorems cover both full-batch
and mini-batch HB. Our theoretical results shed new light on the main features
of gradient descent with heavy-ball momentum, and outline a road-map for
similar analysis of other optimization algorithms.

</details>


### [66] [Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks](https://arxiv.org/abs/2509.08499)
*Chisom Chibuike,Adeyinka Ogunsanya*

Main category: cs.LG

TL;DR: 通过系统性对比10种优化器在心脏病预测任务中的表现，发现RMSProp在收敛速度、稳定性和分类指标方面平衡最佳，被评为最有效的优化器。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化器选择缺乏系统性研究，需要深入探讨优化器选择标准和评估指标。

Method: 使用心脏病数据集训练MLP模型，在一致的训练范式下比较10种优化器的收敛速度、稳定性、AUC、精度和召回率等指标。

Result: RMSProp表现最优：精度0.765、召回率0.827、AUC 0.841，训练时间较快，但稳定性不是最好。Adagrad和Adadelta稳定性更好但收敛更慢。

Conclusion: 建议在计算资源充足时通过系统性评估选择优化器，以提高深度学习模型训练的科学性和性能。

Abstract: Optimization has been an important factor and topic of interest in training
deep learning models, yet less attention has been given to how we select the
optimizers we use to train these models. Hence, there is a need to dive deeper
into how we select the optimizers we use for training and the metrics that
determine this selection. In this work, we compare the performance of 10
different optimizers in training a simple Multi-layer Perceptron model using a
heart disease dataset from Kaggle. We set up a consistent training paradigm and
evaluate the optimizers based on metrics such as convergence speed and
stability. We also include some other Machine Learning Evaluation metrics such
as AUC, Precision, and Recall, which are central metrics to classification
problems. Our results show that there are trade-offs between convergence speed
and stability, as optimizers like Adagrad and Adadelta, which are more stable,
took longer time to converge. Across all our metrics, we chose RMSProp to be
the most effective optimizer for this heart disease prediction task because it
offered a balanced performance across key metrics. It achieved a precision of
0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.
However, it was not the most stable. We recommend that, in less
compute-constrained environments, this method of choosing optimizers through a
thorough evaluation should be adopted to increase the scientific nature and
performance in training deep learning models.

</details>


### [67] [Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures](https://arxiv.org/abs/2509.08530)
*Wen-Bo Xie,Xun Fu,Bin Chen,Yan-Li Lee,Tao Deng,Tian Zou,Xin Wang,Zhen Liu,Jaideep Srivastavad*

Main category: cs.LG

TL;DR: 提出基于图的主动聚类算法，使用两个稀疏图来降低计算成本、减少标注需求并节省内存，在保持鲁棒性的同时显著提升聚类准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据处理中基于成对约束的主动聚类的效率和可扩展性问题，特别是在数据挖掘、知识标注和AI模型预训练等应用中需要降低计算成本、减少用户标注需求并节省内存使用。

Method: 提出图基主动聚类算法，利用两个稀疏图：一个用于表示数据间关系的数据骨架，另一个用于更新数据骨架。这两个图协同工作，通过细化数据骨架中的连通子图来创建嵌套聚类。

Result: 实证分析表明，该算法能够以显著更少的用户约束输入实现更准确的聚类，在计算性能和可扩展性方面优于同类方法，且在不同距离度量下保持鲁棒性。

Conclusion: 所提出的图基主动聚类算法有效解决了大规模数据处理中的效率和可扩展性挑战，在减少计算成本、标注需求和内存使用的同时实现了高质量的聚类结果。

Abstract: In this work, we focus on the efficiency and scalability of pairwise
constraint-based active clustering, crucial for processing large-scale data in
applications such as data mining, knowledge annotation, and AI model
pre-training. Our goals are threefold: (1) to reduce computational costs for
iterative clustering updates; (2) to enhance the impact of user-provided
constraints to minimize annotation requirements for precise clustering; and (3)
to cut down memory usage in practical deployments. To achieve these aims, we
propose a graph-based active clustering algorithm that utilizes two sparse
graphs: one for representing relationships between data (our proposed data
skeleton) and another for updating this data skeleton. These two graphs work in
concert, enabling the refinement of connected subgraphs within the data
skeleton to create nested clusters. Our empirical analysis confirms that the
proposed algorithm consistently facilitates more accurate clustering with
dramatically less input of user-provided constraints, and outperforms its
counterparts in terms of computational performance and scalability, while
maintaining robustness across various distance metrics.

</details>


### [68] [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](https://arxiv.org/abs/2509.08578)
*Hong Liu*

Main category: cs.LG

TL;DR: MAESTRO是一个多模态自适应集成模型，用于流感发病率预测，通过融合监测数据、网络搜索趋势和气象数据，结合频谱-时间架构，实现了0.956的R-square性能。


<details>
  <summary>Details</summary>
Motivation: 及时准确的流感发病率预测对公共卫生决策至关重要，需要开发能够融合多模态数据并处理时间序列复杂特性的鲁棒预测模型。

Method: 采用多模态自适应集成方法，首先将时间序列分解为季节性和趋势成分，然后通过Transformer编码器、Mamba状态空间模型、多尺度时间卷积和频域分析模块进行混合特征增强，使用跨通道注意力机制整合不同数据模态。

Result: 在香港11年流感数据上的评估显示，MAESTRO具有优异的竞争性能，模型拟合度和相对准确性均表现突出，达到了0.956的state-of-the-art R-square值。

Conclusion: MAESTRO证明了先进频谱-时间建模与多模态数据融合的协同作用，为流行病学预测提供了强大统一的框架，其模块化和可复现的管道有助于在其他地区和病原体上部署和扩展。

Abstract: Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

</details>


### [69] [Towards Interpretable Deep Neural Networks for Tabular Data](https://arxiv.org/abs/2509.08617)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab是一种可解释的神经网络架构，通过稀疏自编码器学习单语义特征字典，将预测表示为语义组件的线性组合，在保持高性能的同时实现完全可解释性


<details>
  <summary>Details</summary>
Motivation: 虽然针对表格数据的DNN在预测性能上具有竞争力，但它们是黑盒模型，缺乏可解释性，这在金融和医疗等关键领域应用受限

Method: 使用稀疏自编码器(SAE)在预测潜在空间中学习单语义特征字典，并通过自动化方法为这些特征分配人类可解释的语义

Result: 实证评估显示XNNTab在性能上与最先进的黑盒神经网络模型和经典机器学习方法相当或更优，同时保持完全可解释性

Conclusion: XNNTab成功解决了表格数据建模中性能与可解释性之间的权衡问题，为关键领域的应用提供了既高性能又可解释的解决方案

Abstract: Tabular data is the foundation of many applications in fields such as finance
and healthcare. Although DNNs tailored for tabular data achieve competitive
predictive performance, they are blackboxes with little interpretability. We
introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to
learn a dictionary of monosemantic features within the latent space used for
prediction. Using an automated method, we assign human-interpretable semantics
to these features. This allows us to represent predictions as linear
combinations of semantically meaningful components. Empirical evaluations
demonstrate that XNNTab attains performance on par with or exceeding that of
state-of-the-art, black-box neural models and classical machine learning
approaches while being fully interpretable.

</details>


### [70] [An upper bound of the silhouette validation metric for clustering](https://arxiv.org/abs/2509.08625)
*Hugo Sträng,Tai Dinh*

Main category: cs.LG

TL;DR: 本文提出了针对轮廓系数的数据依赖上界，为聚类质量评估提供了更精确的基准，解决了传统上界1通常无法达到的问题。


<details>
  <summary>Details</summary>
Motivation: 轮廓系数是常用的聚类质量内部评估指标，但传统的上界1在实际数据集中往往无法达到，且数据集特定的最大轮廓宽度未知，这限制了该指标的有效性。

Method: 通过为数据集中的每个数据点推导出其轮廓宽度的尖锐上界，然后聚合这些个体上界，得到ASW的规范数据依赖上界。

Result: 在合成和真实数据集上的实验表明，所提出的上界在许多情况下被证明是接近紧致的，显著丰富了聚类质量评估。

Conclusion: 该方法能够指示单个数据点是否能够被良好放置，支持基于轮廓的优化循环的早期停止，并帮助评估聚类结果与特定数据上最佳可能结果的接近程度。

Abstract: The silhouette coefficient summarizes, per observation, cohesion versus
separation in [-1, 1]; the average silhouette width (ASW) is a common internal
measure of clustering quality where higher values indicate more coveted
results. However, the dataset-specific maximum of ASW is typically unknown, and
the standard upper limit 1 is often unattainable. In this work, we derive for
each data point in a given dataset a sharp upper bound on its silhouette width.
By aggregating these individual bounds, we present a canonical data-dependent
upper bound on ASW that often assumes values well below 1. The presented bounds
can indicate whether individual data points can ever be well placed, enable
early stopping of silhouette-based optimization loops, and help answer a key
question: How close is my clustering result to the best possible outcome on
this specific data? Across synthetic and real datasets, the bounds are provably
near-tight in many cases and offer significant enrichment of cluster quality
evaluation.

</details>


### [71] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: GDR框架使用预训练生成模型将有问题的数据集转换为更适合训练的净化数据集，解决数据枯竭问题


<details>
  <summary>Details</summary>
Motivation: 随着模型训练数据需求超过网络索引数据增长速度，面临数据枯竭问题，而用户生成内容存在隐私泄露和不良内容风险

Method: 使用预训练生成模型对每个真实数据样本生成条件化合成数据，将含有不良内容的数据集转换为净化数据集

Result: GDR在数据集匿名化方面优于工业级解决方案，能够直接对高度不安全数据集进行去毒处理，合成数据自然匹配网络规模数据集的多样性

Conclusion: GDR的简单性和有效性使其成为扩展前沿模型训练数据总量的强大工具

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [72] [Replicable Reinforcement Learning with Linear Function Approximation](https://arxiv.org/abs/2509.08660)
*Eric Eaton,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 该论文提出了首个可证明高效的可复制强化学习算法，用于线性马尔可夫决策过程，解决了函数逼近设置中的可复制性问题


<details>
  <summary>Details</summary>
Motivation: 机器学习领域实验结果复现性一直是个挑战，特别是在强化学习中算法不稳定。虽然表格RL设置中存在可复制算法，但扩展到更实用的函数逼近设置仍是一个开放问题

Method: 首先开发了可复制随机设计回归和未中心化协方差估计的高效算法，然后利用这些工具为线性MDP提供了首个可证明高效的可复制RL算法，涵盖生成模型和情景设置

Result: 成功开发了可复制的线性函数逼近RL方法，并通过实验验证了算法效果，展示了如何激发更一致的神经策略

Conclusion: 这项工作在可复制强化学习方面取得了重要进展，为函数逼近设置提供了首个可证明高效的可复制算法，对提高RL算法的稳定性和一致性具有重要意义

Abstract: Replication of experimental results has been a challenge faced by many
scientific disciplines, including the field of machine learning. Recent work on
the theory of machine learning has formalized replicability as the demand that
an algorithm produce identical outcomes when executed twice on different
samples from the same distribution. Provably replicable algorithms are
especially interesting for reinforcement learning (RL), where algorithms are
known to be unstable in practice. While replicable algorithms exist for tabular
RL settings, extending these guarantees to more practical function
approximation settings has remained an open problem. In this work, we make
progress by developing replicable methods for linear function approximation in
RL. We first introduce two efficient algorithms for replicable random design
regression and uncentered covariance estimation, each of independent interest.
We then leverage these tools to provide the first provably efficient replicable
RL algorithms for linear Markov decision processes in both the generative model
and episodic settings. Finally, we evaluate our algorithms experimentally and
show how they can inspire more consistent neural policies.

</details>


### [73] [Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data](https://arxiv.org/abs/2509.08679)
*Jingya Cheng,Jiazi Tian,Federica Spoto,Alaleh Azhir,Daniel Mork,Hossein Estiri*

Main category: cs.LG

TL;DR: 这篇论文提出了一种诊断信号保真指数(SFI)来量化电子健康记录中诊断数据质量，并通过SFI敏感的检测检验来提高机器学习模型在不同医疗系统间的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在不同医疗系统间的性能洞退通常因分布偏移导致，而诊断信号衰减是一个根本但被忽视的因素，影响着诊断代码的可靠性。

Method: 建立了一个模拟框架生成2,500个合成数据集，每个数据集包1,000名患者，包含现实的人口统计特征、就诊情况和编码模式。SFI指数来自六个可解释组件：诊断特异性、时间一致性、熵、上下文一致性、药物配合度和趋势稳定性。

Result: 在最优参数(α=2.0)时，SFI敏感的检测检验显著提高了所有指标(p<0.001)，从平衡准确率10.3%到召回率32.5%的提升，精确度和F1分数分别提高31.9%和26.1%。

Conclusion: 诊断信号衰减是模型普适性的可处理障碍。SFI敏感的棆准提供了一种实用的、无标签的策略，可以在缺乏结果标签的大规模管理数据集中提高预测性能。

Abstract: \textbf{Background:} Machine learning models trained on electronic health
records (EHRs) often degrade across healthcare systems due to distributional
shift. A fundamental but underexplored factor is diagnostic signal decay:
variability in diagnostic quality and consistency across institutions, which
affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying
diagnostic data quality at the patient level in dementia, and to test SFI-aware
calibration for improving model performance across heterogeneous datasets
without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic
datasets, each with 1,000 patients and realistic demographics, encounters, and
coding patterns based on dementia risk factors. The SFI was derived from six
interpretable components: diagnostic specificity, temporal consistency,
entropy, contextual concordance, medication alignment, and trajectory
stability. SFI-aware calibration applied a multiplicative adjustment, optimized
across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware
calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from
10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in
Precision (31.9\%) and F1-score (26.1\%). Performance approached reference
standards, with F1-score and Recall within 1\% and Balanced Accuracy and
Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model
generalization. SFI-aware calibration provides a practical, label-free strategy
to enhance prediction across healthcare contexts, particularly for large-scale
administrative datasets lacking outcome labels.

</details>


### [74] [Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping](https://arxiv.org/abs/2509.08703)
*Nikasadat Emami,Amirhossein Khalilian-Gourtani,Jianghao Qian,Antoine Ratouchniak,Xupeng Chen,Yao Wang,Adeen Flinker*

Main category: cs.LG

TL;DR: 开发机器学习模型使用ECoG数据预测大脑语言关键区域，结合神经活动、解剖区域和功能连接特征，性能优于单一特征模型，准确率达到ROC-AUC 0.87


<details>
  <summary>Details</summary>
Motivation: ESM虽然是临床金标准但具有侵入性和耗时，需要开发非侵入性替代方法来识别语言关键皮层区域以确保脑部手术安全

Method: 分析16名参与者的颅内ECoG数据，整合神经活动信号、解剖区域标签和功能连接特征，使用RBF核SVM进行试验级预测，并通过MLP聚合电极级分类

Result: 结合区域和连接特征的模型性能与完整特征集相当，优于单一特征模型，最佳模型在保留参与者上达到ROC-AUC 0.87和PR-AUC 0.57

Conclusion: 结合空间和网络信息与非线形建模可显著改善术前功能映射，为ESM提供有前景的替代方案

Abstract: Identifying cortical regions critical for speech is essential for safe brain
surgery in or near language areas. While Electrical Stimulation Mapping (ESM)
remains the clinical gold standard, it is invasive and time-consuming. To
address this, we analyzed intracranial electrocorticographic (ECoG) data from
16 participants performing speech tasks and developed machine learning models
to directly predict if the brain region underneath each ECoG electrode is
critical. Ground truth labels indicating speech arrest were derived
independently from Electrical Stimulation Mapping (ESM) and used to train
classification models. Our framework integrates neural activity signals,
anatomical region labels, and functional connectivity features to capture both
local activity and network-level dynamics. We found that models combining
region and connectivity features matched the performance of the full feature
set, and outperformed models using either type alone. To classify each
electrode, trial-level predictions were aggregated using an MLP applied to
histogram-encoded scores. Our best-performing model, a trial-level RBF-kernel
Support Vector Machine together with MLP-based aggregation, achieved strong
accuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings
highlight the value of combining spatial and network information with
non-linear modeling to improve functional mapping in presurgical evaluation.

</details>


### [75] [Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing](https://arxiv.org/abs/2509.08709)
*Shun Takagi,Satoshi Hasegawa*

Main category: cs.LG

TL;DR: 提出了一种基于短暂TEE模块的服务器扩展方案，实现恶意安全环境下的差分隐私联邦学习，通过可验证证明和客户端审计来确保服务器行为的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有DP-FTRL方法假设服务器是半诚实的，无法应对实际场景中客户端退出或被破坏的情况。TEE直接实现可能引入分叉攻击或可用性问题。

Method: 在服务器端部署短暂TEE模块作为可信计算基(TCB)，生成服务器行为的可验证证明。被选中的客户端参与审计这些证明，通信和计算开销很小。

Result: 方案在保持系统可扩展性和活跃性的同时减小了TCB规模。实验表明在多种实际设置下对客户端只增加很小的常数开销。

Conclusion: 该框架成功解决了恶意服务器环境下的DP-FTRL安全问题，通过TEE和客户端审计的结合提供了形式化的隐私保证，具有实际部署价值。

Abstract: In cross-device private federated learning, differentially private
follow-the-regularized-leader (DP-FTRL) has emerged as a promising
privacy-preserving method. However, existing approaches assume a semi-honest
server and have not addressed the challenge of securely removing this
assumption. This is due to its statefulness, which becomes particularly
problematic in practical settings where clients can drop out or be corrupted.
While trusted execution environments (TEEs) might seem like an obvious
solution, a straightforward implementation can introduce forking attacks or
availability issues due to state management. To address this problem, our paper
introduces a novel server extension that acts as a trusted computing base (TCB)
to realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral
TEE module on the server side to produce verifiable proofs of server actions.
Some clients, upon being selected, participate in auditing these proofs with
small additional communication and computational demands. This extension
solution reduces the size of the TCB while maintaining the system's scalability
and liveness. We provide formal proofs based on interactive differential
privacy, demonstrating privacy guarantee in malicious settings. Finally, we
experimentally show that our framework adds small constant overhead to clients
in several realistic settings.

</details>


### [76] [Compressing CNN models for resource-constrained systems by channel and layer pruning](https://arxiv.org/abs/2509.08714)
*Ahmed Sadaqa,Di Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的混合剪枝框架，结合了通道剪枝和层剪枝两种技术，能够在保持模型准确性的同时显著降低模型复杂度和运行延迟。


<details>
  <summary>Details</summary>
Motivation: 随着卷积神经网络复杂度和规模的增加，在边缘设备上部署大型模型遇到挑战，需要通过模型压缩技术来减小网络规模和复杂度。

Method: 受EfficientNet启发，提出了一种混合剪枝框架，同时进行通道剪枝和层剪枝。该方法采用与EfficientNet相反的方向，通过缩小网络来降低模型复杂度。

Result: 实验结果显示，混合剪枝方法能够显著降低模型的总体复杂度，而准确性与基准模型相比仅有最小程度的下降。在NVIDIA JETSON TX2嵌入式AI设备上部署时，这种复杂度降低转化为了运行延迟的减少。

Conclusion: 该研究提出的混合剪枝框架是一种有效的模型压缩方法，能够在缓解边缘设备部署挑战的同时，保持模型的性能表现。

Abstract: Convolutional Neural Networks (CNNs) have achieved significant breakthroughs
in various fields. However, these advancements have led to a substantial
increase in the complexity and size of these networks. This poses a challenge
when deploying large and complex networks on edge devices. Consequently, model
compression has emerged as a research field aimed at reducing the size and
complexity of CNNs. One prominent technique in model compression is model
pruning. This paper will present a new technique of pruning that combines both
channel and layer pruning in what is called a "hybrid pruning framework".
Inspired by EfficientNet, a renowned CNN architecture known for scaling up
networks from both channel and layer perspectives, this hybrid approach applies
the same principles but in reverse, where it scales down the network through
pruning. Experiments on the hybrid approach demonstrated a notable decrease in
the overall complexity of the model, with only a minimal reduction in accuracy
compared to the baseline model. This complexity reduction translates into
reduced latency when deploying the pruned models on an NVIDIA JETSON TX2
embedded AI device.

</details>


### [77] [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
*Jeffrey Amico,Gabriel Passamani Andrade,John Donaghy,Ben Fielding,Tristin Forbus,Harry Grieve,Semih Kara,Jari Kolehmainen,Yihua Lou,Christopher Nies,Edward Phillip Flores Nuño,Diogo Ortega,Shikhar Rastogi,Austin Virts,Matthew J. Wright*

Main category: cs.LG

TL;DR: SAPO是一种完全去中心化的异步RL后训练算法，专为异构计算节点网络设计，解决了传统RL训练中的并行化瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练需要大规模并行化推理，存在延迟、内存、可靠性等技术挑战以及高昂成本。需要一种去中心化的解决方案来克服这些瓶颈。

Method: SAPO算法在异构计算节点网络中运行，每个节点管理自己的策略模型，通过网络"共享"rollouts，无需假设延迟、模型同质性或硬件要求。

Result: 在控制实验中实现了高达94%的累积奖励增益，并在数千个节点的网络上进行了测试，展示了算法的可扩展性和实用性。

Conclusion: SAPO成功解决了RL后训练的扩展瓶颈问题，为去中心化RL训练提供了新的可能性，允许节点在异构环境中高效协作学习。

Abstract: Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs
requires significant parallelization to scale-up inference, which introduces
non-trivial technical challenges (e.g. latency, memory, and reliability)
alongside ever-growing financial costs. We present Swarm sAmpling Policy
Optimization (SAPO), a fully decentralized and asynchronous RL post-training
algorithm. SAPO is designed for decentralized networks of heterogenous compute
nodes, where each node manages its own policy model(s) while "sharing" rollouts
with others in the network; no explicit assumptions about latency, model
homogeneity, or hardware are required and nodes can operate in silo if desired.
As a result, the algorithm avoids common bottlenecks in scaling RL
post-training while also allowing (and even encouraging) new possibilities. By
sampling rollouts "shared" across the network, it enables "Aha moments" to
propagate, thereby bootstrapping the learning process. In this paper we show
SAPO achieved cumulative reward gains of up to 94% in controlled experiments.
We also share insights from tests on a network with thousands of nodes
contributed by Gensyn community members running the algorithm on diverse
hardware and models during an open-source demo.

</details>


### [78] [Data-driven generative simulation of SDEs using diffusion models](https://arxiv.org/abs/2509.08731)
*Xuefeng Gao,Jiale Zha,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 使用扩散模型生成未知随机微分方程样本路径的新方法，无需显式漂移和扩散系数，通过数据驱动方式从有限样本生成新路径


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法需要显式指定随机微分方程的漂移和扩散系数，限制了在未知SDE情况下的应用。本文旨在开发一种模型无关的数据驱动方法

Method: 采用条件扩散模型，给定有限SDE样本路径集，生成相同SDE的新合成路径。与神经SDE等基准方法进行比较验证

Result: 方法在模拟实验中表现有效，在连续时间均值-方差投资组合选择的强化学习应用中，合成样本路径提升了算法性能

Conclusion: 扩散模型在金融分析和决策中具有广阔应用前景，为未知随机微分方程的路径生成提供了有效的数据驱动解决方案

Abstract: This paper introduces a new approach to generating sample paths of unknown
stochastic differential equations (SDEs) using diffusion models, a class of
generative AI models commonly employed in image and video applications. Unlike
the traditional Monte Carlo methods for simulating SDEs, which require explicit
specifications of the drift and diffusion coefficients, our method takes a
model-free, data-driven approach. Given a finite set of sample paths from an
SDE, we utilize conditional diffusion models to generate new, synthetic paths
of the same SDE. To demonstrate the effectiveness of our approach, we conduct a
simulation experiment to compare our method with alternative benchmark ones
including neural SDEs. Furthermore, in an empirical study we leverage these
synthetically generated sample paths to enhance the performance of
reinforcement learning algorithms for continuous-time mean-variance portfolio
selection, hinting promising applications of diffusion models in financial
analysis and decision-making.

</details>


### [79] [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)
*Dong Han,Zhehong Ai,Pengxiang Cai,Shuzhou Sun,Shanya Lu,Jianpeng Chen,Ben Gao,Lingli Ge,Weida Wang,Xiangxin Zhou,Xihui Liu,Mao Su,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Tao XU,Yuqiang Li,Shufei Zhang*

Main category: cs.LG

TL;DR: ChemBOMAS是一个LLM增强的多智能体系统，通过知识驱动的粗粒度优化和数据驱动的细粒度优化策略，显著提升了贝叶斯优化在化学领域的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯优化在化学应用中因实验数据稀疏和反应机制复杂而效率低下的问题。

Method: 采用两阶段策略：1）知识驱动的粗粒度优化：LLM基于化学知识分解搜索空间，识别有希望的候选区域；2）数据驱动的细粒度优化：LLM生成伪数据点增强BO过程，提高数据利用效率和收敛速度。

Result: 在基准测试中显著优于各种BO算法；湿实验验证中达到96%的最优目标值，远超领域专家的15%。

Conclusion: ChemBOMAS是加速化学发现的有力工具，具有实际应用价值。

Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by sparse experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by LLMs and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
LLMs intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, LLMs enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.

</details>


### [80] [PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints](https://arxiv.org/abs/2509.08750)
*Yuanchun Guo,Bingyan Liu,Yulong Sha,Zhensheng Xian*

Main category: cs.LG

TL;DR: 首个实践性异构模型联邦学习平台PracMHBench，用于在边缘设备约束下评估异质模型联邦学习算法的性能和适用性


<details>
  <summary>Details</summary>
Motivation: 异质模型联邦学习比同构模型更具实践性和灵活性，但缺乏在实际边缘设备约束下的系统化评估和定量分析

Method: 构建PracMHBench平台，对多种异质模型算法进行分类和测试，在不同边缘约束条件下进行广泛实验

Result: 观察到各种算法在不同边缘约束下的适用性和对应的异质性模式

Conclusion: 该研究为异质模型联邦学习领域提供了系统化的实践评估平台，有助于理解各算法在实际部署环境下的表现

Abstract: Federating heterogeneous models on edge devices with diverse resource
constraints has been a notable trend in recent years. Compared to traditional
federated learning (FL) that assumes an identical model architecture to
cooperate, model-heterogeneous FL is more practical and flexible since the
model can be customized to satisfy the deployment requirement. Unfortunately,
no prior work ever dives into the existing model-heterogeneous FL algorithms
under the practical edge device constraints and provides quantitative analysis
on various data scenarios and metrics, which motivates us to rethink and
re-evaluate this paradigm. In our work, we construct the first system platform
\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical
constraints of edge devices, where diverse model heterogeneity algorithms are
classified and tested on multiple data tasks and metrics. Based on the
platform, we perform extensive experiments on these algorithms under the
different edge constraints to observe their applicability and the corresponding
heterogeneity pattern.

</details>


### [81] [Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning](https://arxiv.org/abs/2509.08759)
*Mominul Rubel,Adam Meyers,Gabriel Nicolosi*

Main category: cs.LG

TL;DR: FLM是一种新型神经网络架构，使用余弦激活函数构建多维非谐波傅里叶级数，能够学习频率、振幅和相位偏移作为可训练参数，在科学计算问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够表示完整可分离傅里叶基的神经网络架构，解决传统傅里叶启发模型在多维空间中无法表示完整基的问题。

Method: 采用前馈结构，使用余弦激活函数，将频率、振幅和相位偏移作为可训练参数，构建问题特定的谱基。

Result: FLM在偏微分方程和最优控制问题等科学计算任务中表现优于或相当于SIREN和标准前馈神经网络。

Conclusion: FLM是第一个能够在多维空间中表示完整可分离傅里叶基的标准MLP类架构，为科学计算提供了有效的神经网络解决方案。

Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN)
architecture designed to represent a multidimensional nonharmonic Fourier
series. The FLM uses a simple feedforward structure with cosine activation
functions to learn the frequencies, amplitudes, and phase shifts of the series
as trainable parameters. This design allows the model to create a
problem-specific spectral basis adaptable to both periodic and nonperiodic
functions. Unlike previous Fourier-inspired NN models, the FLM is the first
architecture able to represent a complete, separable Fourier basis in multiple
dimensions using a standard Multilayer Perceptron-like architecture. A
one-to-one correspondence between the Fourier coefficients and amplitudes and
phase-shifts is demonstrated, allowing for the translation between a full,
separable basis form and the cosine phase--shifted one. Additionally, we
evaluate the performance of FLMs on several scientific computing problems,
including benchmark Partial Differential Equations (PDEs) and a family of
Optimal Control Problems (OCPs). Computational experiments show that the
performance of FLMs is comparable, and often superior, to that of established
architectures like SIREN and vanilla feedforward NNs.

</details>


### [82] [ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals](https://arxiv.org/abs/2509.08779)
*Ali Amini,Mohammad Alijanpour,Behnam Latifi,Ali Motie Nasrabadi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于深度学习和脱脑电图信号的新题ADHD诊断方法ADHDeepNet，达到了100%敏感度和99.17%准确度的高性能表现。


<details>
  <summary>Details</summary>
Motivation: ADHD是一种常见的脑部障碍，早期诊断对患者和医疗系统都很重要，但现有诊断方法经常耗时耗力。需要提高诊断的准确性和及时性。

Method: 提出ADHDeepNet深度学习模型，结合时空特征描述、注意力机制和可解释性技术，优化处理EEG信号。采用嵌套交叉验证和数据增帽技术，包括添加高斯噪声。通过分析模型权重和激活模式来识别关键脑区和频率带。

Result: 在121名参与者（61名ADHD，60名健康对照）的数据集上，ADHDeepNet达到了100%的敏感度和99.17%的准确度。模型通过t-SNE可视化技术展示了高维数据的分离效果，提供了模型决策的可解释性。

Conclusion: 这项研究证明了深度学习与EEG信号结合在提高ADHD诊断准确性和效率方面的潜力，为临床诊断提供了一种高效、可靠的新方法。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.

</details>


### [83] [A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management](https://arxiv.org/abs/2509.08822)
*Willy Sucipto,Jianlong Zhou,Ray Seung Min Kwon,Fang Chen*

Main category: cs.LG

TL;DR: 这篇调查性论文综述了小型机器学习(TinyML)技术在蜜蜂养植领域的应用，包括蜜蜂工冠监测、行为识别、疾病病虫检测和分蜂预测，以支持可持续的传果者管理。


<details>
  <summary>Details</summary>
Motivation: 传统蜜蜂工冠监测方式劳动密集且干扰蜜蜂，云端解决方案在远程或资源有限的蜜蜂场实施困难，而TinyML技术能提供低耗能、实时的边缘设备监测方案。

Method: 调查小型机器学习在蜜蜂养植领域的最新创新，从四个关键功能区域进行组织：工冠条件监测、蜜蜂行为识别、疾病病虫检测和分蜂事件预测，同时分析了公开数据集、轻量级模型架构和基准化策略等支持资源。

Result: 识别了数据稀缺、模型通用化挑战、离网环境部署障碍等关键限制因素，同时指出了超高效推理流水线、适应性边缘学习和数据集标准化等新兴机遇。

Conclusion: 通过整合研究和工程实践，该论文为建立可扩展、AI驱动且包含生态信息的监测系统奠定了基础，以支持可持续的传果者管理。

Abstract: Honey bee colonies are essential for global food security and ecosystem
stability, yet they face escalating threats from pests, diseases, and
environmental stressors. Traditional hive inspections are labor-intensive and
disruptive, while cloud-based monitoring solutions remain impractical for
remote or resource-limited apiaries. Recent advances in Internet of Things
(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring
directly on edge devices, offering scalable and non-invasive alternatives. This
survey synthesizes current innovations at the intersection of TinyML and
apiculture, organized around four key functional areas: monitoring hive
conditions, recognizing bee behaviors, detecting pests and diseases, and
forecasting swarming events. We further examine supporting resources, including
publicly available datasets, lightweight model architectures optimized for
embedded deployment, and benchmarking strategies tailored to field constraints.
Critical limitations such as data scarcity, generalization challenges, and
deployment barriers in off-grid environments are highlighted, alongside
emerging opportunities in ultra-efficient inference pipelines, adaptive edge
learning, and dataset standardization. By consolidating research and
engineering practices, this work provides a foundation for scalable, AI-driven,
and ecologically informed monitoring systems to support sustainable pollinator
management.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [84] [Polyglot Persistence in Microservices: Managing Data Diversity in Distributed Systems](https://arxiv.org/abs/2509.08014)
*Festim Halili,Anila Nuhiji,Diellza Mustafai Veliu*

Main category: cs.DB

TL;DR: 微服务架构中多语言持久化策略的研究，通过对比多种数据库技术和实际案例分析，证明该策略在提高适应性、性能和领域对齐的同时也带来管理复杂性问题


<details>
  <summary>Details</summary>
Motivation: 微服务架构在提供扩展性的同时带来了异构分布式数据管理的挑战，多语言持久化作为一种实用解决方案需要被深入研究

Method: 结合理论概念与实践实施证据，应用对比框架评估关系型、文档、键值对、列家族和图数据库的扩展性、一致性、查询表达力等指标，采用业界案例和调查数据

Result: 多语言持久化能够提高系统适应性、性能和领域对齐性，但同时也增加了管理和运维复杂性

Conclusion: 为应对多语言持久化的批截，需要采用saga工作流、事件源码化、outbox集成等架构模式来管理分布式数据一致性和复杂性

Abstract: Microservices architectures have become the foundation for developing
scalable and modern software systems, but they also bring significant
challenges in managing heterogeneous and distributed data. The pragmatic
solution is polyglot persistence, the deliberate use of several different
database technologies adapted to a given microservice requirement - is one such
strategy. This paper examines polyglot persistence in microservice based
systems. This paper brings together theoretical concepts with evidence from
practical implementations and comparative benchmarks of standard database
platforms. A comparative framework is applied to relational, document,
key-value, column-family and graph databases to assess scalability,
consistency, query expressiveness, operational overhead and integration ease.
Empirical data drawn from industry case studies such as Netflix, Uber, and
Shopify, and survey data illustrate real-life adoption trends and challenges.
These findings demonstrate that polyglot persistence increases adaptability ,
performance , domain alignment but also governance or operational complexity.
To cope with such trade-offs, architectural patterns such as saga workflows,
event sourcing, and outbox integration are discussed.

</details>


### [85] [Infinite Stream Estimation under Personalized $w$-Event Privacy](https://arxiv.org/abs/2509.08387)
*Leilei Du,Peng Cheng,Lei Chen,Heng Tao Shen,Xuemin Lin,Wei Xi*

Main category: cs.DB

TL;DR: 提出了个性化w-event隐私保护机制PWSM、PBD和PBA，允许不同用户有不同的隐私需求，在流数据统计估计中比现有方法误差更小


<details>
  <summary>Details</summary>
Motivation: 现有w-event隐私研究主要关注所有用户的同质隐私需求，但实际中不同用户可能有不同的隐私保护要求，需要个性化的隐私保护方案

Method: 设计PWSM机制保持用户在每个时间段的恒定隐私需求；提出PBD和PBA两种解决方案，PBD确保为下一步骤提供至少与之前消耗相同的隐私预算，PBA完全吸收前k个时间段的隐私预算并借用后k个时间段的预算来增加当前时间段的预算

Result: PBD在真实数据集上比BD平均减少68%误差，PBA在合成数据集上比BA平均减少24.9%误差，两种方法都优于现有最优的私有流估计方法

Conclusion: 个性化w-event隐私保护机制能够有效满足不同用户的隐私需求，同时在流数据统计估计中保持更高的准确性，证明了在个性化隐私保护场景下的优越性能

Abstract: Streaming data collection is indispensable for stream data analysis, such as
event monitoring. However, publishing these data directly leads to privacy
leaks. $w$-event privacy is a valuable tool to protect individual privacy
within a given time window while maintaining high accuracy in data collection.
Most existing $w$-event privacy studies on infinite data stream only focus on
homogeneous privacy requirements for all users. In this paper, we propose
personalized $w$-event privacy protection that allows different users to have
different privacy requirements in private data stream estimation. Specifically,
we design a mechanism that allows users to maintain constant privacy
requirements at each time slot, namely Personalized Window Size Mechanism
(PWSM). Then, we propose two solutions to accurately estimate stream data
statistics while achieving $w$-event level $\epsilon$ personalized differential
privacy ( ($w$, $\epsilon$)-EPDP), namely Personalized Budget Distribution
(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the
same privacy budget for the next time step as the amount consumed in the
previous release. PBA fully absorbs the privacy budget from the previous $k$
time slots, while also borrowing from the privacy budget of the next $k$ time
slots, to increase the privacy budget for the current time slot. We prove that
both PBD and PBA outperform the state-of-the-art private stream estimation
methods while satisfying the privacy requirements of all users. We demonstrate
the efficiency and effectiveness of our PBD and PBA on both real and synthetic
data sets, compared with the recent uniformity $w$-event approaches, Budget
Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error
than BD on average on real data sets. Besides, our PBA achieves 24.9% less
error than BA on average on synthetic data sets.

</details>


### [86] [SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors](https://arxiv.org/abs/2509.08395)
*Ruoxuan Li,Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Wangze Ni,Lei Chen,Zhitao Shen,Wei Jia,Xiangyu Wang,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: SINDI是一种稀疏向量最大内积搜索优化方法，通过SIMD加速、内存友好设计和向量剪枝技术，显著提升检索效率，在多个数据集上达到state-of-the-art性能


<details>
  <summary>Details</summary>
Motivation: 现有稀疏向量MIPS算法在生产环境中受限于冗余距离计算、随机内存访问和压缩存储格式对SIMD加速的限制，需要更高效的解决方案

Method: 提出SINDI方法，包含三个关键优化：1) 利用SIMD加速和消除冗余标识符查找实现批量内积计算；2) 用顺序访问倒排列表替代随机内存访问；3) 保留高幅值非零项的向量剪枝

Result: 在MsMarco数据集上Recall@50超过99%时，单线程QPS相比SEISMIC和PyANNs提升4.2-26.4倍，在多个不同规模、语言和模型的数据集上都达到最优性能

Conclusion: SINDI成功解决了稀疏向量MIPS的性能瓶颈，已被集成到Ant Group的开源向量搜索库VSAG中，具有重要的实际应用价值

Abstract: Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path
retrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based
and graph-based algorithms have achieved high search accuracy with practical
efficiency. However, their performance in production environments is often
limited by redundant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hinders the use of
SIMD acceleration. In this paper, we propose the sparse inverted non-redundant
distance index (SINDI), which incorporates three key optimizations: (i)
Efficient Inner Product Computation: SINDI leverages SIMD acceleration and
eliminates redundant identifier lookups, enabling batched inner product
computation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses
to original vectors with sequential accesses to inverted lists, substantially
reducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the
high-magnitude non-zero entries of vectors, improving query throughput while
maintaining accuracy. We evaluate SINDI on multiple real-world datasets.
Experimental results show that SINDI achieves state-of-the-art performance
across datasets of varying scales, languages, and models. On the MsMarco
dataset, when Recall@50 exceeds 99%, SINDI delivers single-thread
query-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared
with SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's
open-source vector search library, VSAG.

</details>


### [87] [Un cadre paraconsistant pour l'{é}valuation de similarit{é} dans les bases de connaissances](https://arxiv.org/abs/2509.08433)
*José-Luis Vilchis Medina*

Main category: cs.DB

TL;DR: 提出了一种基于次协调逻辑的知识库相似性评估框架，能够显式处理矛盾，提供更鲁棒和可解释的相似性度量


<details>
  <summary>Details</summary>
Motivation: 传统相似性评估方法无法有效处理知识库中的矛盾信息，需要一种能够整合不一致性并提供有意义相似度量的框架

Method: 引入新的相似性度量S*，惩罚不一致性同时奖励共享属性；定义次协调超类别Ξ_K*来层次化组织知识实体；包含矛盾提取器E和修复机制

Result: 理论结果保证了S*的自反性、对称性和有界性，为处理冲突知识提供了有效解决方案

Conclusion: 该框架为多智能体系统中的知识管理提供了有前景的解决方案，能够更好地处理矛盾知识

Abstract: This article proposes a paraconsistent framework for evaluating similarity in
knowledge bases. Unlike classical approaches, this framework explicitly
integrates contradictions, enabling a more robust and interpretable similarity
measure. A new measure $ S^* $ is introduced, which penalizes inconsistencies
while rewarding shared properties. Paraconsistent super-categories $ \Xi_K^* $
are defined to hierarchically organize knowledge entities. The model also
includes a contradiction extractor $ E $ and a repair mechanism, ensuring
consistency in the evaluations. Theoretical results guarantee reflexivity,
symmetry, and boundedness of $ S^* $. This approach offers a promising solution
for managing conflicting knowledge, with perspectives in multi-agent systems.

</details>


### [88] [SQLGovernor: An LLM-powered SQL Toolkit for Real World Application](https://arxiv.org/abs/2509.08575)
*Jie Jiang,Siqi Shen,Haining Xie,Yang Li,Yu Shen,Danqing Huang,Bo Qian,Yinjun Wu,Wentao Zhang,Bin Cui,Peng Chen*

Main category: cs.DB

TL;DR: SQLGovernor是一个基于LLM的SQL工具包，通过分段处理策略和混合自学习机制，统一了语法修正、查询重写、修改和一致性验证功能，在复杂OLAP场景中显著提升SQL查询质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界分析环境中的SQL查询（无论是人工编写还是自动生成）经常存在语法错误、效率低下和语义不对齐等问题，特别是在复杂的OLAP场景中，需要一种统一的解决方案来应对这些挑战。

Method: 提出SQLGovernor工具包，采用分段处理策略实现细粒度重写和局部错误修正，结合基于专家反馈的混合自学习机制，通过DBMS输出分析和规则验证持续改进系统性能。

Result: 在BIRD、BIRD CRITIC基准测试和工业数据集上的实验表明，SQLGovernor能够将基础模型的性能提升高达10%，同时最小化对人工专业知识的依赖。生产环境部署证明了其强大的实用性和有效性能。

Conclusion: SQLGovernor通过结构化框架和知识管理增强，为复杂OLAP场景中的SQL查询问题提供了有效的统一解决方案，显著降低了LLM的认知负担并实现了持续的性能改进。

Abstract: SQL queries in real world analytical environments, whether written by humans
or generated automatically often suffer from syntax errors, inefficiency, or
semantic misalignment, especially in complex OLAP scenarios. To address these
challenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies
multiple functionalities, including syntax correction, query rewriting, query
modification, and consistency verification within a structured framework
enhanced by knowledge management. SQLGovernor introduces a fragment wise
processing strategy to enable fine grained rewriting and localized error
correction, significantly reducing the cognitive load on the LLM. It further
incorporates a hybrid self learning mechanism guided by expert feedback,
allowing the system to continuously improve through DBMS output analysis and
rule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as
well as industrial datasets, show that SQLGovernor consistently boosts the
performance of base models by up to 10%, while minimizing reliance on manual
expertise. Deployed in production environments, SQLGovernor demonstrates strong
practical utility and effective performance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [89] [The Linear Reliability Channel](https://arxiv.org/abs/2509.08079)
*Alexander Mariona,Ken R. Duffy,Muriel Médard*

Main category: cs.IT

TL;DR: 线性可靠性通道(LRC)是一种软决策通道，通过符号可靠性排序提供软信息，在高噪声情况下能进行组合数学分析并获得明确的错误指数。


<details>
  <summary>Details</summary>
Motivation: 为了在高噪声环境下对软决策解码进行数学分析，提出一种组合性质的通道模型，以实现对软决策解码优势的定量评估。

Method: 提出线性可靠性通道(LRC)模型，利用收到符号可靠性的排序作为软信息。证明该通道在高噪声时是连续噪声通道的适当近似，并分析了随机码在硬软决策最大概率解码下的错误指数。

Result: 获得了硬软决策最大概率解码的明确错误指数表达式，实现了对软决策解码优势的直接定量评估，并揭示LRC的离散几何特性与二进制对称通道不同。

Conclusion: LRC通道提供了一种新的视角来分析软决策解码，其组合性质允许进行深入的数学分析，为软决策场景下的编码构造开启了新的研究方向。

Abstract: We introduce and analyze a discrete soft-decision channel called the linear
reliability channel (LRC) in which the soft information is the rank ordering of
the received symbol reliabilities. We prove that the LRC is an appropriate
approximation to a general class of discrete modulation, continuous noise
channels when the noise variance is high. The central feature of the LRC is
that its combinatorial nature allows for an extensive mathematical analysis of
the channel and its corresponding hard- and soft-decision maximum likelihood
(ML) decoders. In particular, we establish explicit error exponents for ML
decoding in the LRC when using random codes under both hard- and soft-decision
decoding. This analysis allows for a direct, quantitative evaluation of the
relative advantage of soft-decision decoding. The discrete geometry of the LRC
is distinct from that of the BSC, which is characterized by the Hamming weight,
offering a new perspective on code construction for soft-decision settings.

</details>


### [90] [Holographic Beamforming for Integrated Sensing and Communication with Mutual Coupling Effects](https://arxiv.org/abs/2509.08113)
*Shuhao Zeng,Haobo Zhang,Boya Di,Hongliang Zhang,Zijian Shao,Zhu Han,H. Vincent Poor,Lingyang Song*

Main category: cs.IT

TL;DR: 本文提出了一种考虑互耦效应的全息波束成形算法，用于集成感知与通信系统，通过耦合偶极子近似来抑制旁瓣并提升性能


<details>
  <summary>Details</summary>
Motivation: 现有的全息波束成形方案主要针对通信性能设计，忽略了互耦效应，而ISAC系统中的感知功能对旁瓣水平敏感，忽略互耦会导致显著的旁瓣，从而降低感知性能

Method: 提出了一个可处理的电磁兼容全息ISAC模型，使用耦合偶极子近似以闭式形式表征互耦效应，并开发了高效的互耦感知全息波束成形算法

Result: 数值结果验证了所提算法的有效性，能够有效抑制旁瓣并提升ISAC系统的整体性能

Conclusion: 考虑互耦效应的全息波束成形设计对于ISAC系统至关重要，所提出的算法为解决互耦引入的非凸问题提供了有效解决方案

Abstract: Integrated sensing and communication (ISAC) is envisioned as a key technology
in 6G networks, owing to its potential for high spectral and cost efficiency.
As a promising solution for extremely large-scale arrays, reconfigurable
holographic surfaces (RHS) can be integrated with ISAC to form the holographic
ISAC paradigm, where enlarged radiation apertures of RHS can achieve
significant beamforming gains, thereby improving both communication and sensing
performance. In this paper, we investigate holographic beamforming designs for
ISAC systems, which, unlike existing holographic beamforming schemes developed
for RHS-aided communications, requires explicit consideration of mutual
coupling effects within RHS. This is because, different from prior works only
considering communication performance, ISAC systems incorporate sensing
functionality, which is sensitive to sidelobe levels. Ignoring mutual coupling
in holographic beamforming can lead to notable undesired sidelobes, thus
degrading sensing performance. The consideration of mutual coupling introduces
new challenges, i.e., it induces non-linearity in beamforming problems,
rendering them inherently non-convex. To address this issue, we propose a
tractable electromagnetic-compliant holographic ISAC model that characterizes
mutual coupling in a closed form using coupled dipole approximations. We then
develop an efficient mutual coupling aware holographic beamforming algorithm to
suppress sidelobes and enhance ISAC performance. Numerical results validate
effectiveness of the proposed algorithm.

</details>


### [91] [SCA-LLM: Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM](https://arxiv.org/abs/2509.08139)
*Ke He,Le He,Lisheng Fan,Xianfu Lei,Thang X. Vu,George K. Karagiannidis,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: 提出SCA-LLM框架，通过频谱注意力机制解决LLM在信道预测中的领域不匹配问题，在MIMO-OFDM系统中实现最先进的预测性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在无线通信信道预测中存在领域不匹配问题，现有适配器未能充分利用频谱信息来弥合文本预训练与信道数据之间的领域差距

Method: 提出频谱注意力框架SCA-LLM，设计新型适配器从CSI特征的频谱分量中学习表示，捕获更精细的频谱细节，更好地适配LLM进行信道预测

Result: 在MIMO-OFDM系统中实现最先进的预测性能，相比之前的LLM方法获得高达-2.4 dB的NMSE优势，并展现出强大的泛化能力

Conclusion: SCA-LLM通过频谱注意力机制有效缓解了领域不匹配问题，为LLM在无线通信中的应用提供了更有效的解决方案

Abstract: In recent years, the success of large language models (LLMs) has inspired
growing interest in exploring their potential applications in wireless
communications, especially for channel prediction tasks. However, directly
applying LLMs to channel prediction faces a domain mismatch issue stemming from
their text-based pre-training. To mitigate this, the ``adapter + LLM" paradigm
has emerged, where an adapter is designed to bridge the domain gap between the
channel state information (CSI) data and LLMs. While showing initial success,
existing adapters may not fully exploit the potential of this paradigm. To
address this limitation, this work provides a key insight that learning
representations from the spectral components of CSI features can more
effectively help bridge the domain gap. Accordingly, we propose a
spectral-attentive framework, named SCA-LLM, for channel prediction in
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. Specifically, its novel adapter can capture finer spectral
details and better adapt the LLM for channel prediction than previous methods.
Extensive simulations show that SCA-LLM achieves state-of-the-art prediction
performance and strong generalization, yielding up to $-2.4~\text{dB}$
normalized mean squared error (NMSE) advantage over the previous LLM based
method. Ablation studies further confirm the superiority of SCA-LLM in
mitigating domain mismatch.

</details>


### [92] [The Shannon Upper Bound for the Error Exponent](https://arxiv.org/abs/2509.08425)
*Sergey Tridenski,Anelia Somekh-Baruch*

Main category: cs.IT

TL;DR: 该论文针对离散时间加性白广义高斯噪声信道，在广义输入功率约束下，推导了最优块错误指数的上界，并给出了三种特殊情况的显式渐近上界。


<details>
  <summary>Details</summary>
Motivation: 研究广义高斯噪声信道在功率约束下的最优错误性能，为实际通信系统提供理论性能界限。

Method: 使用类型方法，采用依赖于块长度n的有限字母表，类型数量为n的次指数级。

Result: 推导出了最优块错误指数的上界，并给出了拉普拉斯噪声信道和高斯噪声信道在特定功率约束下的显式渐近上界。

Conclusion: 该方法能够有效分析广义高斯噪声信道的错误性能，为相关通信系统的设计提供了理论依据。

Abstract: For the discrete-time additive white generalized Gaussian noise channel with
a generalized input power constraint, with the respective shape and power
parameters >= 1, we derive an upper bound on the optimal block error exponent.
Explicit asymptotic upper bounds in the limit of a large block length n are
given for three special cases: the Laplace noise channel and the Gaussian noise
channel with the average absolute value constraint, and for the Laplace noise
channel with the second power constraint. The derivation uses the method of
types with finite alphabets of sizes depending on the block length n and with
the number of types sub-exponential in n.

</details>


### [93] [Deep holes of a class of twisted Reed-Solomon codes](https://arxiv.org/abs/2509.08526)
*Haojie Gu,Nan Wang,Jun Zhang*

Main category: cs.IT

TL;DR: 本文研究了扭曲Reed-Solomon码的深洞问题，给出了深洞存在的充要条件，并在特定参数范围内完全确定了所有深洞。


<details>
  <summary>Details</summary>
Motivation: 深洞问题是编码理论中的基本问题，在码构造和密码学中有重要应用。作为Reed-Solomon码的推广，研究扭曲Reed-Solomon码的深洞问题具有重要意义。

Method: 首先给出了扭曲Reed-Solomon码中向量成为深洞的充要条件，然后针对特定参数范围（TRS_k(F_q*,k-1,η)）的码，通过数学证明确定了所有深洞。

Result: 对于偶数q，当3q+2√q-8/4 ≤ k ≤ q-5时，证明了没有其他深洞；对于奇数q，当3q+3√q-5/4 ≤ k ≤ q-5时同样成立。对于q-4 ≤ k ≤ q-2（q为偶数）的情况，完全确定了所有深洞。

Conclusion: 本文系统研究了扭曲Reed-Solomon码的深洞问题，给出了理论上的充要条件，并在特定参数范围内完全解决了深洞的确定问题，为这类码的应用提供了理论基础。

Abstract: The deep hole problem is a fundamental problem in coding theory, and it has
many important applications in code constructions and cryptography. The deep
hole problem of Reed-Solomon codes has gained a lot of attention. As a
generalization of Reed-Solomon codes, we investigate the problem of deep holes
of a class of twisted Reed-Solomon codes in this paper.
  Firstly, we provide the necessary and sufficient conditions for
$\boldsymbol{a}=(a_{0},a_{1},\cdots,a_{n-k-1})\in\mathbb{F}_{q}^{n-k}$ to be
the syndrome of some deep hole of $TRS_{k}(\mathcal{A},l,\eta)$. Next, we
consider the problem of determining all deep holes of the twisted Reed-Solomon
codes $TRS_{k}(\mathbb{F}_{q}^{*},k-1,\eta)$. Specifically, we prove that there
are no other deep holes of $TRS_{k}(\mathbb{F}_{q}^{*},k-1,\eta)$ for
$\frac{3q+2\sqrt{q}-8}{4}\leq k\leq q-5$ when q is even, and
$\frac{3q+3\sqrt{q}-5}{4}\leq k\leq q-5$ when q is odd. We also completely
determine their deep holes for $q-4\leq k\leq q-2$ when $q$ is even.

</details>


### [94] [The Landscape of Fairness: An Axiomatic and Predictive Framework for Network QoE Sensitivity](https://arxiv.org/abs/2509.08551)
*Zhiyuan Ren,Xinke Jian,Wenchi Cheng,Kun Yang*

Main category: cs.IT

TL;DR: 这篇论文提出了一个完整的分析框架，将全网公平性评估从单点测量转化为基于预测敏感性地形的主动工程学科。框架通过QoE不平衡指标、闭式协方差规则、相图和曲率分析，揭示了系统敏感性的关键特征和设计规则。


<details>
  <summary>Details</summary>
Motivation: 全网公平性评估面临挑战，因为它不是静态属性而是高度受SLA参数影响的动态性质。需要一个系统化框架来将公平性评估从单点测量转化为主动的工程设计学科。

Method: 1）基于公平性基本公理构建QoE不平衡指标
2）推导闭式协方差规则，将公平性梯度表达为路径信息论重要性与参数敏感性的协方差
3）构建相图来映射全局公平性地形
4）分析地形曲率得出可操作的设计规则

Result: 框架揭示了系统敏感性地形的关键拓扑学特征，包括稳健的"稳定带"和高风险的"危险榴形区"。推导出优化的"阈值优先"调节策略，为网络设计提供了可行动的规则。

Conclusion: 该框架为网络系统的公平性评估和设计提供了完整的工具集，能够映射、解释和导航系统敏感性地形，从而设计出更稳健和弹性的网络。

Abstract: Evaluating network-wide fairness is challenging because it is not a static
property but one highly sensitive to Service Level Agreement (SLA) parameters.
This paper introduces a complete analytical framework to transform fairness
evaluation from a single-point measurement into a proactive engineering
discipline centered on a predictable sensitivity landscape. Our framework is
built upon a QoE-Imbalance metric whose form is not an ad-hoc choice, but is
uniquely determined by a set of fundamental axioms of fairness, ensuring its
theoretical soundness. To navigate the fairness landscape across the full
spectrum of service demands, we first derive a closed-form covariance rule.
This rule provides an interpretable, local compass, expressing the fairness
gradient as the covariance between a path's information-theoretic importance
and its parameter sensitivity. We then construct phase diagrams to map the
global landscape, revealing critical topological features such as robust
"stable belts" and high-risk "dangerous wedges". Finally, an analysis of the
landscape's curvature yields actionable, topology-aware design rules, including
an optimal "Threshold-First" tuning strategy. Ultimately, our framework
provides the tools to map, interpret, and navigate the landscape of system
sensitivity, enabling the design of more robust and resilient networks.

</details>


### [95] [Low-Complexity CSI Acquisition Exploiting Geographical Diversity in Fluid Antenna System](https://arxiv.org/abs/2509.08598)
*Zhentian Zhang,David Morales-Jimenez,Jian Dang,Zaichen Zhang,Christos Masouros,Hao Jiang*

Main category: cs.IT

TL;DR: 提出基于EM-AMP框架的FAS信道状态信息获取算法，利用地理先验知识提高估计精度、加速收敛并降低复杂度


<details>
  <summary>Details</summary>
Motivation: 流体天线系统需要灵活低复杂度的CSI获取方案来支持大规模连接，现有贪婪算法依赖信号假设，无模型方法复杂度高

Method: 基于期望最大化-近似消息传递(EM-AMP)框架，开发利用FAS地理先验知识的变体算法，进行高效矩阵计算和自适应学习

Result: 仿真验证了所提算法的有效性，在估计精度、收敛速度和复杂度方面均有改善

Conclusion: 该EM-AMP变体算法为FAS大规模部署提供了高效的信道状态信息获取解决方案

Abstract: The fluid antenna system (FAS) employs reconfigurable antennas for high
spatial gains in compact spaces, enhancing physical layer flexibility. Channel
state information (CSI) acquisition is vital for port selection and FAS
optimization. Greedy algorithms rely on signal assumptions, and model-free
methods face high complexity. A flexible, low-complexity solution is needed for
massive connectivity in FAS. Based on expectation maximization-approximate
message passing (EM-AMP) framework, efficient matrix computations and adaptive
learning without prior model knowledge naturally suit CSI acquisition for FAS.
We propose a EM-AMP variant exploiting FAS geographical priors, improving
estimation precision, accelerating convergence, and reducing complexity in
large-scale deployment. Simulations validate the efficacy of the proposed
algorithm.

</details>


### [96] [Fluid Antenna Systems: A Geometric Approach to Error Probability and Fundamental Limits](https://arxiv.org/abs/2509.08815)
*Xusheng Zhu,Kai-Kit Wong,Hao Xu,Han Xiao,Hanjiang Hong,Hyundong Shin,Yangyang Zhang*

Main category: cs.IT

TL;DR: 提出了流体天线系统(FAS)在空间相关信道下的严格误差概率分析框架，推导出符号错误率的闭式渐近表达式，揭示了系统性能与信道空间相关结构的基本缩放规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对流体天线系统在现实空间相关信道下误差概率分析的严格框架，需要建立系统性能与信道空间相关结构之间的基本关系。

Method: 采用双重方法：开发基于几何的算法从信道特征值谱中提取性能阈值；理论证明有效秩收敛于由天线归一化孔径宽度决定的基本极限。

Result: 分析表明可实现的多样性增益由信道有效秩而非天线端口数量决定，提出的有效秩模型比现有方法精度更高，建立了多样性和编码增益的完整表征。

Conclusion: FAS性能改进的根本驱动力是扩大天线可探索孔径以增加有效信道秩，而在固定孔径内增加端口密度只能获得递减的收益回报。

Abstract: The fluid antenna system (FAS) concept is an emerging paradigm that promotes
the utilization of the feature of shape and position reconfigurability in
antennas to broaden the design of wireless communication systems. This also
means that spatial diversity can be exploited in an unconventional way.
However, a rigorous framework for error probability analysis of FAS under
realistic spatially correlated channels has been lacking. In this paper, we
fill this gap by deriving a tight, closed-form asymptotic expression for the
symbol error rate (SER) that establishes the fundamental scaling law linking
the system's SER to the channel's spatial correlation structure. A key insight
of our analysis is that the achievable diversity gain is governed not by the
number of antenna ports, but by the channel's effective rank. To find this
critical parameter, we propose a novel dual-pronged approach. First of all, we
develop a geometry-based algorithm that extracts distinct performance
thresholds from the channel's eigenvalue spectrum. Second, we theoretically
prove that the effective rank converges to a fundamental limit dictated solely
by the antenna's normalized aperture width. We further establish the
equivalence between the threshold identified by the geometric algorithm and the
derived theoretical limit, providing rigorous validation for the proposed
method. Our effective rank model achieves higher accuracy than existing
approaches in the literature. Building on this framework, we offer a complete
characterization of diversity and coding gains. The analysis leads to a
definitive design insight: FAS performance improvements are fundamentally
driven by enlarging the antenna's explorable aperture, which increases the
effective channel rank, whereas increasing port density within a fixed aperture
yields diminishing returns.

</details>


### [97] [ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications](https://arxiv.org/abs/2505.10946)
*Li Qiao,Mahdi Boloursaz Mashhadi,Zhen Gao,Robert Schober,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出基于令牌域的多址接入方案ToDMA，利用MLLM处理令牌碰撞，实现低延迟高质量的多模态语义通信


<details>
  <summary>Details</summary>
Motivation: 传统通信方案在大量设备接入时存在延迟高、效率低的问题，需要利用语义通信和大型语言模型来提升多设备并发通信性能

Method: 设备共享令牌码本和调制码本，通过压缩感知检测活跃令牌和CSI，利用MLLM预测被碰撞的令牌来重建序列

Result: 在文本和图像传输任务中，相比无上下文感知的正交方案显著降低延迟，相比最先进的无上下文非正交方法提供更好的失真和感知质量

Conclusion: ToDMA框架有效解决了多设备语义通信中的令牌碰撞问题，为大规模物联网设备的高效并发通信提供了可行方案

Abstract: Token communications (TokCom) is an emerging generative semantic
communication concept that reduces transmission rates by using context and
multimodal large language model (MLLM)-based token processing, with tokens
serving as universal semantic units across modalities. In this paper, we
propose a semantic multiple access scheme in the token domain, referred to as
token domain multiple access (ToDMA), where a large number of devices share a
token codebook and a modulation codebook for source and channel coding,
respectively. Specifically, each transmitter first tokenizes its source signal
and modulate each token to a codeword. At the receiver, compressed sensing is
employed first to detect active tokens and the corresponding channel state
information (CSI) from the superposed signals. Then, the source token sequences
are reconstructed by clustering the token-associated CSI across multiple time
slots. In case of token collisions, some active tokens cannot be assigned and
some positions in the reconstructed token sequences are empty. We propose to
use pre-trained MLLMs to leverage the context, predict masked tokens, and thus
mitigate token collisions. Simulation results demonstrate the effectiveness of
the proposed ToDMA framework for both text and image transmission tasks,
achieving significantly lower latency compared to context-unaware orthogonal
communication schemes, while also delivering superior distortion and perceptual
quality compared to state-of-the-art context-unaware non-orthogonal
communication methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [98] [Vector embedding of multi-modal texts: a tool for discovery?](https://arxiv.org/abs/2509.08216)
*Beth Plale,Sai Navya Jyesta,Sachith Withana*

Main category: cs.IR

TL;DR: 本研究探索基于视觉语言模型的多模态向量检索在计算机科学教科书多模态内容发现中的效果，通过3600多页教材测试，发现余弦相似度在检索效果上表现最佳


<details>
  <summary>Details</summary>
Motivation: 计算机科学文本包含丰富的叙述内容和图表、算法、图像等视觉元素，需要探索多模态检索方法来改善数字图书馆中这类多模态内容的发现效率

Method: 使用视觉语言模型生成文本和图像的多向量表示，存储在向量数据库中，通过75个自然语言查询测试四种相似度度量方法的检索性能

Result: 余弦相似度在检索语义和视觉相关页面方面表现最有效，能够较好地捕捉多模态内容的语义关联

Conclusion: 研究为数字图书馆发现系统提供了设计见解，证明了向量数据库和多模态嵌入在操作信息检索中的实用性，同时揭示了该方法的优势和局限性

Abstract: Computer science texts are particularly rich in both narrative content and
illustrative charts, algorithms, images, annotated diagrams, etc. This study
explores the extent to which vector-based multimodal retrieval, powered by
vision-language models (VLMs), can improve discovery across multi-modal (text
and images) content. Using over 3,600 digitized textbook pages largely from
computer science textbooks and a Vision Language Model (VLM), we generate
multi-vector representations capturing both textual and visual semantics. These
embeddings are stored in a vector database. We issue a benchmark of 75 natural
language queries and compare retrieval performance to ground truth and across
four similarity (distance) measures. The study is intended to expose both the
strengths and weakenesses of such an approach. We find that cosine similarity
most effectively retrieves semantically and visually relevant pages. We further
discuss the practicality of using a vector database and multi-modal embedding
for operational information retrieval. Our paper is intended to offer design
insights for discovery over digital libraries.
  Keywords: Vector embedding, multi-modal document retrieval, vector database
benchmark, digital library discovery

</details>


### [99] [Soundtracks of Our Lives: How Age Influences Musical Preferences](https://arxiv.org/abs/2509.08337)
*Arsen Matej Golubovikj,Bruce Ferwerda,Alan Said,Marko Talčič*

Main category: cs.IR

TL;DR: 该研究使用LFM-2b数据集分析用户音乐偏好随年龄变化的演化规律，发现年轻用户偏好流行音乐，而年长用户具有更个性化的听歌习惯。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统研究多基于短期用户交互数据，缺乏对用户长期偏好演化的理解。媒体研究和心理学已有关于用户偏好随年龄变化的研究，但在推荐系统领域相关研究较少。

Method: 使用LFM-2b数据集进行纵向研究，该数据集包含足够长的时间跨度和用户年龄信息，能够支持真实的纵向分析。

Result: 识别出用户使用习惯和品味偏好与年龄直接相关：年轻用户倾向于广泛收听当代流行音乐，而年长用户具有更精细化和个性化的听歌习惯。

Conclusion: 研究结果为推荐系统研究开辟了新方向，为未来工作提供了重要指导，强调了考虑用户年龄和长期偏好演化的重要性。

Abstract: The majority of research in recommender systems, be it algorithmic
improvements, context-awareness, explainability, or other areas, evaluates
these systems on datasets that capture user interaction over a relatively
limited time span. However, recommender systems can very well be used
continuously for extended time. Similarly so, user behavior may evolve over
that extended time. Although media studies and psychology offer a wealth of
research on the evolution of user preferences and behavior as individuals age,
there has been scant research in this regard within the realm of user modeling
and recommender systems. In this study, we investigate the evolution of user
preferences and behavior using the LFM-2b dataset, which, to our knowledge, is
the only dataset that encompasses a sufficiently extensive time frame to permit
real longitudinal studies and includes age information about its users. We
identify specific usage and taste preferences directly related to the age of
the user, i.e., while younger users tend to listen broadly to contemporary
popular music, older users have more elaborate and personalized listening
habits. The findings yield important insights that open new directions for
research in recommender systems, providing guidance for future efforts.

</details>
