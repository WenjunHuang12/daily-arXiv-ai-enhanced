<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 5]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.IT](#cs.IT) [Total: 14]
- [cs.MM](#cs.MM) [Total: 2]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Games in Product Form for Demand Response Modelling](https://arxiv.org/abs/2511.19532)
*Thomas Buchholtzer,Michel de Lara*

Main category: cs.GT

TL;DR: 提出了一个通用的模块化框架来处理领导者-跟随者问题，特别关注常被忽视的信息问题。该框架基于Witsenhausen内在模型，在乘积集上表示代理的可用信息，并引入乘积形式博弈来处理偏好信息。


<details>
  <summary>Details</summary>
Motivation: 能源系统正快速变化，生产分散化、波动性增强，需求多样化，使得供需平衡更加复杂。需求响应作为典型的领导者-跟随者问题，需要处理常被忽视的信息问题。

Method: 引入W模型定义博弈规则，基于Witsenhausen内在模型在乘积集上高效表示代理信息，建立乘积形式博弈（W-games），为每个玩家配备偏好（目标函数和信念），并明确纳什和斯塔克伯格均衡的定义。

Result: 成功重新表述了多篇需求响应论文，突出了被忽视的信息问题，并基于泰国需求响应项目提供了应用案例。

Conclusion: 该框架为解决领导者-跟随者问题提供了通用且模块化的方法，特别强调了信息处理的重要性，能够有效应用于需求响应等实际问题。

Abstract: Energy systems are changing rapidly. More and more, energy production is becoming decentralized, highly variable and intermittent (solar, wind), while demand is diversifying (electric vehicles). As a result, balancing supply and demand is becoming more complex, making the adjustment of demand an interesting tool. Demand response is a typical leader-follower problem: a consumer (follower) adjusts his energy consumption based on the prices (or any other incentive) set by the supplier (leader). We propose a versatile and modular framework to address any leader-follower problem, focusing on the handling of often overlooked informational issues. First, we introduce a model that defines the rules of the game (W-model): agents are decision-makers, and Nature encapsulates everything beyond their control, such as private knowledge and exogenous factors. Following the so-called Witsenhausen intrinsic model, we present an efficient way to represent - on a product set, equipped with a product $σ$-algebra - the information available to agents when making decisions. Next, we introduce Games in Product Form (W-games) by equipping each player (a group of agents) with preferences (objective function and belief) over different outcomes. Thereby, we incorporate an additional layer of information, the characteristics of the preferences linked to players, which affects the possible definitions of an equilibrium. We make this explicit in Nash and Stackelberg equilibria. Equipped with this framework, we reformulate several papers on demand response, highlighting overlooked informational issues. We also provide an application based on the Thailand demand response program.

</details>


### [2] [Strategy-robust Online Learning in Contextual Pricing](https://arxiv.org/abs/2511.19842)
*Joon Suk Huh,Kirthevasan Kandasamy*

Main category: cs.GT

TL;DR: 该论文研究了在线上下文定价问题，考虑了买家可能策略性虚报估值的情况，提出了策略鲁棒的遗憾概念，并开发了稀疏更新机制(SUM)来应对买家间的纳什均衡行为。


<details>
  <summary>Details</summary>
Motivation: 在数字市场中，当买家估值未知且需要通过互动推断时，学习有效的定价策略至关重要。传统在线学习框架未考虑买家可能策略性虚报估值以影响未来价格的问题（即策略性过拟合）。

Method: 提出了策略鲁棒的遗憾概念，开发了多项式时间近似方案(PTAS)用于学习线性定价策略，并设计了稀疏更新机制(SUM)作为简单有效的序列机制。

Result: 构建了从在线专家算法到策略鲁棒学习器的黑盒归约，确保了对所有买家纳什均衡的鲁棒性。

Conclusion: 该研究为策略性环境下的在线定价问题提供了理论保证和实用机制，解决了买家策略性行为带来的挑战。

Abstract: Learning effective pricing strategies is crucial in digital marketplaces, especially when buyers' valuations are unknown and must be inferred through interaction. We study the online contextual pricing problem, where a seller observes a stream of context-valuation pairs and dynamically sets prices. Moreover, departing from traditional online learning frameworks, we consider a strategic setting in which buyers may misreport valuations to influence future prices, a challenge known as strategic overfitting (Amin et al., 2013).
  We introduce a strategy-robust notion of regret for multi-buyer online environments, capturing worst-case strategic behavior in the spirit of the Price of Anarchy. Our first contribution is a polynomial-time approximation scheme (PTAS) for learning linear pricing policies in adversarial, adaptive environments, enabled by a novel online sketching technique. Building on this result, we propose our main construction: the Sparse Update Mechanism (SUM), a simple yet effective sequential mechanism that ensures robustness to all Nash equilibria among buyers. Moreover, our construction yields a black-box reduction from online expert algorithms to strategy-robust learners.

</details>


### [3] [Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities](https://arxiv.org/abs/2511.19930)
*Kenta Yamamoto,Teruaki Hayashi*

Main category: cs.GT

TL;DR: 开发了一个多智能体数据市场模拟器，评估声誉系统对数据交易市场的影响，发现PeerTrust系统在价格-质量对齐方面表现最佳，并提出了混合声誉机制来改善市场稳定性。


<details>
  <summary>Details</summary>
Motivation: 数据市场面临信息不对称问题，买家无法在购买前验证数据质量，信任和质量保证成为核心挑战。需要建立有效的制度机制来促进信任形成。

Method: 开发多智能体数据市场模拟器，集成强化学习和逆强化学习来模拟参与者行为，评估五种声誉系统（Time-decay、Bayesian-beta、PageRank、PowerTrust、PeerTrust）的市场效果。

Result: PeerTrust系统在数据价格与质量对齐方面表现最强，同时防止垄断主导。基于此开发了混合声誉机制，提高了价格质量一致性和整体市场稳定性。

Conclusion: 该研究通过将信任和声誉作为内生机制纳入模拟分析，为设计可靠高效的数据生态系统提供了方法论和制度性见解。

Abstract: Recent advances in machine learning and big data analytics have intensified the demand for high-quality cross-domain datasets and accelerated the growth of data trading across organizations. As data become increasingly recognized as an economic asset, data marketplaces have emerged as a key infrastructure for data-driven innovation. However, unlike mature product or service markets, data-trading environments remain nascent and suffer from pronounced information asymmetry. Buyers cannot verify the content or quality before purchasing data, making trust and quality assurance central challenges. To address these issues, this study develops a multi-agent data-market simulator that models participant behavior and evaluates the institutional mechanisms for trust formation. Focusing on the manufacturing sector, where initiatives such as GAIA-X and Catena-X are advancing, the simulator integrates reinforcement learning (RL) for adaptive agent behavior and inverse reinforcement learning (IRL) to estimate utility functions from empirical behavioral data. Using the simulator, we examine the market-level effects of five representative reputation systems-Time-decay, Bayesian-beta, PageRank, PowerTrust, and PeerTrust-and found that PeerTrust achieved the strongest alignment between data price and quality, while preventing monopolistic dominance. Building on these results, we develop a hybrid reputation mechanism that integrates the strengths of existing systems to achieve improved price-quality consistency and overall market stability. This study extends simulation-based data-market analysis by incorporating trust and reputation as endogenous mechanisms and offering methodological and institutional insights into the design of reliable and efficient data ecosystems.

</details>


### [4] [One Action Too Many: Inapproximability of Budgeted Combinatorial Contracts](https://arxiv.org/abs/2511.20110)
*Michal Feldman,Yoav Gal-Tzur,Tomasz Ponitka,Maya Schlesinger*

Main category: cs.GT

TL;DR: 该论文研究了具有组合行动、预算约束的多智能体合同设计问题。主要贡献包括：证明了次模奖励函数在预算约束下无法获得任何有限因子的近似解；发现替代品奖励函数可以在多项式时间内获得常数因子近似；针对加性奖励函数提出了首个FPTAS。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体合同设计中组合行动与预算约束的结合问题，填补现有研究在无预算组合行动和有预算二元行动之间的空白，探索在这种复杂设置下的可计算性边界。

Method: 采用理论分析的方法，通过构造性证明展示次模函数的计算困难性，同时为替代品奖励函数设计确定性多项式时间近似算法，并为加性奖励函数开发首个FPTAS。

Result: 1) 次模奖励函数在预算约束下无法获得任何有限因子近似；2) 替代品奖励函数可获得O(1)常数因子近似；3) 加性奖励函数可获得任意精度的FPTAS近似。

Conclusion: 该研究首次在组合合同设计中划清了预算约束与无预算约束的界限，并确定了替代品函数作为预算组合合同的可计算边界，为多智能体合同设计提供了重要的理论基础。

Abstract: We study multi-agent contract design with combinatorial actions, under budget constraints, and for a broad class of objective functions, including profit (principal's utility), reward, and welfare. Our first result is a strong impossibility: For submodular reward functions, no randomized poly-time algorithm can approximate the optimal budget-feasible value within \textit{any finite factor}, even with demand-oracle access. This result rules out extending known constant-factor guarantees from either (i) unbudgeted settings with combinatorial actions or (ii) budgeted settings with binary actions, to their combination. The hardness is tight: It holds even when all but one agent have binary actions and the remaining agent has just one additional action. On the positive side, we show that gross substitutes rewards (a well-studied strict subclass of submodular functions) admit a deterministic poly-time $O(1)$-approximation, using only value queries. Our results thus draw the first sharp separation between budgeted and unbudgeted settings in combinatorial contracts, and identifies gross substitutes as a tractable frontier for budgeted combinatorial contracts. Finally, we present an FPTAS for additive rewards, demonstrating that arbitrary approximation is tractable under any budget. This constitutes the first FPTAS for the multi-agent combinatorial-actions setting, even in the absence of budget constraints.

</details>


### [5] [Lower Bias, Higher Welfare: How Creator Competition Reshapes Bias-Variance Tradeoff in Recommendation Platforms?](https://arxiv.org/abs/2511.20289)
*Kang Wang,Renzhe Xu,Bo Li*

Main category: cs.GT

TL;DR: 本文研究了内容创作者竞争如何影响推荐系统中偏差-方差权衡，发现在战略环境下平台应选择更弱的正则化以降低偏差，从而提高用户福利。


<details>
  <summary>Details</summary>
Motivation: 理解用户表示学习中的偏差-方差权衡对提升推荐质量至关重要，但在内容创作者根据平台激励进行战略调整时，这一权衡变得更加复杂。

Method: 提出了内容创作者竞争与偏差-方差权衡框架，使用博弈论模型分析平台的正则化强度决策，并在非战略基准和战略环境下比较最优策略。

Result: 理论分析和实验验证表明，与静态环境相比，内容创作者竞争使平台的最优策略倾向于更弱的正则化，即偏好更低的偏差。

Conclusion: 在战略环境中减少偏差能带来更高的用户福利，这对现实世界推荐算法的设计具有实际意义。

Abstract: Understanding the bias-variance tradeoff in user representation learning is essential for improving recommendation quality in modern content platforms. While well studied in static settings, this tradeoff becomes significantly more complex when content creators strategically adapt to platform incentives. To analyze how such competition reshapes the tradeoff for maximizing user welfare, we introduce the Content Creator Competition with Bias-Variance Tradeoff framework, a tractable game-theoretic model that captures the platform's decision on regularization strength in user feature estimation. We derive and compare the platform's optimal policy under two key settings: a non-strategic baseline with fixed content and a strategic environment where creators compete in response to the platform's algorithmic design.
  Our theoretical analysis in a stylized model shows that, compared to the non-strategic environment, content creator competition shifts the platform's optimal policy toward weaker regularization, thereby favoring lower bias in the bias-variance tradeoff. To validate and assess the robustness of these insights beyond the stylized setting, we conduct extensive experiments on both synthetic and real-world benchmark datasets. The empirical results consistently support our theoretical conclusion: in strategic environments, reducing bias leads to higher user welfare. These findings offer practical implications for the design of real-world recommendation algorithms in the presence of content creator competition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [6] [Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization](https://arxiv.org/abs/2511.19830)
*Junhao Zhu,Lu Chen,Xiangyu Ke,Ziquan Fang,Tianyi Li,Yunjun Gao,Christian S. Jensen*

Main category: cs.DB

TL;DR: Nirvana是一个多模态数据分析框架，通过可编程语义操作符和LLM驱动的语义查询处理，结合逻辑和物理查询优化策略，显著提升多模态数据分析的效率。


<details>
  <summary>Details</summary>
Motivation: 传统关系查询操作符在捕捉查询语义方面能力有限，阻碍了多模态分析在现实世界的应用。基础模型特别是大语言模型的出现为开发灵活、语义感知的数据分析系统提供了新机遇。

Method: 1. 基于自然语言转换规则和随机游走搜索的智能逻辑优化器；2. 使用改进分数指标的成本感知物理优化器，为每个操作符选择最有效的LLM后端；3. 结合计算重用和评估下推技术。

Result: 在三个真实世界基准测试中，Nirvana能够将端到端运行时间减少10%-85%，系统处理成本平均降低76%，在效率和可扩展性方面均优于最先进的系统。

Conclusion: Nirvana框架成功解决了多模态数据分析中的关键挑战，通过语义感知的查询处理和优化策略，显著提升了系统性能和成本效益。

Abstract: Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm.
  We present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability.

</details>


### [7] [Updatable Balanced Index for Fast On-device Search with Auto-selection Model](https://arxiv.org/abs/2511.20049)
*Yushuai Ji,Sheng Wang,Zhiyu Chen,Yuan Sun,Zhiyong Peng*

Main category: cs.DB

TL;DR: UnIS是一个针对边缘设备上空间数据搜索优化的索引系统，通过加速BMKD树构建、支持实时插入和自适应查询策略选择，显著提升了kNN搜索和半径搜索的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的平衡多路KD树(BMKD-tree)在边缘设备上存在构建开销大、不支持实时插入、查询性能不一致等问题，需要一种更高效的索引方案来支持边缘数据分析。

Method: 1. 利用数据集分布预测分割超平面来加速BMKD树构建；2. 提出选择性子树重建方案来加速插入时的重新平衡；3. 设计自动选择模型为任意查询任务选择最优搜索策略。

Result: 相比BMKD树，UnIS在索引构建上平均加速17.96倍，插入加速1.60倍，kNN搜索加速7.15倍，半径搜索加速1.09倍。在边缘设备数据集简化任务中，比Lloyd算法加速217倍。

Conclusion: UnIS通过优化索引构建、插入和查询过程，为边缘设备上的空间数据搜索提供了高效解决方案，显著提升了各种搜索操作的性能。

Abstract: Diverse types of edge data, such as 2D geo-locations and 3D point clouds, are collected by sensors like lidar and GPS receivers on edge devices. On-device searches, such as k-nearest neighbor (kNN) search and radius search, are commonly used to enable fast analytics and learning technologies, such as k-means dataset simplification using kNN. To maintain high search efficiency, a representative approach is to utilize a balanced multi-way KD-tree (BMKD-tree). However, the index has shown limited gains, mainly due to substantial construction overhead, inflexibility to real-time insertion, and inconsistent query performance. In this paper, we propose UnIS to address the above limitations. We first accelerate the construction process of the BMKD-tree by utilizing the dataset distribution to predict the splitting hyperplanes. To make the continuously generated data searchable, we propose a selective sub-tree rebuilding scheme to accelerate rebalancing during insertion by reducing the number of data points involved. We then propose an auto-selection model to improve query performance by automatically selecting the optimal search strategy among multiple strategies for an arbitrary query task. Experimental results show that UnIS achieves average speedups of 17.96x in index construction, 1.60x in insertion, 7.15x in kNN search, and 1.09x in radius search compared to the BMKD-tree. We further verify its effectiveness in accelerating dataset simplification on edge devices, achieving a speedup of 217x over Lloyd's algorithm.

</details>


### [8] [Mobility Stream Processing on NebulaStream and MEOS](https://arxiv.org/abs/2511.20084)
*Mariana M. Garcez Duarte,Dwi P. A. Nugroho,Georges Tod,Evert Bevernage,Pieter Moelans,Emine Tas,Esteban Zimanyi,Mahmoud Sakr,Steffen Zeuch,Volker Markl*

Main category: cs.DB

TL;DR: NebulaMEOS结合MEOS时空处理库与NebulaStream物联网数据管理系统，实现实时时空流数据处理，应用于比利时铁路公司的列车监控。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器在移动物体中的广泛应用产生了大量时空流数据，但现有流处理系统缺乏时空处理能力，而时空库主要关注历史数据分析，导致实时时空分析存在挑战。

Method: 将MEOS时空处理库集成到NebulaStream物联网数据管理系统中，利用时空功能实时处理和分析流数据。

Result: 系统能够处理比利时铁路公司列车边缘设备传输的数据流，支持地理围栏和地理空间复杂事件处理，可视化实时列车运行和环境影响。

Conclusion: NebulaMEOS成功填补了实时时空流数据处理的空白，为物联网环境中的实时时空分析提供了可行解决方案。

Abstract: The increasing use of Internet-of-Things (IoT) sensors in moving objects has resulted in vast amounts of spatiotemporal streaming data. To analyze this data in situ, real-time spatiotemporal processing is needed. However, current stream processing systems designed for IoT environments often lack spatiotemporal processing capabilities, and existing spatiotemporal libraries primarily focus on analyzing historical data. This gap makes performing real-time spatiotemporal analytics challenging. In this demonstration, we present NebulaMEOS, which combines MEOS (Mobility Engine Open Source), a spatiotemporal processing library, with NebulaStream, a scalable data management system for IoT applications. By integrating MEOS into NebulaStream, NebulaMEOS utilizes spatiotemporal functionalities to process and analyze streaming data in real-time. We demonstrate NebulaMEOS by querying data streamed from edge devices on trains by the Société Nationale des Chemins de fer Belges (SNCB). Visitors can experience demonstrations of geofencing and geospatial complex event processing, visualizing real-time train operations and environmental impacts.

</details>


### [9] [N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics](https://arxiv.org/abs/2511.20125)
*Yihua Hu,Hao Ding,Wei Dong*

Main category: cs.DB

TL;DR: 提出了N2E框架，将节点差分隐私图分析任务转化为边差分隐私任务，通过距离保持裁剪和最大度近似机制，显著提升了节点差分隐私的实用性。


<details>
  <summary>Details</summary>
Motivation: 节点差分隐私比边差分隐私提供更强的隐私保护，但由于技术挑战而研究较少。需要一种通用框架将节点差分隐私任务转化为边差分隐私任务，以便重用现有的边差分隐私机制。

Method: 提出N2E框架，包含距离保持裁剪机制（限制裁剪后相邻图之间的边距离）和首个节点差分隐私的最大度近似机制，实现紧密的隐私保护裁剪阈值。

Result: 在边计数任务中，理论误差与最优方法匹配；在度分布估计中显著优于现有方法。实验显示边计数误差降低2.5倍，度分布估计误差降低80倍。

Conclusion: N2E框架成功将节点差分隐私任务转化为边差分隐私任务，在保持强隐私保护的同时显著提升了分析结果的准确性。

Abstract: Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation.

</details>


### [10] [An experimental study of existing tools for outlier detection and cleaning in trajectories](https://arxiv.org/abs/2511.20139)
*Mariana M Garcez Duarte,Mahmoud Sakr*

Main category: cs.DB

TL;DR: 本文评估了10个开源库在轨迹数据异常点检测和清理方面的性能，比较了它们的效率和准确性，并提出了建立真实基准的方法，旨在帮助用户选择合适的异常检测工具。


<details>
  <summary>Details</summary>
Motivation: 异常检测和数据清理是数据预处理的关键步骤，对于确保数据分析的完整性和有效性至关重要。本文关注单个轨迹内部的异常点，旨在评估现有工具的实用性能。

Method: 实验评估了10个开源库，比较它们在识别和清理异常点方面的效率和准确性。同时调查了最先进的异常检测算法，并将其分为五类：基于统计的方法、滑动窗口算法、基于聚类的方法、基于图的方法和基于启发式的方法。

Result: 研究提供了这些库性能的深入见解，比较了不同工具在真实世界应用场景下的表现。

Conclusion: 本研究为数据预处理和异常检测方法的发展做出了贡献，并为用户选择最适合其特定异常检测需求的工具提供了指导。

Abstract: Outlier detection and cleaning are essential steps in data preprocessing to ensure the integrity and validity of data analyses. This paper focuses on outlier points within individual trajectories, i.e., points that deviate significantly inside a single trajectory. We experiment with ten open-source libraries to comprehensively evaluate available tools, comparing their efficiency and accuracy in identifying and cleaning outliers. This experiment considers the libraries as they are offered to end users, with real-world applicability. We compare existing outlier detection libraries, introduce a method for establishing ground-truth, and aim to guide users in choosing the most appropriate tool for their specific outlier detection needs. Furthermore, we survey the state-of-the-art algorithms for outlier detection and classify them into five types: Statistic-based methods, Sliding window algorithms, Clustering-based methods, Graph-based methods, and Heuristic-based methods. Our research provides insights into these libraries' performance and contributes to developing data preprocessing and outlier detection methodologies.

</details>


### [11] [Forgetting by Pruning: Data Deletion in Join Cardinality Estimation](https://arxiv.org/abs/2511.20293)
*Chaowei He,Yuanjun Liu,Qingzhi Ma,Shenyuan Ren,Xizhao Luo,Lei Zhao,An Liu*

Main category: cs.DB

TL;DR: 提出了首个专门针对多表学习基数估计系统的遗忘框架CEP，通过分布敏感剪枝和域剪枝解决数据删除带来的挑战，在IMDB和TPC-H数据集上表现优于完全重训练。


<details>
  <summary>Details</summary>
Motivation: 学习基数估计系统中的机器遗忘面临独特挑战，包括属性级敏感性、表间传播和域消失导致多表连接严重高估问题。

Method: CEP框架包含分布敏感剪枝（构建半连接删除结果并计算敏感度分数指导参数剪枝）和域剪枝（移除因删除而完全消除的值域支持）。

Result: 在NeuroCard和FACE架构上评估显示，CEP在多表场景下始终获得最低Q-error，特别是在高删除率下常优于完全重训练，收敛迭代显著减少，计算开销仅为微调时间的0.3%-2.5%。

Conclusion: CEP是首个专门针对多表学习基数估计系统的有效遗忘框架，能高效处理数据删除问题，性能优异且计算开销极小。

Abstract: Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.

</details>


### [12] [The Case for Intent-Based Query Rewriting](https://arxiv.org/abs/2511.20419)
*Gianna Lisa Nicolai,Patrick Hansert,Sebastian Michel*

Main category: cs.DB

TL;DR: 提出INQURE系统，基于意图的查询重写方法，使用LLM理解查询意图并生成替代查询，保持洞察力不变但可改变结构和语法


<details>
  <summary>Details</summary>
Motivation: 解决传统查询重写仅优化执行时间的问题，允许重写改变查询结构和语法但保持洞察力，应对数据访问受限、隐私保护或高成本访问等场景

Method: 使用LLM进行查询理解和类人推理生成替代查询，结合前置表过滤、候选重写剪枝和排序技术

Result: 在900多个数据库表模式的基准集上评估，通过用户研究分析不同方法在运行时间和重写质量方面的优缺点

Conclusion: INQURE系统实现了可行的意图驱动查询重写，为数据访问受限场景提供了有效解决方案

Abstract: With this work, we describe the concept of intent-based query rewriting and present a first viable solution. The aim is to allow rewrites to alter the structure and syntactic outcome of an original query while keeping the obtainable insights intact. This drastically differs from traditional query rewriting, which typically aims to decrease query evaluation time by using strict equivalence rules and optimization heuristics on the query plan. Rewriting queries to queries that only provide a similar insight but otherwise can be entirely different can remedy inaccessible original data tables due to access control, privacy, or expensive data access regarding monetary cost or remote access. In this paper, we put forward INQURE, a system designed for INtent-based QUery REwriting. It uses access to a large language model (LLM) for the query understanding and human-like derivation of alternate queries. Around the LLM, INQURE employs upfront table filtering and subsequent candidate rewrite pruning and ranking. We report on the results of an evaluation using a benchmark set of over 900 database table schemas and discuss the pros and cons of alternate approaches regarding runtime and quality of the rewrites of a user study.

</details>


### [13] [InferF: Declarative Factorization of AI/ML Inferences over Joins](https://arxiv.org/abs/2511.20489)
*Kanchan Chowdhury,Lixi Zhou,Lulu Xie,Xinwei Fu,Jia Zou*

Main category: cs.DB

TL;DR: 提出InferF系统，通过因子化ML计算在多路连接中优化AI/ML推理工作流，减少冗余计算和连接成本，实现最高11.3倍的加速


<details>
  <summary>Details</summary>
Motivation: 现实AI/ML工作流常对多数据集连接后的特征向量进行推理计算，现有因子化ML方法未充分讨论多路连接场景下的影响，需要解决重复数据记录导致的冗余计算问题

Method: 提出InferF系统，将推理工作流表示为多路连接上的可分析表达式，通过贪心算法和遗传算法将因子化计算下推到连接树中的合格节点，最小化推理计算和连接成本

Result: 在Meta开源的Velox数据库引擎上实现InferF，在真实数据集上观察到最高11.3倍的加速，并系统总结了因子化ML何时能受益于AI/ML推理工作流的关键因素

Conclusion: InferF系统能有效优化多路连接场景下的AI/ML推理工作流，通过因子化计算减少冗余，显著提升性能，为实际应用提供了实用解决方案

Abstract: Real-world AI/ML workflows often apply inference computations to feature vectors joined from multiple datasets. To avoid the redundant AI/ML computations caused by repeated data records in the join's output, factorized ML has been proposed to decompose ML computations into sub-computations to be executed on each normalized dataset. However, there is insufficient discussion on how factorized ML could impact AI/ML inference over multi-way joins. To address the limitations, we propose a novel declarative InferF system, focusing on the factorization of arbitrary inference workflows represented as analyzable expressions over the multi-way joins. We formalize our problem to flexibly push down partial factorized computations to qualified nodes in the join tree to minimize the overall inference computation and join costs and propose two algorithms to resolve the problem: (1) a greedy algorithm based on a per-node cost function that estimates the influence on overall latency if a subset of factorized computations is pushed to a node, and (2) a genetic algorithm for iteratively enumerating and evaluating promising factorization plans. We implement InferF on Velox, an open-sourced database engine from Meta, evaluate it on real-world datasets, observed up to 11.3x speedups, and systematically summarized the factors that determine when factorized ML can benefit AI/ML inference workflows.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [14] [The Buffer Minimization Problem for Scheduling Flow Jobs with Conflicts](https://arxiv.org/abs/2511.19690)
*Niklas Haas,Sören Schmitt,Rob van Stee*

Main category: cs.DS

TL;DR: 该论文研究了多处理器系统中带冲突的在线缓冲区最小化问题，在流模型下给出了多种图结构的紧界分析。


<details>
  <summary>Details</summary>
Motivation: 研究多处理器系统中考虑处理器间冲突的在线调度问题，目标是最小化调度过程中所有处理器上的最大工作负载。

Method: 采用流模型分析，其中工作负载连续到达而非离散到达，通过图论方法分析不同图结构的调度性能。

Result: 为4顶点图（除路径图外）、完全图和完全二分图提供了紧界，为完全k分图提供了几乎紧界，并在原始模型中改进了三角形加一边图的分析。

Conclusion: 论文在流模型下系统分析了多种图结构的缓冲区最小化问题，为理解处理器冲突对调度性能的影响提供了理论依据。

Abstract: We consider the online buffer minimization in multiprocessor systems with conflicts problem (in short, the buffer minimization problem) in the recently introduced flow model. In an online fashion, workloads arrive on some of the $n$ processors and are stored in an input buffer. Processors can run and reduce these workloads, but conflicts between pairs of processors restrict simultaneous task execution. Conflicts are represented by a graph, where vertices correspond to processors and edges indicate conflicting pairs. An online algorithm must decide which processors are run at a time; so provide a valid schedule respecting the conflict constraints.
  The objective is to minimize the maximal workload observed across all processors during the schedule. Unlike the original model, where workloads arrive as discrete blocks at specific time points, the flow model assumes workloads arrive continuously over intervals or not at all. We present tight bounds for all graphs with four vertices (except the path, which has been solved previously) and for the families of general complete graphs and complete bipartite graphs. We also recover almost tight bounds for complete $k$-partite graphs.
  For the original model, we narrow the gap for the graph consisting of a triangle and an additional edge to a fourth vertex.

</details>


### [15] [Greedy Algorithms for Shortcut Sets and Hopsets](https://arxiv.org/abs/2511.20111)
*Ben Bals,Joakim Blikstad,Greg Bodwin,Daniel Dadush,Sebastian Forster,Yasamin Nazari*

Main category: cs.DS

TL;DR: 本文提出了贪心算法用于构建跳集和捷径集，证明了在大小上的最优性，并开发了更快的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 探索贪心算法在跳集和捷径集构建中的潜力，提供更简单且概念上清晰的替代方案，同时改进现有算法的效率。

Method: 使用简单的贪心算法构建跳集和捷径集，并基于路径上的贪心集合覆盖近似算法开发确定性算法。

Result: 捷径集大小达到当前最优上界；跳集大小在技术假设下几乎最优；确定性算法在O(mn^{2/3})时间内计算大小为O(n)、跳界为O(n^{1/3})的捷径集。

Conclusion: 贪心算法在跳集和捷径集构建中具有强大的理论保证，虽然计算较慢，但概念简单；确定性算法在效率上有所改进。

Abstract: We explore the power of greedy algorithms for hopsets and shortcut sets. In particular, we propose simple greedy algorithms that, given an input graph $G$ and a parameter $β$, compute a shortcut set or an exact hopset $H$ of hopbound at most $β$, and we prove the following guarantees about the size $|H|$ of the output:
  For shortcut sets, we prove the bound $$|H| \le \tilde{O}\left( \frac{n^2}{β^3} + \frac{n^{3/2}}{β^{3/2}} \right).$$ This matches the current state-of-the-art upper bound by Kogan and Parter [SODA '22].
  For exact hopsets of $n$-node, $m$-edge weighted graphs, the size of the output hopset is existentially optimal up to subpolynomial factors, under some technical assumptions.
  Despite their simplicity and conceptual implications, these greedy algorithms are slower than existing sampling-based approaches. Our second set of results focus on faster deterministic algorithms that are based on a certain greedy set cover approximation algorithm on paths in the transitive closure. One consequence is a deterministic algorithm that takes $O(mn^{2/3})$ time to compute a shortcut set of size $\tilde{O}(n)$ and hopbound $O(n^{1/3})$.

</details>


### [16] [Maintaining Bipartite Colourings on Temporal Graphs on a Budget](https://arxiv.org/abs/2511.20338)
*Duncan Adamson,George B. Mertzios,Paul G. Spirakis*

Main category: cs.DS

TL;DR: 该论文研究时态图中的图着色问题，重点关注维护二分着色的复杂性。证明了即使在每个快照都是二分图的情况下，维护二分着色也是NP难问题，且难以近似。同时提供了精确算法和近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统图着色用于避免网络中的冲突，但现实网络结构会随时间变化。需要研究时态图中的着色维护问题，以处理动态网络中资源访问变化时的冲突避免需求。

Method: 研究了时态图中维护正确着色的决策问题，特别关注二分着色。通过计算复杂性分析和算法设计，包括精确算法和近似算法。

Result: 证明维护二分着色在时态图中是NP难问题，且难以近似。提供了时间复杂度为O(T|E|2^k + nT2^{2k})的精确算法和O(√log(nT))因子近似算法。

Conclusion: 时态图的结构复杂性使得即使在静态图中容易的问题（如二分着色）也变得困难，这对理解动态网络的基本计算问题具有重要意义。

Abstract: Graph colouring is a fundamental problem for networks, serving as a tool for avoiding conflicts via symmetry breaking, for example, avoiding multiple computer processes simultaneously updating the same resource. This paper considers a generalisation of this problem to \emph{temporal graphs}, i.e., to graphs whose structure changes according to an ordered sequence of edge sets. In the simultaneous resource updating problem on temporal graphs, the resources which can be accessed will change, however, the necessity of symmetry breaking to avoid conflicts remains.
  In this paper, we focus on the problem of \emph{maintaining proper colourings} on temporal graphs in general, with a particular focus on bipartite colourings. Our aim is to minimise the total number of times that the vertices change colour, or, in the form of a decision problem, whether we can maintain a proper colouring by allowing not more colour changes than some given \emph{budget}. On the negative side, we show that, despite bipartite colouring being easy on static graphs, the problem of maintaining such a colouring on graphs that are bipartite in each snapshot is NP-Hard to even approximate within \emph{any} constant factor unless the Unique Games Conjecture fails. On the positive side, we provide an exact algorithm for a temporal graph with $n$ vertices, a lifetime $T$ and at most $k$ components in any given snapshot in $O(T \vert E \vert 2^{k} + n T 2^{2k})$ time, and an $O\left(\sqrt{\log(nT)}\right)$-factor approximation algorithm running in $\tilde{O}((nT)^3)$ time.
  Our results contribute to the structural complexity of networks that change with time with respect to a fundamental computational problem.

</details>


### [17] [Robust Algorithms for Finding Cliques in Random Intersection Graphs via Sum-of-Squares](https://arxiv.org/abs/2511.20376)
*Andreas Göbel,Janosch Ruff,Leon Schiller*

Main category: cs.DS

TL;DR: 本文研究了在稠密随机交集图中高效恢复团结构的算法，这是首个处理重叠团恢复问题的研究，解决了比不相交团恢复更困难的挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然已有大量工作研究在随机图中恢复单个或多个不相交团，但重叠团的恢复问题却鲜有探索。由于每个顶点可能属于多项式数量的团，这个任务比不相交团恢复要困难得多，导致简单的组合和谱算法失效。

Method: 采用证明到算法的框架，利用平方和层次结构，开发了能够处理噪声、单调对手和最优数量边损坏的鲁棒算法。

Result: 获得了首个在稠密随机交集图中高效恢复社区结构的算法，包括精确恢复和近似恢复，算法在 $k \gg \sqrt{n \log(n)}$ 时有效。

Conclusion: 这项工作为重叠团恢复问题提供了首个高效解决方案，填补了该领域的重要空白，并展示了平方和层次结构在解决这类复杂问题中的有效性。

Abstract: We study efficient algorithms for recovering cliques in dense random intersection graphs (RIGs). In this model, $d = n^{Ω(1)}$ cliques of size approximately $k$ are randomly planted by choosing the vertices to participate in each clique independently with probability $δ$. While there has been extensive work on recovering one, or multiple disjointly planted cliques in random graphs, the natural extension of this question to recovering overlapping cliques has been, surprisingly, largely unexplored. Moreover, because every vertex can be part of polynomially many cliques, this task is significantly harder than in case of disjointly planted cliques (as recently studied by Kothari, Vempala, Wein and Xu [COLT'23]) and manifests in the failure of simple combinatorial and even spectral algorithms.
  In this work we obtain the first efficient algorithms for recovering the community structure of RIGs both from the perspective of exact and approximate recovery. Our algorithms are further robust to noise, monotone adversaries, a certain, optimal number of edge corruptions, and work whenever $k \gg \sqrt{n \log(n)}$. Our techniques follow the proofs-to-algorithms framework utilizing the sum-of-squares hierarchy.

</details>


### [18] [Counting large patterns in degenerate graphs](https://arxiv.org/abs/2511.20385)
*Christine Awofeso,Patrick Greaves,Oded Lachish,Felix Reidl*

Main category: cs.DS

TL;DR: 本文改进了在d-退化图上子图计数的时间复杂度，提出(c,d)-可定位图的概念，使得算法运行时间对模式图H的大小只有多项式依赖。


<details>
  <summary>Details</summary>
Motivation: Curticapean和Marx的算法在一般图上子图计数的时间复杂度对模式图H的大小有指数依赖，本文旨在对d-退化图这一受限图类改进这一上界。

Method: 提出(c,d)-可定位图的概念，这类图具有最多cd大小的顶点覆盖，并针对这类图设计了改进的子图计数算法。

Result: 对于(c,d)-可定位图，算法运行时间对|H|只有多项式依赖；对于(1,d)-可定位图，运行时间对|G|是线性的。

Conclusion: 在d-退化图上，对于(c,d)-可定位图，子图计数的时间复杂度可以显著改进，且证明了对于几乎不是(1,d)-可定位的图，子图计数仍然是#W[1]-难的。

Abstract: The problem of subgraph counting asks for the number of occurrences of a pattern graph $H$ as a subgraph of a host graph $G$ and is known to be computationally challenging: it is $\#W[1]$-hard even when $H$ is restricted to simple structures such as cliques or paths. Curticapean and Marx (FOCS'14) show that if the graph $H$ has vertex cover number $τ$, subgraph counting has time complexity $O(|H|^{2^{O(τ)}} |G|^{τ+ O(1)})$. This raises the question of whether this upper bound can be improved for input graphs $G$ from a restricted family of graphs. Earlier work by Eppstein~(IPL'94) shows that this is indeed possible, by proving that when $G$ is a $d$-degenerate graph and $H$ is a biclique of arbitrary size, subgraph counting has time complexity $O(d 3^{d/3} |G|)$. We show that if the input is restricted to $d$-degenerate graphs, the upper bound of Curticapean and Marx can be improved for a family of graphs $H$ that includes all bicliques and satisfies a property we call $(c,d)$-locatable. Importantly, our algorithm's running time only has a polynomial dependence on the size of~$H$. A key feature of $(c,d)$-locatable graphs $H$ is that they admit a vertex cover of size at most $cd$. We further characterize $(1,d)$-locatable graphs, for which our algorithms achieve a linear running time dependence on $|G|$, and we establish a lower bound showing that counting graphs which are barely not $(1,d)$-locatable is already $\#\text{W}[1]$-hard. We note that the restriction to $d$-degenerate graphs has been a fruitful line of research leading to two very general results (FOCS'21, SODA'25) and this creates the impression that we largely understand the complexity of counting substructures in degenerate graphs. However, all aforementioned results have an exponential dependency on the size of the pattern graph $H$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [19] [SCoTER: Structured Chain-of-Thought Transfer for Enhanced Recommendation](https://arxiv.org/abs/2511.19514)
*Yang Wu,Qian Li,Yuling Xiong,Hongbo Tang,Xun Liu,Jun Zhang,Huan Yu,Jie Jiang,Hailong Shi*

Main category: cs.IR

TL;DR: SCoTER是一个统一框架，通过自动模式发现和结构保持集成，将LLM的推理能力转移到推荐系统中，消除在线LLM推理成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏自动发现有效推理模式的机制，依赖手动模板或不稳定的零样本提示；同时采用结构坍塌的集成方式，要么在线推理成本过高，要么将推理链压缩为单个向量丢失逐步逻辑。

Method: SCoTER包含两个协同组件：GVM管道用于自动模式发现，以及结构保持集成架构将逐步逻辑转移到高效模型中。通过信息论证明结构保持转移比结构无关方法具有更紧的性能界限。

Result: 在四个基准测试中比强基线TIGER提升3.75%-11.59%；在腾讯广告平台部署中，GMV提升2.14%同时消除在线LLM推理成本。

Conclusion: SCoTER为将结构化LLM推理转移到大规模推荐系统提供了一个原则性且经过生产验证的蓝图。

Abstract: Harnessing the reasoning power of Large Language Models (LLMs) for recommender systems is hindered by two fundamental challenges. First, current approaches lack a mechanism for automated, data-driven discovery of effective reasoning patterns, relying instead on brittle manual templates or unstable zero-shot prompting. Second, they employ structure-collapsing integration: direct prompting incurs prohibitive online inference costs, while feature extraction collapses reasoning chains into single vectors, discarding stepwise logic. To address these challenges, we propose SCoTER (Structured Chain-of-Thought Transfer for Enhanced Recommendation), a unified framework that treats pattern discovery and structure-aware transfer as a jointly optimized problem. Specifically, SCoTER operationalizes this through two synergistic components: a GVM pipeline for automated pattern discovery and a structure-preserving integration architecture that transfers stepwise logic to efficient models. Formally, we provide information-theoretic justification proving that structure-preserving transfer achieves tighter performance bounds than structure-agnostic alternatives. Empirically, experiments on four benchmarks demonstrate improvements of 3.75\%-11.59\% over a strong TIGER backbone. Moreover, in production deployment on the Tencent Advertising Platform, SCoTER achieved a 2.14\% lift in Gross Merchandise Value (GMV) while eliminating online LLM inference costs. Overall, SCoTER establishes a principled and production-validated blueprint for transferring structured LLM reasoning to large-scale recommender systems.

</details>


### [20] [LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training](https://arxiv.org/abs/2511.19931)
*Ziwei Liu,Qidong Liu,Wanyu Wang,Yejing Wang,Tong Xu,Wei Huang,Chong Chen,Peng Chuan,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出LLM-EDT方法解决跨域序列推荐中的不平衡问题和转移问题，通过可转移项目增强器、双阶段训练策略和领域感知分析模块来提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前跨域序列推荐存在不平衡问题（一个领域交互主导整个行为）和转移问题（难以捕捉用户跨域偏好），现有LLM增强方法未能有效处理无关噪声和粗略分析问题。

Method: 使用可转移项目增强器自适应生成跨域行为，采用双阶段训练策略赋予领域特定线程领域共享背景，设计领域感知分析模块总结用户在各领域偏好并自适应聚合。

Result: 在三个公共数据集上的实验验证了LLM-EDT的有效性，代码已开源。

Conclusion: LLM-EDT通过创新的增强、训练和分析方法成功解决了跨域序列推荐中的关键挑战，提升了推荐性能。

Abstract: Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}.

</details>


### [21] [The 2nd Workshop on Human-Centered Recommender Systems](https://arxiv.org/abs/2511.19979)
*Kaike Zhang,Jiakai Tang,Du Su,Shuchang Liu,Julian McAuley,Lina Yao,Qi Cao,Yue Feng,Fei Sun*

Main category: cs.IR

TL;DR: 该论文呼吁推荐系统从优化参与度转向真正理解、涉及和造福人类，强调整合人类价值观如信任、安全、公平、透明和福祉到推荐过程中。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统影响力的增长，传统指标如准确性、点击率和参与度已无法真正反映对人类重要的价值，需要转向以人类为中心的推荐系统设计。

Method: 通过组织人类中心推荐系统研讨会，汇集推荐系统、人机交互、AI安全和社交计算等领域的研究者，围绕人类理解、人类参与和人类影响三个主题轴展开讨论。

Result: 研讨会通过主题演讲、小组讨论和论文展示，涵盖了从基于LLM的交互式推荐器到社会福利优化等主题，促进了跨学科合作。

Conclusion: HCRS旨在通过促进跨学科合作，塑造未来十年负责任且与人类价值观一致的推荐研究发展方向。

Abstract: Recommender systems shape how people discover information, form opinions, and connect with society. Yet, as their influence grows, traditional metrics, e.g., accuracy, clicks, and engagement, no longer capture what truly matters to humans. The workshop on Human-Centered Recommender Systems (HCRS) calls for a paradigm shift from optimizing engagement toward designing systems that truly understand, involve, and benefit people. It brings together researchers in recommender systems, human-computer interaction, AI safety, and social computing to explore how human values, e.g., trust, safety, fairness, transparency, and well-being, can be integrated into recommendation processes. Centered around three thematic axes-Human Understanding, Human Involvement, and Human Impact-HCRS features keynotes, panels, and papers covering topics from LLM-based interactive recommenders to societal welfare optimization. By fostering interdisciplinary collaboration, HCRS aims to shape the next decade of responsible and human-aligned recommendation research.

</details>


### [22] [Popularity Bias Alignment Estimates](https://arxiv.org/abs/2511.19999)
*Anton Lyubinin*

Main category: cs.IR

TL;DR: 扩展了流行度偏差记忆化定理，涵盖任意度分布，并证明了与top-k奇异超空间对齐的上下界估计


<details>
  <summary>Details</summary>
Motivation: 扩展arXiv:2404.12008中的流行度偏差记忆化定理，使其适用于更一般的度分布情况

Method: 将定理推广到任意度分布，并推导与top-k奇异超空间对齐的上下界估计

Result: 获得了适用于任意度分布的流行度偏差记忆化定理，并建立了对齐度的上下界

Conclusion: 成功扩展了流行度偏差记忆化定理的适用范围，并提供了与奇异超空间对齐的严格数学界限

Abstract: We are extending Popularity Bias Memorization theorem from arXiv:archive/2404.12008 in several directions. We extend it to arbitrary degree distributions and also prove both upper and lower estimates for the alignment with top-k singular hyperspace.

</details>


### [23] [Adaptive Knowledge Transfer for Cross-Disciplinary Cold-Start Knowledge Tracing](https://arxiv.org/abs/2511.20009)
*Yulong Deng,Zheng Guan,Min He,Xue Wang,Jie Liu,Zheng Li*

Main category: cs.IR

TL;DR: 提出基于专家混合和对抗生成网络的跨学科冷启动知识追踪框架，解决目标学科学生交互数据不足的问题，通过聚类知识状态、专家混合网络和对抗判别器实现有效的跨领域知识迁移。


<details>
  <summary>Details</summary>
Motivation: 跨学科冷启动知识追踪面临目标学科学生交互数据不足的挑战，现有方法依赖学科间重叠实体进行简单映射，但现实中重叠实体稀缺且简单映射无法捕捉跨学科知识复杂性。

Method: 1) 预训练源学科模型并聚类学生知识状态为K类；2) 聚类属性通过门控机制指导专家混合网络作为跨域映射桥梁；3) 对抗判别器通过拉近相同属性学生特征、推远不同属性特征来增强特征分离，缓解小样本限制。

Result: 在20个极端跨学科冷启动场景中验证了方法的有效性。

Conclusion: 所提框架能够有效解决跨学科冷启动知识追踪中的数据不足问题，通过创新的专家混合和对抗学习机制实现更好的知识迁移和性能预测。

Abstract: Cross-Disciplinary Cold-start Knowledge Tracing (CDCKT) faces a critical challenge: insufficient student interaction data in the target discipline prevents effective knowledge state modeling and performance prediction. Existing cross-disciplinary methods rely on overlapping entities between disciplines for knowledge transfer through simple mapping functions, but suffer from two key limitations: (1) overlapping entities are scarce in real-world scenarios, and (2) simple mappings inadequately capture cross-disciplinary knowledge complexity. To overcome these challenges, we propose Mixed of Experts and Adversarial Generative Network-based Cross-disciplinary Cold-start Knowledge Tracing Framework. Our approach consists of three key components: First, we pre-train a source discipline model and cluster student knowledge states into K categories. Second, these cluster attributes guide a mixture-of-experts network through a gating mechanism, serving as a cross-domain mapping bridge. Third, an adversarial discriminator enforces feature separation by pulling same-attribute student features closer while pushing different-attribute features apart, effectively mitigating small-sample limitations. We validate our method's effectiveness across 20 extreme cross-disciplinary cold-start scenarios.

</details>


### [24] [Towards A Tri-View Diffusion Framework for Recommendation](https://arxiv.org/abs/2511.20122)
*Ximing Chen,Pui Ieng Lei,Yijun Sheng,Yanyan Liu,Zhiguo Gong*

Main category: cs.IR

TL;DR: 提出了一个基于热力学视角的简约扩散框架，通过最大化亥姆霍兹自由能来结合能量最大化和熵减少两个因素，用于推荐系统任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的推荐系统只关注能量最大化，而经典推荐模型关注熵减少，需要结合两者来提升推荐性能。

Method: 1) 提出最大化亥姆霍兹自由能的扩散框架；2) 设计保持固有各向异性的去噪器；3) 采用接受-拒绝Gumbel采样过程来优先处理大量未观测交互。

Result: 理论分析和大量实验表明，所提框架在准确性和效率方面明显优于基线方法。

Conclusion: 通过热力学视角结合能量最大化和熵减少，提出的扩散框架在推荐任务中表现出显著优势。

Abstract: Diffusion models (DMs) have recently gained significant interest for their exceptional potential in recommendation tasks. This stems primarily from their prominent capability in distilling, modeling, and generating comprehensive user preferences. However, previous work fails to examine DMs in recommendation tasks through a rigorous lens. In this paper, we first experimentally investigate the completeness of recommender models from a thermodynamic view. We reveal that existing DM-based recommender models operate by maximizing the energy, while classic recommender models operate by reducing the entropy. Based on this finding, we propose a minimalistic diffusion framework that incorporates both factors via the maximization of Helmholtz free energy. Meanwhile, to foster the optimization, our reverse process is armed with a well-designed denoiser to maintain the inherent anisotropy, which measures the user-item cross-correlation in the context of bipartite graphs. Finally, we adopt an Acceptance-Rejection Gumbel Sampling Process (AR-GSP) to prioritize the far-outnumbered unobserved interactions for model robustness. AR-GSP integrates an acceptance-rejection sampling to ensure high-quality hard negative samples for general recommendation tasks, and a timestep-dependent Gumbel Softmax to handle an adaptive sampling strategy for diffusion models. Theoretical analyses and extensive experiments demonstrate that our proposed framework has distinct superiority over baselines in terms of accuracy and efficiency.

</details>


### [25] [Enhancing Sequential Recommendation with World Knowledge from Large Language Models](https://arxiv.org/abs/2511.20177)
*Tianjie Dai,Xu Chen,Yunmeng Shu,Jinsong Lan,Xiaoyong Zhu,Jiangchao Yao,Bo Zheng*

Main category: cs.IR

TL;DR: GRASP是一个灵活的序列推荐框架，通过生成增强检索和整体注意力增强来整合LLM的世界知识，即使存在幻觉也能有效利用，并在多个数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统协同过滤序列推荐模型因协作信号有限导致性能不佳，现有LLM方法假设LLM生成结果正确但易受幻觉噪声影响。

Method: 提出GRASP框架，包含生成增强检索（描述性合成和相似性检索）和整体注意力增强（多级注意力机制），利用检索到的相似用户/物品作为辅助上下文信息。

Result: 在两个公共基准和一个工业数据集上的综合评估显示，GRASP与不同骨干网络集成时始终实现最先进性能。

Conclusion: GRASP通过生成增强检索和整体注意力增强有效缓解了基于监督方法的噪声指导问题，成功整合了LLM的世界知识。

Abstract: Sequential Recommendation System~(SRS) has become pivotal in modern society, which predicts subsequent actions based on the user's historical behavior. However, traditional collaborative filtering-based sequential recommendation models often lead to suboptimal performance due to the limited information of their collaborative signals. With the rapid development of LLMs, an increasing number of works have incorporated LLMs' world knowledge into sequential recommendation. Although they achieve considerable gains, these approaches typically assume the correctness of LLM-generated results and remain susceptible to noise induced by LLM hallucinations. To overcome these limitations, we propose GRASP (Generation Augmented Retrieval with Holistic Attention for Sequential Prediction), a flexible framework that integrates generation augmented retrieval for descriptive synthesis and similarity retrieval, and holistic attention enhancement which employs multi-level attention to effectively employ LLM's world knowledge even with hallucinations and better capture users' dynamic interests. The retrieved similar users/items serve as auxiliary contextual information for the later holistic attention enhancement module, effectively mitigating the noisy guidance of supervision-based methods. Comprehensive evaluations on two public benchmarks and one industrial dataset reveal that GRASP consistently achieves state-of-the-art performance when integrated with diverse backbones. The code is available at: https://anonymous.4open.science/r/GRASP-SRS.

</details>


### [26] [HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents](https://arxiv.org/abs/2511.20227)
*Anyang Tong,Xiang Niu,ZhiPing Liu,Chang Tian,Yanyan Wei,Zenglin Shi,Meng Wang*

Main category: cs.IR

TL;DR: HKRAG是一个针对视觉丰富文档(VRD)的全新多模态检索增强生成框架，通过显式建模显著知识和细粒度知识来解决现有方法偏向检索显著信息而忽略关键细节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态RAG方法在处理视觉丰富文档时往往偏向检索显著知识（如突出文本和视觉元素），而忽略了关键的细粒度知识（如小文本、上下文细节），导致检索不完整并影响生成答案的准确性和全面性。

Method: 提出HKRAG框架，包含两个关键组件：(1)基于混合掩码的全息检索器，使用显式掩码策略分别建模显著知识和细粒度知识；(2)不确定性引导的智能生成器，动态评估初始答案的不确定性并主动决定如何整合两种知识流。

Result: 在开放域视觉问答基准测试上的广泛实验表明，HKRAG在零样本和有监督设置下均持续优于现有方法。

Conclusion: HKRAG证明了全息知识检索对于VRD理解的关键重要性，能够显著提升多模态检索增强生成系统的性能。

Abstract: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.

</details>


### [27] [HHFT: Hierarchical Heterogeneous Feature Transformer for Recommendation Systems](https://arxiv.org/abs/2511.20235)
*Liren Yu,Wenming Zhang,Silu Zhou,Zhixuan Zhang,Dan Ou*

Main category: cs.IR

TL;DR: HHFT是一种基于Transformer的CTR预测架构，通过语义特征分区、异构Transformer编码器和Hiformer层设计，在工业场景中显著超越DNN基线，已在淘宝生产平台成功部署。


<details>
  <summary>Details</summary>
Motivation: 解决DNN在CTR预测中的局限性，特别是处理异构特征（用户画像、商品信息、行为序列等）时的语义混淆问题。

Method: 1) 语义特征分区：将异构特征按语义分组；2) 异构Transformer编码器：使用块特定的QKV投影和FFN避免语义混淆；3) Hiformer层：捕捉特征间的高阶交互。

Result: Transformer显著优于DNN基线，CTR AUC提升+0.4%，在淘宝生产平台部署后关键业务指标显著提升，GMV增长+0.6%。

Conclusion: HHFT架构在工业CTR预测中表现出色，通过专门的异构特征处理设计解决了传统方法的局限性，实现了业务指标的实质性提升。

Abstract: We propose HHFT (Hierarchical Heterogeneous Feature Transformer), a Transformer-based architecture tailored for industrial CTR prediction. HHFT addresses the limitations of DNN through three key designs: (1) Semantic Feature Partitioning: Grouping heterogeneous features (e.g. user profile, item information, behaviour sequennce) into semantically coherent blocks to preserve domain-specific information; (2) Heterogeneous Transformer Encoder: Adopting block-specific QKV projections and FFNs to avoid semantic confusion between distinct feature types; (3) Hiformer Layer: Capturing high-order interactions across features. Our findings reveal that Transformers significantly outperform DNN baselines, achieving a +0.4% improvement in CTR AUC at scale. We have successfully deployed the model on Taobao's production platform, observing a significant uplift in key business metrics, including a +0.6% increase in Gross Merchandise Value (GMV).

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [28] [The Quality of Information: A Weighted Entropy Approach to Near-Optimal Mastermind](https://arxiv.org/abs/2511.19446)
*Serkan Gür*

Main category: cs.IT

TL;DR: 提出了一种基于加权熵启发式的Mastermind游戏求解方法，通过遗传算法优化权重，实现了接近理论最优的性能（平均4.3488次猜测）。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在Mastermind游戏中的性能有限，需要更有效的信息评估策略来接近理论最优解。

Method: 应用基于Belis-Guias框架的加权熵启发式，为不同反馈类型分配上下文相关的效用值，使用遗传算法优化权重模式。

Result: 固定权重向量达到平均4.3565次猜测（最多5次），阶段加权启发式达到平均4.3488次猜测（最多6次），接近理论最优4.3403。

Conclusion: 该方法在保持计算效率的同时，通过原则性的信息评估显著提升了性能，提供了完整的可复现实现。

Abstract: This paper presents a novel class of information-theoretic strategies for solving the game of Mastermind, achieving state-of-the-art performance among known heuristic methods. The core contribution is the application of a weighted entropy heuristic, based on the Belis-Guias, u framework, which assigns context-dependent utility values to each of the possible feedback types. A genetic algorithm optimization approach discovers interpretable weight patterns that reflect strategic game dynamics. First, I demonstrate that a single, fixed vector of optimized weights achieves a remarkable 4.3565 average guesses with a maximum of 5. Building upon this, I introduce a stage-weighted heuristic with distinct utility vectors for each turn, achieving 4.3488 average guesses with a maximum of 6, approaching the theoretical optimum of 4.3403 by less than 0.2%. The method retains the computational efficiency of classical one-step-ahead heuristics while significantly improving performance through principled information valuation. A complete implementation and all optimized parameters are provided for full reproducibility.

</details>


### [29] [The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication](https://arxiv.org/abs/2511.19550)
*Davide Picca*

Main category: cs.IT

TL;DR: 提出了一个符号学框架来分析大语言模型，将其概念化为随机符号引擎，其输出需要人类主动的非对称解释。通过信息论工具形式化了表达丰富性（符号广度）和解释稳定性（可解读性）之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型分析往往关注模型内部机制，而忽略了从符号学角度理解模型输出与人类解释之间的关系。需要一种能够量化模型表达能力与可理解性之间平衡的理论框架。

Method: 使用信息论工具，将符号广度量化为源熵，可解读性量化为消息与人类解释之间的互信息。引入生成复杂度参数λ来建模这种权衡关系，并定义参数化的符号通道。

Result: 开发了一个可操作的符号学分析工具包，能够对模型进行性能分析、优化提示设计、基于模糊性的风险分析，以及构建自适应符号系统。

Conclusion: 这种基于容量的符号学方法为理解、评估和设计LLM介导的通信提供了严谨且可操作的框架，将分析重点从模型内部转向可观察的文本产物。

Abstract: This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $λ$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.

</details>


### [30] [One-Shot Coding and Applications](https://arxiv.org/abs/2511.19556)
*Yanxiao Liu*

Main category: cs.IT

TL;DR: 本文扩展了泊松函数表示法在单次信息论中的应用，为更复杂场景提供统一的编码方案，能够推导出渐近结果。


<details>
  <summary>Details</summary>
Motivation: 研究单次信息论中的可达性部分，针对信号块长度为1的情况，开发显式编码方案，旨在推导能够蕴含现有渐近结果的一击可达性结果。

Method: 扩展泊松函数表示法的适用性，将其应用于原始版本无法直接应用的更复杂场景，开发进一步扩展版本。

Result: 成功将泊松函数表示法扩展到各种更复杂的场景，为信息论问题提供统一的一击编码方案。

Conclusion: 泊松函数表示法在单次信息论中具有广泛适用性，通过适当扩展可以解决更复杂的信息论问题，并能推导出渐近结果。

Abstract: One-shot information theory addresses scenarios in source coding and channel coding where the signal blocklength is assumed to be 1. In this case, each source and channel can be used only once, and the sources and channels are arbitrary and not required to be memoryless or ergodic. We study the achievability part of one-shot information theory, i.e., we consider explicit coding schemes in the oneshot scenario. The objective is to derive one-shot achievability results that can imply existing (first-order and second-order) asymptotic results when applied to memoryless sources and channels, or applied to systems with memory that behave ergodically.
  Poisson functional representation was first proposed as a one-shot channel simulation technique by Li and El Gamal [118] for proving a strong functional representation lemma. It was later extended to the Poisson matching lemma by Li and Anantharam [117], which provided a unified one-shot coding scheme for a broad class of information-theoretic problems. The main contribution of this thesis is to extend the applicability of Poisson functional representation to various more complicated scenarios, where the original version cannot be applied directly and further extensions must be developed.

</details>


### [31] [A Hybrid Dominant-Interferer Approximation for SINR Coverage in Poisson Cellular Networks](https://arxiv.org/abs/2511.19568)
*Sunder Ram Krishnan,Junaid Farooq,Kumar Vijay Mishra,Xingchen Liu,S. Unnikrishna Pillai,Theodore S. Rappaport*

Main category: cs.IT

TL;DR: 提出了一种混合近似框架，通过结合蒙特卡洛采样主导干扰器和拉普拉斯函数表示残余远场干扰，解决了随机几何和概率干扰模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 随机几何模型涉及复杂嵌套积分和特殊函数，概率干扰模型仅在受限参数下可解，现有方法在通用性和计算效率方面存在局限。

Method: 采用主导干扰器加尾部分解结构，对少量主导干扰器进行蒙特卡洛采样，用拉普拉斯函数表示残余远场干扰，构建模块化、数值稳定且路径损耗无关的估计器。

Result: 该方法在噪声受限和干扰受限场景下均表现良好，理论误差界随主导干扰器数量增加而减小，验证了相对于现有基准模型的优越性。

Conclusion: 所提出的混合框架提供了更通用、计算高效的干扰建模方法，适用于现代蜂窝网络的设计和分析。

Abstract: Accurate radio propagation and interference modeling is essential for the design and analysis of modern cellular networks. Stochastic geometry offers a rigorous framework by treating base station locations as a Poisson point process and enabling coverage characterization through spatial averaging, but its expressions often involve nested integrals and special functions that limit general applicability. Probabilistic interference models seek closed-form characterizations through moment-based approximations, yet these expressions remain tractable only for restricted parameter choices and become unwieldy when interference moments lack closed-form representations. This work introduces a hybrid approximation framework that addresses these challenges by combining Monte Carlo sampling of a small set of dominant interferers with a Laplace functional representation of the residual far-field interference. The resulting dominant-plus-tail structure provides a modular, numerically stable, and path-loss-agnostic estimator suitable for both noise-limited and interference-limited regimes. We further derive theoretical error bounds that decrease with the number of dominant interferers and validate the approach against established stochastic geometry and probabilistic modeling benchmarks.

</details>


### [32] [Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding](https://arxiv.org/abs/2511.19639)
*Niccolò Brembilla,Yinbin Ma,Pietro Belotti,Federico Malucelli,Daniela Tuninetti*

Main category: cs.IT

TL;DR: 提出了一种高效的计算机辅助框架，用于表征线性编码约束下编码缓存系统的基本极限，通过非香农型不等式和问题特定约束获得更紧的逆界。


<details>
  <summary>Details</summary>
Motivation: 受Tian以及Cao和Xu先前工作的启发，旨在在编码缓存系统中，在线性编码约束下更精确地表征基本性能极限。

Method: 考虑适用于可表示多拟阵的非香农型不等式，利用编码缓存的对称结构和问题特定约束来降低线性规划的复杂度。

Result: 推导出的逆界比先前已知的解析方法更紧，并证明了在线性编码放置和传输约束下某些可实现内存-负载权衡点的最优性。

Conclusion: 结果表明，结合最小公共信息构造的小型结构化需求子集可能足以表征线性编码下的最优权衡关系。

Abstract: Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding.

</details>


### [33] [Joint Satellite Power Consumption and Handover Optimization for LEO Constellations](https://arxiv.org/abs/2511.19745)
*Yassine Afif,Mohammed Almekhlafi,Antoine Lesage-Landry,Gunes Karabulut Kurt*

Main category: cs.IT

TL;DR: 该论文提出了一种LEO卫星通信系统中考虑切换频率的功率分配优化方法，通过联合优化传输功率、用户-卫星关联和切换惩罚，在保证用户吞吐量的同时控制切换频率。


<details>
  <summary>Details</summary>
Motivation: LEO卫星星座系统中，由于卫星的动态拓扑，用户需要频繁切换，这会带来额外的信令开销和功耗。随着星座规模增大，切换负担成为显著问题。

Method: 将功率分配问题建模为混合整数凹线性规划(MICP)，联合考虑总传输功率、用户-卫星关联和切换惩罚，使用现成求解器求解。

Result: 蒙特卡洛仿真表明，该方法能有效控制切换频率同时保持高用户吞吐量，用户速率提升约40%且切换频率不会不成比例增加。

Conclusion: 提出的切换感知优化策略在显著改善用户速率的同时，有效控制了切换频率，证明了该方法的有效性。

Abstract: In satellite constellation-based communication systems, continuous user coverage requires frequent handoffs due to the dynamic topology induced by the Low Earth Orbit (LEO) satellites. Each handoff between a satellite and ground users introduces additional signaling and power consumption, which can become a significant burden as the size of the constellation continues to increase. This work focuses on the optimization of the total transmission rate in a LEO-to-user system, by jointly considering the total transmitted power, user-satellite associations, and power consumption, the latter being handled through a penalty on handoff events. We consider a system where LEO satellites serve users located in remote areas with no terrestrial connectivity, and formulate the power allocation problem as a mixed-integer concave linear program (MICP) subject to power and association constraints. Our approach can be solved with off-the-shelf solvers and is benchmarked against a naive baseline where users associate to their closest visible satellite. Extensive Monte Carlo simulations demonstrate the effectiveness of the proposed method in controlling the handoff frequency while maintaining high user throughput. These performance gains highlight the effectiveness of our handover-aware optimization strategy, which ensures that user rates improve significantly, by about 40%, without incurring a disproportionate rise in the handoff frequency.

</details>


### [34] [Two-Step Decoding of Binary $2\times2$ Sum-Rank-Metric Codes](https://arxiv.org/abs/2511.19812)
*Hao Wu,Bocong Chen,Guanghui Zhang,Hongwei Liu*

Main category: cs.IT

TL;DR: 本文解决了Chen-Cheng-Qi提出的开放问题，证明二进制和秩度量码的解码可以完全简化为对组成汉明度量码的解码，无需额外的d1≥2/3dsr条件限制。


<details>
  <summary>Details</summary>
Motivation: 解决Chen-Cheng-Qi在2025年提出的开放问题：是否可以将2×2矩阵块的二进制和秩度量码的解码完全简化为对组成汉明度量码C1和C2的解码，而不需要d1≥2/3dsr的限制条件。

Method: 提出一个简单的两步解码过程：首先对C2进行唯一解码，然后对C1应用单错误/擦除解码。这种方法消除了对d1≥2/3dsr限制条件的理论需求。

Result: 证明了限制性假设d1≥2/3dsr在理论上是不必要的。所得解码器实现了对⌊(dsr-1)/2⌋的唯一解码，总体复杂度为T2+T1，其中T2和T1分别是C2和C1汉明解码器的复杂度。

Conclusion: 这种简化在黑盒模型中是渐进最优的，因为任何和秩解码器都必须固有地解码组成汉明码。对于F4上的BCH或Goppa实例化，解码器在O(ℓ²)时间内运行。

Abstract: We resolve an open problem posed by Chen--Cheng--Qi (IEEE Trans.\ Inf.\ Theory, 2025): can decoding of binary sum-rank-metric codes $\SR(C_1,C_2)$ with $2\times2$ matrix blocks be reduced entirely to decoding the constituent Hamming-metric codes $C_1$ and $C_2$ without the additional requirement $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ that underlies their fast decoder? We answer this in the affirmative by exhibiting a simple two-step procedure: first uniquely decode $C_2$, then apply a single error/erasure decoding of $C_1$.This shows that the restrictive hypothesis $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ is theoretically unnecessary.The resulting decoder achieves unique decoding up to $\lfloor (d_{\mathrm{sr}}-1)/2\rfloor$ with overall cost $T_2+T_1$, where $T_2$ and $T_1$ are the complexities of the Hamming decoders for $C_2$ and $C_1$, respectively. We further show that this reduction is asymptotically optimal in a black-box model, as any sum-rank decoder must inherently decode the constituent Hamming codes.For BCH or Goppa instantiations over $\F_4$, the decoder runs in $O(\ell^2)$ time.

</details>


### [35] [Towards Edge General Intelligence: Knowledge Distillation for Mobile Agentic AI](https://arxiv.org/abs/2511.19947)
*Yuxuan Wu,Linghan Ma,Ruichen Zhang,Yinqiu Liu,Dusit Niyato,Shunpu Tang,Zehui Xiong,Zhu Han,Zhaohui Yang,Kaibin Huang,Zhaoyang Zhang,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本调查探讨了知识蒸馏在边缘通用智能中的应用，重点研究针对无线通信和移动网络的KD技术，以及适合边缘部署的新型架构，旨在为移动代理AI提供高效、可扩展的智能解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决在移动和边缘设备上部署先进代理AI模型时面临的资源限制挑战，包括有限的计算能力、能源和存储资源。

Method: 调查集成知识蒸馏到EGI中，重点关注无线通信特定的KD技术（如信道感知自蒸馏、跨模型CSI反馈蒸馏）和适合边缘部署的新型架构（如Mamba、RWKV）。

Result: 知识蒸馏被定位为在无线边缘实现高效、通信感知和可扩展智能的关键推动因素，能够增强泛化能力并支持多种应用场景。

Conclusion: 知识蒸馏在边缘通用智能中具有重要作用，但仍面临关键挑战，需要进一步研究以推动移动代理AI在EGI时代的发展。

Abstract: Edge General Intelligence (EGI) represents a paradigm shift in mobile edge computing, where intelligent agents operate autonomously in dynamic, resource-constrained environments. However, the deployment of advanced agentic AI models on mobile and edge devices faces significant challenges due to limited computation, energy, and storage resources. To address these constraints, this survey investigates the integration of Knowledge Distillation (KD) into EGI, positioning KD as a key enabler for efficient, communication-aware, and scalable intelligence at the wireless edge. In particular, we emphasize KD techniques specifically designed for wireless communication and mobile networking, such as channel-aware self-distillation, cross-model Channel State Information (CSI) feedback distillation, and robust modulation/classification distillation. Furthermore, we review novel architectures natively suited for KD and edge deployment, such as Mamba, RWKV (Receptance, Weight, Key, Value) and Cross-Architecture distillation, which enhance generalization capabilities. Subsequently, we examine diverse applications in which KD-driven architectures enable EGI across vision, speech, and multimodal tasks. Finally, we highlight the key challenges and future directions for KD in EGI. This survey aims to provide a comprehensive reference for researchers exploring KD-driven frameworks for mobile agentic AI in the era of EGI.

</details>


### [36] [Explainable Deep Learning for Secrecy Energy-Efficiency Maximization in Ambient Backscatter Multi-User NOMA Systems](https://arxiv.org/abs/2511.20108)
*Miled Alam,Abdul Karim Gizzini,Laurent Clavier*

Main category: cs.IT

TL;DR: 本文研究了多用户下行NOMA系统中，在存在被动窃听者的情况下，通过多个环境反向散射通信(AmBC)辅助的保密能效(SEE)优化问题。


<details>
  <summary>Details</summary>
Motivation: 研究AmBC辅助的NOMA系统在存在窃听者时的保密能效问题，探索保密和率和总功耗之间的权衡关系，为下一代物联网应用提供安全高效的通信解决方案。

Method: 对于两个反向散射设备(BD)的特殊情况，推导了最优反射系数和功率分配的闭式解；对于多于两个BD的情况，提出了网格搜索基准方法和可扩展的粒子群优化算法，并设计了基于前馈神经网络的深度学习预测器。

Result: 数值结果表明，AmBC的引入显著提高了SEE，在高噪声环境下相比传统NOMA系统增益高达615%。FNN模型相比最优基准实现了95%以上的准确率，同时降低了复杂度。SHAP分析显示最具影响力的特征对应主导复合信道分量。

Conclusion: 研究表明AmBC-NOMA系统在保密能效方面具有显著优势，结合可解释人工智能技术可以为下一代物联网应用构建可信赖的节能安全通信系统。

Abstract: In this paper, we investigate the secrecy energy-efficiency (SEE) of a multi-user downlink non-orthogonal multiple access (NOMA) system assisted by multiple ambient backscatter communications (AmBC) in the presence of a passive eavesdropper. We analyze both the trade-off and the ratio between the achievable secrecy sum-rate and total power consumption. In the special case of two backscatter devices (BDs), we derive closed-form solutions for the optimal reflection coefficients and power allocation by exploiting the structure of the SEE objective and the Pareto boundary of the feasible set. When more than two BDs are present, the problem becomes analytically intractable. To address this, we propose two efficient optimization techniques: (i) an exhaustive grid-based benchmark method, and (ii) a scalable particle swarm optimization algorithm. Furthermore, we design a deep learning-based predictor using a feedforward neural network (FNN), which closely approximates the optimal solutions. Numerical results show that the inclusion of AmBC significantly improves SEE, with gains up to 615% compared to conventional NOMA in high-noise regimes. Additionally, the FNN model achieves more than 95% accuracy compared to the optimal baseline, while reducing complexity. Finally, we employ SHAP (SHapley Additive exPlanations) to interpret the learned model, revealing that the most influential features correspond to the dominant composite channel components, in accordance with the theoretical system model. This demonstrates the potential of explainable artificial intelligence to build trust in energy-efficient and secure AmBC-NOMA systems for next-generation internet of things applications.

</details>


### [37] [On hierarchical secure aggregation against relay and user collusion](https://arxiv.org/abs/2511.20117)
*Min Xu,Xuejiao Han,Kai Wan,Gennian Ge*

Main category: cs.IT

TL;DR: 本文研究了同构网络中的分层安全聚合问题，针对中继和用户共谋场景，建立了通信负载的基本界限，并设计了通信最优方案。


<details>
  <summary>Details</summary>
Motivation: 安全聚合是联邦学习中保护隐私的关键技术，但现有研究缺乏对中继和用户共谋攻击的系统分析。本文旨在解决同构网络中分层安全聚合的安全性和通信效率问题。

Method: 采用两阶段通信框架：用户向中继发送掩码数据，中继处理并转发编译消息给服务器进行精确求和恢复。利用网络函数计算的方法构建通信最优方案，并分析所需密钥大小的下界。

Result: 建立了用户-中继链路和中继-服务器链路的通信负载基本界限，推导了共谋弹性的可达阈值。在循环网络中证明了密钥大小下界的可达性。

Conclusion: 通过建立分层安全聚合与网络函数计算之间的联系，推动了安全聚合在通信效率和信息论安全性方面的理论界限发展。

Abstract: Secure aggregation (SA) is fundamental to privacy preservation in federated learning (FL), enabling model aggregation while preventing disclosure of individual user updates. This paper addresses hierarchical secure aggregation (HSA) against relay and user collusion in homogeneous networks, where each user connects to $n$ relays and each relay serves $m$ users. In the two-phase communication framework, users transmit masked data to relays, which then process and forward compiled messages to the server for exact sum recovery. The primary objective is to devise a transmission scheme such that the server can finish the aggregation task, while any group of $T_h$ colluding relays and $T_u$ colluding users cannot reveal any information about the data owned by the non-colluding users. In this study, we establish fundamental limits on the communication load, defined as the ratio of transmitted information size to original data size, for each user-relay link and each relay-server link. Achievable thresholds for collusion resilience are also derived. When the number of colluding relays and users falls below certain critical thresholds, we construct communication-optimal schemes using methods from network function computation. A limitation of these schemes is their reliance on large random keys. To address this, we derive a lower bound on the required key size and prove its achievability in cyclic networks, where users are connected to relays in a cyclic wrap-around manner. By establishing a connection between HSA and network function computation, this work advances the theoretical limits of communication efficiency and information-theoretic security in secure aggregation.

</details>


### [38] [General Multi-User Distributed Computing](https://arxiv.org/abs/2511.20127)
*Ali Khalesi*

Main category: cs.IT

TL;DR: 提出了一个统一的学习和信息论框架GMUDC，用于多用户和服务器间的分布式计算和推理，优化计算、通信和精度的联合权衡。


<details>
  <summary>Details</summary>
Motivation: 为分布式和联邦学习系统提供信息能量基础，解决航空、卫星和边缘智能网络中能效和数据效率的关键需求。

Method: 引入淬火设计和退火设计两种分析框架，前者考虑固定的子函数分配和网络拓扑，后者捕捉随机分配和链接的平均性能。

Result: 建立了计算负载、通信负载和重构失真之间的基本权衡关系，揭示了谱覆盖对偶性，将泛化能力与网络拓扑和资源分配联系起来。

Conclusion: 该框架为可扩展和资源最优的分布式系统提供了理论基础，特别适用于能效要求高的网络应用场景。

Abstract: This work develops a unified {learning- and information-theoretic} framework for distributed computation and inference across multiple users and servers. The proposed \emph{General Multi-User Distributed Computing (GMUDC)} model characterizes how computation, communication, and accuracy can be jointly optimized when users demand heterogeneous target functions that are arbitrary transformations of shared real-valued subfunctions. Without any separability assumption, and requiring only that each target function lies in a reproducing-kernel Hilbert space associated with a shift-invariant kernel, the framework remains valid for arbitrary connectivity and task-assignment topologies. A dual analysis is introduced: the \emph{quenched design} considers fixed assignments of subfunctions and network topology, while the \emph{annealed design} captures the averaged performance when assignments and links are drawn uniformly at random from a given ensemble. These formulations reveal the fundamental limits governing the trade-offs among computing load, communication load, and reconstruction distortion under computational and communication budgets~$Γ$ and~$Δ$. The analysis establishes a spectral-coverage duality linking generalization capability with network topology and resource allocation, leading to provably efficient and topology-aware distributed designs. The resulting principles provide an \emph{information-energy foundation} for scalable and resource-optimal distributed and federated learning systems, with direct applications to aeronautical, satellite, and edge-intelligent networks where energy and data efficiency are critical.

</details>


### [39] [CSI Prediction Frameworks for Enhanced 5G Link Adaptation: Performance-Complexity Trade-offs](https://arxiv.org/abs/2511.20160)
*Francisco Díaz-Ruiz,Francisco J. Martín-Vega,Jose A. Cortés,Gerardo Gómez,Mari Carmen Aguayo*

Main category: cs.IT

TL;DR: 该论文提出并评估了两种适用于TDD和FDD系统的CSI预测框架，在有效SINR域操作以降低复杂度。通过比较经典维纳滤波器和基于GRU、LSTM、DNN的深度学习框架，发现维纳滤波器在计算复杂度更低的情况下性能接近GRU，而GRU在不同信道场景下具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 准确及时的CSI对于高效链路自适应至关重要，但信道老化、用户移动性和反馈延迟等挑战显著影响自适应调制编码的性能。

Method: 提出两种CSI预测框架：经典维纳滤波器和基于GRU、LSTM、DNN的深度学习框架，在有效SINR域操作以降低复杂度。

Result: 仿真结果显示，在可获得信道二阶统计信息的情况下，维纳滤波器在MSE和吞吐量方面性能接近GRU，且计算复杂度更低。但GRU模型在不同信道场景下表现出更好的泛化能力。

Conclusion: 基于学习的解决方案更适合TDD系统（基站处理计算），而经典方法的低复杂度使其成为FDD设置的优选（在功率受限的用户设备上进行预测）。

Abstract: Accurate and timely channel state information (CSI) is fundamental for efficient link adaptation. However, challenges such as channel aging, user mobility, and feedback delays significantly impact the performance of adaptive modulation and coding (AMC). This paper proposes and evaluates two CSI prediction frameworks applicable to both time division duplexing (TDD) and frequency division duplexing (FDD) systems. The proposed methods operate in the effective signal to interference plus noise ratio (SINR) domain to reduce complexity while preserving predictive accuracy. A comparative analysis is conducted between a classical Wiener filter and state-of-the-art deep learning frameworks based on gated recurrent units (GRUs), long short-term memory (LSTM) networks, and a delayed deep neural network (DNN). The evaluation considers the accuracy of the prediction in terms of mean squared error (MSE), the performance of the system, and the complexity of the implementation regarding floating point operations (FLOPs). Furthermore, we investigate the generalizability of both approaches under various propagation conditions. The simulation results show that the Wiener filter performs close to GRU in terms of MSE and throughput with lower computational complexity, provided that the second-order statistics of the channel are available. However, the GRU model exhibits enhanced generalization across different channel scenarios. These findings suggest that while learningbased solutions are well-suited for TDD systems where the base station (BS) handles the computation, the lower complexity of classical methods makes them a preferable choice for FDD setups, where prediction occurs at the power-constrained user equipment (UE).

</details>


### [40] [Unified Block Signal Processing Framework for LPWANs: Sequence Index Modulation Spreading](https://arxiv.org/abs/2511.20364)
*Wenkun Wen,Tierui Min,Long Yuan,Minghua Xia*

Main category: cs.IT

TL;DR: 提出了一种用于LPWAN的广义块信号传输统一框架，采用准正交码字实现可靠的多用户分离，支持异步接入和块同步，为下一代LPWAN提供可扩展高效的物理层设计。


<details>
  <summary>Details</summary>
Motivation: 传统符号级方法在LPWAN中存在局限性，需要高接收灵敏度和高效物理层信号处理，因此需要开发统一的块信号传输框架。

Method: 框架包含三个核心组件：信号块向量、块内结构生成器和信号基矩阵，利用循环移位扩频序列形成准正交码字，建立块同步概念基础和基于块相关匹配的统一解调结构。

Result: 该框架实现了可靠的多用户分离（特别是在异步接入下），支持灵活系统实现，并在频移键控和啁啾扩频应用中验证了有效性。

Conclusion: 这项工作为下一代LPWAN的可扩展高效物理层设计奠定了基础，推动了块信号传输技术的发展。

Abstract: Low-power wide-area networks (LPWANs) demand high receiver sensitivity and efficient physical-layer signal processing. This paper introduces a unified framework for generalized block signal transmission in LPWANs, addressing the limitations of conventional symbol-by-symbol approaches. The framework comprises three key components: the signal block vector, the intra-block structure generator, and the signal basis matrix, and leverages quasi-orthogonal codewords formed through cyclically shifted spreading sequences. The resulting quasi-orthogonality enables reliable multi-user separation, particularly under asynchronous access. The framework establishes a conceptual foundation for block synchronization and provides a unified demodulation structure based on block correlation matching. It further supports flexible and systematic implementation, as demonstrated through applications to frequency-shift keying and chirp spread spectrum. This work advances scalable and efficient physical-layer design for next-generation LPWANs.

</details>


### [41] [Dimension-counting bounds for equi-isoclinic subspaces](https://arxiv.org/abs/2511.20642)
*Joseph W. Iverson,Kaysie Rose O*

Main category: cs.IT

TL;DR: 本文在最优子空间填充和等倾子空间理论中做出了四个贡献：新的块相干性下界、偶维等倾子空间的精确计数、等倾子空间数量的新上界，以及在特定维度下该上界可达性的证明。


<details>
  <summary>Details</summary>
Motivation: 研究最优子空间填充和等倾子空间的理论问题，旨在改进相关参数的下界和上界，并精确计数特定情况下的等倾子空间数量。

Method: 主要采用维度计数的方法来证明各个结果，通过数学分析和理论推导来建立新的界限和计数公式。

Result: 获得了块相干性的新下界、偶维等倾子空间的精确计数、等倾子空间数量的新上界，并证明了在特定维度下该上界在复数域和某些实数情况下可达。

Conclusion: 维度计数方法在最优子空间填充和等倾子空间理论中具有重要作用，所获得的四个贡献为该领域提供了新的理论工具和结果。

Abstract: We make four contributions to the theory of optimal subspace packings and equi-isoclinic subspaces: (1) a new lower bound for block coherence, (2) an exact count of equi-isoclinic subspaces of even dimension $r$ in $\mathbb{R}^{2r+1}$ with parameter $α\neq \tfrac{1}{2}$, (3) a new upper bound for the number of $r$-dimensional equi-isoclinic subspaces in $\mathbb{R}^d$ or $\mathbb{C}^d$, and (4) a proof that when $d=2r$, a further refinement of this bound is attained for every $r$ in the complex case and every $r=2^k$ in the real case. For each of these contributions, the proof ultimately relies on a dimension count.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [42] [It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models](https://arxiv.org/abs/2511.19877)
*Xiangyu Zhao,Yaling Shen,Yiwen Jiang,Zimu Wang,Jiahe Liu,Maxmartwell H Cheng,Guilherme C Oliveira,Robert Desimone,Dominic Dwyer,Zongyuan Ge*

Main category: cs.MM

TL;DR: 提出了一种用于抑郁症检测的多模态大语言模型框架，通过时间戳级别的视听特征对齐来增强音频语言模型的视觉理解能力，在DAIC-WoZ数据集上表现优于单模态和现有多模态方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球最普遍的心理健康障碍之一。传统LLM仅处理文本，无法处理心理健康评估中至关重要的音频和视觉非语言线索。虽然多模态LLM前景广阔，但鲜有针对心理学应用定制。

Method: 提出多模态LLM框架，增强音频语言模型的视觉理解能力，在时间戳级别对齐视听特征。这种细粒度对齐改进了跨模态的时间动态建模，同时减少了对大量训练数据和计算资源的需求。

Result: 在DAIC-WoZ数据集上的实验表明，该模型优于单模态方法和先前的多模态方法。

Conclusion: 所提出的框架可以扩展到整合额外的生理信号，为超越心理健康领域的更广泛临床应用铺平道路。

Abstract: Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.

</details>


### [43] [FINE: Factorized multimodal sentiment analysis via mutual INformation Estimation](https://arxiv.org/abs/2511.20167)
*Yadong Liu,Shangfei Wang*

Main category: cs.MM

TL;DR: 提出了一种分解式多模态融合框架，通过将各模态分解为共享和独特表示，并抑制任务无关噪声，来提升多模态情感分析的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临模态异质性挑战，包括异步信号、模态间信息不平衡以及任务无关噪声干扰，这些因素阻碍了学习鲁棒准确的情感表示。

Method: 采用基于互信息的优化策略进行表示分解，引入Q-Former混合模块提取细粒度情感特征，以及动态对比队列进行对比学习以捕获长程判别模式。

Result: 在多个公共数据集上的广泛实验表明，该方法持续优于现有方法，验证了所提框架的有效性和鲁棒性。

Conclusion: 提出的分解式多模态融合框架通过精细的表示分解和噪声抑制，有效提升了多模态情感分析的性能，证明了该方法的优越性。

Abstract: Multimodal sentiment analysis remains a challenging task due to the inherent heterogeneity across modalities. Such heterogeneity often manifests as asynchronous signals, imbalanced information between modalities, and interference from task-irrelevant noise, hindering the learning of robust and accurate sentiment representations. To address these issues, we propose a factorized multimodal fusion framework that first disentangles each modality into shared and unique representations, and then suppresses task-irrelevant noise within both to retain only sentiment-critical representations. This fine-grained decomposition improves representation quality by reducing redundancy, prompting cross-modal complementarity, and isolating task-relevant sentiment cues. Rather than manipulating the feature space directly, we adopt a mutual information-based optimization strategy to guide the factorization process in a more stable and principled manner. To further support feature extraction and long-term temporal modeling, we introduce two auxiliary modules: a Mixture of Q-Formers, placed before factorization, which precedes the factorization and uses learnable queries to extract fine-grained affective features from multiple modalities, and a Dynamic Contrastive Queue, placed after factorization, which stores latest high-level representations for contrastive learning, enabling the model to capture long-range discriminative patterns and improve class-level separability. Extensive experiments on multiple public datasets demonstrate that our method consistently outperforms existing approaches, validating the effectiveness and robustness of the proposed framework.

</details>
