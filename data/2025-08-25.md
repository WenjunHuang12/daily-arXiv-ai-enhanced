<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 7]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Constructing Long Paths in Graph Streams](https://arxiv.org/abs/2508.16022)
*Christian Konrad,Chhaya Trehan*

Main category: cs.DS

TL;DR: 该论文研究了图流模型中最长路径问题，针对无向图和有向图分别提出了算法和空间下界分析。在无向图中实现了半流式算法，而在有向图中证明了高空间复杂度的必要性。


<details>
  <summary>Details</summary>
Motivation: 图流计算模型中，构建长路径通常需要多次遍历，这引发了一个基本问题：是否单次遍历就足以构建非平凡长度的路径？本文旨在研究单次流式模型中的最长路径问题。

Method: 研究单次流式模型中的最长路径近似问题，针对无向图和有向图分别设计算法并分析空间复杂度下界。使用概率方法和空间复杂度分析技术。

Result: 对于无向图，在插入-only和插入-删除模型中，实现了半流式算法，能以高概率计算长度至少为平均度数1/3的路径。对于有向图，证明在插入-only模型中计算(n^{1-o(1)})近似需要Ω(n^2)空间。

Conclusion: 无向图可以在半流式空间内获得常数因子近似，而有向图需要二次空间复杂度。该研究揭示了图流模型中路径构建问题的内在复杂性差异。

Abstract: In the graph stream model of computation, an algorithm processes the edges of
an input graph in one or more sequential passes while using a memory sublinear
in the input size. This model poses significant challenges for constructing
long paths. Many known algorithms tasked with extending an existing path as a
subroutine require an entire pass to add a single additional edge. This raises
a fundamental question: Are multiple passes inherently necessary to construct
paths of non-trivial lengths, or can a single pass suffice? To address this
question, we study the Longest Path problem in the one-pass streaming model. In
this problem, given a desired approximation factor $\alpha$, the objective is
to compute a path of length at least $\lp(G) / \alpha$, where $\lp(G)$ is the
length of a longest path in the input graph. We give algorithms as well as
space lower bounds for both undirected and directed graphs. Our results
include: We show that for undirected graphs, in both the insertion-only and the
insertion-deletion models, there are semi-streaming algorithms, that compute a
path of length at least $d /3$ with high probability, where $d$ is the average
degree of the graph. These algorithms can also yield an $\alpha$-approximation
to Longest Path using space $\tilde{O}(n^2 / \alpha)$. Next, we show that such
a result cannot be achieved for directed graphs, even in the insertion-only
model. We show that computing a $(n^{1 - o(1)})$-approximation to Longest Path
in directed graphs in the insertion-only model requires space $\Omega(n^2)$. We
further show two additional lower bounds. First, we show that semi-streaming
space is insufficient for small constant factor approximations to Longest Path
for undirected graphs in the insertion-only model. Last, in undirected graphs
in the insertion-deletion model, we show that computing an
$\alpha$-approximation requires space $\Omega(n^2 / \alpha^3)$.

</details>


### [2] [PIPQ: Strict Insert-Optimized Concurrent Priority Queue](https://arxiv.org/abs/2508.16023)
*Olivia Grimes,Ahmed Hassan,Panagiota Fatourou,Roberto Palmieri*

Main category: cs.DS

TL;DR: PIPQ是一种严格线性化的并发优先级队列，其设计重点在于并行化插入操作而非传统上加速删除最小操作，通过两级结构实现高性能


<details>
  <summary>Details</summary>
Motivation: 现有并发优先级队列主要关注加速删除最小操作，但实际应用中插入操作往往更频繁，需要一种能够并行化插入操作的新设计

Method: 采用两级结构设计：工作级提供每线程数据结构支持快速并行插入，领导级包含最高优先级元素用于服务删除最小操作

Result: 在各种数据访问模式、操作混合、运行时设置以及图应用集成测试中，PIPQ在多种情况下优于竞争对手，特别是在插入密集型工作负载中表现突出

Conclusion: PIPQ通过专注于并行化插入操作的设计理念，在并发优先级队列领域提供了新的解决方案，特别适合插入密集型应用场景

Abstract: This paper presents PIPQ, a strict and linearizable concurrent priority queue
whose design differs from existing solutions in literature because it focuses
on enabling parallelism of insert operations as opposed to accelerating
delete-min operations, as traditionally done. In a nutshell, PIPQ's structure
includes two levels: the worker level and the leader level. The worker level
provides per-thread data structures enabling fast and parallel insertions. The
leader level contains the highest priority elements in the priority queue and
can thus serve delete-min operations. Our evaluation, which includes an
exploration of different data access patterns, operation mixes, runtime
settings, and an integration into a graph-based application, shows that PIPQ
outperforms competitors in a variety of cases, especially with insert-dominant
workloads.

</details>


### [3] [On the number of MUSs crossing a position](https://arxiv.org/abs/2508.16092)
*Hiroto Fujimaru,Takuya Mieno,Shunsuke Inenaga*

Main category: cs.DS

TL;DR: 本文研究了字符串中最小唯一子串(MUS)的数量分布，证明了在长度为n的字符串中，包含任意给定位置i的MUS数量具有Θ(√n)的上下界。


<details>
  <summary>Details</summary>
Motivation: 虽然已知字符串中MUS的总数最多为n，且可以在线性时间内计算所有MUS，但对于特定位置i包含的MUS数量尚未有精确分析。研究这一问题有助于深入理解MUS在字符串中的分布特性。

Method: 通过理论分析，建立了包含给定位置i的MUS数量的上界和下界证明。构造了特定的字符串实例来达到下界，同时证明了上界的紧致性。

Result: 证明了对于任意长度为n的字符串T和任意位置i，包含i的MUS数量|MUS(T,i)|的上界和下界都是Θ(√n)，即存在常数c1,c2>0使得c1√n ≤ |MUS(T,i)| ≤ c2√n。

Conclusion: 该研究首次给出了包含特定位置的MUS数量的精确渐近界限，揭示了MUS在字符串中的分布具有相对集中的特性，这一结果为字符串算法和理论计算机科学提供了新的洞见。

Abstract: A string $w$ is said to be a minimal unique substring (MUS) of a string $T$
if $w$ occurs exactly once in $T$, and any proper substring of $w$ occurs at
least twice in $T$. It is known that the number of MUSs in a string $T$ of
length $n$ is at most $n$, and that the set $MUS(T)$ of all MUSs in $T$ can be
computed in $O(n)$ time [Ilie and Smyth, 2011]. Let $MUS(T,i)$ denote the set
of MUSs that contain a position $i$ in a string $T$. In this short paper, we
present matching $\Theta(\sqrt{n})$ upper and lower bounds for the number
$|MUS(T,i)|$ of MUSs containing a position $i$ in a string $T$ of length $n$.

</details>


### [4] [Symmetry-breaking symmetry in directed spectral partitioning](https://arxiv.org/abs/2508.16173)
*Dimosthenis Pasadakis,Raphael S. Steiner,Pál András Papp,Toni Böhnlein,Albert-Jan N. Yzelman*

Main category: cs.DS

TL;DR: 提出了一种打破经典谱二分对称性的新方法，通过激励有向切割边的对齐来生成无环二分和拓扑排序，在重用距离和最小线性排列方面比现有Gorder算法提升高达17倍


<details>
  <summary>Details</summary>
Motivation: 经典谱二分方法存在对称性问题，无法有效处理有向图的边对齐和局部性优化，需要新的方法来生成具有优异局部性的无环二分和拓扑排序

Method: 通过打破经典谱二分的对称性，激励有向切割边的对齐，从而生成无环二分划分，并进一步为有向无环图产生拓扑排序

Result: 新方法在总重用距离和最小线性排列方面显著优于现有Gorder算法，性能提升高达17倍

Conclusion: 该方法通过打破对称性和激励边对齐，能够有效提升有向无环图的局部性性能，为图处理算法提供了新的优化思路

Abstract: We break the symmetry in classical spectral bi-partitioning in order to
incentivise the alignment of directed cut edges. We use this to generate
acyclic bi-partitions and furthermore topological orders of directed acyclic
graphs with superb locality. The new approach outperforms the state-of-the-art
Gorder algorithm by up to $17\times$ on total reuse distance and minimum linear
arrangement.

</details>


### [5] [Linear Layouts Revisited: Stacks, Queues, and Exact Algorithms](https://arxiv.org/abs/2508.16319)
*Thomas Depian,Simon D. Fink,Robert Ganian,Vaishali Surianarayanan*

Main category: cs.DS

TL;DR: 本文提出了三种新的堆栈和队列布局算法，分别针对不同参数复杂度，显著扩展了对这些问题的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管堆栈和队列布局已被广泛研究，但在计算复杂性方面仍存在许多基本问题未解决。本文旨在解决文献中的开放性问题，提供更高效的算法。

Method: 1) 基于顶点完整性的固定参数算法，使用新开发的Ramsey剪枝技术；2) 避免双指数依赖的n^(O(q*l))算法；3) 改进的2^(O(n))单页队列布局算法。

Result: 成功开发了三种新算法：第一个算法推广了基于顶点覆盖数的先前算法；第二个算法首次避免了参数的双指数依赖；第三个算法将时间复杂度从n^(O(n))改进到2^(O(n))。

Conclusion: 这些算法显著扩展了我们对堆栈和队列布局计算复杂性的理解，为相关领域提供了新的技术工具和理论突破。

Abstract: In spite of the extensive study of stack and queue layouts, many fundamental
questions remain open concerning the complexity-theoretic frontiers for
computing stack and queue layouts. A stack (resp. queue) layout places vertices
along a line and assigns edges to pages so that no two edges on the same page
are crossing (resp. nested). We provide three new algorithms which together
substantially expand our understanding of these problems:
  (1) A fixed-parameter algorithm for computing minimum-page stack and queue
layouts w.r.t. the vertex integrity of an n-vertex graph G. This result is
motivated by an open question in the literature and generalizes the previous
algorithms parameterizing by the vertex cover number of G. The proof relies on
a newly developed Ramsey pruning technique. Vertex integrity intuitively
measures the vertex deletion distance to a subgraph with only small connected
components.
  (2) An n^(O(q * l)) algorithm for computing l-page stack and queue layouts of
page width at most q. This is the first algorithm avoiding a double-exponential
dependency on the parameters. The page width of a layout measures the maximum
number of edges one needs to cross on any page to reach the outer face.
  (3) A 2^(O(n)) algorithm for computing 1-page queue layouts. This improves
upon the previously fastest n^(O(n)) algorithm and can be seen as a counterpart
to the recent subexponential algorithm for computing 2-page stack layouts
[ICALP'24], but relies on an entirely different technique.

</details>


### [6] [Going Beyond Twin-width? CSPs with Unbounded Domain and Few Variables](https://arxiv.org/abs/2508.16389)
*Peter Jonsson,Victor Lagerkvist,Jorke M. de Vlas,Magnus Wahlström*

Main category: cs.DS

TL;DR: 本文研究无界域约束满足问题(udCSP)的模型，针对变量少但域大小无界的情况。通过代数理论和Galois连接分析三种映射类型(无限制、one-hot、单调)的复杂度，发现单调映射的复杂度取决于有序多态性，特别是"连接器"多态性作为FPT边界。


<details>
  <summary>Details</summary>
Motivation: 研究udCSP模型是为了解决参数化算法中经常出现的变量少但域无界的问题，这类问题在MinCSP和Directed Multicut等FPT算法中都有体现，需要建立系统的理论框架来分析其计算复杂度。

Method: 使用代数方法，通过Galois连接和部分多函数理论分析udCSP的复杂度。研究三种映射类型：无限制映射、one-hot映射和单调映射，重点关注单调映射中的有序多态性概念。

Result: 无限制映射在所有非平凡情况下都是W[1]-难的；one-hot映射与Marx的布尔加权CSP的FPT二分法一致；单调映射的复杂度取决于"连接器"多态性，其缺失导致W[1]-难，存在则对二元语言意味着有界twin-width和FPT。

Conclusion: 建立了udCSP的代数理论框架，确定了不同映射类型的复杂度特征，特别是发现了单调映射中"连接器"多态性作为FPT边界的关键作用，为非二元语言的FPT问题留下了开放问题。

Abstract: We study a model of constraint satisfaction problems geared towards instances
with few variables but with domain of unbounded size (udCSP). Our model is
inspired by recent work on FPT algorithms for MinCSP where frequently both
upper and lower bounds on the parameterized complexity of a problem correspond
to $k$-variable udCSPs; e.g., the FPT algorithms for Boolean MinCSP (Kim et
al., SODA 2023) and Directed Multicut with three cut requests (Hatzel et al.,
SODA 2023) both reduce to k-variable udCSPs, and the canonical W[1]-hardness
construction in the area, Paired Min Cut by Marx and Razgon (IPL 2009), is
effectively a k-variable udCSP.
  The udCSP framework represents constraints with unbounded domains via a
collection $\mathcal{M}$ of unary maps into a finite-domain base language
$\Gamma$. We develop an algebraic theory for studying the complexity of
udCSP$(\Gamma,\mathcal{M})$ with a Galois connection based on partial
multifunctions.
  We study three types of maps: unrestricted, one-hot, and monotone. For
unrestricted maps, the problem is W[1]-hard for all but trivial cases, and for
one-hot maps, the characterization coincides with Marx' FPT dichotomy for
Boolean Weighted CSPs (Computational Complexity 2005). For the case of monotone
maps Mo, we show that the complexity depends on restricted identifies we call
ordered polymorphisms; we identify the "connector" polymorphism as the likely
FPT boundary. We show that its absence implies that udCSP($\Gamma$,Mo) defines
all permutations, and the problem is W[1]-hard; while its presence for a binary
language implies bounded twin-width, and the problem is FPT (Twin-Width IV;
Bonnet et al., JACM 2024). For non-binary languages, where twin-width does not
apply, the polymorphism coincides with a notion of bounded projected grid-rank;
however, we leave the FPT question for this case open.

</details>


### [7] [Quality control in sublinear time: a case study via random graphs](https://arxiv.org/abs/2508.16531)
*Cassandra Marcussen,Ronitt Rubinfeld,Madhu Sudan*

Main category: cs.DS

TL;DR: 本文提出了"质量控制问题"新概念，旨在设计算法来区分高质量输入（来自特定分布）与对抗性低质量输入，相比单独测试分布或质量函数能获得更高效率。


<details>
  <summary>Details</summary>
Motivation: 传统算法设计关注平均性能，但实际应用中需要判断特定输入是否可信。质量控制问题旨在以更高效的方式验证输入质量，避免完全分布测试或质量函数测试的高成本。

Method: 研究子线性质量控制问题，特别针对随机图模型G_{n,p}和k-团计数函数ρ_k。通过理论分析证明质量控制问题相比单独测试分布或质量函数可以显著减少查询复杂度。

Result: 对于随机图G_{n,p}和k-团计数，质量控制问题仅需p^{-O(k)}次查询和时间，相比单独测试分布（不可能）或测试ρ_k≈1（需要p^{-Ω(k²)}次）效率显著提升。对于最大度为Δ(H)的模体H，查询复杂度为p^{-O(Δ(H))}。

Conclusion: 质量控制问题为算法可信性验证提供了新的理论框架，在随机图等场景下能实现超多项式效率提升，为处理大规模数据的高效质量验证开辟了新方向。

Abstract: Many algorithms are designed to work well on average over inputs. When
running such an algorithm on an arbitrary input, we must ask: Can we trust the
algorithm on this input? We identify a new class of algorithmic problems
addressing this, which we call "Quality Control Problems." These problems are
specified by a (positive, real-valued) "quality function" $\rho$ and a
distribution $D$ such that, with high probability, a sample drawn from $D$ is
"high quality," meaning its $\rho$-value is near $1$. The goal is to accept
inputs $x \sim D$ and reject potentially adversarially generated inputs $x$
with $\rho(x)$ far from $1$. The objective of quality control is thus weaker
than either component problem: testing for "$\rho(x) \approx 1$" or testing if
$x \sim D$, and offers the possibility of more efficient algorithms.
  In this work, we consider the sublinear version of the quality control
problem, where $D \in \Delta(\{0,1\}^N)$ and the goal is to solve the $(D
,\rho)$-quality problem with $o(N)$ queries and time. As a case study, we
consider random graphs, i.e., $D = G_{n,p}$ (and $N = \binom{n}2$), and the
$k$-clique count function $\rho_k := C_k(G)/\mathbb{E}_{G' \sim
G_{n,p}}[C_k(G')]$, where $C_k(G)$ is the number of $k$-cliques in $G$. Testing
if $G \sim G_{n,p}$ with one sample, let alone with sublinear query access to
the sample, is of course impossible. Testing if $\rho_k(G)\approx 1$ requires
$p^{-\Omega(k^2)}$ samples. In contrast, we show that the quality control
problem for $G_{n,p}$ (with $n \geq p^{-ck}$ for some constant $c$) with
respect to $\rho_k$ can be tested with $p^{-O(k)}$ queries and time, showing
quality control is provably superpolynomially more efficient in this setting.
More generally, for a motif $H$ of maximum degree $\Delta(H)$, the respective
quality control problem can be solved with $p^{-O(\Delta(H))}$ queries and
running time.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: Z-Pruner是一种无需重新训练的后训练剪枝方法，通过结合权重更新幅度和激活模式来有效识别和消除冗余参数，在多个LLM架构上超越现有剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但模型规模过大带来了部署、扩展和能效方面的挑战，现有剪枝方法要么导致性能显著下降，要么需要计算成本高昂的微调。

Method: Z-Pruner利用权重更新幅度和激活模式来识别冗余参数，是一种模型无关、高效且易于实现的后训练剪枝方法，无需任何重新训练。

Result: 在LLaMA-2、LLaMA-3和OPT等多种LLM架构上，Z-Pruner在标准语言基准测试中超越了需要密集权重更新的最先进剪枝方法，获得了最低的困惑度分数和最高的零样本准确率平均分。

Conclusion: Z-Pruner提供了一种有效的方法来减少大型语言模型的规模，同时保持性能，解决了部署和能效方面的挑战。

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [9] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net是一个新颖的多模态情感分析深度学习框架，通过渐进式门控融合、自适应门控仲裁和参数高效微调策略，在保持高性能的同时显著减少了可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态情感分析中深度、上下文依赖的融合问题，同时确保模型的高效性和可解释性，特别是在资源受限的场景下。

Method: 提出了渐进式层内融合范式（Cross-Attention机制）、自适应门控仲裁机制以及混合参数高效微调策略（LoRA+Post-Fusion Adapters）。

Result: 在MOSI数据集上取得了最先进的性能：MAE为0.691，F1-Score为86.9%，仅使用3.09M可训练参数。

Conclusion: PGF-Net通过创新的融合架构和参数效率策略，实现了高性能、高效率的多模态情感分析，在性能和计算效率之间取得了优越的平衡。

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [10] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 提出一种结合频谱分析和概率预测的简化ECG分割架构，替代复杂的BiLSTM模型，提高计算效率的同时保持高精度分割性能


<details>
  <summary>Details</summary>
Motivation: 现有ECG分割模型依赖复杂的多层架构如BiLSTM，计算强度大且效率低下，需要开发更高效的解决方案

Method: 采用频谱分析与概率预测结合的简化架构，用简单层替代复杂层来捕获P、QRS和T波的时空特征，并应用可解释AI(XAI)增强模型透明度

Result: 实现了高分割准确率：QRS波97.00%、T波93.33%、P波96.07%，在提高计算效率的同时提供精确分割

Conclusion: 简化架构不仅提高了计算效率，还提供了精确的心电信号分割，是心脏信号监测的实用有效解决方案

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [11] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: TPLA是一种针对张量并行环境的注意力机制优化方案，通过将潜在表示和输入维度分片到不同设备，在保持压缩KV缓存优势的同时实现高效张量并行计算。


<details>
  <summary>Details</summary>
Motivation: 在张量并行环境中，Multi-Head Latent Attention (MLA)需要每个设备加载完整缓存，丧失了压缩KV缓存的优势。需要一种既能保持压缩缓存效益又能适应张量并行的注意力机制。

Method: 提出Tensor-Parallel Latent Attention (TPLA)：将潜在表示和每个头的输入维度分片到多个设备，在各分片上独立进行注意力计算，然后通过all-reduce操作合并结果。使用正交变换（如Hadamard变换或PCA）来减轻分片间干扰。

Result: 在DeepSeek-V3和Kimi-K2模型上实现了1.79倍和1.93倍的加速，在32K token上下文长度下保持性能，在常识推理和LongBench基准测试上表现良好。

Conclusion: TPLA成功解决了张量并行环境下压缩KV缓存效率低下的问题，提供了一种无需重新训练的高效解决方案，可与FlashAttention-3集成实现端到端加速。

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [12] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 提出基于Transformer的时间序列因果发现框架，通过梯度分析提取因果结构和时滞，结合注意力掩码整合先验知识，显著提升因果发现性能


<details>
  <summary>Details</summary>
Motivation: 解决时间序列因果发现中的两个关键挑战：复杂非线性依赖关系和伪相关性

Method: 使用多层Transformer时间序列预测器捕捉长程非线性时序关系，通过梯度分析提取因果结构和时滞，引入基于注意力掩码的先验知识整合机制

Result: 在因果发现F1-score上提升12.8%，因果时滞估计准确率达到98.9%，显著优于现有最优方法

Conclusion: 该框架有效解决了复杂非线性时序因果发现问题，通过整合先验知识成功抑制了伪因果关系的干扰

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [13] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 这是一篇关于高维数据降维嵌入技术的综述性评论文章，评估了各种嵌入算法的效果，提出最佳实践建议，并讨论了该领域的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 高维数据集在各种学科和应用领域无处不在，直接处理高维数据面临挑战，需要降维嵌入技术来支持数据可视化、探索和分析。近年来众多嵌入算法的出现导致了研究领域的分散和混乱，实践者缺乏明确的指导。

Method: 通过详细的批判性综述，分析最新的研究进展，推导出创建和使用低维嵌入的最佳实践方法，并在多种数据集上评估普遍的嵌入方法。

Result: 文章提供了对各种嵌入算法的系统性评估和比较分析，为实践者提供了明确的方法选择指南，同时识别了不同方法在不同数据集上的表现特征。

Conclusion: 这篇综述性评论增强了高维数据降维嵌入领域的一致性，为未来研究提供了基础。文章持议了该领域仍面临的技术挑战和开放性问题，并展望了未来的研究方向，以促进这一重要技术领域的发展。

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [14] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 本文提出了一种将元启发式算法与图表示学习(GRL)相结合的GL-GRASP方法，用于解决约束增量图绘制问题(C-IGDP)，相比传统GRASP方法在解质量和可扩展性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习与元启发式结合的方法通常耗时且不如手工设计的启发式算法有竞争力，因此需要探索更轻量级的学习策略来提取图结构信息。

Method: 在GRASP的构建阶段引入图表示学习技术，开发了GL-GRASP方法，使用深度学习为基础的节点嵌入技术来指导搜索过程。

Result: GL-GRASP在原始积分度量上优于现有文献中的GRASP启发式方法，在更密集实例的可扩展性测试中也表现出更强的鲁棒性。

Conclusion: 图表示学习与元启发式算法的结合是一种有效的方法，能够提升约束增量图绘制问题的求解性能，特别是在解质量评估和时间效率方面都显示出优势。

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [15] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: 开发了一种基于机器学习和IIR滤波器的车载轮缘磨损监测系统，通过位移和温度传感器实时监测轮缘磨损，准确率达到98.2%，可集成到铁路物联网系统中提升安全性。


<details>
  <summary>Details</summary>
Motivation: 铁路轮轨相互作用对系统安全至关重要，需要精确的测量系统来监测轮缘磨损状况，以确保铁路运营安全。

Method: 使用位移和温度传感器采集数据，通过回归模型的机器学习算法动态训练，并设计无限脉冲响应滤波器(IIR)来减少车辆动力学和传感器噪声的影响。

Result: 机器学习算法有效抵消了传感器对温度效应的非线性响应，准确率达到96.5%；结合IIR滤波器实时降噪后，准确率提升至98.2%。

Conclusion: 该系统与铁路通信嵌入式系统（如物联网设备）集成，能够提供无与伦比的实时轮缘磨损和轨道不规则状况监测，确保铁路系统运营的安全性和效率。

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [16] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 提出一种基于自适应离散化和乐观消除的上下文多臂赌博机策略，用于处理奖励向量在偏好锥排序下的分布偏移问题，并引入基于偏好的遗憾概念来衡量性能。


<details>
  <summary>Details</summary>
Motivation: 研究在奖励向量存在分布偏移的情况下，如何设计自适应的上下文多臂赌博机学习策略，以应对现实世界中环境动态变化带来的挑战。

Method: 采用自适应离散化和乐观消除的方法构建策略，该策略能够自我调整以适应底层分布偏移，通过建立帕累托前沿之间的距离来定义新的遗憾度量标准。

Result: 在各种分布偏移假设下建立了策略的遗憾上界，这些界限推广了无分布偏移情况下的已知结果，并在存在分布偏移时能够优雅地随问题参数缩放。

Conclusion: 所提出的策略在分布偏移环境下表现良好，遗憾界限具有理论保证，为处理动态环境中的多目标强化学习问题提供了有效解决方案。

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [17] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 提出了一种新的平衡传播框架，通过引入中间误差信号解决深度网络中的梯度消失问题，首次将知识蒸馏和局部误差信号整合到EP中，在CIFAR数据集上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统的平衡传播方法在深层网络中面临梯度消失问题，导致能量最小化和梯度计算难以收敛，限制了其在深层架构中的应用。

Method: 提出新颖的EP框架，通过引入中间误差信号来增强信息流和神经元动力学的收敛性，首次将知识蒸馏和局部误差信号整合到平衡传播中。

Result: 在CIFAR-10和CIFAR-100数据集上实现了最先进的性能，展示了在深度VGG架构上的可扩展性。

Conclusion: 这项工作显著提升了平衡传播方法的可扩展性，为其在现实世界系统中的应用铺平了道路。

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [18] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文对量子联邦学习(QFL)进行了全面综述，涵盖了其概念、基础、应用和挑战，包括市场机会、架构、优化技术、安全机制以及多个领域的应用案例。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算和联邦学习的优势，实现隐私保护的分布式量子增强学习，解决分布式量子系统中高效安全模型训练的挑战。

Method: 采用系统性文献综述方法，从基础概念、架构拓扑、通信方案、优化技术、安全机制等多个维度分析QFL框架，并研究其在车辆网络、医疗健康、卫星网络等领域的应用。

Result: 提供了QFL的全面技术概览，包括分类体系、实现框架和平台分析，以及详细的案例研究，总结了该领域的关键见解和经验教训。

Conclusion: QFL是一个快速发展的前沿领域，虽然面临诸多技术挑战，但在隐私保护分布式量子学习方面具有巨大潜力，需要进一步研究解决当前问题并探索未来发展路径。

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [19] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: 该论文继续发展Cartan神经网络项目，重点研究非紧对称空间的数学基础，包括Tits Satake向量丛、分离器构造、双曲空间格林函数新表示，以及Bolza黎曼曲面拉普拉斯特征函数构造策略。


<details>
  <summary>Details</summary>
Motivation: 为Cartan神经网络的下一步发展建立数学基础，特别是引入基于非紧对称空间的网络层，并通过可解群同态进行映射，受卷积神经网络启发构建数学框架。

Method: 采用群论方法构造非紧对称空间的分离器，研究Δ₈,₃,₂铺砌群及其Fuchsian子群，开发双曲空间拉普拉斯格林函数和热核的新表示，提出基于Abel-Jacobi映射的黎曼曲面特征函数构造策略。

Result: 获得了所有非紧对称空间U/H的分离器群论构造，实现了g=3 Fermat四次曲线和g=2 Bolza曲面的均匀化，发现了双曲空间拉普拉斯格林函数和热核的新表示形式。

Conclusion: 建立了Cartan神经网络所需的数学基础框架，为后续基于非紧对称空间的神经网络层设计提供了理论支撑，并在多个数学领域取得了实质性进展。

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [20] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: 提出了PAC-MCoFL框架，使用多智能体强化学习和博弈论解决多服务提供商联邦学习中的非合作问题，通过帕累托最优策略优化资源分配和客户端分配


<details>
  <summary>Details</summary>
Motivation: 多服务提供商联邦学习生态系统存在非合作动态，隐私约束和竞争利益阻碍了通信和计算资源的集中优化

Method: 整合帕累托行动者-评论家原则与期望回归，设计三元笛卡尔分解机制处理高维动作空间，开发可扩展变体PAC-MCoFL-p降低计算复杂度

Result: 在总奖励和超体积指标上分别比最新MARL解决方案提升约5.8%和4.2%，能更有效平衡个体服务提供商和系统性能

Conclusion: 该框架为多服务提供商联邦学习提供了有效的分布式优化解决方案，具有理论收敛保证和实际性能优势

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [21] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 提出了非平稳线性判别分析(NSLDA)和非平稳二次判别分析(NSQDA)框架，通过状态空间模型处理时间分布漂移问题，相比传统方法有显著改进


<details>
  <summary>Details</summary>
Motivation: 传统判别分析假设训练数据同分布，但在实际应用中观测数据随时间收集，类条件分布会发生漂移，导致静态分类器不可靠

Method: 将判别分析嵌入状态空间模型，使用Kalman平滑处理每时间步多个样本，开发EM方法联合估计系统参数，以及GMM-Kalman方法同时恢复未观测时间标签和参数，对非线性漂移使用粒子平滑

Result: 大量模拟实验显示相比静态LDA、QDA和SVM基线有持续改进，对噪声、缺失数据和类别不平衡具有鲁棒性

Conclusion: 为时间分布漂移下的判别分析建立了统一且数据高效的基础框架

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [22] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 任务算术是一种有效的模型合并技术，本文为其提供了理论解释，证明任务向量与损失梯度等价，单轮微调即可获得良好合并效果


<details>
  <summary>Details</summary>
Motivation: 尽管任务算术在模型合并中表现出色，但缺乏对其工作原理的理论解释，需要建立严谨的理论基础

Method: 建立任务向量与损失梯度的数学联系，证明在标准梯度下降下单轮微调的任务向量等价于负梯度，并对多轮情况给出近似误差界

Result: 在7个视觉基准测试中验证理论，证明首轮梯度在范数和方向上主导微调轨迹，单轮微调模型合并效果与完全收敛模型相当

Conclusion: 任务算术可视为近似多任务学习，其有效性源于早期训练动态，为模型合并提供了理论依据和实践指导

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [23] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 提出基于对称相位紧急度函数的遗传编程方法，用于交通信号控制策略学习，通过共享子树表示转向运动紧急度，显著提升传统GP方法的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于遗传编程的交通信号控制方法无法一致处理不同交通信号相位的共同交通特征

Method: 使用对称相位紧急度函数计算相位紧急度，表示为两个共享子树的聚合，每个子树代表相位中转向运动的紧急度，并采用遗传编程方法进化该函数

Result: 在CityFlow交通模拟器上基于多个真实数据集验证，对称紧急度函数表示能显著提升学习到的交通信号控制策略性能

Conclusion: 该方法能够进化出有效、人类可理解且易于部署的交通信号控制策略

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [24] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 本文主张医疗领域机器学习模型必须具备可解释性、可共享性、可重现性和可问责性，提出了基于核方法、原型学习和深度核模型等内在可解释方法作为黑盒模型的替代方案，并通过生成式AI和联邦学习实现数据隐私保护下的跨机构协作。


<details>
  <summary>Details</summary>
Motivation: 医疗等高风险领域部署的机器学习模型需要获得医生信任和监管批准，但黑盒模型缺乏透明度难以满足这些要求，因此需要重新思考机器学习的基础设计原则。

Method: 采用内在可解释建模方法（稀疏核方法、原型学习、深度核模型），结合严格评估、公平性和不确定性量化，并利用生成式AI和联邦学习等协作学习范式实现数据隐私保护。

Result: 提出了一个完整的框架，使医疗AI不仅保持高准确性，同时具备透明度、可信度和临床可转化性，能够满足医疗领域的特殊需求。

Conclusion: 通过重新设计机器学习基础架构，整合可解释性、问责制和协作学习，可以开发出既准确又透明可信的医疗AI系统，真正适用于现实世界临床环境。

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [25] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: CommonKV是一种无需训练的方法，通过相邻参数共享实现跨层KV缓存压缩，使用SVD分解和自适应预算分配策略，在多种压缩比下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的KV缓存随序列长度增长而急剧增加，现有跨层KV缓存共享方法要么需要修改模型架构并重新预训练，要么在高压缩率下性能显著下降。

Method: 利用跨层隐藏状态高度相似性的观察，使用奇异值分解(SVD)实现相邻参数间的权重共享，生成更易合并的潜在KV缓存，并引入基于余弦相似度的自适应预算分配策略。

Result: 在多个骨干模型和基准测试中，该方法在各种压缩比下始终优于现有的低秩和跨层方法，且与量化和驱逐方法正交，最终可实现98%压缩率而无显著性能损失。

Conclusion: CommonKV提供了一种有效的训练无关的KV缓存压缩方案，通过参数共享和自适应策略解决了内存挑战，为大规模语言模型部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [26] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了机器学习在微出行系统中的数据集、技术方法和具体应用，填补了该领域文献不足的空白。


<details>
  <summary>Details</summary>
Motivation: 微出行系统已成为城市交通的重要组成部分，但缺乏针对机器学习在微出行中应用的专门文献，需要填补这一研究空白。

Method: 收集和分析各种微出行相关数据集，从空间、时间和特征维度进行讨论；详细概述机器学习模型在微出行中的应用，包括优势、挑战和具体用例。

Result: 系统梳理了机器学习在需求预测、能源管理和安全等微出行应用中的方法，为提高效率、准确性和用户体验提供了技术支撑。

Conclusion: 提出了未来研究方向，旨在帮助研究者更好地理解这一领域，推动微出行系统的优化发展。

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [27] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种无需微调LLM的新型自适应代理学习范式，通过基于记忆的在线强化学习实现低成本持续适应


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖静态手工反射工作流程而显得僵化，要么需要计算密集的梯度更新，需要一种更高效的自适应方法

Method: 将问题形式化为记忆增强马尔可夫决策过程（M-MDP），配备神经案例选择策略来指导行动决策，使用可微分或非参数化情景记忆存储过往经验，通过记忆重写机制基于环境反馈持续更新策略

Result: 在GAIA验证集上达到87.88% Pass@3，测试集79.40%；在DeepResearcher数据集上达到66.6% F1和80.4% PM，优于最先进的基于训练的方法，在分布外任务上案例记忆带来4.7%-9.6%的绝对提升

Conclusion: 该方法为开发无需梯度更新、能够持续实时学习的通用LLM代理提供了一条可扩展且高效的途径，推动了机器学习在开放式技能获取和深度研究场景中的发展

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [28] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: 本文发现ODE-based扩散采样中存在collapse errors现象，即采样数据在局部数据空间过度集中，并提出新指标量化该效应，揭示了低噪声区域分数学习对高噪声区域的负面影响。


<details>
  <summary>Details</summary>
Motivation: 尽管确定性采样器在扩散模型中广泛应用，但其潜在局限性尚未得到充分探索。本文旨在识别和分析ODE-based扩散采样中先前未被认识的collapse errors现象。

Method: 引入新指标量化collapse errors效应，通过实验观察see-saw效应（低噪声区域的分数学习对高噪声区域产生负面影响），并应用现有的采样、训练和架构技术来验证解释。

Result: 在各种设置下都观察到collapse errors现象，发现高噪声区域的拟合不足与确定性采样器的动态特性共同导致了collapse errors。

Conclusion: 这项工作为ODE-based扩散采样中的collapse errors提供了详实的实证证据，强调了分数学习与确定性采样之间相互作用这一被忽视但基础性方面需要进一步研究。

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [29] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN是一个基于GNN的时空克里金框架，通过解耦相位模块、动态数据驱动的元数据图建模和对抗迁移学习策略，有效处理时空数据缺失问题，提升模式有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在确保推断时空模式的有效性和泛化性方面存在困难，特别是在捕捉动态空间依赖性和时间偏移，以及优化未知传感器的泛化性方面。

Method: 提出STA-GANN框架，包含三个核心组件：(1)解耦相位模块感知和调整时间戳偏移；(2)动态数据驱动的元数据图建模利用时序数据和元数据更新空间关系；(3)对抗迁移学习策略确保泛化性。

Result: 在来自四个领域的九个数据集上进行广泛验证，理论和实验证据均表明STA-GANN具有优越性能。

Conclusion: STA-GANN通过创新的图神经网络架构和对抗学习策略，有效解决了时空克里金任务中的模式有效性和泛化性问题，在多个领域数据集上表现出色。

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [30] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: SPL-LNS是一种采样增强的神经大邻域搜索求解器，通过局部信息提案和事后重标注方法提升ILP求解性能，显著超越现有神经LNS求解器


<details>
  <summary>Details</summary>
Motivation: 解决传统贪婪神经LNS求解器容易陷入局部最优和样本效率低的问题

Method: 将LNS建模为随机过程，引入局部信息提案机制来逃离局部最优，开发事后重标注方法进行高效训练

Result: 在各种规模和类型的整数线性规划问题上，SPL-LNS显著超越了先前的神经LNS求解器

Conclusion: SPL-LNS通过采样增强和高效训练方法，有效解决了神经LNS求解器的局部最优和样本效率问题

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [31] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 该论文提出了一种基于MiniRocket特征提取和线性分类器的MI-EEG分类新方法，在PhysioNet数据集上达到98.63%的准确率，优于深度学习模型且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决MI-EEG信号分类面临的挑战，包括信号的非平稳性、时变性和个体差异性，以及多类别分类和个体间自然变异导致的低准确率问题。

Method: 提出两种方法：1）使用MiniRocket进行高效特征提取，然后用线性分类器进行分类；2）提出CNN-LSTM深度学习架构作为基线模型进行对比。

Result: 在PhysioNet数据集上，MiniRocket方法达到98.63%的平均准确率，CNN-LSTM方法达到98.06%的平均准确率，MiniRocket方法性能更优且计算成本更低。

Conclusion: 所提出的方法能显著提高运动想象EEG分类准确率，为MI-EEG特征提取和分类提供了新的见解，MiniRocket特征提取结合线性分类器是高效且有效的解决方案。

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [32] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM是一种参数规模感知的稀疏微调框架，通过梯度权重比和熵引导掩码，在仅更新0.1%参数的情况下，性能优于全参数微调1.6%


<details>
  <summary>Details</summary>
Motivation: 传统PEFT方法只最大化绝对更新规模，不考虑参数原始尺度，导致模型行为变化有限。需要一种相对参数尺度的更新方法来实现更有意义的下游适应

Method: 提出梯度权重比和熵引导掩码(GEM)框架：1)优先更新相对于预训练值比例显著的参数；2)基于参数值熵自适应确定每层需要调节的参数数量

Result: 在GLUE、SuperGLUE通用任务和GSM8k、MBPP领域特定任务上，仅更新0.1%参数就实现了比全参数微调高1.6%的准确率

Conclusion: GEM通过参数规模感知和分布敏感的稀疏微调，更有效地利用计算预算，在极低参数更新率下实现了优异的性能表现

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [33] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO是一种改进的降维技术，通过两阶段优化过程同时保留数据的局部和全局结构，解决了传统方法在保持局部或全局结构时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统降维技术要么专注于保持局部邻域结构，要么专注于保持全局结构，这可能导致对高维数据流形整体排列的错误结论。需要一种能够同时有效捕捉局部和全局结构的方法。

Method: UMATO将UMAP的优化过程分为两个阶段：第一阶段使用代表性点构建骨架布局，第二阶段在保持区域特征的同时投影剩余点。

Result: 定量实验验证UMATO在全局结构保持方面优于包括UMAP在内的广泛使用的降维技术，虽然在局部结构保持方面略有损失。同时表现出更好的可扩展性和对初始化和子采样的稳定性。

Conclusion: UMATO通过两阶段优化有效平衡了局部和全局结构保持，提高了高维数据可视化分析的可靠性，是进行可靠高维数据分析的更有效工具。

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [34] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: PIANO是一个改进的物理信息神经网络框架，通过自回归建模解决时间相关PDE问题，相比传统PINNs具有更好的稳定性和准确性


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)在求解时间相关偏微分方程时存在点预测问题，忽略了动力系统的自回归特性，导致不稳定和预测不准确

Method: 提出Physics-Informed Autoregressive Networks (PIANO)框架，采用自回归方式运行，显式地将未来预测条件化于过去状态，通过自监督展开机制训练并强制物理约束

Result: 理论分析证明PINNs存在时间不稳定性，而PIANO通过自回归建模实现稳定性。在挑战性时间相关PDE上达到最先进性能，显著提高准确性和稳定性，在天气预报任务中表现优异

Conclusion: PIANO通过引入自回归建模机制，有效解决了传统PINNs在时间相关PDE求解中的稳定性问题，为动力系统建模提供了更可靠的深度学习框架

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [35] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-José-Revuelta,Xinheng Wang,José Ramón Garmendia-Leiza,Carlos Alberola-López,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: 提出基于可解释人工智能(XAI)的咳嗽声谱分析框架，使用CNN和遮挡图识别诊断相关频段，通过频带分解发现COPD等呼吸疾病的特征性声谱模式。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的咳嗽声分析方法来区分慢性阻塞性肺疾病(COPD)与其他呼吸系统疾病，为呼吸疾病诊断提供基于声学特征的客观指标。

Method: 使用卷积神经网络(CNN)分析咳嗽信号的时频表示，采用遮挡图识别诊断相关区域，将频谱分解为5个频带进行针对性特征提取和分析。

Result: 发现不同疾病组在不同频带的声谱模式存在差异，能够区分COPD与其他呼吸疾病，以及慢性与非慢性患者群体，识别出可解释的声谱标志物。

Conclusion: 该方法揭示了咳嗽声学的病理生理特征，证明了频率分辨的XAI增强分析在生物医学信号解释和呼吸疾病诊断转化中的价值。

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [36] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: 在尼日利亚北部空气质量预测研究中，Prophet模型在季节性数据中表现与LSTM相当或更好，而LSTM在突变数据中更优，表明简单模型在资源受限环境下可能优于复杂深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决低资源地区空气质量数据不规则和稀缺的问题，系统比较先进机器学习模型在约束条件下的性能，为资源受限环境提供实用的预测方法选择指导。

Method: 使用2018-2023年19个州的月度观测数据，对比评估LSTM网络和Facebook Prophet模型在多种污染物（CO、SO2、SO4）预测中的表现。

Result: Prophet模型在季节性和长期趋势主导的时间序列中往往达到或超过LSTM的准确性，而LSTM在具有突然结构变化的数据集中表现更好。

Conclusion: 研究挑战了深度学习模型必然优于简单方法的假设，强调模型与数据对齐的重要性，支持在资源受限环境中采用上下文敏感、计算高效的预测方法而非盲目追求复杂性。

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [37] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: FEST是一个用于评估合成表格数据的系统框架，整合了隐私指标、相似性和机器学习效用指标，提供全面的隐私-效用权衡分析。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的合成数据评估框架，特别是在平衡隐私保护和数据效用方面存在空白。

Method: 开发FEST框架，集成多种隐私指标（攻击型和距离型）以及相似性和机器学习效用指标，构建开源Python库并在多个数据集上进行验证。

Result: FEST有效分析了不同合成数据生成模型的隐私-效用权衡，证明了其评估效果。

Conclusion: FEST填补了合成数据评估框架的空白，为系统评估合成数据的隐私保护和数据效用提供了有效工具。

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [38] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: 提出Chunked Data Shapley (C-DaSh)方法，通过分块处理和优化子集选择来高效计算数据Shapley值，大幅提升计算效率同时保持高质量结果


<details>
  <summary>Details</summary>
Motivation: 随着数据量和多样性增加，评估数据质量对机器学习分析至关重要。现有Data Shapley方法在大型数据集上面临计算复杂度高的问题，限制了实际应用

Method: 将数据集分块处理，使用优化子集选择和单次随机梯度下降来估计每个数据块的贡献值，显著减少计算时间

Result: 在多种真实分类和回归任务上，C-DaSh相比现有Shapley近似方法在计算效率上提升80-2300倍，在检测低质量数据区域方面更准确

Conclusion: 该方法实现了对大型表格数据集质量的实用化测量，支持分类和回归流程，解决了Data Shapley计算的可扩展性问题

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [39] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: 这篇论文对大语言模型的联邦学习调优进行了综述性评估，提出了基于模型访问权限和参数效率的分类法，将方法分为白盒、灰盒和黑盒技术，并讨论了黑盒推理API方向的研究趋势。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中调优大语言模型时的计算和通信挑战，应对实际场景中模型内部信息访问受限的问题，推动黑盒推理范式的发展。

Method: 提出了一个两维分类法：基于模型访问权限和基于参数效率的优化。将方法分为白盒（完全访问）、灰盒（部分访问）和黑盒（仅推理）三类，对每个类别中的代表性方法进行分析。

Result: 完成了对联邦学习中大语言模型调优方法的系统性综述，明确了不同模型访问方式下的技术分类，持续跟踪了黑盒推理API方向的最新研究进展。

Conclusion: 这份综述为联邦学习中大语言模型调优领域提供了结构化的分析框架，指出了未来研究的有前景方向和开放性挑战，尤其是在黑盒模式下的效率和效果优化问题。

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [40] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: 提出一种深度学习模型，通过学习稀疏二元表示来生成辅助知识概念(KCs)，这些表示捕捉了超越人工标注的潜在概念结构，可提升学生建模和自适应练习推荐的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型依赖人工标注的知识概念，但这些标注可能存在不完整、错误或过于笼统的问题，限制了推荐系统的准确性。

Method: 使用深度学习模型学习稀疏二元表示，每个比特表示潜在概念的存在与否，生成辅助KCs，这些表示可与经典模型和现代深度学习KT架构兼容。

Result: 实验表明，辅助KCs能提升学生建模的预测性能，增强基于强化学习的推荐策略和简单规划方法，在模拟学生环境中显著改善学习效果。

Conclusion: 辅助知识概念表示能有效补充人工标注的不足，为智能辅导系统提供更准确的学生知识状态建模和个性化推荐能力。

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [41] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: 提出了REFINE框架，通过结构化错误分析和针对性反馈来增强多模态大语言模型的推理能力，显著提升效率并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏结构化错误分析框架，特别是在多模态场景中整合视觉和文本输入增加了复杂性，需要系统化的错误学习和反馈机制。

Method: REFINE采用师生框架，通过三种结构化查询（Feed-Target、Feed-Check、Feed-Path）构建针对性反馈，优先处理相关视觉信息、诊断关键失败点并制定纠正措施。

Result: 实验结果显示显著的速度提升、计算成本降低以及良好的泛化能力，证明了REFINE在多模态推理中的有效性。

Conclusion: REFINE通过结构化错误反馈机制有效提升了多模态推理性能，为LLMs的错误学习和适应提供了可扩展的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [42] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: 提出一种整体性空间网络计算机物理感知框架，通过能力与意图相结合的多任务学习方法提升威胁评估的精确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决传统威胁评估中可靠性和安全性分离分析导致的系统特定标准过拟合问题，以及空间网络中新兴交互链路带来的复杂威胁场景挑战。

Method: 1. 信号特征提取算法进行直观威胁理解
2. 多任务学习架构：一份任务评估可靠性能力，另一份任务解码信号意图
3. 可适应威胁评估机制，满足不同安全和可靠性需求

Result: 框架在威胁检测和评估方面显著提升了系统稳健性，性能超过传统顺序方法，能够有效处理空间网络中的复杂威胁场景。

Conclusion: 该意图驱动的威胁模型框架通过整合能力与意图分析，为空间网络提供了更加精准和灵活的威胁评估能力，适用于具有新兴交互链路的现代空间网络环境。

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [43] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: OwkinZero模型通过强化学习在生物医学基准测试上超越大型商业LLM，展示了专业模型在生物推理任务中的优异表现和泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生物医学推理任务上存在明显短板，限制了其在转化医学和生物医学发现中的应用

Method: 创建8个包含30万问答对的生物医学基准数据集，通过基于可验证奖励的强化学习对开源LLM进行后训练

Result: 8-32B参数的OwkinZero模型在生物基准测试中显著优于更大的商业LLM，展现出跨任务的泛化能力

Conclusion: 针对性的强化学习和精心策划的数据可以解锁专业模型的泛化性能，加速AI驱动的生物发现

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [44] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM-VAE的无监督在线学习框架，用于检测供水管网中的管道堵塞（集体异常）和背景泄漏（概念漂移），在非平稳条件下实现鲁棒检测和自适应。


<details>
  <summary>Details</summary>
Motivation: 供水管网对公共福祉和经济稳定至关重要，但面临管道堵塞和背景泄漏等挑战，且存在数据非平稳性和标记数据有限等操作约束。

Method: 结合长短期记忆变分自编码器（LSTM-VAE）和双重漂移检测机制的无监督在线学习框架，具有轻量级、内存高效的设计，支持实时边缘级监控。

Result: 在两个实际供水管网上的实验表明，该方法在检测异常和适应循环漂移方面始终优于强基线方法。

Conclusion: 该方法在动态供水管网环境中进行无监督事件检测方面表现出有效性，特别适用于非平稳条件下的故障检测。

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [45] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: NIAQUE是一个用于概率回归迁移学习的神经网络模型，通过排列不变性实现任意分位数估计，在预训练和微调后显著提升回归任务性能


<details>
  <summary>Details</summary>
Motivation: 概率回归领域的迁移学习研究不足，需要开发能够有效进行概率回归迁移学习的新方法

Method: 提出NIAQUE模型，利用排列不变性设计，先在多样化下游回归数据集上预训练，然后在特定目标数据集上微调

Result: 在Kaggle竞赛中表现优于基于树的模型和最近的神经基础模型TabPFN、TabDPT，展示了概率迁移学习的积极影响

Conclusion: NIAQUE是一个强大且可扩展的概率回归框架，通过迁移学习有效提升了预测性能

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [46] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出了FraPPE算法，通过高效求解下界中的最小化和最大化问题，解决了偏好锥纯探索问题的最优样本复杂度计算


<details>
  <summary>Details</summary>
Motivation: 现有的Preference-based Pure Exploration算法无法高效计算任意偏好锥下的最优下界，需要开发计算效率高的算法来达到理论最优样本复杂度

Method: 推导下界的三个结构性质来简化最小化问题，使用Frank-Wolfe优化器加速最大化问题，将计算复杂度降至O(KL²)

Result: FraPPE算法渐近达到最优样本复杂度，在合成和真实数据集上的实验显示其样本复杂度最低

Conclusion: 成功填补了计算效率空白，为任意偏好锥提供了首个高效达到理论最优的PrePEx算法

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [47] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille是一个深度学习框架，专门用于处理轮廓数据，通过复数循环卷积实现旋转和循环位移的等变性，并在形状分类、重建和轮廓回归任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 轮廓数据在多个领域中普遍存在，如计算机视觉中的物体边界、气象学中的等值线等。这些数据通常具有旋转和循环位移的特性，因此需要深度学习模型具备相应的等变性。

Method: 采用复数循环卷积来实现旋转和循环位移的等变性，并引入了等变非线性层、粗化层和全局池化层，以获得用于下游任务的不变表示。

Result: 在形状分类、重建和轮廓回归等实验中，RotaTouille框架表现出良好的效果，验证了其等变性和实用性。

Conclusion: RotaTouille框架成功实现了对轮廓数据的旋转和循环位移等变性，为处理此类数据提供了一种有效的深度学习解决方案。

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [48] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine是一种模型无关的即插即用后处理方法，通过结合基础回归器输出和基于排名的估计来提升回归性能，特别适用于数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习回归器在标签充足时表现优异，但在数据稀缺时准确性下降。需要一种方法能够利用专家知识（成对排名）来提升回归性能，而无需重新训练模型。

Method: RankRefine通过逆方差加权将基础回归器的输出与基于排名的估计相结合。给定查询项和具有已知属性的小型参考集，该方法利用成对排名信息进行精炼，无需重新训练。

Result: 在分子属性预测任务中，仅使用20个通过通用大语言模型获得的成对比较，RankRefine实现了高达10%的平均绝对误差相对降低。

Conclusion: RankRefine提供了一种实用且广泛适用的方法，特别是在低数据设置中，通过人类专家或通用LLM提供的排名信息即可改善跨多个领域的回归性能。

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [49] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: 这篇论文通过分析204个GitHub仓库，研究了13种开源偏见检测API的实际使用情况、使用场景和开发者遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习系统在敏感环境中做出生活改变性决策，确保这些系统不会对特定群体产生歧视性决策至关重要。需要了解开源偏见检测API的实际使用情况和挑战。

Method: 采用定性研究方法，分析了1885个借候仓库中的204个使用13种偏见检测API的GitHub仓库，研究它们的使用场景、用途和开发者遇到的问题。

Result: 发现这些API主要用于两种目的：学习和解决实际问题，涉及17种不同使用场景。开发者对偏见检测技术不熟悉，遇到许多故障排除问题，经常需要寻求意见和资源。

Conclusion: 研究结果对未来偏见相关软件工程研究具有重要意义，同时为教育工作者开发更先进的课程提供了指导。

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [50] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: 该论文探讨了零样本强化学习在现实世界部署中的挑战，提出了解决数据质量、可观测性和数据可用性约束的新方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多问题无法通过廉价模拟获得数据，现有模拟器只能近似正确且在训练分布外可能严重错误，导致训练环境与部署环境之间存在不可避免的错位。

Method: 提出了一套在数据质量约束（数据集小且同质）、可观测性约束（状态、动态和奖励通常只能部分观测）和数据可用性约束（不能总是假设先验数据访问）下执行零样本强化学习的方法。

Result: 通过一系列实证研究揭示了现有方法的缺陷，并证明了所提出技术的有效性。

Conclusion: 这些设计使强化学习方法向能够部署解决现实世界问题的目标迈进了一步。

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [51] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 该论文研究大学招生中的公平队列选择问题，比较了一次性决策和顺序决策两种设置下的公平性策略。


<details>
  <summary>Details</summary>
Motivation: 研究大学招生中如何在未知申请人群体的前提下制定公平的录取政策，特别关注政策透明性和公平性保障。

Method: 采用一次性设置（预先固定透明政策）和顺序设置（根据新申请数据动态更新政策）两种方法，使用历史招生数据训练群体模型来优化录取策略。

Result: 分析了一次性设置下所得政策的公平性特性，包括精英统治和群体平等原则。

Conclusion: 论文提出了基于历史数据的招生政策优化框架，为大学招生公平性提供了理论分析和实践指导。

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [52] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: FLAMES框架系统评估数学推理数据合成策略，发现增加问题复杂度、保持高覆盖率比只保留可靠解更重要，并开发出在多个数学基准上表现优异的新数据集


<details>
  <summary>Details</summary>
Motivation: 现有改进LLM数学推理的合成数据方法使用不同设置，难以比较数据合成策略的效果，需要系统研究影响合成数学推理数据性能的各种因素

Method: 提出FLAMES框架，系统研究10种现有数据合成策略和多个影响因素，基于研究发现设计两种新的数据合成策略，并开发FLAMES数据集

Result: FLAMES数据集在OlympiadBench(+15.7)、CollegeMath(+4.5)、GSMPlus(+6.5)和MATH(+3.1)上超越公开数据集，微调Qwen2.5-Math-7B达到81.4%的MATH分数，超过Llama3 405B、GPT-4o和Claude 3.5 Sonnet

Conclusion: 系统研究揭示了合成数学推理数据的最佳平衡点，FLAMES框架和数据集为改进LLM数学推理提供了有效方法，展示了小模型通过优质合成数据可以超越大模型的表现

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [53] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级、数据高效的图神经网络模型，用于预测多种拓扑结构的无线电电路性能指标，在减少训练数据需求的同时显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统无线电电路模拟工具计算成本高，现有的机器学习假模型需要大量数据才能在不同拓扑结构上具有良好的泛化能力，且难以准确建模偏斜和多峰的性能指标分布。

Method: 使用设备-端子级别的图神经网络(GNN)模型，捕捉二极管级对称性和细粒度连接细节，并结合掩码自回归流(MAF)输出头来提高对复杂目标分布建模的验敏性。

Result: 在多个数据集上实验，对称平均绝对百分比误差(sMAPE)和平均相对误差(MRE)分别为2.40%和2.91%，与之前的工作相比，MRE提高3.14倍且训练样本数量减少2.24倍。

Conclusion: 该方法通过端子级电路-图转换和能够建模复杂密度的ML架构，显示了在快速、准确的无线电电路设计自动化方面的有效性。

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [54] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: RLPF是一个结合强化学习和物理反馈的3D分子生成框架，通过力场评估提供物理反馈，显著提升分子结构的物理稳定性和合理性


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成3D分子结构时难以保证物理一致性，特别是力场平衡结构的问题

Method: 将3D分子生成建模为马尔可夫决策过程，使用近端策略优化微调等变扩散模型，引入基于力场评估的奖励函数提供物理反馈

Result: 在QM9和GEOM-drug数据集上，RLPF相比现有方法显著提高了分子稳定性

Conclusion: 将物理反馈融入生成模型具有重要价值，RLPF框架为物理现实的3D分子生成提供了有效解决方案

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [55] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: Doctor方法通过双重检查机制改进离线强化学习中的目标对齐，在数据集内外都能准确控制策略性能水平


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法主要关注最大化累积回报，但许多实际应用需要精确控制策略性能水平。基于监督学习的强化学习方法在目标回报对齐方面存在困难

Method: 提出Doctor方法，通过双重检查Transformer机制实现目标对齐，在数据集内插值和外推时都能保持准确的性能控制

Result: 在动态治疗机制基准测试EpiCare上，该方法能有效调节治疗策略的激进程度，平衡治疗效果和不良事件风险

Conclusion: Doctor方法解决了离线强化学习中目标回报对齐的关键问题，实现了更精确和灵活的策略性能控制

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [56] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究表明RL微调可以恢复SFT导致的OOD性能损失，主要通过修复奇异向量方向偏移而非寻找新解决方案，并提出低成本恢复方法


<details>
  <summary>Details</summary>
Motivation: 重新审视SFT和RL-FT两个阶段如何重塑模型表示和OOD性能，解决大语言模型训练中的过拟合和分布偏移问题

Method: 使用24点卡游戏的OOD变体和基于频谱的诊断方法，分析奇异向量方向和奇异值变化

Result: RL-FT可以恢复大部分SFT导致的OOD性能损失（Llama-11B从8.97%到15.38%，Qwen-7B从17.09%到19.66%），但严重过拟合时无法完全恢复；奇异向量方向偏移比奇异值幅度更重要；低秩和浅层恢复有效

Conclusion: RL主要抵消SFT引起的方向漂移而非寻找新解决方案，提出了低成本的恢复方法（低秩UV合并和浅层重置）供实践者使用

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [57] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 研究探索大型语言模型能否从自然语言规则描述生成棋盘游戏的数字版本代码，测试了三种先进LLM在12款游戏上的表现，最佳模型Claude 3.7 Sonnet实现了55.6%的无错误游戏生成。


<details>
  <summary>Details</summary>
Motivation: 实现棋盘游戏的代码编写通常耗时，而LLM已被证明能在简单上下文信息下生成领域特定代码，因此研究LLM是否能够从自然语言规则描述生成棋盘游戏代码，为快速棋盘游戏代码生成框架奠定基础。

Method: 使用三种先进LLM（Claude、DeepSeek和ChatGPT）对12款流行和冷门游戏进行编码测试，包括自由形式和基于Boardwalk通用游戏API的形式。对游戏和组件进行匿名化处理以避免预训练知识影响，测试实现的可玩性和规则符合性。

Result: 方法被证明可行，表现最佳的Claude 3.7 Sonnet模型产生了55.6%的无错误游戏。虽然遵守API会增加错误频率，但错误的严重程度更显著地依赖于LLM本身。

Conclusion: 研究为创建集成此过程的框架奠定了基础，使棋盘游戏的开发更加便捷，并指出了未来步骤来进一步完善这一过程。

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [58] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA是一种针对噪声、稀疏和稀缺数据的多目标贝叶斯优化算法，通过整合实验不确定性先验知识和信任区域策略，在有限实验预算下更高效地收敛到帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化方法在处理具有实验不确定性（相同输入产生不同输出）的稀疏、稀缺数据集时表现不佳，这在物理实验和模拟实验中很常见，导致实验资源分配效率低下和次优设计。

Method: 提出NOSTRA算法框架，整合实验不确定性的先验知识构建更准确的代理模型，同时采用信任区域策略将采样集中在设计空间的有希望区域，通过战略性地利用先验信息和细化搜索区域来加速收敛。

Result: 通过两个具有不同实验不确定性水平的测试函数验证，NOSTRA在处理噪声、稀疏和稀缺数据方面优于现有方法，能够有效优先考虑那些能提高已识别帕累托前沿准确性的区域采样。

Conclusion: NOSTRA提供了一个在有限实验预算场景下实用的资源高效算法，确保在噪声、稀疏和稀缺数据条件下获得高效性能，为实际实验优化问题提供了有效解决方案。

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [59] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器(SAE)的L0超参数设置对特征学习至关重要，过高或过低都会导致特征混合问题，提出了确定正确L0值的方法


<details>
  <summary>Details</summary>
Motivation: 现有研究将L0视为自由参数，但实际L0设置不当会导致SAE无法学习LLM的底层特征，需要研究L0对SAE性能的影响

Method: 研究BatchTopK SAE中L0参数的影响，开发确定正确L0值的方法，在玩具模型和LLM中进行验证

Result: L0过低会使SAE混合相关特征以改善重构，L0过高会导致退化解并混合特征，大多数常用SAE的L0设置过低

Conclusion: 为了训练具有正确特征的SAE，必须正确设置L0参数，这是SAE训练的关键因素

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [60] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: BAD-ACTS基准测试评估LLM智能体系统安全性，发现单个恶意智能体即可高效诱导有害行为，并提出基于消息监控的防御方案


<details>
  <summary>Details</summary>
Motivation: 确保智能体系统安全使用需要全面了解其在受攻击时可能表现的各种恶意行为，评估LLM智能体系统对抗诱导有害行为攻击的鲁棒性

Method: 提出智能体系统危害分类法，创建BAD-ACTS基准测试（包含4个不同应用环境的智能体系统实现和188个高质量有害行为示例），分析攻击者控制系统中一个智能体时对其他智能体的操纵效果

Result: 攻击成功率很高，表明单个敌对智能体就能对系统安全产生重大影响，即使使用简单的基于提示的防御策略，攻击仍然有效

Conclusion: 提出的基于消息监控的防御更有效，BAD-ACTS基准为智能体系统安全研究提供了多样化的测试平台

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [61] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: 提出了MuST2-Learn框架，通过多视角时空类型学习来预测市政服务请求的处理时间，解决了动态时空相关性、异质服务类型交互和同类型内服务时间高变异等挑战。


<details>
  <summary>Details</summary>
Motivation: 市政311系统等服务请求处理时间不透明，降低了居民满意度和系统效率，需要准确预测服务时间来解决这些问题。

Method: 使用多视角时空类型学习框架，包含类型间编码器捕获异质服务类型关系、类型内变异编码器建模同类型服务时间变异，以及时空编码器捕获各类型的时空相关性。

Result: 在两个真实数据集上的实验表明，MuST2-Learn将平均绝对误差降低了至少32.5%，优于现有最先进方法。

Conclusion: 该框架有效解决了市政服务请求时间预测的复杂挑战，显著提升了预测精度，有助于提高市政服务透明度和居民满意度。

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [62] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 提出了PSD算法用于逃离严格鞍点，具有显式常数和梯度下降/鞍点逃离阶段的明确分离，在非凸优化中达到二阶平稳点


<details>
  <summary>Details</summary>
Motivation: 在非凸优化中，严格鞍点问题是一个重要挑战，现有方法在理论分析和实际性能之间存在差距，需要更严格的算法设计和理论保证

Method: 提出了扰动鞍点逃离下降(PSD)算法，包含梯度下降阶段和鞍点逃离阶段，使用有限差分变体PSD-Probe和随机扩展PSGD

Result: 算法以高概率找到(ε,√(ρε))-近似二阶平稳点，梯度评估次数为O(ℓΔ_f/ε²)加上每逃离事件O((ℓ/√(ρε))log(d/δ))次评估，最多需要O(ℓΔ_f/ε²)个事件

Conclusion: 理论预测通过合成函数和实际机器学习任务得到验证，确认了对数维度依赖性和预测的每事件函数下降，提供了完整的算法规范

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [63] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: 提出了一种使基于LSTM的太阳风暴预测模型可解释的方法，通过注意力机制和后处理技术来理解模型预测的推理过程。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通常被视为黑盒，缺乏透明度使得难以理解模型预测背后的推理过程，特别是在太阳风暴预测这种关键应用中。

Method: 使用带有注意力机制的LSTM网络建模太阳活动区的时间序列数据，并应用后处理模型无关技术来解释模型预测。

Result: 成功为基于LSTM的太阳风暴预测模型添加了可解释性，能够阐明输入序列中影响预测结果的因素。

Conclusion: 这是首次在基于LSTM的太阳风暴预测模型中实现可解释性，提高了模型的可靠性和可问责性。

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [64] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: 本文提出了一种基于TinyML的工业过程监控方案，通过部署在微控制器上的8位卷积神经网络，实现了高准确率的切削过程质量监控，为工业4.0旧设备智能化改造提供了可行解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了在工业4.0背景下，对长期使用的工业机器进行智能化改造，通过无线监控系统实现过程监控功能，利用TinyML技术降低计算资源消耗。

Method: 开发了完整的TinyML流程，包括数据集生成、机器学习模型开发，以及在微控制器上实现和评估预处理和分类流水线。创建了新的MillingVibes数据集，开发了12.59kiB参数存储的8位量化卷积神经网络模型。

Result: 在ARM Cortex M4F微控制器上达到了100.0%的测试准确率，推理时间15.4ms，每次量化CNN推理消耗1.462mJ能量。

Conclusion: 证明了TinyML系统在结构集成过程质量监控中的可行性，为未来的TinyML过程监控解决方案提供了参考标准。

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [65] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 提出了PSSFL框架和FedMox方法，解决边缘设备在联邦学习中计算资源有限和标注数据稀缺的问题，通过专家混合架构实现基础模型的有效适配


<details>
  <summary>Details</summary>
Motivation: 基础模型需要适应下游任务，但在隐私敏感应用中，云端模型无法直接访问边缘私有数据。联邦学习虽然提供隐私保护方案，但现有方法忽视了边缘设备的计算资源限制和标注数据稀缺问题

Method: 提出Practical Semi-Supervised Federated Learning (PSSFL)框架，其中边缘设备只有未标注的低分辨率数据，服务器有有限的标注高分辨率数据。设计了Federated Mixture of Experts (FedMox)框架，采用稀疏专家混合架构，使用空间路由器对齐不同分辨率特征，Soft-Mixture策略稳定半监督学习

Result: 在真实自动驾驶数据集上的实验表明，FedMox在PSSFL设置下有效适配基础模型，显著提升性能，同时约束边缘设备的内存成本

Conclusion: 该工作为联邦场景下可扩展和隐私保护的基础模型适配开辟了新途径

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [66] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: 比较了儿童语音识别中微调成人ASR模型与从头开始训练的差异，发现SSL表示存在成人语音偏见，从头开始训练可缓解偏见，模型规模在10亿参数内持续改进，开源模型更适合儿童语音研究


<details>
  <summary>Details</summary>
Motivation: 儿童语音识别面临声学变异性大和标注数据有限的问题，现有研究主要关注微调成人ASR模型，但与从头开始训练的比较研究不足

Method: 使用ESPnet框架，在多个数据集上比较从头开始训练方法，评估不同SSL表示（WavLM、XEUS）和解码器架构，分析模型规模扩展效果

Result: SSL表示存在成人语音偏见，从头开始训练儿童语音可缓解偏见；模型规模在1B参数内持续改进，之后性能趋于平稳；专有模型（如Whisper）在年龄相关任务中存在局限性

Conclusion: 开源数据模型对于可靠的儿童语音研究至关重要，公开基准为稳健的儿童语音处理训练策略提供了重要见解

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [67] [Combined Approximations for Uniform Operational Consistent Query Answering](https://arxiv.org/abs/2508.15814)
*Marco Calautti,Ester Livshits,Andreas Pieris,Markus Schneider*

Main category: cs.DB

TL;DR: 本文研究了操作一致性查询回答（CQA）在组合复杂度下的近似计算问题，证明了在自连接自由且有界广义超树宽度的合取查询下存在高效近似方案，并指出放弃这些语法限制后可能不存在高效近似。


<details>
  <summary>Details</summary>
Motivation: 操作一致性查询回答框架基于操作序列构建修复，之前研究在数据复杂度下存在高效近似，但在组合复杂度（查询作为输入）下的近似性尚未明确。本文旨在填补这一空白。

Method: 引入新的计数复杂度类SpanTL，利用树自动机的近似性结果证明该类问题存在高效近似方案，并将目标问题归入SpanTL类。

Result: 证明了对于自连接自由且有界广义超树宽度的合取查询，操作CQA在组合复杂度下存在高效随机近似方案；同时证明放弃自连接自由性或广义超树宽度限制后可能不存在此类近似。

Conclusion: 本文建立了操作一致性查询回答在组合复杂度下的近似计算理论框架，通过引入SpanTL复杂度类并利用树自动机理论，为这类问题提供了可行的近似解决方案，同时明确了近似性的边界条件。

Abstract: Operational consistent query answering (CQA) is a recent framework for CQA
based on revised definitions of repairs, which are built by applying a sequence
of operations (e.g., fact deletions) starting from an inconsistent database
until we reach a database that is consistent w.r.t. the given set of
constraints. It has been recently shown that there is an efficient
approximation for computing the percentage of repairs that entail a given query
when we focus on primary keys, conjunctive queries, and assuming the query is
fixed (i.e., in data complexity). However, it has been left open whether such
an approximation exists when the query is part of the input (i.e., in combined
complexity). We show that this is the case when we focus on self-join-free
conjunctive queries of bounded generelized hypertreewidth. We also show that it
is unlikely that efficient approximation schemes exist once we give up one of
the adopted syntactic restrictions, i.e., self-join-freeness or bounding the
generelized hypertreewidth. Towards the desired approximation, we introduce a
counting complexity class, called $\mathsf{SpanTL}$, show that each problem in
it admits an efficient approximation scheme by using a recent approximability
result about tree automata, and then place the problem of interest in
$\mathsf{SpanTL}$.

</details>


### [68] [MAAdvisor: Zero-Shot Index Advisor using Multi-Agent LLMs](https://arxiv.org/abs/2508.16044)
*Zhaodonghui Li,Haitao Yuan,Jiachen Shi,Hao Zhang,Yu Rong,Gao Cong*

Main category: cs.DB

TL;DR: MAAdvisor是一个基于多智能体框架的零样本LLM索引推荐系统，通过分解索引推荐问题为多个子步骤，使用全局和局部智能体协作实现高效索引选择。


<details>
  <summary>Details</summary>
Motivation: 传统索引推荐方法存在计算时间高、泛化能力差的问题，现有基于LLM的方法无法达到最优效果且演示准备成本高。

Method: 将索引推荐分解为规划、选择、组合、修订和反思五个子步骤，设计LLM嵌入的全局和局部智能体分别处理不同步骤。

Result: 实验表明MAAdvisor不仅达到启发式方法的最优性能，还优于基于学习和提示的方法，具有更高的效率和更好的零样本推理能力。

Conclusion: 多智能体框架有效解决了索引推荐问题，MAAdvisor在性能、效率和泛化能力方面均表现出色，为零样本索引推荐提供了新思路。

Abstract: Index recommendation is one of the most important problems in database
management system (DBMS) optimization. Given queries and certain index-related
constraints, traditional methods rely on heuristic optimization or
learning-based models to select effective indexes and improve query
performance. However, heuristic optimization suffers from high computation
time, and learning-based models lose generalisability due to training for
different workloads and database schemas. With the recent rapid development of
large language models (LLMs), methods using prompt tuning have been proposed to
enhance the efficiency of index selection. However, such methods still can not
achieve the state-of-the-art (SOTA) results, and preparing the index selection
demonstrations is also resource-intensive. To address these issues, we propose
MAAdvisor, a zero-shot LLM-based index advisor with a multi-agent framework. We
decompose the index recommendation problem into sub-steps, including planning,
selection, combination, revision, and reflection. A set of LLM-embedded agents
is designed to handle each one of the different sub-steps. Our method utilizes
global agents to control the index selection process and local agents to select
and revise indexes. Through extensive experiments, we show that our proposed
MAAdvisor not only achieves the SOTA performance compared to the heuristic
methods, but also outperforms learning-based and prompt-based methods with
higher efficiency and better zero-shot inference ability.

</details>


### [69] [Attribute Filtering in Approximate Nearest Neighbor Search: An In-depth Experimental Study](https://arxiv.org/abs/2508.16263)
*Mocheng Li,Xiao Yan,Baotong Lu,Yue Zhang,James Cheng,Chenhao Ma*

Main category: cs.DB

TL;DR: 本文提出了一个统一的过滤近似最近邻搜索框架，对现有10种算法进行系统分类、组件分析和实验评估，为方法选择和未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着结构化与非结构化数据融合，过滤近似最近邻搜索算法涌现但缺乏统一分析框架，需要系统比较不同算法的核心技术和性能差异。

Method: 提出统一接口和分类体系，分析索引结构、剪枝策略和入口点选择等关键组件，在4个数据集上对10种算法12种方法进行大规模实验评估。

Result: 实验覆盖1000万条数据，选择性从0.1%到100%，深入揭示了剪枝、入口点选择和边缘过滤成本对性能的影响。

Conclusion: 总结了各方法的优缺点，提供了实用选择指南，指出了未来研究方向，并开源了代码库FANNBench。

Abstract: With the growing integration of structured and unstructured data, new methods
have emerged for performing similarity searches on vectors while honoring
structured attribute constraints, i.e., a process known as Filtering
Approximate Nearest Neighbor (Filtering ANN) search. Since many of these
algorithms have only appeared in recent years and are designed to work with a
variety of base indexing methods and filtering strategies, there is a pressing
need for a unified analysis that identifies their core techniques and enables
meaningful comparisons.
  In this work, we present a unified Filtering ANN search interface that
encompasses the latest algorithms and evaluate them extensively from multiple
perspectives. First, we propose a comprehensive taxonomy of existing Filtering
ANN algorithms based on attribute types and filtering strategies. Next, we
analyze their key components, i.e., index structures, pruning strategies, and
entry point selection, to elucidate design differences and tradeoffs. We then
conduct a broad experimental evaluation on 10 algorithms and 12 methods across
4 datasets (each with up to 10 million items), incorporating both synthetic and
real attributes and covering selectivity levels from 0.1% to 100%. Finally, an
in-depth component analysis reveals the influence of pruning, entry point
selection, and edge filtering costs on overall performance. Based on our
findings, we summarize the strengths and limitations of each approach, provide
practical guidelines for selecting appropriate methods, and suggest promising
directions for future research. Our code is available at:
https://github.com/lmccccc/FANNBench.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [70] [Ransomware Negotiation: Dynamics and Privacy-Preserving Mechanism Design](https://arxiv.org/abs/2508.15844)
*Haohui Zhang,Sirui Shen,Xinyu Hu,Chenglu Jin*

Main category: cs.GT

TL;DR: 这篇论文通过有限时限交替招片讨价模型形式分析了疯狂病毒攻击中攻击者与受害者的协商动态，并设计了一种负费斯激励兼容机制，通过安全双方计算实现私密保护的自动化协商。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究主要集中在预防性防御，但疯狂病毒攻击后的协商动态很少被深入研究。不完全信息（攻击者不知道受害者数据价值，受害者不知道攻击者的最低赔付金额）会延长协商并增加受害者的业务中断成本。

Method: 使用有限旰期交替招片讨价游戏模型进行形式分析，设计负费斯激励兼容机制，通过基于混流电路的安全双方计算实现该机制，避免需要可信中介机构并在整个协商过程中保护双方隐私。

Result: 分析显示讨价如何改变双方的最优策略。设计的机制能够在不要求任何一方曝露私人估值的情况下，促进快速达成公平的赔付协议。

Conclusion: 这是第一个基于对疯狂病毒协商动态形式分析的自动化、私密保护协商机制，有助于减少协商时间和降低受害者的业务中断成本。

Abstract: Ransomware attacks have become a pervasive and costly form of cybercrime,
causing tens of millions of dollars in losses as organizations increasingly pay
ransoms to mitigate operational disruptions and financial risks. While prior
research has largely focused on proactive defenses, the post-infection
negotiation dynamics between attackers and victims remains underexplored. This
paper presents a formal analysis of attacker-victim interactions in modern
ransomware incidents using a finite-horizon alternating-offers bargaining game
model. Our analysis demonstrates how bargaining alters the optimal strategies
of both parties. In practice, incomplete information-attackers lacking
knowledge of victims' data valuations and victims lacking knowledge of
attackers' reservation ransoms-can prolong negotiations and increase victims'
business interruption costs. To address this, we design a Bayesian
incentive-compatible mechanism that facilitates rapid agreement on a fair
ransom without requiring either party to disclose private valuations. We
further implement this mechanism using secure two-party computation based on
garbled circuits, thereby eliminating the need for trusted intermediaries and
preserving the privacy of both parties throughout the negotiation. To the best
of our knowledge, this is the first automated, privacy-preserving negotiation
mechanism grounded in a formal analysis of ransomware negotiation dynamics.

</details>


### [71] [Data Auctions for Retrieval Augmented Generation](https://arxiv.org/abs/2508.16007)
*Minbiao Han,Seyed A. Esmaeili,Michael Albert,Haifeng Xu*

Main category: cs.GT

TL;DR: 本文研究RAG任务中的数据销售问题，提出基于覆盖率的估值函数，设计多项式时间(1-1/e)近似算法，并通过数据燃烧技术实现激励兼容性


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI应用中RAG任务的数据销售问题，关注数据控制和先验无关的收入最大化，特别是在每个数据点只能分配给一个买家的场景下

Method: 使用覆盖率基础的估值函数建模买家估值，设计多项式时间近似算法，并通过数据燃烧后处理步骤实现激励兼容性

Result: 证明了福利最大化问题是NP难问题，但设计了(1-1/e)近似算法，实验在合成和真实图像文本数据集上验证了算法的有效性

Conclusion: 提出的数据燃烧技术成功解决了效率与激励兼容性之间的权衡，为RAG数据销售提供了实用的解决方案

Abstract: We study the problem of data selling for Retrieval Augmented Generation (RAG)
tasks in Generative AI applications. We model each buyer's valuation of a
dataset with a natural coverage-based valuation function that increases with
the inclusion of more relevant data points that would enhance responses to
anticipated queries. Motivated by issues such as data control and prior-free
revenue maximization, we focus on the scenario where each data point can be
allocated to only one buyer. We show that the problem of welfare maximization
in this setting is NP-hard even with two bidders, but design a polynomial-time
$(1-1/e)$ approximation algorithm for any number of bidders. Unfortunately,
however, this efficient allocation algorithm fails to be incentive compatible.
The crux of our approach is a carefully tailored post-processing step called
\emph{data burning} which retains the $(1-1/e)$ approximation factor but
achieves incentive compatibility. Our thorough experiments on synthetic and
real-world image and text datasets demonstrate the practical effectiveness of
our algorithm compared to popular baseline algorithms for combinatorial
auctions.

</details>


### [72] [Proportional Representation in Rank Aggregation](https://arxiv.org/abs/2508.16177)
*Patrick Lederer*

Main category: cs.GT

TL;DR: 提出三种比例排序聚合方法，确保输出排序能按权重比例反映输入排序的偏好


<details>
  <summary>Details</summary>
Motivation: 传统排序聚合方法都是多数主义导向，无法保证比例公平性，需要设计能按权重比例代表每个输入排序的公平方法

Method: 提出了三种社会福利函数：比例顺序Borda规则、排名等额分配方法和流量调整Borda规则，基于成对比较一致性的比例公平条件

Result: 比例顺序Borda规则满足基本比例公平条件，流量调整Borda规则满足更强的公平条件，排名等额分配方法在保持公平性的同时具有功利主义特性

Conclusion: 成功将比例代表概念从委员会选举和参与式预算扩展到排序聚合领域，提供了可行的比例公平排序聚合解决方案

Abstract: In rank aggregation, the task is to aggregate multiple weighted input
rankings into a single output ranking. While numerous methods, so-called social
welfare functions (SWFs), have been suggested for this problem, all of the
classical SWFs tend to be majoritarian and are thus not acceptable when a
proportional ranking is required. Motivated by this observation, we will design
SWFs that guarantee that every input ranking is proportionally represented by
the output ranking. Specifically, our central fairness condition requires that
the number of pairwise comparisons between candidates on which an input ranking
and the output ranking agree is proportional to the weight of the input
ranking. As our main contribution, we present a simple SWF called the
Proportional Sequential Borda rule, which satisfies this condition. Moreover,
we introduce two variants of this rule: the Ranked Method of Equal Shares,
which has a more utilitarian flavor while still satisfying our fairness
condition, and the Flow-adjusting Borda rule, which satisfies an even stronger
fairness condition. Many of our axioms and techniques are inspired by results
on approval-based committee voting and participatory budgeting, where the
concept of proportional representation has been studied in depth.

</details>


### [73] [Strategyproof Randomized Social Choice for Restricted Sets of Utility Functions](https://arxiv.org/abs/2508.16195)
*Patrick Lederer*

Main category: cs.GT

TL;DR: 该论文提出了U-策略防护性概念，通过限制效用函数集合来规避Gibbard的负面结果，分析了U-策略防护性与决策确定性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: Gibbard(1977)的经典结果表明，所有策略防护的社会决策方案要么对投票者或备选方案不公平，要么需要大量随机化。为规避这一负面结果，需要探索新的策略防护性概念。

Method: 提出U-策略防护性概念，只要求预定义的效用函数集合U中的投票者无法操纵。分析U-策略防护性与各种决策确定性概念之间的权衡关系。

Result: 当U中的效用函数最偏好选项远优于其他选项时，存在U-策略防护的SDS；但U-策略防护性与孔多塞一致性不兼容；当投票者对前两个偏好选项接近无差异时，无法显著优于均匀随机独裁制。

Conclusion: 通过限制效用函数集合可以实现部分策略防护性，但在保持策略防护性的同时提高决策确定性存在理论限制，需要在防护性和效率之间进行权衡。

Abstract: Social decision schemes (SDSs) map the voters' preferences over multiple
alternatives to a probability distribution over these alternatives. In a
seminal result, Gibbard (1977) has characterized the set of SDSs that are
strategyproof with respect to all utility functions and his result implies that
all such SDSs are either unfair to the voters or alternatives, or they require
a significant amount of randomization. To circumvent this negative result, we
propose the notion of $U$-strategyproofness which postulates that only voters
with a utility function in a predefined set $U$ cannot manipulate. We then
analyze the tradeoff between $U$-strategyproofness and various decisiveness
notions that restrict the amount of randomization of SDSs. In particular, we
show that if the utility functions in the set $U$ value the best alternative
much more than other alternatives, there are $U$-strategyproof SDSs that choose
an alternative with probability $1$ whenever all but $k$ voters rank it first.
On the negative side, we demonstrate that $U$-strategyproofness is incompatible
with Condorcet-consistency if the set $U$ satisfies minimal symmetry
conditions. Finally, we show that no ex post efficient and $U$-strategyproof
SDS can be significantly more decisive than the uniform random dictatorship if
the voters are close to indifferent between their two favorite alternatives.

</details>


### [74] [Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games](https://arxiv.org/abs/2508.16245)
*Cole Wyeth,Marcus Hutter,Jan Leike,Jessica Taylor*

Main category: cs.GT

TL;DR: 本文解决了Kalai和Lehrer的经典"grain of truth"问题，构建了一个包含所有可计算策略的类，使得贝叶斯最优策略能够学习预测其他玩家的策略，并在未知环境中收敛到ε-纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决多玩家无限博弈中贝叶斯玩家学习预测其他玩家策略的问题，特别是寻找一个足够大的策略类，使得贝叶斯最优策略能够包含其中并实现相互一致的信念。

Method: 构建一个包含所有可计算策略的类，并证明贝叶斯最优策略对于该类中的任何合理先验都存在。在已知重复阶段博弈中使用KL收敛，在未知环境中使用Thompson采样。

Result: 成功构建了包含所有可计算策略的类，证明了贝叶斯最优策略的存在性，在已知环境中实现收敛，在未知环境中收敛到ε-纳什均衡，并提供了计算近似方法。

Conclusion: 本文为经典的grain of truth问题提供了完整的形式化解决方案，扩展了可处理策略类的范围，并在理论和计算层面都提供了可行的实现途径。

Abstract: A Bayesian player acting in an infinite multi-player game learns to predict
the other players' strategies if his prior assigns positive probability to
their play (or contains a grain of truth). Kalai and Lehrer's classic grain of
truth problem is to find a reasonably large class of strategies that contains
the Bayes-optimal policies with respect to this class, allowing
mutually-consistent beliefs about strategy choice that obey the rules of
Bayesian inference. Only small classes are known to have a grain of truth and
the literature contains several related impossibility results. In this paper we
present a formal and general solution to the full grain of truth problem: we
construct a class of strategies wide enough to contain all computable
strategies as well as Bayes-optimal strategies for every reasonable prior over
the class. When the "environment" is a known repeated stage game, we show
convergence in the sense of [KL93a] and [KL93b]. When the environment is
unknown, agents using Thompson sampling converge to play $\varepsilon$-Nash
equilibria in arbitrary unknown computable multi-agent environments. Finally,
we include an application to self-predictive policies that avoid planning.
While these results use computability theory only as a conceptual tool to solve
a classic game theory problem, we show that our solution can naturally be
computationally approximated arbitrarily closely.

</details>


### [75] [A QoE-Driven Personalized Incentive Mechanism Design for AIGC Services in Resource-Constrained Edge Networks](https://arxiv.org/abs/2508.16251)
*Hongjia Wu,Minrui Xu,Zehui Xiong,Lin Gao,Haoyuan Pan,Dusit Niyato,Tse-Tin Chan*

Main category: cs.GT

TL;DR: 这篇论文提出了一种基于移动边缘计算(MEC)的AIGC服务个性化激励机制，通过多维度QoE指标和双干扰奖励优化算法，在降低计算通信开销、服务成本和资源消耗方面取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，AIGC服务的个性化需求日益增长，但AIGC服务提供商面临用户需求主观复杂、计算通信资源受限等挑战，需要有效的激励机制来促进个性化服务提供。

Method: 首先建立了一个新的多维度QoE指标，综合考虑准确性、标记数量和及时性。在MEC网络环境下，将问题形式化为具有平衡约束的平衡问题(EPEC)，用户作为领导者确定奖励，服务提供商作为跟随者优化资源分配。提出双干扰奖励优化算法来降低自适应定价的实现复杂度。

Result: 实验结果显示，与现有最佳方案相比，该机制实现了平均计算和通信开销降低64.9%，用户平均服务成本降低66.5%，服务提供商资源消耗降低76.8%。

Conclusion: 该研究成功开发了一种高效的AIGC个性化服务激励机制，通过创新的QoE评估指标和优化算法，有效解决了移动边缘计算环境下的资源约束问题，为AIGC服务的商业化应用提供了重要技术支撑。

Abstract: With rapid advancements in large language models (LLMs), AI-generated content
(AIGC) has emerged as a key driver of technological innovation and economic
transformation. Personalizing AIGC services to meet individual user demands is
essential but challenging for AIGC service providers (ASPs) due to the
subjective and complex demands of mobile users (MUs), as well as the
computational and communication resource constraints faced by ASPs. To tackle
these challenges, we first develop a novel multi-dimensional
quality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC
services by integrating accuracy, token count, and timeliness. We focus on a
mobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs
deploying differentiated AIGC models on edge servers and multiple MUs with
heterogeneous QoE requirements requesting AIGC services from ASPs. To
incentivize ASPs to provide personalized AIGC services under MEC resource
constraints, we propose a QoE-driven incentive mechanism. We formulate the
problem as an equilibrium problem with equilibrium constraints (EPEC), where
MUs as leaders determine rewards, while ASPs as followers optimize resource
allocation. To solve this, we develop a dual-perturbation reward optimization
algorithm, reducing the implementation complexity of adaptive pricing.
Experimental results demonstrate that our proposed mechanism achieves a
reduction of approximately $64.9\%$ in average computational and communication
overhead, while the average service cost for MUs and the resource consumption
of ASPs decrease by $66.5\%$ and $76.8\%$, respectively, compared to
state-of-the-art benchmarks.

</details>


### [76] [A Social Choice Analysis of Optimism's Retroactive Project Funding](https://arxiv.org/abs/2508.16285)
*Eyal Briman,Nimrod Talmon,Angela Kreitenweis,Muhammad Idrees*

Main category: cs.GT

TL;DR: 论文分析了Optimism RetroPGF项目的资金分配机制，发现现有系统存在显著缺陷，并提出基于功利主义移动幻影机制的改进方案，以提升社会福利和策略证明性。


<details>
  <summary>Details</summary>
Motivation: Optimism RetroPGF作为区块链生态中的重要倡议，已分配超过1亿美元资金并有13亿美元储备，但当前分配系统存在重大缺陷，需要改进治理机制以应对如此大规模的资金管理。

Method: 利用计算社会选择技术和多智能体系统洞察，建议采用Freeman等人2019年提出的功利主义移动幻影机制，该机制旨在增强社会福利（使用L1范数）同时满足策略证明性。

Result: 提出了一个改进DAO资金分配机制的形式化框架，该机制能够更好地满足治理需求，为去中心化治理和公共物品分配提供理论支持。

Conclusion: 研究为设计改进的DAO资金分配机制提供了正式框架，有助于推动去中心化治理和公共物品分配的更广泛讨论和实践应用。

Abstract: The Optimism Retroactive Project Funding (RetroPGF) is a key initiative
within the blockchain ecosystem that retroactively rewards projects deemed
valuable to the Ethereum and Optimism communities. Managed by the Optimism
Collective, a decentralized autonomous organization (DAO), RetroPGF represents
a large-scale experiment in decentralized governance. Funding rewards are
distributed in OP tokens, the native digital currency of the ecosystem. As of
this writing, four funding rounds have been completed, collectively allocating
over 100M dollars, with an additional 1.3B dollars reserved for future rounds.
However, we identify significant shortcomings in the current allocation system,
underscoring the need for improved governance mechanisms given the scale of
funds involved.
  Leveraging computational social choice techniques and insights from
multiagent systems, we propose improvements to the voting process by
recommending the adoption of a utilitarian moving phantoms mechanism. This
mechanism, originally introduced by Freeman et al. in 2019, is designed to
enhance social welfare (using the L1 norm) while satisfying strategyproofness
-- two key properties aligned with the application's governance requirements.
Our analysis provides a formal framework for designing improved funding
mechanisms for DAOs, contributing to the broader discourse on decentralized
governance and public goods allocation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 提出T-ILR方法，通过模糊LTLf解释将时态逻辑规范直接集成到深度学习序列任务中，在准确性和计算效率方面超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法主要处理静态域的符号知识集成，而处理时态逻辑规范的方法较少，且依赖显式有限状态自动机表示

Method: 扩展Iterative Local Refinement (ILR)神经符号算法，利甦模糊LTLf解释，形成T-ILR方法，直接将有限迹线上的线性时态逻辑规范集成到深度学习序列任务中

Result: 在图像序列分类应用中，T-ILR方法在准确性和计算效率方面都超过了现有最优方法

Conclusion: T-ILR为处理时态逻辑规范提供了一种高效的神经符号框架，在保持计算效率的同时提高了模型性能

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [78] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 提出了一种可解释性AI框架CoFE，通过生成反事实ECG来解释AI-ECG模型的预测决策，提高临床应用可靠性


<details>
  <summary>Details</summary>
Motivation: 解决AI基于ECG预测模型在临床实践中的可解释性需求，促进医生对AI模型决策的信任和采纳

Method: 开发CoFE框架，生成反事实ECG信号，用于演示特定特征（如振幅、间期）对模型预测决策的影响

Result: 在房频分类和钾水平回归模型中验证，发现CoFE显示的ECG特征变化与已知临床知识一致

Conclusion: CoFE框架能够明确显示ECG中有效特征的位置和对模型预测的影响机制，有助于提高AI-ECG模型的可解释性和临床决策效果

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [79] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 提出了一种基于自适应规划图的训练免费多模态多跳问答框架，通过动态规划、检索和推理模块实现灵活的多路径探索，无需昂贵训练即可达到或超越现有训练依赖模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多跳问答方法依赖顺序检索和推理，容易因中间步骤错误而失败，且多模态模型训练成本高昂。需要一种无需训练、能动态探索多路径的解决方案。

Method: 使用自适应规划图框架，包含规划模块（分析当前状态决定下一步行动）、检索模块（针对不同模态的动态适配策略）和推理模块，实现多路径动态探索。

Result: 在MultimodalQA和WebQA数据集上的实验表明，该方法无需训练即可匹配或超越依赖训练的现有模型性能。

Conclusion: 提出的训练免费自适应规划图框架有效解决了多模态多跳问答中的错误传播和训练成本问题，为多模态推理提供了灵活高效的解决方案。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [80] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP是一个多模态基础模型，通过CNN-Transformer编码器处理结构化EHR时间序列数据，并与非结构化文本融合，在临床预测和叙事生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHRs)包含丰富的多模态临床数据，但现有方法将数值数据序列化为文本会丢失时间和定量细节，需要更好的多模态融合方法来提升临床预测和文档生成能力。

Method: 采用两阶段训练：1)生成式预训练，通过掩码特征预测和下一时间步预测学习时间动态；2)多任务微调，用于临床预测任务。使用CNN-Transformer编码器处理结构化时间序列，通过跨模态注意力与LLaMA解码器融合。

Result: 在MIMIC-IV数据集上，心衰预测AUROC=0.923，2型糖尿病AUROC=0.817，30天再入院AUROC=0.627。叙事生成ROUGE-L=0.135，BERTScore-F1=0.545。人工评估显示在忠实性、流畅性和临床实用性方面表现最佳。

Conclusion: GDP证明单一多模态基础模型既能预测临床事件又能生成高质量临床叙事，其灵活架构可扩展到其他模态，有望减少医院文档工作量而不牺牲准确性。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [81] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 该研究探索数字规划中城市舒适度的理论解释和评估方法，重点关注多维分析、数据支持和AI辅助三个关键维度


<details>
  <summary>Details</summary>
Motivation: 确保宜居性和舒适性是城市规划的基本目标，但目前缺乏清晰的城市舒适度定义和综合评估框架

Method: 探索城市舒适度的理论解释和方法论，强调多维分析、数据支持和AI辅助三个维度

Result: 提出了数字规划背景下城市舒适度评估的理论框架和方法论方向

Conclusion: 需要建立更清晰的城市舒适度定义和综合评估框架，数字技术特别是AI可以为城市舒适度评估提供重要支持

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [82] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: MSEF框架通过多层可导向嵌入融合，让LLM能够直接访问所有深度的时序模式，解决了现有方法中时序信息在深层逐渐消失的问题，在7个基准测试中平均MSE降低31.8%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM时序预测方法存在浅层集成问题，时序表示主要在输入层接入，导致深层时序信息逐渐消失，文本嵌入与时序表示之间的适应效果不佳。

Method: 提出多层可导向嵌入融合(MSEF)框架，利用现有时序基础模型提取语义丰富的嵌入，通过层特定导向向量与LLM中间文本表示融合，实现时序和文本模态的持续对齐优化。

Result: 在7个基准测试中相比基线方法表现出显著性能提升，平均MSE降低31.8%，证明了有效的少样本学习能力。

Conclusion: MSEF通过深层时序信息融合机制，成功解决了LLM时序预测中的信息衰减问题，为时序分析与语言模型的深度融合提供了有效解决方案。

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [83] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本文提出了InMind评估框架，用于测试LLMs在社交推理游戏中捕捉和应用个性化推理风格的能力，发现通用LLMs依赖词汇线索而难以适应动态策略，推理增强型LLMs展现出风格敏感推理的早期迹象。


<details>
  <summary>Details</summary>
Motivation: 现有评估往往忽视了个性化推理风格对社交情境理解的影响，需要开发能够评估LLMs是否能够捕捉和应用个性化推理方法的框架。

Method: 提出InMind框架，通过增强结构化游戏数据（包含回合级策略追踪和赛后反思），在观察者和参与者两种模式下收集数据，支持4个认知驱动的任务来评估静态对齐和动态适应能力。

Result: 在Avalon游戏中对11个先进LLMs的评估显示，通用LLMs（包括GPT-4o）经常依赖词汇线索，难以将反思锚定在时间游戏中或适应演化策略；而推理增强型LLMs（如DeepSeek-R1）展现出风格敏感推理的早期迹象。

Conclusion: 当前LLMs在个性化适应性推理方面存在重要局限，InMind框架为认知对齐的人机交互迈出了重要一步。

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [84] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent是一个新颖的多智能体框架，用于从红外光谱中解析分子结构，模拟专家分析流程并具有良好扩展性


<details>
  <summary>Details</summary>
Motivation: 现有红外光谱分析方法无法充分反映专家分析过程，缺乏整合多种化学知识的灵活性，而这对实际分析场景至关重要

Method: 提出多智能体框架，每个智能体专门负责红外光谱解释的特定方面，通过互补角色实现集成推理

Result: 实验表明IR-Agent不仅提高了实验红外光谱的基线性能，还展现出对各种化学信息形式的强适应性

Conclusion: 该框架成功模拟了专家驱动的红外分析流程，提高了结构解析的准确性，并具有良好的扩展性

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [85] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 提出Food Claim-Traceability Network (FCN)作为印度食品知识图谱FKG.in的扩展，用于追踪、验证和情境化食品相关声明，采用半自动化知识管理流程和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前食品声明领域存在科学、文化和商业等各种主张，但追踪、验证和情境化这些声明的基础设施仍然分散且不发达，需要建立透明可追溯的食品知识生态系统。

Method: 基于FKG.in知识图谱构建FCN网络，设计本体结构，使用Reddit数据和大型语言模型进行半自动化知识管理，建立包含结构化模式和溯源感知管道的声明提取验证系统。

Result: 开发了FKG.in-FCN的概念验证，能够整合策划数据输入、结构化模式和溯源感知管道，实现食品相关声明的提取和验证。

Conclusion: 该方法具有应用无关性，可适应不同地理、烹饪或监管环境，通过结构化、可验证和可解释的方式建模食品声明及其可追溯性，有助于建立更透明和负责任的食品知识生态系统。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [86] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 提出了首个眼科多模态数据集MM-Retinal-Reason和眼科专用多模态推理模型OphthaReason，通过不确定性感知动态思维方法在基础和复杂推理任务上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型主要关注基于视觉特征匹配的基础推理，但真实临床诊断需要整合异质临床信息与多模态医学影像数据的复杂推理过程

Method: 设计不确定性感知动态思维(UADT)方法，通过熵估计样本级不确定性，使用成形优势机制动态调节模型探索深度

Result: 在基础和复杂推理任务上分别超越通用MLLMs、医学MLLMs、RL医学MLLMs和眼科MLLMs至少24.92%、15.00%、21.20%和17.66%

Conclusion: 该研究填补了医学多模态推理的空白，提出的数据集和模型能够有效模拟真实临床思维模式，在眼科诊断推理方面表现出色

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [87] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: 本文提出Preference Chain方法，结合图检索增强生成和LLM，在数据稀缺环境下模拟交通系统中的人类行为，在Replica数据集上表现优于标准LLM。


<details>
  <summary>Details</summary>
Motivation: 城市环境中人类行为理解很重要，但新开发区域的行为数据收集困难。现有生成代理方法在产生一致、上下文敏感和现实的行为输出方面存在不足。

Method: 提出Preference Chain方法，整合图检索增强生成(RAG)与大语言模型(LLM)，增强交通系统中人类行为的上下文感知模拟。

Result: 在Replica数据集上的实验表明，Preference Chain在符合真实世界交通方式选择方面优于标准LLM。

Conclusion: 该方法为数据稀缺环境中模拟复杂人类行为提供了有前景的框架，可用于新兴城市移动性建模、个性化出行行为分析和动态交通预测。

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [88] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2是一种基于进化算法的模型融合方法，通过动态调整融合边界、多样性保持机制和启发式吸引力度量，实现从零开始进化模型，在计算效率上优于CMA-ES，并在语言和图像生成任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法需要手动划分参数组进行融合，限制了潜在组合的探索和性能提升。

Method: 提出M2N2进化算法，包含三个关键特征：动态调整融合边界、基于自然竞争资源的多样性保持机制、启发式吸引力度量来识别最有前景的模型对进行融合。

Result: 首次证明模型融合可以从零开始进化模型，在MNIST分类任务上达到与CMA-ES相当的性能但计算更高效，在语言和图像生成模型中实现最先进性能。

Conclusion: M2N2展示了强大的鲁棒性和多功能性，能够保持超出适应度函数明确优化的关键模型能力，为模型融合提供了新的进化方法。

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [89] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 本文开发了一个用于药物资产尽职调查的竞争对手发现AI代理系统，通过LLM代理将多模态非结构化数据转换为结构化评估语料库，在私有基准测试中达到83%的召回率，显著优于现有方案，并将分析师处理时间从2.5天缩短至约3小时。


<details>
  <summary>Details</summary>
Motivation: 当前药物竞争格局分析面临数据分散、付费墙限制、本体不匹配、别名众多等多重挑战，现有LLM系统无法可靠检索所有竞争药物名称，且缺乏公认的公共基准测试。

Method: 使用LLM代理将5年多模态非结构化尽职调查备忘录转换为结构化评估语料库，建立指示到竞争药物的映射，并引入LLM作为评判代理来过滤误报和抑制幻觉。

Result: 在基准测试中达到83%的召回率，优于OpenAI Deep Research（65%）和Perplexity Labs（60%）。在生物技术VC基金案例中，分析师处理时间从2.5天降至约3小时（约20倍提升）。

Conclusion: 该系统成功解决了药物竞争格局分析的挑战，在生产和实际应用中证明了其有效性，显著提高了分析效率和准确性。

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


### [90] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: GROW-AI框架通过六项标准评估AI的成长性，使用多游戏结构和统一日志记录，提供可比较的AI成熟度评估


<details>
  <summary>Details</summary>
Motivation: 扩展AI评估框架，回答"机器能否成长"的问题，作为图灵测试的自然继承者

Method: 基于六项主要标准(C1-C6)，通过特定"游戏"进行评估，分为四个竞技场，使用标准化AI日志记录所有决策和行动，采用专家先验方法确定权重，计算成长指数作为六个分数的算术平均值

Result: 该方法能够对不同类型AI实体(机器人、软件代理、LLMs)进行一致且可比较的成长水平评估，多游戏结构突出优势和薄弱领域，统一日志保证评估的可追溯性和可复制性

Conclusion: GROW-AI通过将人类成长过程概念性转移到AI领域，结合心理学、机器人学、计算机科学和伦理学的视角，不仅测量性能还捕捉AI实体向成熟度演进的路径

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [91] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0是一个支持工具型智能体与环境交互的框架，提供统一接口、异步设计、内置智能体和工程支持，用于构建可扩展的智能体应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，智能体需要结合内在知识和动态工具使用来增强处理现实世界任务的能力，需要一个支持灵活高效工具交互的框架。

Method: 抽象基础组件并提供统一接口和可扩展模块，基于ReAct范式设计异步架构，集成特定场景的内置智能体，提供可视化评估模块和安全沙箱环境。

Result: 开发了一个支持人类-智能体和智能体-智能体交互模式的框架，提高了执行效率，使长轨迹智能体应用更易于管理和追踪。

Conclusion: AgentScope 1.0为构建可扩展、自适应和有效的智能体应用提供了实用基础，支持快速部署到生产环境。

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [92] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: 提出了Instruct-Verify-and-Act (IVA)框架，用于处理视觉-语言-动作模型中的错误前提指令，通过检测、澄清和执行三个步骤提升机器人对错误指令的响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在处理包含错误前提的自然语言指令时表现不佳，这些指令引用了环境中不存在的对象或条件，需要模型能够识别并适当响应这类错误请求。

Method: 构建大规模指令调优设置，使用结构化语言提示训练VLA模型。利用上下文增强的半合成数据集，包含配对的正确定位和错误前提指令，实现鲁棒的检测和自然语言纠正。

Result: IVA将错误前提检测准确率提高了97.56%，在错误前提场景中的成功响应率提高了50.78%。

Conclusion: IVA框架有效解决了VLA模型处理错误前提指令的问题，通过统一的检测-验证-执行流程显著提升了模型的鲁棒性和实用性。

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [93] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 提出了一种基于因果发现的深度学习框架，用于毫米波MIMO系统的波束对齐，通过因果特征选择显著减少输入选择时间和波束扫描开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的波束对齐方法忽视了输入输出之间的因果关系，导致可解释性差、泛化能力弱和波束扫描开销大。

Method: 提出两阶段因果波束选择算法：首先通过因果发现学习接收功率输入与最优波束之间的贝叶斯图，然后基于该图指导DL分类器的因果特征选择。

Result: 模拟结果显示，该方法在保持传统方法性能的同时，输入选择时间减少94.4%，波束扫描开销降低59.4%。

Conclusion: 因果感知的DL框架能够有效识别因果相关特征，显著提升波束对齐效率，为6G及以后通信系统提供快速、自适应且鲁棒的波束管理方案。

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [94] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: GLARE是一个基于代理的法律推理框架，通过动态调用不同模块获取关键法律知识，解决大语言模型在法律判决预测中因缺乏法律知识导致的推理不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在法律领域由于缺乏专业知识，存在推理能力不足的问题，影响了法律判决预测的准确性。

Method: 提出了GLARE框架，通过代理机制动态调用不同模块来获取关键法律知识，增强推理的广度和深度。

Result: 在真实数据集上的实验验证了该方法的有效性，生成的推理链提高了可解释性。

Conclusion: GLARE框架能够有效提升法律判决预测的推理能力，并为实际应用提供了可能性。

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [95] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: MoDER方法通过模块化框架训练多个文本专家，在推理时组合专家来合成改进的原型，从而增强视觉语言模型的零样本能力，在14个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练视觉语言模型具有零样本分类能力，但在下游任务与预训练领域差异较大时仍需微调。现有方法主要关注保持零样本能力，本文旨在将其转化为增强。

Method: 提出MoDular Embedding Recomposition (MoDER)方法，训练多个文本专家（每个专家专注于一个已见类别），存储在基础中心。推理时通过查询中心并组合检索到的专家来合成改进的原型。

Result: 在两个流行的零样本增量协议（Class-IL和MTIL）上验证，涵盖14个数据集，证明了方法的有效性。

Conclusion: MoDER方法成功地将零样本能力的保持转化为增强，通过模块化专家组合的方式提升了视觉语言模型在持续学习中的性能。

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [96] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 使用扩散模型进行神经符号学习，通过两阶段训练策略解决逻辑推理难题，在数独、迷宫等基准测试中表现出色


<details>
  <summary>Details</summary>
Motivation: 让神经网络学习复杂逻辑约束和进行符号推理是一个关键挑战，需要将神经网络的输出分布引导到更接近符号约束的方向

Method: 采用基于扩散模型的管道，使用两阶段训练策略：第一阶段培养基本推理能力，第二阶段系统学习逻辑约束。将扩散推理器建模为马尔可夫决策过程，使用改进的近端策略优化算法进行微调

Result: 在数独、迷宫、路径规划和偏好学习等经典符号推理基准测试中，该方法实现了出色的准确性和逻辑一致性

Conclusion: 扩散模型架构能够有效进行神经符号学习，解决逻辑推理问题，在神经网络中实现了优秀的逻辑约束满足能力

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [97] [Beyond Interpretability: Exploring the Comprehensibility of Adaptive Video Streaming through Large Language Models](https://arxiv.org/abs/2508.16448)
*Lianchen Jia,Chaoyang Li,Ziqi Yuan,Jiahui Chen,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.MM

TL;DR: ComTree是一个考虑可理解性的码率自适应算法生成框架，通过生成满足性能要求的决策树集合，并利用大语言模型评估开发者可理解性，最终选择最易人类理解和增强的解决方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法的黑盒特性使开发者难以理解决策过程和针对特定应用场景进行优化，现有研究通过决策树转换提高了可解释性，但可解释性并不等同于开发者的主观可理解性。

Method: 首先生成满足性能要求的完整决策树集合，然后利用大语言模型评估这些决策树对开发者的可理解性，最终选择最有利于人类理解和增强的解决方案。

Result: 实验结果表明，ComTree在保持竞争力的性能的同时显著提高了可理解性，显示出进一步发展的潜力。

Conclusion: ComTree框架成功解决了深度学习算法在自适应视频流中的可理解性问题，为开发者提供了更易理解和优化的解决方案，具有重要的实践价值和发展前景。

Abstract: Over the past decade, adaptive video streaming technology has witnessed
significant advancements, particularly driven by the rapid evolution of deep
learning techniques. However, the black-box nature of deep learning algorithms
presents challenges for developers in understanding decision-making processes
and optimizing for specific application scenarios. Although existing research
has enhanced algorithm interpretability through decision tree conversion,
interpretability does not directly equate to developers' subjective
comprehensibility. To address this challenge, we introduce \texttt{ComTree},
the first bitrate adaptation algorithm generation framework that considers
comprehensibility. The framework initially generates the complete set of
decision trees that meet performance requirements, then leverages large
language models to evaluate these trees for developer comprehensibility,
ultimately selecting solutions that best facilitate human understanding and
enhancement. Experimental results demonstrate that \texttt{ComTree}
significantly improves comprehensibility while maintaining competitive
performance, showing potential for further advancement. The source code is
available at https://github.com/thu-media/ComTree.

</details>


### [98] [Towards User-level QoE: Large-scale Practice in Personalized Optimization of Adaptive Video Streaming](https://arxiv.org/abs/2508.16454)
*Lianchen Jia,Chao Zhou,Chaoyang Li,Jiangchuan Liu,Lifeng Sun*

Main category: cs.MM

TL;DR: LingXi是一个基于用户体验的大规模个性化自适应视频流系统，通过分析用户参与度和退出率来动态优化算法参数，在快手平台上实现了观看时间、码率和卡顿时间的显著改善。


<details>
  <summary>Details</summary>
Motivation: 传统基于系统级QoS指标的优化方法在现代大规模流媒体系统中已接近性能极限，用户级体验质量(QoE)与算法优化目标的对齐仍是一个未解决的挑战。

Method: 使用退出率作为关键指标，基于生产环境日志分析QoS指标与退出率的关联性，开发个性化退出率预测器，通过蒙特卡洛采样和在线贝叶斯优化迭代确定最优参数。

Result: 在快手平台8%流量的大规模A/B测试中，LingXi实现了总观看时间增加0.15%、码率提升0.1%、所有用户卡顿时间减少1.3%，低带宽用户的卡顿时间更是减少了15%。

Conclusion: LingXi是首个基于用户级体验的大规模部署个性化自适应视频流系统，通过将用户参与度分析与算法优化相结合，显著提升了视频流服务的用户体验质量。

Abstract: Traditional optimization methods based on system-wide Quality of Service
(QoS) metrics have approached their performance limitations in modern
large-scale streaming systems. However, aligning user-level Quality of
Experience~(QoE) with algorithmic optimization objectives remains an unresolved
challenge. Therefore, we propose \texttt{LingXi}, the first large-scale
deployed system for personalized adaptive video streaming based on user-level
experience. \texttt{LingXi} dynamically optimizes the objectives of adaptive
video streaming algorithms by analyzing user engagement. Utilizing exit rate as
a key metric, we investigate the correlation between QoS indicators and exit
rates based on production environment logs, subsequently developing a
personalized exit rate predictor. Through Monte Carlo sampling and online
Bayesian optimization, we iteratively determine optimal parameters. Large-scale
A/B testing utilizing 8\% of traffic on Kuaishou, one of the largest short
video platforms, demonstrates \texttt{LingXi}'s superior performance.
\texttt{LingXi} achieves a 0.15\% increase in total viewing time, a 0.1\%
improvement in bitrate, and a 1.3\% reduction in stall time across all users,
with particularly significant improvements for low-bandwidth users who
experience a 15\% reduction in stall time.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [99] [Straggler-Resilient Federated Learning over A Hybrid Conventional and Pinching Antenna Network](https://arxiv.org/abs/2508.15821)
*Bibo Wu,Fang Fang,Ming Zeng,Xianbin Wang*

Main category: cs.IT

TL;DR: 通过混合缩放天线网络和深度强化学习优化，有效解决聚合学习中的"u6162行者"问题，提升通信效率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 聚合学习中存在"u6162行者"问题，影响学习效率。缩放天线可以动态建立强直视链路，但需要有效的网络结构和资源分配策略来充分发挥其优势。

Method: 提出混合缠细天线网络(HCPAN)，采用模糊逻辑客户端分类方案，并通过深度强化学习算法优化天线部署位置和资源分配，以最小化总体时间。

Result: 模拟结果验证了所提方案在提升聚合学习性能方面的优加性，通过优化缩放天线部署有效解决了通信效率问题。

Conclusion: 该研究成功地展示了缩放天线技术在聚合学习中的应用价值，通过智能的网络结构设计和资源优化策略，显著提升了系统的通信效率和学习性能。

Abstract: Leveraging pinching antennas in wireless network enabled federated learning
(FL) can effectively mitigate the common "straggler" issue in FL by dynamically
establishing strong line-of-sight (LoS) links on demand. This letter proposes a
hybrid conventional and pinching antenna network (HCPAN) to significantly
improve communication efficiency in the non-orthogonal multiple access
(NOMA)-enabled FL system. Within this framework, a fuzzy logic-based client
classification scheme is first proposed to effectively balance clients' data
contributions and communication conditions. Given this classification, we
formulate a total time minimization problem to jointly optimize pinching
antenna placement and resource allocation. Due to the complexity of variable
coupling and non-convexity, a deep reinforcement learning (DRL)-based algorithm
is developed to effectively address this problem. Simulation results validate
the superiority of the proposed scheme in enhancing FL performance via the
optimized deployment of pinching antenna.

</details>


### [100] [Tri-Hybrid Beamforming for Radiation-Center Reconfigurable Antenna Array: Spectral Efficiency and Energy Efficiency](https://arxiv.org/abs/2508.15924)
*Yinchen Li,Chenhao Qi,Shiwen Mao,Octavia A. Dobre*

Main category: cs.IT

TL;DR: 基于辐射中心可重配天线数组的三混合放形架构，包含数字、模拟和电磁放形，通过三循环交替优化算法实现频谱效率和能源效率最大化。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用可重配天线数组的灵活性，提高空口质量和能源效率，应对硬件约束和功耗问题。

Method: 提出三混合放形架构，包含数字、模拟和电磁放形。使用三循环交替优化算法：内循环和中循环用惩罚对偶分解优化数字和模拟放形器，外循环用坐标下降法选择辐射中心。对于能源效率优化，提出双二次变换分数规划算法，并为降低复杂度提出拉格朗日对偶变换算法。

Result: 模拟结果证明辐射中心可重配天线数组在提高频谱效率和能源效率方面具有巨大潜力。拉格朗日对偶变换算法在只有轻微性能损失的情况下显著降低了计算复杂度。

Conclusion: 该三混合放形方案通过结合不同级别的放形技术，有效提高了系统性能。拉格朗日对偶变换算法为实际应用提供了高效的计算解决方案。

Abstract: In this paper, we propose a tri-hybrid beamforming (THBF) architecture based
on the radiation-center (RC) reconfigurable antenna array (RCRAA), including
the digital beamforming, analog beamforming, and electromagnetic (EM)
beamforming, where the EM beamformer design is modeled as RC selection. Aiming
at spectral efficiency (SE) maximization subject to the hardware and power
consumption constraints, we propose a tri-loop alternating optimization (TLAO)
scheme for the THBF design, where the digital and analog beamformers are
optimized based on the penalty dual decomposition in the inner and middle
loops, and the RC selection is determined through the coordinate descent method
in the outer loop. Aiming at energy-efficiency (EE) maximization, we develop a
dual quadratic transform-based fractional programming (DQTFP) scheme, where the
TLAO scheme is readily used for the THBF design. To reduce the computational
complexity, we propose the Lagrange dual transform-based fractional programming
(LDTFP) scheme, where each iteration has a closed-form solution. Simulation
results demonstrate the great potential of the RCRAA in improving both SE and
EE. Compared to the DQTFP scheme, the LDTFP scheme significantly reduces the
computational complexity with only minor performance loss.

</details>


### [101] [Multi-User SLNR-Based Precoding With Gold Nanoparticles in Vehicular VLC Systems](https://arxiv.org/abs/2508.16075)
*Geonho Han,Hyuckjin Choi,Hyesang Cho,Jeong Hyeon Han,Ki Tae Nam,Junil Choi*

Main category: cs.IT

TL;DR: 通过金纳米粒子的旋光特性降低LED相关性，采用SLNR预编码和RGB比例优化，提高车辆可见光通信系统的多用户性能和安全性


<details>
  <summary>Details</summary>
Motivation: 车辆可见光通信(VVLC)系统中LED灯之间存在高相关性，影响空间复用和数据速率提升

Method: 利用金纳米粒子的旋光特性降低LED相关性，采用SLNR基础的预编码支持多用户，通过广义Rayleigh比和逐步凸近似优化RGB比例

Result: 模拟结果显示，优化后的SLNR预编码在多用户车辆环境中显著提高了总速率，在监听场景中提高了保密速率

Conclusion: LED去相关性和RGB比例优化对提升VVLC系统性能至关重要，金纳米粒子技术为解决车辆可见光通信挑战提供了有效方案

Abstract: Visible spectrum is an emerging frontier in wireless communications for
enhancing connectivity and safety in vehicular environments. The vehicular
visible light communication (VVLC) system is a key feature in leveraging
existing infrastructures, but it still has several critical challenges.
Especially, VVLC channels are highly correlated due to the small gap between
light emitting diodes (LEDs) in each headlight, making it difficult to increase
data rates by spatial multiplexing. In this paper, we exploit recently
synthesized gold nanoparticles (GNPs) to reduce the correlation between LEDs,
i.e., the chiroptical properties of GNPs for differential absorption depending
on the azimuth angle of incident light are used to mitigate the LED
correlation. In addition, we adopt a signal-to-leakage-plus-noise ratio
(SLNR)-based precoder to support multiple users. The ratio of RGB light sources
in each LED also needs to be optimized to maximize the sum SLNR satisfying a
white light constraint for illumination since the GNPs can vary the color of
transmitted light by the differential absorption across wavelength. The
nonconvex optimization problems for precoders and RGB ratios can be solved by
the generalized Rayleigh quotient with the approximated shot noise and
successive convex approximation (SCA). The simulation results show that the
SLNR-based precoder with the optimized RGB ratios significantly improves the
sum rate in a multi-user vehicular environment and the secrecy rate in a
wiretapping scenario. The proposed SLNR-based precoding verifies that the
decorrelation between LEDs and the RGB ratio optimization are essential to
enhance the VVLC performance.

</details>


### [102] [Implicit and Explicit Formulas of the Joint RDF for a Tuple of Multivariate Gaussian Sources with Individual Square-Error Distortions](https://arxiv.org/abs/2508.16301)
*Evagoras Stylianou,Charalambos D. Charalambous,Themistoklis Charalambous*

Main category: cs.IT

TL;DR: 本文提出了相关多元高斯源在个体平方误差失真下的联合率失真函数的闭式解，特别针对对称失真情况给出了显式表达式


<details>
  <summary>Details</summary>
Motivation: 分析相关多元高斯源在个体平方误差失真下的联合率失真函数，为多变量高斯源的率失真理论提供闭式解

Method: 利用Hotelling的典型变量形式，推导非线性方程组来描述联合率失真函数，对于对称失真情况使用两个注水变量进行显式表达

Result: 获得了联合率失真函数的闭式表征，特别在对称失真情况下得到了简洁的显式表达式

Conclusion: 该结果显著提升了对多元高斯源联合率失真函数的理解，并推动了闭式解的发展

Abstract: This paper analyzes the joint Rate Distortion Function (RDF) of correlated
multivariate Gaussian sources with individual square-error distortions.
Leveraging Hotelling's canonical variable form, presented is a closed-form
characterization of the joint RDF, that involves {a system of nonlinear
equations. Furthermore, for the special case of symmetric distortions (i.e.,
equal distortions), the joint RDF is explicitly expressed in terms of} two
water-filling variables. The results greatly improve our understanding and
advance the development of closed-form solutions of the joint RDF for
multivariate Gaussian sources with individual square-error distortions.

</details>


### [103] [Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude Economy Networks](https://arxiv.org/abs/2508.16379)
*Feibo Jiang,Li Dong,Xitao Pan,Kezhi Wang,Cunhua Pan*

Main category: cs.IT

TL;DR: 提出ARMAIT框架，结合Agentic RAG和MAIT混合神经网络，用于多无人机轨迹优化，通过T-GRPO方法实现离散和连续轨迹空间的统一策略优化。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机轨迹优化问题，需要能够自主解释高级任务需求并识别关键优化组件的智能系统，同时需要高效处理不同系统规模的建模需求。

Method: 基于LLM构建框架，集成Agentic RAG和无人机专用知识库；提出MAIT混合架构结合注意力机制和Mamba的高效时序表示；开发T-GRPO方法进行统一策略梯度优化。

Result: 大量实验结果验证了ARMAIT框架的可行性和有效性。

Conclusion: 该框架成功实现了多无人机轨迹的智能优化，结合了大型语言模型的解释能力和混合神经网络的高效建模能力。

Abstract: This paper proposes a novel Agentic Retrieval-augmented generation with
Mamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned
Aerial Vehicle (UAV) trajectory optimization. The framework is built upon Large
Language Models (LLMs), incorporating Retrieval-Augmented Generation (RAG)
empowered by Agentic AI and integrated with a UAV-specific knowledge base.
Through the Agentic RAG, the LLM autonomously interprets high-level task
requirements and identifies the key components necessary for trajectory
optimization, including model inputs and outputs, network architecture, reward
functions, and task constraints. To support efficient modeling across different
system scales, we introduce the Mamba-Attention Integrated Transformer (MAIT),
a hybrid neural architecture that combines the long-range dependency modeling
capability of attention mechanisms with the efficient temporal dynamic
representation of Mamba. Furthermore, a Trajectory-Group Relative Policy
Optimization (T-GRPO) method is proposed to achieve unified policy gradient
optimization in both discrete and continuous trajectory spaces for MAIT
training. Extensive experimental results validate the feasibility and
effectiveness of the proposed ARMAIT framework.

</details>


### [104] [Enhanced Successive Cancellation List Decoder for Long Polar Codes Targeting 6G Air Interface](https://arxiv.org/abs/2508.16498)
*Jiajie Li,Sihui Shen,Warren J. Gross*

Main category: cs.IT

TL;DR: 本文提出了一系列算法技术，包括PE SCL、BE SCL、BE GPSCL和IDA解码方法，用于降低长极化码的内存使用和计算复杂度，以满足6G通信标准的性能要求。


<details>
  <summary>Details</summary>
Motivation: 为满足6G通信标准对能源和面积成本降低的要求，需要解决长极化码在连续列表解码中的内存使用和计算复杂度挑战。

Method: 提出了四种改进方法：1、干扰增强(PE)SCL解码器；2、偏置增强(BE)SCL解码器；3、BE广义分区(SPSCL)解码器；4、输入分布感知(IDA)解码技术。

Result: PE SCL解码器用L列大小达到2L的解码性能；BE GPSCL解码器降低67%内存使用；IDA技术实现5.4倍计算复杂度降低，性能下降仅0.05dB以内。

Conclusion: 所提出的算法技术能够有效降低长极化码的实现成本，在保持解码性能的同时显著减少内存和计算资源需求，适用于未来6G通信标准。

Abstract: The 6th generation communication standard's air interface requires innovation
in channel coding to fulfill anticipated energy and area cost reduction
requirements. In this paper, we propose algorithmic techniques to enable the
implementation of long polar codes (e.g., length 8K bits) in next-generation
communications standards by addressing key challenges in memory usage and
computational complexity presented by successive decoding list (SCL) polar
decoding. Perturbation-enhanced (PE) successive cancelation list (SCL) decoders
with a list size of $L$ reach the decoding performance of the SCL decoder with
a list size of $2L$. The proposed bias-enhanced (BE) SCL decoders, which
simplifies the PE SCL decoder based on insights gained by an ablation study,
returns similar decoding performance to PE SCL decoders. Also, proposed BE
generalized partitioned SCL (GPSCL) decoders with a list size of $8$ have a
$67\%$ reduction in the memory usage and similar decoding performance compared
to SCL decoders with a list size of $16$. Furthermore, input-distribution-aware
(IDA) decoding is applied to BE GPSCL decoders. Up to $5.4\times$ reduction in
the computational complexity is achieved compared to SCL decoders with a list
size of $16$. The degraded decoding performance is at most $0.05\text{ dB}$
compared to BE GPSCL decoders without IDA decoding.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [105] [Estimating the Effective Topics of Articles and journals Abstract Using LDA And K-Means Clustering Algorithm](https://arxiv.org/abs/2508.16046)
*Shadikur Rahman,Umme Ayman Koana,Aras M. Ismael,Karmand Hussein Abdalla*

Main category: cs.IR

TL;DR: 本研究利用LDA、K-Means聚类和WordNet进行文本关键词提取，为学术文献搜索提供更准确的搜索字符串生成方案


<details>
  <summary>Details</summary>
Motivation: 解决文本文档数量快速增长带来的信息组织和搜索困难，通过主题建模和文本聚类技术提升文档管理效率

Method: 结合使用LDA主题建模模型、K-Means聚类算法以及WordNet词汇数据库进行关键词提取

Result: K-Means聚类和LDA算法在文本关键词提取任务中表现出最可靠的性能

Conclusion: 该研究方法能够有效帮助研究人员生成准确的搜索字符串，避免学术文献搜索中的误解

Abstract: Analyzing journals and articles abstract text or documents using topic
modelling and text clustering has become a modern solution for the increasing
number of text documents. Topic modelling and text clustering are both
intensely involved tasks that can benefit one another. Text clustering and
topic modelling algorithms are used to maintain massive amounts of text
documents. In this study, we have used LDA, K-Means cluster and also lexical
database WordNet for keyphrases extraction in our text documents. K-Means
cluster and LDA algorithms achieve the most reliable performance for keyphrase
extraction in our text documents. This study will help the researcher to make a
search string based on journals and articles by avoiding misunderstandings.

</details>


### [106] [Hierarchical Vision-Language Reasoning for Multimodal Multiple-Choice Question Answering](https://arxiv.org/abs/2508.16148)
*Ao Zhou,Zebo Gu,Tenghao Sun,Jiawen Chen,Mingsheng Tu,Zifeng Cheng,Yafeng Yin,Zhiwei Jiang,Qing Gu*

Main category: cs.IR

TL;DR: 提出了一种结合多模态分层推理和检索优化的日本PDF文档理解框架，通过子问题分解的语义验证策略显著提升复杂文档的深度语义解析能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在多选问答任务中处理复杂布局PDF文档时存在局限性，且对英语训练数据有强烈偏见，导致日语等语言场景性能不佳

Method: 结合多模态分层推理机制和Colqwen优化的检索方法，创新性地引入通过子问题分解的语义验证策略

Result: 实验结果表明该框架显著增强了模型对复杂文档的深度语义解析能力，并在实际应用场景中表现出优越的鲁棒性

Conclusion: 该框架有效解决了多语言PDF文档理解中的挑战，为复杂布局文档的多模态理解提供了有效解决方案

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
multimodal understanding capabilities in Visual Question Answering (VQA) tasks
by integrating visual and textual features. However, under the challenging
ten-choice question evaluation paradigm, existing methods still exhibit
significant limitations when processing PDF documents with complex layouts and
lengthy content. Notably, current mainstream models suffer from a strong bias
toward English training data, resulting in suboptimal performance for Japanese
and other language scenarios. To address these challenges, this paper proposes
a novel Japanese PDF document understanding framework that combines multimodal
hierarchical reasoning mechanisms with Colqwen-optimized retrieval methods,
while innovatively introducing a semantic verification strategy through
sub-question decomposition. Experimental results demonstrate that our framework
not only significantly enhances the model's deep semantic parsing capability
for complex documents, but also exhibits superior robustness in practical
application scenarios.

</details>


### [107] [Similarity-Based Supervised User Session Segmentation Method for Behavior Logs](https://arxiv.org/abs/2508.16106)
*Yongzhi Jin,Kazushi Okamoto,Kei Harada,Atsushi Shibata,Koki Karube*

Main category: cs.IR

TL;DR: 提出基于相似性特征的监督式会话分割方法，使用物品共现嵌入、文本嵌入和价格特征，通过LightGBM等分类器预测会话边界，在真实数据集上取得良好效果


<details>
  <summary>Details</summary>
Motivation: 用户兴趣在会话内可能发生变化，需要合适的会话分割来建模动态行为

Method: 使用固定窗口内的物品相似性特征（共现嵌入、文本嵌入、价格），训练监督分类器预测会话边界

Result: LightGBM模型表现最佳，F1分数0.806，PR-AUC 0.831

Conclusion: 该方法能有效进行会话分割并捕捉动态用户行为

Abstract: In information recommendation, a session refers to a sequence of user actions
within a specific time frame. Session-based recommender systems aim to capture
short-term preferences and generate relevant recommendations. However, user
interests may shift even within a session, making appropriate segmentation
essential for modeling dynamic behaviors. In this study, we propose a
supervised session segmentation method based on similarity features derived
from action embeddings and attributes. We compute the similarity scores between
items within a fixed-size window around each candidate segmentation point,
using four types of features: item co-occurrence embeddings, text embeddings of
titles and brands, and price. These features are used to train supervised
classifiers (LightGBM, XGBoost, CatBoost, support vector machine, and logistic
regression) to predict the session boundaries. We construct a manually
annotated dataset from real user browsing histories and evaluate the
segmentation performance using F1-score, area under the precision-recall curve
(PR-AUC), and area under the receiver operating characteristic curve. The
LightGBM model achieves the best performance, with an F1-score of 0.806 and a
PR-AUC of 0.831. These results demonstrate the effectiveness of the proposed
method for session segmentation and its potential to capture dynamic user
behaviors.

</details>


### [108] [Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation](https://arxiv.org/abs/2508.16126)
*Haitao Lin,Zhen Yang,Jiawei Xue,Ziji Zhang,Luzhu Wang,Yikun Gu,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: Spacetime-GR是首个面向大规模在线POI推荐任务的时空感知生成模型，通过创新的时空编码和地理感知索引策略，解决了传统生成推荐在时空敏感场景中的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐在视频和商品推荐中表现出色，但在POI推荐中，用户偏好受时空变化影响显著，传统方法难以有效处理这种时空敏感性，因此需要开发专门的时空感知生成模型。

Method: 提出地理感知的分层POI索引策略处理大规模词汇建模；设计新颖的时空编码模块将时空上下文融入用户行为序列；引入多模态POI嵌入增强语义理解；开发后训练适配策略支持多种输出格式。

Result: 在公开基准数据集和工业级大规模数据集上均表现出优于现有方法的性能，在POI推荐准确性和排序质量方面均有显著提升，并成功部署到数亿级POI和用户的在线服务中。

Conclusion: Spacetime-GR成功将生成模型的序列建模能力扩展到时空敏感的POI推荐领域，通过创新的时空信息编码和多模态融合，实现了大规模在线POI推荐的高效部署和优异性能。

Abstract: Building upon the strong sequence modeling capability, Generative
Recommendation (GR) has gradually assumed a dominant position in the
application of recommendation tasks (e.g., video and product recommendation).
However, the application of Generative Recommendation in Point-of-Interest
(POI) recommendation, where user preferences are significantly affected by
spatiotemporal variations, remains a challenging open problem. In this paper,
we propose Spacetime-GR, the first spacetime-aware generative model for
large-scale online POI recommendation. It extends the strong sequence modeling
ability of generative models by incorporating flexible spatiotemporal
information encoding. Specifically, we first introduce a geographic-aware
hierarchical POI indexing strategy to address the challenge of large vocabulary
modeling. Subsequently, a novel spatiotemporal encoding module is introduced to
seamlessly incorporate spatiotemporal context into user action sequences,
thereby enhancing the model's sensitivity to spatiotemporal variations.
Furthermore, we incorporate multimodal POI embeddings to enrich the semantic
understanding of each POI. Finally, to facilitate practical deployment, we
develop a set of post-training adaptation strategies after sufficient
pre-training on action sequences. These strategies enable Spacetime-GR to
generate outputs in multiple formats (i.e., embeddings, ranking scores and POI
candidates) and support a wide range of downstream application scenarios (i.e.,
ranking and end-to-end recommendation). We evaluate the proposed model on both
public benchmark datasets and large-scale industrial datasets, demonstrating
its superior performance over existing methods in terms of POI recommendation
accuracy and ranking quality. Furthermore, the model is the first generative
model deployed in online POI recommendation services that scale to hundreds of
millions of POIs and users.

</details>


### [109] [Cross-Modal Prototype Augmentation and Dual-Grained Prompt Learning for Social Media Popularity Prediction](https://arxiv.org/abs/2508.16147)
*Ao Zhou,Mingsheng Tu,Luping Wang,Tenghao Sun,Zifeng Cheng,Yafeng Yin,Zhiwei Jiang,Qing Gu*

Main category: cs.IR

TL;DR: 提出多类框架，通过层次原型和对比学习改进视觉-文本对齐，结合双粒度提示学习和跨模态注意力机制，实现最先进的多模态社交媒体流行度预测性能


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体流行度预测方法存在视觉-文本对齐不足、无法捕捉跨内容相关性和层次模式的问题，需要更有效的多模态整合方法

Method: 建立多类框架，引入层次原型进行结构增强，使用对比学习改进视觉-文本对齐，整合双粒度提示学习和跨模态注意力机制进行细粒度类别建模

Result: 在基准指标上实现了最先进的性能，为多模态社交媒体分析建立了新的参考标准

Conclusion: 所提出的方法有效解决了多模态社交媒体数据中的对齐和层次模式捕捉问题，显著提升了流行度预测的准确性

Abstract: Social Media Popularity Prediction is a complex multimodal task that requires
effective integration of images, text, and structured information. However,
current approaches suffer from inadequate visual-textual alignment and fail to
capture the inherent cross-content correlations and hierarchical patterns in
social media data. To overcome these limitations, we establish a multi-class
framework , introducing hierarchical prototypes for structural enhancement and
contrastive learning for improved vision-text alignment. Furthermore, we
propose a feature-enhanced framework integrating dual-grained prompt learning
and cross-modal attention mechanisms, achieving precise multimodal
representation through fine-grained category modeling. Experimental results
demonstrate state-of-the-art performance on benchmark metrics, establishing new
reference standards for multimodal social media analysis.

</details>


### [110] [EGRA:Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation](https://arxiv.org/abs/2508.16170)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Yongjie Wang,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: EGRA是一个多模态推荐系统，通过预训练模型构建物品关系图来平衡协同模式和模态感知相似性，并引入双层动态对齐机制来改善模态-行为表示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐方法存在两个关键限制：1) 使用原始模态特征构建物品关系图时未能有效平衡协同信号和模态语义，且容易受到模态噪声影响；2) 使用统一的对齐权重和固定对齐强度，限制了模态-行为对齐效果。

Method: EGRA采用预训练MMR模型生成表示来构建物品关系图，增强行为图的丰富性和抗噪性。同时提出双层动态对齐权重机制，根据实体对齐程度动态分配对齐强度，并在训练过程中逐步增加整体对齐强度。

Result: 在五个数据集上的大量实验表明，EGRA显著优于现有方法，证实了其有效性。

Conclusion: EGRA通过改进的物品关系图构建方法和动态对齐机制，有效解决了多模态推荐中的稀疏性和对齐问题，提升了推荐质量。

Abstract: MultiModal Recommendation (MMR) systems have emerged as a promising solution
for improving recommendation quality by leveraging rich item-side modality
information, prompting a surge of diverse methods. Despite these advances,
existing methods still face two critical limitations. First, they use raw
modality features to construct item-item links for enriching the behavior
graph, while giving limited attention to balancing collaborative and
modality-aware semantics or mitigating modality noise in the process. Second,
they use a uniform alignment weight across all entities and also maintain a
fixed alignment strength throughout training, limiting the effectiveness of
modality-behavior alignment. To address these challenges, we propose EGRA.
First, instead of relying on raw modality features, it alleviates sparsity by
incorporating into the behavior graph an item-item graph built from
representations generated by a pretrained MMR model. This enables the graph to
capture both collaborative patterns and modality aware similarities with
enhanced robustness against modality noise. Moreover, it introduces a novel
bi-level dynamic alignment weighting mechanism to improve modality-behavior
representation alignment, which dynamically assigns alignment strength across
entities according to their alignment degree, while gradually increasing the
overall alignment intensity throughout training. Extensive experiments on five
datasets show that EGRA significantly outperforms recent methods, confirming
its effectiveness.

</details>


### [111] [Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings](https://arxiv.org/abs/2508.16210)
*Ziyin Xiao,Toyotaro Suzumura*

Main category: cs.IR

TL;DR: DUP-OT是一个用于非重叠跨域推荐的框架，通过高斯混合模型表示用户偏好，并使用最优传输进行跨域对齐，有效解决了无重叠用户/物品时的知识迁移问题。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐系统需要重叠用户或物品作为桥梁，这在现实场景中往往不现实。非重叠跨域推荐面临两大挑战：缺乏直接连接桥梁和领域间分布差异大。同时，现有方法用离散向量表示用户偏好，无法捕捉细粒度的多面性特征。

Method: DUP-OT框架包含三个阶段：1)共享预处理：使用基于评论的嵌入和自编码器编码两个领域的用户和物品；2)用户GMM权重学习：将用户偏好建模为高斯混合模型并学习权重；3)跨域评分预测：通过最优传输对齐高斯分量，实现从源域到目标域的偏好迁移。

Result: 在Amazon评论数据集上的实验表明，DUP-OT能够有效缓解领域差异，在非重叠跨域推荐设置下优于最先进的基线方法。

Conclusion: DUP-OT通过分布式的用户偏好表示和最优传输技术，成功解决了非重叠跨域推荐的关键挑战，为现实场景中的推荐系统提供了有效的知识迁移方案。

Abstract: Cross-Domain Recommender (CDR) systems aim to transfer knowledge from dense
to sparse domains, alleviating data sparsity and cold-start issues in
single-domain recommendation. While many methods assume overlapping users or
items to connect domains, this is often unrealistic in real-world settings.
Thus, non-overlapping CDR systems, which require no shared users or items, are
needed.
  However, non-overlapping CDR is challenging due to: (1) the absence of
overlap preventing direct bridges between domains, and (2) large distributional
discrepancies degrading transfer performance. Moreover, most recommenders
represent user preferences as discrete vectors, failing to capture their
fine-grained, multi-faceted nature.
  We propose DUP-OT (Distributional User Preferences with Optimal Transport), a
framework for non-overlapping CDR. DUP-OT has three stages: (1) Shared
Preprocessing, where review-based embeddings and an autoencoder encode users
and items from both domains; (2) User GMM Weight Learning, which models user
preferences as Gaussian mixtures with learned weights; and (3) Cross-domain
Rating Prediction, where optimal transport aligns Gaussian components across
domains, enabling preference transfer from source to target.
  Experiments on Amazon review datasets show that DUP-OT effectively mitigates
domain discrepancy and outperforms state-of-the-art baselines under the
non-overlapping CDR setting.

</details>


### [112] [OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval](https://arxiv.org/abs/2508.16438)
*Yu Liu,Yanbing Liu,Fangfang Yuan,Cong Cao,Youbang Sun,Kun Peng,WeiZhuo Chen,Jianjun Li,Zhiyuan Ma*

Main category: cs.IR

TL;DR: OPERA是一个新颖的推理驱动检索框架，通过目标规划模块和推理执行模块解决复杂多跳检索任务中的规划、检索和过滤问题，使用MAPGRPO方法训练，在复杂基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在复杂推理导向的多跳检索任务中面临三大挑战：推理导向规划无效、推理驱动检索次优、推理引导过滤不足，这些问题的根本原因在于检索与推理之间的弱耦合。

Method: 提出OPERA架构，包含目标规划模块(GPM)将问题分解为子目标，推理执行模块(REM)执行精确推理和有效检索。使用新颖的MAPGRPO(多智能体渐进组相对策略优化)方法进行训练。

Result: 在复杂多跳基准测试中表现出优越性能，验证了MAPGRPO方法和OPERA设计的有效性。

Conclusion: OPERA框架通过强化检索与推理的耦合关系，有效解决了复杂多跳检索任务中的关键挑战，为推理驱动检索提供了新的解决方案。

Abstract: Recent advances in large language models (LLMs) and dense retrievers have
driven significant progress in retrieval-augmented generation (RAG). However,
existing approaches face significant challenges in complex reasoning-oriented
multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior
methods struggle to generate robust multi-step plans for complex queries, as
rule-based decomposers perform poorly on out-of-template questions. 2)
Suboptimal reasoning-driven retrieval: Related methods employ limited query
reformulation, leading to iterative retrieval loops that often fail to locate
golden documents. 3) Insufficient reasoning-guided filtering: Prevailing
methods lack the fine-grained reasoning to effectively filter salient
information from noisy results, hindering utilization of retrieved knowledge.
Fundamentally, these limitations all stem from the weak coupling between
retrieval and reasoning in current RAG architectures. We introduce the
Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel
reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)
decomposes questions into sub-goals, which are executed by a Reason-Execute
Module (REM) with specialized components for precise reasoning and effective
retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative
Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex
multi-hop benchmarks show OPERA's superior performance, validating both the
MAPGRPO method and OPERA's design. Code is available at
https://github.com/Ameame1/OPERA.

</details>


### [113] [A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering](https://arxiv.org/abs/2508.16516)
*Lin Li,Chunyang Li,Yu Yin,Xiaohui Tao,Jianwei Zhang*

Main category: cs.IR

TL;DR: GNAQ是一种基于图结构的动态量化训练方法，通过节点感知的动态量化策略和关系感知梯度估计，在保持GNN推荐性能的同时大幅减少模型大小和计算成本。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在推荐系统中表现出色，但在资源受限的边缘设备上部署时面临高嵌入参数和计算成本的挑战。传统量化方法忽略图结构，导致消息传递中的误差累积和量化嵌入质量下降。

Method: 提出GNAQ方法：1）基于节点特征分布初始化量化区间；2）通过GNN层消息传递动态细化量化尺度；3）使用图关系感知梯度估计替代传统直通估计器；4）适应个体节点嵌入的节点感知动态量化策略。

Result: 在四个真实数据集上，GNAQ在2位量化下比最先进方法平均提升27.8% Recall@10和17.6% NDCG@10。能保持全精度模型性能的同时将模型大小减少8-12倍，训练速度比基线量化方法快两倍。

Conclusion: GNAQ通过利用图结构信息进行动态量化，有效解决了GNN在推荐系统中的部署效率问题，在保持准确性的同时显著提升了计算效率和模型压缩效果。

Abstract: In the realm of collaborative filtering recommendation systems, Graph Neural
Networks (GNNs) have demonstrated remarkable performance but face significant
challenges in deployment on resource-constrained edge devices due to their high
embedding parameter requirements and computational costs. Using common
quantization method directly on node embeddings may overlooks their graph based
structure, causing error accumulation during message passing and degrading the
quality of quantized embeddings.To address this, we propose Graph based
Node-Aware Dynamic Quantization training for collaborative filtering (GNAQ), a
novel quantization approach that leverages graph structural information to
enhance the balance between efficiency and accuracy of GNNs for Top-K
recommendation. GNAQ introduces a node-aware dynamic quantization strategy that
adapts quantization scales to individual node embeddings by incorporating graph
interaction relationships. Specifically, it initializes quantization intervals
based on node-wise feature distributions and dynamically refines them through
message passing in GNN layers. This approach mitigates information loss caused
by fixed quantization scales and captures hierarchical semantic features in
user-item interaction graphs. Additionally, GNAQ employs graph relation-aware
gradient estimation to replace traditional straight-through estimators,
ensuring more accurate gradient propagation during training. Extensive
experiments on four real-world datasets demonstrate that GNAQ outperforms
state-of-the-art quantization methods, including BiGeaR and N2UQ, by achieving
average improvement in 27.8\% Recall@10 and 17.6\% NDCG@10 under 2-bit
quantization. In particular, GNAQ is capable of maintaining the performance of
full-precision models while reducing their model sizes by 8 to 12 times; in
addition, the training time is twice as fast compared to quantization baseline
methods.

</details>


### [114] [Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis](https://arxiv.org/abs/2508.16550)
*Nirmal Gaud,Prasad Krishna Murthy,Mostaque Md. Morshedur Hassan,Abhijit Ganguly,Vinay Mali,Ms Lalita Bhagwat Randive,Abhaypratap Singh*

Main category: cs.IR

TL;DR: Enhanced NIRMAL优化器通过引入阻尼Nesterov加速机制，在保持原有策略基础上提升了收敛稳定性，在CIFAR-100等复杂数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 改进原始NIRMAL优化器，通过阻尼Nesterov加速机制来提升收敛稳定性，同时保留原有的国际象棋启发的梯度下降策略

Method: 引入(α, r)-阻尼Nesterov加速机制，结合梯度下降、动量、随机扰动、自适应学习率和非线性变换等策略，在MNIST、FashionMNIST、CIFAR-10和CIFAR-100数据集上使用定制CNN架构进行测试

Result: 在CIFAR-100上达到46.06%测试准确率和最低测试损失1.960435，超越原始NIRMAL的44.34%，接近SGD with Momentum的46.43%

Conclusion: Enhanced NIRMAL在复杂数据集上展现出优异的泛化能力和稳定性，是有效的优化器改进方案

Abstract: This study introduces the Enhanced NIRMAL (Novel Integrated Robust
Multi-Adaptation Learning with Damped Nesterov Acceleration) optimizer, an
improved version of the original NIRMAL optimizer. By incorporating an
$(\alpha, r)$-damped Nesterov acceleration mechanism, Enhanced NIRMAL improves
convergence stability while retaining chess-inspired strategies of gradient
descent, momentum, stochastic perturbations, adaptive learning rates, and
non-linear transformations.
  We evaluate Enhanced NIRMAL against Adam, SGD with Momentum, Nesterov, and
the original NIRMAL on four benchmark image classification datasets: MNIST,
FashionMNIST, CIFAR-10, and CIFAR-100, using tailored convolutional neural
network (CNN) architectures.
  Enhanced NIRMAL achieves a test accuracy of 46.06\% and the lowest test loss
(1.960435) on CIFAR-100, surpassing the original NIRMAL (44.34\% accuracy) and
closely rivaling SGD with Momentum (46.43\% accuracy). These results underscore
Enhanced NIRMAL's superior generalization and stability, particularly on
complex datasets.

</details>


### [115] [ORCA: Mitigating Over-Reliance for Multi-Task Dwell Time Prediction with Causal Decoupling](https://arxiv.org/abs/2508.16573)
*Huishi Luo,Fuzhen Zhuang,Yongchun Zhu,Yiqing Wu,Bo Kang,Ruobing Xie,Feng Xia,Deqing Wang,Jin Dong*

Main category: cs.IR

TL;DR: ORCA通过因果解耦方法解决多任务学习中停留时间预测对CTR的过度依赖问题，有效改善中等停留时间的预测偏差


<details>
  <summary>Details</summary>
Motivation: 发现多任务学习模型在停留时间预测中系统性地将预测结果偏向最短和最长时间段，低估中等持续时间，这是由于过度依赖CTR与DT之间的伪相关

Method: 提出ORCA框架，采用因果解耦方法：1）显式建模并减去CTR的负迁移，保留正迁移；2）特征级反事实干预；3）任务交互模块与实例逆加权，削弱CTR介导效应，恢复直接DT语义

Result: 实验显示DT指标平均提升10.6%，且不损害CTR性能

Conclusion: ORCA是模型无关的易部署解决方案，能有效缓解多任务学习中对CTR的过度依赖问题，提升停留时间预测准确性

Abstract: Dwell time (DT) is a critical post-click metric for evaluating user
preference in recommender systems, complementing the traditional click-through
rate (CTR). Although multi-task learning is widely adopted to jointly optimize
DT and CTR, we observe that multi-task models systematically collapse their DT
predictions to the shortest and longest bins, under-predicting the moderate
durations. We attribute this moderate-duration bin under-representation to
over-reliance on the CTR-DT spurious correlation, and propose ORCA to address
it with causal-decoupling. Specifically, ORCA explicitly models and subtracts
CTR's negative transfer while preserving its positive transfer. We further
introduce (i) feature-level counterfactual intervention, and (ii) a
task-interaction module with instance inverse-weighting, weakening CTR-mediated
effect and restoring direct DT semantics. ORCA is model-agnostic and easy to
deploy. Experiments show an average 10.6% lift in DT metrics without harming
CTR. Code is available at
https://github.com/Chrissie-Law/ORCA-Mitigating-Over-Reliance-for-Multi-Task-Dwell-Time-Prediction-with-Causal-Decoupling.

</details>
