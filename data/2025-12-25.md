<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints](https://arxiv.org/abs/2512.20781)
*Youjin Jung,Seongwoo Cho,Hyun-seok Min,Sungchul Choi*

Main category: cs.IR

TL;DR: SoFT提出了一种无需训练、即插即用的过滤模块，用于零样本组合图像检索，通过提取规定性和禁止性约束来重新排序结果，并构建了多目标数据集进行更全面的评估。


<details>
  <summary>Details</summary>
Motivation: 现有零样本组合图像检索方法通常使用单一融合查询，容易稀释关键信息且无法处理用户想要避免的内容。此外，当前基准测试假设每个查询只有一个正确答案，忽略了修改文本的模糊性。

Method: SoFT利用多模态大语言模型从参考图像-修改文本对中提取两种互补约束：规定性（必须包含）和禁止性（必须避免）约束，作为语义过滤器来奖励或惩罚候选图像以重新排序结果。同时构建两阶段数据集管道，创建多目标三元组并重写修改文本以提高评估精度。

Result: 在CIReVL检索器基础上应用SoFT，在CIRR数据集上R@5提升至65.25（+12.94），在CIRCO数据集上mAP@50提升至27.93（+6.13），在FashionIQ数据集上R@50提升至58.44（+4.59），显示出广泛的有效性。

Conclusion: SoFT通过提取互补约束和构建更全面的评估框架，有效解决了零样本组合图像检索中的信息稀释和模糊性问题，显著提升了检索性能。

Abstract: Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL, a ZS-CIR retriever, SoFT raises R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.

</details>


### [2] [MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model](https://arxiv.org/abs/2512.20916)
*Haoyu Wang,Yitong Wang,Jining Wang*

Main category: cs.IR

TL;DR: 提出MMSRARec方法，通过多模态大语言模型总结物品为关键词，结合协同信号，实现高效、可解释的多模态序列推荐


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在推荐系统应用中存在三个主要问题：1）现有方法生成的物品表示可解释性有限，难以迁移到基于语言模型的推荐系统；2）将用户行为序列转换为图像-文本对进行多次MLLM推理，计算成本过高；3）现有MLLM推荐系统通常忽略协同信号的整合

Method: 1）使用MLLM将物品总结为简洁关键词，并通过奖励机制（包含总结长度、信息损失和重构难度）微调模型，实现自适应总结策略；2）受检索增强生成启发，将协同信号转换为相应关键词作为补充上下文；3）通过多任务学习的监督微调，使MLLM与多模态序列推荐对齐

Result: 在常见推荐数据集上的广泛评估证明了MMSRARec的有效性，展示了其能够高效且可解释地理解用户行为历史和物品信息，实现准确推荐

Conclusion: MMSRARec方法在推荐性能、可解释性和计算成本之间取得了良好平衡，解决了当前MLLM推荐系统的主要局限性，为多模态序列推荐提供了新的有效解决方案

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.

</details>


### [3] [Accurate and Diverse Recommendations via Propensity-Weighted Linear Autoencoders](https://arxiv.org/abs/2512.20896)
*Kazuma Onishi,Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.IR

TL;DR: 提出一种基于sigmoid函数的倾向性评分方法，解决MNAR数据中传统幂律倾向性评分过度惩罚热门物品的问题，在保持推荐准确性的同时提升推荐多样性。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中用户-物品交互存在MNAR问题，热门物品的交互更频繁被观察到。传统基于幂律函数的逆倾向性评分(IPS)过度惩罚热门物品，损害其推荐性能，需要更平衡的方法。

Method: 重新定义倾向性评分，将物品观察频率的对数通过sigmoid函数转换，保持幂律评分的简单性同时允许更灵活的调整。将新评分整合到倾向于推荐热门物品的线性自编码器模型中。

Result: 实验结果表明，该方法显著提高了推荐列表中物品的多样性，同时没有牺牲推荐准确性。

Conclusion: 提出的sigmoid倾向性评分方法有效解决了传统IPS过度惩罚热门物品的问题，实现了推荐多样性和准确性的更好平衡。

Abstract: In real-world recommender systems, user-item interactions are Missing Not At Random (MNAR), as interactions with popular items are more frequently observed than those with less popular ones. Missing observations shift recommendations toward frequently interacted items, which reduces the diversity of the recommendation list. To alleviate this problem, Inverse Propensity Scoring (IPS) is widely used and commonly models propensities based on a power-law function of item interaction frequency. However, we found that such power-law-based correction overly penalizes popular items and harms their recommendation performance. We address this issue by redefining the propensity score to allow broader item recommendation without excessively penalizing popular items. The proposed score is formulated by applying a sigmoid function to the logarithm of the item observation frequency, maintaining the simplicity of power-law scoring while allowing for more flexible adjustment. Furthermore, we incorporate the redefined propensity score into a linear autoencoder model, which tends to favor popular items, and evaluate its effectiveness. Experimental results revealed that our method substantially improves the diversity of items in the recommendation list without sacrificing recommendation accuracy.

</details>


### [4] [Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions](https://arxiv.org/abs/2512.21076)
*Suraj Kumar,Utsav Kumar Nareti,Soumi Chattopadhyay,Chandranath Adak,Prolay Mallick*

Main category: cs.IR

TL;DR: HiGeMine是一个两阶段分层图书体裁挖掘框架，通过零样本语义对齐过滤用户评论噪声，结合双路径图分类架构，利用标签共现图建模体裁依赖关系，显著提升分层体裁分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有图书体裁分类方法通常将其建模为扁平的单标签任务，忽略了体裁的层次结构，并且过度依赖嘈杂、主观的用户评论，这降低了分类的可靠性。需要一种能够有效整合权威书籍简介和用户评论，同时考虑体裁层次结构的鲁棒方法。

Method: 提出HiGeMine两阶段框架：第一阶段使用零样本语义对齐策略过滤用户评论，保留与书籍简介语义一致的内容；第二阶段采用双路径、两级的图分类架构：第一级粗粒度二元分类器区分小说与非小说，第二级多标签分类器进行细粒度体裁预测，使用标签共现图建模体裁间依赖关系，并利用预训练语言模型获取上下文表示。

Result: 构建了新的分层图书体裁数据集进行系统评估，大量实验表明HiGeMine在分层体裁分类任务中始终优于强基线方法，为利用结构化和非结构化文本数据进行分层图书体裁分析提供了有效解决方案。

Conclusion: HiGeMine框架通过整合权威书籍简介和过滤后的用户评论，结合层次化分类架构和体裁依赖建模，为图书体裁分类提供了原则性和有效的解决方案，显著提升了分类性能。

Abstract: Accurate book genre classification is fundamental to digital library organization, content discovery, and personalized recommendation. Existing approaches typically model genre prediction as a flat, single-label task, ignoring hierarchical genre structure and relying heavily on noisy, subjective user reviews, which often degrade classification reliability. We propose HiGeMine, a two-phase hierarchical genre mining framework that robustly integrates user reviews with authoritative book blurbs. In the first phase, HiGeMine employs a zero-shot semantic alignment strategy to filter reviews, retaining only those semantically consistent with the corresponding blurb, thereby mitigating noise, bias, and irrelevance. In the second phase, we introduce a dual-path, two-level graph-based classification architecture: a coarse-grained Level-1 binary classifier distinguishes fiction from non-fiction, followed by Level-2 multi-label classifiers for fine-grained genre prediction. Inter-genre dependencies are explicitly modeled using a label co-occurrence graph, while contextual representations are derived from pretrained language models applied to the filtered textual content. To facilitate systematic evaluation, we curate a new hierarchical book genre dataset. Extensive experiments demonstrate that HiGeMine consistently outperformed strong baselines across hierarchical genre classification tasks. The proposed framework offers a principled and effective solution for leveraging both structured and unstructured textual data in hierarchical book genre analysis.

</details>


### [5] [Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces](https://arxiv.org/abs/2512.21021)
*Andre Rusli,Miao Cao,Shoma Ishimoto,Sho Akiyama,Max Frenzel*

Main category: cs.IR

TL;DR: 该论文提出了一种针对日本C2C市场的领域感知文本嵌入方法，通过微调购买驱动的查询-标题对、使用角色特定前缀建模查询-物品不对称性，并应用Matryoshka表示学习获得紧凑的截断鲁棒嵌入，显著提升了搜索质量和效率。


<details>
  <summary>Details</summary>
Motivation: C2C市场面临独特的检索挑战：简短模糊的查询、嘈杂的用户生成列表以及严格的生产约束。需要专门针对日本C2C市场（Mercari）的文本嵌入方法来提升搜索质量。

Method: 1. 在购买驱动的查询-标题对上微调文本嵌入模型；2. 使用角色特定前缀建模查询和物品之间的不对称性；3. 应用Matryoshka表示学习获得紧凑且截断鲁棒的嵌入表示。

Result: 离线评估显示相比通用编码器有持续提升，特别是用Matryoshka截断替代PCA压缩时改进显著。手动评估显示能更好处理专有名词、市场特定语义和术语重要性对齐。在线A/B测试显示用户收入和搜索流程效率有统计显著提升，交易频率保持不变。

Conclusion: 领域感知嵌入方法能在大规模应用中提升相关性和效率，为更丰富的LLM时代搜索体验提供了实用基础。

Abstract: Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user-generated listings; and strict production constraints. This paper reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan's largest C2C marketplace. We experimented with fine-tuning on purchase-driven query-title pairs, using role-specific prefixes to model query-item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further highlights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.

</details>


### [6] [Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection](https://arxiv.org/abs/2512.21039)
*Roopa Bukke,Soumya Pandey,Suraj Kumar,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.IR

TL;DR: AMPEND-LS：基于LLM-SLM协同的多模态假新闻检测框架，通过多角色证据推理和可信度融合机制，在准确性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息的快速传播对公众信任、政策和安全构成重大风险，需要可靠的自动化假新闻检测。现有方法在多模态内容处理、领域泛化和可解释性方面存在不足。

Method: 提出AMPEND-LS框架，整合文本、视觉和上下文信号，通过LLM驱动的结构化推理流程，结合反向图像搜索、知识图谱路径和说服策略分析。引入可信度融合机制（语义相似性、领域可信度、时间上下文）和补充的SLM分类器来减轻LLM不确定性和幻觉。

Result: 在三个基准数据集上的广泛实验表明，AMPEND-LS在准确性、F1分数和鲁棒性方面持续优于最先进的基线方法。定性案例研究进一步突出了其透明推理能力和对演化虚假信息的韧性。

Conclusion: 这项工作推动了自适应、可解释和证据感知系统的发展，用于保护在线信息完整性，为应对不断演变的虚假信息威胁提供了有效解决方案。

Abstract: The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.

</details>


### [7] [ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling](https://arxiv.org/abs/2512.21257)
*Chuan Wang,Gaoming Yang,Han Wu,Jiakai Tang,Jiahao Yu,Jian Wu,Jianwu Hu,Junjun Zheng,Shuwen Xiao,Yeqiu Yang,Yuning Jiang,Ahjol Nurlanbek,Binbin Cao,Bo Zheng,Fangmei Zhu,Gaoming Zhou,Huimin Yi,Huiping Chu,Jin Huang,Jinzhe Shan,Kenan Cui,Longbin Li,Silu Zhou,Wen Chen,Xia Ming,Xiang Gao,Xin Yao,Xingyu Wen,Yan Zhang,Yiwen Hu,Yulin Wang,Ziheng Bao,Zongyuan Wu*

Main category: cs.IR

TL;DR: ReaSeq是一个推理增强的推荐框架，利用大语言模型的世界知识解决传统日志驱动推荐系统的知识贫乏和系统盲区问题，在淘宝排名系统中显著提升了各项指标。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统在日志驱动范式下存在两个根本限制：(1)基于ID的物品表示知识贫乏，导致数据稀疏下兴趣建模脆弱；(2)系统对日志外用户兴趣存在盲区，限制了平台边界内的模型性能。这些问题源于过度依赖浅层交互统计和闭环反馈，而忽视了大语言模型从海量语料中学到的产品语义和跨域行为模式的世界知识。

Method: ReaSeq通过显式和隐式推理利用大语言模型的世界知识。具体采用：1) 显式思维链推理：通过多智能体协作将结构化产品知识提炼成语义丰富的物品表示；2) 隐式推理：通过扩散大语言模型推断合理的日志外行为。

Result: 在淘宝排名系统（服务数亿用户）中部署ReaSeq取得了显著提升：IPV和CTR提升超过6.0%，订单量提升超过2.9%，GMV提升超过2.5%。

Conclusion: ReaSeq验证了世界知识增强推理相对于纯日志驱动方法的有效性，为解决工业推荐系统的根本限制提供了有效方案。

Abstract: Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora.
  To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: >6.0% in IPV and CTR, >2.9% in Orders, and >2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [8] [Uplink RSMA Performance Analysis with Rate Adaptation: A Stochastic Geometry Approach](https://arxiv.org/abs/2512.20883)
*Xinyi Guo,Li You,Qiong Liu,Xiqi Gao,Xiang-Gen Xia*

Main category: cs.IT

TL;DR: 本文提出了一种基于随机几何的统一分析框架，用于大规模上行链路速率分割多址接入网络，该框架结合了有限调制编码方案速率适配，能够联合捕获空间干扰耦合和离散速率行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注下行链路和单小区设计，而大规模部署下的上行链路RSMA建模和分析仍未被充分探索。需要一种能够连接理论可处理性和实际现实性的分析框架。

Method: 基于随机几何，提出了一个统一的分析框架，集成了有限MCS速率适配。该框架联合捕获空间干扰耦合和离散速率行为，推导了条件接收速率、其空间平均值以及通过元分布的高阶统计量的可处理表达式。

Result: 该框架不仅推广了现有的NOMA和OMA分析，还提供了关于离散速率适配如何重塑密集RSMA网络中的干扰动态和公平性的新见解。

Conclusion: 提出的统一分析框架为大规模上行链路RSMA网络提供了理论分析工具，能够量化平均和用户特定速率性能，为下一代无线网络设计提供了重要参考。

Abstract: Rate-splitting multiple access (RSMA) has emerged as a promising technique for efficient interference management in next-generation wireless networks. While most existing studies focus on downlink and single-cell designs, the modeling and analysis of uplink RSMA under large-scale deployments remain largely unexplored. On the basis of stochastic geometry (SG), this paper introduces a unified analytical framework that integrates finite modulation and coding scheme (MCS)-based rate adaptation. This framework jointly captures spatial interference coupling and discrete rate behavior to bridge theoretical tractability and practical realism. Within this framework, we derive tractable expressions for the conditional received rate (CRR), its spatial average, and higher-order statistics via the meta distribution, thereby quantifying both the mean and user-specific rate performance. Results show that the proposed unified framework not only generalizes existing non-orthogonal multiple access (NOMA) and orthogonal multiple access (OMA) analyses but also provides new insights into how discrete rate adaptation reshapes interference dynamics and fairness in dense RSMA-enabled networks.

</details>


### [9] [Knowledge-Driven 3D Semantic Spectrum Map: KE-VQ-Transformer Based UAV Semantic Communication and Map Completion](https://arxiv.org/abs/2512.20984)
*Wei Wu,Lingyi Wang,Fuhui Zhou,Zhaohui Yang,Qihui Wu*

Main category: cs.IT

TL;DR: 提出知识增强的语义频谱地图补全框架，通过物理信号传播模型约束提升AI驱动的3D频谱地图重建性能


<details>
  <summary>Details</summary>
Motivation: 在复杂通信环境和稀疏采样数据下，传统统计机器学习方法容易被表面数据相关性误导且缺乏可解释性，难以高效获取和传输3D频谱地图

Method: 提出知识增强的语义频谱地图补全框架，包含基于物理信号传播模型的专家知识约束；设计KE-VQ-Transformer多尺度低复杂度智能补全方法，采用稀疏窗口避免超大3D注意力计算；引入KMSE和RKMSE新指标，开发联合离线在线训练方法

Result: 仿真表明所提方案在RKMSE指标上优于现有最先进基准方案

Conclusion: 通过融入物理知识约束，提出的框架能够捕捉真实世界物理特性，避免陷入表面数据分布思维，实现更准确、物理一致的频谱地图补全

Abstract: Artificial intelligence (AI)-native three-dimensional (3D) spectrum maps are crucial in spectrum monitoring for intelligent communication networks. However, it is challenging to obtain and transmit 3D spectrum maps in a spectrum-efficient, computation-efficient, and AI-driven manner, especially under complex communication environments and sparse sampling data. In this paper, we consider practical air-to-ground semantic communications for spectrum map completion, where the unmanned aerial vehicle (UAV) measures the spectrum at spatial points and extracts the spectrum semantics, which are then utilized to complete spectrum maps at the ground device. Since statistical machine learning can easily be misled by superficial data correlations with the lack of interpretability, we propose a novel knowledge-enhanced semantic spectrum map completion framework with two expert knowledge-driven constraints from physical signal propagation models. This framework can capture the real-world physics and avoid getting stuck in the mindset of superficial data distributions. Furthermore, a knowledge-enhanced vector-quantized Transformer (KE-VQ-Transformer) based multi-scale low-complex intelligent completion approach is proposed, where the sparse window is applied to avoid ultra-large 3D attention computation, and the multi-scale design improves the completion performance. The knowledge-enhanced mean square error (KMSE) and root KMSE (RKMSE) are introduced as novel metrics for semantic spectrum map completion that jointly consider the numerical precision and physical consistency with the signal propagation model, based on which a joint offline and online training method is developed with supervised and unsupervised knowledge loss. The simulation demonstrates that our proposed scheme outperforms the state-of-the-art benchmark schemes in terms of RKMSE.

</details>


### [10] [Coding-Logic Correspondence: Turning Information and Communication Networks into Logical Formulae via Hypergraph Heyting Algebra](https://arxiv.org/abs/2512.21112)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 提出使用混淆超图（超混淆）作为信息模型，替代传统随机变量方法，形成Heyting代数，将通信网络需求表达为逻辑公式，直接计算最优编码方案


<details>
  <summary>Details</summary>
Motivation: 传统基于随机变量的信息理论方法无法直接处理信息的合取、析取和蕴含运算，需要一种新的信息模型来统一表达通信网络的各种编码需求

Method: 使用混淆超图作为信息模型，建立Heyting代数结构，利用直觉主义逻辑与Heyting代数的对应关系，将网络编码、索引编码、Slepian-Wolf编码等通信需求表达为逻辑公式

Result: 通过超图Heyting代数可以直接计算最优编码方案，最优通信成本由超图的熵给出（在对数差距内），建立了编码场景与逻辑公式之间的对应关系

Conclusion: 混淆超图模型为信息理论提供了新的代数框架，建立了类似Curry-Howard对应的编码与逻辑对应关系，统一了多种通信编码问题

Abstract: We propose using confusion hypergraphs (hyperconfusions) as a model of information. In contrast to the conventional approach using random variables, we can now perform conjunction, disjunction and implication of information, forming a Heyting algebra. Using the connection between Heyting algebra and intuitionistic logic, we can express the requirements of a communication network (e.g., network coding, index coding, Slepian-Wolf coding) as a logical formula, allowing us to use the hypergraph Heyting algebra to directly compute the optimal coding scheme. The optimal communication cost is simply given by the entropy of the hypergraph (within a logarithmic gap). This gives a surprising correspondence between coding settings and logical formulae, similar to the Curry-Howard correspondence between proofs and computer programs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [In-Place BWT and Lyndon Array Construction in Constant Space](https://arxiv.org/abs/2512.20869)
*Felipe A. Louza,Arnaud Lefebvre*

Main category: cs.DS

TL;DR: 扩展Crochemore等人的原地BWT算法，使用O(1)额外空间构建Lyndon数组


<details>
  <summary>Details</summary>
Motivation: 现有Lyndon数组构建方法通常需要额外空间，本文旨在开发一种仅需O(1)额外空间的算法，适用于无界字母表

Method: 扩展Crochemore等人的原地BWT算法，在从右到左构建BWT过程中增量维护后缀的字典序排名，然后通过next-smaller-value过程推导Lyndon数组

Result: 成功实现了仅需O(1)额外空间的Lyndon数组构建算法，适用于无界字母表，但时间复杂度为二次方

Conclusion: 该方法概念简单，虽然二次时间复杂度不适合实际应用，但在理论上提供了仅需O(1)额外空间的Lyndon数组构建方案

Abstract: We present an extension of the in-place BWT algorithm of Crochemore et al. [8] that enables the construction of the Lyndon array using O(1) extra space. Our approach incrementally maintains the lexicographic ranks of the suffixes during the right-to-left BWT construction and then derives the Lyndon array through a simple next-smaller-value procedure. Although not intended for practical use due to its quadratic running time, the method is conceptually simple and works for unbounded alphabets.

</details>


### [12] [Fairness in the k-Server Problem](https://arxiv.org/abs/2512.20960)
*Mohammadreza Daneshvaramoli,Helia Karisani,Mohammad Hajiesmaili,Shahin Kamali,Cameron Musco*

Main category: cs.DS

TL;DR: 该论文首次形式化研究k-server问题中的公平性，在最小化总移动成本的同时，要求成本在服务器间公平分配。提出了(α,β)-公平性定义，并证明在线和离线设置中都能在不损失竞争性的前提下实现公平。


<details>
  <summary>Details</summary>
Motivation: 传统k-server问题只关注最小化总移动成本，忽略了成本在服务器间的公平分配。实际应用中，服务器可能属于不同实体或具有不同资源限制，需要确保没有服务器承担不成比例的成本负担。

Method: 提出(α,β)-公平性形式化定义：任何服务器承担的成本不超过总成本的α/k加上β。在离线设置中，设计确定性算法将最优解转换为公平解；在线设置中，将竞争性算法转换为公平的随机在线算法。还分析了经典确定性双覆盖算法(DCA)在不同度量空间中的公平性表现。

Result: 1) 离线设置：可将最优解转换为(1+ε, O(diam·log k/ε))-公平解，仅增加O(diam·k log k/ε)成本；2) 在线设置：可将竞争性算法转换为公平的随机在线算法；3) DCA在线度量和k=2的树度量上是公平的，但在一般树度量上不公平。

Conclusion: 公平性可以在k-server问题中实现而不显著损失性能，但完全自适应对手下的在线公平性仍是开放问题。DCA在某些特殊度量空间上具有公平性，但在一般树度量上不成立。

Abstract: We initiate a formal study of fairness for the $k$-server problem, where the objective is not only to minimize the total movement cost, but also to distribute the cost equitably among servers. We first define a general notion of $(α,β)$-fairness, where, for parameters $α\ge 1$ and $β\ge 0$, no server incurs more than an $α/k$-fraction of the total cost plus an additive term $β$. We then show that fairness can be achieved without a loss in competitiveness in both the offline and online settings. In the offline setting, we give a deterministic algorithm that, for any $\varepsilon > 0$, transforms any optimal solution into an $(α,β)$-fair solution for $α= 1 + \varepsilon$ and $β= O(\mathrm{diam} \cdot \log k / \varepsilon)$, while increasing the cost of the solution by just an additive $O(\mathrm{diam} \cdot k \log k / \varepsilon)$ term. Here $\mathrm{diam}$ is the diameter of the underlying metric space. We give a similar result in the online setting, showing that any competitive algorithm can be transformed into a randomized online algorithm that is fair with high probability against an oblivious adversary and still competitive up to a small loss.
  The above results leave open a significant question: can fairness be achieved in the online setting, either with a deterministic algorithm or a randomized algorithm, against a fully adaptive adversary? We make progress towards answering this question, showing that the classic deterministic Double Coverage Algorithm (DCA) is fair on line metrics and on tree metrics when $k = 2$. However, we also show a negative result: DCA fails to be fair for any non-vacuous parameters on general tree metrics.

</details>


### [13] [Time-Bucketed Balance Records: Bounded-Storage Ephemeral Tokens for Resource-Constrained Systems](https://arxiv.org/abs/2512.20962)
*Shaun Scovil,Bhargav Chickmagalur Nanjundappa*

Main category: cs.DS

TL;DR: 提出时间分桶余额记录数据结构，解决可替代代币TTL语义中的存储爆炸问题，将存储限制在O(k)记录内


<details>
  <summary>Details</summary>
Motivation: 可替代代币的时间到期(TTL)语义需要跟踪每个存入单位的单独过期时间。传统实现为每个存款创建新余额记录，导致存储无限增长且易受拒绝服务攻击

Method: 采用时间分桶余额记录数据结构，将时间离散化为k个桶，将同一桶内的存款合并以限制唯一过期时间戳数量

Result: 证明三个关键属性：(1)存储限制在k+1条记录内，(2)实际过期时间至少等于配置的TTL，(3)攻击者无法将受害者操作成本提高到O(k)摊销最坏情况以上。提供Solidity参考实现并测量燃气成本

Conclusion: 时间分桶余额记录数据结构有效解决了TTL代币的存储爆炸问题，在保证安全性的同时实现了实际效率

Abstract: Fungible tokens with time-to-live (TTL) semantics require tracking individual expiration times for each deposited unit. A naive implementation creates a new balance record per deposit, leading to unbounded storage growth and vulnerability to denial-of-service attacks. We present time-bucketed balance records, a data structure that bounds storage to O(k) records per account while guaranteeing that tokens never expire before their configured TTL. Our approach discretizes time into k buckets, coalescing deposits within the same bucket to limit unique expiration timestamps. We prove three key properties: (1) storage is bounded by k+1 records regardless of deposit frequency, (2) actual expiration time is always at least the configured TTL, and (3) adversaries cannot increase a victim's operation cost beyond O(k)[amortized] worst case. We provide a reference implementation in Solidity with measured gas costs demonstrating practical efficiency.

</details>


### [14] [Approximation Schemes for Planar Graph Connectivity Problems](https://arxiv.org/abs/2512.21128)
*Meike Neuwohner,Vera Traub,Rico Zenklusen*

Main category: cs.DS

TL;DR: 本文提出了一种新的平面图分解技术，解决了k边连通和k顶点连通子图的最小化问题，以及平面图k连通性增强问题，为这些经典网络设计问题提供了PTAS（多项式时间近似方案）。


<details>
  <summary>Details</summary>
Motivation: 在平面图中寻找最小的k边连通子图或通过添加最小边集将k边连通图增强为(k+1)边连通图是网络设计中的基本问题。这些问题在一般图中是APX难的，但在平面图设置中情况尚不清楚，特别是对于较大的k值。现有分解技术（如Baker技术）难以处理连通性这种全局性质。

Method: 提出了一种新颖且简洁的平面图分解技术，专门针对连通性问题设计。该技术利用了最小k边连通图的结构特性，能够处理连通性这种全局性质。

Result: 该技术为任意k值的平面图最小k边连通或k顶点连通生成子图问题提供了PTAS。进一步地，对于任意常数k的平面k连通性增强问题也获得了PTAS。同时证明了NP难性结果，表明这些结果在本质上是最优的。

Conclusion: 本文提出的分解技术成功解决了平面图中经典连通性问题的近似算法设计难题，为这些网络设计问题提供了高效的近似方案，并通过NP难性证明确立了结果的紧致性。

Abstract: Finding a smallest subgraph that is k-edge-connected, or augmenting a k-edge-connected graph with a smallest subset of given candidate edges to become (k+1)-edge-connected, are among the most fundamental Network Design problems. They are both APX-hard in general graphs. However, this hardness does not carry over to the planar setting, which is not well understood, except for very small values of k. One main obstacle in using standard decomposition techniques for planar graphs, like Baker's technique and extensions thereof, is that connectivity requirements are global (rather than local) properties that are not captured by existing frameworks.
  We present a novel, and arguably clean, decomposition technique for such classical connectivity problems on planar graphs. This technique immediately implies PTASs for the problems of finding a smallest k-edge-connected or k-vertex-connected spanning subgraph of a planar graph for arbitrary k. By leveraging structural results for minimally k-edge-connected graphs, we further obtain a PTAS for planar k-connectivity augmentation for any constant k. We complement this with an NP-hardness result, showing that our results are essentially optimal.

</details>


### [15] [An O($nlogn$) approximate knapsack algorithm](https://arxiv.org/abs/2512.21195)
*Nick Dawes*

Main category: cs.DS

TL;DR: 提出一种改进的动态规划算法，能快速准确解决大规模0/1背包问题，具有O(nlogn)时间/空间复杂度，误差可预测，且精度随解规模k增长而提升。


<details>
  <summary>Details</summary>
Motivation: 传统动态规划算法解决大规模0/1背包问题时面临时间和空间复杂度高的问题，需要开发更高效的近似算法来处理大规模实例。

Method: 采用改进的动态规划算法，通过优化数据结构（可能使用近似技术或剪枝策略）来降低复杂度，同时保持可预测的误差界限。

Result: 算法在k=10³时平均最大分数误差为10⁻⁴，k=10⁵时误差降至10⁻⁷；在普通桌面计算机上，n=10³问题处理时间为10⁻³秒，n=10⁶问题仅需2秒。

Conclusion: 该算法为大规模0/1背包问题提供了高效实用的解决方案，在可接受的误差范围内显著提升了计算效率，适用于实际应用场景。

Abstract: A modified dynamic programming algorithm rapidly and accurately solves large 0/1 knapsack problems. It has computational O($nlogn$), space O($nlogn$) and predictable maximum error. Experimentally it's accuracy increases faster than linearly with the solution size $k$. Problems with $k=10^3$ are solved with an average maximum fractional error of $10^{-4}$ and problems with $k=10^5$ with an average maximum fractional error of $10^{-7}$. The algorithm runs in constant time for all problems with a given $n$. On a common desktop computer the algorithm processes $n=10^3$ problems in $10^{-3}$ seconds and $n=10^6$ problems in 2 seconds.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [16] [Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems](https://arxiv.org/abs/2512.20688)
*Stefano Grassi*

Main category: cs.GT

TL;DR: 论文提出机制基础智能(MBI)范式，通过可微分价格机制(DPM)解决多智能体系统的信息分散和激励对齐问题，实现线性可扩展的协调优化。


<details>
  <summary>Details</summary>
Motivation: 传统自主多智能体系统存在根本性脆弱：难以解决哈耶克信息问题（获取分散的私有知识）和赫维茨激励问题（使局部行动与全局目标对齐），导致协调计算不可行。

Method: 提出机制基础智能(MBI)范式，将智能重新概念化为多个"大脑"协调的涌现现象。核心是可微分价格机制(DPM)，计算精确损失梯度作为动态的VCG等价激励信号，保证主导策略激励相容(DSIC)并收敛到全局最优。贝叶斯扩展确保在信息不对称下的激励相容(BIC)。

Result: 框架随智能体数量线性扩展(O(N))，绕过Dec-POMDP的组合复杂性，经验上比无模型强化学习快50倍。通过结构性地将智能体自利与集体目标对齐，提供了可证明高效、可审计且可泛化的协调方法。

Conclusion: 基于经济原则的机制基础智能为可信赖、可扩展的多智能体智能提供了理论基础，解决了传统多智能体系统的根本协调问题。

Abstract: Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple "brains", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \mathbf{G}_i = - \frac{\partial \mathcal{L}}{\partial \mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.

</details>


### [17] [(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols](https://arxiv.org/abs/2512.20864)
*Suhyeon Lee,Dieu-Huyen Nguyen,Donghwan Lee*

Main category: cs.GT

TL;DR: 区块链挑战协议激励分析：单赢家设计无法同时满足诚实无损失和欺诈威慑，多赢家设计可达成


<details>
  <summary>Details</summary>
Motivation: 现有区块链挑战协议（如Truebit和乐观汇总）依赖诚实挑战者来检测欺诈，但缺乏对诚实挑战者激励和欺诈者惩罚的严格分析，特别是在最坏情况下

Method: 建立包含合谋少数派、异质成本和三种排序模式的模型，分析诚实无损失和欺诈威慑两个目标能否同时实现

Result: 单赢家设计中激励设计不可能或规模受限；多赢家设计中可获得简单明确的条件使两个目标同时成立

Conclusion: 多赢家设计比单赢家设计在激励诚实挑战者和威慑欺诈方面更具优势，为区块链挑战协议提供了更好的激励保障

Abstract: Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.

</details>


### [18] [Policy-Conditioned Policies for Multi-Agent Task Solving](https://arxiv.org/abs/2512.21024)
*Yue Lin,Shuhui Zhu,Wenhao Li,Ang Li,Dan Qiao,Pascal Poupart,Hongyuan Zha,Baoxiang Wang*

Main category: cs.GT

TL;DR: 提出用可解释的源代码表示策略，利用LLM作为近似解释器，实现程序均衡，通过文本梯度优化解决多智能体协调问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体任务中，直接基于对手策略调整策略存在"表示瓶颈"问题：神经策略是不透明的高维参数向量，其他智能体无法理解。需要一种可解释的策略表示方法。

Method: 将策略表示为人类可解释的源代码，利用LLM作为近似解释器，实现程序均衡。提出程序化迭代最优响应(PIBR)算法，通过文本梯度优化策略代码，使用游戏效用和运行时单元测试的结构化反馈。

Result: 该方法有效解决了多个标准协调矩阵游戏和合作性Level-Based Foraging环境，证明了程序化策略表示和LLM优化的可行性。

Conclusion: 通过将策略表示为源代码并利用LLM进行优化，成功克服了多智能体学习中的表示瓶颈问题，为可解释的多智能体协调提供了新范式。

Abstract: In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.

</details>
