<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.IT](#cs.IT) [Total: 15]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [CSQL: Mapping Documents into Causal Databases](https://arxiv.org/abs/2601.08109)
*Sridhar Mahadevan*

Main category: cs.DB

TL;DR: CSQL是一个将非结构化文本文档自动转换为可SQL查询的因果数据库的系统，支持因果干预和结构化因果查询，超越了传统的关联检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有系统如RAG或知识图谱主要支持关联检索，无法支持因果分析。需要一种能够从文档集合中提取因果信息并支持因果查询的系统。

Method: 基于DEMOCRITUS系统从因果话语中提取数千个局部因果模型，然后将这些模型编译成因果数据库，支持SQL查询和因果代数操作。

Result: 成功将TCC数据集中的经济学论文编译成包含265,656个因果主张实例的因果数据库，覆盖45,319篇论文、44年数据和1,575种报告方法，支持语料库级别的因果查询和纵向分析。

Conclusion: CSQL作为从非结构化文档到因果数据库的编译器，具有广泛的应用前景，可用于商业、人文和科学等多个领域，实现了从文档集合中进行因果分析的能力。

Abstract: We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer "why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: "What are the strongest causal influences on bipedalism?'' or "Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.

</details>


### [2] [SVFusion: A CPU-GPU Co-Processing Architecture for Large-Scale Real-Time Vector Search](https://arxiv.org/abs/2601.08528)
*Yuchen Peng,Dingyu Yang,Zhongle Xie,Ji Sun,Lidan Shou,Ke Chen,Gang Chen*

Main category: cs.DB

TL;DR: SVFusion是一个GPU-CPU-磁盘协同的实时向量搜索框架，通过分层索引架构和负载感知缓存机制，在保证高召回率的同时显著提升查询吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有CPU方案支持更新但吞吐量低，GPU加速系统性能高但面临动态更新困难和GPU内存有限的挑战，需要解决大规模连续向量搜索中准确性和速度的性能缺口。

Method: 采用分层向量索引架构，结合CPU-GPU协同处理、负载感知向量缓存机制、CUDA多流优化、自适应资源管理和并发控制，确保查询和更新交错时的数据一致性。

Result: 相比基线方法，SVFusion平均实现20.9倍更高的吞吐量，延迟降低1.3到50.7倍，同时在大规模数据集的各种流式工作负载下保持高召回率。

Conclusion: SVFusion通过GPU-CPU-磁盘协同框架成功填补了实时向量搜索的性能缺口，为需要高准确性和速度的大规模连续向量搜索提供了有效解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS) underpins modern applications such as information retrieval and recommendation. With the rapid growth of vector data, efficient indexing for real-time vector search has become rudimentary. Existing CPU-based solutions support updates but suffer from low throughput, while GPU-accelerated systems deliver high performance but face challenges with dynamic updates and limited GPU memory, resulting in a critical performance gap for continuous, large-scale vector search requiring both accuracy and speed. In this paper, we present SVFusion, a GPU-CPU-disk collaborative framework for real-time vector search that bridges sophisticated GPU computation with online updates. SVFusion leverages a hierarchical vector index architecture that employs CPU-GPU co-processing, along with a workload-aware vector caching mechanism to maximize the efficiency of limited GPU memory. It further enhances performance through real-time coordination with CUDA multi-stream optimization and adaptive resource management, along with concurrency control that ensures data consistency under interleaved queries and updates. Empirical results demonstrate that SVFusion achieves significant improvements in query latency and throughput, exhibiting a 20.9x higher throughput on average and 1.3x to 50.7x lower latency compared to baseline methods, while maintaining high recall for large-scale datasets under various streaming workloads.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [3] [A survey: Information search time optimization based on RAG (Retrieval Augmentation Generation) chatbot](https://arxiv.org/abs/2601.07838)
*Jinesh Patel,Arpit Malhotra,Ajay Pande,Prateek Caire*

Main category: cs.IR

TL;DR: RAG聊天机器人相比传统搜索方法在组织内信息检索中能节省80-95%的时间


<details>
  <summary>Details</summary>
Motivation: 研究RAG聊天机器人在组织内部复杂信息检索中的效率，特别是与传统搜索方法相比的时间节省效果

Method: 在"X Systems"公司对105名员工进行调研，比较使用标准搜索技术与RAG聊天机器人处理相同查询的时间差异

Result: RAG聊天机器人相比传统搜索方法平均能节省80-95%的搜索时间

Conclusion: RAG聊天机器人不仅能显著节省信息检索时间，还能有效优化搜索流程

Abstract: Retrieval-Augmented Generation (RAG) based chatbots are not only useful for information retrieval through questionanswering but also for making complex decisions based on injected private data.we present a survey on how much search time can be saved when retrieving complex information within an organization called "X Systems"(a stealth mode company) by using a RAG-based chatbot compared to traditional search methods. We compare the information retrieval time using standard search techniques versus the RAG-based chatbot for the same queries. Our results conclude that RAG-based chatbots not only save time in information retrieval but also optimize the search process effectively. This survey was conducted with a sample of 105 employees across departments, average time spending on information retrieval per query was taken as metric. Comparison shows us, there are average 80-95% improvement on search when use RAG based chatbot than using standard search.

</details>


### [4] [Cost and accuracy of long-term graph memory in distributed LLM-based multi-agent systems](https://arxiv.org/abs/2601.07978)
*Benedict Wolff,Jacopo Bennati*

Main category: cs.IR

TL;DR: 该研究比较了分布式多智能体系统中两种长期记忆框架：向量存储的mem0和知识图谱Graphiti，发现mem0在效率上显著优于Graphiti，而准确率差异不显著，mem0是成本与准确率平衡的最优选择。


<details>
  <summary>Details</summary>
Motivation: 分布式多智能体系统使用大语言模型实现协作智能并保护隐私，但在网络约束下对长期记忆的系统性评估仍然有限，需要比较不同记忆框架的性能表现。

Method: 使用灵活的测试平台，在无约束和约束网络条件下比较mem0（基于向量的记忆框架）和Graphiti（基于图的知识图谱），采用LOCOMO长上下文基准，测量计算、财务和准确率指标。

Result: mem0在效率上显著优于Graphiti，具有更快的加载时间、更低的资源消耗和最小的网络开销，而准确率差异在统计上不显著。通过统计帕累托效率框架分析，mem0是平衡成本和准确率的最优选择。

Conclusion: 在分布式多智能体系统中，mem0作为基于向量的记忆框架，在保持准确率的同时提供了更高的效率，是网络约束下长期记忆管理的更优解决方案。

Abstract: Distributed multi-agent systems use large language models to enable collaborative intelligence while preserving privacy, yet systematic evaluations of long-term memory under network constraints remain limited. This study presents a flexible testbed comparing mem0, a vector-based memory framework, and Graphiti, a graph-based knowledge graph, using the LOCOMO long-context benchmark. Experiments were conducted under unconstrained and constrained network conditions, measuring computational, financial, and accuracy metrics. Results indicate that mem0 significantly outperforms Graphiti in efficiency, with faster loading times, lower resource consumption, and minimal network overhead, while accuracy differences are not statistically significant. Applying a statistical pareto efficiency framework, mem0 is identified as the optimal choice that balances cost and accuracy in DMAS.

</details>


### [5] [VeriTaS: The First Dynamic Benchmark for Multimodal Automated Fact-Checking](https://arxiv.org/abs/2601.08611)
*Mark Rothermel,Marcus Kornmann,Marcus Rohrbach,Anna Rohrbach*

Main category: cs.IR

TL;DR: VeriTaS是首个动态多模态自动事实核查基准，包含24,000个真实世界声明，覆盖54种语言，通过自动化流程定期更新以防止数据泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查基准存在任务范围窄、模态单一、语言多样性不足、静态化导致数据泄露等问题，使得基准性能无法可靠反映实际核查能力。

Method: 开发了完全自动化的七阶段流水线：规范化声明表述、检索原始媒体、将异构专家裁决映射到新颖的标准化解耦评分方案，并包含文本理由。每季度更新数据。

Result: 构建了包含24,000个声明的基准，来自108个专业事实核查组织，覆盖54种语言和文本/视听内容。人工评估显示自动化标注与人工判断高度一致。

Conclusion: VeriTaS建立了抗数据泄露的动态基准，支持在基础模型快速演进时代进行有意义的事实核查评估，代码和数据将公开。

Abstract: The growing scale of online misinformation urgently demands Automated Fact-Checking (AFC). Existing benchmarks for evaluating AFC systems, however, are largely limited in terms of task scope, modalities, domain, language diversity, realism, or coverage of misinformation types. Critically, they are static, thus subject to data leakage as their claims enter the pretraining corpora of LLMs. As a result, benchmark performance no longer reliably reflects the actual ability to verify claims. We introduce Verified Theses and Statements (VeriTaS), the first dynamic benchmark for multimodal AFC, designed to remain robust under ongoing large-scale pretraining of foundation models. VeriTaS currently comprises 24,000 real-world claims from 108 professional fact-checking organizations across 54 languages, covering textual and audiovisual content. Claims are added quarterly via a fully automated seven-stage pipeline that normalizes claim formulation, retrieves original media, and maps heterogeneous expert verdicts to a novel, standardized, and disentangled scoring scheme with textual justifications. Through human evaluation, we demonstrate that the automated annotations closely match human judgments. We commit to update VeriTaS in the future, establishing a leakage-resistant benchmark, supporting meaningful AFC evaluation in the era of rapidly evolving foundation models. We will make the code and data publicly available.

</details>


### [6] [Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models](https://arxiv.org/abs/2601.08148)
*Seokho Ahn,Sungbok Shin,Young-Duk Seo*

Main category: cs.IR

TL;DR: SPiKE模型结合LLMs和KGs优势，通过LLMs从多源知识中提取压缩语义档案，KGs传播这些档案，实现更优推荐效果。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统中用户档案构建与利用缺乏共识，需要更丰富的信息化档案来提升推荐质量。LLMs擅长从多样化知识源提取压缩推理，而KGs更适合传播档案扩展覆盖范围。

Method: 提出SPiKE模型，包含三个核心组件：1) 实体档案生成：使用LLMs为所有KG实体生成语义档案；2) 档案感知KG聚合：将这些档案整合到KG中；3) 成对档案偏好匹配：在训练中对齐LLM和KG的表示。

Result: 实验表明，SPiKE在真实场景中持续优于最先进的基于KG和LLM的推荐系统。

Conclusion: 结合LLMs的知识提取能力和KGs的传播能力，能够构建更有效的用户档案，显著提升推荐系统性能。

Abstract: Rich and informative profiling to capture user preferences is essential for improving recommendation quality. However, there is still no consensus on how best to construct and utilize such profiles. To address this, we revisit recent profiling-based approaches in recommender systems along four dimensions: 1) knowledge base, 2) preference indicator, 3) impact range, and 4) subject. We argue that large language models (LLMs) are effective at extracting compressed rationales from diverse knowledge sources, while knowledge graphs (KGs) are better suited for propagating these profiles to extend their reach. Building on this insight, we propose a new recommendation model, called SPiKE. SPiKE consists of three core components: i) Entity profile generation, which uses LLMs to generate semantic profiles for all KG entities; ii) Profile-aware KG aggregation, which integrates these profiles into the KG; and iii) Pairwise profile preference matching, which aligns LLM- and KG-based representations during training. In experiments, we demonstrate that SPiKE consistently outperforms state-of-the-art KG- and LLM-based recommenders in real-world settings.

</details>


### [7] [Markovian Pre-Trained Transformer for Next-Item Recommendation](https://arxiv.org/abs/2601.08275)
*Cong Xu,Guoliang Li,Jun Wang,Wei Zhang*

Main category: cs.IR

TL;DR: MPT是一种基于马尔可夫链预训练的推荐模型，仅通过轻量级适配器微调就能达到SOTA性能，揭示了序列推荐中"马尔可夫性"的本质特征


<details>
  <summary>Details</summary>
Motivation: 现有先进序列推荐器意外地主要依赖最近一次交互进行预测，历史交互仅作为推断用户一般身份的辅助线索。这种"马尔可夫性"特征需要通用推荐模型有效总结用户序列，特别强调最新交互

Method: 提出马尔可夫预训练变换器(MPT)，完全在合成马尔可夫链上预训练，通过预测马尔可夫链的下一个状态，学习从上下文估计转移概率（序列总结的自适应方式）并关注最后状态以确保准确状态转移

Result: 在三个不同平台的五个公共数据集上进行广泛实验，验证了马尔可夫预训练优于传统推荐预训练和近期语言预训练范式

Conclusion: MPT具有成为通用可迁移推荐模型的潜力，一方面通过马尔可夫链预训练获得序列总结能力，另一方面可利用无限可控的马尔可夫链数据提升模型容量，而无需依赖异构交互数据

Abstract: We introduce the Markovian Pre-trained Transformer (MPT) for next-item recommendation, a transferable model fully pre-trained on synthetic Markov chains, yet capable of achieving state-of-the-art performance by fine-tuning a lightweight adaptor. This counterintuitive success stems from the observation of the `Markovian' nature: advanced sequential recommenders coincidentally rely on the latest interaction to make predictions, while the historical interactions serve mainly as auxiliary cues for inferring the user's general, non-sequential identity. This characteristic necessitates the capabilities of a universal recommendation model to effectively summarize the user sequence, with particular emphasis on the latest interaction. MPT inherently has the potential to be universal and transferable. On the one hand, when trained to predict the next state of Markov chains, it acquires the capabilities to estimate transition probabilities from the context (one adaptive manner for summarizing sequences) and attend to the last state to ensure accurate state transitions. On the other hand, unlike the heterogeneous interaction data, an unlimited amount of controllable Markov chains is available to boost the model capacity. We conduct extensive experiments on five public datasets from three distinct platforms to validate the superiority of Markovian pre-training over traditional recommendation pre-training and recent language pre-training paradigms.

</details>


### [8] [AgriLens: Semantic Retrieval in Agricultural Texts Using Topic Modeling and Language Models](https://arxiv.org/abs/2601.08283)
*Heba Shakeel,Tanvir Ahmad,Tanya Liyaqat,Chandni Saxena*

Main category: cs.IR

TL;DR: 提出一个统一框架，用于可解释的主题建模、零样本主题标注和主题引导的语义检索，应用于农业文本语料库


<details>
  <summary>Details</summary>
Motivation: 随着各领域非结构化文本数据快速增长，需要可扩展的方法来实现可解释的信息组织、摘要和检索，特别是在标注数据有限的专门领域

Method: 使用BERTopic提取语义连贯的主题，将每个主题转换为结构化提示，让语言模型以零样本方式生成有意义的主题标签和摘要，通过密集嵌入和向量搜索支持查询和文档探索，并包含专门的评估模块

Result: 该框架支持在标注数据有限的专门领域（如农业）进行可扩展且可解释的信息访问

Conclusion: 提出的统一框架能够有效解决大规模文本语料库中的主题建模、标注和检索问题，为专门领域的信息访问提供了实用的解决方案

Abstract: As the volume of unstructured text continues to grow across domains, there is an urgent need for scalable methods that enable interpretable organization, summarization, and retrieval of information. This work presents a unified framework for interpretable topic modeling, zero-shot topic labeling, and topic-guided semantic retrieval over large agricultural text corpora. Leveraging BERTopic, we extract semantically coherent topics. Each topic is converted into a structured prompt, enabling a language model to generate meaningful topic labels and summaries in a zero-shot manner. Querying and document exploration are supported via dense embeddings and vector search, while a dedicated evaluation module assesses topical coherence and bias. This framework supports scalable and interpretable information access in specialized domains where labeled data is limited.

</details>


### [9] [MLPlatt: Simple Calibration Framework for Ranking Models](https://arxiv.org/abs/2601.08345)
*Piotr Bajger,Roman Dusek,Krzysztof Galias,Paweł Młyniec,Aleksander Wawer,Paweł Zawistowski*

Main category: cs.IR

TL;DR: 提出MLPlatt方法，用于电商排名模型的后处理校准，将排名输出转换为可解释的点击率概率，同时保持排序质量。


<details>
  <summary>Details</summary>
Motivation: 电商排名模型通常存在可解释性差和缺乏尺度校准的问题，特别是在使用典型排名损失函数训练时。需要一种方法能够将排名器输出转换为可解释的点击率概率，同时保持项目排序不变。

Method: 提出MLPlatt方法，这是一种简单有效的排名模型校准方法。该方法具有上下文感知设计，能够保持项目排序，并将排名器输出转换为可用于下游任务的点击率概率。方法考虑了不同分类字段（如用户国家或设备）的分层校准。

Result: 在两个数据集上，MLPlatt优于现有方法，在F-ECE（字段预期校准误差）上相比其他方法提高了超过10%。最重要的是，实现了高质量校准而不损害排名质量。

Conclusion: MLPlatt是一种有效的排名模型后处理校准方法，能够将排名输出转换为可解释的点击率概率，同时保持排序质量，满足电商平台的业务需求。

Abstract: Ranking models are extensively used in e-commerce for relevance estimation. These models often suffer from poor interpretability and no scale calibration, particularly when trained with typical ranking loss functions. This paper addresses the problem of post-hoc calibration of ranking models. We introduce MLPlatt: a simple yet effective ranking model calibration method that preserves the item ordering and converts ranker outputs to interpretable click-through rate (CTR) probabilities usable in downstream tasks. The method is context-aware by design and achieves good calibration metrics globally, and within strata corresponding to different values of a selected categorical field (such as user country or device), which is often important from a business perspective of an E-commerce platform. We demonstrate the superiority of MLPlatt over existing approaches on two datasets, achieving an improvement of over 10\% in F-ECE (Field Expected Calibration Error) compared to other methods. Most importantly, we show that high-quality calibration can be achieved without compromising the ranking quality.

</details>


### [10] [Scalable Sequential Recommendation under Latency and Memory Constraints](https://arxiv.org/abs/2601.08360)
*Adithya Parthasarathy,Aswathnarayan Muthukrishnan Kirubakaran,Vinoth Punniyamoorthy,Nachiappan Chockalingam,Lokesh Butra,Kabilan Kannan,Abhirup Mazumder,Sumit Saha*

Main category: cs.IR

TL;DR: HoloMambaRec：结合全息降维表示和选择性状态空间编码的轻量级序列推荐系统，在保持低内存复杂度的同时实现线性时间处理


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在序列推荐中存在二次注意力复杂度问题，需要截断用户历史记录，限制了长序列建模能力。需要在严格内存和延迟约束下实现长范围用户行为建模

Method: 1) 使用全息降维表示进行属性感知嵌入，通过循环卷积绑定项目和属性信息；2) 采用受Mamba启发的浅层选择性状态空间编码器，实现线性时间序列处理和恒定时间递归推理

Result: 在Amazon Beauty和MovieLens-1M数据集上，HoloMambaRec在10个epoch的有限训练预算下，性能优于SASRec，与GRU4Rec相当，同时保持显著更低的内存复杂度

Conclusion: HoloMambaRec为可扩展、元数据感知的序列推荐提供了一个实用且可扩展的替代方案，具有时间捆绑和推理时压缩的前向兼容机制

Abstract: Sequential recommender systems must model long-range user behavior while operating under strict memory and latency constraints. Transformer-based approaches achieve strong accuracy but suffer from quadratic attention complexity, forcing aggressive truncation of user histories and limiting their practicality for long-horizon modeling. This paper presents HoloMambaRec, a lightweight sequential recommendation architecture that combines holographic reduced representations for attribute-aware embedding with a selective state space encoder for linear-time sequence processing. Item and attribute information are bound using circular convolution, preserving embedding dimensionality while encoding structured metadata. A shallow selective state space backbone, inspired by recent Mamba-style models, enables efficient training and constant-time recurrent inference. Experiments on Amazon Beauty and MovieLens-1M datasets demonstrate that HoloMambaRec consistently outperforms SASRec and achieves competitive performance with GRU4Rec under a constrained 10-epoch training budget, while maintaining substantially lower memory complexity. The design further incorporates forward-compatible mechanisms for temporal bundling and inference-time compression, positioning HoloMambaRec as a practical and extensible alternative for scalable, metadata-aware sequential recommendation.

</details>


### [11] [PosIR: Position-Aware Heterogeneous Information Retrieval Benchmark](https://arxiv.org/abs/2601.08363)
*Ziyang Zeng,Dun Zhang,Yu Yan,Xu Sun,Yudong Zhou,Yuqing Yang*

Main category: cs.IR

TL;DR: PosIR是一个用于诊断检索模型中位置偏差的基准测试，包含310个数据集、10种语言、31个领域，通过将相关性与精确引用范围绑定来分离文档长度与信息位置的影响。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索模型虽然取得了显著成功，但对相关信息位置敏感性（位置偏差）的严格评估仍然不足。现有基准测试通常使用位置无关的相关性标签，混淆了处理长文本的挑战与对特定证据位置的偏见。

Method: 构建PosIR基准测试，包含310个数据集，涵盖10种语言和31个领域。通过严格的流程将相关性与精确的引用范围绑定，从而严格分离文档长度与信息位置。使用10个最先进的嵌入模型进行实验，并进行基于梯度的显著性分析。

Result: 1) 在长文本设置下，PosIR上的性能与MMTEB基准测试相关性差，暴露了当前短文本基准测试的局限性；2) 位置偏差普遍存在且随文档长度加剧，大多数模型表现出首因偏差，某些模型显示意外的近因偏差；3) 基于梯度的显著性分析揭示了驱动这些位置偏好的不同内部注意力机制。

Conclusion: PosIR作为一个有价值的诊断框架，有助于促进开发位置鲁棒的检索系统。

Abstract: While dense retrieval models have achieved remarkable success, rigorous evaluation of their sensitivity to the position of relevant information (i.e., position bias) remains largely unexplored. Existing benchmarks typically employ position-agnostic relevance labels, conflating the challenge of processing long contexts with the bias against specific evidence locations. To address this challenge, we introduce PosIR (Position-Aware Information Retrieval), a comprehensive benchmark designed to diagnose position bias in diverse retrieval scenarios. PosIR comprises 310 datasets spanning 10 languages and 31 domains, constructed through a rigorous pipeline that ties relevance to precise reference spans, enabling the strict disentanglement of document length from information position. Extensive experiments with 10 state-of-the-art embedding models reveal that: (1) Performance on PosIR in long-context settings correlates poorly with the MMTEB benchmark, exposing limitations in current short-text benchmarks; (2) Position bias is pervasive and intensifies with document length, with most models exhibiting primacy bias while certain models show unexpected recency bias; (3) Gradient-based saliency analysis further uncovers the distinct internal attention mechanisms driving these positional preferences. In summary, PosIR serves as a valuable diagnostic framework to foster the development of position-robust retrieval systems.

</details>


### [12] [GraphFusionSBR: Denoising Multi-Channel Graphs for Session-Based Recommendation](https://arxiv.org/abs/2601.08497)
*Jia-Xin He,Hung-Hsuan Chen*

Main category: cs.IR

TL;DR: 提出多通道推荐模型，通过知识图谱、超图和线图三个通道从多源信息中捕捉用户意图，采用自适应去噪和互信息最大化提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于会话的推荐系统存在项目交互主导和噪声会话问题，需要更好地从会话中捕捉隐含用户意图。

Method: 提出包含知识图谱通道、会话超图通道和会话线图通道的多通道模型，自适应去除知识图谱冗余边减少噪声，知识图谱表示与超图表示协同预测缓解项目主导，生成会话内注意力去噪，最大化超图和线图通道间的互信息作为辅助任务。

Result: 实验表明该方法提升了电子商务和多媒体推荐等多种推荐场景的准确性，代码已在GitHub开源。

Conclusion: 多通道模型能有效捕捉多源信息，通过自适应去噪和互信息最大化等技术解决了现有模型的局限性，提高了推荐系统的性能。

Abstract: Session-based recommendation systems must capture implicit user intents from sessions. However, existing models suffer from issues such as item interaction dominance and noisy sessions. We propose a multi-channel recommendation model, including a knowledge graph channel, a session hypergraph channel, and a session line graph channel, to capture information from multiple sources. Our model adaptively removes redundant edges in the knowledge graph channel to reduce noise. Knowledge graph representations cooperate with hypergraph representations for prediction to alleviate item dominance. We also generate in-session attention for denoising. Finally, we maximize mutual information between the hypergraph and line graph channels as an auxiliary task. Experiments demonstrate that our method enhances the accuracy of various recommendations, including e-commerce and multimedia recommendations. We release the code on GitHub for reproducibility.\footnote{https://github.com/hohehohe0509/DSR-HK}

</details>


### [13] [RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors](https://arxiv.org/abs/2601.08705)
*Miaomiao Cai,Zhijie Zhang,Junfeng Fang,Zhiyong Cheng,Xiang Wang,Meng Wang*

Main category: cs.IR

TL;DR: 提出RMBRec框架，通过信息论原理解决多行为推荐中辅助行为噪声、弱相关或语义不对齐的问题，提升目标行为预测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多行为推荐面临辅助行为（如点击、加购）通常存在噪声、弱相关或与目标行为（如购买）语义不对齐的问题，导致偏好学习偏差和性能下降。现有方法缺乏处理这种行为不一致性的鲁棒机制。

Method: 提出RMBRec框架，基于信息论鲁棒性原则：最大化预测信息同时最小化其在异构行为环境中的方差。包含两个模块：表示鲁棒性模块（RRM）通过最大化用户辅助行为与目标行为表示间的互信息增强局部语义一致性；优化鲁棒性模块（ORM）通过最小化跨行为预测风险的方差来强制全局稳定性（这是不变风险最小化的高效近似）。

Result: 在三个真实世界数据集上的实验表明，RMBRec不仅在准确性上优于最先进方法，而且在各种噪声扰动下保持显著稳定性。

Conclusion: RMBRec通过局部-全局协作，以理论一致的方式桥接了表示纯化和优化不变性，为多行为推荐中的行为不一致问题提供了鲁棒解决方案。

Abstract: Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.
  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users' auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.

</details>


### [14] [FusID: Modality-Fused Semantic IDs for Generative Music Recommendation](https://arxiv.org/abs/2601.08764)
*Haven Kim,Yupeng Hou,Julian McAuley*

Main category: cs.IR

TL;DR: FusID是一个多模态融合的语义ID框架，通过联合编码跨模态信息、学习统一表示和产品量化，解决了现有方法中的冗余问题和模态间交互缺失问题，在音乐推荐任务中实现了零ID冲突并提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统使用语义ID表示物品，但现有方法独立标记化每个模态存在两个关键限制：1）跨模态冗余降低了效率；2）未能捕捉模态间交互限制了物品表示能力。

Method: FusID包含三个关键组件：1）多模态融合，通过联合编码跨模态信息学习统一表示；2）表示学习，使频繁共现的物品嵌入更接近，同时保持区分性并防止特征冗余；3）产品量化，将融合的连续嵌入转换为多个离散标记以缓解ID冲突。

Result: 在多模态下一首歌曲推荐基准测试中，FusID实现了零ID冲突（确保每个标记序列精确映射到一首歌曲），缓解了码本利用不足问题，并在MRR和Recall@k（k=1,5,10,20）指标上优于基线方法。

Conclusion: FusID通过多模态融合和产品量化有效解决了现有语义ID方法的局限性，在推荐系统中实现了更好的物品表示和推荐性能。

Abstract: Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).

</details>


### [15] [MemRec: Collaborative Memory-Augmented Agentic Recommender System](https://arxiv.org/abs/2601.08816)
*Weixin Chen,Yuhan Zhao,Jingyuan Huang,Zihe Ye,Clark Mingxuan Ju,Tong Zhao,Neil Shah,Li Chen,Yongfeng Zhang*

Main category: cs.IR

TL;DR: MemRec是一个推荐系统框架，通过将推理与内存管理解耦，利用专门的LM_Mem管理动态协作内存图，为下游LLM_Rec提供高信号上下文，实现高效协作增强。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统代理依赖孤立内存，忽视了关键的协作信号。需要解决两个挑战：从庞大图上下文中提取信息而不增加推理代理的认知负荷，以及高效演化协作内存而不产生过高计算成本。

Method: 提出MemRec框架，架构上将推理与内存管理解耦。引入专门的LM_Mem管理动态协作内存图，通过高效检索和成本效益高的异步图传播管道，为下游LLM_Rec提供合成的、高信号上下文。

Result: 在四个基准测试上的广泛实验表明，MemRec实现了最先进的性能。架构分析确认了其灵活性，建立了平衡推理质量、成本和隐私的新帕累托边界，支持包括本地开源模型在内的多样化部署。

Conclusion: MemRec通过架构解耦解决了协作信号整合的挑战，实现了高效协作增强，为推荐系统在智能代理时代提供了灵活、成本效益高的解决方案。

Abstract: The evolution of recommender systems has shifted preference storage from rating matrices and dense embeddings to semantic memory in the agentic era. Yet existing agents rely on isolated memory, overlooking crucial collaborative signals. Bridging this gap is hindered by the dual challenges of distilling vast graph contexts without overwhelming reasoning agents with cognitive load, and evolving the collaborative memory efficiently without incurring prohibitive computational costs. To address this, we propose MemRec, a framework that architecturally decouples reasoning from memory management to enable efficient collaborative augmentation. MemRec introduces a dedicated, cost-effective LM_Mem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLM_Rec. The framework operates via a practical pipeline featuring efficient retrieval and cost-effective asynchronous graph propagation that evolves memory in the background. Extensive experiments on four benchmarks demonstrate that MemRec achieves state-of-the-art performance. Furthermore, architectural analysis confirms its flexibility, establishing a new Pareto frontier that balances reasoning quality, cost, and privacy through support for diverse deployments, including local open-source models. Code:https://github.com/rutgerswiselab/memrec and Homepage: https://memrec.weixinchen.com

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [16] [Derandomizing Matrix Concentration Inequalities from Free Probability](https://arxiv.org/abs/2601.08111)
*Robert Wang,Lap Chi Lau,Hong Zhou*

Main category: cs.DS

TL;DR: 提出多项式时间确定性算法，实现自由概率理论中的矩阵浓度不等式，应用于矩阵Spencer问题和近Ramanujan图构造


<details>
  <summary>Details</summary>
Motivation: 自由概率理论中的矩阵浓度不等式具有理论价值，但缺乏高效构造算法。本文旨在设计多项式时间确定性算法，将这些理论结果转化为实际可计算的构造

Method: 基于自由概率理论的概念和技术，开发多项式时间确定性算法，构造满足矩阵浓度不等式保证的结果

Result: 成功设计出多项式时间确定性算法，实现了矩阵浓度不等式的构造性保证，并应用于解决矩阵Spencer问题和构造近Ramanujan图

Conclusion: 自由概率理论不仅对数学分析有用，也能支持高效计算，为理论结果提供实际算法实现

Abstract: Recently, sharp matrix concentration inequalities~\cite{BBvH23,BvH24} were developed using the theory of free probability. In this work, we design polynomial time deterministic algorithms to construct outcomes that satisfy the guarantees of these inequalities. As direct consequences, we obtain polynomial time deterministic algorithms for the matrix Spencer problem~\cite{BJM23} and for constructing near-Ramanujan graphs. Our proofs show that the concepts and techniques in free probability are useful not only for mathematical analyses but also for efficient computations.

</details>


### [17] [Protrusion Decompositions Revisited: Uniform Lossy Kernels for Reducing Treewidth and Linear Kernels for Hitting Disconnected Minors](https://arxiv.org/abs/2601.08424)
*Roohani Sharma,Michał Włodarczyk*

Main category: cs.DS

TL;DR: 该论文提出了树宽d-删除问题的近似核化算法，获得了大小为g(d)·k⁵的2-近似核，并展示了通过调用有界大小实例的预言机可使近似因子任意接近1，同时在稀疏图类上获得了线性核。


<details>
  <summary>Details</summary>
Motivation: F-删除问题作为顶点覆盖和反馈顶点集的推广，先前工作已证明在核大小中对参数F的指数依赖不可避免。本文旨在避免这种非均匀性，通过近似核化来获得更简洁的核。

Method: 1. 为树宽d-删除问题设计简单的2-近似核化算法，获得大小为g(d)·k⁵的核；2. 通过调用有界大小实例的预言机，使近似因子可任意接近1；3. 在排除拓扑子图的稀疏图类上推广Kim等人的核化算法，获得线性核。

Result: 1. 获得了树宽d-删除问题的2-近似核，大小为g(d)·k⁵；2. 证明了通过预言机调用可使近似因子任意接近1；3. 在稀疏图类上获得了线性核，放宽了F中所有图必须连通的要求。

Conclusion: 本文通过近似核化方法避免了F-删除问题核大小中对参数F的指数依赖，为树宽d-删除问题提供了简洁的近似核，并在稀疏图类上获得了更强的核化结果。

Abstract: Let F be a finite family of graphs. In the F-Deletion problem, one is given a graph G and an integer k, and the goal is to find k vertices whose deletion results in a graph with no minor from the family F. This may be regarded as a far-reaching generalization of Vertex Cover and Feedback vertex Set. In their seminal work, Fomin, Lokshtanov, Misra & Saurabh [FOCS 2012] gave a polynomial kernel for this problem when the family F contains a planar graph. As the size of their kernel is g(F) * k^{f(F)}, a natural follow-up question was whether the dependence on F in the exponent of k can be avoided. The answer turned out to be negative: Giannapoulou, Jansen, Lokshtanov & Saurabh [TALG 2017] proved that this is already inevitable for the special case of the Treewidth-d-Deletion problem.
  In this work, we show that this non-uniformity can be avoided at the expense of a small loss. First, we present a simple 2-approximate kernelization algorithm for Treewidth-d-Deletion with kernel size g(d) * k^5. Next, we show that the approximation factor can be made arbitrarily close to 1, if we settle for a kernelization protocol with O(1) calls to an oracle that solves instances of size bounded by a uniform polynomial in k.
  We also obtain linear kernels on sparse graph classes when F contains a planar graph, whereas the previously known theorems required all graphs in F to be connected. Specifically, we generalize the kernelization algorithm by Kim, Langer, Paul, Reidl, Rossmanith, Sau & Sikdar [TALG 2015] on graph classes that exclude a topological minor.

</details>


### [18] [FPT Approximations for Connected Maximum Coverage](https://arxiv.org/abs/2601.08639)
*Tanmay Inamdar,Satyabrata Jana,Madhumita Kundu,Daniel Lokshtanov,Saket Saurabh,Meirav Zehavi*

Main category: cs.DS

TL;DR: 本文研究连通性约束的覆盖问题，提出Partial Connected Red-Blue Dominating Set模型，建立了该问题的参数化复杂性边界，包括固定参数可解性和参数化近似算法。


<details>
  <summary>Details</summary>
Motivation: 重新审视连通性约束的覆盖问题，通过统一的模型（Partial Connected Red-Blue Dominating Set）来捕捉先前文献中研究的连通变体问题，如Max Coverage、Partial Dominating Set和Partial Vertex Cover的连通版本。

Method: 提出Partial Connected Red-Blue Dominating Set模型：给定红蓝二分图G、红色顶点上的辅助连通图G_conn，以及整数k和t，任务是找到k个红色顶点的子集，使其至少支配t个蓝色顶点，并且在G_conn中诱导出连通子图。首先识别从已知问题继承的（参数化）不可近似性结果，然后证明该问题对于参数t是固定参数可解的。当二分图不包含K_{d,d}作为子图时，设计了近似t（和k）的参数化近似方案。

Result: 1. 该问题对于参数t是固定参数可解的；2. 当二分图排除K_{d,d}作为子图时，设计了近似t和k的参数化近似方案；3. 这些FPT近似方案对G_conn没有任何限制；4. 这些结果共同划定了连通性约束覆盖问题的硬度与FPT可近似性之间的边界。

Conclusion: 本文通过Partial Connected Red-Blue Dominating Set模型统一了连通性约束的覆盖问题，建立了该问题的参数化复杂性边界，包括固定参数可解性和参数化近似算法，为连通性约束覆盖问题的研究提供了理论框架。

Abstract: We revisit connectivity-constrained coverage through a unifying model, Partial Connected Red-Blue Dominating Set. Given a red-blue bipartite graph $G$ and an auxiliary connectivity graph $G_{conn}$ on red vertices, and integers $k, t$, the task is to find a $k$-sized subset of red vertices that dominates at least $t$ blue vertices, and that induces a connected subgraph in $G_{conn}$. This formulation captures connected variants of Max Coverage, Partial Dominating Set, and Partial Vertex Cover studied in prior literature.
  After identifying (parameterized) inapproximability results inherited from known problems, we first show that the problem is fixed-parameter tractable by $t$. Furthermore, when the bipartite graph excludes $K_{d,d}$ as a subgraph, we design (resp. efficient) parameterized approximation schemes for approximating $t$ (resp. $k$). Notably, these FPT approximations do not impose any restrictions on $G_{conn}$. Together, these results chart the boundary between hardness and FPT-approximability for connectivity-constrained coverage.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [19] [Efficient Synthesis for Two-Dimensional Strand Arrays with Row Constraints](https://arxiv.org/abs/2601.07968)
*Boaz Moav,Ryan Gabrys,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究DNA合成中的空间约束问题，分析在每行每周期最多合成一条链的限制下，两条链的期望合成时间，提出策略并证明最优性边界。


<details>
  <summary>Details</summary>
Motivation: 受大规模DNA合成技术驱动，研究在空间约束下的DNA链合成问题。在阵列中合成DNA链时，每行每周期最多只能合成一条链，需要分析这种约束对合成效率的影响。

Method: 将合成过程建模为马尔可夫链，推导期望合成时间的上下界。提出"滞后优先"策略，分析不同字母表大小下的性能。对于二进制情况，研究单符号前瞻的改进效果。最后提出动态规划算法计算最优离线调度。

Result: 证明"滞后优先"策略对于任意大小为q的字母表，能达到(q+3)L/2的渐近期望完成时间，且无前瞻的在线策略无法超越此界限。二进制情况下，单符号前瞻可将时间改进到7L/3。动态规划算法能计算任意固定序列对的最优离线调度。

Conclusion: 首次为空间约束下的DNA合成提供了理论分析框架和性能边界，为未来研究此类约束下的最优合成策略奠定了基础。

Abstract: We study the theoretical problem of synthesizing multiple DNA strands under spatial constraints, motivated by large-scale DNA synthesis technologies. In this setting, strands are arranged in an array and synthesized according to a fixed global synthesis sequence, with the restriction that at most one strand per row may be synthesized in any synthesis cycle. We focus on the basic case of two strands in a single row and analyze the expected completion time under this row-constrained model. By decomposing the process into a Markov chain, we derive analytical upper and lower bounds on the expected synthesis time. We show that a simple laggard-first policy achieves an asymptotic expected completion time of (q+3)L/2 for any alphabet of size q, and that no online policy without look-ahead can asymptotically outperform this bound. For the binary case, we show that allowing a single-symbol look-ahead strictly improves performance, yielding an asymptotic expected completion time of 7L/3. Finally, we present a dynamic programming algorithm that computes the optimal offline schedule for any fixed pair of sequences. Together, these results provide the first analytical bounds for synthesis under spatial constraints and lay the groundwork for future studies of optimal synthesis policies in such settings.

</details>


### [20] [Distributed Detection under Stringent Resource Constraints](https://arxiv.org/abs/2601.07989)
*Abdelaziz Bounhar,Mireille Sarkiss,Michèle Wigger*

Main category: cs.IT

TL;DR: 本文研究了分布式检测中的Stein指数，分析了三种严格通信约束下的性能：1)信道使用次数随观测数n次线性增长；2)信道使用次数为n但施加几乎必然的块输入成本约束；3)仅在期望上施加块输入约束。发现了基于DMC连接性的二分现象。


<details>
  <summary>Details</summary>
Motivation: 研究在严格通信约束下的分布式检测性能，特别是当传感器通过离散无记忆信道向决策中心通信时，在不同约束条件下的Stein指数表现。探索信道连接性对检测性能的影响。

Method: 针对三种通信约束场景：1)信道使用次数次线性增长；2)几乎必然的块输入成本约束；3)期望约束。分析部分连接和全连接DMC下的Stein指数，采用Han的零速率编码策略适配部分连接DMC，并提出了新的编码策略和逆证明。

Result: 发现二分现象：对于部分连接DMC，Stein指数与Han和Kobayashi以及Shalaby和Papamarcou在零速率无噪声通信场景下的指数相同；对于全连接DMC，在前两种场景下Stein指数退化为局部测试指数，传感器通信变得无用；在第三种场景下传感器仍有帮助但性能下降。

Conclusion: DMC的连接结构对分布式检测的Stein指数有决定性影响。部分连接DMC能保持零速率通信的性能，而全连接DMC在严格约束下性能严重退化。期望约束相比几乎必然约束能保留更多传感器价值。

Abstract: This paper identifies the Stein-exponent of distributed detection when the sensor communicates to the decision center over a discrete memoryless channel (DMC) subject to one of three stringent communication constraints: 1) The number of channel uses of the DMC grows sublinearly in the number of source observations n; 2) The number of channel uses is n but a block-input cost constraint is imposed almost surely, which grows sublinearly in n; 3) The block-input constraint is imposed only on expectation. We identify a dichotomy in the Stein-exponent of all these setups depending on the structure of the DMC's transition law. Under any of these three constraints, when the DMC is partially-connected (i.e., some outputs cannot be induced by certain inputs) the Stein-exponent matches the exponent identified by Han and Kobayashi and by Shalaby and Papamarcou for the scenario where communication is of zero-rate but over a noiseless link. We prove our result by adapting Han's zero-rate coding strategy to partially-connected DMCs.
  In contrast, for fully-connected DMCs, in our scenarios 1) and 2) the Stein-exponent collapses to that of a local test at the decision center, rendering the remote sensor and communication useless. %To prove this result, we propose new converse proofs relying on change of measure arguments.
  In scenario 3), the sensor remains beneficial even for fully-connected DMCs, however also collapses compared to the case of a partially-connected DMC. Moreover, the Stein-exponent is larger when the expectation constraint is imposed only under the null hypothesis compared to when it is imposed under both hypotheses. To prove these results, we propose both new coding strategies and new converse proofs.

</details>


### [21] [The many faces of multivariate information](https://arxiv.org/abs/2601.08030)
*Thomas F. Varley*

Main category: cs.IT

TL;DR: 提出了一个统一框架Δ^k，将多种高阶信息度量（双总相关、S信息、O信息）统一为参数k的函数，并揭示了高阶冗余与协同作用的层次结构。


<details>
  <summary>Details</summary>
Motivation: 现有信息论中有多种度量高阶信息共享的统计量，但缺乏统一框架。需要一种更一般化的方法来理解复杂系统中高阶冗余和协同相互作用。

Method: 提出Δ^k函数作为统一框架，通过自由参数k可以恢复不同现有度量：Δ^0对应S信息，Δ^1对应双总相关，Δ^2对应负O信息。同时利用熵共轭框架得到其共轭Γ^k。

Result: Δ^k形成了高阶协同作用的层次结构：Δ^k>0表示系统由高于k阶的相互作用主导，Δ^k<0表示由低于k阶的相互作用主导，Δ^k=0表示系统完全由k阶协同作用组成。Γ^k则形成了类似的高阶冗余层次结构。

Conclusion: 该统一框架为理解高阶冗余和协同相互作用提供了新视角，将现有多种度量整合为更连贯的结构，有助于复杂系统中高阶结构的提取和分析。

Abstract: Extracting higher-order structures from multivariate data has become an area of intensive study in complex systems science, as these multipartite interactions can reveal insights into fundamental features of complex systems like emergent phenomena. Information theory provides a natural language for exploring these interactions, as it elegantly formalizes the problem of comparing ``wholes" and ``parts" using joint, conditional, and marginal entropies. A large number of distinct statistics have been developed over the years, all aiming to capture different aspects of ``higher-order" information sharing. Here, we show that three of them (the dual total correlation, S-information, and O-information) are special cases of a more general function, $Δ^{k}$ which is parameterized by a free parameter $k$. For different values of $k$, we recover different measures: $Δ^{0}$ is equal to the S-information, $Δ^{1}$ is equal to the dual total correlation, and $Δ^{2}$ is equal to the negative O-information. Generally, the $Δ^{k}$ function is arranged into a hierarchy of increasingly high-order synergies; for a given value of $k$, if $Δ^{k}>0$, then the system is dominated by interactions with order greater than $k$, while if $Δ^{k}<0$, then the system is dominated by interactions with order lower than $k$. $Δ^{k}=0$ if the system is composed entirely of synergies of order-k. Using the entropic conjugation framework, we also find that the conjugate of $Δ^{k}$, which we term $Γ^{k}$ is arranged into a similar hierarchy of increasingly high-order redundancies. These results provide new insights into the nature of both higher-order redundant and synergistic interactions, and helps unify the existing zoo of measures into a more coherent structure.

</details>


### [22] [Cardinality-consistent flag codes with longer type vectors](https://arxiv.org/abs/2601.08144)
*Junfeng Jia,Yanxun Chang*

Main category: cs.IT

TL;DR: 该论文提出了两种旗码构造方法：一种是最优距离旗码，具有最长可能的类型向量；另一种是具有更长类型向量的旗码，两者都达到了相同的码字数量。


<details>
  <summary>Details</summary>
Motivation: 旗码将常维码推广到考虑具有规定维数的嵌套子空间序列作为码字。需要构建具有良好性质的旗码，特别是能够达到最优距离和大码字数量的构造。

Method: 提出了一种统一的构造方法，结合了循环轨道旗码，在有限域F_q^n上构建两种旗码族（其中n=sk+h，s≥2，0≤h<k）。

Result: 成功构造了两种旗码：1）具有最长可能类型向量(1,2,...,k,n-k,...,n-1)的最优距离旗码；2）具有更长类型向量(1,2,...,k+h,2k+h,...,(s-2)k+h,n-k,...,n-1)的旗码。两种旗码都达到了相同的码字数量∑_{i=1}^{s-1} q^{ik+h}+1。

Conclusion: 该论文提供了一个统一的旗码构造框架，能够生成具有最优距离特性和较大码字数量的旗码，扩展了旗码的设计空间。

Abstract: Flag codes generalize constant dimension codes by considering sequences of nested subspaces with prescribed dimensions as codewords. A comprehensive construction, which unites cyclic orbit flag codes, yields two families of flag codes on $\mathbb{F}^n_q$ (where $n=sk+h$ with $s\geq 2$ and $0\leq h < k$): optimum distance flag codes of the longest possible type vector $(1, 2, \ldots, k, n-k, \ldots, n-1)$ and flag codes with longer type vectors $(1, 2, \ldots, k+h, 2k+h, \ldots, (s-2)k+h, n-k, \ldots, n-1)$. These flag codes achieve the same cardinality $\sum^{s-1}_{i=1}q^{ik+h}+1$.

</details>


### [23] [From Antenna Abundance to Antenna Intelligence in 6G Gigantic MIMO Systems](https://arxiv.org/abs/2601.08326)
*Emil Björnson,Amna Irshad,Özlem Tugfe Demir,Giuseppe Thadeu Freitas de Abreu,Alva Kosasih,Vitaly Petrov*

Main category: cs.IT

TL;DR: 论文提出通过智能非均匀稀疏阵列设计减少天线数量，替代传统大规模MIMO依赖大量天线的做法，实现更高效、可持续的Gigantic MIMO系统。


<details>
  <summary>Details</summary>
Motivation: 当前大规模MIMO系统依赖大量天线实现高谱效，但未来网络需要数百天线，带来硬件复杂度、成本和功耗问题。需要更智能的阵列设计来减少天线需求。

Method: 采用非均匀稀疏阵列设计，基于预优化的不规则阵列或实时可移动天线，进行站点特定的天线布局。这些原理受无线定位研究启发，通过非均匀空间采样减少冗余。

Result: 数值模拟显示，使用更少天线即可实现优越的多用户MIMO性能，提高平均和速率等指标。智能天线设计可替代单纯增加天线数量。

Conclusion: 未来天线阵列设计需要范式转变：用天线智能替代天线数量，为高效、自适应、可持续的Gigantic MIMO系统开辟新机会。

Abstract: Current cellular systems achieve high spectral efficiency through Massive MIMO, which leverages an abundance of antennas to create favorable propagation conditions for multiuser spatial multiplexing. Looking towards future networks, the extrapolation of this paradigm leads to systems with many hundreds of antennas per base station, raising concerns regarding hardware complexity, cost, and power consumption. This article suggests more intelligent array designs that reduce the need for excessive antenna numbers. We revisit classical uniform array design principles and explain how their uniform spatial sampling leads to unnecessary redundancies in practical deployment scenarios. By exploiting non-uniform sparse arrays with site-specific antenna placements -- based on either pre-optimized irregular arrays or real-time movable antennas -- we demonstrate how superior multiuser MIMO performance can be achieved with far fewer antennas. These principles are inspired by previous works on wireless localization. We explain and demonstrate numerically how these concepts can be adapted for communications to improve the average sum rate and similar metrics. The results suggest a paradigm shift for future antenna array design, where antenna intelligence replaces sheer antenna count. This opens new opportunities for efficient, adaptable, and sustainable Gigantic MIMO systems.

</details>


### [24] [Movable Antenna for Integrating Near-field Channel Estimation and Localization](https://arxiv.org/abs/2601.08357)
*Chongjia Sun,Ziwei Wan,Lipeng Zhu,Zhenyu Xiao,Zhen Gao,Rui Zhang*

Main category: cs.IT

TL;DR: 提出一种基于可移动天线的宽带近场ISAC多阶段设计框架，通过子区域划分、NOMP角度估计、LSRC散射体定位和信道估计优化，显著提升近场感知精度和通信性能。


<details>
  <summary>Details</summary>
Motivation: 可移动天线为未来无线通信系统带来新的自由度，其大范围移动使无线信道传输进入近场区域，为集成感知与通信带来新的性能提升机会。然而，近场ISAC系统需要高效的设计框架来处理近场效应并优化性能。

Method: 提出多阶段设计框架：1）将天线移动区域划分为多个子区域；2）在每个子区域使用牛顿化正交匹配追踪算法进行高精度角度估计；3）提出近场定位通过子区域射线聚类方法识别散射体位置；4）基于散射体位置估计优化近场信道估计。

Result: 仿真结果表明，所提方案能显著提升可移动天线的感知精度和信道估计性能，为基于可移动天线的近场ISAC提供了高效解决方案。

Conclusion: 该多阶段设计框架有效解决了可移动天线近场ISAC系统的关键问题，通过子区域处理和联合优化实现了感知与通信性能的协同提升，为未来无线通信系统提供了有前景的技术路径。

Abstract: Movable antenna (MA) introduces a new degree of freedom for future wireless communication systems by enabling the adaptive adjustment of antenna positions. Its large-range movement renders wireless channels transmission into the near-field region, which brings new performance enhancement for integrated sensing and communication (ISAC). This paper proposes a novel multi-stage design framework for broadband near-field ISAC assisted by MA. The framework first divides the MA movement area into multiple subregions, and employs the Newtonized orthogonal matching pursuit algorithm (NOMP) to achieve high-precision angle estimation in each subregion. Subsequently, a method called near-field localization via subregion ray clustering (LSRC) is proposed for identifying the positions of scatterers. This method finds the coordinates of each scatterer by jointly processing the angle estimates across all subregions. Finally, according to the estimated locations of the scatterers, the near-field channel estimation (CE) is refined for improving communication performance. Simulation results demonstrate that the proposed scheme can significantly enhance MA sensing accuracy and CE, providing an efficient solution for MA-aided near-field ISAC.

</details>


### [25] [On the Generalization Error of Differentially Private Algorithms Via Typicality](https://arxiv.org/abs/2601.08386)
*Yanxiao Liu,Chun Hei Michael Shiu,Lele Wang,Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文从信息论角度研究随机学习算法的泛化误差，特别关注差分隐私算法的更紧致边界。通过典型性论证和隐私算法的稳定性，将互信息和最大泄漏上界化为显式可计算公式，改进现有边界并得到新的泛化误差保证。


<details>
  <summary>Details</summary>
Motivation: 研究随机学习算法的泛化误差，特别关注差分隐私算法，因为已知泛化误差可以通过互信息和最大泄漏来界定，但现有边界不够紧致，需要改进以获得更精确的泛化误差保证。

Method: 采用信息论视角，使用典型性论证和差分隐私算法的稳定性特性，将互信息和最大泄漏上界化为显式可计算的公式。第一部分改进Rodríguez-Gálvez等人的互信息边界，第二部分推导学习算法最大泄漏的新上界。

Result: 严格改进了Rodríguez-Gálvez等人的互信息边界，并推导了学习算法最大泄漏的新上界。这些信息度量的边界直接转化为泛化误差的保证，包括期望保证和高概率保证。

Conclusion: 通过信息论方法，特别是利用差分隐私算法的稳定性，本文为随机学习算法（尤其是隐私算法）提供了更紧致的泛化误差边界，改进了现有结果并提供了新的理论保证。

Abstract: We study the generalization error of stochastic learning algorithms from an information-theoretic perspective, with a particular emphasis on deriving sharper bounds for differentially private algorithms. It is well known that the generalization error of stochastic learning algorithms can be bounded in terms of mutual information and maximal leakage, yielding in-expectation and high-probability guarantees, respectively. In this work, we further upper bound mutual information and maximal leakage by explicit, easily computable formulas, using typicality-based arguments and exploiting the stability properties of private algorithms. In the first part of the paper, we strictly improve the mutual-information bounds by Rodríguez-Gálvez et al. (IEEE Trans. Inf. Theory, 2021). In the second part, we derive new upper bounds on the maximal leakage of learning algorithms. In both cases, the resulting bounds on information measures translate directly into generalization error guarantees.

</details>


### [26] [An Efficient Algorithm to Sample Quantum Low-Density Parity-Check Codes](https://arxiv.org/abs/2601.08387)
*Paolo Santini*

Main category: cs.IT

TL;DR: 提出一种高效算法，用于采样随机稀疏矩阵作为量子LDPC码的校验矩阵，该算法基于信息集解码(ISD)技术，是纯组合方法而非代数构造。


<details>
  <summary>Details</summary>
Motivation: 现有量子LDPC码构造方法多为代数方法，需要满足特定性质（如准循环性），限制了随机性。需要一种更通用、更随机的稀疏自正交矩阵采样方法。

Method: 使用信息集解码(ISD)技术逐行采样稀疏矩阵H，确保H满足自正交条件HH^T=0。算法是纯组合方法，不依赖代数结构，可推广到非二进制有限域和更一般的量子稳定子LDPC码。

Result: 算法在理论参数范围内可行，计算复杂度可预期。数值模拟和基准测试证实了方法的可行性和高效性。

Conclusion: 提出了一种概念简单但高效的随机稀疏自正交矩阵采样算法，为量子LDPC码提供了更灵活、更随机的构造方法，克服了现有代数方法的局限性。

Abstract: In this paper, we present an efficient algorithm to sample random sparse matrices to be used as check matrices for quantum Low-Density Parity-Check (LDPC) codes. To ease the treatment, we mainly describe our algorithm as a technique to sample a dual-containing binary LDPC code, hence, a sparse matrix $\mathbf H\in\mathbb F_2^{r\times n}$ such that $\mathbf H\mathbf H^\top = \mathbf 0$. However, as we show, the algorithm can be easily generalized to sample dual-containing LDPC codes over non binary finite fields as well as more general quantum stabilizer LDPC codes.
  While several constructions already exist, all of them are somewhat algebraic as they impose some specific property (e.g., the matrix being quasi-cyclic). Instead, our algorithm is purely combinatorial as we do not require anything apart from the rows of $\mathbf H$ being sparse enough. In this sense, we can think of our algorithm as a way to sample sparse, self-orthogonal matrices that are as random as possible.
  Our algorithm is conceptually very simple and, as a key ingredient, uses Information Set Decoding (ISD) to sample the rows of $\mathbf H$, one at a time. The use of ISD is fundamental as, without it, efficient sampling would not be feasible. We give a theoretical characterization of our algorithm, determining which ranges of parameters can be sampled as well as the expected computational complexity. Numerical simulations and benchmarks confirm the feasibility and efficiency of our approach.

</details>


### [27] [Distribution Estimation with Side Information](https://arxiv.org/abs/2601.08535)
*Haricharan Balasundaram,Andrew Thangaraj*

Main category: cs.IT

TL;DR: 本文研究了在具有额外侧信息的情况下离散分布估计的经典问题，通过局部模型和偏序模型理论分析了侧信息对平方误差风险的改进效果。


<details>
  <summary>Details</summary>
Motivation: 在大字母表数据集（如文本语料库）中，自然存在通过词向量嵌入的相似性推断出的词义/相似性等侧信息。本文旨在探索如何利用这些侧信息来改进离散分布估计。

Method: 提出了两种侧信息模型：1）局部模型，其中未知分布位于已知分布的邻域内；2）偏序模型，其中字母表被划分为已知的高概率集和低概率集。理论分析了这两种模型中平方误差风险的改进。

Result: 理论分析表明，在两种模型中，可用的侧信息都能显著改善平方误差风险。在自然语言和合成数据上的模拟实验验证了这些改进效果。

Conclusion: 利用自然语言处理中常见的词向量相似性等侧信息，可以显著改进离散分布估计的性能，为大规模字母表数据集的分析提供了新的理论框架和实用方法。

Abstract: We consider the classical problem of discrete distribution estimation using i.i.d. samples in a novel scenario where additional side information is available on the distribution. In large alphabet datasets such as text corpora, such side information arises naturally through word semantics/similarities that can be inferred by closeness of vector word embeddings, for instance. We consider two specific models for side information--a local model where the unknown distribution is in the neighborhood of a known distribution, and a partial ordering model where the alphabet is partitioned into known higher and lower probability sets. In both models, we theoretically characterize the improvement in a suitable squared-error risk because of the available side information. Simulations over natural language and synthetic data illustrate these gains.

</details>


### [28] [On the Optimality of Decode and Forward for Some Cooperative Broadcast Channels](https://arxiv.org/abs/2601.08592)
*Nicolas Le Gouic,Yossef Steinberg,Michèle Wigger*

Main category: cs.IT

TL;DR: 本文针对具有从强接收机到弱接收机单向协作的更强大广播信道，提出了新的容量区域边界点，并通过简单的叠加编码和译码转发方案实现。


<details>
  <summary>Details</summary>
Motivation: 研究具有单向协作的更强大广播信道的容量区域，探索如何通过协作机制提升信道性能，特别是当强接收机可以协助弱接收机时的容量边界。

Method: 采用简单的编码方案：在发射端使用叠加编码，在强接收机处采用译码转发策略。强接收机解码后转发信息给弱接收机，实现协作通信。

Result: 确定了该类信道新的容量区域边界点。将一般结果应用于高斯广播信道以及由二进制擦除信道（BEC）到强接收机和二进制对称信道（BSC）到弱接收机组成的广播信道。

Conclusion: 通过叠加编码和译码转发方案，成功刻画了具有单向协作的更强大广播信道的新容量边界，为这类协作广播信道的性能分析提供了理论框架。

Abstract: This article characterizes new boundary points on the capacity region of certain classes of more capable broadcast channels (BC) with uni-directional cooperation from the stronger to the weaker receiver. The new boundary points are achieved by a simple coding scheme that employs superposition coding at the transmitter with decode and forward at the stronger receiver. We evaluate our general result for Gaussian BCs and for a BC consisting of a binary erasure channel (BEC) to the stronger receiver and a binary symmetric channel (BSC) to the weaker receiver.

</details>


### [29] [Quantum CSS LDPC Codes based on Dyadic Matrices for Belief Propagation-based Decoding](https://arxiv.org/abs/2601.08636)
*Alessio Baldelli,Massimo Battaglioni,Jonathan Mandelbaum,Sisi Miao,Laurent Schmalen*

Main category: cs.IT

TL;DR: 提出基于二元矩阵的代数构造方法，设计具有围长6的经典和量子LDPC码，并满足CAMEL-ensemble四元BP解码器的兼容性条件


<details>
  <summary>Details</summary>
Motivation: 量子LDPC码在量子纠错中需要在纠错能力和实现复杂度之间取得平衡，需要设计既具有良好纠错性能又便于实现的量子LDPC码

Method: 使用二元矩阵代数构造方法，首先生成具有围长6的经典二元准二元LDPC码，然后扩展到CSS框架，构建满足CAMEL-ensemble四元BP解码器兼容性条件的两个分量奇偶校验矩阵

Result: 成功设计出满足兼容性条件的量子LDPC码，确保所有不可避免的长度为4的循环都聚集在单个变量节点中，从而可以通过对该变量节点进行消元来减轻其负面影响

Conclusion: 基于二元矩阵的代数构造方法为设计实用的量子LDPC码提供了一种有效途径，能够在保持良好纠错性能的同时满足现代解码器的兼容性要求

Abstract: Quantum low-density parity-check (QLDPC) codes provide a practical balance between error-correction capability and implementation complexity in quantum error correction (QEC). In this paper, we propose an algebraic construction based on dyadic matrices for designing both classical and quantum LDPC codes. The method first generates classical binary quasi-dyadic LDPC codes whose Tanner graphs have girth 6. It is then extended to the Calderbank-Shor-Steane (CSS) framework, where the two component parity-check matrices are built to satisfy the compatibility condition required by the recently introduced CAMEL-ensemble quaternary belief propagation decoder. This compatibility condition ensures that all unavoidable cycles of length 4 are assembled in a single variable node, allowing the mitigation of their detrimental effects by decimating that variable node.

</details>


### [30] [Multivariate Polynomial Codes for Efficient Matrix Chain Multiplication in Distributed Systems](https://arxiv.org/abs/2601.08708)
*Jesús Gómez-Vilardebò*

Main category: cs.IT

TL;DR: 本文提出两种多元多项式编码方案，专门用于分布式环境中的矩阵链乘法，在计算复杂度和存储开销之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 分布式计算集群中的矩阵链乘法存在"拖后腿"问题，现有编码计算策略主要针对两个矩阵相乘的简单情况，扩展到矩阵链乘法时，单变量多项式编码会导致计算和存储开销显著增加，限制了可扩展性。

Method: 提出了两种专门为分布式矩阵链乘法设计的多元多项式编码方案，相比单变量扩展方法，这些方案虽然增加了计算成本，但能显著降低存储开销。

Result: 多元多项式编码方案在计算和存储效率之间展现出基本权衡关系：虽然增加了计算成本，但能大幅减少存储开销，为大规模分布式线性代数任务提供了实用解决方案。

Conclusion: 多元多项式编码在分布式矩阵链乘法中具有实际应用潜力，通过计算复杂度和存储开销之间的权衡，为解决大规模分布式线性代数任务提供了新思路。

Abstract: We study the problem of computing matrix chain multiplications in a distributed computing cluster. In such systems, performance is often limited by the straggler problem, where the slowest worker dominates the overall computation latency. To resolve this issue, several coded computing strategies have been proposed, primarily focusing on the simplest case: the multiplication of two matrices. These approaches successfully alleviate the straggler effect, but they do so at the expense of higher computational complexity and increased storage needs at the workers. However, in many real-world applications, computations naturally involve long chains of matrix multiplications rather than just a single two-matrix product. Extending univariate polynomial coding to this setting has been shown to amplify the costs -- both computation and storage overheads grow significantly, limiting scalability. In this work, we propose two novel multivariate polynomial coding schemes specifically designed for matrix chain multiplication in distributed environments. Our results show that while multivariate codes introduce additional computational cost at the workers, they can dramatically reduce storage overhead compared to univariate extensions. This reveals a fundamental trade-off between computation and storage efficiency, and highlights the potential of multivariate codes as a practical solution for large-scale distributed linear algebra tasks.

</details>


### [31] [On the Algebraic Structure Underlying the Support Enumerators of Linear Codes](https://arxiv.org/abs/2601.08744)
*Nitin Kenjale,Anuradha S. Garge*

Main category: cs.IT

TL;DR: 论文引入支持分布和支持枚举器作为经典重量分布和重量枚举器的细化，捕捉线性分组码的坐标级活动，建立了计数公式和MacWilliams型恒等式，推导了基于支持分布相等的自对偶条件。


<details>
  <summary>Details</summary>
Motivation: 经典重量分布和重量枚举器只关注码字的整体重量，无法捕捉坐标级别的活动信息。需要更细粒度的工具来分析线性码的结构特性，特别是坐标层面的非零模式。

Method: 引入支持分布和支持枚举器概念，建立计数第i个坐标非零的码字数量的公式。推导支持分布之间的MacWilliams型恒等式，描述线性码与其对偶码之间坐标信息的变换关系。

Result: 获得了支持分布的计数公式和MacWilliams型恒等式，基于支持分布相等推导出自对偶码的判定条件。这些结果提供了比经典重量理论更详细的码结构理解。

Conclusion: 支持分布和支持枚举器是分析线性码坐标级活动的有效工具，建立的MacWilliams型恒等式和对偶理论补充了经典重量理论，为码结构分析提供了更精细的框架。

Abstract: In this paper, we have introduced the concepts of support distribution and the support enumerator as refinements of the classical weight distribution and weight enumerator respectively, capturing coordinate level activity in linear block codes. More precisely, we have established formula for counting codewords in the linear code C whose i-th coordinate is nonzero. Moreover, we derived a MacWilliam's type identity, relating the normalized support enumerators of a linear code and its dual, explaining how coordinate information transforms under duality. Using this identity we deduce a condition for self duality based on the equality of support distributions. These results provide a more detailed understanding of code structure and complement classical weight based duality theory.

</details>


### [32] [Majority-Logic Decoding of Binary Locally Recoverable Codes: A Probabilistic Analysis](https://arxiv.org/abs/2601.08765)
*Hoang Ly,Emina Soljanin,Philip Whiting*

Main category: cs.IT

TL;DR: 本文研究了二进制线性局部可修复码在多数逻辑解码下的纠错性能，推导了在BEC和BSC信道上的解码失败概率上界，揭示了最坏情况保证与典型性能之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 局部可修复码（LRCs）最初是为分布式存储系统设计的，通过仅访问少量其他符号实现高效擦除恢复。虽然其结构特性（如界和构造）已被广泛研究，但LRCs在随机擦除和错误下的性能尚未得到充分探索。

Method: 研究二进制线性LRCs在多数逻辑解码（MLD）下的纠错性能，关注具有固定局部性和可变可用性的LRCs。推导在记忆二进制擦除信道（BEC）和二进制对称信道（BSC）上的解码失败概率的显式上界。

Result: 分析表明，在可用性满足温和增长条件下，块解码失败概率渐近消失，多数逻辑解码可以成功纠正几乎所有与块长度成线性关系的错误和擦除模式。结果揭示了最坏情况保证与随机信道模型下典型性能之间的显著差距。

Conclusion: 本文首次系统分析了LRCs在随机信道下的性能，为实际分布式存储系统的设计提供了理论依据，表明多数逻辑解码在实际应用中可能比最坏情况分析预测的更有效。

Abstract: Locally repairable codes (LRCs) were originally introduced to enable efficient recovery from erasures in distributed storage systems by accessing only a small number of other symbols. While their structural properties-such as bounds and constructions-have been extensively studied, the performance of LRCs under random erasures and errors has remained largely unexplored. In this work, we study the error- and erasure-correction performance of binary linear LRCs under majority-logic decoding (MLD). Focusing on LRCs with fixed locality and varying availability, we derive explicit upper bounds on the probability of decoding failure over the memoryless Binary Erasure Channel (BEC) and Binary Symmetric Channel (BSC). Our analysis characterizes the behavior of the bit-error rate (BER) and block-error rate (BLER) as functions of the locality and availability parameters. We show that, under mild growth conditions on the availability, the block decoding failure probability vanishes asymptotically, and that majority-logic decoding can successfully correct virtually all of error and erasure patterns of weight linear in the blocklength. The results reveal a substantial gap between worst-case guarantees and typical performance under stochastic channel models.

</details>


### [33] [LWM-Spectro: A Foundation Model for Wireless Baseband Signal Spectrograms](https://arxiv.org/abs/2601.08780)
*Namhyun Kim,Sadjad Alikhani,Ahmed Alkhateeb*

Main category: cs.IT

TL;DR: LWM-Spectro是一个基于Transformer的基础模型，通过在I/Q信号时频谱图上进行大规模预训练，结合掩码建模、对比学习和MoE架构，学习通用的无线表示，在调制分类等下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 无线通信中的I/Q基带信号编码了物理层和信道特性，但由于通信系统异构、传播环境多样、标记数据有限，直接从原始信号学习鲁棒且可迁移的表示具有挑战性。

Method: 提出LWM-Spectro模型：1) 将I/Q数据表示为时频谱图；2) 基于Transformer架构；3) 使用自监督掩码建模和对比学习；4) 采用混合专家(MoE)架构；5) 在大规模I/Q数据上进行预训练。

Result: 模型在调制分类和联合SNR/移动性识别等下游任务中，在少样本和数据丰富情况下均优于现有深度学习方法，提供了统一的无线学习基础。

Conclusion: LWM-Spectro通过学习通用的无线表示，有效解决了无线信号表示学习的挑战，为各种无线任务提供了强大的基础模型。

Abstract: The received in-phase and quadrature (I/Q) baseband signals inherently encode physical-layer and channel characteristics of wireless links. Learning robust and transferable representations directly from such raw signals, however, remains challenging due to heterogeneous communication systems, diverse propagation environments, and limited labeled data. To address this, we present LWM-Spectro, a transformer-based foundation model pretrained on large-scale I/Q data represented as time-frequency spectrograms. The model leverages self-supervised masked modeling, contrastive learning, and a mixture-of-experts (MoE) architecture to learn general-purpose wireless representations. These representations transfer effectively to downstream tasks such as modulation classification and joint SNR/mobility recognition, even with minimal supervision. Across tasks, LWM-Spectro consistently outperforms state-of-the-art deep learning baselines in both few-shot and data-rich regimes, providing a unified foundation for wireless learning.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [34] [MLLM-VADStory: Domain Knowledge-Driven Multimodal LLMs for Video Ad Storyline Insights](https://arxiv.org/abs/2601.07850)
*Jasmine Yang,Poppy Zhang,Shawndra Hill*

Main category: cs.MM

TL;DR: 提出MLLM-VADStory框架，利用领域知识指导多模态大语言模型，系统量化视频广告故事线理解，通过功能单元分割和分类分析广告叙事结构。


<details>
  <summary>Details</summary>
Motivation: 视频广告的叙事结构对广告效果有重要影响，但缺乏系统化、可扩展的分析方法。现有方法难以大规模理解广告故事线的功能结构和叙事模式。

Method: 基于广告叙事由功能意图结构化的核心思想，将广告分割为功能单元，使用广告特定的功能角色分类法对每个单元进行分类，然后聚合功能序列以恢复数据驱动的故事线结构。

Result: 在4个行业子垂直领域的5万个社交媒体视频广告上应用该框架，发现故事型创意能提高视频留存率，并推荐了表现最佳的故事弧线来指导广告创意设计。

Conclusion: 该框架展示了利用领域知识指导MLLMs生成可扩展的视频广告故事线洞察的价值，使其成为理解视频创意的一般性多功能工具。

Abstract: We propose MLLM-VADStory, a novel domain knowledge-guided multimodal large language models (MLLM) framework to systematically quantify and generate insights for video ad storyline understanding at scale. The framework is centered on the core idea that ad narratives are structured by functional intent, with each scene unit performing a distinct communicative function, delivering product and brand-oriented information within seconds. MLLM-VADStory segments ads into functional units, classifies each unit's functionality using a novel advertising-specific functional role taxonomy, and then aggregates functional sequences across ads to recover data-driven storyline structures. Applying the framework to 50k social media video ads across four industry subverticals, we find that story-based creatives improve video retention, and we recommend top-performing story arcs to guide advertisers in creative design. Our framework demonstrates the value of using domain knowledge to guide MLLMs in generating scalable insights for video ad storylines, making it a versatile tool for understanding video creatives in general.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [35] [How Hard Is It to Rig a Tournament When Few Players Can Beat or Be Beaten by the Favorite?](https://arxiv.org/abs/2601.08530)
*Zhonghao Wang,Junqiang Peng,Yuxi Liu,Mingyu Xiao*

Main category: cs.GT

TL;DR: 本文研究了淘汰赛中的赛程安排问题，证明了当指定选手的入度或出度较小时，该问题是固定参数可解的。


<details>
  <summary>Details</summary>
Motivation: 淘汰赛赛程安排问题（TFP）已知是NP难的，但可以通过反馈弧集或反馈顶点集等结构参数实现固定参数可解。本文旨在探索新的结构参数：指定选手的入度（能击败该选手的人数）和出度（该选手能击败的人数），研究当这些参数较小时问题是否可高效求解。

Method: 引入两个新参数：指定选手v*的入度k（能击败v*的选手数）和出度ℓ（v*能击败的选手数）。通过参数化算法设计，证明TFP在参数化为k或ℓ时是固定参数可解的。特别是入度参数化的算法设计较为复杂和技术性强。

Result: 证明了TFP在参数化为指定选手的入度k或出度ℓ时是固定参数可解的。入度参数化的算法尤其复杂。值得注意的是，即使其他结构参数（如反馈弧集或反馈顶点集）很大时，入度k仍可能很小，这为问题提供了新的解决视角。

Conclusion: 本文为淘汰赛赛程安排问题引入了新的结构参数（指定选手的入度和出度），并证明了在这些参数较小时问题可高效求解，显著扩展了该问题的参数化算法理解。

Abstract: In knockout tournaments, players compete in successive rounds, with losers eliminated and winners advancing until a single champion remains. Given a tournament digraph $D$, which encodes the outcomes of all possible matches, and a designated player $v^* \in V(D)$, the \textsc{Tournament Fixing} problem (TFP) asks whether the tournament can be scheduled in a way that guarantees $v^*$ emerges as the winner. TFP is known to be NP-hard, but is fixed-parameter tractable (FPT) when parameterized by structural measures such as the feedback arc set (fas) or feedback vertex set (fvs) number of the tournament digraph. In this paper, we introduce and study two new structural parameters: the number of players who can defeat $v^*$ (i.e., the in-degree of $v^*$, denoted by $k$) and the number of players that $v^*$ can defeat (i.e., the out-degree of $v^*$, denoted by $\ell$).
  A natural question is that: can TFP be efficiently solved when $k$ or $\ell$ is small? We answer this question affirmatively by showing that TFP is FPT when parameterized by either the in-degree or out-degree of $v^*$. Our algorithm for the in-degree parameterization is particularly involved and technically intricate. Notably, the in-degree $k$ can remain small even when other structural parameters, such as fas or fvs, are large. Hence, our results offer a new perspective and significantly broaden the parameterized algorithmic understanding of the \textsc{Tournament Fixing} problem.

</details>


### [36] [Robust Stable Matchings: Dealing with Changes in Preferences](https://arxiv.org/abs/2601.07959)
*Rohith Reddy Gangam,Tung Mai,Nitya Raju,Vijay V. Vazirani*

Main category: cs.GT

TL;DR: 研究稳定匹配在偏好变化下的鲁棒性，分析不同偏好扰动模型下的结构性质与计算复杂度边界


<details>
  <summary>Details</summary>
Motivation: 传统稳定匹配理论针对单一偏好实例，但实际匹配市场中偏好可能变化、误报或存在不确定性。需要研究跨多个偏好实例都保持稳定的鲁棒匹配，理解其结构性质与计算特性

Method: 建立偏好扰动层次模型：1) 单个代理偏好列表向上移位；2) 单个代理任意排列变化；3) 多个代理任意偏好变化。分析每种情况下鲁棒稳定匹配的格结构、Birkhoff偏序、最优匹配计算、多面体完整性

Result: 刻画了鲁棒稳定匹配在不同扰动模型下是否形成子格、是否允许高效枚举、能否高效计算最优匹配、多面体是否完整。提供反例展示性质失效边界，并给出参数化算法（O(n^k)时间，k为偏好变化代理数）

Conclusion: 系统研究了鲁棒稳定匹配的结构与计算特性，精确划定了可处理与难处理情况的边界，为偏好不确定的匹配市场提供了理论基础

Abstract: We study stable matchings that are robust to preference changes in the two-sided stable matching setting of Gale and Shapley [GS62]. Given two instances $A$ and $B$ on the same set of agents, a matching is said to be robust if it is stable under both instances. This notion captures desirable robustness properties in matching markets where preferences may evolve, be misreported, or be subject to uncertainty. While the classical theory of stable matchings reveals rich lattice, algorithmic, and polyhedral structure for a single instance, it is unclear which of these properties persist when stability is required across multiple instances. Our work initiates a systematic study of the structural and computational behavior of robust stable matchings under increasingly general models of preference changes.
  We analyze robustness under a hierarchy of perturbation models:
  1. a single upward shift in one agent's preference list,
  2. an arbitrary permutation change by a single agent, and
  3. arbitrary preference changes by multiple agents on both sides.
  For each regime, we characterize when:
  1. the set of robust stable matchings forms a sublattice,
  2. the lattice of robust stable matchings admits a succinct Birkhoff partial order enabling efficient enumeration,
  3. worker-optimal and firm-optimal robust stable matchings can be computed efficiently, and
  4. the robust stable matching polytope is integral (by studying its LP formulation).
  We provide explicit counterexamples demonstrating where these structural and geometric properties break down, and complement these results with XP-time algorithms running in $O(n^k)$ time, parameterized by $k$, the number of agents whose preferences change. Our results precisely delineate the boundary between tractable and intractable cases for robust stable matchings.

</details>


### [37] [Cities at Play: Improving Equilibria in Urban Neighbourhood Games](https://arxiv.org/abs/2601.08642)
*Martin Gairing,Adrian Vetta,Zhanzhan Zhao*

Main category: cs.GT

TL;DR: 论文提出了一种基于博弈论的Schelling有界邻域模型，通过精心设计的小规模投资来协调个体激励与社会福利，避免Braess悖论，实现社会效益最大化。


<details>
  <summary>Details</summary>
Motivation: 当个体根据本地条件做出战略性选择时，城市应如何投资以提高社会福利？传统方法可能因个体策略性反应而导致结果恶化（类似Braess悖论），需要找到能协调个体激励与社会目标的投资策略。

Method: 采用博弈论版本的Schelling有界邻域模型，个体基于反映本地人口的凹、非单调效用函数选择邻域。通过精心设计的小规模投资修改效用函数，确保纳什均衡与社会福利目标一致。

Result: 证明通过最多0.81ε²·opt的成本修改效用函数，可以保证每个纳什均衡的社会福利至少达到ε·opt（opt为最优社会福利）。这为战略城市规划提供了量化保证。

Conclusion: 有针对性的干预可以将超负结果转化为超正回报，为战略性城市规划和分散集体行为提供了新见解，展示了小规模投资如何协调个体激励与社会目标。

Abstract: How should cities invest to improve social welfare when individuals respond strategically to local conditions? We model this question using a game-theoretic version of Schelling's bounded neighbourhood model, where agents choose neighbourhoods based on concave, non-monotonic utility functions reflecting local population. While naive improvements may worsen outcomes - analogous to Braess' paradox - we show that carefully designed, small-scale investments can reliably align individual incentives with societal goals. Specifically, modifying utilities at a total cost of at most $0.81 ε^2 \cdot \texttt{opt}$ guarantees that every resulting Nash equilibrium achieves a social welfare of at least $ε\cdot \texttt{opt}$, where $\texttt{opt}$ is the optimum social welfare. Our results formalise how targeted interventions can transform supra-negative outcomes into supra-positive returns, offering new insights into strategic urban planning and decentralised collective behaviour.

</details>
