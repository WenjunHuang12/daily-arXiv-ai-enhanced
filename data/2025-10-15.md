<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 6]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network](https://arxiv.org/abs/2510.11866)
*Michael Crystal,Guy Goren,Scott Duke Kominers*

Main category: cs.GT

TL;DR: 本文对Shelby存储网络协议进行了首次正式激励属性证明，通过博弈论模型分析了其激励兼容性。


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化存储是Web3生态的核心组件，但大多数存储协议缺乏对其激励属性的正式分析。没有良好设计的激励机制，系统可能无法真正实现去中心化存储。

Method: 使用博弈论模型分析Shelby协议，结合链下审计和偶尔的链上验证机制，并研究联盟行为。

Result: 研究发现仅靠链下审计会导致普遍逃避责任，但Shelby的同行审计与偶尔链上验证组合在自然参数设置下能实现激励兼容。

Conclusion: Shelby协议在适当参数下具有激励兼容性，通过简单修改可以增强其抗合谋能力。

Abstract: Decentralized storage is one of the most natural applications built on
blockchains and a central component of the Web3 ecosystem. Yet despite a decade
of active development -- from IPFS and Filecoin to more recent entrants -- most
of these storage protocols have received limited formal analysis of their
incentive properties. Claims of incentive compatibility are sometimes made, but
rarely proven. This gap matters: without well-designed incentives, a system may
distribute storage but fail to truly decentralize it.
  We analyze Shelby -- a storage network protocol recently proposed by Aptos
Labs and Jump Crypto -- and provide the first formal proof of its incentive
properties. Our game-theoretic model shows that while off-chain audits alone
collapse to universal shirking, Shelby's combination of peer audits with
occasional on-chain verification yields incentive compatibility under natural
parameter settings. We also examine coalition behavior and outline a simple
modification that strengthens the protocol's collusion-resilience.

</details>


### [2] [Fair Division of Indivisible Items](https://arxiv.org/abs/2510.12158)
*Kevin Hsu*

Main category: cs.GT

TL;DR: 本文研究了不可分割物品的公平分配问题，包括一般模型和图形模型。主要贡献：1) 在m≤n+5时证明了混合物品（物品和杂务）的MMS分配存在性；2) 证明了多图中EFX定向的NP完全性；3) 给出了杂务图中EF1和EFX定向的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究不可分割物品的公平分配问题，特别是探索在一般模型和图形模型下各种公平标准（MMS、EF1、EFX）的可实现性和计算复杂性。

Method: 采用组合优化和图论方法，分析不同约束条件下的分配存在性和算法复杂性。对于混合物品MMS分配，使用归纳和案例分析方法；对于EFX定向，使用复杂性理论和多项式时间算法设计。

Result: 1) 当m≤n+5时，混合物品的MMS分配存在，除非所有代理的MMS阈值都为负；2) 多图中EFX定向判定是NP完全的；3) 杂务图中EF1和EFX定向可在多项式时间内判定。

Conclusion: 本文在公平分配领域取得了重要进展，揭示了混合物品MMS分配的存在条件，证明了EFX定向的复杂性，并发现了物品和杂务分配在计算复杂性上的根本差异。

Abstract: We study the fair division of indivisible items. In the general model, the
goal is to allocate $m$ indivisible items to $n$ agents while satisfying
fairness criteria such as MMS, EF1, and EFX. We also study a
recently-introduced graphical model that represents the fair division problem
as a multigraph, in which vertices correspond to agents and edges to items. The
graphical model stipulates that an item can have non-zero marginal utility to
an agent only if its corresponding edge is incident to the agent's
corresponding vertex. We study orientations (allocations that allocate each
edge to an endpoint) in this model, as they are particularly desirable.
  Our first contribution concerns MMS allocations of mixed manna (i.e. a
mixture of goods and chores) in the general model. It is known that MMS
allocations of goods exist when $m \leq n+5$. We generalize this and show that
when $m \leq n+5$, MMS allocations of mixed manna exist as long as $n \leq 3$,
there is an agent whose MMS threshold is non-negative, or every item is a
chore. Remarkably, our result leaves only the case where every agent has a
negative MMS threshold unanswered.
  Our second contribution concerns EFX orientations of multigraphs of goods. We
show that deciding whether EFX orientations exist for multigraphs is
NP-complete, even for symmetric bi-valued multigraphs. Complementarily, we show
symmetric bi-valued multigraphs that do not contain non-trivial odd multitrees
have EFX orientations that can be found in polynomial time.
  Our third contribution concerns EF1 and EFX orientations of graphs and
multigraphs of chores. We obtain polynomial-time algorithms for deciding
whether such graphs have EF1 and EFX orientations, resolving a previous
conjecture and showing a fundamental difference between goods and chores
division. In addition, we show that the analogous problems for multigraphs are
NP-hard.

</details>


### [3] [Single-Deviation Stability in Additively Separable Hedonic Games with Constrained Coalition Sizes](https://arxiv.org/abs/2510.12641)
*Martin Bullinger,Adam Dunajski,Edith Elkind,Matan Gilboa*

Main category: cs.GT

TL;DR: 该论文研究了在联盟规模有固定上下界限制的可加可分离享乐博弈中的稳定性问题，分析了四种基于单智能体偏离的稳定性概念及其变体，并提供了关于稳定结果存在性的完整图景和计算复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 研究在联盟规模受限的可加可分离享乐博弈中各种稳定性概念的存在性和计算复杂性，填补了该领域的研究空白。

Method: 考虑了四种经典稳定性概念（纳什稳定、个体稳定、契约纳什稳定、契约个体稳定）及其两种变体，分析了在不同规模参数下稳定结果的存在性，并对只有上界约束的情况进行了计算复杂性分析。

Result: 提供了关于稳定结果存在性的完整图景，获得了契约个体稳定性和契约纳什稳定性的多项式时间算法（后者要求上界为2），并在下界至少为2时获得了纳什稳定性和契约个体稳定性的进一步结果。

Conclusion: 该研究为联盟规模受限的享乐博弈稳定性分析提供了系统的理论框架和算法结果，揭示了不同稳定性概念在规模约束下的行为特征。

Abstract: We study stability in additively separable hedonic games when coalition sizes
have to respect fixed size bounds. We consider four classic notions of
stability based on single-agent deviations, namely, Nash stability, individual
stability, contractual Nash stability, and contractual individual stability.
For each stability notion, we consider two variants: in one, the coalition left
behind by a deviator must still be of a valid size, and in the other there is
no such constraint. We provide a full picture of the existence of stable
outcomes with respect to given size parameters. Additionally, when there are
only upper bounds, we fully characterize the computational complexity of the
associated existence problem. In particular, we obtain polynomial-time
algorithms for contractual individual stability and contractual Nash stability,
where the latter requires an upper bound of 2. We obtain further results for
Nash stability and contractual individual stability, when the lower bound is at
least 2.

</details>


### [4] [On the Complexity of Nucleolus Computation for Bipartite b-Matching Games](https://arxiv.org/abs/2105.07161)
*Jochen Koenemann,Justin Toth,Felix Zhou*

Main category: cs.GT

TL;DR: 本文研究了二分图上b-匹配博弈中核仁计算的复杂性，证明了即使在最大度为7的二分图上计算简单b-匹配博弈的核仁也是NP难的，但在b值有界为2的特殊情况下给出了一些正面结果。


<details>
  <summary>Details</summary>
Motivation: 研究b-匹配博弈中核仁计算的复杂性，特别是在二分图上的计算难度，为理解这类博弈问题的计算边界提供理论支撑。

Method: 通过理论证明和算法设计，一方面证明了计算核仁的NP难度，另一方面针对b值有界的情况设计了高效算法。

Result: 证明了在最大度为7的二分图上计算简单b-匹配博弈的核仁是NP难的；在b=2的特殊情况下，设计了当常数个顶点满足b(v)=2时的有效算法，以及计算非简单b-匹配核仁的有效算法。

Conclusion: b-匹配博弈的核仁计算在一般情况下是困难的，但在特定约束条件下可以找到高效的计算方法。

Abstract: We explore the complexity of nucleolus computation in b-matching games on
bipartite graphs. We show that computing the nucleolus of a simple b-matching
game is NP-hard even on bipartite graphs of maximum degree 7. We complement
this with partial positive results in the special case where b values are
bounded by 2. In particular, we describe an efficient algorithm when a constant
number of vertices satisfy b(v) = 2 as well as an efficient algorithm for
computing the non-simple b-matching nucleolus when b = 2.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [5] [Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval](https://arxiv.org/abs/2510.12014)
*Eric He,Akash Gupta,Adian Liusie,Vatsal Raina,Piotr Molenda,Shirom Chabra,Vyas Raina*

Main category: cs.IR

TL;DR: 提出了一种将大型视觉语言模型的偏好排序蒸馏到基于嵌入的系统中的框架，在保持推理效率的同时提升个性化文本-图像检索能力


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入的方法(如CLIP)主要针对字面文本-图像对训练，难以处理产品推荐中的抽象或人物驱动属性；而视觉语言模型虽然能灵活对齐文本和图像，但受限于上下文窗口无法直接处理大规模检索

Method: 通过将强大视觉语言模型的偏好排序知识蒸馏到基于嵌入的系统中，转移其细粒度对齐能力，同时保持嵌入方法的推理扩展性

Result: 在人物驱动的产品推荐任务上，该方法显著优于现有的基于嵌入的基线方法

Conclusion: 该方法为个性化文本-图像检索提供了一个高效的解决方案，成功结合了视觉语言模型的语义理解能力和嵌入方法的大规模检索效率

Abstract: Text--image retrieval is necessary for applications such as product
recommendation. Embedding-based approaches like CLIP enable efficient
large-scale retrieval via vector similarity search, but they are primarily
trained on literal caption-like text--image pairs and often fail to capture
abstract or persona-driven attributes common in product recommendation
applications (e.g., ``a gift for a mother who loves gardening''). In contrast,
state-of-the-art vision--language models (vLLMs) can align text with images in
a flexible manner, but their limited context window prevents them from directly
handling retrieval over large catalogs. We propose a framework that distills
the preference rankings of a powerful vLLM into an embedding-based system,
transferring its nuanced alignment abilities while maintaining the
inference-time scalability of an embedding-based approach. Experiments on
persona-driven product recommendation tasks demonstrate that our method
significantly outperforms existing embedding-based baselines, providing an
efficient solution for personalized text--image retrieval.

</details>


### [6] [MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation](https://arxiv.org/abs/2510.12054)
*Wenjin Xie,Tao Jia*

Main category: cs.IR

TL;DR: 提出了MIARec模型，通过基于引力的方法测量学者间的相互学术影响力，并将其融入图表示学习的消息传播过程中，以解决现有图推荐方法忽略学术网络中不对称影响力的问题。


<details>
  <summary>Details</summary>
Motivation: 随着科学文献的快速扩张，学者对精确高质量的论文推荐需求增加。现有图推荐方法在学习图表示时往往忽略了学术网络中普遍存在的不对称学术影响力。

Method: 使用基于引力的方法测量学者间的相互学术影响力，并将该影响力融入图表示学习的消息传播特征聚合过程。采用多通道聚合方法捕获不同单关系子网络的个体嵌入及其相互依赖嵌入。

Result: 在真实数据集上的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型。

Conclusion: MIARec模型在科学论文推荐任务中表现出有效性，能够更全面地理解异构学术网络。

Abstract: With the rapid expansion of scientific literature, scholars increasingly
demand precise and high-quality paper recommendations. Among various
recommendation methodologies, graph-based approaches have garnered attention by
effectively exploiting the structural characteristics inherent in scholarly
networks. However, these methods often overlook the asymmetric academic
influence that is prevalent in scholarly networks when learning graph
representations. To address this limitation, this study proposes the
Mutual-Influence-Aware Recommendation (MIARec) model, which employs a
gravity-based approach to measure the mutual academic influence between
scholars and incorporates this influence into the feature aggregation process
during message propagation in graph representation learning. Additionally, the
model utilizes a multi-channel aggregation method to capture both individual
embeddings of distinct single relational sub-networks and their interdependent
embeddings, thereby enabling a more comprehensive understanding of the
heterogeneous scholarly network. Extensive experiments conducted on real-world
datasets demonstrate that the MIARec model outperforms baseline models across
three primary evaluation metrics, indicating its effectiveness in scientific
paper recommendation tasks.

</details>


### [7] [Reinforced Preference Optimization for Recommendation](https://arxiv.org/abs/2510.12211)
*Junfei Tan,Yuxin Chen,An Zhang,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Xiang Wang*

Main category: cs.IR

TL;DR: 提出ReRe框架，通过强化学习优化基于大语言模型的推荐系统，解决负样本建模不足和隐式奖励依赖问题，在三个真实数据集上表现优于传统和LLM推荐器


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐系统存在两个核心限制：缺乏高质量负样本建模和依赖隐式奖励。强化学习与可验证奖励(RLVR)提供了解决方案，但应用于生成式推荐仍面临挑战

Method: 提出ReRe框架，包含约束束搜索提高采样效率和多样化硬负样本，同时用辅助排序奖励增强基于规则的准确性奖励以提供更细粒度的监督

Result: 在三个真实数据集上的广泛实验表明，ReRe在排序性能上持续优于传统和基于LLM的推荐器。分析显示ReRe不仅提升了基础和SFT初始化模型的性能，还在不同骨干网络家族和规模上具有鲁棒泛化能力

Conclusion: 除了实证收益外，系统研究了RLVR在推荐系统中的设计空间，包括生成、采样策略、奖励建模和优化算法，为未来研究提供了见解

Abstract: Recent breakthroughs in large language models (LLMs) have fundamentally
shifted recommender systems from discriminative to generative paradigms, where
user behavior modeling is achieved by generating target items conditioned on
historical interactions. Yet current generative recommenders still suffer from
two core limitations: the lack of high-quality negative modeling and the
reliance on implicit rewards. Reinforcement learning with verifiable rewards
(RLVR) offers a natural solution by enabling on-policy sampling of harder
negatives and grounding optimization in explicit reward signals. However,
applying RLVR to generative recommenders remains non-trivial. Its unique
generation space often leads to invalid or repetitive items that undermine
sampling efficiency, and ranking supervision is sparse since most items receive
identical zero rewards. To address these challenges, we propose Reinforced
Preference Optimization for Recommendation (ReRe), a reinforcement-based
paradigm tailored to LLM-based recommenders, an important direction in
generative recommendation. ReRe incorporates constrained beam search to improve
sampling efficiency and diversify hard negatives, while augmenting rule-based
accuracy rewards with auxiliary ranking rewards for finer-grained supervision.
Extensive experiments on three real-world datasets demonstrate that ReRe
consistently outperforms both traditional and LLM-based recommenders in ranking
performance. Further analysis shows that ReRe not only enhances performance
across both base and SFT-initialized models but also generalizes robustly
across different backbone families and scales. Beyond empirical gains, we
systematically investigate the design space of RLVR in recommendation across
generation, sampling strategy, reward modeling, and optimization algorithm,
offering insights for future research.

</details>


### [8] [An Empirical Study for Representations of Videos in Video Question Answering via MLLMs](https://arxiv.org/abs/2510.12299)
*Zhi Li,Yanan Wang,Hao Niu,Julio Vizcarra,Masato Taya*

Main category: cs.IR

TL;DR: 该论文系统评估了多模态大语言模型在视频问答任务中不同视频表示方法的效果和效率权衡，发现视觉帧显著提升准确率但计算成本高，而字幕提供轻量有效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频问答方面取得显著进展，但尚不清楚哪种视频表示方法最有效，以及不同模态如何在任务准确率和计算效率之间取得平衡。

Method: 在VideoMME和LongVideoBench两个基准上，系统评估了单模态输入（仅问题、字幕、视觉帧、音频信号）以及多模态组合的表现。

Result: 视觉帧大幅提升准确率但带来巨大的GPU内存和推理延迟成本，字幕则为长视频提供了轻量且有效的替代方案。

Conclusion: 研究揭示了效果与效率之间的明确权衡，为设计资源感知的MLLM视频问答系统提供了实用见解。

Abstract: Multimodal large language models have recently achieved remarkable progress
in video question answering (VideoQA) by jointly processing visual, textual,
and audio information. However, it remains unclear which video representations
are most effective for MLLMs, and how different modalities balance task
accuracy against computational efficiency. In this work, we present a
comprehensive empirical study of video representation methods for VideoQA with
MLLMs. We systematically evaluate single modality inputs question only,
subtitles, visual frames, and audio signals as well as multimodal combinations,
on two widely used benchmarks: VideoMME and LongVideoBench. Our results show
that visual frames substantially enhance accuracy but impose heavy costs in GPU
memory and inference latency, while subtitles provide a lightweight yet
effective alternative, particularly for long videos. These findings highlight
clear trade-offs between effectiveness and efficiency and provide practical
insights for designing resource-aware MLLM-based VideoQA systems.

</details>


### [9] [Causal Inspired Multi Modal Recommendation](https://arxiv.org/abs/2510.12325)
*Jie Yang,Chenyang Gu,Zixuan Liu*

Main category: cs.IR

TL;DR: 提出因果启发的多模态推荐框架，通过双通道跨模态扩散模块识别模态混杂因子，使用后门调整和前门调整来消除模态混杂和交互偏差，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统忽视了两个关键偏差：模态混杂（潜在因素同时驱动多个模态并影响用户偏好）和交互偏差（真实偏好与曝光效应和偶然点击的噪声混杂），导致虚假的特征-偏好关联。

Method: 引入双通道跨模态扩散模块识别隐藏模态混杂因子；使用后门调整结合分层匹配和向量量化码本来阻断混杂路径；应用前门调整结合因果拓扑重建构建去混杂的因果子图。

Result: 在三个真实电子商务数据集上的广泛实验表明，该方法显著优于最先进的基线方法，同时保持强可解释性。

Conclusion: 提出的因果启发的多模态推荐框架有效解决了模态混杂和交互偏差问题，在提升推荐性能的同时保持了良好的可解释性。

Abstract: Multimodal recommender systems enhance personalized recommendations in
e-commerce and online advertising by integrating visual, textual, and user-item
interaction data. However, existing methods often overlook two critical biases:
(i) modal confounding, where latent factors (e.g., brand style or product
category) simultaneously drive multiple modalities and influence user
preference, leading to spurious feature-preference associations; (ii)
interaction bias, where genuine user preferences are mixed with noise from
exposure effects and accidental clicks. To address these challenges, we propose
a Causal-inspired multimodal Recommendation framework. Specifically, we
introduce a dual-channel cross-modal diffusion module to identify hidden modal
confounders, utilize back-door adjustment with hierarchical matching and
vector-quantized codebooks to block confounding paths, and apply front-door
adjustment combined with causal topology reconstruction to build a deconfounded
causal subgraph. Extensive experiments on three real-world e-commerce datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines while maintaining strong interpretability.

</details>


### [10] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 研究发现ColBERT中使用的单层线性投影在多向量密集检索中存在局限性，通过替换为更复杂的FFN网络结构（如深层非线性FFN、GLU块和跳跃连接）可以显著提升检索性能，最佳变体在多个基准测试中平均性能提升超过2个NDCG@10点。


<details>
  <summary>Details</summary>
Motivation: 探索MaxSim算子对多向量模型训练梯度流的影响，揭示单层线性投影在ColBERT等模型中的固有局限性，并研究更先进的投影结构能否改善这些限制。

Method: 设计并系统评估替代投影块，包括深层非线性FFN块、GLU块和跳跃连接等结构，通过消融研究分析各参数对性能的影响。

Result: 许多投影变体优于原始线性投影，最佳变体在跨领域检索基准上平均性能提升超过2 NDCG@10点，特别是放大中间投影和残差连接对性能提升至关重要。

Conclusion: 替换ColBERT模型的线性层是一个稳健的即插即用升级方案，效果在不同随机种子下保持一致，证实了更复杂投影结构的有效性。

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [11] [A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning](https://arxiv.org/abs/2510.12369)
*Yang Xiang,Li Fan,Chenke Yin,Chengtao Ji*

Main category: cs.IR

TL;DR: 提出了一种分层量化框架，通过自加权机制实现任务自适应的多尺度聚合，在保持编码器冻结的同时通过轻量级门控过程调节信息流。


<details>
  <summary>Details</summary>
Motivation: 现有图标记化方法在线性化、连续化和量化方面存在局限性，特别是基于量化的标记器以固定或任务无关的方式组织层次信息，无法动态重新加权不同层级的贡献。

Method: 分层量化框架，引入自加权机制进行任务自适应多尺度聚合，保持编码器冻结，通过轻量级门控过程调节信息流。

Result: 在节点分类和链接预测基准数据集上的实验表明，在可比较的计算预算下，相比强基线方法取得了持续改进。

Conclusion: 该方法实现了参数高效的适应，能够动态调整不同层级的信息贡献，无需重新训练编码器。

Abstract: Recent progress in language and vision foundation models demonstrates the
importance of discrete token interfaces that transform complex inputs into
compact sequences for large-scale modeling. Extending this paradigm to graphs
requires a tokenization scheme that handles non-Euclidean structures and
multi-scale dependencies efficiently. Existing approaches to graph
tokenization, linearized, continuous, and quantized, remain limited in
adaptability and efficiency. In particular, most current quantization-based
tokenizers organize hierarchical information in fixed or task-agnostic ways,
which may either over-represent or under-utilize structural cues, and lack the
ability to dynamically reweight contributions from different levels without
retraining the encoder. This work presents a hierarchical quantization
framework that introduces a self-weighted mechanism for task-adaptive
aggregation across multiple scales. The proposed method maintains a frozen
encoder while modulating information flow through a lightweight gating process,
enabling parameter-efficient adaptation to diverse downstream tasks.
Experiments on benchmark datasets for node classification and link prediction
demonstrate consistent improvements over strong baselines under comparable
computational budgets.

</details>


### [12] [Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance](https://arxiv.org/abs/2510.12461)
*Andrei Chernov,Haroon Wahab,Oleg Novitskij*

Main category: cs.IR

TL;DR: 提出TextGCN和TextGCN-MLP两种方法，通过图卷积层直接处理LLM生成的物品标题嵌入，在推荐系统中实现先进的零样本和领域内性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注微调LLM生成推荐或将LLM嵌入集成到下游模型中，本文探索后一方向，旨在结合语言语义和图消息传递提升推荐性能。

Method: TextGCN使用参数无关的图卷积层直接处理LLM生成的物品标题嵌入；TextGCN-MLP在此基础上添加可训练的多层感知器，使用对比损失进行训练。

Result: TextGCN在零样本推荐中达到最先进性能，显著优于先前方法；TextGCN-MLP在领域内基准测试中达到最优性能，但零样本性能低于TextGCN。

Conclusion: TextGCN架构通过结合语言语义和图消息传递实现了优异的零样本性能，而TextGCN-MLP在领域内性能更优，体现了领域专业化与零样本泛化之间的权衡。

Abstract: In recent years, various approaches have been proposed to leverage large
language models (LLMs) for incorporating textual information about items into
recommender systems. Existing methods primarily focus on either fine-tuning
LLMs to generate recommendations or integrating LLM-based embeddings into
downstream models. In this work, we follow the latter direction and propose
\textbf{TextGCN}, which applies parameter-free graph convolution layers
directly over LLM-based item-title embeddings, instead of learning ID-based
embeddings as in traditional methods. By combining language semantics with
graph message passing, this architecture achieves state-of-the-art zero-shot
performance, significantly outperforming prior approaches. Furthermore, we
introduce \textbf{TextGCN-MLP}, which extends TextGCN with a trainable
multilayer perceptron trained using a contrastive loss, achieving
state-of-the-art in-domain performance on recommendation benchmarks. However,
the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,
highlighting the trade-off between in-domain specialization and zero-shot
generalization. We release our code on github at
\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.

</details>


### [13] [SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch](https://arxiv.org/abs/2510.12604)
*Qihang Zhao,Zhongbo Sun,Xiaoyang Zheng,Xian Guo,Siyuan Wang,Zihan Liang,Mingcan Peng,Ben Chen,Chenyi Lei*

Main category: cs.IR

TL;DR: SMILE是一种基于语义ID融合对齐的物品表示增强方法，通过RQ-OPQ编码和两步对齐机制解决冷启动物品的协同信息不足问题，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现代搜索和推荐平台中，冷启动物品的协同信息不足加剧了马太效应，影响平台多样性。现有方法未能考虑协同与内容之间的不对称性以及物品间的细粒度差异。

Method: 使用RQ-OPQ编码量化物品内容和协同信息，进行两步对齐：RQ编码传递物品间共享的协同信号，OPQ编码学习物品的差异化信息。

Result: 大规模工业数据集离线实验显示SMILE的优越性，在线A/B测试证实显著改进：物品点击率+1.66%，买家数+1.57%，订单量+2.17%。

Conclusion: SMILE通过语义ID融合对齐有效解决了冷启动物品的协同信息不足问题，提升了推荐系统的性能和平台多样性。

Abstract: With the rise of modern search and recommendation platforms, insufficient
collaborative information of cold-start items exacerbates the Matthew effect of
existing platform items, challenging platform diversity and becoming a
longstanding issue. Existing methods align items' side content with
collaborative information to transfer collaborative signals from
high-popularity items to cold-start items. However, these methods fail to
account for the asymmetry between collaboration and content, nor the
fine-grained differences among items. To address these issues, we propose
SMILE, an item representation enhancement approach based on fused alignment of
semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and
collaborative information, followed by a two-step alignment: RQ encoding
transfers shared collaborative signals across items, while OPQ encoding learns
differentiated information of items. Comprehensive offline experiments on
large-scale industrial datasets demonstrate superiority of SMILE, and rigorous
online A/B tests confirm statistically significant improvements: item CTR
+1.66%, buyers +1.57%, and order volume +2.17%.

</details>


### [14] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: 本文系统研究了参数化检索增强生成(PRAG)机制，发现参数化文档仅捕获文档的部分语义信息，单独使用性能不如文本级交互。但当参数化表示与文本文档结合使用时，模型能更有效地利用相关信息，对噪声输入更鲁棒，获得比单独使用任一来源更好的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管参数化检索增强生成(PRAG)作为RAG的新兴形式受到关注，但其参数注入机制仍未被充分理解。本文旨在阐明参数注入在PRAG中的作用机制。

Method: 通过系统研究PRAG，分析参数化文档与文本文档的交互机制，比较单独使用参数化文档、文本文档以及两者结合时的性能差异。

Result: 参数化文档仅捕获文档的部分语义信息，单独使用性能较差；但参数化表示编码了高层次文档信息，当与输入上下文中的文本文档结合时，能增强模型对文档的理解，使模型更有效地利用相关信息，对噪声输入更鲁棒。

Conclusion: 建议联合使用参数化和文本文档，并提倡增加参数化表示的信息含量以推进PRAG的发展。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


### [15] [SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model](https://arxiv.org/abs/2510.12709)
*Lin Lin,Jiefeng Long,Zhihe Wan,Yuchi Wang,Dingkang Yang,Shuang Yang,Yueyang Yao,Xu Chen,Zirui Guo,Shengqiang Li,Weiran Li,Hanyu Li,Yaling Mou,Yan Qiu,Haiyang Yu,Xiao Liang,Hongsheng Li,Chao Feng*

Main category: cs.IR

TL;DR: SAIL-Embedding是一个全模态嵌入基础模型，通过多阶段训练策略和架构设计解决多模态嵌入模型在现实应用中的挑战，在检索任务中达到SOTA性能，并在推荐场景中显著提升用户生命周期指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态嵌入模型在现实应用中的局限性，包括有限模态支持、训练机制不稳定和工业领域差距等问题。

Method: 采用多阶段训练方案：内容感知渐进训练增强模型对下游任务的适应性；协作感知推荐增强训练通过知识蒸馏和用户历史兴趣挖掘来适应推荐场景；同时开发随机专业化和数据集驱动模式匹配来增强训练灵活性和泛化性。

Result: 在不同检索任务中达到SOTA性能；在抖音精选场景中实现7天LT增益+0.158%和14天LT增益+0.144%；在抖音信息流排序模型中，匹配特征带来+0.08% AUC增益。

Conclusion: SAIL-Embedding通过精心设计的训练策略有效解决了多模态嵌入模型的现实应用挑战，在多个真实场景中显著提升了推荐系统的性能指标。

Abstract: Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis](https://arxiv.org/abs/2510.12642)
*Meihui Zhang,Liming Wang,Chi Zhang,Zhaojing Luo*

Main category: cs.DB

TL;DR: Aixel是一个统一、自适应、可扩展的AI驱动数据分析系统，通过四层架构解决现有系统在数据集成、学习和管理方面的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据分析需要整合不同格式和来源的数据，同时应对不断变化的目标和预算。现有系统在数据库、分析库和调优服务之间的碎片化导致用户交互复杂、适应性有限、性能不佳和组件扩展性差。

Method: 设计四层架构：应用层、任务层、模型层和数据层。任务层提供声明式接口捕获用户意图，解析为可执行操作计划；优化器编译调度计划以满足精度、延迟和成本目标；模型层支持版本化存储和自适应构建；数据层提供统一数据管理能力。

Result: Aixel实现了用户友好、自适应、高效和可扩展的系统，通过内置的重用和缓存机制提高效率，支持任务对齐的漂移检测和安全更新。

Conclusion: Aixel通过统一的多层架构有效解决了数据分析系统中存在的碎片化问题，为AI驱动的数据分析提供了一个综合解决方案。

Abstract: A growing trend in modern data analysis is the integration of data management
with learning, guided by accuracy, latency, and cost requirements. In practice,
applications draw data of different formats from many sources. In the
meanwhile, the objectives and budgets change over time. Existing systems handle
these applications across databases, analysis libraries, and tuning services.
Such fragmentation leads to complex user interaction, limited adaptability,
suboptimal performance, and poor extensibility across components. To address
these challenges, we present Aixel, a unified, adaptive, and extensible system
for AI-powered data analysis. The system organizes work across four layers:
application, task, model, and data. The task layer provides a declarative
interface to capture user intent, which is parsed into an executable operator
plan. An optimizer compiles and schedules this plan to meet specified goals in
accuracy, latency, and cost. The task layer coordinates the execution of data
and model operators, with built-in support for reuse and caching to improve
efficiency. The model layer offers versioned storage for index, metadata,
tensors, and model artifacts. It supports adaptive construction, task-aligned
drift detection, and safe updates that reuse shared components. The data layer
provides unified data management capabilities, including indexing,
constraint-aware discovery, task-aligned selection, and comprehensive feature
management. With the above designed layers, Aixel delivers a user friendly,
adaptive, efficient, and extensible system.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [17] [Approximate Proximal Operators for Analog Compressed Sensing Using PN-junction Diode](https://arxiv.org/abs/2510.12065)
*Soma Furusawa,Taisei Kato,Ryo Hayakawa,Kazunori Hayashi*

Main category: cs.IT

TL;DR: 该论文提出使用PN结二极管的正向电压-电流特性实现ℓ1和MCP正则化函数的近似邻近算子，用于模拟压缩感知。


<details>
  <summary>Details</summary>
Motivation: 为了实现模拟压缩感知，需要找到ℓ1和MCP正则化函数的近似邻近算子的实际实现方法。

Method: 利用PN结二极管的电压-电流特性构建近似邻近算子，并在近端梯度法中应用这些算子进行压缩感知。

Result: 通过计算机仿真验证了所提方法的稀疏重构性能，并考虑了模拟器件引入的加性噪声影响。

Conclusion: 提出的基于二极管特性的近似邻近算子方法在压缩感知中具有可行性，能够实现有效的稀疏重构。

Abstract: In order to realize analog compressed sensing, the paper considers
approximate proximal operators of the $\ell_1$ and minimax concave penalty
(MCP) regularization functions. Specifically, we propose to realize the
approximate functions by an electric analog circuit using forward
voltage-current (V-I) characteristics of the PN-junction diodes. To confirm the
validity of the proposed approach, we employ the proposed approximate proximal
operators for the $\ell_1$ and MCP regularization functions in compressed
sensing with the proximal gradient method. The sparse reconstruction
performance of the algorithms using the proposed approximate proximal operators
is demonstrated via computer simulations taking into account the impact of
additive noise introduced by analog devices.

</details>


### [18] [FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning](https://arxiv.org/abs/2510.12078)
*Sijing Xie,Dingzhu Wen,Changsheng You,Qimei Chen,Mehdi Bennis,Kaibin Huang*

Main category: cs.IT

TL;DR: 提出了FedLoDrop框架，在联邦LoRA中对可训练矩阵的行列应用dropout，通过优化dropout率和资源分配来最小化泛化误差上界，解决边缘网络资源受限下的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 为了在降低训练成本的同时增强大语言模型的泛化能力，需要解决联邦学习在边缘部署时面临的网络资源限制和过拟合问题。

Method: 提出FedLoDrop框架，对联邦LoRA的可训练矩阵行列应用dropout；建立泛化误差界和收敛性分析；通过分支定界法和惩罚连续凸近似算法优化dropout率和资源分配。

Result: 数值结果表明该方法能有效缓解过拟合并提高泛化能力，同时降低通信成本。

Conclusion: FedLoDrop框架通过dropout机制和资源优化，在边缘计算环境中实现了更好的泛化性能和资源效率平衡。

Abstract: Fine-tuning (FT) large language models (LLMs) is crucial for adapting
general-purpose models to specific tasks, enhancing accuracy and relevance with
minimal resources. To further enhance generalization ability while reducing
training costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a
new framework that applies dropout to the rows and columns of the trainable
matrix in Federated LoRA. A generalization error bound and convergence analysis
under sparsity regularization are obtained, which elucidate the fundamental
trade-off between underfitting and overfitting. The error bound reveals that a
higher dropout rate increases model sparsity, thereby lowering the upper bound
of pointwise hypothesis stability (PHS). While this reduces the gap between
empirical and generalization errors, it also incurs a higher empirical error,
which, together with the gap, determines the overall generalization error. On
the other hand, though dropout reduces communication costs, deploying FedLoDrop
at the network edge still faces challenges due to limited network resources. To
address this issue, an optimization problem is formulated to minimize the upper
bound of the generalization error, by jointly optimizing the dropout rate and
resource allocation subject to the latency and per-device energy consumption
constraints. To solve this problem, a branch-and-bound (B\&B)-based method is
proposed to obtain its globally optimal solution. Moreover, to reduce the high
computational complexity of the B\&B-based method, a penalized successive
convex approximation (P-SCA)-based algorithm is proposed to efficiently obtain
its high-quality suboptimal solution. Finally, numerical results demonstrate
the effectiveness of the proposed approach in mitigating overfitting and
improving the generalization capability.

</details>


### [19] [Hybrid centralized-distributed precoding in fronthaul-constrained CF-mMIMO systems](https://arxiv.org/abs/2510.12406)
*Zahra Mobini,Hien Quoc Ngo,Ardavan Rahimian,Anvar Tukmanov,David Townend,Michail Matthaiou,Simon L. Cotton*

Main category: cs.IT

TL;DR: 提出了一种混合集中-分布式预编码策略，用于前传受限的无蜂窝大规模MIMO系统，通过动态用户分组和功率控制来优化频谱效率。


<details>
  <summary>Details</summary>
Motivation: 解决前传容量受限的无蜂窝大规模MIMO系统中频谱效率与前传需求之间的权衡问题，传统完全集中或分布式方法无法灵活适应不同系统配置。

Method: 将用户分为两组：一组采用集中式预编码，另一组采用分布式预编码。通过优化用户分组和功率控制来最大化总频谱效率，同时满足前传和接入点功率约束。

Result: 数值结果表明混合方案具有优越性能，在各种系统配置下始终优于完全集中和分布式方法，展现了良好的适应性。

Conclusion: 所提出的混合预编码策略在前传受限的无蜂窝大规模MIMO系统中实现了频谱效率与前传需求的有效平衡，提供了灵活且高性能的解决方案。

Abstract: We investigate a fronthaul-limited cell-free massive multiple-input
multiple-output (CF-mMIMO) system and propose a hybrid centralized-distributed
precoding strategy that dynamically adapts to varying fronthaul and spectral
efficiency (SE) requirements. The proposed approach divides users into two
groups: one served by centralized precoding and the other by distributed
precoding. We formulate a novel optimization problem for user grouping and
power control aimed at maximizing the sum SE, subject to fronthaul and
per-access point (AP) power constraints. To tackle the problem, we transform it
into a tractable form and propose efficient solution algorithms. Numerical
results confirm the hybrid scheme's versatility and superior performance,
consistently outperforming fully centralized and distributed approaches across
diverse system configurations.

</details>


### [20] [Phase Transitions of the Additive Uniform Noise Channel with Peak Amplitude and Cost Constraint](https://arxiv.org/abs/2510.12427)
*Jonas Stapmanns,Catarina Dias,Luke Eilers,Tobias Kühn,Jean-Pascal Pfister*

Main category: cs.IT

TL;DR: 该论文研究了在加性均匀噪声信道下，量化在什么条件下是最优的。通过分析容量实现输入分布与噪声水平、平均成本约束和成本函数曲率的关系，发现当成本函数为凹函数时，最优输入分布是离散的；当成本函数为凸函数且成本约束有效时，最优输入分布支撑整个区间。


<details>
  <summary>Details</summary>
Motivation: 探究在加性均匀噪声信道中，量化在什么条件下能够达到信道容量最优，这对于理解信道编码和量化策略的优化具有重要意义。

Method: 在峰值幅度和成本约束下，分析加性均匀噪声信道的容量实现输入分布，考虑噪声水平、平均成本约束和成本函数曲率的影响。

Result: 当成本函数为凹函数时，容量实现输入分布是离散的；当成本函数为凸函数且成本约束有效时，容量实现输入分布的支撑覆盖整个区间。对于离散最优输入分布的情况，推导出了信道容量的解析表达式。

Conclusion: 量化在成本函数为凹函数时是最优的，而在成本函数为凸函数且成本约束有效时，连续输入分布是最优的。这为信道编码设计提供了理论指导。

Abstract: Under which condition is quantization optimal? We address this question in
the context of the additive uniform noise channel under peak amplitude and cost
constraints. We compute analytically the capacity-achieving input distribution
as a function of the noise level, the average cost constraint, and the
curvature of the cost function. We find that when the cost function is concave,
the capacity-achieving input distribution is discrete, whereas when the cost
function is convex and the cost constraint is active, the support of the
capacity-achieving input distribution spans the entire interval. For the cases
of a discrete capacity-achieving input distribution, we derive the analytical
expressions for the capacity of the channel.

</details>


### [21] [CoNet-Rx: Collaborative Neural Networks for OFDM Receivers](https://arxiv.org/abs/2510.12739)
*Mohanad Obeed,Ming Jian*

Main category: cs.IT

TL;DR: 提出了一种名为CoNet的新型神经网络架构，用于OFDM接收器，通过多个小型子网络从不同角度处理信号特征，并通过交互操作融合输出，显著提升检测性能并减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的OFDM接收器方法通常从计算机视觉领域改编而来，不适合无线通信场景，存在计算资源需求高、内存占用大、推理延迟显著等问题，限制了在资源受限环境中的应用。

Method: CoNet使用多个小型ResNet或CNN子网络同时处理信号特征，从不同角度捕获信道相关性和干扰模式，然后通过元素级乘法等交互操作融合子网络的输出。

Result: 仿真结果表明，在相同网络大小和计算复杂度下，CoNet在误码率方面显著优于传统残差网络架构，并减少了推理延迟。

Conclusion: CoNet为OFDM接收器提供了一种高效的神经网络架构，能够在保持高性能的同时降低资源需求，适合资源受限的无线通信应用。

Abstract: Deep learning (DL) based methods for orthogonal frequency division
multiplexing (OFDM) radio receivers demonstrated higher signal detection
performance compared to the traditional receivers. However, the existing
DL-based models, usually adapted from computer vision, aren't well suited for
wireless communications. These models require high computational resources and
memory, and have significant inference delays, limiting their use in
resource-constrained settings. Additionally, reducing network size to ease
resource demands often leads to notable performance degradation. This paper
introduces collaborative networks (CoNet), a novel neural network (NN)
architecture designed for OFDM receivers. CoNet uses multiple small ResNet or
CNN subnetworks to simultaneously process signal features from different
perspectives like capturing channel correlations and interference patterns.
These subnetworks fuse their outputs through interaction operations (e.g.,
element-wise multiplication), significantly enhancing detection performance.
Simulation results show CoNet significantly outperforms traditional
architectures like residual networks (ResNets) in bit error rate (BER) and
reduces inference delay when both nets have the same size and the same
computational complexity.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication](https://arxiv.org/abs/2510.12265)
*Sami Khairy,Gabriel Mittag,Vishak Gopal,Ross Cutler*

Main category: cs.MM

TL;DR: 提出了一种基于人类参与的数据驱动带宽估计框架，通过离线强化学习训练神经网络带宽估计器，在真实视频会议系统中显著提升用户体验质量。


<details>
  <summary>Details</summary>
Motivation: 实时通信中的带宽估计面临网络架构快速演进、协议栈复杂化以及QoE指标定义困难等挑战，需要更准确的方法来提升视频会议系统的用户体验。

Method: 首先训练基于主观用户评估的客观QoE奖励模型，收集100万条真实网络轨迹数据，然后提出新颖的分布式离线强化学习算法来训练神经网络带宽估计器。

Result: 真实A/B测试显示，相比基线带宽估计器，该方法将主观差评通话比例降低了11.41%，离线RL算法在D4RL基准测试中也表现出良好泛化能力。

Conclusion: 该数据驱动的人类参与框架有效解决了实时通信中的带宽估计挑战，显著提升了视频会议系统的用户体验质量。

Abstract: The quality of experience (QoE) delivered by video conferencing systems is
significantly influenced by accurately estimating the time-varying available
bandwidth between the sender and receiver. Bandwidth estimation for real-time
communications remains an open challenge due to rapidly evolving network
architectures, increasingly complex protocol stacks, and the difficulty of
defining QoE metrics that reliably improve user experience. In this work, we
propose a deployed, human-in-the-loop, data-driven framework for bandwidth
estimation to address these challenges. Our approach begins with training
objective QoE reward models derived from subjective user evaluations to measure
audio and video quality in real-time video conferencing systems. Subsequently,
we collect roughly $1$M network traces with objective QoE rewards from
real-world Microsoft Teams calls to curate a bandwidth estimation training
dataset. We then introduce a novel distributional offline reinforcement
learning (RL) algorithm to train a neural-network-based bandwidth estimator
aimed at improving QoE for users. Our real-world A/B test demonstrates that the
proposed approach reduces the subjective poor call ratio by $11.41\%$ compared
to the baseline bandwidth estimator. Furthermore, the proposed offline RL
algorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond
bandwidth estimation.

</details>


### [23] [M3ST-DTI: A multi-task learning model for drug-target interactions based on multi-modal features and multi-stage alignment](https://arxiv.org/abs/2510.12445)
*Xiangyu Li,Ran Su,Liangliang Liu*

Main category: cs.MM

TL;DR: M3ST-DTI是一个用于药物-靶点相互作用预测的多任务学习模型，通过多阶段整合和对齐多模态特征来提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉深层模态内特征交互和实现有效跨模态对齐方面存在不足，限制了预测性能和泛化能力。

Method: 整合文本、结构和功能三种特征，使用自注意力机制和混合池化图注意力模块增强模态内表示，通过MCA与Gram损失进行早期特征对齐，BCA模块捕获细粒度交互，深度正交融合模块减少特征冗余。

Result: 在基准数据集上的广泛评估表明，M3ST-DTI在不同指标上始终优于最先进的方法。

Conclusion: M3ST-DTI通过多阶段多模态特征整合和对齐，显著提升了药物-靶点相互作用预测的性能。

Abstract: Accurate prediction of drug-target interactions (DTI) is pivotal in drug
discovery. However, existing approaches often fail to capture deep intra-modal
feature interactions or achieve effective cross-modal alignment, limiting
predictive performance and generalization. To address these challenges, we
propose M3ST-DTI, a multi-task learning model that enables multi-stage
integration and alignment of multi modal features for DTI prediction. M3ST-DTI
incorporates three types of features-textual, structural, and functional and
enhances intra-modal representations using self-attention mechanisms and a
hybrid pooling graph attention module. For early-stage feature alignment and
fusion, the model in tegrates MCA with Gram loss as a structural constraint. In
the later stage, a BCA module captures fine-grained interactions between drugs
and targets within each modality, while a deep orthogonal fusion module
mitigates feature redundancy.Extensive evaluations on benchmark datasets
demonstrate that M3ST-DTI consistently outperforms state-of-the art methods
across diverse metrics

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [24] [Thin Trees via $k$-Respecting Cut Identities](https://arxiv.org/abs/2510.12050)
*Mohit Daga*

Main category: cs.DS

TL;DR: 该论文提出了一种新的k-尊重切割身份机制，用于高效验证生成树的薄度，解决了长期存在的薄树验证问题。


<details>
  <summary>Details</summary>
Motivation: 薄生成树在图论、近似算法和组合优化中具有重要意义，但即使验证给定树是否为薄树的基本算法任务也一直难以解决，因为检查薄度需要考虑指数级数量的切割。

Method: 引入了k-尊重切割身份的新机制，通过预处理后构建树局部预言机，能够在固定k的情况下高效计算生成树的精确k-薄度证书。

Result: 提出了首个在时间O(n^2+n^k)内计算任意生成树精确k-薄度证书的程序，在平面图和曲面嵌入图中获得了更尖锐的保证。

Conclusion: 该框架为薄树验证提供了高效解决方案，并在结构化设置中获得了改进的保证，对薄树猜想和相关算法有重要意义。

Abstract: Thin spanning trees lie at the intersection of graph theory, approximation
algorithms, and combinatorial optimization. They are central to the
long-standing \emph{thin tree conjecture}, which asks whether every
$k$-edge-connected graph contains an $O(1/k)$-thin tree, and they underpin
algorithmic breakthroughs such as the $O(\log n/\log\log n)$-approximation for
ATSP. Yet even the basic algorithmic task of \emph{verifying} that a given tree
is thin has remained elusive: checking thinness requires reasoning about
exponentially many cuts, and no efficient certificates have been known.
  We introduce a new machinery of \emph{$k$-respecting cut identities}, which
express the weight of every cut that crosses a spanning tree in at most $k$
edges as a simple function of pairwise ($2$-respecting) cuts. This yields a
tree-local oracle that, after $O(n^2)$ preprocessing, evaluates such cuts in
$O_k(1)$ time. Building on this oracle, we give the first procedure to compute
the exact $k$-thinness certificate $\Theta_k(T)$ of any spanning tree for fixed
$k$ in time $\tilde O(n^2+n^k)$, outputting both the certificate value and a
witnessing cut.
  Beyond general graphs, our framework yields sharper guarantees in structured
settings. In planar graphs, duality with cycles and dual girth imply that every
spanning tree admits a verifiable certificate $\Theta_k(T)\le k/\lambda$ (hence
$O(1/\lambda)$ for constant $k$). In graphs embedded on a surface of genus
$\gamma$, refined counting gives certified (per-cut) bounds $O((\log
n+\gamma)/\lambda)$ via the same ensemble coverage.

</details>


### [25] [Engineering Dominating Patterns: A Fine-grained Case Study](https://arxiv.org/abs/2510.12232)
*Jonathan Dransfeld,Marvin Künnemann,Mirza Redzic,Marcus Wunderlich*

Main category: cs.DS

TL;DR: 该论文研究了支配H模式问题，在稀疏图上开发了条件最优的算法，并通过实验验证了定制化分支定界方法的有效性，相比现有求解器性能提升可达两个数量级。


<details>
  <summary>Details</summary>
Motivation: 尽管支配H模式问题在最坏情况下难以显著改进，但最近研究表明在稀疏图中存在改进潜力。本文旨在探索条件最优算法在实际输入上的表现。

Method: 开发了多种方法进行实验评估，包括使用Glasgow子图求解器、SAT求解器Kissat、ILP求解器Gurobi的基线方法，以及定制化的分支定界算法配合精心设计的剪枝技术。

Result: 条件最优算法的简单实现与现有求解器性能相当，而定制的分支定界方法在测试实例上实现了高达两个数量级的性能提升。

Conclusion: 针对特定模式H的支配H模式问题，通过精心设计的定制化分支定界方法可以在实际输入上实现显著性能改进，证明了条件最优算法在实际应用中的有效性。

Abstract: The \emph{Dominating $H$-Pattern} problem generalizes the classical
$k$-Dominating Set problem: for a fixed \emph{pattern} $H$ and a given graph
$G$, the goal is to find an induced subgraph $S$ of $G$ such that (1) $S$ is
isomorphic to $H$, and (2) $S$ forms a dominating set in $G$. Fine-grained
complexity results show that on worst-case inputs, any significant improvement
over the naive brute-force algorithm is unlikely, as this would refute the
Strong Exponential Time Hypothesis. Nevertheless, a recent work by Dransfeld et
al. (ESA 2025) reveals some significant improvement potential particularly in
\emph{sparse} graphs.
  We ask: Can algorithms with conditionally almost-optimal worst-case
performance solve the Dominating $H$-Pattern, for selected patterns $H$,
efficiently on practical inputs? We develop and experimentally evaluate several
approaches on a large benchmark of diverse datasets, including baseline
approaches using the Glasgow Subgraph Solver (GSS), the SAT solver Kissat, and
the ILP solver Gurobi.
  Notably, while a straightforward implementation of the algorithms -- with
conditionally close-to-optimal worst-case guarantee -- performs comparably to
existing solvers, we propose a tailored Branch-\&-Bound approach --
supplemented with careful pruning techniques -- that achieves improvements of
up to two orders of magnitude on our test instances.

</details>


### [26] [Exact Matching and Top-k Perfect Matching Parameterized by Neighborhood Diversity or Bandwidth](https://arxiv.org/abs/2510.12552)
*Nicolas El Maalouly,Kostas Lakis*

Main category: cs.DS

TL;DR: 本文研究了精确匹配(EM)和Top-k完美匹配(TkPM)问题在blown-up图上的算法，提出了参数化算法和近似方案，并针对有界带宽的原型图开发了次指数时间递归算法。


<details>
  <summary>Details</summary>
Motivation: 精确匹配问题虽然存在随机多项式时间算法，但确定性算法仅适用于特殊情况，这使其成为测试P=RP假设的自然候选者。本文旨在研究该问题及其等价问题TkPM在blown-up图上的高效算法。

Method: 针对TkPM问题，提出了参数化算法(参数为k和邻域多样性)，并扩展为近似方案。对于有界带宽的原型图，开发了次指数时间递归算法，并将该方法应用于EM问题。

Result: 开发了FPT算法处理TkPM问题，时间复杂度的参数依赖性较软。对有界带宽的原型图实现了次指数时间算法，并将该方法成功应用于EM问题。

Conclusion: 通过利用图中存在多个不相交分隔器的特性，避免了在分割阶段包含任何"坏"顶点，这种方法类似于在有界树宽实例上使用动态规划的技术。

Abstract: The Exact Matching (EM) problem asks whether there exists a perfect matching
which uses a prescribed number of red edges in a red/blue edge-colored graph.
While there exists a randomized polynomial-time algorithm for the problem, only
some special cases admit a deterministic one so far, making it a natural
candidate for testing the P=RP hypothesis. A polynomial-time equivalent
problem, Top-k Perfect Matching (TkPM), asks for a perfect matching maximizing
the weight of the $k$ heaviest edges.
  We study the above problems, mainly the latter, in the scenario where the
input is a blown-up graph, meaning a graph which had its vertices replaced by
cliques or independent sets. We describe an FPT algorithm for TkPM
parameterized by $k$ and the neighborhood diversity of the input graph, which
is essentially the size of the graph before the blow-up; this graph is also
called the prototype. We extend this algorithm into an approximation scheme
with a much softer dependency on the aforementioned parameters, time-complexity
wise. Moreover, for prototypes with bounded bandwidth but unbounded size, we
develop a recursive algorithm that runs in subexponential time. Utilizing
another algorithm for EM on bounded neighborhood diversity graphs, we adapt
this recursive subexponential algorithm to EM.
  Our approach is similar to the use of dynamic programming on e.g. bounded
treewidth instances for various problems. The main point is that the existence
of many disjoint separators is utilized to avoid including in the separator any
of a set of ``bad'' vertices during the split phase.

</details>


### [27] [Lossless Derandomization for Undirected Single-Source Shortest Paths and Approximate Distance Oracles](https://arxiv.org/abs/2510.12598)
*Shuyi Yan*

Main category: cs.DS

TL;DR: 提出了一种确定性算法，通过自适应选择球大小，实现了最优的平均球大小Θ(n/r)，并成功应用于去随机化最短路径算法和近似距离预言机。


<details>
  <summary>Details</summary>
Motivation: 传统的随机化算法通过均匀采样中心可以达到最优球大小，但去随机化方法会引入O(log n)因子，在某些情况下代价过高。需要一种无损失的去随机化方法。

Method: 利用算法可以自适应选择球大小而非由输入固定的特性，提出简单确定性算法，在平均情况下达到最优球大小Θ(n/r)。

Result: 实现了确定性算法达到最优平均球大小，成功去随机化了[DMSY23]的最短路径算法和Thorup-Zwick近似距离预言机，没有时间/空间复杂度损失。

Conclusion: 自适应球大小选择是实现高效去随机化的关键，该技术可广泛应用于图算法中，避免了传统去随机化方法的性能损失。

Abstract: A common step in algorithms related to shortest paths in undirected graphs is
that, we select a subset of vertices as centers, then grow a ball around each
vertex until a center is reached. We want the balls to be as small as possible.
A randomized algorithm can uniformly sample $r$ centers to achieve the optimal
(expected) ball size of $\Theta(n/r)$. A folklore derandomization is to use the
$O(\log n)$ approximation for the set cover problem in the hitting set version
where we want to hit all the balls with the centers.
  However, the extra $O(\log n)$ factor is sometimes too expensive. For
example, the recent $O(m\sqrt{\log n\log\log n})$ undirected single-source
shortest path algorithm [DMSY23] beats Dijkstra's algorithm in sparse graphs,
but the folklore derandomization would make it dominated by Dijkstra's.
  In this paper, we exploit the fact that the sizes of these balls can be
adaptively chosen by the algorithm instead of fixed by the input. We propose a
simple deterministic algorithm achieving the optimal ball size of $\Theta(n/r)$
on average. Furthermore, given any polynomially large cost function of the ball
size, we can still achieve the optimal cost on average. It allows us to
derandomize [DMSY23], resulting in a deterministic $O(m\sqrt{\log n\log\log
n})$ algorithm for undirected single-source shortest path.
  In addition, we show that the same technique can also be used to derandomize
the seminal Thorup-Zwick approximate distance oracle [TZ05], also without any
loss in the time/space complexity.

</details>


### [28] [Edge-weighted Online Stochastic Matching: Beating $1-\frac1e$](https://arxiv.org/abs/2210.12543)
*Shuyi Yan*

Main category: cs.DS

TL;DR: 本文提出了首个突破1-1/e界限的边权重在线随机匹配算法，达到了0.645的竞争比。


<details>
  <summary>Details</summary>
Motivation: 自Feldman等人提出(1-1/e)竞争的Suggested Matching算法以来，一般边权重在线随机匹配问题一直没有改进。

Method: 基于Jaillet和Lu的LP，设计算法预处理将边分为两类，在Suggested Matching基础上调整匹配策略，早期和晚期分别优化不同类别的性能，同时保持不同边匹配事件的高度独立性。

Result: 实现了0.645的竞争比，首次突破了1-1/e的界限。

Conclusion: 通过平衡不同阶段的匹配策略，最终保证了每条边的匹配概率。

Abstract: We study the edge-weighted online stochastic matching problem. Since Feldman,
Mehta, Mirrokni, and Muthukrishnan proposed the $(1-\frac1e)$-competitive
Suggested Matching algorithm, there has been no improvement for the general
edge-weighted online stochastic matching problem. In this paper, we introduce
the first algorithm beating the $1-\frac1e$ barrier in this setting, achieving
a competitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we
design an algorithmic preprocessing, dividing all edges into two classes. Then
based on the Suggested Matching algorithm, we adjust the matching strategy to
improve the performance on one class in the early stage and on another class in
the late stage, while keeping the matching events of different edges highly
independent. By balancing them, we finally guarantee the matched probability of
every single edge.

</details>


### [29] [Vizing's Theorem in Deterministic Almost-Linear Time](https://arxiv.org/abs/2510.12619)
*Sepehr Assadi,Soheil Behnezhad,Sayan Bhattacharya,Martín Costa,Shay Solomon,Tianyi Zhang*

Main category: cs.DS

TL;DR: 本文提出了一种确定性几乎线性时间的(Δ+1)-边着色算法，突破了之前Õ(m√n)的时间复杂度障碍。


<details>
  <summary>Details</summary>
Motivation: Vizing定理表明任何最大度为Δ的图都可以用Δ+1种颜色进行边着色。虽然已有随机化算法达到近乎线性时间复杂度，但确定性算法的时间复杂度一直停留在Õ(m√n)。本文旨在突破这一确定性时间复杂度的障碍。

Method: 提出了一种新的确定性颜色类型稀疏化方法，该方法在几乎线性时间内运行（而非亚线性时间），但可以用于着色更大规模的边集。

Result: 开发了一个确定性几乎线性时间的(Δ+1)-着色算法，运行时间为m·2^O(√logΔ)·log n = m^(1+o(1))。

Conclusion: 成功突破了确定性(Δ+1)-边着色算法的时间复杂度障碍，证明了确定性算法的时间复杂度可以降低到Õ(m√n)以下。

Abstract: Vizing's theorem states that any $n$-vertex $m$-edge graph of maximum degree
$\Delta$ can be edge colored using at most $\Delta + 1$ different colors.
Vizing's original proof is easily translated into a deterministic $O(mn)$ time
algorithm. This deterministic time bound was subsequently improved to $\tilde
O(m \sqrt n)$ time, independently by [Arjomandi, 1982] and by [Gabow et al.,
1985].
  A series of recent papers improved the time bound of $\tilde O(m\sqrt{n})$
using randomization, culminating in the randomized near-linear time
$(\Delta+1)$-coloring algorithm by [Assadi, Behnezhad, Bhattacharya, Costa,
Solomon, and Zhang, 2025]. At the heart of all of these recent improvements,
there is some form of a sublinear time algorithm. Unfortunately, sublinear time
algorithms as a whole almost always require randomization. This raises a
natural question: can the deterministic time complexity of the problem be
reduced below the $\tilde O(m\sqrt{n})$ barrier?
  In this paper, we answer this question in the affirmative. We present a
deterministic almost-linear time $(\Delta+1)$-coloring algorithm, namely, an
algorithm running in $m \cdot 2^{O(\sqrt{\log \Delta})} \cdot \log n =
m^{1+o(1)}$ time. Our main technical contribution is to entirely forego
sublinear time algorithms. We do so by presenting a new deterministic
color-type sparsification approach that runs in almost-linear (instead of
sublinear) time, but can be used to color a much larger set of edges.

</details>
