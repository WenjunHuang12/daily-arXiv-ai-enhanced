{"id": "2511.19690", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.19690", "abs": "https://arxiv.org/abs/2511.19690", "authors": ["Niklas Haas", "Sören Schmitt", "Rob van Stee"], "title": "The Buffer Minimization Problem for Scheduling Flow Jobs with Conflicts", "comment": "17 pages, 1 figure, to appear in SOFSEM 2026", "summary": "We consider the online buffer minimization in multiprocessor systems with conflicts problem (in short, the buffer minimization problem) in the recently introduced flow model. In an online fashion, workloads arrive on some of the $n$ processors and are stored in an input buffer. Processors can run and reduce these workloads, but conflicts between pairs of processors restrict simultaneous task execution. Conflicts are represented by a graph, where vertices correspond to processors and edges indicate conflicting pairs. An online algorithm must decide which processors are run at a time; so provide a valid schedule respecting the conflict constraints.\n  The objective is to minimize the maximal workload observed across all processors during the schedule. Unlike the original model, where workloads arrive as discrete blocks at specific time points, the flow model assumes workloads arrive continuously over intervals or not at all. We present tight bounds for all graphs with four vertices (except the path, which has been solved previously) and for the families of general complete graphs and complete bipartite graphs. We also recover almost tight bounds for complete $k$-partite graphs.\n  For the original model, we narrow the gap for the graph consisting of a triangle and an additional edge to a fourth vertex."}
{"id": "2511.20111", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.20111", "abs": "https://arxiv.org/abs/2511.20111", "authors": ["Ben Bals", "Joakim Blikstad", "Greg Bodwin", "Daniel Dadush", "Sebastian Forster", "Yasamin Nazari"], "title": "Greedy Algorithms for Shortcut Sets and Hopsets", "comment": null, "summary": "We explore the power of greedy algorithms for hopsets and shortcut sets. In particular, we propose simple greedy algorithms that, given an input graph $G$ and a parameter $β$, compute a shortcut set or an exact hopset $H$ of hopbound at most $β$, and we prove the following guarantees about the size $|H|$ of the output:\n  For shortcut sets, we prove the bound $$|H| \\le \\tilde{O}\\left( \\frac{n^2}{β^3} + \\frac{n^{3/2}}{β^{3/2}} \\right).$$ This matches the current state-of-the-art upper bound by Kogan and Parter [SODA '22].\n  For exact hopsets of $n$-node, $m$-edge weighted graphs, the size of the output hopset is existentially optimal up to subpolynomial factors, under some technical assumptions.\n  Despite their simplicity and conceptual implications, these greedy algorithms are slower than existing sampling-based approaches. Our second set of results focus on faster deterministic algorithms that are based on a certain greedy set cover approximation algorithm on paths in the transitive closure. One consequence is a deterministic algorithm that takes $O(mn^{2/3})$ time to compute a shortcut set of size $\\tilde{O}(n)$ and hopbound $O(n^{1/3})$."}
{"id": "2511.20338", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.20338", "abs": "https://arxiv.org/abs/2511.20338", "authors": ["Duncan Adamson", "George B. Mertzios", "Paul G. Spirakis"], "title": "Maintaining Bipartite Colourings on Temporal Graphs on a Budget", "comment": null, "summary": "Graph colouring is a fundamental problem for networks, serving as a tool for avoiding conflicts via symmetry breaking, for example, avoiding multiple computer processes simultaneously updating the same resource. This paper considers a generalisation of this problem to \\emph{temporal graphs}, i.e., to graphs whose structure changes according to an ordered sequence of edge sets. In the simultaneous resource updating problem on temporal graphs, the resources which can be accessed will change, however, the necessity of symmetry breaking to avoid conflicts remains.\n  In this paper, we focus on the problem of \\emph{maintaining proper colourings} on temporal graphs in general, with a particular focus on bipartite colourings. Our aim is to minimise the total number of times that the vertices change colour, or, in the form of a decision problem, whether we can maintain a proper colouring by allowing not more colour changes than some given \\emph{budget}. On the negative side, we show that, despite bipartite colouring being easy on static graphs, the problem of maintaining such a colouring on graphs that are bipartite in each snapshot is NP-Hard to even approximate within \\emph{any} constant factor unless the Unique Games Conjecture fails. On the positive side, we provide an exact algorithm for a temporal graph with $n$ vertices, a lifetime $T$ and at most $k$ components in any given snapshot in $O(T \\vert E \\vert 2^{k} + n T 2^{2k})$ time, and an $O\\left(\\sqrt{\\log(nT)}\\right)$-factor approximation algorithm running in $\\tilde{O}((nT)^3)$ time.\n  Our results contribute to the structural complexity of networks that change with time with respect to a fundamental computational problem."}
{"id": "2511.20376", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.20376", "abs": "https://arxiv.org/abs/2511.20376", "authors": ["Andreas Göbel", "Janosch Ruff", "Leon Schiller"], "title": "Robust Algorithms for Finding Cliques in Random Intersection Graphs via Sum-of-Squares", "comment": null, "summary": "We study efficient algorithms for recovering cliques in dense random intersection graphs (RIGs). In this model, $d = n^{Ω(1)}$ cliques of size approximately $k$ are randomly planted by choosing the vertices to participate in each clique independently with probability $δ$. While there has been extensive work on recovering one, or multiple disjointly planted cliques in random graphs, the natural extension of this question to recovering overlapping cliques has been, surprisingly, largely unexplored. Moreover, because every vertex can be part of polynomially many cliques, this task is significantly harder than in case of disjointly planted cliques (as recently studied by Kothari, Vempala, Wein and Xu [COLT'23]) and manifests in the failure of simple combinatorial and even spectral algorithms.\n  In this work we obtain the first efficient algorithms for recovering the community structure of RIGs both from the perspective of exact and approximate recovery. Our algorithms are further robust to noise, monotone adversaries, a certain, optimal number of edge corruptions, and work whenever $k \\gg \\sqrt{n \\log(n)}$. Our techniques follow the proofs-to-algorithms framework utilizing the sum-of-squares hierarchy."}
{"id": "2511.19877", "categories": ["cs.MM", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.19877", "abs": "https://arxiv.org/abs/2511.19877", "authors": ["Xiangyu Zhao", "Yaling Shen", "Yiwen Jiang", "Zimu Wang", "Jiahe Liu", "Maxmartwell H Cheng", "Guilherme C Oliveira", "Robert Desimone", "Dominic Dwyer", "Zongyuan Ge"], "title": "It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models", "comment": null, "summary": "Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health."}
{"id": "2511.19446", "categories": ["cs.IT", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.19446", "abs": "https://arxiv.org/abs/2511.19446", "authors": ["Serkan Gür"], "title": "The Quality of Information: A Weighted Entropy Approach to Near-Optimal Mastermind", "comment": null, "summary": "This paper presents a novel class of information-theoretic strategies for solving the game of Mastermind, achieving state-of-the-art performance among known heuristic methods. The core contribution is the application of a weighted entropy heuristic, based on the Belis-Guias, u framework, which assigns context-dependent utility values to each of the possible feedback types. A genetic algorithm optimization approach discovers interpretable weight patterns that reflect strategic game dynamics. First, I demonstrate that a single, fixed vector of optimized weights achieves a remarkable 4.3565 average guesses with a maximum of 5. Building upon this, I introduce a stage-weighted heuristic with distinct utility vectors for each turn, achieving 4.3488 average guesses with a maximum of 6, approaching the theoretical optimum of 4.3403 by less than 0.2%. The method retains the computational efficiency of classical one-step-ahead heuristics while significantly improving performance through principled information valuation. A complete implementation and all optimized parameters are provided for full reproducibility."}
{"id": "2511.19514", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19514", "abs": "https://arxiv.org/abs/2511.19514", "authors": ["Yang Wu", "Qian Li", "Yuling Xiong", "Hongbo Tang", "Xun Liu", "Jun Zhang", "Huan Yu", "Jie Jiang", "Hailong Shi"], "title": "SCoTER: Structured Chain-of-Thought Transfer for Enhanced Recommendation", "comment": "12 pages,4 figures", "summary": "Harnessing the reasoning power of Large Language Models (LLMs) for recommender systems is hindered by two fundamental challenges. First, current approaches lack a mechanism for automated, data-driven discovery of effective reasoning patterns, relying instead on brittle manual templates or unstable zero-shot prompting. Second, they employ structure-collapsing integration: direct prompting incurs prohibitive online inference costs, while feature extraction collapses reasoning chains into single vectors, discarding stepwise logic. To address these challenges, we propose SCoTER (Structured Chain-of-Thought Transfer for Enhanced Recommendation), a unified framework that treats pattern discovery and structure-aware transfer as a jointly optimized problem. Specifically, SCoTER operationalizes this through two synergistic components: a GVM pipeline for automated pattern discovery and a structure-preserving integration architecture that transfers stepwise logic to efficient models. Formally, we provide information-theoretic justification proving that structure-preserving transfer achieves tighter performance bounds than structure-agnostic alternatives. Empirically, experiments on four benchmarks demonstrate improvements of 3.75\\%-11.59\\% over a strong TIGER backbone. Moreover, in production deployment on the Tencent Advertising Platform, SCoTER achieved a 2.14\\% lift in Gross Merchandise Value (GMV) while eliminating online LLM inference costs. Overall, SCoTER establishes a principled and production-validated blueprint for transferring structured LLM reasoning to large-scale recommender systems."}
{"id": "2511.19532", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.19532", "abs": "https://arxiv.org/abs/2511.19532", "authors": ["Thomas Buchholtzer", "Michel de Lara"], "title": "Games in Product Form for Demand Response Modelling", "comment": null, "summary": "Energy systems are changing rapidly. More and more, energy production is becoming decentralized, highly variable and intermittent (solar, wind), while demand is diversifying (electric vehicles). As a result, balancing supply and demand is becoming more complex, making the adjustment of demand an interesting tool. Demand response is a typical leader-follower problem: a consumer (follower) adjusts his energy consumption based on the prices (or any other incentive) set by the supplier (leader). We propose a versatile and modular framework to address any leader-follower problem, focusing on the handling of often overlooked informational issues. First, we introduce a model that defines the rules of the game (W-model): agents are decision-makers, and Nature encapsulates everything beyond their control, such as private knowledge and exogenous factors. Following the so-called Witsenhausen intrinsic model, we present an efficient way to represent - on a product set, equipped with a product $σ$-algebra - the information available to agents when making decisions. Next, we introduce Games in Product Form (W-games) by equipping each player (a group of agents) with preferences (objective function and belief) over different outcomes. Thereby, we incorporate an additional layer of information, the characteristics of the preferences linked to players, which affects the possible definitions of an equilibrium. We make this explicit in Nash and Stackelberg equilibria. Equipped with this framework, we reformulate several papers on demand response, highlighting overlooked informational issues. We also provide an application based on the Thailand demand response program."}
{"id": "2511.19830", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19830", "abs": "https://arxiv.org/abs/2511.19830", "authors": ["Junhao Zhu", "Lu Chen", "Xiangyu Ke", "Ziquan Fang", "Tianyi Li", "Yunjun Gao", "Christian S. Jensen"], "title": "Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization", "comment": null, "summary": "Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm.\n  We present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability."}
{"id": "2511.20385", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.20385", "abs": "https://arxiv.org/abs/2511.20385", "authors": ["Christine Awofeso", "Patrick Greaves", "Oded Lachish", "Felix Reidl"], "title": "Counting large patterns in degenerate graphs", "comment": null, "summary": "The problem of subgraph counting asks for the number of occurrences of a pattern graph $H$ as a subgraph of a host graph $G$ and is known to be computationally challenging: it is $\\#W[1]$-hard even when $H$ is restricted to simple structures such as cliques or paths. Curticapean and Marx (FOCS'14) show that if the graph $H$ has vertex cover number $τ$, subgraph counting has time complexity $O(|H|^{2^{O(τ)}} |G|^{τ+ O(1)})$. This raises the question of whether this upper bound can be improved for input graphs $G$ from a restricted family of graphs. Earlier work by Eppstein~(IPL'94) shows that this is indeed possible, by proving that when $G$ is a $d$-degenerate graph and $H$ is a biclique of arbitrary size, subgraph counting has time complexity $O(d 3^{d/3} |G|)$. We show that if the input is restricted to $d$-degenerate graphs, the upper bound of Curticapean and Marx can be improved for a family of graphs $H$ that includes all bicliques and satisfies a property we call $(c,d)$-locatable. Importantly, our algorithm's running time only has a polynomial dependence on the size of~$H$. A key feature of $(c,d)$-locatable graphs $H$ is that they admit a vertex cover of size at most $cd$. We further characterize $(1,d)$-locatable graphs, for which our algorithms achieve a linear running time dependence on $|G|$, and we establish a lower bound showing that counting graphs which are barely not $(1,d)$-locatable is already $\\#\\text{W}[1]$-hard. We note that the restriction to $d$-degenerate graphs has been a fruitful line of research leading to two very general results (FOCS'21, SODA'25) and this creates the impression that we largely understand the complexity of counting substructures in degenerate graphs. However, all aforementioned results have an exponential dependency on the size of the pattern graph $H$."}
{"id": "2511.20167", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.20167", "abs": "https://arxiv.org/abs/2511.20167", "authors": ["Yadong Liu", "Shangfei Wang"], "title": "FINE: Factorized multimodal sentiment analysis via mutual INformation Estimation", "comment": "15 pages, 9 figures, conference", "summary": "Multimodal sentiment analysis remains a challenging task due to the inherent heterogeneity across modalities. Such heterogeneity often manifests as asynchronous signals, imbalanced information between modalities, and interference from task-irrelevant noise, hindering the learning of robust and accurate sentiment representations. To address these issues, we propose a factorized multimodal fusion framework that first disentangles each modality into shared and unique representations, and then suppresses task-irrelevant noise within both to retain only sentiment-critical representations. This fine-grained decomposition improves representation quality by reducing redundancy, prompting cross-modal complementarity, and isolating task-relevant sentiment cues. Rather than manipulating the feature space directly, we adopt a mutual information-based optimization strategy to guide the factorization process in a more stable and principled manner. To further support feature extraction and long-term temporal modeling, we introduce two auxiliary modules: a Mixture of Q-Formers, placed before factorization, which precedes the factorization and uses learnable queries to extract fine-grained affective features from multiple modalities, and a Dynamic Contrastive Queue, placed after factorization, which stores latest high-level representations for contrastive learning, enabling the model to capture long-range discriminative patterns and improve class-level separability. Extensive experiments on multiple public datasets demonstrate that our method consistently outperforms existing approaches, validating the effectiveness and robustness of the proposed framework."}
{"id": "2511.19550", "categories": ["cs.IT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19550", "abs": "https://arxiv.org/abs/2511.19550", "authors": ["Davide Picca"], "title": "The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication", "comment": null, "summary": "This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $λ$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication."}
{"id": "2511.19931", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19931", "abs": "https://arxiv.org/abs/2511.19931", "authors": ["Ziwei Liu", "Qidong Liu", "Wanyu Wang", "Yejing Wang", "Tong Xu", "Wei Huang", "Chong Chen", "Peng Chuan", "Xiangyu Zhao"], "title": "LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training", "comment": null, "summary": "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}."}
{"id": "2511.19842", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.19842", "abs": "https://arxiv.org/abs/2511.19842", "authors": ["Joon Suk Huh", "Kirthevasan Kandasamy"], "title": "Strategy-robust Online Learning in Contextual Pricing", "comment": "32 pages", "summary": "Learning effective pricing strategies is crucial in digital marketplaces, especially when buyers' valuations are unknown and must be inferred through interaction. We study the online contextual pricing problem, where a seller observes a stream of context-valuation pairs and dynamically sets prices. Moreover, departing from traditional online learning frameworks, we consider a strategic setting in which buyers may misreport valuations to influence future prices, a challenge known as strategic overfitting (Amin et al., 2013).\n  We introduce a strategy-robust notion of regret for multi-buyer online environments, capturing worst-case strategic behavior in the spirit of the Price of Anarchy. Our first contribution is a polynomial-time approximation scheme (PTAS) for learning linear pricing policies in adversarial, adaptive environments, enabled by a novel online sketching technique. Building on this result, we propose our main construction: the Sparse Update Mechanism (SUM), a simple yet effective sequential mechanism that ensures robustness to all Nash equilibria among buyers. Moreover, our construction yields a black-box reduction from online expert algorithms to strategy-robust learners."}
{"id": "2511.20049", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20049", "abs": "https://arxiv.org/abs/2511.20049", "authors": ["Yushuai Ji", "Sheng Wang", "Zhiyu Chen", "Yuan Sun", "Zhiyong Peng"], "title": "Updatable Balanced Index for Fast On-device Search with Auto-selection Model", "comment": "Accepted for publication in the 42nd IEEE International Conference on Data Engineering (ICDE 2026). To appear", "summary": "Diverse types of edge data, such as 2D geo-locations and 3D point clouds, are collected by sensors like lidar and GPS receivers on edge devices. On-device searches, such as k-nearest neighbor (kNN) search and radius search, are commonly used to enable fast analytics and learning technologies, such as k-means dataset simplification using kNN. To maintain high search efficiency, a representative approach is to utilize a balanced multi-way KD-tree (BMKD-tree). However, the index has shown limited gains, mainly due to substantial construction overhead, inflexibility to real-time insertion, and inconsistent query performance. In this paper, we propose UnIS to address the above limitations. We first accelerate the construction process of the BMKD-tree by utilizing the dataset distribution to predict the splitting hyperplanes. To make the continuously generated data searchable, we propose a selective sub-tree rebuilding scheme to accelerate rebalancing during insertion by reducing the number of data points involved. We then propose an auto-selection model to improve query performance by automatically selecting the optimal search strategy among multiple strategies for an arbitrary query task. Experimental results show that UnIS achieves average speedups of 17.96x in index construction, 1.60x in insertion, 7.15x in kNN search, and 1.09x in radius search compared to the BMKD-tree. We further verify its effectiveness in accelerating dataset simplification on edge devices, achieving a speedup of 217x over Lloyd's algorithm."}
{"id": "2511.19556", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.19556", "abs": "https://arxiv.org/abs/2511.19556", "authors": ["Yanxiao Liu"], "title": "One-Shot Coding and Applications", "comment": "A Thesis for the Degree of Doctor of Philosophy in Information Engineering, The Chinese University of Hong Kong", "summary": "One-shot information theory addresses scenarios in source coding and channel coding where the signal blocklength is assumed to be 1. In this case, each source and channel can be used only once, and the sources and channels are arbitrary and not required to be memoryless or ergodic. We study the achievability part of one-shot information theory, i.e., we consider explicit coding schemes in the oneshot scenario. The objective is to derive one-shot achievability results that can imply existing (first-order and second-order) asymptotic results when applied to memoryless sources and channels, or applied to systems with memory that behave ergodically.\n  Poisson functional representation was first proposed as a one-shot channel simulation technique by Li and El Gamal [118] for proving a strong functional representation lemma. It was later extended to the Poisson matching lemma by Li and Anantharam [117], which provided a unified one-shot coding scheme for a broad class of information-theoretic problems. The main contribution of this thesis is to extend the applicability of Poisson functional representation to various more complicated scenarios, where the original version cannot be applied directly and further extensions must be developed."}
{"id": "2511.19979", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19979", "abs": "https://arxiv.org/abs/2511.19979", "authors": ["Kaike Zhang", "Jiakai Tang", "Du Su", "Shuchang Liu", "Julian McAuley", "Lina Yao", "Qi Cao", "Yue Feng", "Fei Sun"], "title": "The 2nd Workshop on Human-Centered Recommender Systems", "comment": null, "summary": "Recommender systems shape how people discover information, form opinions, and connect with society. Yet, as their influence grows, traditional metrics, e.g., accuracy, clicks, and engagement, no longer capture what truly matters to humans. The workshop on Human-Centered Recommender Systems (HCRS) calls for a paradigm shift from optimizing engagement toward designing systems that truly understand, involve, and benefit people. It brings together researchers in recommender systems, human-computer interaction, AI safety, and social computing to explore how human values, e.g., trust, safety, fairness, transparency, and well-being, can be integrated into recommendation processes. Centered around three thematic axes-Human Understanding, Human Involvement, and Human Impact-HCRS features keynotes, panels, and papers covering topics from LLM-based interactive recommenders to societal welfare optimization. By fostering interdisciplinary collaboration, HCRS aims to shape the next decade of responsible and human-aligned recommendation research."}
{"id": "2511.19930", "categories": ["cs.GT", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19930", "abs": "https://arxiv.org/abs/2511.19930", "authors": ["Kenta Yamamoto", "Teruaki Hayashi"], "title": "Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities", "comment": "10 pages, 10 figures", "summary": "Recent advances in machine learning and big data analytics have intensified the demand for high-quality cross-domain datasets and accelerated the growth of data trading across organizations. As data become increasingly recognized as an economic asset, data marketplaces have emerged as a key infrastructure for data-driven innovation. However, unlike mature product or service markets, data-trading environments remain nascent and suffer from pronounced information asymmetry. Buyers cannot verify the content or quality before purchasing data, making trust and quality assurance central challenges. To address these issues, this study develops a multi-agent data-market simulator that models participant behavior and evaluates the institutional mechanisms for trust formation. Focusing on the manufacturing sector, where initiatives such as GAIA-X and Catena-X are advancing, the simulator integrates reinforcement learning (RL) for adaptive agent behavior and inverse reinforcement learning (IRL) to estimate utility functions from empirical behavioral data. Using the simulator, we examine the market-level effects of five representative reputation systems-Time-decay, Bayesian-beta, PageRank, PowerTrust, and PeerTrust-and found that PeerTrust achieved the strongest alignment between data price and quality, while preventing monopolistic dominance. Building on these results, we develop a hybrid reputation mechanism that integrates the strengths of existing systems to achieve improved price-quality consistency and overall market stability. This study extends simulation-based data-market analysis by incorporating trust and reputation as endogenous mechanisms and offering methodological and institutional insights into the design of reliable and efficient data ecosystems."}
{"id": "2511.20084", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20084", "abs": "https://arxiv.org/abs/2511.20084", "authors": ["Mariana M. Garcez Duarte", "Dwi P. A. Nugroho", "Georges Tod", "Evert Bevernage", "Pieter Moelans", "Emine Tas", "Esteban Zimanyi", "Mahmoud Sakr", "Steffen Zeuch", "Volker Markl"], "title": "Mobility Stream Processing on NebulaStream and MEOS", "comment": null, "summary": "The increasing use of Internet-of-Things (IoT) sensors in moving objects has resulted in vast amounts of spatiotemporal streaming data. To analyze this data in situ, real-time spatiotemporal processing is needed. However, current stream processing systems designed for IoT environments often lack spatiotemporal processing capabilities, and existing spatiotemporal libraries primarily focus on analyzing historical data. This gap makes performing real-time spatiotemporal analytics challenging. In this demonstration, we present NebulaMEOS, which combines MEOS (Mobility Engine Open Source), a spatiotemporal processing library, with NebulaStream, a scalable data management system for IoT applications. By integrating MEOS into NebulaStream, NebulaMEOS utilizes spatiotemporal functionalities to process and analyze streaming data in real-time. We demonstrate NebulaMEOS by querying data streamed from edge devices on trains by the Société Nationale des Chemins de fer Belges (SNCB). Visitors can experience demonstrations of geofencing and geospatial complex event processing, visualizing real-time train operations and environmental impacts."}
{"id": "2511.19568", "categories": ["cs.IT", "eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.19568", "abs": "https://arxiv.org/abs/2511.19568", "authors": ["Sunder Ram Krishnan", "Junaid Farooq", "Kumar Vijay Mishra", "Xingchen Liu", "S. Unnikrishna Pillai", "Theodore S. Rappaport"], "title": "A Hybrid Dominant-Interferer Approximation for SINR Coverage in Poisson Cellular Networks", "comment": null, "summary": "Accurate radio propagation and interference modeling is essential for the design and analysis of modern cellular networks. Stochastic geometry offers a rigorous framework by treating base station locations as a Poisson point process and enabling coverage characterization through spatial averaging, but its expressions often involve nested integrals and special functions that limit general applicability. Probabilistic interference models seek closed-form characterizations through moment-based approximations, yet these expressions remain tractable only for restricted parameter choices and become unwieldy when interference moments lack closed-form representations. This work introduces a hybrid approximation framework that addresses these challenges by combining Monte Carlo sampling of a small set of dominant interferers with a Laplace functional representation of the residual far-field interference. The resulting dominant-plus-tail structure provides a modular, numerically stable, and path-loss-agnostic estimator suitable for both noise-limited and interference-limited regimes. We further derive theoretical error bounds that decrease with the number of dominant interferers and validate the approach against established stochastic geometry and probabilistic modeling benchmarks."}
{"id": "2511.19999", "categories": ["cs.IR", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19999", "abs": "https://arxiv.org/abs/2511.19999", "authors": ["Anton Lyubinin"], "title": "Popularity Bias Alignment Estimates", "comment": null, "summary": "We are extending Popularity Bias Memorization theorem from arXiv:archive/2404.12008 in several directions. We extend it to arbitrary degree distributions and also prove both upper and lower estimates for the alignment with top-k singular hyperspace."}
{"id": "2511.20110", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.20110", "abs": "https://arxiv.org/abs/2511.20110", "authors": ["Michal Feldman", "Yoav Gal-Tzur", "Tomasz Ponitka", "Maya Schlesinger"], "title": "One Action Too Many: Inapproximability of Budgeted Combinatorial Contracts", "comment": "Accepted to ITCS 2026", "summary": "We study multi-agent contract design with combinatorial actions, under budget constraints, and for a broad class of objective functions, including profit (principal's utility), reward, and welfare. Our first result is a strong impossibility: For submodular reward functions, no randomized poly-time algorithm can approximate the optimal budget-feasible value within \\textit{any finite factor}, even with demand-oracle access. This result rules out extending known constant-factor guarantees from either (i) unbudgeted settings with combinatorial actions or (ii) budgeted settings with binary actions, to their combination. The hardness is tight: It holds even when all but one agent have binary actions and the remaining agent has just one additional action. On the positive side, we show that gross substitutes rewards (a well-studied strict subclass of submodular functions) admit a deterministic poly-time $O(1)$-approximation, using only value queries. Our results thus draw the first sharp separation between budgeted and unbudgeted settings in combinatorial contracts, and identifies gross substitutes as a tractable frontier for budgeted combinatorial contracts. Finally, we present an FPTAS for additive rewards, demonstrating that arbitrary approximation is tractable under any budget. This constitutes the first FPTAS for the multi-agent combinatorial-actions setting, even in the absence of budget constraints."}
{"id": "2511.20125", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20125", "abs": "https://arxiv.org/abs/2511.20125", "authors": ["Yihua Hu", "Hao Ding", "Wei Dong"], "title": "N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics", "comment": null, "summary": "Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation."}
{"id": "2511.19639", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.19639", "abs": "https://arxiv.org/abs/2511.19639", "authors": ["Niccolò Brembilla", "Yinbin Ma", "Pietro Belotti", "Federico Malucelli", "Daniela Tuninetti"], "title": "Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding", "comment": null, "summary": "Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding."}
{"id": "2511.20009", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20009", "abs": "https://arxiv.org/abs/2511.20009", "authors": ["Yulong Deng", "Zheng Guan", "Min He", "Xue Wang", "Jie Liu", "Zheng Li"], "title": "Adaptive Knowledge Transfer for Cross-Disciplinary Cold-Start Knowledge Tracing", "comment": "10 pages, 5 figures", "summary": "Cross-Disciplinary Cold-start Knowledge Tracing (CDCKT) faces a critical challenge: insufficient student interaction data in the target discipline prevents effective knowledge state modeling and performance prediction. Existing cross-disciplinary methods rely on overlapping entities between disciplines for knowledge transfer through simple mapping functions, but suffer from two key limitations: (1) overlapping entities are scarce in real-world scenarios, and (2) simple mappings inadequately capture cross-disciplinary knowledge complexity. To overcome these challenges, we propose Mixed of Experts and Adversarial Generative Network-based Cross-disciplinary Cold-start Knowledge Tracing Framework. Our approach consists of three key components: First, we pre-train a source discipline model and cluster student knowledge states into K categories. Second, these cluster attributes guide a mixture-of-experts network through a gating mechanism, serving as a cross-domain mapping bridge. Third, an adversarial discriminator enforces feature separation by pulling same-attribute student features closer while pushing different-attribute features apart, effectively mitigating small-sample limitations. We validate our method's effectiveness across 20 extreme cross-disciplinary cold-start scenarios."}
{"id": "2511.20289", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.20289", "abs": "https://arxiv.org/abs/2511.20289", "authors": ["Kang Wang", "Renzhe Xu", "Bo Li"], "title": "Lower Bias, Higher Welfare: How Creator Competition Reshapes Bias-Variance Tradeoff in Recommendation Platforms?", "comment": "KDD 2026", "summary": "Understanding the bias-variance tradeoff in user representation learning is essential for improving recommendation quality in modern content platforms. While well studied in static settings, this tradeoff becomes significantly more complex when content creators strategically adapt to platform incentives. To analyze how such competition reshapes the tradeoff for maximizing user welfare, we introduce the Content Creator Competition with Bias-Variance Tradeoff framework, a tractable game-theoretic model that captures the platform's decision on regularization strength in user feature estimation. We derive and compare the platform's optimal policy under two key settings: a non-strategic baseline with fixed content and a strategic environment where creators compete in response to the platform's algorithmic design.\n  Our theoretical analysis in a stylized model shows that, compared to the non-strategic environment, content creator competition shifts the platform's optimal policy toward weaker regularization, thereby favoring lower bias in the bias-variance tradeoff. To validate and assess the robustness of these insights beyond the stylized setting, we conduct extensive experiments on both synthetic and real-world benchmark datasets. The empirical results consistently support our theoretical conclusion: in strategic environments, reducing bias leads to higher user welfare. These findings offer practical implications for the design of real-world recommendation algorithms in the presence of content creator competition."}
{"id": "2511.20139", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20139", "abs": "https://arxiv.org/abs/2511.20139", "authors": ["Mariana M Garcez Duarte", "Mahmoud Sakr"], "title": "An experimental study of existing tools for outlier detection and cleaning in trajectories", "comment": null, "summary": "Outlier detection and cleaning are essential steps in data preprocessing to ensure the integrity and validity of data analyses. This paper focuses on outlier points within individual trajectories, i.e., points that deviate significantly inside a single trajectory. We experiment with ten open-source libraries to comprehensively evaluate available tools, comparing their efficiency and accuracy in identifying and cleaning outliers. This experiment considers the libraries as they are offered to end users, with real-world applicability. We compare existing outlier detection libraries, introduce a method for establishing ground-truth, and aim to guide users in choosing the most appropriate tool for their specific outlier detection needs. Furthermore, we survey the state-of-the-art algorithms for outlier detection and classify them into five types: Statistic-based methods, Sliding window algorithms, Clustering-based methods, Graph-based methods, and Heuristic-based methods. Our research provides insights into these libraries' performance and contributes to developing data preprocessing and outlier detection methodologies."}
{"id": "2511.19745", "categories": ["cs.IT", "eess.SP", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.19745", "abs": "https://arxiv.org/abs/2511.19745", "authors": ["Yassine Afif", "Mohammed Almekhlafi", "Antoine Lesage-Landry", "Gunes Karabulut Kurt"], "title": "Joint Satellite Power Consumption and Handover Optimization for LEO Constellations", "comment": null, "summary": "In satellite constellation-based communication systems, continuous user coverage requires frequent handoffs due to the dynamic topology induced by the Low Earth Orbit (LEO) satellites. Each handoff between a satellite and ground users introduces additional signaling and power consumption, which can become a significant burden as the size of the constellation continues to increase. This work focuses on the optimization of the total transmission rate in a LEO-to-user system, by jointly considering the total transmitted power, user-satellite associations, and power consumption, the latter being handled through a penalty on handoff events. We consider a system where LEO satellites serve users located in remote areas with no terrestrial connectivity, and formulate the power allocation problem as a mixed-integer concave linear program (MICP) subject to power and association constraints. Our approach can be solved with off-the-shelf solvers and is benchmarked against a naive baseline where users associate to their closest visible satellite. Extensive Monte Carlo simulations demonstrate the effectiveness of the proposed method in controlling the handoff frequency while maintaining high user throughput. These performance gains highlight the effectiveness of our handover-aware optimization strategy, which ensures that user rates improve significantly, by about 40%, without incurring a disproportionate rise in the handoff frequency."}
{"id": "2511.20122", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20122", "abs": "https://arxiv.org/abs/2511.20122", "authors": ["Ximing Chen", "Pui Ieng Lei", "Yijun Sheng", "Yanyan Liu", "Zhiguo Gong"], "title": "Towards A Tri-View Diffusion Framework for Recommendation", "comment": "13 pages, 11 figures, accepted by KDD2026 (First Cycle)", "summary": "Diffusion models (DMs) have recently gained significant interest for their exceptional potential in recommendation tasks. This stems primarily from their prominent capability in distilling, modeling, and generating comprehensive user preferences. However, previous work fails to examine DMs in recommendation tasks through a rigorous lens. In this paper, we first experimentally investigate the completeness of recommender models from a thermodynamic view. We reveal that existing DM-based recommender models operate by maximizing the energy, while classic recommender models operate by reducing the entropy. Based on this finding, we propose a minimalistic diffusion framework that incorporates both factors via the maximization of Helmholtz free energy. Meanwhile, to foster the optimization, our reverse process is armed with a well-designed denoiser to maintain the inherent anisotropy, which measures the user-item cross-correlation in the context of bipartite graphs. Finally, we adopt an Acceptance-Rejection Gumbel Sampling Process (AR-GSP) to prioritize the far-outnumbered unobserved interactions for model robustness. AR-GSP integrates an acceptance-rejection sampling to ensure high-quality hard negative samples for general recommendation tasks, and a timestep-dependent Gumbel Softmax to handle an adaptive sampling strategy for diffusion models. Theoretical analyses and extensive experiments demonstrate that our proposed framework has distinct superiority over baselines in terms of accuracy and efficiency."}
{"id": "2511.19446", "categories": ["cs.IT", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.19446", "abs": "https://arxiv.org/abs/2511.19446", "authors": ["Serkan Gür"], "title": "The Quality of Information: A Weighted Entropy Approach to Near-Optimal Mastermind", "comment": null, "summary": "This paper presents a novel class of information-theoretic strategies for solving the game of Mastermind, achieving state-of-the-art performance among known heuristic methods. The core contribution is the application of a weighted entropy heuristic, based on the Belis-Guias, u framework, which assigns context-dependent utility values to each of the possible feedback types. A genetic algorithm optimization approach discovers interpretable weight patterns that reflect strategic game dynamics. First, I demonstrate that a single, fixed vector of optimized weights achieves a remarkable 4.3565 average guesses with a maximum of 5. Building upon this, I introduce a stage-weighted heuristic with distinct utility vectors for each turn, achieving 4.3488 average guesses with a maximum of 6, approaching the theoretical optimum of 4.3403 by less than 0.2%. The method retains the computational efficiency of classical one-step-ahead heuristics while significantly improving performance through principled information valuation. A complete implementation and all optimized parameters are provided for full reproducibility."}
{"id": "2511.20293", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20293", "abs": "https://arxiv.org/abs/2511.20293", "authors": ["Chaowei He", "Yuanjun Liu", "Qingzhi Ma", "Shenyuan Ren", "Xizhao Luo", "Lei Zhao", "An Liu"], "title": "Forgetting by Pruning: Data Deletion in Join Cardinality Estimation", "comment": "AAAI26", "summary": "Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time."}
{"id": "2511.19812", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.19812", "abs": "https://arxiv.org/abs/2511.19812", "authors": ["Hao Wu", "Bocong Chen", "Guanghui Zhang", "Hongwei Liu"], "title": "Two-Step Decoding of Binary $2\\times2$ Sum-Rank-Metric Codes", "comment": "16 pages", "summary": "We resolve an open problem posed by Chen--Cheng--Qi (IEEE Trans.\\ Inf.\\ Theory, 2025): can decoding of binary sum-rank-metric codes $\\SR(C_1,C_2)$ with $2\\times2$ matrix blocks be reduced entirely to decoding the constituent Hamming-metric codes $C_1$ and $C_2$ without the additional requirement $d_1\\ge\\tfrac{2}{3}d_{\\mathrm{sr}}$ that underlies their fast decoder? We answer this in the affirmative by exhibiting a simple two-step procedure: first uniquely decode $C_2$, then apply a single error/erasure decoding of $C_1$.This shows that the restrictive hypothesis $d_1\\ge\\tfrac{2}{3}d_{\\mathrm{sr}}$ is theoretically unnecessary.The resulting decoder achieves unique decoding up to $\\lfloor (d_{\\mathrm{sr}}-1)/2\\rfloor$ with overall cost $T_2+T_1$, where $T_2$ and $T_1$ are the complexities of the Hamming decoders for $C_2$ and $C_1$, respectively. We further show that this reduction is asymptotically optimal in a black-box model, as any sum-rank decoder must inherently decode the constituent Hamming codes.For BCH or Goppa instantiations over $\\F_4$, the decoder runs in $O(\\ell^2)$ time."}
{"id": "2511.20177", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20177", "abs": "https://arxiv.org/abs/2511.20177", "authors": ["Tianjie Dai", "Xu Chen", "Yunmeng Shu", "Jinsong Lan", "Xiaoyong Zhu", "Jiangchao Yao", "Bo Zheng"], "title": "Enhancing Sequential Recommendation with World Knowledge from Large Language Models", "comment": null, "summary": "Sequential Recommendation System~(SRS) has become pivotal in modern society, which predicts subsequent actions based on the user's historical behavior. However, traditional collaborative filtering-based sequential recommendation models often lead to suboptimal performance due to the limited information of their collaborative signals. With the rapid development of LLMs, an increasing number of works have incorporated LLMs' world knowledge into sequential recommendation. Although they achieve considerable gains, these approaches typically assume the correctness of LLM-generated results and remain susceptible to noise induced by LLM hallucinations. To overcome these limitations, we propose GRASP (Generation Augmented Retrieval with Holistic Attention for Sequential Prediction), a flexible framework that integrates generation augmented retrieval for descriptive synthesis and similarity retrieval, and holistic attention enhancement which employs multi-level attention to effectively employ LLM's world knowledge even with hallucinations and better capture users' dynamic interests. The retrieved similar users/items serve as auxiliary contextual information for the later holistic attention enhancement module, effectively mitigating the noisy guidance of supervision-based methods. Comprehensive evaluations on two public benchmarks and one industrial dataset reveal that GRASP consistently achieves state-of-the-art performance when integrated with diverse backbones. The code is available at: https://anonymous.4open.science/r/GRASP-SRS."}
{"id": "2511.20419", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20419", "abs": "https://arxiv.org/abs/2511.20419", "authors": ["Gianna Lisa Nicolai", "Patrick Hansert", "Sebastian Michel"], "title": "The Case for Intent-Based Query Rewriting", "comment": "Published in the 2nd International Workshop on Data-driven AI (DATAI) 2025", "summary": "With this work, we describe the concept of intent-based query rewriting and present a first viable solution. The aim is to allow rewrites to alter the structure and syntactic outcome of an original query while keeping the obtainable insights intact. This drastically differs from traditional query rewriting, which typically aims to decrease query evaluation time by using strict equivalence rules and optimization heuristics on the query plan. Rewriting queries to queries that only provide a similar insight but otherwise can be entirely different can remedy inaccessible original data tables due to access control, privacy, or expensive data access regarding monetary cost or remote access. In this paper, we put forward INQURE, a system designed for INtent-based QUery REwriting. It uses access to a large language model (LLM) for the query understanding and human-like derivation of alternate queries. Around the LLM, INQURE employs upfront table filtering and subsequent candidate rewrite pruning and ranking. We report on the results of an evaluation using a benchmark set of over 900 database table schemas and discuss the pros and cons of alternate approaches regarding runtime and quality of the rewrites of a user study."}
{"id": "2511.19947", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19947", "abs": "https://arxiv.org/abs/2511.19947", "authors": ["Yuxuan Wu", "Linghan Ma", "Ruichen Zhang", "Yinqiu Liu", "Dusit Niyato", "Shunpu Tang", "Zehui Xiong", "Zhu Han", "Zhaohui Yang", "Kaibin Huang", "Zhaoyang Zhang", "Kai-Kit Wong"], "title": "Towards Edge General Intelligence: Knowledge Distillation for Mobile Agentic AI", "comment": "21 pages, 6 figures", "summary": "Edge General Intelligence (EGI) represents a paradigm shift in mobile edge computing, where intelligent agents operate autonomously in dynamic, resource-constrained environments. However, the deployment of advanced agentic AI models on mobile and edge devices faces significant challenges due to limited computation, energy, and storage resources. To address these constraints, this survey investigates the integration of Knowledge Distillation (KD) into EGI, positioning KD as a key enabler for efficient, communication-aware, and scalable intelligence at the wireless edge. In particular, we emphasize KD techniques specifically designed for wireless communication and mobile networking, such as channel-aware self-distillation, cross-model Channel State Information (CSI) feedback distillation, and robust modulation/classification distillation. Furthermore, we review novel architectures natively suited for KD and edge deployment, such as Mamba, RWKV (Receptance, Weight, Key, Value) and Cross-Architecture distillation, which enhance generalization capabilities. Subsequently, we examine diverse applications in which KD-driven architectures enable EGI across vision, speech, and multimodal tasks. Finally, we highlight the key challenges and future directions for KD in EGI. This survey aims to provide a comprehensive reference for researchers exploring KD-driven frameworks for mobile agentic AI in the era of EGI."}
{"id": "2511.20227", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20227", "abs": "https://arxiv.org/abs/2511.20227", "authors": ["Anyang Tong", "Xiang Niu", "ZhiPing Liu", "Chang Tian", "Yanyan Wei", "Zenglin Shi", "Meng Wang"], "title": "HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents", "comment": null, "summary": "Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding."}
{"id": "2511.20489", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20489", "abs": "https://arxiv.org/abs/2511.20489", "authors": ["Kanchan Chowdhury", "Lixi Zhou", "Lulu Xie", "Xinwei Fu", "Jia Zou"], "title": "InferF: Declarative Factorization of AI/ML Inferences over Joins", "comment": "Accepted to SIGMOD 2026 as full research paper. This archived version has a full appendix", "summary": "Real-world AI/ML workflows often apply inference computations to feature vectors joined from multiple datasets. To avoid the redundant AI/ML computations caused by repeated data records in the join's output, factorized ML has been proposed to decompose ML computations into sub-computations to be executed on each normalized dataset. However, there is insufficient discussion on how factorized ML could impact AI/ML inference over multi-way joins. To address the limitations, we propose a novel declarative InferF system, focusing on the factorization of arbitrary inference workflows represented as analyzable expressions over the multi-way joins. We formalize our problem to flexibly push down partial factorized computations to qualified nodes in the join tree to minimize the overall inference computation and join costs and propose two algorithms to resolve the problem: (1) a greedy algorithm based on a per-node cost function that estimates the influence on overall latency if a subset of factorized computations is pushed to a node, and (2) a genetic algorithm for iteratively enumerating and evaluating promising factorization plans. We implement InferF on Velox, an open-sourced database engine from Meta, evaluate it on real-world datasets, observed up to 11.3x speedups, and systematically summarized the factors that determine when factorized ML can benefit AI/ML inference workflows."}
{"id": "2511.20108", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20108", "abs": "https://arxiv.org/abs/2511.20108", "authors": ["Miled Alam", "Abdul Karim Gizzini", "Laurent Clavier"], "title": "Explainable Deep Learning for Secrecy Energy-Efficiency Maximization in Ambient Backscatter Multi-User NOMA Systems", "comment": "This article has been submitted to the IEEE Transactions on Green Communications and Networking as a Regular Paper", "summary": "In this paper, we investigate the secrecy energy-efficiency (SEE) of a multi-user downlink non-orthogonal multiple access (NOMA) system assisted by multiple ambient backscatter communications (AmBC) in the presence of a passive eavesdropper. We analyze both the trade-off and the ratio between the achievable secrecy sum-rate and total power consumption. In the special case of two backscatter devices (BDs), we derive closed-form solutions for the optimal reflection coefficients and power allocation by exploiting the structure of the SEE objective and the Pareto boundary of the feasible set. When more than two BDs are present, the problem becomes analytically intractable. To address this, we propose two efficient optimization techniques: (i) an exhaustive grid-based benchmark method, and (ii) a scalable particle swarm optimization algorithm. Furthermore, we design a deep learning-based predictor using a feedforward neural network (FNN), which closely approximates the optimal solutions. Numerical results show that the inclusion of AmBC significantly improves SEE, with gains up to 615% compared to conventional NOMA in high-noise regimes. Additionally, the FNN model achieves more than 95% accuracy compared to the optimal baseline, while reducing complexity. Finally, we employ SHAP (SHapley Additive exPlanations) to interpret the learned model, revealing that the most influential features correspond to the dominant composite channel components, in accordance with the theoretical system model. This demonstrates the potential of explainable artificial intelligence to build trust in energy-efficient and secure AmBC-NOMA systems for next-generation internet of things applications."}
{"id": "2511.20235", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20235", "abs": "https://arxiv.org/abs/2511.20235", "authors": ["Liren Yu", "Wenming Zhang", "Silu Zhou", "Zhixuan Zhang", "Dan Ou"], "title": "HHFT: Hierarchical Heterogeneous Feature Transformer for Recommendation Systems", "comment": null, "summary": "We propose HHFT (Hierarchical Heterogeneous Feature Transformer), a Transformer-based architecture tailored for industrial CTR prediction. HHFT addresses the limitations of DNN through three key designs: (1) Semantic Feature Partitioning: Grouping heterogeneous features (e.g. user profile, item information, behaviour sequennce) into semantically coherent blocks to preserve domain-specific information; (2) Heterogeneous Transformer Encoder: Adopting block-specific QKV projections and FFNs to avoid semantic confusion between distinct feature types; (3) Hiformer Layer: Capturing high-order interactions across features. Our findings reveal that Transformers significantly outperform DNN baselines, achieving a +0.4% improvement in CTR AUC at scale. We have successfully deployed the model on Taobao's production platform, observing a significant uplift in key business metrics, including a +0.6% increase in Gross Merchandise Value (GMV)."}
{"id": "2511.20117", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20117", "abs": "https://arxiv.org/abs/2511.20117", "authors": ["Min Xu", "Xuejiao Han", "Kai Wan", "Gennian Ge"], "title": "On hierarchical secure aggregation against relay and user collusion", "comment": null, "summary": "Secure aggregation (SA) is fundamental to privacy preservation in federated learning (FL), enabling model aggregation while preventing disclosure of individual user updates. This paper addresses hierarchical secure aggregation (HSA) against relay and user collusion in homogeneous networks, where each user connects to $n$ relays and each relay serves $m$ users. In the two-phase communication framework, users transmit masked data to relays, which then process and forward compiled messages to the server for exact sum recovery. The primary objective is to devise a transmission scheme such that the server can finish the aggregation task, while any group of $T_h$ colluding relays and $T_u$ colluding users cannot reveal any information about the data owned by the non-colluding users. In this study, we establish fundamental limits on the communication load, defined as the ratio of transmitted information size to original data size, for each user-relay link and each relay-server link. Achievable thresholds for collusion resilience are also derived. When the number of colluding relays and users falls below certain critical thresholds, we construct communication-optimal schemes using methods from network function computation. A limitation of these schemes is their reliance on large random keys. To address this, we derive a lower bound on the required key size and prove its achievability in cyclic networks, where users are connected to relays in a cyclic wrap-around manner. By establishing a connection between HSA and network function computation, this work advances the theoretical limits of communication efficiency and information-theoretic security in secure aggregation."}
{"id": "2511.20127", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20127", "abs": "https://arxiv.org/abs/2511.20127", "authors": ["Ali Khalesi"], "title": "General Multi-User Distributed Computing", "comment": null, "summary": "This work develops a unified {learning- and information-theoretic} framework for distributed computation and inference across multiple users and servers. The proposed \\emph{General Multi-User Distributed Computing (GMUDC)} model characterizes how computation, communication, and accuracy can be jointly optimized when users demand heterogeneous target functions that are arbitrary transformations of shared real-valued subfunctions. Without any separability assumption, and requiring only that each target function lies in a reproducing-kernel Hilbert space associated with a shift-invariant kernel, the framework remains valid for arbitrary connectivity and task-assignment topologies. A dual analysis is introduced: the \\emph{quenched design} considers fixed assignments of subfunctions and network topology, while the \\emph{annealed design} captures the averaged performance when assignments and links are drawn uniformly at random from a given ensemble. These formulations reveal the fundamental limits governing the trade-offs among computing load, communication load, and reconstruction distortion under computational and communication budgets~$Γ$ and~$Δ$. The analysis establishes a spectral-coverage duality linking generalization capability with network topology and resource allocation, leading to provably efficient and topology-aware distributed designs. The resulting principles provide an \\emph{information-energy foundation} for scalable and resource-optimal distributed and federated learning systems, with direct applications to aeronautical, satellite, and edge-intelligent networks where energy and data efficiency are critical."}
{"id": "2511.20160", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20160", "abs": "https://arxiv.org/abs/2511.20160", "authors": ["Francisco Díaz-Ruiz", "Francisco J. Martín-Vega", "Jose A. Cortés", "Gerardo Gómez", "Mari Carmen Aguayo"], "title": "CSI Prediction Frameworks for Enhanced 5G Link Adaptation: Performance-Complexity Trade-offs", "comment": null, "summary": "Accurate and timely channel state information (CSI) is fundamental for efficient link adaptation. However, challenges such as channel aging, user mobility, and feedback delays significantly impact the performance of adaptive modulation and coding (AMC). This paper proposes and evaluates two CSI prediction frameworks applicable to both time division duplexing (TDD) and frequency division duplexing (FDD) systems. The proposed methods operate in the effective signal to interference plus noise ratio (SINR) domain to reduce complexity while preserving predictive accuracy. A comparative analysis is conducted between a classical Wiener filter and state-of-the-art deep learning frameworks based on gated recurrent units (GRUs), long short-term memory (LSTM) networks, and a delayed deep neural network (DNN). The evaluation considers the accuracy of the prediction in terms of mean squared error (MSE), the performance of the system, and the complexity of the implementation regarding floating point operations (FLOPs). Furthermore, we investigate the generalizability of both approaches under various propagation conditions. The simulation results show that the Wiener filter performs close to GRU in terms of MSE and throughput with lower computational complexity, provided that the second-order statistics of the channel are available. However, the GRU model exhibits enhanced generalization across different channel scenarios. These findings suggest that while learningbased solutions are well-suited for TDD systems where the base station (BS) handles the computation, the lower complexity of classical methods makes them a preferable choice for FDD setups, where prediction occurs at the power-constrained user equipment (UE)."}
{"id": "2511.20364", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20364", "abs": "https://arxiv.org/abs/2511.20364", "authors": ["Wenkun Wen", "Tierui Min", "Long Yuan", "Minghua Xia"], "title": "Unified Block Signal Processing Framework for LPWANs: Sequence Index Modulation Spreading", "comment": "13 pages, 9 figures, 5 tables; submitted for possible publication", "summary": "Low-power wide-area networks (LPWANs) demand high receiver sensitivity and efficient physical-layer signal processing. This paper introduces a unified framework for generalized block signal transmission in LPWANs, addressing the limitations of conventional symbol-by-symbol approaches. The framework comprises three key components: the signal block vector, the intra-block structure generator, and the signal basis matrix, and leverages quasi-orthogonal codewords formed through cyclically shifted spreading sequences. The resulting quasi-orthogonality enables reliable multi-user separation, particularly under asynchronous access. The framework establishes a conceptual foundation for block synchronization and provides a unified demodulation structure based on block correlation matching. It further supports flexible and systematic implementation, as demonstrated through applications to frequency-shift keying and chirp spread spectrum. This work advances scalable and efficient physical-layer design for next-generation LPWANs."}
{"id": "2511.20642", "categories": ["cs.IT", "math.CO", "math.FA", "math.MG"], "pdf": "https://arxiv.org/pdf/2511.20642", "abs": "https://arxiv.org/abs/2511.20642", "authors": ["Joseph W. Iverson", "Kaysie Rose O"], "title": "Dimension-counting bounds for equi-isoclinic subspaces", "comment": null, "summary": "We make four contributions to the theory of optimal subspace packings and equi-isoclinic subspaces: (1) a new lower bound for block coherence, (2) an exact count of equi-isoclinic subspaces of even dimension $r$ in $\\mathbb{R}^{2r+1}$ with parameter $α\\neq \\tfrac{1}{2}$, (3) a new upper bound for the number of $r$-dimensional equi-isoclinic subspaces in $\\mathbb{R}^d$ or $\\mathbb{C}^d$, and (4) a proof that when $d=2r$, a further refinement of this bound is attained for every $r$ in the complex case and every $r=2^k$ in the real case. For each of these contributions, the proof ultimately relies on a dimension count."}
