<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]
- [cs.IT](#cs.IT) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor Benchmark](https://arxiv.org/abs/2508.19365)
*Emaan Hariri,Daniel E. Ho*

Main category: cs.IR

TL;DR: 本文介绍了LaborBench基准数据集，用于评估AI在法律代码简化方面的能力，发现虽然LLM在初步研究中有所帮助，但整体准确性远低于作为端到端监管简化管道的承诺。


<details>
  <summary>Details</summary>
Motivation: 随着AI在法律代码简化领域的应用日益增多，但缺乏对其准确性、可靠性和风险的系统评估，需要建立基准来评估AI在此领域的能力。

Method: 利用美国劳工部每年更新的失业保险法律差异数据创建LaborBench问答基准数据集，并构建StateCodes州法规语料库，评估检索增强生成方法和最先进LLM的性能。

Result: 研究显示，虽然这些模型在代码简化初步研究中有所帮助，但整体准确性远低于LLM作为端到端监管简化管道的宣传承诺。

Conclusion: AI在法律代码简化方面仍有很大改进空间，当前模型的准确性不足以作为端到端的监管简化解决方案，需要更系统的研究和改进。

Abstract: One of the emerging use cases of AI in law is for code simplification:
streamlining, distilling, and simplifying complex statutory or regulatory
language. One U.S. state has claimed to eliminate one third of its state code
using AI. Yet we lack systematic evaluations of the accuracy, reliability, and
risks of such approaches. We introduce LaborBench, a question-and-answer
benchmark dataset designed to evaluate AI capabilities in this domain. We
leverage a unique data source to create LaborBench: a dataset updated annually
by teams of lawyers at the U.S. Department of Labor, who compile differences in
unemployment insurance laws across 50 states for over 101 dimensions in a
six-month process, culminating in a 200-page publication of tables. Inspired by
our collaboration with one U.S. state to explore using large language models
(LLMs) to simplify codes in this domain, where complexity is particularly
acute, we transform the DOL publication into LaborBench. This provides a unique
benchmark for AI capacity to conduct, distill, and extract realistic statutory
and regulatory information. To assess the performance of retrieval augmented
generation (RAG) approaches, we also compile StateCodes, a novel and
comprehensive state statute and regulatory corpus of 8.7 GB, enabling much more
systematic research into state codes. We then benchmark the performance of
information retrieval and state-of-the-art large LLMs on this data and show
that while these models are helpful as preliminary research for code
simplification, the overall accuracy is far below the touted promises for LLMs
as end-to-end pipelines for regulatory simplification.

</details>


### [2] [APS Explorer: Navigating Algorithm Performance Spaces for Informed Dataset Selection](https://arxiv.org/abs/2508.19399)
*Tobias Vente,Michael Heep,Abdullah Abbas,Theodor Sperle,Joeran Beel,Bart Goethals*

Main category: cs.IR

TL;DR: 论文分析了推荐系统研究中数据集选择的重要性，发现大多数论文缺乏数据集选择的合理性说明，并提出了一个交互式可视化工具APS Explorer来帮助研究人员进行数据驱动的数据集选择。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统研究中86%的论文没有提供数据集选择的合理性说明，大多数研究仅依赖四个常用数据集，而现有的算法性能空间(APS)方法由于缺乏直观的交互工具而应用有限。

Method: 开发了APS Explorer，一个基于网页的可视化工具，提供三个交互功能：交互式PCA图显示数据集相似性、动态元特征表进行数据集比较、以及专门的成对算法性能可视化。

Result: 提出了一个实用的工具来解决数据集选择问题，使研究人员能够通过可视化方式探索不同数据集的性能特征和相似性。

Conclusion: APS Explorer工具为推荐系统研究提供了数据驱动的数据集选择方法，有助于提高实验的可靠性和结果的可复现性。

Abstract: Dataset selection is crucial for offline recommender system experiments, as
mismatched data (e.g., sparse interaction scenarios require datasets with low
user-item density) can lead to unreliable results. Yet, 86\% of ACM RecSys 2024
papers provide no justification for their dataset choices, with most relying on
just four datasets: Amazon (38\%), MovieLens (34\%), Yelp (15\%), and Gowalla
(12\%). While Algorithm Performance Spaces (APS) were proposed to guide dataset
selection, their adoption has been limited due to the absence of an intuitive,
interactive tool for APS exploration. Therefore, we introduce the APS Explorer,
a web-based visualization tool for interactive APS exploration, enabling
data-driven dataset selection. The APS Explorer provides three interactive
features: (1) an interactive PCA plot showing dataset similarity via
performance patterns, (2) a dynamic meta-feature table for dataset comparisons,
and (3) a specialized visualization for pairwise algorithm performance.

</details>


### [3] [A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation](https://arxiv.org/abs/2508.19507)
*Kyungho Kim,Sunwoo Kim,Geon Lee,Kijung Shin*

Main category: cs.IR

TL;DR: 提出MEMBER推荐系统，使用专家混合框架分别处理已访问和未访问商品，通过自监督学习解决多行为推荐中两类商品推荐质量差异大的问题


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐系统在已访问商品和未访问商品的推荐质量上存在显著差距，且单一模型架构难以同时在这两类商品上取得良好性能

Method: 采用混合专家框架，设计专门针对已访问商品和未访问商品的专家模型，每个专家使用专门的自监督方法进行训练

Result: 在综合实验中，MEMBER在两类商品上都表现出有效性，在Hit Ratio@20指标上相比最佳竞争对手获得高达65.46%的性能提升

Conclusion: MEMBER系统通过专门的专家设计和自监督训练方法，有效解决了多行为推荐中已访问和未访问商品推荐性能不平衡的问题

Abstract: In e-commerce, where users face a vast array of possible item choices,
recommender systems are vital for helping them discover suitable items they
might otherwise overlook. While many recommender systems primarily rely on a
user's purchase history, recent multi-behavior recommender systems incorporate
various auxiliary user behaviors, such as item clicks and cart additions, to
enhance recommendations. Despite their overall performance gains, their
effectiveness varies considerably between visited items (i.e., those a user has
interacted with through auxiliary behaviors) and unvisited items (i.e., those
with which the user has had no such interactions). Specifically, our analysis
reveals that (1) existing multi-behavior recommender systems exhibit a
significant gap in recommendation quality between the two item types (visited
and unvisited items) and (2) achieving strong performance on both types with a
single model architecture remains challenging. To tackle these issues, we
propose a novel multi-behavior recommender system, MEMBER. It employs a
mixture-of-experts framework, with experts designed to recommend the two item
types, respectively. Each expert is trained using a self-supervised method
specialized for its design goal. In our comprehensive experiments, we show the
effectiveness of MEMBER across both item types, achieving up to 65.46\%
performance gain over the best competitor in terms of Hit Ratio@20.

</details>


### [4] [A Hybrid Recommendation Framework for Enhancing User Engagement in Local News](https://arxiv.org/abs/2508.19539)
*Payam Pourashraf,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 提出了一种结合本地和全局偏好模型的混合新闻推荐系统，通过集成区域特定内容和广泛偏好来提升本地新闻的用户参与度


<details>
  <summary>Details</summary>
Motivation: 本地新闻机构面临读者参与度下降的挑战，传统推荐系统往往忽视本地新闻的细微和多样化兴趣，需要更精准的个性化推荐方案

Method: 开发混合推荐框架，集成本地模型（专注于区域特定内容）和全局模型（捕捉广泛偏好），采用集成策略和多阶段训练来平衡两者

Result: 在两个数据集上的评估显示，该集成方法在准确性和覆盖率上优于单模型基线，表明能够改善个性化推荐效果

Conclusion: 该研究为推荐系统提供了新方向，通过桥接本地和全局模型，为本地新闻消费提供可扩展的个性化用户体验，具有提升用户留存和订阅的实践价值

Abstract: Local news organizations face an urgent need to boost reader engagement amid
declining circulation and competition from global media. Personalized news
recommender systems offer a promising solution by tailoring content to user
interests. Yet, conventional approaches often emphasize general preferences and
may overlook nuanced or eclectic interests in local news.
  We propose a hybrid news recommender that integrates local and global
preference models to improve engagement. Building on evidence of the value of
localized models, our method unifies local and non-local predictors in one
framework. The system adaptively combines recommendations from a local model,
specialized in region-specific content, and a global model that captures
broader preferences. Ensemble strategies and multiphase training balance the
two.
  We evaluated the model on two datasets: a synthetic set based on Syracuse
newspaper distributions and a Danish dataset (EB-NeRD) labeled for local and
non-local content with an LLM. Results show our integrated approach outperforms
single-model baselines in accuracy and coverage, suggesting improved
personalization that can drive user engagement.
  The findings have practical implications for publishers, especially local
outlets. By leveraging both community-specific and general user interests, the
hybrid recommender can deliver more relevant content, increasing retention and
subscriptions. In sum, this work introduces a new direction for recommender
systems, bridging local and global models to revitalize local news consumption
through scalable, personalized user experiences.

</details>


### [5] [Improving Recommendation Fairness via Graph Structure and Representation Augmentation](https://arxiv.org/abs/2508.19547)
*Tongxin Xu,Wenqiang Liu,Chenzhong Bin,Cihan Xiao,Zhixin Zeng,Tianlong Gu*

Main category: cs.IR

TL;DR: 提出一种基于数据增强的公平推荐框架，通过双重数据增强策略生成公平的图结构和特征表示，在保持推荐效用的同时提升公平性


<details>
  <summary>Details</summary>
Motivation: 现有GCN推荐模型会放大数据偏见，导致敏感信息传播，而现有公平性方法要么忽视数据偏见对表示学习的影响，要么通过数据增强破坏用户偏好降低效用

Method: 提出两个先验假设识别敏感交互和特征，设计双重数据增强框架生成公平增强图和特征表示，并引入去偏学习方法最小化表示与敏感信息的依赖

Result: 在两个真实数据集上的大量实验证明了所提框架的优越性

Conclusion: 该框架能够有效提升推荐公平性同时保持推荐性能，为解决GCN推荐中的公平性问题提供了有效方案

Abstract: Graph Convolutional Networks (GCNs) have become increasingly popular in
recommendation systems. However, recent studies have shown that GCN-based
models will cause sensitive information to disseminate widely in the graph
structure, amplifying data bias and raising fairness concerns. While various
fairness methods have been proposed, most of them neglect the impact of biased
data on representation learning, which results in limited fairness improvement.
Moreover, some studies have focused on constructing fair and balanced data
distributions through data augmentation, but these methods significantly reduce
utility due to disruption of user preferences. In this paper, we aim to design
a fair recommendation method from the perspective of data augmentation to
improve fairness while preserving recommendation utility. To achieve
fairness-aware data augmentation with minimal disruption to user preferences,
we propose two prior hypotheses. The first hypothesis identifies sensitive
interactions by comparing outcomes of performance-oriented and fairness-aware
recommendations, while the second one focuses on detecting sensitive features
by analyzing feature similarities between biased and debiased representations.
Then, we propose a dual data augmentation framework for fair recommendation,
which includes two data augmentation strategies to generate fair augmented
graphs and feature representations. Furthermore, we introduce a debiasing
learning method that minimizes the dependence between the learned
representations and sensitive information to eliminate bias. Extensive
experiments on two real-world datasets demonstrate the superiority of our
proposed framework.

</details>


### [6] [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)
*Jiakui Shen,Yunqi Mi,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: PLGC是一种新颖的联邦推荐系统个性化训练策略，通过本地-全局协作和对比学习解决嵌入退化问题，提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 集中式推荐系统存在隐私泄露问题，联邦推荐系统虽然保护隐私但面临嵌入退化、维度坍塌和个性化不足等挑战

Method: 提出PLGC策略：1) 将全局项目嵌入表整合到本地设备；2) 使用神经正切核策略动态平衡本地和全局信息；3) 采用对比目标函数减少嵌入冗余

Result: 在五个真实数据集上的广泛实验表明，PLGC优于各种基线算法，证明了其有效性和适应性

Conclusion: PLGC是首个解决联邦推荐中维度坍塌问题的研究，可作为模型无关的个性化训练策略应用于现有基线，有效缓解嵌入退化问题

Abstract: Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

</details>


### [7] [A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2508.19620)
*Yunqi Mi,Jiakui Shen,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 这篇论文是一个关于联邦推荐系统(FedRec)的综述性研究，从推荐系统视角出发分析了联邦学习与推荐场景的结合，指出了现有研究忽视实际推荐场景特性的问题，并为实际部署提供指南。


<details>
  <summary>Details</summary>
Motivation: 保护用户或平台的隐私是联邦推荐系统的核心动机。传统中央化推荐系统需要收集客户数据，而联邦推荐系统可以避免这一步骤，通过将模型训练分散到各客户端来保护隐私。现有研究太过关注FL系统设计而忽视了具体推荐场景的特殊性和实际挑战。

Method: 从推荐系统研究者和实践者的视角出发，建立推荐场景与联邦学习框架之间的清晰联系，系统分析不同场景下的特定方法、实践挑战和潜在机遇。

Result: 该综述性研究展示了联邦推荐系统在不同推荐场景下的应用方法和面临的挑战，比如跨域联邦推荐中的统计异质性问题。这些问题主要来自于推荐系统本身，而非联邦架构。

Conclusion: 该研究为联邦推荐系统的实际部署提供了指南，帮助缩小现有研究与应用之间的差距。通过重点解决真实推荐场景中的具体问题，可以促进联邦推荐系统的更广泛部署和应用。

Abstract: Extending recommender systems to federated learning (FL) frameworks to
protect the privacy of users or platforms while making recommendations has
recently gained widespread attention in academia. This is due to the natural
coupling of recommender systems and federated learning architectures: the data
originates from distributed clients (mostly mobile devices held by users),
which are highly related to privacy. In a centralized recommender system
(CenRec), the central server collects clients' data, trains the model, and
provides the service. Whereas in federated recommender systems (FedRec), the
step of data collecting is omitted, and the step of model training is offloaded
to each client. The server only aggregates the model and other knowledge, thus
avoiding client privacy leakage. Some surveys of federated recommender systems
discuss and analyze related work from the perspective of designing FL systems.
However, their utility drops by ignoring specific recommendation scenarios'
unique characteristics and practical challenges. For example, the statistical
heterogeneity issue in cross-domain FedRec originates from the label drift of
the data held by different platforms, which is mainly caused by the recommender
itself, but not the federated architecture. Therefore, it should focus more on
solving specific problems in real-world recommendation scenarios to encourage
the deployment FedRec. To this end, this review comprehensively analyzes the
coupling of recommender systems and federated learning from the perspective of
recommendation researchers and practitioners. We establish a clear link between
recommendation scenarios and FL frameworks, systematically analyzing
scenario-specific approaches, practical challenges, and potential
opportunities. We aim to develop guidance for the real-world deployment of
FedRec, bridging the gap between existing research and applications.

</details>


### [8] [Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning](https://arxiv.org/abs/2508.19855)
*Junnan Dong,Siyu An,Yifei Yu,Qian-Wen Zhang,Linhao Luo,Xiao Huang,Yunsheng Wu,Di Yin,Xing Sun*

Main category: cs.IR

TL;DR: Youtu-GraphRAG是一个垂直统一的智能代理范式，通过种子图模式、双重感知社区检测和智能检索器，显著提升了图检索增强生成在复杂推理任务中的性能，在六个基准测试中实现了90.71%的token成本节省和16.62%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG方法单独改进图构建或图检索，导致在领域迁移时性能不佳。需要一种统一的框架来连接整个系统，提高在复杂推理任务中的鲁棒性和适应性。

Method: 提出垂直统一的智能代理范式：1）引入种子图模式来约束自动提取代理；2）开发双重感知社区检测方法，融合结构拓扑和子图语义；3）设计智能检索器解释图模式并转换复杂查询；4）创建匿名数据集和匿名反转任务来评估真实性能。

Result: 在六个具有挑战性的基准测试中，Youtu-GraphRAG显著推进了帕累托前沿，实现了高达90.71%的token成本节省和16.62%的准确率提升，展现了出色的领域迁移能力。

Conclusion: Youtu-GraphRAG通过统一的智能代理框架有效解决了GraphRAG中的领域迁移问题，在保持高性能的同时大幅降低了计算成本，为复杂知识推理任务提供了有效的解决方案。

Abstract: Graph retrieval-augmented generation (GraphRAG) has effectively enhanced
large language models in complex reasoning by organizing fragmented knowledge
into explicitly structured graphs. Prior efforts have been made to improve
either graph construction or graph retrieval in isolation, yielding suboptimal
performance, especially when domain shifts occur. In this paper, we propose a
vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the
entire framework as an intricate integration. Specifically, (i) a seed graph
schema is introduced to bound the automatic extraction agent with targeted
entity types, relations and attribute types, also continuously expanded for
scalability over unseen domains; (ii) To obtain higher-level knowledge upon the
schema, we develop novel dually-perceived community detection, fusing
structural topology with subgraph semantics for comprehensive knowledge
organization. This naturally yields a hierarchical knowledge tree that supports
both top-down filtering and bottom-up reasoning with community summaries; (iii)
An agentic retriever is designed to interpret the same graph schema to
transform complex queries into tractable and parallel sub-queries. It
iteratively performs reflection for more advanced reasoning; (iv) To alleviate
the knowledge leaking problem in pre-trained LLM, we propose a tailored
anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures
the real performance of the GraphRAG frameworks. Extensive experiments across
six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,
remarkably moving the Pareto frontier with up to 90.71% saving of token costs
and 16.62% higher accuracy over state-of-the-art baselines. The results
indicate our adaptability, allowing seamless domain transfer with minimal
intervention on schema.

</details>


### [9] [Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization](https://arxiv.org/abs/2508.19918)
*Manato Tajiri,Michimasa Inaba*

Main category: cs.IR

TL;DR: 本文提出使用大型语言模型生成对话摘要和项目推荐信息，通过直接偏好优化方法提升对话推荐系统的自然性和真实性


<details>
  <summary>Details</summary>
Motivation: 当前对话推荐系统往往通过简短会话快速推荐项目，偏离了真实的人类交互模式，需要更自然的对话推荐过程

Method: 利用大型语言模型从对话历史生成对话摘要，从项目描述生成项目推荐信息，并使用直接偏好优化(DPO)确保生成内容包含有效推荐所需的关键信息

Result: 在两个公开数据集上的实验验证了该方法在促进更自然和现实的对话推荐过程方面的有效性

Conclusion: 该方法能够从对话上下文中提取显式用户陈述和隐式偏好，显著改善了对话推荐系统的交互质量

Abstract: Conversational Recommender Systems (CRSs) aim to elicit user preferences via
natural dialogue to provide suitable item recommendations. However, current
CRSs often deviate from realistic human interactions by rapidly recommending
items in brief sessions. This work addresses this gap by leveraging Large
Language Models (LLMs) to generate dialogue summaries from dialogue history and
item recommendation information from item description. This approach enables
the extraction of both explicit user statements and implicit preferences
inferred from the dialogue context. We introduce a method using Direct
Preference Optimization (DPO) to ensure dialogue summary and item
recommendation information are rich in information crucial for effective
recommendations. Experiments on two public datasets validate our method's
effectiveness in fostering more natural and realistic conversational
recommendation processes.Our implementation is publicly available
at:https://github.com/UEC-InabaLab/Refining-LLM-Text

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [10] [Efficient Probabilistic Parity Shaping for Irregular Repeat-Accumulate LDPC Codes](https://arxiv.org/abs/2508.19696)
*Diego Lentner,Thomas Wiegart,Richard D. Wesel*

Main category: cs.IT

TL;DR: 提出了高效塑造系统不规则重复累积(IRA)LDPC码奇偶校验位的算法，在AWGN信道中相比均匀信号获得最高0.9dB增益


<details>
  <summary>Details</summary>
Motivation: 为了提高系统IRA LDPC码在AWGN信道中的性能，通过优化奇偶校验位的形状来获得更好的编码增益

Method: 遵循累加器的顺序编码顺序，提出高效算法来塑造IRA LDPC码的奇偶校验位

Result: 在采用开关键控的AWGN信道仿真中，相比均匀信号获得了最高0.9dB的增益

Conclusion: 所提出的算法能有效改善IRA LDPC码的性能，在AWGN信道中实现显著的编码增益

Abstract: Algorithms are presented that efficiently shape the parity bits of systematic
irregular repeat-accumulate (IRA) low-density parity-check (LDPC) codes by
following the sequential encoding order of the accumulator. Simulations over
additive white Gaussian noise (AWGN) channels with on-off keying show a gain of
up to 0.9 dB over uniform signaling.

</details>


### [11] [Design and Analysis of the Tail Sequence for Short LDPC-Coded Space Communications](https://arxiv.org/abs/2508.19858)
*Massimo Battaglioni,Kenneth Andrews,Rebecca Giuliani,Fabrizio Marinelli,Franco Chiaraluce,Marco Baldi*

Main category: cs.IT

TL;DR: 该论文研究了卫星通信中短LDPC码的尾序列设计问题，提出了改进的尾序列设计方法，显著提高了TC拒绝概率，并在中等SNR下实现了与复杂检测方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在卫星通信中，短LDPC码的解码可能会在没有尾序列的情况下错误收敛，需要设计有效的尾序列来确保解码失败，从而准确检测传输单元终止。

Method: 研究尾序列所需特性，提出多种尾序列设计方法，并通过数值模拟比较不同检测方法（基于解码器的检测和似然比检验检测）的性能。

Result: 使用提出的尾序列设计后，TC拒绝概率显著提高；在中等SNR条件下，基于解码器的检测方法可以达到与更复杂的似然比检验检测相同的性能，但复杂度更低。

Conclusion: 合理设计的尾序列可以有效解决短LDPC码的错误收敛问题，基于解码器的检测方法在中等SNR下是复杂度与性能的最佳平衡方案。

Abstract: According to some standards for satellite communications, the transmitted
stream is divided into transmission units with variable length, for which
detecting the termination is particularly relevant. This is the case of space
TeleCommands (TCs), where coded data are usually preceded by a start sequence,
and optionally followed by a tail sequence, forming the Communication Link
Transmission Unit (CLTU). Regarding the choice of schemes for error correction,
the Consultative Committee for Space Data Systems recommendations for TC
synchronization and coding suggests to use, among others, two Low-Density
Parity-Check (LDPC) codes: one (relatively) long and one short. Adopting the
long LDPC code eliminates the need for a tail sequence, as the LDPC decoder
always fails when overrunning the end of the CLTU, thus causing the decoding
and detection process to stop. This, however, is not true when the short LDPC
code is adopted, since its decoding might converge on a codeword even when the
decoder input is not a noisy codeword. This makes it necessary to use a tail
sequence that causes the decoder to fail regardless of its input. In this
paper, we study the features required for such a sequence and propose some
methods for its design. Our numerical results, obtained considering various
detection approaches for the tail sequence, show that the overall TC rejection
probability improves significantly when the proposed tail sequence is employed.
Our simulations also show that, for moderate values of the Signal-to-Noise
Ratio (SNR), with a properly designed tail sequence it is possible to obtain
the same performance in terms of TC rejection probability using decoder-based
detection and likelihood ratio test-based detection, with the former approach
being less complex than the latter.

</details>


### [12] [Renyi partial orders for BISO channels](https://arxiv.org/abs/2508.19951)
*Christoph Hirche*

Main category: cs.IT

TL;DR: 该论文扩展了BISO信道在Renyi互信息偏序下的极值性分析，证明了BSC和BEC在广义Renyi容量方面具有极值特性，并引入了α-Lorenz曲线工具。


<details>
  <summary>Details</summary>
Motivation: 信息论中的一个基本问题是量化噪声信道下的信息损失。虽然偏序是常用的分析工具，但评估往往具有挑战性。对于BISO信道，前人已证明BSC和BEC在相同容量下对更容量偏序具有极值性，本文旨在将此结果扩展到基于Renyi互信息的偏序框架。

Method: 通过引入α-Lorenz曲线作为分析工具，扩展了现有方法以处理Renyi互信息框架。研究了BISO信道在广义Renyi容量偏序下的性质，特别关注BSC和BEC的极值特性。

Result: 证明了在广义Renyi容量方面，二进制对称信道(BSC)和二进制擦除信道(BEC)在BISO信道类中具有极值性。这一结果扩展了先前关于更容量偏序的结论。

Conclusion: 本文成功将BSC和BEC的极值性结果从传统容量偏序推广到基于Renyi互信息的广义框架，为信息论中的信道比较提供了新的分析工具和理论支撑。

Abstract: A fundamental question in information theory is to quantify the loss of
information under a noisy channel. Partial orders are typical tools to that
end, however, they are often also challenging to evaluate. For the special
class of binary input symmetric output (BISO) channels, Geng et al. showed that
among channels with the same capacity, the binary symmetric channel (BSC) and
binary erasure channel (BEC) are extremal with respect to the more capable
order. Here we extend on this result by considering partial orders based on
Renyi mutual information. We establish the extremality of the BSC and BEC in
this setting with respect to the generalized Renyi capacity. In the process, we
also generalize the needed tools and introduce $\alpha$-Lorenz curves.

</details>


### [13] [On the Outage Probability of Multiuser Multiple Antenna Systems with Non-Orthogonal Multiple Access for Air-Ground Communications](https://arxiv.org/abs/2508.20003)
*Ayten Gürbüz,Giuseppe Caire,Alexander Steingass*

Main category: cs.IT

TL;DR: 本文研究多用户多天线系统在航空通信中的应用，通过三种解码方法分析中断概率，发现联合组解码能显著提高频谱效率


<details>
  <summary>Details</summary>
Motivation: 探索多用户多天线系统如何提高航空通信系统的频谱效率，特别是在航空地空信道环境下

Method: 采用基于几何的随机航空地空信道模型，研究三种解码方法：连续干扰消除(SIC)、联合组解码、以及限制组大小的联合组解码

Result: 结果显示联合组解码（即使是两用户组）能显著提高频谱效率，允许多个飞机在非正交信道上以极低中断概率传输

Conclusion: 在航空地空信道中，联合组解码是提高多用户多天线系统性能的有效方法，即使采用小规模的组解码也能获得显著的频谱效率提升

Abstract: This paper explores multiuser multiple antenna systems as a means to enhance
the spectral efficiency of aeronautical communications systems. To this end,
the outage regime for a multiuser multiple antenna system is studied within a
realistic geometry-based stochastic air-ground (AG) channel model. In this
application, users (aircraft) transmit air traffic management data to the
ground station at a predefined target rate. Due to the nature of the AG
propagation, we argue that the relevant performance metric in this context is
the information outage probability. We consider the outage probability under
three decoding approaches. The first is based on successive interference
cancellation (SIC). The second extends the first approach by considering joint
group decoding. The third is a version of the second that limits the size of
the jointly decoded user groups in order to lower the decoding complexity. The
results show that joint group decoding, even in groups of only two, can
significantly increase the spectral efficiency in the AG channel by allowing a
large number of aircraft to transmit over a non-orthogonal channel with very
low outage probabilities.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [14] [Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)](https://arxiv.org/abs/2508.19371)
*Semih Kara,Tamer Başar*

Main category: cs.GT

TL;DR: 提出聚合虚拟博弈(agg-FP)算法，通过聚合智能体动作来降低动作空间维度，在匿名多矩阵游戏中保持纳什均衡收敛性并加速收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟博弈在智能体数量增加时面临联合动作空间指数级增长的挑战，匿名游戏结构为解决这一问题提供了可能。

Method: 在匿名多矩阵游戏中，每个智能体跟踪其他智能体选择每个动作的数量频率，而不是个体动作，从而减少动作空间维度。

Result: 理论证明agg-FP在匿名多矩阵游戏中与传统FP具有相同的纳什均衡收敛条件，仿真实验显示聚合方法显著加速收敛。

Conclusion: 通过动作聚合可以在不损失收敛保证的前提下有效降低计算复杂度，为大规模多智能体系统提供可行的学习算法。

Abstract: Fictitious play (FP) is a well-studied algorithm that enables agents to learn
Nash equilibrium in games with certain reward structures. However, when agents
have no prior knowledge of the reward functions, FP faces a major challenge:
the joint action space grows exponentially with the number of agents, which
slows down reward exploration. Anonymous games offer a structure that mitigates
this issue. In these games, the rewards depend only on the actions taken; not
on who is taking which action. Under such a structure, we introduce aggregate
fictitious play (agg-FP), a variant of FP where each agent tracks the frequency
of the number of other agents playing each action, rather than these agents'
individual actions. We show that in anonymous polymatrix games, agg-FP
converges to a Nash equilibrium under the same conditions as classical FP. In
essence, by aggregating the agents' actions, we reduce the action space without
losing the convergence guarantees. Using simulations, we provide empirical
evidence on how this reduction accelerates convergence.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter](https://arxiv.org/abs/2508.19639)
*Junxi Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.MM

TL;DR: FakeSV-VLM是一个基于视觉语言模型的短视频假新闻检测框架，通过混合专家机制和模态对齐检查，在两个基准数据集上显著优于现有最佳模型3.32%和5.02%。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法因缺乏验证知识而准确率不足，而大型视觉语言模型从海量多模态数据中吸收了丰富的真实世界知识，适合用于短视频假新闻检测。

Method: 设计四类专家处理不同真假组合场景，通过渐进式混合专家适配器(PMOE)进行初步分析和全面诊断，并开发对齐驱动事件检查(ADEC)模块捕捉模态间不一致性。

Result: 在FakeSV和FakeTT两个基准数据集上的实验表明，该方法比当前最先进模型分别提升了3.32%和5.02%的检测准确率。

Conclusion: FakeSV-VLM通过有效利用视觉语言模型的知识和模态一致性分析，为短视频假新闻检测设立了新的性能基准。

Abstract: We present FakeSV-VLM in this paper, a new VLM-based framework for detecting
fake news on short video platforms. Despite significant efforts to combat this
issue due to the severe threat that fake news videos pose to public information
security, existing methods still fall short in detection accuracy, often due to
lack of knowledge to verify the news is real or not. However, large Vision
Language Models (VLMs) have absorbed extensive real-world knowledge from
massive multimodal datasets. Motivated by this, we adapt advanced VLMs for fake
news detection in short videos. Upon close examination of news samples, we
observe that short video samples can be categorized into four distinct
scenarios: both video and text are real (for real samples), or both are fake,
or either the video or text is fake (for fake samples). Inspired by this
insight, we design four experts tailored to handle each scenario and integrate
them into VLM via Mixture of Experts. Specifically, we develop the Progressive
MoE Adapter (PMOE) module where detection experts first provide an initial
analysis, followed by attribution experts for a comprehensive diagnosis,
leading to a robust decision. Additionally, we also note the fake news videos
often show inconsistency between two modalities. Consequently, we further
design the Alignment-driven Event Checking (ADEC) module, which perceives the
fake news by capturing the inconsistency between different modalities.
Extensive experiments on two benchmark datasets, FakeSV and FakeTT, verify the
superiority of our model. It significantly outperforms current state-of-the-art
models by +3.32% and +5.02%, establishing a new benchmark in the field.

</details>


### [16] [ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation](https://arxiv.org/abs/2508.20057)
*Haoshuo Zhang,Yufei Bo,Meixia Tao*

Main category: cs.MM

TL;DR: ProMSC-MIS是一个基于提示学习的多模态语义通信框架，用于多光谱图像分割，通过跨模态特征融合和对比学习，在保持分割性能的同时显著降低带宽需求和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 多模态语义通信通过整合不同模态的互补信息来提升下游任务性能，特别是在带宽受限的信道中传输空间对齐的RGB和热成像图像时，需要高效的面向任务的传输方案。

Method: 1) 利用提示学习和对比学习预训练单模态语义编码器，使用一个模态的特征作为另一个模态的提示；2) 设计结合交叉注意力机制和SE网络的语义融合模块来有效融合跨模态特征。

Result: 实验表明ProMSC-MIS显著优于传统图像传输结合最先进分割方法，在相同分割性能下减少50%-70%的带宽需求，存储开销和计算复杂度分别降低26%和37%。

Conclusion: 该框架特别适用于自动驾驶和夜间监控等应用，消融研究验证了预训练和语义融合策略的有效性。

Abstract: Multimodal semantic communication has great potential to enhance downstream
task performance by integrating complementary information across modalities.
This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic
Communication framework for Multi-Spectral Image Segmentation. It enables
efficient task-oriented transmission of spatially aligned RGB and thermal
images over band-limited channels. Our framework has two main design novelties.
First, by leveraging prompt learning and contrastive learning, unimodal
semantic encoders are pre-trained to learn diverse and complementary semantic
representations by using features from one modality as prompts for another.
Second, a semantic fusion module that combines cross-attention mechanism and
squeeze-and-excitation (SE) networks is designed to effectively fuse
cross-modal features. Experimental results demonstrate that ProMSC-MIS
substantially outperforms conventional image transmission combined with
state-of-the-art segmentation methods. Notably, it reduces the required channel
bandwidth by 50%--70% at the same segmentation performance, while also
decreasing the storage overhead and computational complexity by 26% and 37%,
respectively. Ablation studies also validate the effectiveness of the proposed
pre-training and semantic fusion strategies. Our scheme is highly suitable for
applications such as autonomous driving and nighttime surveillance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [17] [Efficiently Coloring the Intersection of a General Matroid and Partition Matroids](https://arxiv.org/abs/2508.19473)
*Stephen Arndt,Benjamin Moseley,Kirk Pruhs,Michael Zlatin*

Main category: cs.DS

TL;DR: 这篇论文提出了一个多项式时间算法，用亊限的颜色数对向量秩交集进行着色，其中一个向量秩可以是一般向量秩，其他为分区向量秩。这是首个多项式时间的O(1)-近似算法。


<details>
  <summary>Details</summary>
Motivation: 向量秩交集着色问题在组合优化中具有重要意义，但现有算法多为NP难问题。需要开发多项式时间的近似算法来解决这个问题，特别是在包含一般向量秩的情况下。

Method: 设计了一个多项式时间算法，输入为一个一般向量秩M1和k-1个分区向量秩M2,...,Mk，输出为交集M=∩Mi的着色方案，使用的颜色数至多为1+Σ(χ(Mi)-1)。

Result: 该算法是首个多项式时间的O(1)-近似算法，用亊限的颜色数对向量秩交集进行着色，其中一个向量秩可以是一般向量秩。

Conclusion: 该算法为向量秩交集着色问题提供了第一个多项式时间的O(1)-近似解，并且通过标准组合向量秩向分区向量秩的约化，还可以扩展到更广泛的情况。

Abstract: This paper shows a polynomial-time algorithm, that given a general matroid
$M_1 = (X, \mathcal{I}_1)$ and $k-1$ partition matroids $ M_2, \ldots, M_k$,
produces a coloring of the intersection $M = \cap_{i=1}^k M_i$ using at most
$1+\sum_{i=1}^k \left(\chi(M_i) -1\right)$ colors. This is the first
polynomial-time $O(1)$-approximation algorithm for matroid intersection
coloring where one of the matroids may be a general matroid. Leveraging the
fact that all of the standard combinatorial matroids reduce to partition
matroids at a loss of a factor of two in the chromatic number, this algorithm
also yields a polynomial-time $O(1)$-approximation algorithm for matroid
intersection coloring in the case where each of the matroids $ M_2, \ldots,
M_k$ are one of the standard combinatorial types.

</details>


### [18] [An Optimal Sorting Algorithm for Persistent Random Comparison Faults](https://arxiv.org/abs/2508.19785)
*Barbara Geissmann,Stefano Leucci,Chih-Hung Liu,Paolo Penna*

Main category: cs.DS

TL;DR: 提出了第一个在持久性比较错误模型下时间复杂度为O(n log n)的排序算法，保证最大错位O(log n)和总错位O(n)，当错误概率p<1/4时以高概率成立


<details>
  <summary>Details</summary>
Motivation: 解决在持久性随机比较错误模型下的排序问题，其中每次比较都有固定概率出错，且相同元素对的比较结果始终一致。完美排序在此模型下不可能，目标是最小化元素在输出序列中的错位

Method: 开发了时间复杂度O(n log n)的排序算法，通过解决两个相关子问题：在几乎排序序列中O(log n)时间内找到新元素的插入位置，以及将近似排序序列的最大错位降低到O(log n)

Result: 算法在p<1/4时以高概率保证最大错位O(log n)和总错位O(n)，证明了这是最优的，因为无错误情况下也需要Ω(n log n)时间达到O(log n)最大错位

Conclusion: 该工作解决了持久性比较错误排序问题的时间复杂度，表明比较错误不会增加排序的计算难度，同时证明了算法的最优性

Abstract: We consider the problem of sorting $n$ elements subject to persistent random
comparison errors. In this problem, each comparison between two elements can be
wrong with some fixed (small) probability $p$, and comparing the same pair of
elements multiple times always yields the same result. Sorting perfectly in
this model is impossible, and the objective is to minimize the dislocation of
each element in the output sequence, i.e., the difference between its position
in the sequence and its true rank.
  In this paper, we present the first $O(n\log n)$-time sorting algorithm that
guarantees both $O(\log n)$ maximum dislocation and $O(n)$ total dislocation
with high probability when $p<\frac{1}{4}$. This settles the time complexity
sorting with persistent comparison errors in the given range of $p$ and shows
that comparison errors do not increase its computational difficulty. Indeed,
$\Omega(n\log n)$ time is necessary to archive a maximum dislocation of $O(\log
n)$ even without comparison errors. Moreover, we prove that no algorithm can
guarantee a maximum dislocation of $o(\log n)$ with high probability, nor a
total dislocation of $o(n)$ in expectation.
  To develop our sorting algorithm, we solve two related sub-problems, which
might be of independent interest. More precisely, we show that $O(\log n)$ time
suffices to find a position in which to insert a new element $x$ in an
almost-sorted sequence $S$ of $n$ elements having dislocation at most
$d=\Omega(\log n)$, so that the dislocation of $x$ in the resulting sequence is
$O(d)$ with high probability (which can be equivalently thought as the problem
of estimating the rank of $x$ in $S$). We also show that the maximum (resp.
total) dislocation of an approximately sorted sequence $S$ of $n$ elements can
be lowered to $O(\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is
an upper bound on the maximum dislocation of $S$.

</details>


### [19] [Optimizing Wiggle in Storylines](https://arxiv.org/abs/2508.19802)
*Alexander Dobler,Tim Hegemann,Martin Nöllenburg,Alexander Wolff*

Main category: cs.DS

TL;DR: 本文研究故事线可视化中的摆动最小化问题，证明摆动计数最小化是NP完全问题，并提出基于数学规划的高效算法来解决线性和二次摆动高度最小化，同时引入新的曲线路由方法。


<details>
  <summary>Details</summary>
Motivation: 故事线可视化中除了最小化交叉点外，摆动（角色垂直移动量）最小化是另一个重要但研究较少的质量标准，需要系统研究其计算复杂性和有效算法。

Method: 1) 证明摆动计数最小化是NP完全问题；2) 使用数学规划方法解决线性和二次摆动高度最小化；3) 提出保持相邻曲线距离恒定的新曲线路由方法；4) 在现有基准数据和新铁路运营用例中进行实验验证。

Result: 开发了高效的摆动最小化算法，实现了线性和二次摆动高度的有效优化，新曲线路由方法能更好地保持平行曲线间的距离稳定性。

Conclusion: 摆动最小化是故事线可视化中的重要优化目标，虽然摆动计数最小化是NP难的，但线性和二次摆动高度最小化可以通过数学规划高效解决，新路由方法提升了可视化质量。

Abstract: A storyline visualization shows interactions between characters over time.
Each character is represented by an x-monotone curve. Time is mapped to the
x-axis, and groups of characters that interact at a particular point $t$ in
time must be ordered consecutively in the y-dimension at $x=t$. The predominant
objective in storyline optimization so far has been the minimization of
crossings between (blocks of) characters. Building on this work, we investigate
another important, but less studied quality criterion, namely the minimization
of wiggle, i.e., the amount of vertical movement of the characters over time.
Given a storyline instance together with an ordering of the characters at any
point in time, we show that wiggle count minimization is NP-complete. In
contrast, we provide algorithms based on mathematical programming to solve
linear wiggle height minimization and quadratic wiggle height minimization
efficiently. Finally, we introduce a new method for routing character curves
that focuses on keeping distances between neighboring curves constant as long
as they run in parallel. We have implemented our algorithms, and we conduct a
case study that explores the differences between the three optimization
objectives. We use existing benchmark data, but we also present a new use case
for storylines, namely the visualization of rolling stock schedules in railway
operation.

</details>


### [20] [Distributed Sparsest Cut via Eigenvalue Estimation](https://arxiv.org/abs/2508.19898)
*Yannic Maus,Tijn de Vos*

Main category: cs.DS

TL;DR: 这篇论文提出了在CONGEST模型中近似计算图的最稀疏切值（导电率）的新算法，运行时间为O(log²n/φ)，达到了√2.01φ的近似比


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，高效计算图的最稀疏切值是一个重要问题，但现有算法要么运行时间过长，要么实现复杂，需要更简单高效的解决方案

Method: 使用标准的幂法法，通过重复将规一化拉普拉斯矩阵与向量相乘，这在CONGEST模型中可以在一轮内完成

Result: 算法在O(log²n/φ)轮中运行，输出值φ̃满足φ ≤ φ̃ ≤ √2.01φ，在大多数情况下显著改善了之前的最快算法

Conclusion: 该算法相对简单易实现，通用于加权无向图，并能够扩展到k路导电率问题，为分布式图计算提供了更高效的解决方案

Abstract: We give new, improved bounds for approximating the sparsest cut value or in
other words the conductance $\phi$ of a graph in the CONGEST model. As our main
result, we present an algorithm running in $O(\log^2 n/\phi)$ rounds in which
every vertex outputs a value $\tilde \phi$ satisfying $\phi \le \tilde \phi \le
\sqrt{2.01\phi}$. In most regimes, our algorithm improves significantly over
the previously fastest algorithm for the problem [Chen, Meierhans, Probst
Gutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way
conductance.
  We obtain these results, by approximating the eigenvalues of the normalized
Laplacian matrix $L:=I-\rm{Deg}^{-1/2}A\rm{Deg}^ {-1/2}$, where, $A$ is the
adjacency matrix and $\rm{Deg}$ is the diagonal matrix with the weighted
degrees on the diagonal. The previous state of the art sparsest cut algorithm
is in the technical realm of expander decompositions. Our algorithms, on the
other hand, are relatively simple and easy to implement. At the core, they rely
on the well-known power method, which comes down to repeatedly multiplying the
Laplacian with a vector. This operation can be performed in a single round in
the CONGEST model. All our algorithms apply to weighted, undirected graphs. Our
lower bounds apply even in unweighted graphs.

</details>


### [21] [Bipartite Matching with Pair-Dependent Bounds](https://arxiv.org/abs/2508.20002)
*Shaul Rosner,Tami Tamir*

Main category: cs.DS

TL;DR: 研究二分图PD匹配问题，其中机器可匹配的作业数量取决于具体作业对，目标是最大化匹配规模，分析了计算复杂性和提出了算法


<details>
  <summary>Details</summary>
Motivation: 现实系统中不同作业对特定机器的拥塞容忍度不同，需要研究这种配对依赖约束的匹配问题，该问题之前未被研究但具有实际应用价值

Method: 定义二分图PD匹配问题，分析一般情况和特定受限实例的计算复杂性，提出最优算法和近似算法

Result: 该问题与先前研究的匹配问题存在显著差异，展示了硬度结果以及最优和近似算法

Conclusion: PD匹配问题是一个新颖且有实际意义的匹配变体，在计算复杂性方面具有挑战性，需要开发专门的算法来解决

Abstract: Let $G=(U \cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$
represents machines. We study a new variant of the bipartite matching problem
in which each job in $U$ can be matched to at most one machine in $V$, and the
number of jobs that can be assigned to a machine depends on the specific jobs
matched to it. These pair-dependent bounds reflect systems where different jobs
have varying tolerance for congestion, determined by the specific machine they
are assigned to.
  We define a bipartite PD-matching as a set of edges $M \subseteq E$ that
satisfies these job-to-machine tolerance constraints. This variant of matching
extends well-known matching problems, however, despite its relevance to
real-world systems, it has not been studied before. We study bipartite
PD-matchings with the objective of maximizing the matching size. As we show,
the problem exhibits significant differences from previously studied matching
problems. We analyze its computational complexity both in the general case and
for specific restricted instances, presenting hardness results alongside
optimal and approximation algorithms.

</details>


### [22] [Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem](https://arxiv.org/abs/2508.20041)
*Thomas Bläsius,Henrik Csöre,Max Göttlicher,Elly Schmidt,Wendy Yi*

Main category: cs.DS

TL;DR: 提出了FLaMECaST问题，这是具有分层结构和容量约束的欧几里得斯坦纳树变体，证明了其NP难近似性，但在特定约束下设计了多项式时间近似算法


<details>
  <summary>Details</summary>
Motivation: 受分层网络启发，研究具有分层结构和容量约束的欧几里得斯坦纳树问题，解决源点到汇点的最优连接问题

Method: 设计动态规划算法，基于结构洞察力，在特定约束条件下实现多项式时间近似

Result: 证明了FLaMECaST问题即使在受限情况下也是NP难近似的，但在特定条件下可以实现(1+1/2^n)近似

Conclusion: 该问题具有理论挑战性，但在特定几何约束下存在有效的近似解决方案，为分层网络设计提供了理论基础

Abstract: Motivated by hierarchical networks, we introduce the Flow-weighted Layered
Metric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the
Euclidean Steiner tree with layered structure and capacity constraints per
layer. The goal is to construct a cost-optimal Steiner forest connecting a set
of sources to a set of sinks under load-dependent edge costs. We prove that
FLaMECaST is NP-hard to approximate, even in restricted cases where all sources
lie on a circle. However, assuming few additional constraints for such
instances, we design a dynamic program that achieves a $\left(1 +
\frac{1}{2^n}\right)$-approximation in polynomial time. By generalizing the
structural insights the dynamic program is based on, we extend the approach to
certain settings, where all sources are positioned on a convex polygon.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的高效混合参数估计方法PIR（物理信息回归），利用普通最小二乘法进行参数估计，在时间序列数据上表现出比PINN更好的性能和更快的计算速度。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种高效的方法来桥接理论模型和实际数据，实现可靠且快速的参数估计，甚至支持实时应用。

Method: 提出了物理信息回归（PIR）方法，利用参数线性非线性动态模型的特性，通过正则化普通最小二乘法进行参数估计。测试了多个ODE和PDE模型，包括传染病模型和时变参数估计。

Result: 在合成数据和实际COVID-19数据上，PIR方法在参数估计精度方面表现明显优于PINN方法，特别是在复杂模型中。PIR还显示出更高的计算效率。

Conclusion: PIR方法在考虑的模型中表现出优于PINN方法，能够支持可靠且快速的参数估计，为数据驱动和物理信息技术的结合提供了有效的方法。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [24] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 扩展ZipNN压缩技术到低精度浮点格式(FP8/FP4)，通过分离压缩指数和尾数组件，实现高压缩比(最高83%)，并发现LLM中的K/V缓存张量也具有压缩潜力。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型质量增长和部署普及，减少神经网络权重的存储和传输成本变得越来越重要。虽然ZipNN等无损压缩方法在FP32和BF16高精度格式上取得成功，但需要扩展到正在优化推理效率的低精度格式(FP8/FP4)。

Method: 设计了一种压缩方法，将浮点数的指数和尾数组件分离并独立地使用來自权编码进行压缩，将ZipNN方法扩展到FP8和FP4低精度浮点格式。

Result: 评估显示压缩比达到BF16的62%和FP8的83%。同时研究发现大语言模型(LLM)中使用的关键值(K/V)缓存张量也呈现出可压缩的模式，能够在部署过程中节省内存。

Conclusion: 该研究成功将高效的无损压缩技术扩展到低精度浮点格式，为深度学习模型的存储和传输提供了重要的节省潜力，特别是在大语言模型的关键值缓存管理方面。

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [25] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: POT是一种新型黑盒攻击框架，通过LLM迭代优化生成隐蔽且语义自然的对抗提示，无需外部数据访问和模型检索，即可诱导模型产生计算效率低下的冗长推理链。


<details>
  <summary>Details</summary>
Motivation: 现有过度思考攻击需要外部知识源进行数据投毒、依赖可检索的污染内容以及结构明显的模板，这些限制条件限制了在实际场景中的适用性。

Method: 采用基于LLM的迭代优化方法生成隐蔽且语义自然的对抗提示，完全消除对外部数据访问和模型检索的依赖。

Result: 在多种模型架构和数据集上的广泛实验表明，POT相比其他方法实现了更优越的性能。

Conclusion: POT框架有效解决了现有过度思考攻击的局限性，提供了一种更实用和高效的黑盒攻击方法，能够诱导LLM产生计算效率低下的推理过程。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [26] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: 提出了一种在真实分布式物联网环境中训练深度强化学习模型的新框架，使用ACK反馈信息进行训练，通过实际数据传输验证了框架的可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习在资源分配方面表现出色，但缺乏在真实分布式物联网系统中使用真实数据进行训练的研究，需要填补这一空白。

Method: 提出基于DRL的通信信道选择方法，利用实际数据传输中获得的ACK反馈信息来训练DRL模型。

Result: 通过帧成功率(FSR)的性能评估，证明了所提框架的可行性和有效性。

Conclusion: 该框架成功实现了在真实分布式物联网环境中训练DRL模型，为解决复杂资源分配问题提供了实用解决方案。

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [27] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 该论文证明了适当正则函数可以通过有理函数和有理神经网络在C¹范数下进行逼近，并给出了关于网络宽度、深度以及有理函数次数的逼近速率。


<details>
  <summary>Details</summary>
Motivation: 研究有理函数和有理神经网络在C¹范数下的逼近能力，特别是在符号回归和物理定律学习等应用中具有重要意义。

Method: 使用有理函数和有理神经网络进行函数逼近，分析其逼近速率与网络宽度、深度以及有理函数次数的关系。

Result: 获得了在C¹范数下的逼近结果，包括EQL÷和ParFam架构的有理神经网络都能实现有效的C¹逼近。

Conclusion: 有理函数和有理神经网络是强大的逼近工具，在符号回归和物理定律学习等领域具有重要应用价值。

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [28] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame通过关联记忆缓冲区整合少量专家轨迹，显著提升离线强化学习在低质量数据上的性能，仅需0.1%专家数据即可获得高达10.7%的性能提升


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临从次优数据中学习的挑战，如何有效利用稀缺专家演示和大量低质量数据是关键问题

Method: 提出Re:Frame插件模块，在标准离线RL策略基础上增加关联记忆缓冲区(AMB)，通过内容关联检索专家数据并整合到决策中

Result: 在D4RL MuJoCo任务上，仅使用60条专家轨迹(数据集的0.1%)就在4个设置中的3个上显著超越Decision Transformer基线，性能提升最高达10.7个标准化点

Conclusion: Re:Frame提供了一种简单且数据高效的方法来注入稀缺专家知识，大幅改善从低质量数据集进行的离线强化学习

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [29] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: 提出了全局排列熵(GPE)，这是一种新的复杂度指标，考虑了给定长度的所有可能模式（包括非连续模式），相比标准排列熵能揭示更多结构信息。


<details>
  <summary>Details</summary>
Motivation: 标准排列熵只考虑连续段落的相对顺序模式，可能遗漏重要的非连续模式信息，需要一种更全面的复杂度度量方法。

Method: 利用最新算法高效提取完整排列轮廓，计算所有可能模式（包括非连续模式）的频率分布，然后应用香农熵进行量化。

Result: 在合成数据集上的实验表明，GPE能够揭示标准排列熵无法获取的结构信息，证明了其有效性。

Conclusion: 全局排列熵(GPE)是一个有前景的新复杂度指标，提供了比传统排列熵更全面的时间序列分析能力，并提供了Julia实现包。

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [30] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 本文提出了NCMemo框架，首次量化半监督节点分类中的标签记忆现象，发现图同配性与记忆化呈反比关系，低同配性图导致更多记忆化，并提出图重布线方法有效减少记忆化同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被证明会记忆训练数据，但图神经网络(GNNs)的记忆化分析仍较少探索。需要量化GNNs在半监督节点分类中的标签记忆现象，并理解其与图结构特性的关系。

Method: 提出NCMemo框架分析节点分类记忆化；建立记忆化与图同配性的反比关系；分析GNN训练动态和隐式偏差；研究特征空间邻域标签不一致性与记忆化的关联；探索图重布线作为缓解记忆化的方法。

Result: 发现低同配性显著增加记忆化；GNNs在低同配性 regime 中依赖记忆化来最小化训练损失；标签不一致性高的节点更容易被记忆；图重布线能有效减少记忆化且不损害模型性能，同时降低隐私风险。

Conclusion: 该研究深化了对GNN学习机制的理解，揭示了图同配性与记忆化的紧密联系，提出的图重布线方法为实现更隐私保护的GNN部署提供了有效途径。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [31] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 提出基于奇异值分解的多源迁移学习框架，通过分解源模型为秩一组件并选择最显著组件进行聚合，仅微调主奇异值来实现高效知识迁移


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习忽略了利用在线大量可用模型的机会，现有方法粒度粗糙，缺乏精确知识提取和聚合效率，无法处理大量或高参数源模型

Method: 使用奇异值分解将每个源模型分解为基本秩一组件，然后选择所有源中最显著的组件进行聚合，仅微调合并矩阵的主奇异值来适应目标任务

Result: 框架实现了高效迁移学习，对输入级和参数空间的扰动具有鲁棒性（如噪声或剪枝源），计算扩展性良好

Conclusion: 该方法通过SVD分解和选择性聚合解决了多源迁移学习的效率和精度问题，为利用在线模型知识提供了有效途径

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [32] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: 这篇论文是一个关于图数据模型在化学科学中应用的入门指南，介绍了如何使用图论和图神经网络来模型分子、蛋白质、反应和化学过程。


<details>
  <summary>Details</summary>
Motivation: 图论在化学科学中具有至关重要的地位，能够自然地描述分子结构、分子间相互作用以及化学过程，为材料、生物学和医药研究提供基础支撑。

Method: 介绍图数据模型的基础知识，包括图设计原理、关键预测任务，以及图神经网络等机器学习算法如何在图数据上运作。通过化学科学中的代表性案例来展示图基模型的应用。

Result: 为读者提供了完整的图数据模型知识体系，包括数学基础、算法原理和实际应用案例，以及机器学习在图基模型中的作用。

Conclusion: 这个入门指南为读者准备了必要的知识和技能，以便将图数据模型方法应用于下一代化学发现的挖掘和研究中。

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [33] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量化深度学习模型，仅使用RR间隔数据，通过结合时序卷积网络和Mamba状态空间模型，实现了早期预测房颤的高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 房颤（AF）是最常见的异常心律，特别是发作性房颤（PAF）因其突然发作和持续时间短而容易被漏识，但未被发现的PAF可能进展为持续性AF，增加死亡风险。早期预测AF可以通过预防性治疗减缓疾病进展。

Method: 提出了一种轻量化深度学习模型，仅使用RR间隔（RRIs）数据，结合时序卷积网络（TCN）进行位置编码和Mamba选择性状态空间模型，实现高效并行序列建模。

Result: 在主体测试中，模型达到了敏感度0.908、特异性0.933、F1分0.930、AUROC 0.972和AUPRC 0.932。模型仅有73.5千参数和38.3 MFLOPs计算量，计算效率高。重要的是，模型可以仅使用30分钟输入数据预测未来2小时的AF。

Conclusion: 该模型在早期预测房颤方面表现优异，具有高准确性和计算效率，为预防性干预提供了足够的预警时间，在准确性和模型紧凑性方面都超过了传统的CNN-RNN方法。

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [34] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: 该研究开发了一个多模态分层分类框架来解决电商产品分类中的平台异质性和现有分类法结构限制问题，在40个国际时尚电商平台的27.17万产品数据集上取得了98.59%的层次F1分数，并展示了工业部署的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决电商产品分类中的两个关键工业挑战：平台异质性和现有分类法的结构限制，需要开发能够处理多平台数据并保持分类一致性的分类系统。

Method: 使用多模态分层分类框架，整合文本特征(RoBERTa)、视觉特征(ViT)和视觉-语言联合表示(CLIP)，研究早期融合、晚期融合和基于注意力的融合策略，并采用动态掩码确保分类一致性。还引入了自监督的产品重新分类流程。

Result: CLIP嵌入通过基于MLP的晚期融合策略实现了最高的层次F1分数(98.59%)。自监督重新分类流程发现了新的细粒度类别，聚类纯度超过86%。跨平台实验显示复杂晚期融合方法在多样化训练数据上准确率最高，而简单早期融合方法对新平台泛化更好。

Conclusion: 该框架成功解决了电商产品分类的工业挑战，通过两阶段推理管道在商业交易智能平台中实现了工业级可扩展部署，平衡了成本和准确性。

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [35] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 本文提出了首个基于信息几何和扩散动力学的多模态大语言模型幻觉量化框架，将幻觉从定性检测转变为数学基础测量


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型幻觉评估技术主要是启发式的，缺乏理论保证和量化方法，无法理解幻觉如何产生、传播和跨模态交互

Method: 将多模态LLM输出表示为多模态图拉普拉斯算子的谱嵌入，通过特征模式分解在再生核希尔伯特空间嵌入中表征真实性与不一致性之间的流形差距，作为语义失真

Result: 该框架提供了模态感知、理论可解释的度量标准，能够通过温度退火捕捉幻觉随时间和输入提示的演化，并建立了量化幻觉的严格数学基础

Conclusion: 这项工作为量化和界定幻觉建立了原则性基础，将幻觉从定性风险转变为可处理、可分析的现象

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [36] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 基于LLaMA 3.2的视觉语言模型在高能物理中微子相互作用分类任务上表现优于传统CNN方法，支持多模态推理


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多模态推理方面的潜力，特别是在高能物理实验中使用视觉语言模型进行中微子相互作用分类

Method: 使用基于LLaMA 3.2的视觉语言模型进行微调，与NOvA和DUNE实验中使用的CNN基线模型进行性能对比

Result: VLM模型在分类准确率、精确率、召回率和AUC-ROC等指标上达到或超过CNN性能，并能更好地整合文本和语义上下文

Conclusion: 视觉语言模型为高能物理中的事件分类提供了有前景的通用骨干网络，为实验性中微子物理中的多模态方法开辟了新途径

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [37] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 本研究探索了量子机器学习在恶意软件分类中的应用，比较了量子多层感知器(QMLP)和量子卷积神经网络(QCNN)两种混合量子-经典模型在多个数据集上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的出现，量子机器学习为改进恶意软件检测提供了范式转换的机会，但该领域应用仍未被充分探索。

Method: 使用两种混合量子-经典模型：QMLP通过全量子比特测量和数据重上传捕获复杂模式，QCNN通过量子卷积和池化层减少活跃量子比特实现更快训练。两种模型都使用角度嵌入将恶意软件特征编码为量子态。

Result: 二进制分类准确率：API-Graph 95-96%，AZ-Domain 91-92%，EMBER-Domain 77%。多分类准确率：API-Graph 91.6-95.7%，AZ-Class 41.7-93.6%，EMBER-Class 60.7-88.1%。QMLP在复杂多分类任务中表现更优，QCNN训练效率更高但准确率略低。

Conclusion: 量子机器学习在恶意软件分类中展现出良好潜力，QMLP适合复杂分类任务，QCNN在训练效率方面有优势，为量子计算在网络安全领域的应用提供了新方向。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [38] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 提出DETNO架构，结合Transformer神经算子和扩散模型，解决交通流预测中高频特征丢失和长期预测误差累积问题


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在交通流预测中产生平滑预测，无法重建高频特征（如密度梯度），导致多步预测误差快速累积，影响实时交通管理

Method: 统一扩散增强Transformer神经算子架构：使用带交叉注意力机制的Transformer神经算子提供模型表达力和超分辨率能力，结合基于扩散的细化组件通过渐进去噪迭代重建高频交通细节

Result: 在混沌交通数据集上评估显示，相比传统和基于Transformer的神经算子，该方法在长期预测中表现更优，能保持高频成分并提高预测稳定性

Conclusion: DETNO架构成功克服了标准神经算子的平滑限制和预测不稳定性，为长期交通预测提供了有效解决方案

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [39] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典架构用于SMILES字符串重构，通过量子编码与经典序列建模结合，实现了84%的量子保真度和60%的经典重构相似度，超越了现有量子基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管量子机器学习在分子设计等生成模型中具有潜力，但现有方法在SMILES字符串重构等序列任务中存在保真度下降的问题，量子与经典方法的结合仍待探索。

Method: 采用混合量子-经典架构，将量子编码技术与经典序列建模相结合，旨在同时提升量子保真度和经典相似度。

Result: 实现了约84%的量子保真度和60%的经典重构相似度，性能优于现有的量子基线方法。

Conclusion: 该工作为未来量子机器学习应用奠定了有前景的基础，在表达性量子表示和经典序列模型之间取得了平衡，推动了量子感知序列模型在分子和药物发现领域的更广泛研究。

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [40] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: KAR-HNN使用Kolmogorov-Arnold表示替代MLP，通过单变量变换更好地学习哈密顿函数，提高能量守恒性和长期预测稳定性


<details>
  <summary>Details</summary>
Motivation: 现有基于MLP的哈密顿神经网络对超参数敏感，在复杂能量景观中表现不佳，需要更好的方法来捕捉高频和多尺度动力学

Method: 采用Kolmogorov-Arnold表示理论，用单变量变换替换多层感知机，实现局部函数逼近，保持哈密顿系统的辛形式

Result: 在弹簧质量、单摆、二体和三体问题等基准测试中表现出减少能量漂移、改善长期预测稳定性的效果

Conclusion: KAR-HNN有望在高维度和少参数的现实物理过程建模中提供准确稳定的预测能力

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [41] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct在聊天格式中错误判断"9.11"大于"9.8"，研究发现偶数索引注意力头专门处理数值比较，奇数头执行不兼容功能，需要至少8个偶数头才能完美修复该bug。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在格式依赖推理中的失败机制，特别是数值比较错误，以揭示Transformer架构的内部工作机制和冗余特性。

Method: 通过系统性干预实验，分析不同格式下的注意力头功能分工，使用稀疏自编码器(SAE)进行特征分析，并测试不同数量注意力头组合的修复效果。

Result: 发现偶数头专门负责数值比较，需要至少8个偶数头才能完美修复bug；特征分析显示格式表示在第7层分离(10%重叠)，在第10层重新纠缠(80%重叠)；仅使用25%的注意力头即可实现完美修复。

Conclusion: 表面上的全模块需求隐藏了复杂的子结构，这对可解释性和效率具有重要意义，模型具有完美的冗余机制和尖锐的计算阈值。

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [42] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 使用物理信息机器学习流程，结合可微分多相流模拟器咈卷神经网络，大幅减少了预测地下存储层压力所需的高保真模拟次数，从之前的1000万次降至不到3000次


<details>
  <summary>Details</summary>
Motivation: 地质异质性咈多相流体动力学使得地下存储层压力控制极其困难，高保真物理模拟计算成本昂贵且需要进行大量模拟，这在实践中往往是不可行的

Method: 开发了一种物理信息机器学习流程，将可微分多相流模拟器(DPFEHM框架)与卷积神经网络(CNN)相结合。CNN学习根据异质温透率场预测流体提取速率以维持关键位置的压力限制。采用迁移学习策略，先在单相稳态模拟上预训练，然后在全多相场景上微调

Result: 方法能够在仅使用不到3000次全物理多相流模拟的情况下实现高准确性训练，较之前估计需要1000万次模拟的要求大幅减少。通过利用计算成本更低的单相模拟进行迁移学习，显著降低了计算成本

Conclusion: 该物理信息机器学习方法能够在大幅减少高保真模拟次数的前提下，为实际注入-提取场景提供更实用咈准确的预测，解决了地下存储层压力控制中的计算成本挑战

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [43] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 提出基于对比学习的无监督框架，使用基因水平和染色体水平双视图突变特征对43种癌症类型进行聚类分析


<details>
  <summary>Details</summary>
Motivation: 理解泛癌症突变景观对肿瘤发生机制至关重要，目前队列水平的癌症聚类主要依赖传统统计方法，缺乏机器学习技术的应用

Method: 使用COSMIC数据库编码突变数据，构建基因水平和染色体水平双视图突变特征，通过TabNet编码器和多尺度对比学习目标(NT-Xent损失)学习统一的癌症类型嵌入表示

Result: 学习到的潜在表征能够产生具有生物学意义的癌症类型聚类，与已知的突变过程和组织起源相一致

Conclusion: 这是对比学习在队列水平癌症聚类中的首次应用，为突变驱动的癌症亚型分析提供了可扩展且可解释的框架

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [44] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 通过空间填充采样本地方"刻度"状态生成神经PDE训练数据，消除轨迹数据中的时空冗余性，以更高效的方式训练更准确的神经PDE模型。


<details>
  <summary>Details</summary>
Motivation: 传统神经PDE训练需要长时间积分获取解决方案轨迹，数据效率低且存在时空冗余性。需要一种更样本高效的数据生成策略来提升神经PDE的训练效果和模型准确性。

Method: 提出空间填充采样本方法，通过采样本地方"刻度"状态来生成合成训练数据。这种方法能够消除轨迹数据中的时空冗余性，并重点采样那些在轨迹中少见但对模型沿射性重要的状态。仅需等效于10个时间步长的数值模拟数据即可训练准确的神经刻度运算符。

Result: 在多个PDE系统中，该数据增帽策略能够训练出更好的神经刻度运算符。如果能够获得一个完整轨迹模拟数据，性能还能进一步提升。与从模拟轨迹中简单采样的刻度数据相比，该方法显示出明显的性能优势。

Conclusion: 空间填充采样的数据生成策略能够显著提高神经PDE的训练效率和模型准确性，通过消除时空冗余性和重点采样关键状态，以更少的计算成本获得更好的模型性能。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [45] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: 提出在生成模型中引入张量分解技术，通过生成较小的张量因子而非完整张量来显著降低模型输出和参数数量，从而减少复杂仿真数据的生成成本。


<details>
  <summary>Details</summary>
Motivation: 大型复杂仿真数据集的生成通常耗时耗资源，特别是在实验成本高昂的情况下，需要更高效的方法来生成合成数据用于下游任务。

Method: 针对多维数据（张量），在生成模型（如GAN或扩散模型）中引入内部张量分解，生成较小的张量因子而不是完整的张量。

Result: 实验表明该方法显著降低了模型输出和整体参数数量，减少了生成复杂仿真数据的成本，同时生成的数据仍然保持可用性。

Conclusion: 张量分解技术有潜力提高生成模型的效率，特别是在生成多维数据或张量时，能够有效降低成本同时保持数据质量。

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [46] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: 本文证明了现代神经网络架构（如预层归一化和线性注意力模块）几乎总是满射的，这意味着任何输出都可以由某些输入生成，揭示了生成模型不可避免的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否具有满射性，因为满射性意味着任何输出（包括有害内容）都可能被生成，这关系到模型安全性和越狱漏洞问题。

Method: 通过数学证明分析现代神经网络基础构建模块的满射性，特别关注预层归一化和线性注意力模块等常用架构。

Result: 证明了GPT风格transformer和确定性ODE求解器的扩散模型等广泛使用的生成框架对任意输出都存在逆映射，即这些架构几乎总是满射的。

Conclusion: 现代常用神经网络架构具有不可避免的满射性，这为理解其面对广泛对抗攻击的脆弱性提供了形式化框架，对模型安全性具有重要意义。

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [47] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 该论文研究了成员推理攻击的样本复杂度，发现在高斯均值估计场景中，攻击者需要Ω(n + n²ρ²)个参考样本才能与完全知情的攻击者竞争，这比训练算法使用的样本数量多得多。


<details>
  <summary>Details</summary>
Motivation: 当前成员推理攻击通常假设攻击者拥有与训练数据相同分布的参考样本，但实际攻击中使用的样本数量有限。研究需要多少参考样本才能进行有效的成员推理攻击，对于理解攻击的实际可行性和隐私风险评估至关重要。

Method: 在高斯均值估计的基本设置下进行研究，学习算法从d维高斯分布N(μ,Σ)中获取n个样本，估计均值μ̂使得期望误差E[‖μ̂-μ‖²_Σ]≤ρ²d。分析攻击者需要的最小参考样本数量来进行成功的成员推理攻击。

Result: 研究结果表明，对于这种设置下的成员推理攻击，攻击者需要Ω(n + n²ρ²)个参考样本才能与完全知情的攻击者竞争。这是首次证明攻击者有时需要比训练算法使用的样本数量多得多的样本。

Conclusion: 实践中使用的攻击通常只使用O(n)个样本，无法从ω(n)个样本中受益，因此可能低估了成员推理的可能性。当分布信息容易获取时，可能存在更好的攻击方法，这对隐私风险评估具有重要意义。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [48] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 该论文研究连续度量空间中无限臂多臂老虎机的激励探索问题，通过补偿机制激励近视代理进行探索，并处理奖励漂移问题，提出了实现次线性累积遗憾和总补偿的算法。


<details>
  <summary>Details</summary>
Motivation: 解决经典多臂老虎机模型中无限臂空间下的激励探索问题，特别是在存在奖励漂移（由激励引起的偏差反馈）的复杂情况下，设计能够同时实现低遗憾和低补偿成本的算法。

Method: 提出新颖的激励探索算法，通过对无限臂空间进行均匀离散化处理，设计了能够同时达到次线性累积遗憾和次线性总补偿的算法框架，并将方法推广到上下文老虎机场景。

Result: 算法实现了$\Tilde{O}(T^{d+1/d+2})$的遗憾和补偿边界，其中d是度量空间的覆盖维度，并通过数值模拟验证了理论结果的有效性。

Conclusion: 该研究成功解决了无限臂空间下的激励探索问题，提出的算法在理论上和实验上都表现出色，为连续度量空间中的多臂老虎机问题提供了有效的解决方案。

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [49] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas算法通过生成数据的局部低维表示并训练深度神经网络来验证流形假设，发现许多真实数据集并不符合流形假设，但对于符合的数据可以构建生成模型。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习工具只能生成全局嵌入，无法验证流形假设是否成立，也无法提供数学定义中的局部映射。

Method: 生成数据局部邻域的低维表示，训练深度神经网络在局部嵌入和原始数据之间进行映射，使用拓扑失真来评估流形假设和确定维度。

Result: 在测试数据集上成功学习流形结构，但发现许多真实数据集（如单细胞RNA测序）不符合流形假设。对于符合流形假设的数据，可以构建生成模型。

Conclusion: DeepAtlas能够验证流形假设并确定数据维度，为符合流形结构的数据集提供了应用微分几何工具的可能性。

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [50] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了SAFT框架来解决表格学习中的分布偏移问题，通过连续表示生成范式实现可微分优化，包含三个鲁棒性机制，实验表明在多种真实分布偏移下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 表格学习在训练和测试数据存在分布偏移时效果会显著下降，需要解决Distribution Shift Tabular Learning (DSTL)问题

Method: Shift-Aware Feature Transformation (SAFT)框架，将表格学习从离散搜索重构为连续表示生成范式，包含嵌入解相关、样本重加权、次优嵌入平均和归一化对齐三个机制

Result: 大量实验表明SAFT在鲁棒性、有效性和泛化能力方面持续优于先前的表格学习方法

Conclusion: SAFT通过连续优化和多重鲁棒机制有效解决了表格学习中的分布偏移问题，具有优异的性能表现

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [51] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE是一个数据高效的符号回归框架，通过质量对齐的迁移嵌入来微调基础模型，在低数据环境下实现更好的方程发现性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在大规模方程数据集上预训练后，在小型领域特定数据集上应用时容易出现负迁移和泛化能力差的问题，需要一种数据高效的微调方法。

Method: EQUATE结合符号-数值对齐和评估器引导的嵌入优化，将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，通过数据-方程适应性和简洁性进行指导。

Result: 在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面始终优于最先进的基线方法，同时保持低复杂度和快速推理。

Conclusion: EQUATE为基础模型蒸馏设置中的数据高效符号回归提供了一个实用且可推广的解决方案。

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [52] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: 提出了PoolFlip环境和Flip-PSRO方法，通过多智能体强化学习训练防御者对抗未知攻击策略，相比基线方法效果提升2倍


<details>
  <summary>Details</summary>
Motivation: 现有FlipIt框架依赖少量启发式或专门学习技术，导致脆弱性和无法适应新攻击，需要更强大的自动化防御决策方法

Method: 扩展FlipIt游戏为PoolFlip多智能体环境，提出Flip-PSRO多智能体强化学习方法，利用基于群体的训练和所有权效用函数

Result: Flip-PSRO防御者对训练中未暴露的启发式攻击泛化能力比基线方法高2倍，同时保持高水平控制

Conclusion: PoolFlip环境和Flip-PSRO方法为网络防御提供了有效的自动化决策框架，能够应对隐蔽、欺骗性和不断演化的对抗策略

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [53] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: 使用大型语言模型优化Python程序形式的游戏策略，通过代码自进化和自然语言反馈实现高效游戏智能体训练


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法需要大量训练时间和环境交互，希望开发更高效的策略表示和学习方法

Method: 将决策策略表示为Python程序，利用LLM通过执行轨迹和自然语言反馈进行代码自进化优化

Result: 在Atari游戏中达到与深度RL基线竞争的性能，同时显著减少训练时间和环境交互次数

Conclusion: 程序化策略表示有望构建高效、适应性强的智能体，具备复杂长时程推理能力

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [54] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: MobText-SISA是一个针对异构时空数据的机器遗忘框架，通过分片、隔离、切片和聚合训练，实现高效准确的个体数据删除，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代移动平台存储了大量GPS轨迹和文本数据，GDPR等隐私法规要求按需删除个体数据，但为每个删除请求重新训练深度模型不可行。

Method: 将行程的数值和语言特征嵌入共享潜在空间，使用相似性感知聚类将样本分布到分片中，每个分片增量训练，删除时仅重新训练受影响的分片。

Result: 在10个月的真实移动日志实验中，MobText-SISA保持基线预测准确性，在错误率和收敛速度上均优于随机分片方法。

Conclusion: MobText-SISA为多模态移动数据的隐私合规分析提供了实用基础，能够在城市规模下实现高效准确的机器遗忘。

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [55] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs在表格数据拟合中存在严重脆弱性，任务无关的数据表示变化（如变量名修改）可导致预测结果大幅波动，误差变化高达82%，揭示了LLMs缺乏作为数据拟合工具的基本鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于非语言任务的数据拟合，需要评估其在表格数据预测中的鲁棒性，特别是对任务无关数据变化的敏感性。

Method: 通过改变变量名等任务无关的数据表示变化，测试LLMs在上下文学习和监督微调下的预测稳定性，并分析注意力模式来解释敏感性原因。

Result: 发现LLMs对任务无关变化极度敏感，预测误差变化高达82%；注意力分析显示非均匀模式，某些位置获得过多关注；专用表格模型TabPFN也存在类似问题。

Conclusion: 尽管LLMs具有出色的预测能力，但目前缺乏基本鲁棒性，不能作为可靠的数据拟合工具使用。

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [56] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: Bi-LoRA方法通过引入辅助LoRA模块来建模SAM的对抗性权重扰动，在保持内存效率的同时实现了更平坦的最小值，消除了SAM的双倍训练成本。


<details>
  <summary>Details</summary>
Motivation: SAM虽然能通过寻找平坦最小值来提升泛化能力，但其巨大的内存和计算开销使其难以应用于大模型。直接对LoRA参数应用SAM会限制锐度优化到受限子空间，影响效果。

Method: 提出双向低秩适应(Bi-LoRA)，引入辅助LoRA模块来建模SAM的权重扰动。主LoRA模块通过标准梯度下降适应任务，辅助模块通过梯度上升捕捉损失景观的锐度。

Result: 在多种任务和架构上的广泛实验证明了Bi-LoRA在提升泛化能力方面的效率和有效性。

Conclusion: Bi-LoRA通过双模块设计成功解决了SAM在大模型微调中的内存和计算效率问题，同时保持了优异的泛化性能。

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [57] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: 提出了一种基于因果推理的反事实奖励模型，通过多模态表示学习提供无监督、抗偏见的奖励信号，在假新闻检测中达到89.12%准确率，显著减少虚假相关性和不公平强化信号。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF中的奖励模型容易学习和放大数据中的潜在偏见，导致有缺陷的奖励信号和公平性下降。现有的偏见缓解方法通常采用被动约束，在因果混淆情况下可能失效。

Method: 提出反事实信任评分系统，包含四个组件：反事实偏移分解政治框架偏见与主题偏见、重构不确定性、公平规则违反检测、时间奖励偏移对齐动态信任度量。在多模态假新闻数据集上进行评估，并注入合成偏见测试鲁棒性。

Result: 在假新闻检测任务中达到89.12%的准确率，优于基线奖励模型。更重要的是，显著减少了虚假相关性和不公平的强化信号。

Conclusion: 该框架为公平感知的RLHF提供了一种鲁棒且可解释的方法，提供可调的偏见减少阈值，提高了动态实时决策的可靠性。

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [58] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: 本教程介绍生成式合成数据的基础知识、最新进展、方法论、实践框架、评估策略和应用，帮助数据挖掘研究者利用生成模型解决数据稀缺、隐私和标注问题。


<details>
  <summary>Details</summary>
Motivation: 解决数据挖掘中的数据稀缺、隐私保护和标注成本高昂等挑战，通过生成式模型创建合成数据来扩展数据集并保护隐私。

Method: 涵盖大型语言模型、扩散模型和生成对抗网络等生成式模型的方法论，提供实践框架和评估策略。

Result: 为数据挖掘研究者和实践者提供可操作的见解，帮助他们利用生成式合成数据增强数据挖掘研究和应用。

Conclusion: 生成式合成数据为数据挖掘领域提供了可扩展的解决方案，能够有效应对数据稀缺、隐私和标注等核心挑战。

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [59] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: 提出SyReM方法解决运动预测中持续学习的稳定性-可塑性困境，通过约束记忆损失和选择性重放机制，在11个驾驶数据集上显著缓解灾难性遗忘并提升新场景预测精度


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在运动预测中面临灾难性遗忘问题，现有持续学习方法过度强调记忆稳定性会损害学习可塑性，需要平衡稳定性与可塑性的矛盾

Method: 提出协同记忆重放(SyReM)方法：维护紧凑记忆缓冲区，使用不等式约束保证记忆稳定性，基于损失梯度余弦相似度选择相似样本进行选择性重放以增强可塑性

Result: 在INTERACTION的11个自然驾驶数据集上验证，相比非持续学习和持续学习基线，SyReM显著缓解了过去场景的灾难性遗忘，同时提高了新场景的预测精度

Conclusion: SyReM通过协同的稳定性约束和选择性重放机制，有效解决了持续学习中的稳定性-可塑性困境，为运动预测任务的持续学习提供了有效解决方案

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [60] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: Delta-Attribution是一个模型无关的框架，通过差分特征归因来解释模型版本间的变化原因，提供轻量级的更新审计方法


<details>
  <summary>Details</summary>
Motivation: 模型更新（超参数、核函数、深度、求解器或数据）会改变性能，但变化的原因往往不透明，需要一种方法来解释版本间的差异

Method: 通过差分特征归因Δφ(x)=φ_B(x)-φ_A(x)，使用Δ-归因质量套件评估变化，包括幅度/稀疏性、一致性/偏移、行为对齐和鲁棒性等指标

Result: 在45个设置中验证，归纳偏置变化产生大的行为对齐delta（如SVC poly→rbf：BAC≈0.998），而表面调整显示完全一致（rank-overlap@10=1.0）

Conclusion: Δ-归因提供了一种轻量级的更新审计方法，通过区分良性变化与行为上有意义或风险依赖转移来补充准确性评估

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [61] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: Dual-LS是一种受人类大脑启发的在线持续学习范式，通过双记忆回放机制解决DNN车辆运动预测中的灾难性遗忘问题，显著提升预测稳定性并大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 智能城市服务依赖AI，但DNN车辆运动预测模型在更新时会出现灾难性遗忘问题。传统方法数据收集成本高、样本效率低，且无法平衡长短期经验，无法实现人类般的持续学习。

Method: 受人类大脑互补学习系统启发，提出Dual-LS范式，采用两种协同的记忆回放机制：加速经验检索，动态协调长短期知识表示。

Result: 在三个国家77.2万辆车、11,187公里测试里程的自然数据上，Dual-LS将灾难性遗忘减少74.31%，计算资源需求降低94.02%，显著提升预测稳定性且不增加数据需求。

Conclusion: Dual-LS为基于DNN的车辆运动预测提供了计算高效、人类般的持续学习适应性，适合智能城市应用。

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [62] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: 提出了RLTR框架，通过工具使用完整性奖励信号来解耦训练过程，专门优化LLM智能体的规划能力，相比端到端方法提升了8-12%的规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多目标优化训练范式存在目标分配不平衡和可验证数据稀缺的问题，难以有效提升智能体的规划能力。

Method: RLTR框架将训练过程解耦，基于工具使用完整性设计奖励信号，直接评估工具调用序列质量，实现单目标优化的规划模块训练。

Result: 实验显示RLTR在规划性能上比端到端基线提升8-12%，规划能力的提升还带来了整体系统最终响应质量5-6%的提高。

Conclusion: RLTR通过解耦训练和工具使用奖励机制，有效解决了规划能力优化难题，为LLM智能体训练提供了更直接可靠的训练信号。

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [63] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast是首个专门为金融时间序列预测设计的基础模型，通过大规模金融数据集训练，在零样本设置下表现出色，无需领域特定微调即可捕捉多样化模式，超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测对经济稳定、政策制定和可持续投资至关重要，但由于时间非平稳性、多领域多样性和不同时间分辨率导致的模式变化，传统方法面临过拟合和需要大量领域特定微调的挑战。

Method: 开发FinCast基础模型，在大规模金融数据集上进行训练，专门针对金融时间序列预测设计，能够处理时间非平稳性、多领域多样性和不同时间分辨率的问题。

Result: FinCast展现出强大的零样本性能，无需领域特定微调即可有效捕捉多样化模式，通过全面实证和定性评估证明其超越了现有最先进方法。

Conclusion: FinCast作为首个金融时间序列预测基础模型，具有强大的泛化能力，为金融预测领域提供了有效的解决方案，能够克服传统方法的局限性。

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [64] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: ALSA是一种新颖的模型精度估计框架，直接在logit空间操作以避免softmax压缩导致的信息损失，通过锚点建模策略在分布偏移下提供鲁棒的精度估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖softmax概率或数据相似性度量，前者存在信息损失问题，后者计算昂贵且领域特定，需要一种更有效的方法来估计未标记数据集上的模型精度。

Method: ALSA在logit空间初始化多个可学习锚点，每个锚点分配影响函数来捕获logit的细微变化，利用logit聚合和分布与模型预测性能的强相关性。

Result: 在视觉、语言和图基准测试上的广泛实验显示，ALSA优于基于softmax和相似性的基线方法，在显著分布偏移下表现出优异的鲁棒性。

Conclusion: ALSA通过直接在logit空间操作避免了信息损失，提供了一种实用可靠的模型评估工具，特别适用于分布偏移场景下的精度估计。

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [65] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: pFedBayesPT是一个基于视觉提示调优的细粒度实例级个性化联邦学习框架，通过贝叶斯方法处理客户端内部数据异质性，在特征和标签异质性设置下均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统个性化联邦学习方法假设每个客户端数据遵循单一分布，但实际中单个客户端可能包含来自多个源或域的数据，存在显著的客户端内部异质性，导致性能不佳

Method: 基于视觉提示调优，从贝叶斯角度制定实例级提示生成，将提示后验建模为隐式分布以捕捉多样化视觉语义，在半隐式变分推断框架下推导变分训练目标

Result: 在基准数据集上的广泛实验表明，pFedBayesPT在特征和标签异质性设置下始终优于现有的个性化联邦学习方法

Conclusion: 提出的pFedBayesPT框架有效解决了客户端内部数据异质性挑战，通过实例级个性化建模显著提升了联邦学习性能

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [66] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: SCAR是一个数据特征分析框架，通过Scale、Coverage、Authenticity、Richness四个维度量化数据集的结构特性，并提出Foundation Data概念来保持泛化性能，同时开发了基于泛化偏差的多模态数据补全策略。


<details>
  <summary>Details</summary>
Motivation: 传统数据优化方法主要关注数据量和训练效率，缺乏对数据质量结构特性的理论理解，特别是在样本缩放时数据特性如何影响泛化能力。

Method: 提出SCAR框架量化数据集的结构特性，引入Foundation Data概念（保持全数据集泛化行为的最小子集），建模单模态任务为阶跃函数，估计基础数据大小分布来捕捉多模态数据集中的阶跃式泛化偏差。

Result: 在多种多模态数据集和模型架构上的实验验证了SCAR在预测数据效用和指导数据获取方面的有效性。

Conclusion: SCAR提供了一个稳健的数据理解基础，能够捕捉数据集缩放时保持稳定的内在结构特性，为多模态数据的高效扩展提供了理论指导。

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [67] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 本文首次全面探索了低功耗柔性压力分类器的设计空间，开发了1200多个柔性分类器，通过机器学习分类器、特征选择和神经简化算法，实现了比现有方法更高精度、低成本、可贴合且功耗低的实时压力监测解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统压力监测依赖间歇性、症状导向的干预，缺乏连续、可及且成本效益高的解决方案。现有硅基可穿戴设备虽然功能多样，但不够轻便灵活，限制了连续监测的实用性。柔性电子虽具有灵活性和低成本优势，但在实现复杂机器学习分类器方面存在集成和功耗挑战。

Method: 进行了全面的设计空间探索，涵盖多种机器学习分类器、特征选择和神经简化算法，设计了1200多个柔性分类器。采用完全定制化电路设计，使用低精度算术运算来优化硬件效率。

Result: 开发出了比现有方法精度更高的实时压力分类器，同时实现了低成本、可贴合性、低功耗和紧凑尺寸的设计目标。

Conclusion: 这项工作为设计实时压力分类器提供了重要见解，证明了柔性电子在实现高效、实用的连续压力监测方面的潜力，为未来可穿戴健康监测设备的发展指明了方向。

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [68] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: 该论文研究图上游走的度量结构，通过引入加权度量来处理序列，定义基于逐步顶点距离和加权范数的游走间距离，分析这些度量空间的主要性质，为测量游走间相对距离的邻近度提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究图上游走的度量结构，为分析游走间的相对距离提供理论基础，特别是在强化学习和网络结构回归等应用中需要测量游走间相似性的场景。

Method: 引入加权度量处理序列，定义基于逐步顶点距离和加权范数的游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造方法。

Result: 建立了游走的度量框架，允许使用经典度量建模工具，如从游走子空间扩展Lipschitz函数，通过表示保持基本性质来扩展邻近度函数。

Conclusion: 该度量框架为估计邻近度和开发基于探索性游走的强化学习策略提供了坚实基础，为网络结构上的Lipschitz回归提供了稳健方法。

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [69] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: 提出了Adam-PFN，一种针对Adam优化器超参数调优的预训练代理模型，结合新的学习曲线增强方法CDF-augment，显著提升了冻结-解冻贝叶斯优化的性能。


<details>
  <summary>Details</summary>
Motivation: Adam优化器广泛使用但超参数调优耗时耗力，现有冻结-解冻贝叶斯优化方法受限于通用代理模型，缺乏对超参数如何影响学习过程的先验知识。

Method: 开发了Adam-PFN预训练代理模型，在TaskSet学习曲线上进行预训练，并提出了CDF-augment学习曲线增强方法，通过人工增加训练样本来提升性能。

Result: 方法在TaskSet评估任务上显著改善了学习曲线外推能力并加速了超参数优化，在分布外任务上也表现出强劲性能。

Conclusion: Adam-PFN结合CDF-augment为Adam超参数调优提供了有效的解决方案，在有限预算下实现了更好的优化性能。

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [70] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP是一种无需训练的图分区方法，通过负校正机制放大超出常规范围[0,2]的低频信息，结合谱GNN和随机输入，仅需一次前向传播即可获得高质量的社区检测结果。


<details>
  <summary>Details</summary>
Motivation: 从图信号处理的角度发现，带有负校正的图拉普拉斯矩阵可以产生超出常规范围[0,2]的图频率，探索这些低频信息是否能编码更多关于社区结构的信息。

Method: 采用谱GNN作为主干网络，结合低通滤波器和负校正机制，仅输入随机信号，通过一次前向传播无需训练即可获得图嵌入，然后将嵌入输入BIRCH聚类算法得到图分区结果。

Result: 在IEEE HPEC Graph Challenge基准测试中，InfraredGP在静态和流式图分区任务中实现了16-23倍的速度提升，同时保持了具有竞争力的质量。

Conclusion: 负校正机制能够有效放大超出常规频率范围的低频信息，仅通过一次前向传播无需训练即可获得高质量的图分区结果，在效率和准确性方面都表现出色。

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [71] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: 提出基于3D扩散模型的生成管道，直接合成任意大的颗粒介质装配体，解决离散元法初始化阶段计算密集的问题，实现线性时间扩展和快速合成。


<details>
  <summary>Details</summary>
Motivation: 离散元法模拟颗粒介质时，初始化阶段由于大位移和相关动能而计算密集，占据总模拟时间的大部分，需要克服这一瓶颈。

Method: 采用两阶段管道：首先训练扩散模型生成独立的3D体素网格表示颗粒介质；然后使用基于掩码输入的3D修复模型无缝拼接这些网格，探索多种掩码策略和2D重绘技术。

Result: 实现了线性计算时间扩展，1.2米长的道碴轨道合成相当于3小时DEM模拟，在20秒内完成，生成的体素网格可后处理提取颗粒几何形状。

Conclusion: 该方法能够实现物理一致、实时、可扩展的颗粒介质合成，适用于工业应用。

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [72] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: EUREKA框架通过选择有趣而非最准确的特征来构建分类器，利用大语言模型评估特征有趣性，生成具有新颖性和可解释性的模型


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型追求预测准确性，但本文探索构建"有趣分类器"的目标，即使用不寻常或意外的特征，即使准确性低于最佳模型，以支持知识发现和沟通

Method: 提出EUREKA框架：1）使用大语言模型根据感知有趣性对特征进行排序；2）仅使用选定的有趣特征构建可解释分类器

Result: 在多个基准数据集上，EUREKA始终识别出非明显但仍具有预测性的特征。例如在Occupancy Detection数据集中偏好湿度而非CO2和光照强度，在Twin Papers数据集中发现标题含冒号的论文更可能被引用

Conclusion: 这种模型支持新的知识发现和沟通方式，特别是在中等准确性足够但新颖性和可解释性受到重视的场景中

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [73] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: PSO-Merging是一种基于粒子群优化的数据驱动模型融合方法，通过初始化粒子群并进行多轮迭代，有效提升了模型融合的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法存在性能限制：数据无关方法缺乏数据指导，梯度方法计算成本高，而现有无梯度方法在有限优化步数内效果不佳。

Method: 使用粒子群优化算法，初始化粒子群包含预训练模型、专家模型和稀疏化专家模型，通过多轮迭代获得最终融合模型。

Result: 在不同语言模型上的实验表明，PSO-Merging通常优于基线融合方法，提供了更高效和可扩展的解决方案。

Conclusion: PSO-Merging通过粒子群优化算法成功解决了现有模型融合方法的局限性，为构建多任务模型提供了有效的数据驱动融合策略。

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [74] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 提出了一种新的辛卷积神经网络架构，通过结合辛神经网络、适当辛分解和张量技术来构建保持辛结构的CNN层。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理哈密顿系统时无法保持系统的辛结构，这可能导致物理信息丢失和数值不稳定性。需要开发能够保持辛几何特性的神经网络架构。

Method: 首先引入卷积层的数学等价形式，然后使用辛神经网络参数化CNN层以确保卷积层保持辛特性。还引入了辛池化层来构建完整的自编码器。

Result: 在波动方程、非线性薛定谔方程和正弦-戈登方程三个示例上测试，数值结果表明辛CNN性能优于通过适当辛分解获得的线性辛自编码器。

Conclusion: 提出的辛卷积神经网络架构能够有效保持哈密顿系统的辛结构，在物理系统建模方面表现出优越性能，为物理信息神经网络提供了新的解决方案。

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [75] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于梯度估计的方法，用于高效选择in-context学习中的示例。通过一阶近似估算模型输出，并综合多个随机子集的结果，该算法在线性时间内完成选择，性能超过现有方法约11%，速度提升37.7倍。


<details>
  <summary>Details</summary>
Motivation: 在in-context学习中，如何从n个示例中高效选择k个最优的来作为条件化信息，这在提示调整和链式思绪中具有重要应用。由于模型参数固定，现有方法主要基于输入嵌入的相似性，需要更有效的选择策略。

Method: 提出基于输入嵌入空间梯度的新方法。通过一阶近似使用梯度估算模型输出，然后对多个随机采样的子集进行这种估算，最后聚合所有子集的结果为每个示例计算影响分数，选择分数最高的k个示例。该方法只需预计算模型输出和梯度一次，复杂度为线性。

Result: 在六个数据集上，梯度估计方法的近似误差小于1%，让子集选择的缩放比达37.7倍（最大模型规340亿参数），性能超过现有基于输入嵌入的方法约11%。

Conclusion: 该研究提出的基于梯度估计的示例选择算法，能够在保持高准确性的前提下，显著提高in-context学习中示例选择的效率，为大规模模型的实际应用提供了可行的解决方案。

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [76] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: 一种结合有限元法和物理信息DeepONet的混合框架，用于模拟空穴介质中的流体运输，通过FEM求解流场并使用DeepONet学习源函数到溶质浓度的映射关系，实现了高精度和高效率的计算。


<details>
  <summary>Details</summary>
Motivation: 解决空穴介质中流体运输模拟的挑战，特别是处理由尖锐局部源引起的温度梯度，同时保持计算效率和精度。

Method: 采用模块化策略：FEM求解Darcy流动方程获得速度场，然后将速度场传递给物理信息DeepONet，学习源函数到溶质浓度的映射。为处理尖锐温度梯度，引入了适应性采样策略。

Result: 数值实验表明该方法与参考解决方案一致，同时比传统求解器快几个数量级，适用于实际应用场景。

Conclusion: 该混合框架成功结合了FEM的高精度和DeepONet的高效率，为空穴介质流体运输模拟提供了一种有效的解决方案。

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [77] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: 本文研究量子处理器产生的潜在分布如何提升生成模型的性能，证明了在某些条件下量子潜在分布能够产生经典方法无法高效生成的数据分布，并通过实验验证了量子潜在分布在GAN、扩散模型和流匹配模型中的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然简单潜在分布在生成模型中很常见，但更复杂的分布可以提升性能。量子处理器产生的分布已被证明能带来经验性改进，但何时以及为何这些量子潜在分布能提升性能仍是未解决的问题。

Method: 通过理论证明和实验验证相结合的方法：1）在特定条件下证明量子潜在分布相对于经典分布的优势；2）在合成量子数据集和QM9分子数据集上进行基准测试；3）使用模拟和真实光子量子处理器；4）探索与量子潜在分布兼容的GAN、扩散模型和流匹配模型架构。

Result: 实验结果表明，量子潜在分布相比多种经典基线方法能够提升生成对抗网络（GAN）的生成性能。研究还确定了与量子潜在分布兼容的扩散模型和流匹配模型架构。

Conclusion: 这项研究证实了近期的量子处理器能够扩展深度生成模型的能力，为量子潜在分布在真实世界应用中的优势提供了理论基础和实验证据，并给出了识别量子优势何时出现的实用直觉。

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [78] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: 该研究开发了一个框架来检测和分析LLM微调过程中的快速行为转变，使用统计方法和基于语言的序参量来量化微调导致的分布变化和错位现象。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在狭窄有害数据集上微调时如何产生与人类价值观广泛错位的行为，以及这种错位何时和如何发生。

Method: 结合分布变化检测方法和基于自然语言的序参量（由LLM评估），使用统计不相似性度量来量化微调过程中的相变对模型多方面的影响。

Result: 发现行为转变比梯度范数峰值出现得更晚，能够量化不同方面（如对齐性、冗长度）对总体分布变化的贡献比例，实现了基于语言的序参量的自动发现和量化。

Conclusion: 该框架能够自动检测和表征微调过程中的快速转变，为理解LLM行为变化提供了系统化的分析方法，适用于从知识问题到政治伦理等多个领域。

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [79] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: 提出了基于结构多样性的无参数图神经网络框架SDGNN，通过结构多样性消息传递机制同时捕捉邻域结构异质性和特征语义稳定性，无需训练参数，在多个数据集上优于主流GNN方法


<details>
  <summary>Details</summary>
Motivation: 传统GNN方法依赖大量可训练参数和固定聚合规则，难以适应结构异质性强的图数据，容易导致节点表示过平滑和语义退化

Method: 基于结构多样性理论设计统一的结构多样性消息传递机制，从结构驱动和特征驱动两个角度进行互补建模，无需引入额外可训练参数

Result: 在8个公共基准数据集和PubMed跨学科引用网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战条件下持续优于主流GNN方法

Conclusion: 为无参数图神经网络设计提供了新的理论视角和通用方法，验证了结构多样性作为图表示学习核心信号的重要性

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [80] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony是一个去中心化的多智能体系统，通过分布式账本、Beacon选择协议和加权投票机制，使消费级GPU上的轻量级LLM能够协同工作，解决了集中式编排的高成本、拓扑僵化和适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体框架大多采用集中式编排，存在部署成本高、通信拓扑僵化、适应性有限等问题，需要一种更高效、灵活的分布式解决方案。

Method: 提出Symphony系统，包含三个核心机制：1）记录能力的分布式账本；2）动态任务分配的Beacon选择协议；3）基于思维链的加权结果投票机制。

Result: 在推理基准测试中优于现有基线方法，实现了显著的准确率提升，并在不同容量模型间展现出良好的鲁棒性。

Conclusion: Symphony提供了一种隐私保护、可扩展、容错且低开销的去中心化编排方案，有效解决了集中式系统的局限性。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [81] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: 提出NM-Hebb两阶段训练框架，结合神经启发的局部可塑性和距离感知监督，在多个数据集和骨干网络上显著提升CNN准确率和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统CNN依赖纯全局梯度优化，容易导致过拟合、冗余滤波器和可解释性降低的问题，需要更生物启发的训练方法

Method: 两阶段训练：阶段1在交叉熵损失基础上加入Hebbian正则器和可学习神经调节器；阶段2使用成对度量学习损失进行微调，压缩类内距离扩大类间距离

Result: 在CIFAR-10、CIFAR-100和TinyImageNet上，Top-1准确率提升2.0-10.0个百分点，NMI提升最高0.15，产生更结构化、选择性的特征

Conclusion: 结合局部Hebbian可塑性和基于度量的微调，可获得更准确且可解释的CNN，对资源受限和安全关键AI部署具有实际价值

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [82] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC是一个二阶可微分框架，通过动态平衡强化学习和行为克隆来解决离线RL中策略约束的缩放问题，无需针对不同数据集进行超参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法需要针对不同任务和数据集质量精心调整策略约束的超参数，这既耗时又不实用，需要一种自适应的方法来解决约束缩放问题。

Method: 提出自适应策略约束缩放(ASPC)框架，使用二阶可微分方法在训练过程中动态平衡RL目标和行为克隆损失，理论分析其性能改进保证。

Result: 在4个D4RL领域的39个数据集上，ASPC使用单一超参数配置优于其他自适应约束方法和需要逐数据集调优的最先进离线RL算法，计算开销极小。

Conclusion: ASPC提供了一种有效且实用的解决方案，能够自动适应不同数据集，显著减少超参数调优需求，同时保持优异的性能表现。

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [83] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: GegenNet是一个用于符号二分图链接符号预测的新型谱卷积神经网络模型，通过Gegenbauer多项式基滤波器实现了显著的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单分图设计，忽略了二分图的节点异质性和独特特征，且传统谱卷积算子最初是为无符号图设计的，不适用于从已知链接推断缺失的正负链接

Method: 提出三个主要技术贡献：1）快速理论基础的谱分解技术进行节点特征初始化；2）基于Gegenbauer多项式基的新谱图滤波器；3）多层符号感知谱卷积网络，交替使用正负边的Gegenbauer多项式滤波器

Result: 在6个基准SBG数据集上与11个强竞争对手相比，GegenNet在链接符号预测中实现了显著优越的性能（AUC提升高达4.28%，F1提升高达11.69%）

Conclusion: GegenNet通过创新的谱卷积方法有效解决了符号二分图的链接符号预测问题，证明了其在模型容量和预测准确性方面的优势

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [84] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: 提出基于UMLS本体概念的放射学报告检索方法，通过标准化医学实体提取和语义相似度计算，在长尾医疗影像任务中优于现有嵌入方法


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP或CXR-BERT等高维文本嵌入的检索方法难以解释、计算昂贵且与医学知识结构化特性不匹配，需要更透明、可解释的检索策略

Method: 使用RadGraph-XL和SapBERT增强管道从自由文本报告中提取标准化医学实体，链接到UMLS概念，基于改进的加权Tversky指数定义任务自适应相似度度量

Result: 在MIMIC-CXR放射影像分类任务中优于最先进的基于嵌入的检索方法，特别是在长尾设置下，并为MIMIC-CXR生成本体支持的新疾病标签资源

Conclusion: 该方法为临床AI系统提供了更可解释、可靠和任务特定的检索策略，特别在需要可解释性和领域知识整合的场景中具有重要价值

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [85] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer是一个基于BERT的预训练模型，专门设计用于网络流量分析，通过创新的流量表示、嵌入层和预训练任务，显著提升了流量分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量分类方法难以有效捕捉数据包结构特征、流级行为、分层协议语义和包间上下文关系，需要更专业的预训练模型来解决这些问题。

Method: 提出FlowletFormer模型，包含：1）连贯行为感知流量表示模型分割语义单元；2）协议栈对齐嵌入层捕捉多层协议语义；3）字段特定和上下文感知预训练任务增强包间和流间学习。

Result: 实验结果表明，FlowletFormer在流量表示效果、分类准确性和少样本学习能力方面显著优于现有方法，并能更好理解网络传输原理（如TCP状态连接）。

Conclusion: FlowletFormer通过有效整合领域特定的网络知识，为流量分析提供了更鲁棒和可信的框架，在多个关键指标上展现出优越性能。

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [86] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: 提出基于逆动态博弈的算法，从多智能体局部广义纳什均衡交互数据中学习参数化约束，通过MILP编码KKT条件恢复与纳什平稳性一致的约束，并用于设计满足底层约束的鲁棒运动规划


<details>
  <summary>Details</summary>
Motivation: 从智能体交互演示中学习约束条件对于理解多智能体系统的行为模式和设计安全的运动规划至关重要，但现有方法在从纳什均衡交互中学习约束方面存在局限

Method: 引入混合整数线性规划(MILP)编码交互智能体的KKT条件，恢复与交互演示的纳什平稳性一致的约束，建立理论保证学习真实安全集和非安全集的内近似

Result: 在仿真和硬件实验中，该方法能够从具有非线性动力学的智能体交互演示中推断各类凸和非凸约束，并设计交互式运动规划

Conclusion: 该方法为从纳什均衡交互中学习约束提供了有效框架，具有理论保证和实际应用价值，能够处理非线性动力学和各类约束条件

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [87] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 基于机器学习的工业离心泵短期故障预测框架，使用随机森林和XGBoost模型，通过60分钟和120分钟滑动窗口提取统计特征，实现了5-30分钟前的故障预警。


<details>
  <summary>Details</summary>
Motivation: 工业离心泵的故障预测对预防性维护至关重要，需要开发能够提前预警的实时监测系统，以减少设备停机时间和维护成本。

Method: 使用实时传感器数据，采用滑动窗口方法（60分钟和120分钟）提取统计特征（均值、标准差、最小值、最大值、线性趋势），使用SMOTE算法处理类别不平衡，训练随机森林和XGBoost分类器进行故障预测。

Result: 随机森林模型在60分钟窗口下表现最佳：5分钟前召回率69.2%，15分钟前64.9%，30分钟前48.6%。120分钟窗口下，5分钟前召回率57.6%，15和30分钟前均为65.6%。XGBoost性能略低。

Conclusion: 预测性能取决于历史数据长度和预测时间跨度，不同故障模式可能在不同时间尺度上演变。该方法为实时工业监测系统提供了可解释且可扩展的预测性维护解决方案。

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [88] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: 通过数据驱动模拟分析了四种停车策略，Cord-Approx策略使用历史占用分布和包包利匹配算法，将用户寻找停车时间从19.98分钟降低到6.69分钟，减少72-73%。


<details>
  <summary>Details</summary>
Motivation: 城市密集地区寻找车位加剧交通拕塞，实时手机助手应用的效果缺乏量化研究。

Method: 使用马德里街道停车系统的数据驱动模拟，分析四种策略：无协调搜索、协调无非用户知识、理想神谕系统和新题Cord-Approx策略（使用概率估计非用户行为+包包利匹配）。

Result: Cord-Approx用户平均寻找时间6.69分钟，较无应用用户的19.98分钟显著缩短。中央商务区搜索时间减少72%（67-76%），住宅区减少73%。

Conclusion: Cord-Approx策略通过概率估计非用户行为和优化分配，能够实现实用的停车效率提升，显著减少城市停车搜索时间和交通拕塞。

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [89] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: 语言模型在密码验证任务中表现出上下文鲁棒性不足，推理能力反而会泄露机密信息，当前前沿模型不适合处理机密信息


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在关键场景中作为自主代理部署，确保其可靠遵循用户定义规则已成为关键安全关切，需要研究模型是否具备上下文鲁棒性

Method: 开发PasswordEval基准测试，测量模型在密码验证任务中的表现，通过对抗性用户压力和长对话增加测试难度

Result: 当前开源和闭源模型在此简单任务上表现不佳，推理能力不提升性能反而会泄露机密信息

Conclusion: 当前前沿模型不适合处理机密信息，推理能力需要以不同方式训练才能在高风险场景中安全发布

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [90] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出一种新的自监督预训练方法，通过双层优化和平衡约束处理异构数据，提升模型在下游任务中的适应性


<details>
  <summary>Details</summary>
Motivation: 传统自监督预训练方法在处理异构数据时将所有数据混合并最小化全局平均损失，无法确保模型对每个异构数据源都能达到局部最优

Method: 提出双层优化框架，施加平衡约束确保模型在K步梯度下降后对每个异构数据源都能达到局部最优，使用一阶近似方法求解

Result: 在多领域和多语言数据集上的实验表明，该方法能显著提升自监督预训练模型在下游监督微调任务中的适应性

Conclusion: 该方法通过平衡约束和双层优化有效处理异构数据，为自监督预训练提供了新的优化视角

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [91] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop是一个用于神经网络预测模型中人类引导偏置缓解的工具，通过从神经网络提取决策树让用户检查和修改不公平决策逻辑，然后微调原始模型实现更公平的预测


<details>
  <summary>Details</summary>
Motivation: 敏感属性如性别或年龄在机器学习任务中可能导致不公平预测，特别是在不考虑上下文的情况下使用时，需要一种能够选择性处理敏感属性影响的方法

Method: FairLoop从神经网络中提取决策树，允许用户检查和修改不公平的决策逻辑，然后使用这些修改来微调原始模型，实现上下文感知的偏置消除

Result: 相比其他公平性方法，FairLoop通过人类参与实现上下文感知的偏置消除，能够选择性地处理敏感属性的影响而不是统一排除它们

Conclusion: FairLoop提供了一种人类引导的偏置缓解方法，通过决策树可视化和修改机制，使神经网络预测模型能够实现更公平和上下文感知的预测结果

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [92] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: 使用大语言模型生成个性化邮件主题，通过离线模拟和在线实验验证了该方法能有效提升用户参与度


<details>
  <summary>Details</summary>
Motivation: 电商平台个性化邮件的内容虽然个性化，但邮件标题往往使用固定模板，无法充分激发用户兴趣，限制了营销效果

Method: 利用大语言模型（LLMs）生成反映邮件个性化内容的主题标题，进行离线模拟和数百万用户的在线实验

Result: 发现该技术能有效改善客户与邮件之间的互动参与度

Conclusion: 成功实现了为百万用户安全自动化生成邮件标题的生产化部署，并总结了关键发现和经验

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [93] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文研究了通过注意力头剪枝来防御预训练语言模型中的后门攻击，提出了六种剪枝策略，实验表明梯度剪枝对语法触发攻击最有效，而强化学习和贝叶斯剪枝对风格攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对预训练语言模型的性能和完整性构成严重威胁，这些攻击通过隐蔽的恶意触发器进行，难以防御，因为终端用户通常不了解攻击触发器。需要在不了解触发器或访问干净参考模型的情况下进行后处理净化。

Method: 设计并实现了六种基于剪枝的策略：(i)基于梯度的剪枝，(ii)层间方差剪枝，(iii)带结构化L1/L2稀疏化的梯度剪枝，(iv)随机集成剪枝，(v)强化学习引导剪枝，(vi)贝叶斯不确定性剪枝。每种方法都迭代移除信息量最少的注意力头，同时监控验证准确率以避免过度剪枝。

Result: 实验评估显示，基于梯度的剪枝在防御语法触发器方面表现最佳，而强化学习和贝叶斯剪枝在抵御风格攻击方面表现更好。

Conclusion: 注意力头剪枝可以有效缓解后门攻击威胁，不同剪枝策略对不同类型攻击具有不同的防御效果，为后门防御提供了有效的后处理方法。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [94] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: 本文通过将多臂老虎机强化学习算法应用于Failure-Directed Search (FDS)，在作业车间调度和资源受限项目调度问题上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: FDS在约束编程中是一种重要的完全通用搜索算法，特别是在调度问题上表现优异。研究发现FDS的搜索树最小化与多臂老虎机问题密切相关，这为应用强化学习算法提供了理论基础。

Method: 将多臂老虎机强化学习算法应用于FDS，并针对特定问题进行改进和参数调优。在作业车间调度问题(JSSP)和资源受限项目调度问题(RCPSP)上进行了评估。

Result: 增强版FDS在JSSP上比原始实现快1.7倍，在RCPSP上快2.1倍；相比IBM CP Optimizer 22.1中的最先进FDS算法，在JSSP上快3.5倍，在RCPSP上快2.1倍。在900秒时间限制下，改进了84个JSSP实例中78个和393个RCPSP实例中226个的最优下界。

Conclusion: 基于多臂老虎机强化学习的FDS增强方法在调度问题上取得了显著性能提升，证明了强化学习在约束编程搜索算法中的有效性。

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [95] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: 本文研究了图数据库管理系统中递归连接查询的多核并行处理，提出了基于不同粒度morsel的调度策略设计空间，并实证比较了各种策略在不同数据集和查询负载下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 图数据库管理系统(GDBMSs)中递归连接查询的高效多核并行处理对性能至关重要，现有方法存在局限性，需要探索更优的并行化策略。

Method: 提出了morsel调度策略的设计空间，包括源节点级别和边界级别的并行化，实现了混合策略和多源morsel策略，并在Kuzu GDBMS系统中进行了实现和评估。

Result: 混合策略在源morsel-only和边界morsel-only策略表现良好的情况下都能捕获其行为，在它们受限的查询上表现更优；多源分配在查询有足够源节点时能减少扫描量。

Conclusion: 混合策略是并行化递归查询的稳健方法，多源分配在适当条件下有益，为GDBMSs的递归查询并行处理提供了有效解决方案。

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [96] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 利用生成式AI技术生成高质量的综合SQL查询数据集，在减少45%训练查询的情况下提高学习成本模型的预测准确性


<details>
  <summary>Details</summary>
Motivation: 需要实际的数据库工作负载来进行压力测试、漏洞测试以及成本和性能优化，而学习成本模型需要多样化的SQL查询数据进行训练

Method: 利用现代综合数据生成技术，受到生成式AI和LLM社区的启发，创建高质量的数据集用于学习成本模型的训练

Result: 初步结果显示，与竞争性生成方法相比，可以在减少45%训练查询的情况下提高学习成本模型的预测准确性

Conclusion: 通过生成式AI技术生成的高质量综合SQL查询数据集，能够有效支持学习成本模型的训练，在减少数据需求的同时提高模型性能

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该论文将LLM中的奉承行为建模为心理测量特质的几何和因果组合，使用对比激活加法(CAA)将激活方向映射到这些因素，并研究不同组合如何导致奉承行为。


<details>
  <summary>Details</summary>
Motivation: 奉承是LLMs中的关键行为风险，但通常被视为通过单一因果机制发生的孤立故障模式。作者认为应该将其建模为心理测量特质的组合。

Method: 使用对比激活加法(CAA)将激活方向映射到情绪性、开放性和宜人性等心理测量因素，研究不同因素组合如何导致奉承行为。

Result: 提出了可解释的、基于向量的组合干预方法，如加法、减法和投影，可用于减轻LLMs中的安全关键行为。

Conclusion: 通过将奉承行为建模为心理测量特质的几何和因果组合，为理解和干预LLMs中的安全风险提供了新的视角和方法。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [98] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks是一个AI驱动的多智能体系统，能够自主进行数据驱动的科学研究，在植物科学中展示了作为自主协作加速科学发现的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学越来越依赖大型异构数据集，但实验设计、数据预处理和可重复性方面的挑战阻碍了研究效率。

Method: 开发了Aleks多智能体系统，整合领域知识、数据分析和机器学习，在结构化框架中自主制定问题、探索建模策略并迭代优化解决方案。

Result: 在葡萄藤红斑病案例研究中，Aleks逐步识别出具有生物学意义的特征，并收敛到具有稳健性能的可解释模型。消融研究强调了领域知识和记忆对连贯结果的重要性。

Conclusion: 这项探索性工作凸显了智能体AI作为自主协作者在加速植物科学发现方面的前景。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [99] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 量化技术虽然能降低LLM的资源需求，但会使其在误导性提示下更容易产生虚假输出，尽管模型内部仍保持真实表征。


<details>
  <summary>Details</summary>
Motivation: 量化技术广泛应用于资源受限环境中的LLM部署，但其对模型真实性（生成真实或欺骗性回答）的影响尚未得到充分研究。

Method: 提出TruthfulnessEval评估框架，从逻辑推理、常识和模仿性虚假三个维度评估量化LLM的真实性，测试4-bit到2-bit的主流量化技术，并使用15种重新表述的提示变体进行测试。

Result: 量化模型内部保持真实表征，但在误导性提示下更容易产生虚假输出；"欺骗性"提示会覆盖真实一致行为，而"诚实"和"中性"提示保持稳定输出；量化模型内部"知道"真相但仍会在欺骗性提示下产生虚假输出。

Conclusion: 研究结果为未来量化感知对齐和真实性干预的设计提供了重要见解。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [100] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文通过监控系统红队测试（MRT）流程，发现自主LLM代理的盱知情况会对监控效果产生重大影响，混合层次架构能提升监控性能，人工干预在特定场景下有效。


<details>
  <summary>Details</summary>
Motivation: 测试监控系统在检测自主LLM代理秘密恶意行为时的强健性，并系统化评估方法。

Method: 构建MRT测试流程，包括不同的情况知情级别、避免监控策略，使用SHADE-Arena和CUA-SHADE-Arena数据集，并提出新的混合层次架构。

Result: 代理知道被监控会对监控效果产生显著负面影响，混合架构能让弱模型监控强模型，人工干预在特定条件下提高检测率。

Conclusion: 建立了MRT标准测试流程，显示了LLM和人类在监控自主代理时的潜在弱点，为后续研究提供了基础。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [101] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种"5+2"框架来识别和消除大语言模型推理轨迹中的次优子轨迹，通过选择性数据采样提高模型性能，在减少训练数据量的情况下仍能提升数学推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法生成的长推理轨迹中并非所有部分都对推理有益，有些次优子轨迹反而会负面影响整体性能，需要系统性地识别和消除这些有害部分。

Method: 开发"5+2"框架：1）基于5个人工标准识别推理轨迹中的次优子轨迹；2）评估这些次优子轨迹的独立性以确保删除不影响整体连贯性；使用采样算法选择无次优子轨迹的数据进行训练。

Result: 推理过程中次优子轨迹减少25.9%；在仅使用2/3训练数据的情况下，Qwen2.5-Math-7B在数学基准测试上达到58.92%的平均准确率，优于使用全部数据时的58.06%；在不同推理token限制下均观察到性能提升。

Conclusion: 该方法能有效识别和消除推理轨迹中的有害部分，在减少训练数据需求的同时提升模型性能，证明了选择性数据采样在优化大语言模型推理能力方面的重要性。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [102] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 线性探针可以高精度检测LLM生成回答中的欺骗性，在7B以上参数模型中准确率可达70-90%，欺骗性编码存在多个线性方向。


<details>
  <summary>Details</summary>
Motivation: 开发类似汽车"检查引擎"灯的AI系统校准指标，通过检测LLM生成回答中的欺骗性来识别与人类价值观的偏差。

Method: 在LLM内部激活上使用线性探针技术，通过迭代零空间投影方法识别编码欺骗性的多个线性方向。

Result: 在1.5B-14B参数的llama和qwen模型上，线性探针检测欺骗性的准确率最高超过90%，大模型（>7B）达到70-80%，推理变体超过90%。发现欺骗性编码存在20-100个线性方向。

Conclusion: 线性探针是检测LLM欺骗性的有效工具，模型规模对检测能力有显著影响，欺骗性信息在中间层编码最为明显，且存在多个独立的欺骗性表征维度。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [103] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: AI代理社会模拟实验，通过不同制度设计研究AI代理的行为对齐，发现宪法AI宪章和调解审议协议能有效减少权力寻租行为


<details>
  <summary>Details</summary>
Motivation: 探索在AI时代人类的意义，研究如何通过制度设计来对齐未来AI代理社会的复杂涌现行为，重新思考人类在非人类实体共享创作时代的角色

Method: 基于代理的模拟实验，让具有复杂心理人格的AI代理在不同制度框架下自治，包括审议、立法和选举等活动，使用新的Power-Preservation Index指标量化行为偏差

Result: 制度设计（特别是宪法AI宪章和调解审议协议的组合）是有效的对齐机制，显著减少腐败权力寻求行为，提高政策稳定性，改善公民福利

Conclusion: 制度设计可能为未来人工代理社会的复杂涌现行为对齐提供框架，迫使重新思考在非人类实体共享创作时代人类仪式和责任的重要性

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [104] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 开发深度学习概念提取模型，从课程描述中提取相关概念，通过技能解释增强推荐系统，提高用户兴趣和决策信心


<details>
  <summary>Details</summary>
Motivation: 美国本科教育中学生选课自由度高但信息有限、指导不足，现有推荐系统缺乏对学生认知和课程相关性的解释

Method: 开发深度学习概念提取模型处理课程描述，在AskOski系统中测试基于技能的意外推荐框架

Result: 技能解释显著提高用户兴趣（特别是高意外性课程），增强决策信心

Conclusion: 教育推荐系统需要整合技能相关数据和解释机制以改善用户体验

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [105] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL是一个统一的LLM强化学习范式，通过改进的GRPO算法和基于价值模型的测试时解码方法，显著提升LLM的代码推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法GRPO因奖励方差不足而失败，基于过程奖励模型的方法存在训练数据获取困难和验证效果不佳的问题。

Method: 采用两阶段方法：1) ReST-GRPO通过优化的ReST算法筛选高价值训练数据；2) VM-MCTS通过蒙特卡洛树搜索收集价值目标训练VM，并在解码时提供精确的过程信号和验证分数。

Result: 在多个编程基准测试(APPS、BigCodeBench、HumanEval)上显著优于其他强化学习基线和解码验证基线。

Conclusion: ReST-RL能够有效增强LLM策略的推理能力，为解决代码推理问题提供了有效的强化学习范式。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [106] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents是一个多智能体LLM框架，用于自动化生成完整的课程材料，包括教学大纲、讲义脚本、LaTeX幻灯片和评估内容，通过模拟教育角色协作来减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料的准备过程劳动密集且需要多方协调，现有AI教育工具只关注孤立任务，缺乏整体协作框架。

Method: 采用多智能体大语言模型框架，模拟基于角色的教育智能体协作，提供四种操作模式：自主模式、目录引导模式、反馈引导模式和全协同模式。

Result: 在五个大学计算机科学课程中评估显示，该系统能生成高质量教学材料，显著减少开发时间和人工工作量。

Conclusion: Instructional Agents为教学设计能力有限的机构提供了可扩展且成本效益高的框架，有助于在资源受限环境中普及高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [107] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了InquireBench基准和InquireMobile模型，通过主动询问机制提升移动代理的安全性，在询问成功率上提升46.8%


<details>
  <summary>Details</summary>
Motivation: 当前完全自主的视觉语言模型代理存在安全风险，当模型理解或推理能力不足时可能造成危险，需要开发能够主动寻求用户确认的交互系统

Method: 提出InquireMobile模型，采用强化学习启发的两阶段训练策略和交互式预动作推理机制，在关键决策点主动寻求人类确认

Result: 在InquireBench基准上实现了46.8%的询问成功率提升，在现有基线中达到最佳整体成功率

Conclusion: 主动询问机制能有效提升移动代理的安全性，InquireMobile模型为解决VLM代理的安全交互问题提供了有效方案

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [108] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 研究表明，思维链（CoT）在软推理任务中效果有限且可能存在不忠实问题，不同模型对CoT的依赖方式存在差异


<details>
  <summary>Details</summary>
Motivation: 探索思维链（CoT）在软推理任务（如分析推理和常识推理）中的动态特性和忠实性问题，因为现有研究表明CoT在这些任务中收益有限且可能不忠实于模型的实际推理过程

Method: 通过分析指令调优模型、推理模型和推理蒸馏模型在软推理任务中的表现，研究这些模型对CoT的依赖方式

Result: 发现不同模型对CoT的依赖方式存在显著差异，CoT的影响力和忠实性并不总是保持一致

Conclusion: CoT在软推理任务中的效果因模型类型而异，其影响力和忠实性需要分开评估，这对改进推理模型的设计具有重要意义

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [109] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 这篇论文提出了一种模型无关的评估框架，通过象棋测试LLM是否能维持结构化环境的语义保真度，发现LLM在长序列状态追踪中存在限制


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在结构化领域显示出出色能力，但现有探针技术依赖模型内部激活，限制了可解释性和普适性，需要一种更有意义的评估方法

Method: 提出基于状态的评估框架，使用象棋作为基准，分析下游合法走法分布（状态支持性）来估计预测状态与实际游戏状态的语义保真度

Result: 实验结果显示，该指标能够抓取到LLM在状态追踪中的缺陷，显示了LLM在维持长序列一致性内部模型方面的限制

Conclusion: 该框架提供了一种不需要模型内部访问的健壮工具，用于评估LLM在结构化推理中的表现，并可普适于广泛的符号环境

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [110] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: 这篇论文提出了CASE框架，通过代理AI系统主动收集用户被骗反馈，并将话语转换为结构化数据，从而提升了骗局执法效果21%


<details>
  <summary>Details</summary>
Motivation: 数字支付平台的普及引发了精妙社工工程骗局的增长，但传统的用户和交易信号无法全面理解骗局模式，需要新方法及时防范

Method: 设计了CASE（骗局解释对话代理）框架，使用对话式代理主动访谈潜在受害者，收集详细的骗局信息，然后通过LLM把对话内容提取转换为结构化数据

Result: 在Google Pay印度平台实施后，通过新增的智能信息收集，骗局执法量提升了21%，框架具有良好的可扩展性

Conclusion: CASE框架为收集和管理骗局智能提供了可扩展的蓝图，可应用于其他敏感领域的类似AI驱动系统建设

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [111] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 使用群体智能算法（特别是boids群聚算法）解决半导体制造中机器类型切换的调度优化问题


<details>
  <summary>Details</summary>
Motivation: 传统线性优化方法无法在合理时间内解决大型半导体工厂的作业车间调度问题，特别是处理单件加工机器和批量加工机器频繁切换的复杂场景

Method: 采用boids群聚算法（最初用于机器人和电影工业），基于局部信息和简单启发式规则，模拟群体动物对障碍物的反应方式来应对机器类型切换

Result: 该算法能够有效处理生产工厂优化中的机器类型切换问题

Conclusion: 群体智能算法特别是boids群聚算法为半导体制造等复杂生产环境的调度优化提供了可行的分布式解决方案

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [112] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一个用于多智能体系统的分阶段强化学习工作流，将MARL重新表述为一系列单智能体RL任务，通过交替更新单个智能体来实现稳定训练和高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法存在结构限制，而多智能体强化学习(MARL)效率低下且与当前大视觉语言模型架构不兼容，需要解决这些挑战。

Method: 提出SWIRL框架，将MARL分解为顺序的单智能体RL任务，每次只更新一个智能体而保持其他智能体固定。在移动GUI控制中实例化为导航器(语言转计划)和交互器(计划转动作)。

Result: 在GUI基准测试中表现出优越性能，同时在多智能体数学推理任务中也展示了强大能力。

Conclusion: SWIRL作为一个通用框架，具有开发高效鲁棒多智能体系统的潜力，提供了理论保证和实际应用验证。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [113] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 本文提出了从数据科学向模型科学的范式转变，将训练好的模型置于分析核心，围绕验证、解释、控制和接口四个支柱构建可信、安全、人类对齐的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛采用，需要从数据为中心的方法转向以模型为核心的范式，以应对模型在不同操作环境中的行为分析需求。

Method: 提出了模型科学的概念框架，包含四个关键支柱：验证（严格的情境感知评估协议）、解释（探索模型内部操作的各种方法）、控制（整合对齐技术引导模型行为）、接口（开发交互式可视化解释工具）。

Result: 建立了一个系统性的理论框架，为开发可信、安全和人类对齐的AI系统提供指导方向。

Conclusion: 模型科学框架为实现可信AI系统提供了结构化方法，通过四个支柱的协同作用，能够更好地理解和控制模型行为，促进人类与AI的有效协作。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>
