{"id": "2512.23872", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.23872", "abs": "https://arxiv.org/abs/2512.23872", "authors": ["Emily McMillon", "Kathryn Haymaker"], "title": "Hierarchical Quasi-cyclic Codes from Reed-Solomon and Polynomial Evaluation Codes", "comment": "23 pages, 5 figures, 1 table", "summary": "We introduce the first example of algebraically constructed hierarchical quasi-cyclic codes. These codes are built from Reed-Solomon codes using a 1964 construction of superimposed codes by Kautz and Singleton. We show both the number of levels in the hierarchy and the index of these Reed-Solomon derived codes are determined by the field size. We show that this property also holds for certain additional classes of polynomial evaluation codes.\n  We provide explicit code parameters and properties as well as some additional bounds on parameters such as rank and distance. In particular, starting with Reed-Solomon codes of dimension $k=2$ yields hierarchical quasi-cyclic codes with Tanner graphs of girth 6.\n  We present a table of small code parameters and note that some of these codes meet the best known minimum distance for binary codes, with the additional hierarchical quasi-cyclic structure. We draw connections to similar constructions in the literature, but importantly, while existing literature on related codes is largely simulation-based, we present a novel algebraic approach to determining new bounds on parameters of these codes."}
{"id": "2512.24039", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.24039", "abs": "https://arxiv.org/abs/2512.24039", "authors": ["Shengsong Luo", "Ruilin Wu", "Chongbin Xu", "Junjie Ma", "Xiaojun Yuan", "Xin Wang"], "title": "Continuous Angular Power Spectrum Recovery From Channel Covariance via Chebyshev Polynomials", "comment": "14 pages", "summary": "This paper proposes a Chebyshev polynomial expansion framework for the recovery of a continuous angular power spectrum (APS) from channel covariance. By exploiting the orthogonality of Chebyshev polynomials in a transformed domain, we derive an exact series representation of the covariance and reformulate the inherently ill-posed APS inversion as a finite-dimensional linear regression problem via truncation. The associated approximation error is directly controlled by the tail of the APS's Chebyshev series and decays rapidly with increasing angular smoothness. Building on this representation, we derive an exact semidefinite characterization of nonnegative APS and introduce a derivative-based regularizer that promotes smoothly varying APS profiles while preserving transitions of clusters. Simulation results show that the proposed Chebyshev-based framework yields accurate APS reconstruction, and enables reliable downlink (DL) covariance prediction from uplink (UL) measurements in a frequency division duplex (FDD) setting. These findings indicate that jointly exploiting smoothness and nonnegativity in a Chebyshev domain provides an effective tool for covariance-domain processing in multi-antenna systems."}
{"id": "2512.24087", "categories": ["cs.IT", "cs.AI", "cs.LG", "eess.SP", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.24087", "abs": "https://arxiv.org/abs/2512.24087", "authors": ["Lei Liu", "Yuhao Chi", "Shunqi Huang", "Zhaoyang Zhang"], "title": "Random Multiplexing", "comment": null, "summary": "As wireless communication applications evolve from traditional multipath environments to high-mobility scenarios like unmanned aerial vehicles, multiplexing techniques have advanced accordingly. Traditional single-carrier frequency-domain equalization (SC-FDE) and orthogonal frequency-division multiplexing (OFDM) have given way to emerging orthogonal time-frequency space (OTFS) and affine frequency-division multiplexing (AFDM). These approaches exploit specific channel structures to diagonalize or sparsify the effective channel, thereby enabling low-complexity detection. However, their reliance on these structures significantly limits their robustness in dynamic, real-world environments. To address these challenges, this paper studies a random multiplexing technique that is decoupled from the physical channels, enabling its application to arbitrary norm-bounded and spectrally convergent channel matrices. Random multiplexing achieves statistical fading-channel ergodicity for transmitted signals by constructing an equivalent input-isotropic channel matrix in the random transform domain. It guarantees the asymptotic replica MAP bit-error rate (BER) optimality of AMP-type detectors for linear systems with arbitrary norm-bounded, spectrally convergent channel matrices and signaling configurations, under the unique fixed point assumption. A low-complexity cross-domain memory AMP (CD-MAMP) detector is considered, leveraging the sparsity of the time-domain channel and the randomness of the equivalent channel. Optimal power allocations are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random multiplexing systems. The optimal coding principle and replica constrained-capacity optimality of CD-MAMP detector are investigated for random multiplexing systems. Additionally, the versatility of random multiplexing in diverse wireless applications is explored."}
{"id": "2512.24110", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.24110", "abs": "https://arxiv.org/abs/2512.24110", "authors": ["Chong Han", "Mingjie Zhu", "Wenqi Zhao", "Ziming Yu", "Guolong Huang", "Guangjian Wang", "Wen Tong", "Wenjun Zhang"], "title": "When Wires Can't Keep Up: Reconfigurable AI Data Centers Empowered by Terahertz Wireless Communications", "comment": null, "summary": "The explosive growth of artificial intelligence (AI) workloads in modern data centers demands a radical transformation of interconnect architectures. Traditional copper and optical wiring face fundamental challenges in latency, power consumption, and rigidity, constraining the scalability of distributed AI clusters. This article introduces a vision for Terahertz (THz) Wireless Data Center (THz-WDC) that combines ultra-broadband capacity, one-hop low-latency communication, and energy efficiency in the short-to-medium range (1-100m). Performance and technical requirements are first articulated, including up to 1 Tbps per link, aggregate throughput up to 10 Tbps via spatial multiplexing, sub-50 ns single-hop latency, and sub-10 pJ/bit energy efficiency over 20m. To achieve these ambitious goals, key enabling technologies are explored, including digital-twin-based orchestration, low-complexity beam manipulation technologies, all-silicon THz transceivers, and low-complexity analog baseband architectures. Moreover, as future data centers shift toward quantum and chiplet-based modular architectures, THz wireless links provide a flexible mechanism for interconnecting, testing, and reconfiguring these modules. Finally, numerical analysis is presented on the latency and power regimes of THz versus optical and copper interconnects, identifying the specific distance and throughput domains where THz links can surpass conventional wired solutions. The article concludes with a roadmap toward wireless-defined, reconfigurable, and sustainable AI data centers."}
{"id": "2512.23781", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.23781", "abs": "https://arxiv.org/abs/2512.23781", "authors": ["Jackie Baek", "Will Ma", "Dmitry Mitrofanov"], "title": "Personalized Promotions in Practice: Dynamic Allocation and Reference Effects", "comment": null, "summary": "Partnering with a large online retailer, we consider the problem of sending daily personalized promotions to a userbase of over 20 million customers. We propose an efficient policy for determining, every day, the promotion that each customer should receive (10%, 12%, 15%, 17%, or 20% off), while respecting global allocation constraints. This policy was successfully deployed to see a 4.5% revenue increase during an A/B test, by better targeting promotion-sensitive customers and also learning intertemporal patterns across customers.\n  We also consider theoretically modeling the intertemporal state of the customer. The data suggests a simple new combinatorial model of pricing with reference effects, where the customer remembers the best promotion they saw over the past $\\ell$ days as the \"reference value\", and is more likely to purchase if this value is poor. We tightly characterize the structure of optimal policies for maximizing long-run average revenue under this model -- they cycle between offering poor promotion values $\\ell$ times and offering good values once."}
{"id": "2512.23756", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.23756", "abs": "https://arxiv.org/abs/2512.23756", "authors": ["Pierre Mackenzie"], "title": "Sparse Random Matrices for Dimensionality Reduction", "comment": null, "summary": "The Johnson-Lindenstrauss (JL) theorem states that a set of points in high-dimensional space can be embedded into a lower-dimensional space while approximately preserving pairwise distances with high probability Johnson and Lindenstrauss (1984). The standard JL theorem uses dense random matrices with Gaussian entries. However, for some applications, sparse random matrices are preferred as they allow for faster matrix-vector multiplication. I outline the constructions and proofs introduced by Achlioptas (2003) and the contemporary standard by Kane and Nelson (2014). Further, I implement and empirically compare these sparse constructions with standard Gaussian JL matrices."}
{"id": "2512.23961", "categories": ["cs.IR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23961", "abs": "https://arxiv.org/abs/2512.23961", "authors": ["Junjie H. Xu"], "title": "An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System", "comment": "5 pages, 1 figure", "summary": "This research presents a cutting-edge recommendation system utilizing agentic AI for KYC (Know Your Customer in the financial domain), and its evaluation across five distinct content verticals: Advertising (Ad), News, Gossip, Sharing (User-Generated Content), and Technology (Tech). The study compares the performance of four experimental groups, grouping by the intense usage of KYC, benchmarking them against the Normalized Discounted Cumulative Gain (nDCG) metric at truncation levels of $k=1$, $k=3$, and $k=5$. By synthesizing experimental data with theoretical frameworks and industry benchmarks from platforms such as Baidu and Xiaohongshu, this research provides insight by showing experimental results for engineering a large-scale agentic recommendation system."}
{"id": "2512.23925", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23925", "abs": "https://arxiv.org/abs/2512.23925", "authors": ["Amir Shaikhha"], "title": "Hojabr: Towards a Theory of Everything for AI and Data Analytics", "comment": null, "summary": "Modern data analytics pipelines increasingly combine relational queries, graph processing, and tensor computation within a single application, but existing systems remain fragmented across paradigms, execution models, and research communities. This fragmentation results in repeated optimization efforts, limited interoperability, and strict separation between logical abstractions and physical execution strategies.\n  We propose Hojabr as a unified declarative intermediate language to address this problem. Hojabr integrates relational algebra, tensor algebra, and constraint-based reasoning within a single higher-order algebraic framework, in which joins, aggregations, tensor contractions, and recursive computations are expressed uniformly. Physical choices, such as join algorithms, execution models, and sparse versus dense tensor representations, are handled as constraint-specialization decisions rather than as separate formalisms. Hojabr supports bidirectional translation with existing declarative languages, enabling programs to be both lowered into Hojabr for analysis and optimization and lifted back into their original declarative form. By making semantic, structural, and algebraic properties explicit, and by supporting extensibility across the compilation stack, Hojabr enables systematic reasoning and reuse of optimization techniques across database systems, machine learning frameworks, and compiler infrastructures."}
{"id": "2512.24217", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.24217", "abs": "https://arxiv.org/abs/2512.24217", "authors": ["Runtian Zhu", "Lingfei Jin"], "title": "Efficient Decoding of Twisted GRS Codes and Roth--Lempel Codes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "MDS codes play a central role in practice due to their broad applications. To date, most known MDS codes are generalized Reed-Solomon (GRS) codes, leaving codes that are not equivalent to GRS codes comparatively less understood. Studying this non-GRS regime is therefore of intrinsic theoretical interest, and is also practically relevant since the strong algebraic structure of GRS codes can be undesirable in cryptographic settings. Among the known non-GRS codes, twisted generalized Reed-Solomon (TGRS) codes and Roth-Lempel codes are two representative families of non-GRS codes that have attracted significant attention. Though substantial work has been devoted to the construction and structural analysis of TGRS and Roth-Lempel codes, comparatively little attention has been paid to their decoding, and many problems remain open. In this paper, we propose list and unique decoding algorithms for TGRS codes and Roth-Lempel codes based on the Guruswami-Sudan algorithm. Under suitable parameter conditions, our algorithms achieve near-linear running time in the code length, improving upon the previously best-known quadratic-time complexity. Our TGRS decoder supports fixed-rate TGRS codes with up to O(n^2) twists, substantially extending prior work that only handled the single-twist case. For Roth-Lempel codes, we provide what appears to be the first efficient decoder. Moreover, our list decoders surpass the classical unique-decoding radius for a broad range of parameters. Finally, we incorporate algebraic manipulation detection (AMD) codes into the list-decoding framework, enabling recovery of the correct message from the output list with high probability."}
{"id": "2512.24105", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24105", "abs": "https://arxiv.org/abs/2512.24105", "authors": ["Maxime Lucet", "Nawal Benabbou", "Aurélie Beynier", "Nicolas Maudet"], "title": "Multilevel Fair Allocation", "comment": null, "summary": "We introduce the concept of multilevel fair allocation of resources with tree-structured hierarchical relations among agents. While at each level it is possible to consider the problem locally as an allocation of an agent to its children, the multilevel allocation can be seen as a trace capturing the fact that the process is iterated until the leaves of the tree. In principle, each intermediary node may have its own local allocation mechanism. The main challenge is then to design algorithms which can retain good fairness and efficiency properties. In this paper we propose two original algorithms under the assumption that leaves of the tree have matroid-rank utility functions and the utility of any internal node is the sum of the utilities of its children. The first one is a generic polynomial-time sequential algorithm that comes with theoretical guarantees in terms of efficiency and fairness. It operates in a top-down fashion -- as commonly observed in real-world applications -- and is compatible with various local algorithms. The second one extends the recently proposed General Yankee Swap to the multilevel setting. This extension comes with efficiency guarantees only, but we show that it preserves excellent fairness properties in practice."}
{"id": "2512.24037", "categories": ["cs.DS", "cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.24037", "abs": "https://arxiv.org/abs/2512.24037", "authors": ["Aritra Banik", "Sujoy Bhore", "Palash Dey", "Abhishek Sahu"], "title": "Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds", "comment": "Accepted as a full paper in AAMAS 2026", "summary": "The kidney exchange mechanism allows many patient-donor pairs who are otherwise incompatible with each other to come together and exchange kidneys along a cycle. However, due to infrastructure and legal constraints, kidney exchange can only be performed in small cycles in practice. In reality, there are also some altruistic donors who do not have any paired patients. This allows us to also perform kidney exchange along paths that start from some altruistic donor. Unfortunately, the computational task is NP-complete. To overcome this computational barrier, an important line of research focuses on designing faster algorithms, both exact and using the framework of parameterized complexity.\n  The standard parameter for the kidney exchange problem is the number $t$ of patients that receive a healthy kidney. The current fastest known deterministic FPT algorithm for this problem, parameterized by $t$, is $O^\\star\\left(14^t\\right)$. In this work, we improve this by presenting a deterministic FPT algorithm that runs in time $O^\\star\\left((4e)^t\\right)\\approx O^\\star\\left(10.88^t\\right)$. This problem is also known to be W[1]-hard parameterized by the treewidth of the underlying undirected graph. A natural question here is whether the kidney exchange problem admits an FPT algorithm parameterized by the pathwidth of the underlying undirected graph. We answer this negatively in this paper by proving that this problem is W[1]-hard parameterized by the pathwidth of the underlying undirected graph. We also present some parameterized intractability results improving the current understanding of the problem under the framework of parameterized complexity."}
{"id": "2512.24246", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24246", "abs": "https://arxiv.org/abs/2512.24246", "authors": ["Jie Luo", "Wenyu Zhang", "Xinming Zhang", "Yuan Fang"], "title": "Time-Aware Adaptive Side Information Fusion for Sequential Recommendation", "comment": "10 pages. Accepted by WSDM'26", "summary": "Incorporating item-side information, such as category and brand, into sequential recommendation is a well-established and effective approach for improving performance. However, despite significant advancements, current models are generally limited by three key challenges: they often overlook the fine-grained temporal dynamics inherent in timestamps, exhibit vulnerability to noise in user interaction sequences, and rely on computationally expensive fusion architectures. To systematically address these challenges, we propose the Time-Aware Adaptive Side Information Fusion (TASIF) framework. TASIF integrates three synergistic components: (1) a simple, plug-and-play time span partitioning mechanism to capture global temporal patterns; (2) an adaptive frequency filter that leverages a learnable gate to denoise feature sequences adaptively, thereby providing higher-quality inputs for subsequent fusion modules; and (3) an efficient adaptive side information fusion layer, this layer employs a \"guide-not-mix\" architecture, where attributes guide the attention mechanism without being mixed into the content-representing item embeddings, ensuring deep interaction while ensuring computational efficiency. Extensive experiments on four public datasets demonstrate that TASIF significantly outperforms state-of-the-art baselines while maintaining excellent efficiency in training. Our source code is available at https://github.com/jluo00/TASIF."}
{"id": "2512.24078", "categories": ["cs.DB", "cs.CG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24078", "abs": "https://arxiv.org/abs/2512.24078", "authors": ["Junyu Liao", "Ashwin Lall", "Mitsunori Ogihara", "Raymond Wong"], "title": "High-dimensional Regret Minimization", "comment": null, "summary": "Multi-criteria decision making in large databases is very important in real world applications. Recently, an interactive query has been studied extensively in the database literature with the advantage of both the top-k query (with limited output size) and the skyline query (which does not require users to explicitly specify their preference function). This approach iteratively asks the user to select the one preferred within a set of options. Based on rounds of feedback, the query learns the implicit preference and returns the most favorable as a recommendation.\n  However, many modern applications in areas like housing or financial product markets feature datasets with hundreds of attributes. Existing interactive algorithms either fail to scale or require excessive user interactions (often exceeding 1000 rounds). Motivated by this, we propose FHDR (Fast High-Dimensional Reduction), a novel framework that takes less than 0.01s with fewer than 30 rounds of interaction. It is considered a breakthrough in the field of interactive queries since most, if not all, existing studies are not scalable to high-dimensional datasets.\n  Extensive experiments demonstrate that FHDR outperforms the best-known algorithms by at least an order of magnitude in execution time and up to several orders of magnitude in terms of the number of interactions required, establishing a new state of the art for scalable interactive regret minimization."}
{"id": "2512.24232", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.24232", "abs": "https://arxiv.org/abs/2512.24232", "authors": ["Jiaxin Lyu", "Guanghui He"], "title": "SC-LDPC Codes Over $\\mathbb{F}_q$: Minimum Distance, Decoding Analysis and Threshold Saturation", "comment": null, "summary": "We investigate random spatially coupled low-density parity-check (SC-LDPC) code ensembles over finite fields. Under different variable-node edge-spreading rules, the random Tanner graphs of several coupled ensembles are defined by multiple independent, uniformly random monomial maps. The two main coupled ensembles considered are referred to as the standard coupled ensemble and the improved coupled ensemble. We prove that both coupled ensembles exhibit asymptotically good minimum distance and minimum stopping set size. Theoretical and numerical results show that the improved coupled ensemble can achieve better distance performance than the standard coupled ensemble. We introduce the essential preliminaries and analytical tools needed to analyze the iterative decoding threshold of coupled ensembles over any finite field. We consider a class of memoryless channels with special symmetry, termed q-ary input memoryless symmetric channels (QMSCs), and show that, for these channels, the distribution of channel messages (in form of probability vectors) likewise exhibits this symmetry. Consequently, we define symmetric probability measures and their reference measures on a finite-dimensional probability simplex, analyze their foundational properties and those of their linear functionals, endow their respective spaces with metric topologies, and conduct an in-depth study of their degradation theory. Based on our analytical framework, we establish a universal threshold saturation result for both of the coupled ensembles over a q-ary finite field on QMSCs. Specifically, as the coupling parameters increase, the belief-propagation threshold of a coupled system saturates to a well-defined threshold that depends only on the underlying ensemble and the channel family."}
{"id": "2512.24467", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.24467", "abs": "https://arxiv.org/abs/2512.24467", "authors": ["Ulle Endriss"], "title": "On the Difficulty of Measuring Divisiveness of Proposals under Ranked Preferences", "comment": null, "summary": "Given the stated preferences of several people over a number of proposals regarding public policy initiatives, some of those proposals might be judged to be more ``divisive'' than others. When designing online participatory platforms to support digital democracy initiatives enabling citizens to deliberate over such proposals, we might wish to equip those platforms with the functionality to retrieve the most divisive proposals currently under discussion. Such a service would be useful for analysing the progress of deliberation and steering discussion towards issues that still require further debate. Guided by this use case, we explore possibilities for providing a clear definition of what it means to select a set of most divisive proposals on the basis of people's stated preferences over proposals. Then, employing the axiomatic method familiar from social choice theory, we show that the task of selecting the most divisive proposals in a manner that satisfies certain seemingly mild normative requirements faces a number of fundamental difficulties."}
{"id": "2512.24355", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.24355", "abs": "https://arxiv.org/abs/2512.24355", "authors": ["Julia Chuzhoy", "Ron Mosenzon", "Ohad Trabelsi"], "title": "Faster Algorithms for Global Minimum Vertex-Cut in Directed Graphs", "comment": "122 pages, 0 figures, to be published in SODA2026", "summary": "We study the directed global minimum vertex-cut problem: given a directed vertex-weighted graph $G$, compute a vertex-cut $(L,S,R)$ in $G$ of minimum value, which is defined to be the total weight of all vertices in $S$. The problem, together with its edge-based variant, is one of the most basic in graph theory and algorithms, and has been studied extensively. The fastest currently known algorithm for directed global minimum vertex-cut (Henzinger, Rao and Gabow, FOCS 1996 and J. Algorithms 2000) has running time $\\tilde{O}(mn)$, where $m$ and $n$ denote the number of edges and vertices in the input graph, respectively. A long line of work over the past decades led to faster algorithms for other main versions of the problem, including the undirected edge-based setting (Karger, STOC 1996 and J. ACM 2000), directed edge-based setting (Cen et al., FOCS 2021), and undirected vertex-based setting (Chuzhoy and Trabelsi, STOC 2025). However, for the vertex-based version in directed graphs, the 29 year-old $\\tilde{O}(mn)$-time algorithm of Henzinger, Rao and Gabow remains the state of the art to this day, in all edge-density regimes. In this paper we break the $Θ(mn)$ running time barrier for the first time, by providing a randomized algorithm for directed global minimum vertex-cut, with running time $O\\left(mn^{0.976}\\cdot\\operatorname{polylog} W\\right)$ where $W$ is the ratio of largest to smallest vertex weight. Additionally, we provide a randomized $O\\left(\\min\\left\\{m^{1+o(1)}\\cdot k,n^{2+o(1)}\\right\\}\\right)$-time algorithm for the unweighted version of directed global minimum vertex-cut, where $k$ is the value of the optimal solution. The best previous algorithm for the problem achieved running time $\\tilde O\\left(\\min\\left\\{k^2 \\cdot m, mn^{11/12+o(1)}, n^{2+o(1)}\\right\\}\\right)$ (Forster et al., SODA 2020, Li et al., STOC 2021)."}
{"id": "2512.24268", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24268", "abs": "https://arxiv.org/abs/2512.24268", "authors": ["Pankayaraj Pathmanathan", "Michael-Andrei Panaitescu-Liess", "Cho-Yu Jason Chiang", "Furong Huang"], "title": "RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation", "comment": "Published at AAAI 2026 Workshop on New Frontiers in Information Retrieval [Oral]", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments."}
{"id": "2512.24824", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.24824", "abs": "https://arxiv.org/abs/2512.24824", "authors": ["Yuzhen Chen", "Bin Yao"], "title": "LMG Index: A Robust Learned Index for Multi-Dimensional Performance Balance", "comment": null, "summary": "Index structures are fundamental for efficient query processing on large-scale datasets. Learned indexes model the indexing process as a prediction problem to overcome the inherent trade-offs of traditional indexes. However, most existing learned indexes optimize only for limited objectives like query latency or space usage, neglecting other practical evaluation dimensions such as update efficiency and stability. Moreover, many learned indexes rely on assumptions about data distributions or workloads, lacking theoretical guarantees when facing unknown or evolving scenarios, which limits their generality in real-world systems.\n  In this paper, we propose LMIndex, a robust framework for learned indexing that leverages a efficient query/update top-layer structure (theoretically $O(1)$ when the key type is fixed) and a efficient optimal error threshold training algorithm (approach $O(1)$ in practice). Building upon this, we develop LMG (LMIndex with gaps), a variant employing a novel gap allocation strategy to enhance update performance and maintain stability under dynamic workloads. Extensive evaluations show that LMG achieves competitive or leading performance, including bulk loading (up to 8.25$\\times$ faster), point queries (up to 1.49$\\times$ faster), range queries (up to 4.02$\\times$ faster than B+Tree), update (up to 1.5$\\times$ faster on read-write workloads), stability (up to 82.59$\\times$ lower coefficient of variation), and space usage (up to 1.38$\\times$ smaller). These results demonstrate that LMG effectively breaks the multi-dimensional performance trade-offs inherent in state-of-the-art approaches, offering a balanced and versatile framework."}
{"id": "2512.24468", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.24468", "abs": "https://arxiv.org/abs/2512.24468", "authors": ["Augustin Cosse"], "title": "Infinite families of graphs and stable completion of arbitrary matrices, Part I", "comment": null, "summary": "We study deterministic constructions of graphs for which the unique completion of low rank matrices is generically possible regardless of the values of the entries. We relate the completability to the presence of some patterns (particular unions of self-avoiding walks) in the subgraph of the lattice graph generated from the support of the bi-adjacency matrix. The construction makes it possible to design infinite families of graphs on which exact and stable completion is possible for every fixed rank matrix through the sum-of-squares hierarchy."}
{"id": "2512.24934", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.24934", "abs": "https://arxiv.org/abs/2512.24934", "authors": ["Ameet Gadekar", "Aristides Gionis", "Suhas Thejaswi", "Sijing Tu"], "title": "Fair Committee Selection under Ordinal Preferences and Limited Cardinal Information", "comment": "AAMAS EA'26", "summary": "We study the problem of fair $k$-committee selection under an egalitarian objective. Given $n$ agents partitioned into $m$ groups (\\eg, demographic quotas), the goal is to aggregate their preferences to form a committee of size $k$ that guarantees minimum representation from each group while minimizing the maximum \\emph{cost} incurred by any agent. We model this setting as the ordinal fair $k$-center problem, where agents are embedded in an unknown metric space, and each agent reports a complete preference ranking (i.e., ordinal information) over all agents, consistent with the underlying distance metric (i.e., cardinal information). The cost incurred by an agent with respect to a committee is defined as its distance to the closest committee member. The quality of an algorithm is evaluated using the notion of distortion, which measures the worst-case ratio between the cost of the committee produced by the algorithm and the cost of an optimal committee, when given complete access to the underlying metric space.\n  When cardinal information is not available, no constant distortion is possible for the ordinal $k$-center problem, even without fairness constraints, when $k\\geq 3$ [Burkhardt et.al., AAAI'24]. To overcome this hardness, we allow limited access to cardinal information by querying the metric space. In this setting, our main contribution is a factor-$5$ distortion algorithm that requires only $O(k \\log^2 k)$ queries. Along the way, we present an improved factor-$3$ distortion algorithm using $O(k^2)$ queries."}
{"id": "2512.24325", "categories": ["cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.24325", "abs": "https://arxiv.org/abs/2512.24325", "authors": ["Wan Jiang", "Xinyi Zang", "Yudong Zhao", "Yusi Zou", "Yunfei Lu", "Junbo Tong", "Yang Liu", "Ming Li", "Jiani Shi", "Xin Yang"], "title": "MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems", "comment": "12 pages, 5 figures", "summary": "Modern recommender systems face significant computational challenges due to growing model complexity and traffic scale, making efficient computation allocation critical for maximizing business revenue. Existing approaches typically simplify multi-stage computation resource allocation, neglecting inter-stage dependencies, thus limiting global optimality. In this paper, we propose MaRCA, a multi-agent reinforcement learning framework for end-to-end computation resource allocation in large-scale recommender systems. MaRCA models the stages of a recommender system as cooperative agents, using Centralized Training with Decentralized Execution (CTDE) to optimize revenue under computation resource constraints. We introduce an AutoBucket TestBench for accurate computation cost estimation, and a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off accordingly. Since its end-to-end deployment in the advertising pipeline of a leading global e-commerce platform in November 2024, MaRCA has consistently handled hundreds of billions of ad requests per day and has delivered a 16.67% revenue uplift using existing computation resources."}
{"id": "2512.24773", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.24773", "abs": "https://arxiv.org/abs/2512.24773", "authors": ["Anas K. Saeed", "Mahmoud M. Salim", "Ali Arshad Nasir", "Ali H. Muqaibel"], "title": "Throughput Optimization in UAV-Mounted RIS under Jittering and Imperfect CSI via DRL", "comment": null, "summary": "Reconfigurable intelligent surfaces (RISs) mounted on unmanned aerial vehicles (UAVs) can reshape wireless propagation on-demand. However, their performance is sensitive to UAV jitter and cascaded channel uncertainty. This paper investigates a downlink multiple-input single-output UAV-mounted RIS system in which a ground multiple-antenna base station (BS) serves multiple single-antenna users under practical impairments. Our goal is to maximize the expected throughput under stochastic three-dimensional UAV jitter and imperfect cascaded channel state information (CSI) based only on the available channel estimates. This leads to a stochastic nonconvex optimization problem subject to a BS transmit power constraint and strict unit-modulus constraints on all RIS elements. To address this problem, we design a model-free deep reinforcement learning (DRL) framework with a contextual bandit formulation. A differentiable feasibility layer is utilized to map continuous actions to feasible solutions, while the reward is a Monte Carlo estimate of the expected throughput. We instantiate this framework with constrained variants of deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3) that do not use target networks. Simulations show that the proposed algorithms yield higher throughput than conventional alternating optimization-based weighted minimum mean-square error (AO-WMMSE) baselines under severe jitter and low CSI quality. Across different scenarios, the proposed methods achieve performance that is either comparable to or slightly below the AO-WMMSE benchmark, based on sample average approximation (SAA) with a relative gap ranging from 0-12%. Moreover, the proposed DRL controllers achieve online inference times of 0.6 ms per decision versus roughly 370-550 ms for AO-WMMSE solvers."}
{"id": "2512.25016", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.25016", "abs": "https://arxiv.org/abs/2512.25016", "authors": ["Gabriel Siqueira", "Alexsandro Oliveira Alexandrino", "Zanoni Dias"], "title": "Approximations for the Weighted Reversal, Transposition, and Indel Distance Problem with Intergenic Region Information", "comment": null, "summary": "Genome rearrangement distances are an established method in genome comparison. Works in this area may include various rearrangement operations representing large-scale mutations, gene orientation information, the number of nucleotides in intergenic regions, and weights reflecting the expected frequency of each operation. In this article, we model genomes containing at most one copy of each gene by considering gene sequences, with orientations, and representing intergenic regions according to their nucleotide lengths. We looked at a problem called Weighted Reversal, Transposition, and Indel Distance, which seeks the minimal cost sequence composed by the rearrangement operations of reversals, transposition, and indels, capable of transforming one genome into another. We leverage a structure called Labeled Intergenic Breakpoint Graph to show an algorithm for that problem with guaranteed approximations considering some sets of weights for the operations."}
{"id": "2512.24366", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24366", "abs": "https://arxiv.org/abs/2512.24366", "authors": ["Ben Kabongo", "Vincent Guigue"], "title": "On the Factual Consistency of Text-based Explainable Recommendation Models", "comment": "13 pages, 2 figures, 4 tables", "summary": "Text-based explainable recommendation aims to generate natural-language explanations that justify item recommendations, to improve user trust and system transparency. Although recent advances leverage LLMs to produce fluent outputs, a critical question remains underexplored: are these explanations factually consistent with the available evidence? We introduce a comprehensive framework for evaluating the factual consistency of text-based explainable recommenders. We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a ground truth that isolates and focuses on their factual content. Applying this pipeline to five categories from the Amazon Reviews dataset, we create augmented benchmarks for fine-grained evaluation of explanation quality. We further propose statement-level alignment metrics that combine LLM- and NLI-based approaches to assess both factual consistency and relevance of generated explanations. Across extensive experiments on six state-of-the-art explainable recommendation models, we uncover a critical gap: while models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), all our factuality metrics reveal alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%). These findings underscore the need for factuality-aware evaluation in explainable recommendation and provide a foundation for developing more trustworthy explanation systems."}
{"id": "2512.25020", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.25020", "abs": "https://arxiv.org/abs/2512.25020", "authors": ["Danny Hermelin", "Danny Segev", "Dvir Shabtay"], "title": "Approximation Algorithms for Fair Repetitive Scheduling", "comment": null, "summary": "We consider a recently introduced fair repetitive scheduling problem involving a set of clients, each asking for their associated job to be daily scheduled on a single machine across a finite planning horizon. The goal is to determine a job processing permutation for each day, aiming to minimize the maximum total completion time experienced by any client. This problem is known to be NP-hard for quite restrictive settings, with previous work offering exact solution methods for highly-structured special cases.\n  In this paper, we focus on the design of approximation algorithms with provable performance guarantees. Our main contributions can be briefly summarized as follows:\n  (i) When job processing times are day-dependent, we devise a polynomial-time LP-based $2$-approximation, as well as a polynomial-time approximation scheme for a constant number of days.\n  (ii) With day-invariant processing times, we obtain a surprisingly simple $(\\frac{1+\\sqrt{2}}{2}+ε)$-approximation in polynomial time. This setting is also shown to admit a quasi-polynomial-time approximation scheme for an arbitrary number of days.\n  The key technical component driving our approximation schemes is a novel batching technique, where jobs are conceptually grouped into batches, subsequently leading either to a low-dimensional dynamic program or to a compact configuration LP. Concurrently, while developing our constant-factor approximations, we propose a host of lower-bounding mechanisms that may be of broader interest."}
{"id": "2512.24711", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24711", "abs": "https://arxiv.org/abs/2512.24711", "authors": ["Kangyang Luo", "Shuzheng Si", "Yuzhuo Bai", "Cheng Gao", "Zhitong Wang", "Cheng Huang", "Yingli Shen", "Yufeng Han", "Wenhao Li", "Cunliang Kong", "Maosong Sun"], "title": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints", "comment": null, "summary": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints."}
{"id": "2512.25033", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.25033", "abs": "https://arxiv.org/abs/2512.25033", "authors": ["Sotiris Kanellopoulos", "Edouard Nemery", "Christos Pergaminelis", "Minas Marios Sotiriou", "Manolis Vasilakis"], "title": "EF(X) Orientations: A Parameterized Complexity Perspective", "comment": null, "summary": "The concept of fair orientations in graphs was introduced by Christodoulou, Fiat, Koutsoupias, and Sgouritsa in 2023, naturally modeling fair division scenarios in which resources are only contested by neighbors. In this model, vertices represent agents and undirected edges represent goods; edges have to be oriented towards one of their endpoints, i.e., allocated to one of their adjacent agents. Although EFX orientations (envy-free up to any good) have been extensively studied in this setting, EF orientations (envy-free) remain unexplored. In this work, we initiate their study, mostly under the lens of parameterized complexity, presenting various tractable cases, hardness results, and parameterizations. Our results concern both simple graphs and multigraphs. Interestingly, many of our results transfer to EFX orientations, thus complementing and improving upon previous work; notably, we answer an open question regarding the structural parameterized complexity of the latter problem on graphs of polynomially-bounded valuations. We also show that EF orientations are tractable in cases in which EFX orientations are not, particularly for binary valuations. Lastly, we consider charity in the orientation setting, establishing algorithms for finding the minimum amount of edges that have to be removed from a graph in order for EF(X) orientations to exist."}
{"id": "2512.24715", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24715", "abs": "https://arxiv.org/abs/2512.24715", "authors": ["Kang Fu", "Honglei Zhang", "Xuechao Zou", "Yidong Li"], "title": "MDiffFR: Modality-Guided Diffusion Generation for Cold-start Items in Federated Recommendation", "comment": null, "summary": "Federated recommendations (FRs) provide personalized services while preserving user privacy by keeping user data on local clients, which has attracted significant attention in recent years. However, due to the strict privacy constraints inherent in FRs, access to user-item interaction data and user profiles across clients is highly restricted, making it difficult to learn globally effective representations for new (cold-start) items. Consequently, the item cold-start problem becomes even more challenging in FRs. Existing solutions typically predict embeddings for new items through the attribute-to-embedding mapping paradigm, which establishes a fixed one-to-one correspondence between item attributes and their embeddings. However, this one-to-one mapping paradigm often fails to model varying data distributions and tends to cause embedding misalignment, as verified by our empirical studies. To this end, we propose MDiffFR, a novel generation-based modality-guided diffusion method for cold-start items in FRs. In this framework, we employ a tailored diffusion model on the server to generate embeddings for new items, which are then distributed to clients for cold-start inference. To align item semantics, we deploy a pre-trained modality encoder to extract modality features as conditional signals to guide the reverse denoising process. Furthermore, our theoretical analysis verifies that the proposed method achieves stronger privacy guarantees compared to existing mapping-based approaches. Extensive experiments on four real datasets demonstrate that our method consistently outperforms all baselines in FRs."}
{"id": "2512.24762", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24762", "abs": "https://arxiv.org/abs/2512.24762", "authors": ["Guorui Zhou", "Honghui Bao", "Jiaming Huang", "Jiaxin Deng", "Jinghao Zhang", "Junda She", "Kuo Cai", "Lejian Ren", "Lu Ren", "Qiang Luo", "Qianqian Wang", "Qigen Hu", "Rongzhou Zhang", "Ruiming Tang", "Shiyao Wang", "Wuchao Li", "Xiangyu Wu", "Xinchen Luo", "Xingmei Wang", "Yifei Hu", "Yunfan Wu", "Zhanyu Liu", "Zhiyang Zhang", "Zixing Zhang", "Bo Chen", "Bin Wen", "Chaoyi Ma", "Chengru Song", "Chenglong Chu", "Defu Lian", "Fan Yang", "Feng Jiang", "Hongtao Cheng", "Huanjie Wang", "Kun Gai", "Pengfei Zheng", "Qiang Wang", "Rui Huang", "Siyang Mao", "Tingting Gao", "Wei Yuan", "Yan Wang", "Yang Zhou", "Yi Su", "Zexuan Cheng", "Zhixin Ling", "Ziming Li"], "title": "OpenOneRec Technical Report", "comment": null, "summary": "While the OneRec series has successfully unified the fragmented recommendation pipeline into an end-to-end generative framework, a significant gap remains between recommendation systems and general intelligence. Constrained by isolated data, they operate as domain specialists-proficient in pattern matching but lacking world knowledge, reasoning capabilities, and instruction following. This limitation is further compounded by the lack of a holistic benchmark to evaluate such integrated capabilities. To address this, our contributions are: 1) RecIF Bench & Open Data: We propose RecIF-Bench, a holistic benchmark covering 8 diverse tasks that thoroughly evaluate capabilities from fundamental prediction to complex reasoning. Concurrently, we release a massive training dataset comprising 96 million interactions from 160,000 users to facilitate reproducible research. 2) Framework & Scaling: To ensure full reproducibility, we open-source our comprehensive training pipeline, encompassing data processing, co-pretraining, and post-training. Leveraging this framework, we demonstrate that recommendation capabilities can scale predictably while mitigating catastrophic forgetting of general knowledge. 3) OneRec-Foundation: We release OneRec Foundation (1.7B and 8B), a family of models establishing new state-of-the-art (SOTA) results across all tasks in RecIF-Bench. Furthermore, when transferred to the Amazon benchmark, our models surpass the strongest baselines with an average 26.8% improvement in Recall@10 across 10 diverse datasets (Figure 1). This work marks a step towards building truly intelligent recommender systems. Nonetheless, realizing this vision presents significant technical and theoretical challenges, highlighting the need for broader research engagement in this promising direction."}
{"id": "2512.24787", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24787", "abs": "https://arxiv.org/abs/2512.24787", "authors": ["Yunsheng Pang", "Zijian Liu", "Yudong Li", "Shaojie Zhu", "Zijian Luo", "Chenyun Yu", "Sikai Wu", "Shichen Shen", "Cong Xu", "Bin Wang", "Kai Jiang", "Hongyong Yu", "Chengxiang Zhuo", "Zang Li"], "title": "HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment", "comment": null, "summary": "Slate recommendation, where users are presented with a ranked list of items simultaneously, is widely adopted in online platforms. Recent advances in generative models have shown promise in slate recommendation by modeling sequences of discrete semantic IDs autoregressively. However, existing autoregressive approaches suffer from semantically entangled item tokenization and inefficient sequential decoding that lacks holistic slate planning. To address these limitations, we propose HiGR, an efficient generative slate recommendation framework that integrates hierarchical planning with listwise preference alignment. First, we propose an auto-encoder utilizing residual quantization and contrastive constraints to tokenize items into semantically structured IDs for controllable generation. Second, HiGR decouples generation into a list-level planning stage for global slate intent, followed by an item-level decoding stage for specific item selection. Third, we introduce a listwise preference alignment objective to directly optimize slate quality using implicit user feedback. Experiments on our large-scale commercial media platform demonstrate that HiGR delivers consistent improvements in both offline evaluations and online deployment. Specifically, it outperforms state-of-the-art methods by over 10% in offline recommendation quality with a 5x inference speedup, while further achieving a 1.22% and 1.73% increase in Average Watch Time and Average Video Views in online A/B tests."}
{"id": "2512.24943", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24943", "abs": "https://arxiv.org/abs/2512.24943", "authors": ["Chenji Lu", "Zhuo Chen", "Hui Zhao", "Zhenyi Wang", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment", "comment": null, "summary": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation."}
{"id": "2512.24078", "categories": ["cs.DB", "cs.CG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.24078", "abs": "https://arxiv.org/abs/2512.24078", "authors": ["Junyu Liao", "Ashwin Lall", "Mitsunori Ogihara", "Raymond Wong"], "title": "High-dimensional Regret Minimization", "comment": null, "summary": "Multi-criteria decision making in large databases is very important in real world applications. Recently, an interactive query has been studied extensively in the database literature with the advantage of both the top-k query (with limited output size) and the skyline query (which does not require users to explicitly specify their preference function). This approach iteratively asks the user to select the one preferred within a set of options. Based on rounds of feedback, the query learns the implicit preference and returns the most favorable as a recommendation.\n  However, many modern applications in areas like housing or financial product markets feature datasets with hundreds of attributes. Existing interactive algorithms either fail to scale or require excessive user interactions (often exceeding 1000 rounds). Motivated by this, we propose FHDR (Fast High-Dimensional Reduction), a novel framework that takes less than 0.01s with fewer than 30 rounds of interaction. It is considered a breakthrough in the field of interactive queries since most, if not all, existing studies are not scalable to high-dimensional datasets.\n  Extensive experiments demonstrate that FHDR outperforms the best-known algorithms by at least an order of magnitude in execution time and up to several orders of magnitude in terms of the number of interactions required, establishing a new state of the art for scalable interactive regret minimization."}
