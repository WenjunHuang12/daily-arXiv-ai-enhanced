<div id=toc></div>

# Table of Contents

- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1] [Failures to Surface Harmful Contents in Video Large Language Models](https://arxiv.org/abs/2508.10974)
*Yuxin Cao,Wei Song,Derui Wang,Jingling Xue,Jin Song Dong*

Main category: cs.MM

TL;DR: 当前视频大语言模型（VideoLLMs）存在安全漏洞，容易忽略视频中的有害内容，主要由于帧采样不足、空间信息丢失和编码-解码脱节。


<details>
  <summary>Details</summary>
Motivation: 揭示VideoLLMs在处理视频内容时的设计缺陷，尤其是对有害内容的忽略问题，以推动改进。

Method: 通过根因分析发现三个设计缺陷，并设计三种零查询黑盒攻击来验证这些缺陷。

Result: 实验表明，主流VideoLLMs在90%以上的情况下会忽略有害内容。

Conclusion: 当前VideoLLMs的设计存在根本性漏洞，需改进采样策略、令牌压缩和解码机制以确保语义覆盖。

Abstract: Video Large Language Models (VideoLLMs) are increasingly deployed on numerous
critical applications, where users rely on auto-generated summaries while
casually skimming the video stream. We show that this interaction hides a
critical safety gap: if harmful content is embedded in a video, either as
full-frame inserts or as small corner patches, state-of-the-art VideoLLMs
rarely mention the harmful content in the output, despite its clear visibility
to human viewers. A root-cause analysis reveals three compounding design flaws:
(1) insufficient temporal coverage resulting from the sparse, uniformly spaced
frame sampling used by most leading VideoLLMs, (2) spatial information loss
introduced by aggressive token downsampling within sampled frames, and (3)
encoder-decoder disconnection, whereby visual cues are only weakly utilized
during text generation. Leveraging these insights, we craft three zero-query
black-box attacks, aligning with these flaws in the processing pipeline. Our
large-scale evaluation across five leading VideoLLMs shows that the harmfulness
omission rate exceeds 90% in most cases. Even when harmful content is clearly
present in all frames, these models consistently fail to identify it. These
results underscore a fundamental vulnerability in current VideoLLMs' designs
and highlight the urgent need for sampling strategies, token compression, and
decoding mechanisms that guarantee semantic coverage rather than speed alone.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: PaperRegister提出了一种支持灵活粒度论文搜索的系统，通过离线分层索引和在线自适应检索，解决了传统基于摘要的索引无法满足细粒度查询需求的问题。


<details>
  <summary>Details</summary>
Motivation: 传统论文搜索系统仅基于摘要构建索引，无法满足灵活粒度的搜索需求（如模块配置等细节）。

Method: 提出PaperRegister，包含离线分层索引和在线自适应检索，将传统摘要索引转化为分层索引树。

Result: 实验表明，PaperRegister在多种粒度搜索任务中表现优异，尤其在细粒度场景下显著优于现有方法。

Conclusion: PaperRegister是解决灵活粒度论文搜索需求的有效方案，具有实际应用潜力。

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [3] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 论文提出+VeriRel方法，将验证成功纳入文档排序，显著提升科学事实检查的证据检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖通用信息检索算法，仅基于相关性而非证据支持性排序文档，无法满足科学事实检查的需求。

Method: 提出+VeriRel方法，在文档排序中引入验证成功的反馈。

Result: 在三个科学事实检查数据集上，+VeriRel在证据检索和下游验证中表现领先。

Conclusion: 研究表明，将验证反馈整合到文档相关性评估中，对科学事实检查系统具有潜力，未来可探索细粒度相关性评估。

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


### [4] [Role-Augmented Intent-Driven Generative Search Engine Optimization](https://arxiv.org/abs/2508.11158)
*Xiaolu Chen,Haojie Wu,Jie Bao,Zhen Chen,Yong Liao,Hu Huang*

Main category: cs.IR

TL;DR: 论文提出了一种针对生成式搜索引擎（GSEs）的优化方法（G-SEO），通过角色增强和意图驱动的内容优化，解决了传统SEO在GSE中的不适用问题。


<details>
  <summary>Details</summary>
Motivation: 传统SEO在生成式搜索引擎中效果不佳，内容创作者面临可见性下降的挑战，需要新的优化方法。

Method: 提出Role-Augmented Intent-Driven G-SEO方法，通过多样化信息角色建模搜索意图，并扩展数据集和评估标准。

Result: 实验表明，搜索意图能有效指导内容优化，显著提升GSE中的内容可见性和主观印象。

Conclusion: G-SEO方法为生成式搜索引擎提供了有效的优化路径，解决了传统SEO的局限性。

Abstract: Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG), are reshaping information retrieval.
While commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive
semantic synthesis capabilities, their black-box nature fundamentally
undermines established Search Engine Optimization (SEO) practices. Content
creators face a critical challenge: their optimization strategies, effective in
traditional search engines, are misaligned with generative retrieval contexts,
resulting in diminished visibility. To bridge this gap, we propose a
Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)
method, providing a structured optimization pathway tailored for GSE scenarios.
Our method models search intent through reflective refinement across diverse
informational roles, enabling targeted content enhancement. To better evaluate
the method under realistic settings, we address the benchmarking limitations of
prior work by: (1) extending the GEO dataset with diversified query variations
reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a
6-level LLM-augmented evaluation rubric for fine-grained human-aligned
assessment. Experimental results demonstrate that search intent serves as an
effective signal for guiding content optimization, yielding significant
improvements over single-aspect baseline approaches in both subjective
impressions and objective content visibility within GSE responses.

</details>


### [5] [Representation Quantization for Collaborative Filtering Augmentation](https://arxiv.org/abs/2508.11194)
*Yunze Luo,Yinjie Jiang,Gaode Chen,Jingchi Wang,Shicheng Wang,Ruina Sun,Jiang Yuezihan,Jun Zhang,Jian Liang,Han Li,Kun Gai,Kaigui Bian*

Main category: cs.IR

TL;DR: 论文提出了一种名为DQRec的两阶段协同推荐算法，通过结合交互序列和属性提取行为特征，解决了数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 协同过滤算法面临数据稀疏性问题，现有方法在连接用户和项目时受限于粗粒度和稀疏属性，无法有效提取行为特征。

Method: 提出DQ-VAE算法，分解预训练表示嵌入并量化生成语义ID，用于特征和链接增强。

Result: 实验表明，DQRec在多个数据集上优于基线方法。

Conclusion: DQRec通过提取行为特征和增强链接，显著提升了推荐系统的性能。

Abstract: As the core algorithm in recommendation systems, collaborative filtering (CF)
algorithms inevitably face the problem of data sparsity. Since CF captures
similar users and items for recommendations, it is effective to augment the
lacking user-user and item-item homogeneous linkages. However, existing methods
are typically limited to connecting through overlapping interacted neighbors or
through similar attributes and contents. These approaches are constrained by
coarse-grained, sparse attributes and fail to effectively extract behavioral
characteristics jointly from interaction sequences and attributes. To address
these challenges, we propose a novel two-stage collaborative recommendation
algorithm, DQRec: Decomposition-based Quantized Variational AutoEncoder
(DQ-VAE) for Recommendation. DQRec augments features and homogeneous linkages
by extracting the behavior characteristics jointly from interaction sequences
and attributes, namely patterns, such as user multi-aspect interests. Inspired
by vector quantization (VQ) technology, we propose a new VQ algorithm, DQ-VAE,
which decomposes the pre-trained representation embeddings into distinct
dimensions, and quantize them to generates semantic IDs. We utilize the
generated semantic IDs as the extracted patterns mentioned above. By
integrating these semantic ID patterns into the recommendation process through
feature and linkage augmentation, the system enriches both latent and explicit
user and item features, identifies pattern-similar neighbors, and thereby
improves the efficiency of information diffusion. Experimental comparisons with
baselines across multiple datasets demonstrate the superior performance of the
proposed DQRec method.

</details>


### [6] [Mitigating Filter Bubble from the Perspective of Community Detection: A Universal Framework](https://arxiv.org/abs/2508.11239)
*Ming Tang,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 提出CD-CGCN框架，通过社区检测和对抗学习缓解推荐系统中的过滤气泡问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统过度关注准确性而牺牲多样性，加剧过滤气泡效应。

Method: 结合条件判别器和社区重加权图卷积网络，利用社区标签进行对抗学习。

Result: 实验验证CD-CGCN能有效减轻过滤气泡，同时保持推荐质量，并捕捉用户跨社区偏好。

Conclusion: CD-CGCN为缓解过滤气泡问题提供了通用且有效的解决方案。

Abstract: In recent years, recommender systems have primarily focused on improving
accuracy at the expense of diversity, which exacerbates the well-known filter
bubble effect. This paper proposes a universal framework called CD-CGCN to
address the filter bubble issue in recommender systems from a community
detection perspective. By analyzing user-item interaction histories with a
community detection algorithm, we reveal that state-of-the-art recommendations
often focus on intra-community items, worsening the filter bubble effect.
CD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and
a Community-reweighted Graph Convolutional Network which can be plugged into
most recommender models. Using adversarial learning based on community labels,
it counteracts the extracted community attributes and incorporates an inference
strategy tailored to the user's specific filter bubble state. Extensive
experiments on real-world datasets with multiple base models validate its
effectiveness in mitigating filter bubbles while preserving recommendation
quality. Additionally, by applying community debiasing to the original test set
to construct an unbiased test set, we observe that CD-CGCN demonstrates
superior performance in capturing users' inter-community preferences.

</details>


### [7] [INFNet: A Task-aware Information Flow Network for Large-Scale Recommendation Systems](https://arxiv.org/abs/2508.11565)
*Kaiyuan Li,Dongdong Mao,Yongxiang Tang,Yanhua Cheng,Yanxiang Zeng,Chao Wang,Xialong Liu,Peng Jiang*

Main category: cs.IR

TL;DR: 论文提出了一种名为INFNet的任务感知架构，用于解决大规模推荐系统中特征交互的挑战，通过双流设计实现高效特征处理，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征交互策略在大规模推荐系统中面临计算成本高和多任务建模能力受限的问题。

Method: 提出INFNet，将特征分为三类令牌，采用异构和同构交替信息块的双流设计，包括跨注意力机制和代理门控单元。

Result: 在多个离线基准测试中表现优异，线上广告系统部署后，收入和点击率显著提升。

Conclusion: INFNet通过任务感知架构有效解决了特征交互的挑战，为大规模推荐系统提供了高效解决方案。

Abstract: Feature interaction has long been a cornerstone of ranking models in
large-scale recommender systems due to its proven effectiveness in capturing
complex dependencies among features. However, existing feature interaction
strategies face two critical challenges in industrial applications: (1) The
vast number of categorical and sequential features makes exhaustive interaction
computationally prohibitive, often resulting in optimization difficulties. (2)
Real-world recommender systems typically involve multiple prediction
objectives, yet most current approaches apply feature interaction modules prior
to the multi-task learning layers. This late-fusion design overlooks
task-specific feature dependencies and inherently limits the capacity of
multi-task modeling. To address these limitations, we propose the Information
Flow Network (INFNet), a task-aware architecture designed for large-scale
recommendation scenarios. INFNet distinguishes features into three token types,
categorical tokens, sequence tokens, and task tokens, and introduces a novel
dual-flow design comprising heterogeneous and homogeneous alternating
information blocks. For heterogeneous information flow, we employ a
cross-attention mechanism with proxy that facilitates efficient cross-modal
token interaction with balanced computational cost. For homogeneous flow, we
design type-specific Proxy Gated Units (PGUs) to enable fine-grained intra-type
feature processing. Extensive experiments on multiple offline benchmarks
confirm that INFNet achieves state-of-the-art performance. Moreover, INFNet has
been successfully deployed in a commercial online advertising system, yielding
significant gains of +1.587% in Revenue (REV) and +1.155% in Click-Through Rate
(CTR).

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [A Gentle Wakeup Call: Symmetry Breaking with Less Collision Cost](https://arxiv.org/abs/2508.11006)
*Umesh Biswas,Maxwell Young*

Main category: cs.DS

TL;DR: 论文提出了一种名为Aim-High的随机唤醒算法，旨在解决对称性破坏问题，显著降低了延迟和碰撞成本。


<details>
  <summary>Details</summary>
Motivation: 现有唤醒算法在延迟方面表现良好，但碰撞成本较高，导致总延迟显著增加。本文旨在设计一种算法，同时优化延迟和碰撞成本。

Method: 设计了随机唤醒算法Aim-High，针对静态和动态版本的唤醒问题，分析了其在不同条件下的性能。

Result: 对于足够大的碰撞成本C，Aim-High的延迟和碰撞成本接近O(√C)；否则，静态和动态版本的延迟分别为O(poly(log n))和O(n poly(log n))。

Conclusion: Aim-High算法在降低延迟和碰撞成本方面表现优异，并通过下界结果验证了其性能的优越性。

Abstract: The wakeup problem addresses the fundamental challenge of symmetry breaking.
There are $n$ devices sharing a time-slotted multiple access channel. In any
fixed slot, if a single device sends a packet, it succeeds; however, if two or
more devices send, then there is a collision and none of the corresponding
packets succeed. For the static version of wakeup, all packets are initially
active (i.e., can send and listen on the channel); for the dynamic version, the
packets become active at arbitrary times. In both versions, the goal is to
successfully send a single packet.
  Prior results on wakeup have largely focused on the number of slots until the
first success; that is, the latency. However, in many modern systems,
collisions introduce significant delay, an aspect that current wakeup
algorithms do not address. For instance, while existing results for static
wakeup have polylogarithmic-in-$n$ latency, they can incur additional latency
that is {\it linear} in the cost of a collision $C$. Thus, the total latency is
large and dominated by the contributions from collisions.
  Here, we design and analyze a randomized wakeup algorithm, Aim-High. For
sufficiently large $C$ and with bounded error, Aim-High has latency and
expected collision cost that is nearly $O(\sqrt{C})$ for both the static and
dynamic versions. Otherwise, the latency and expected collision cost are
$O(\texttt{poly}{(\log n)})$ for the static setting, and
$O(n\,\texttt{poly}{(\log n)})$ for the dynamic setting. We also establish
lower bounds that complement these results.

</details>


### [9] [Sampling tree-weighted partitions without sampling trees](https://arxiv.org/abs/2508.11130)
*Sarah Cannon,Wesley Pegden,Jamie Tucker-Foltz*

Main category: cs.DS

TL;DR: 本文提出了一种新的算法，用于直接采样平衡树加权2分区，无需先采样生成树，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 计算重划分析中对平衡树加权k分区的需求推动了研究，现有方法因需重复采样生成树而效率低下。

Method: 提出直接采样平衡树加权2分区的算法，避免生成树采样的瓶颈，适用于一类平面图。

Result: 算法在典型平面图上预期线性时间O(n)运行，比现有生成树采样方法更快。

Conclusion: 新算法显著提升了采样效率，并为均匀随机树的精确采样提供了改进。

Abstract: This paper gives a new algorithm for sampling tree-weighted partitions of a
large class of planar graphs. Formally, the tree-weighted distribution on
$k$-partitions of a graph weights $k$-partitions proportional to the product of
the number of spanning trees of each partition class. Recent work on problems
in computational redistricting analysis has driven special interest in the
conditional distribution where all partition classes have the same size
(balanced partitions). One class of Markov chains in wide use aims to sample
from balanced tree-weighted $k$-partitions using a sampler for balanced
tree-weighted 2-partitions. Previous implementations of this 2-partition
sampler would draw a random spanning tree and check whether it contains an edge
whose removal produces a balanced 2-component forest; if it does, this
2-partition is accepted, otherwise the algorithm rejects and repeats. In
practice, this is a significant computational bottleneck.
  We show that in fact it is possible to sample from the balanced tree-weighted
2-partition distribution directly, without first sampling a spanning tree; the
acceptance and rejection rates are the same as in previous samplers. We prove
that on a wide class of planar graphs encompassing network structures typically
arising from the geographic data used in computational redistricting, our
algorithm takes expected linear time $O(n)$. Notably, this is asymptotically
faster than the best known method to generate random trees, which is $O(n
\log^2 n)$ for approximate sampling and $O(n^{1 + \log \log \log n / \log \log
n})$ for exact sampling. Additionally, we show that a variant of our algorithm
also gives a speedup to $O(n \log n)$ for exact sampling of uniformly random
trees on these families of graphs, improving the bounds for both exact and
approximate sampling.

</details>


### [10] [Face-hitting dominating sets in planar graphs: Alternative proof and linear-time algorithm](https://arxiv.org/abs/2508.11444)
*Therese Biedl*

Main category: cs.DS

TL;DR: 论文提出了一种新的构造性证明方法，将平面图的顶点划分为两个支配面命中集，并能在线性时间内完成。


<details>
  <summary>Details</summary>
Motivation: 原证明依赖于四色定理且非算法化，难以实现。本文旨在提供一种更简单、可实现的构造性证明。

Method: 通过将图分解为2连通分量、寻找耳分解以及在3-正则平面图中计算完美匹配，实现顶点划分。

Result: 证明了平面图可在相同限制下划分为两个支配面命中集，且算法复杂度为线性时间。

Conclusion: 新方法简化了证明过程，并提供了高效的算法实现。

Abstract: In a recent paper, Francis, Illickan, Jose and Rajendraprasad showed that
every $n$-vertex plane graph $G$ has (under some natural restrictions) a
vertex-partition into two sets $V_1$ and $V_2$ such that each $V_i$ is
\emph{dominating} (every vertex of $G$ contains a vertex of $V_i$ in its closed
neighbourhood) and \emph{face-hitting} (every face of $G$ is incident to a
vertex of $V_i$). Their proof works by considering a supergraph $G'$ of $G$
that has certain properties, and among all such graphs, taking one that has the
fewest edges. As such, their proof is not algorithmic. Their proof also relies
on the 4-color theorem, for which a quadratic-time algorithm exists, but it
would not be easy to implement.
  In this paper, we give a new proof that every $n$-vertex plane graph $G$ has
(under the same restrictions) a vertex-partition into two dominating
face-hitting sets. Our proof is constructive, and requires nothing more
complicated than splitting a graph into 2-connected components, finding an ear
decomposition, and computing a perfect matching in a 3-regular plane graph. For
all these problems, linear-time algorithms are known and so we can find the
vertex-partition in linear time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 本文提出了一种智能基础化方法，用于处理ASPIC+中的一阶规则，通过转换为Datalog程序并简化规则，保持推理正确性且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有ASPIC+方法仅支持命题规则，而一阶规则需要基础化步骤，可能导致输入理论规模指数增长，缺乏专用解决方案。

Method: 将一阶ASPIC+实例转换为Datalog程序，利用Datalog引擎获取基础替换，并针对ASPIC+形式化提出简化规则。

Result: 原型实现通过实证评估展示了方法的可扩展性。

Conclusion: 提出的智能基础化方法有效解决了ASPIC+中一阶规则推理的规模和效率问题。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [12] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 论文提出了一种多主体算法追索框架，解决了现实世界中多个追索寻求者和提供者之间的交互问题，通过优化社会福祉和资源分配来实现系统级设计。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，算法追索通常涉及多个交互的利益相关者，而现有研究多关注单个体和单模型场景，忽略了多主体竞争有限资源的复杂性。

Method: 采用加权二分图匹配问题建模多对多交互，提出三层优化框架：基础容量匹配、最优容量再分配和成本感知优化。

Result: 实验验证表明，该框架能在系统设置最小修改下实现接近最优的社会福祉。

Conclusion: 该研究将算法追索从个体推荐扩展到系统级设计，为社会福祉和个体可操作性提供了可行路径。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [13] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动的逆优化器，结合PPO框架自动生成高质量质子治疗计划，显著提升效率和效果。


<details>
  <summary>Details</summary>
Motivation: 质子治疗计划优化中，传统方法依赖人工调整参数和计算密集型逆优化，耗时且效率低。

Method: 采用L2O方法预测优化步骤，结合Transformer处理长上下文，PPO框架自动调整参数。

Result: L2O逆优化器效率提升36.41%，效果提升22.97%，计划生成时间平均2.55小时。

Conclusion: 该框架在多种临床条件下生成高质量计划，优于或媲美人工计划。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [14] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文扩展了假设基础论证（ABA）中的可接受性概念，研究了强和弱可接受性，并引入了相应的优选、完备和基础语义，适用于非平坦ABA。


<details>
  <summary>Details</summary>
Motivation: 研究ABA中标准可接受性概念的替代方案，填补强可接受性在ABA中的研究空白。

Method: 使用抽象双极集合基础论证框架（BSAFs）作为形式工具，分析强和弱可接受性在非平坦ABA中的性质。

Result: 证明了强和弱可接受性在非平坦ABA中保持模块化特性，但也存在标准可接受性的一些缺陷。

Conclusion: 本文为ABA中的强和弱可接受性提供了理论基础，并讨论了解决其缺陷的方法。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [15] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 论文提出新数据集评估大型推理模型（LRMs）在信息不完整问题上的表现，发现其缺乏主动询问信息的能力，并揭示了过度思考和幻觉行为。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估LRMs在定义明确问题上的表现，而真正智能的代理应能主动询问缺失信息。

Method: 构建包含两类不完整问题的数据集，系统评估LRMs的表现，并分析其行为。

Result: LRMs无法主动询问信息，且表现出过度思考和幻觉行为。监督微调在此能力学习上存在潜力与挑战。

Conclusion: 研究为开发真正智能的LRMs提供了新视角，强调需超越单纯问题解决能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [16] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE提出了一种动态知识图谱嵌入框架，通过自适应维度扩展和动态蒸馏机制，显著提升了嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的知识图谱是动态演化的，现有方法未能充分考虑更新规模和系统评估。

Method: SAGE根据更新规模确定嵌入维度并扩展空间，采用动态蒸馏机制平衡新旧知识。

Result: 在七个基准测试中，SAGE显著优于基线方法，MRR、H@1和H@10分别提升1.38%、1.25%和1.6%。

Conclusion: SAGE证明了自适应嵌入维度在动态知识图谱嵌入中的重要性，性能优于固定维度方法。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [17] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: 论文提出CRAFT-GUI框架，通过课程学习和细粒度奖励设计，解决了GUI任务中难度差异和奖励信号单一的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在GUI环境中忽视任务难度差异和奖励信号单一，导致学习效率低下。

Method: 提出基于GRPO的课程学习框架CRAFT-GUI，结合规则和模型评估设计细粒度奖励函数。

Result: 在公开和内部基准测试中分别提升5.6%和10.3%，验证了方法的有效性。

Conclusion: 结合强化学习和课程学习能显著提升GUI交互任务的性能。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [18] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 论文提出AIM-Bench基准，评估LLM代理在不确定供应链管理中的决策行为，发现其存在类似人类的决策偏差，并探索缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理在库存决策中的能力和潜在偏差，填补其在不确定环境下决策研究的空白。

Method: 通过AIM-Bench基准进行一系列库存补充实验，评估LLM代理的决策行为。

Result: 不同LLM表现出类似人类的决策偏差，并探索了缓解策略（认知反思和信息共享）。

Conclusion: 需谨慎考虑LLM在库存决策中的潜在偏差，为开发人本决策支持系统提供方向。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [19] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena是一个实时排行榜，通过从AI应用中收集的人类反馈对模型进行排名，解决了静态数据集和通用领域提示的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准和排行榜依赖静态数据集或通用领域提示，无法反映模型在真实应用中的表现。

Method: 平台整合了成对模型比较到用户交互中，采用Bradley-Terry模型，并引入Placement Matches和Proximity Sampling两项创新。

Result: Inclusion Arena提供可靠稳定的排名，数据传递性更高，且能显著减少恶意操纵风险。

Conclusion: 该平台旨在加速开发真正适用于实际用户场景的LLMs和MLLMs。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [20] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 论文提出了一种概率性地标的概念，并将其应用于UCT算法以分解MDP问题，结果表明地标能显著提升在线概率规划的性能。


<details>
  <summary>Details</summary>
Motivation: 地标在经典规划中已有重要应用，但在随机领域很少使用，本文旨在填补这一空白。

Method: 通过形式化概率地标，并调整UCT算法以利用地标作为子目标来分解MDP，核心在于平衡贪心地标达成与最终目标达成。

Result: 在基准测试中，选择合适的地标能显著提升UCT算法的性能，但贪心与长期目标的平衡因问题而异。

Conclusion: 地标可以为解决MDP的随时算法提供有效指导。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [21] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM与问题分解的新型规划器，通过LLM4Inspire和LLM4Predict两种范式辅助分解大规模规划问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中的状态空间爆炸问题，并探索如何结合LLM与领域知识生成有效计划。

Method: 提出LLM辅助规划器，结合问题分解，使用LLM4Inspire（通用知识启发）和LLM4Predict（领域知识推断）两种范式。

Result: 实验表明，LLM能有效修剪搜索空间并找到可行解，其中结合领域知识的LLM4Predict表现更优。

Conclusion: 结合LLM与领域知识的问题分解方法在大规模规划问题中具有潜力，尤其是LLM4Predict范式。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Tabularis Formatus: Predictive Formatting for Tables](https://arxiv.org/abs/2508.11121)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Gust Verbruggen*

Main category: cs.DB

TL;DR: TaFo是一种神经符号方法，用于自动生成表格的条件格式建议，解决了用户不熟悉规则创建和界面不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有条件格式规则创建复杂，依赖用户输入，TaFo旨在实现完全预测性和自动化。

Method: 结合组件合成系统和语言模型语义知识，支持基于值的格式，自动学习规则触发和视觉属性。

Result: TaFo在180万公开工作簿上测试，生成更准确、多样和完整的建议，性能提升15.6%-26.5%。

Conclusion: TaFo优于现有系统，为表格格式化提供了高效自动化解决方案。

Abstract: Spreadsheet manipulation software are widely used for data management and
analysis of tabular data, yet the creation of conditional formatting (CF) rules
remains a complex task requiring technical knowledge and experience with
specific platforms. In this paper we present TaFo, a neuro-symbolic approach to
generating CF suggestions for tables, addressing common challenges such as user
unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo
takes inspiration from component based synthesis systems and extends them with
semantic knowledge of language models and a diversity preserving rule
ranking.Unlike previous methods focused on structural formatting, TaFo uniquely
incorporates value-based formatting, automatically learning both the rule
trigger and the associated visual formatting properties for CF rules. By
removing the dependency on user specification used by existing techniques in
the form of formatted examples or natural language instruction, TaFo makes
formatting completely predictive and automated for the user. To evaluate TaFo,
we use a corpus of 1.8 Million public workbooks with CF and manual formatting.
We compare TaFo against a diverse set of symbolic and neural systems designed
for or adapted for the task of table formatting. Our results show that TaFo
generates more accurate, diverse and complete formatting suggestions than
current systems and outperforms these by 15.6\%--26.5\% on matching user added
ground truth rules in tables.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 本文提出了一种基于合作博弈的多准则投票集成方法，以解决现有集成学习中权重分配单一的问题，实验证明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有集成学习方法在权重分配时仅考虑单一评价准则，无法充分利用分类器的先验信息，限制了模型性能的提升。

Method: 通过合作博弈在多准则情境下进行决策，同时考虑分类器的多种先验信息，实现更合理的权重分配。

Result: 在Open-ML-CC18数据集上的实验表明，该方法性能优于其他权重分配方法。

Conclusion: 提出的多准则合作博弈方法能有效提升集成学习的性能，为复杂场景下的模型优化提供了新思路。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [24] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker是一个15B参数的模型，性能媲美32B参数模型，但内存占用仅为一半。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内存和计算成本上的限制，使其更适合企业应用。

Method: 采用四阶段训练流程：基础模型扩展、持续预训练、监督微调、GRPO强化学习。

Result: 在多项基准测试中，性能达到或超过32B参数模型。

Conclusion: Apriel-Nemotron-15B-Thinker以更小的规模实现了高性能，适合企业部署。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [25] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络的压缩元学习框架，通过优化编码和解码阶段，提高了压缩学习的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的快速扩大，需要快速高效的参数学习技术。现有的压缩学习方法未能充分利用数据的底层结构。

Method: 使用神经网络元学习压缩学习的编码和解码阶段，提出压缩元学习框架。

Result: 该框架在多个应用中（如压缩PCA、压缩岭回归等）表现优于现有方法。

Conclusion: 压缩元学习框架为高效、隐私友好的学习提供了新方向。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [26] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 提出了一种基于提示的持续学习方法（PCL），用于解决医疗领域数据共享受限和分布偏移的问题，显著提升了分类准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 医疗领域因伦理和社会限制难以共享数据，传统集中式学习不可行，且数据分布易偏移，现有持续学习方法多针对自然图像，医疗领域研究不足。

Method: 采用统一的提示池和最小扩展策略，通过冻结部分提示减少计算开销，并引入新的正则化项平衡知识保留与适应。

Result: 在三个糖尿病视网膜病变数据集上，分类准确率提升至少10%，F1分数提高9分，同时降低推理成本。

Conclusion: PCL方法有望推动可持续医疗AI发展，支持实时诊断和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [27] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 提出了一种基于相对优势的去偏框架，通过比较用户和项目组的参考分布来校正观看时间，生成基于分位数的偏好信号，并采用两阶段架构分离分布估计和偏好学习。


<details>
  <summary>Details</summary>
Motivation: 原始观看时间受视频时长、流行度和用户行为等混杂因素影响，可能导致推荐模型偏差。

Method: 提出相对优势去偏框架，通过参考分布校正观看时间，采用两阶段架构和分布嵌入参数化分位数。

Result: 离线和在线实验显示推荐准确性和鲁棒性显著提升。

Conclusion: 该方法有效解决了观看时间偏差问题，提升了推荐系统的性能。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [28] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert是一个可解释的逆合成框架，结合大型语言模型和专用模型，通过强化学习提供自然语言解释，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成预测模型依赖静态模式匹配，缺乏逻辑决策能力，导致黑箱决策。

Method: Retro-Expert通过三个组件实现协作推理：(1)专用模型进行浅层推理，(2)大型语言模型生成预测和解释路径，(3)强化学习优化决策策略。

Result: 实验表明，Retro-Expert在多个指标上优于现有模型，并提供专家认可的解释。

Conclusion: Retro-Expert不仅提升了预测性能，还通过可解释性增强了化学合成的实用性。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [29] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: FGAT框架通过图神经网络和注意力机制，结合视觉和文本特征，同时建模穿搭兼容性和用户偏好，显著提升时尚推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 时尚产业快速扩张，用户难以在电商平台找到兼容商品。现有研究独立处理穿搭兼容性和个性化推荐，忽略物品与用户偏好的复杂交互。

Method: 提出FGAT框架，构建用户、穿搭和物品的三层层次图，结合视觉和文本特征，利用图注意力机制动态加权节点重要性。

Result: 在POG数据集上，FGAT在精度、HR、召回率、NDCG和准确率上优于基线模型HFGN。

Conclusion: 结合多模态特征、层次图结构和注意力机制，显著提升个性化时尚推荐的准确性和效率。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [30] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb是一种合成数据生成框架，显著提升预训练性能，优于现有合成数据集，并揭示合成数据质量的关键因素。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练中，数据量的简单扩展已遇到瓶颈，合成数据成为提升性能的新方向，但其质量影响因素尚不明确。

Method: 提出BeyondWeb框架，生成高质量合成数据，并通过14项基准测试验证其性能。

Result: BeyondWeb在性能上超越Cosmopedia和Nemotron-Synth，训练速度更快，小模型表现优于大模型。

Conclusion: 高质量合成数据需多因素联合优化，BeyondWeb展示了其潜力，但需科学方法和实践经验支持。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [31] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了首个模型选择框架M&C，帮助用户高效选择预训练的文本到图像（T2I）模型，避免对所有模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 公共预训练T2I模型促进了模型的民主化，但用户面临选择最佳模型的挑战。

Method: M&C框架基于匹配图，包含模型和数据集节点，以及模型-数据和数据-数据对的边，通过图嵌入特征预测最佳微调模型。

Result: 在10个T2I模型和32个数据集上的实验表明，M&C在61.3%的情况下成功预测最佳模型，其余情况下也能选择接近最优的模型。

Conclusion: M&C为预训练T2I模型的选择提供了高效解决方案，显著减少了微调成本。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [32] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: CURE框架通过两阶段方法（高熵关键令牌再生和静态初始状态采样）解决RLVR中的熵崩溃问题，提升LLM的推理能力和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法因静态初始状态采样导致熵崩溃和多样性不足，限制了模型性能的持续提升。

Method: CURE分为两阶段：1）高熵关键令牌再生以增强探索；2）静态初始状态采样以加强利用。

Result: 在Qwen-2.5-Math-7B上，CURE在六个数学基准测试中性能提升5%，同时保持高熵水平。

Conclusion: CURE通过平衡探索与利用，显著提升模型性能，成为RLVR领域的先进方法。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [33] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 该论文通过扩展强彩票假设（SLTH）到量化网络，证明了在有限精度下，目标离散神经网络可以被精确表示，并给出了初始网络过参数化的最优界限。


<details>
  <summary>Details</summary>
Motivation: 量化是提高神经网络效率的关键技术，但现有理论主要针对连续设置，无法直接应用于量化网络。本文旨在填补这一理论空白。

Method: 基于Borgs等人的数分割问题研究，推导了量化随机子集和问题的新理论结果，并扩展SLTH框架到有限精度网络。

Result: 在量化设置下，目标离散神经网络可以被精确表示，且初始网络的过参数化界限是最优的。

Conclusion: 本文为量化神经网络的理论研究提供了新工具，扩展了SLTH的应用范围，并证明了量化网络的高效表示可能性。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [34] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出了一种新的不确定性量化方法zono-conformal prediction，通过构建预测zonotopes解决现有方法计算成本高和数据依赖性强的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高、数据依赖性强，且多维度输出依赖关系捕捉能力有限。

Method: 引入zono-conformal prediction，基于区间预测模型和reachset-conformant identification，通过线性程序构建预测zonotopes。

Result: 在数值实验中，zono-conformal predictors比现有方法更高效且保守性更低，同时保持相似的测试数据覆盖率。

Conclusion: zono-conformal prediction为多维度输出提供了一种高效且统计有效的不确定性量化方法。

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [35] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 论文探讨了学习或更新信念中的“信心”概念，区分了它与概率或似然的不同，并提供了两种测量方法。


<details>
  <summary>Details</summary>
Motivation: 研究学习过程中对信息的信任程度及其对信念状态的影响，填补了现有文献中对“信心”概念形式化定义的空白。

Method: 通过公理化定义“信心”学习，提出两种连续测量方法，并在附加假设下推导出更简洁的表示形式。

Result: 证明了信心可以统一表示，并展示了基于信心的学习在向量场和损失函数中的表示。

Conclusion: 贝叶斯规则是优化学习者在特定条件下的特例，信心概念为学习理论提供了新的视角。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [36] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 论文提出一种广义非正态分布（广义非正态正态分布），其精度矩阵仍能反映变量间的条件独立性，并提出一种高效算法用于从这类数据中恢复条件独立结构。


<details>
  <summary>Details</summary>
Motivation: 针对非高斯分布中协方差和精度矩阵无法直接反映变量独立性结构的问题，研究在特定条件下（如广义非正态正态分布）精度矩阵仍能提供条件独立性信息。

Method: 提出广义非正态正态分布的概念，并设计一种简单且计算高效的算法，利用精度矩阵推断条件独立性结构。

Result: 通过合成实验和实际数据验证了算法的有效性。

Conclusion: 广义非正态正态分布及其算法为非高斯分布下的条件独立性推断提供了新思路。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [37] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 该论文研究了LIME和SHAP等事后解释方法在对抗性操纵下的脆弱性，并提出了一种模块化测试框架来评估增强和集成解释方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LIME和SHAP等解释方法被广泛用于评估模型偏见和泛化能力，但它们容易被操纵以掩盖有害偏见。作者旨在验证这些方法的脆弱性并探索改进策略。

Method: 作者复现了COMPAS实验以验证先前发现，并引入模块化测试框架，系统评估不同性能分类器上的增强和集成解释方法。

Result: 研究发现某些LIME/SHAP集成配置能显著提高偏见检测能力，表明其在高风险机器学习系统部署中的潜在价值。

Conclusion: 论文强调了改进解释方法鲁棒性的重要性，并展示了集成方法在增强透明度方面的潜力。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [38] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 本文提出了一种基于丰度的Set Transformer方法，用于构建微生物组样本的固定大小嵌入，通过加权序列嵌入提升分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略微生物分类单元丰度的生物学重要性，仅通过简单平均序列嵌入表示样本，限制了性能。

Method: 提出丰度感知的Set Transformer变体，通过复制嵌入向量并加权其丰度，利用自注意力机制聚合。

Result: 在真实微生物组分类任务中，该方法优于平均池化和未加权的Set Transformer，部分任务达到完美性能。

Conclusion: 丰度感知聚合方法为微生物组表示提供了更稳健且生物学意义更强的解决方案。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [39] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 本文提出了一种经济可行的预测编码方法，通过数据管理流程将即时消息分组为日聊，结合特征选择和逻辑回归分类器，并利用降维技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 即时消息的非正式性和小规模为法律行业中的文档分类（预测编码）带来了额外挑战。

Method: 采用数据管理流程将消息分组为日聊，进行特征选择并使用逻辑回归分类器，同时通过降维技术优化性能。

Result: 在富含定量信息的Instant Bloomberg数据集上测试，展示了方法的有效性及成本节约。

Conclusion: 该方法为即时消息的预测编码提供了经济高效的解决方案，并通过降维进一步提升了性能。

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [40] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 提出了一种多模态系统，用于早期预测ICD编码，融合临床笔记和表格数据，通过预训练编码器和跨模态注意力优化表示。


<details>
  <summary>Details</summary>
Motivation: 早期预测ICD编码有助于识别健康风险、优化治疗和资源分配，但现有研究多关注出院后分类。

Method: 采用多模态系统，结合临床笔记和表格数据，使用预训练编码器、特征池化和跨模态注意力，并提出加权时间损失函数。

Result: 实验表明，该方法优于当前最先进系统。

Conclusion: 多模态融合和时间加权损失有效提升了早期预测性能。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [41] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 论文研究了分段仿射正则化（PAR）在监督学习中的理论和应用，展示了其在过参数化情况下的量化特性，推导了多种PAR的闭式近端映射，并分析了其在统计上的保证。


<details>
  <summary>Details</summary>
Motivation: 解决离散或量化变量的优化问题，因其组合性质而极具挑战性，PAR提供了一种基于连续优化的灵活建模和计算框架。

Method: 研究了PAR的理论基础，推导了闭式近端映射，并提出了多种优化方法（如近端梯度法、加速变体和ADMM）。

Result: 在过参数化情况下，PAR正则化损失函数的临界点表现出高度量化特性，且能获得与传统正则化方法相似的统计保证。

Conclusion: PAR为量化问题提供了一种有效的理论和计算框架，适用于多种优化和统计场景。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [42] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 论文提出了一种名为CTRL的元学习方法，旨在解决多源数据中的分布偏移和样本量差异问题，同时提高整体准确性和保留源间异质性。


<details>
  <summary>Details</summary>
Motivation: 在多源数据（如不同地理位置或群体）的机器学习任务中，需要确保预测不仅整体准确，还能在单个源内可靠并保留源间差异。例如，难民安置项目中需要为小规模源生成差异化预测。

Method: CTRL结合了跨域残差学习和自适应池化/聚类技术，通过元学习方法平衡数据量和数据质量之间的权衡。

Result: 在5个大规模数据集（包括瑞士国家庇护计划数据）上，CTRL在多个关键指标上优于现有基准方法。

Conclusion: CTRL在多源数据场景中表现出色，能够同时提升准确性和保留源间异质性。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [43] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 提出了一种基于分布表示学习的高阶贝叶斯网络分类器（NeuralKDB），通过神经网络架构学习特征值的分布表示，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器因参数爆炸和数据稀疏性问题，难以建模高阶特征依赖关系，限制了其在复杂数据上的表现。

Method: 提出NeuralKDB，通过神经网络学习特征值的分布表示，并设计随机梯度下降算法进行高效训练。

Result: 在60个UCI数据集上的实验表明，NeuralKDB在捕捉高阶特征依赖和分类性能上显著优于传统贝叶斯网络分类器及其他竞争方法。

Conclusion: NeuralKDB通过分布表示学习有效解决了高阶依赖建模问题，为贝叶斯网络分类器提供了新的设计范式。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [44] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 本文研究了物联网（IoT）中多模态在线联邦学习（MMO-FL）面临的模态数量和质量不平衡（QQI）问题，并提出了一种名为QQR的算法来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着边缘智能的发展，IoT设备需要处理多模态数据，但设备的不稳定性导致数据收集时出现模态数量和质量不平衡，影响学习性能。

Method: 提出了Modality Quantity and Quality Rebalanced (QQR)算法，通过原型学习在训练过程中动态调整模态不平衡。

Result: 在两个真实多模态数据集上的实验表明，QQR算法在模态不平衡条件下优于基准方法。

Conclusion: QQR算法有效解决了MMO-FL中的模态不平衡问题，提升了学习性能。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [45] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出了一种半监督生成模型，结合信息瓶颈原则，解决多视图学习中的缺失视图和标签问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视图学习常面临缺失视图和标签的问题，现有方法无法充分利用未标记数据，需改进。

Method: 提出半监督生成模型，结合信息瓶颈原则，最大化未标记样本的似然，并在潜在空间中进行跨视图互信息最大化。

Result: 在图像和多组学数据上，模型在预测和填补缺失视图方面表现优于现有方法。

Conclusion: 该模型能有效利用未标记数据，提升多视图学习的性能。

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [46] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 论文提出了一种混合量子-经典架构QBM-VAE，利用量子处理器高效采样Boltzmann分布，解决了传统高斯先验在复杂生物数据中的局限性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统概率深度学习依赖高斯先验，无法准确捕捉复杂非高斯数据（如生物数据），阻碍科学发现。Boltzmann分布虽更优，但经典计算机难以实现。

Method: 提出QBM-VAE，结合量子处理器采样Boltzmann分布作为先验，构建深度生成模型。

Result: 在百万级单细胞数据上，QBM-VAE优于传统高斯模型（如VAE、SCVI），在数据整合、细胞分类等任务中表现更优。

Conclusion: QBM-VAE展示了量子优势在深度学习中的实际应用，为开发混合量子AI模型提供了可转移的蓝图。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [47] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 论文提出了一种基于调制的元学习框架，用于结构保持的动态系统建模，解决了传统方法需要固定系统配置和昂贵重新训练的问题。


<details>
  <summary>Details</summary>
Motivation: 传统结构保持方法在动态系统建模中需要固定系统配置和明确参数知识，限制了其在多查询或参数变化场景中的应用。元学习提供了解决方案，但现有方法存在训练不稳定或泛化能力有限的问题。

Method: 引入基于调制的元学习框架，通过紧凑的潜在表示直接调节结构保持模型，避免对系统参数的明确知识和显式优化的需求。

Result: 实验表明，该方法在少样本学习场景中实现了准确预测，同时保持了动态稳定性和参数空间中的有效泛化性能。

Conclusion: 提出的调制框架为参数化动态系统提供了可扩展和泛化的学习能力，克服了传统方法的局限性。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [48] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 论文探讨了认知行为的计算实现，提出因果抽象理论作为分析工具，并讨论了表示在计算中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究认知行为的计算实现问题，探讨如何通过因果抽象理论理解计算与表示的关系。

Method: 利用因果抽象理论分析计算实现，并结合深度学习中的神经网络讨论。

Result: 提出了基于因果抽象的计算实现框架，并强调了表示在预测和泛化中的重要性。

Conclusion: 因果抽象理论为理解计算实现和表示提供了有效视角，尤其在预测和泛化中具有重要价值。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [49] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: 该研究提出了一种名为“Borrowing From the Future (BFF)”的多模态对比框架，旨在通过利用后期数据提升早期儿科风险评估的预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管后期风险评估通常更精确，但临床需要尽早进行可靠的风险评估。

Method: BFF将每个时间窗口视为独立模态，利用后期数据隐式监督早期学习。

Result: 在两个真实儿科预测任务中验证，BFF显著提升了早期风险评估性能。

Conclusion: BFF为早期儿科风险评估提供了一种有效方法，代码已开源。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [50] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF是一个统一的、可训练的扩散机制，解决了现有GNN在适应性、深度和理论理解上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的GNN存在适应性差、深度受限和理论理解不足的问题。

Method: 提出GODNF框架，通过节点特定行为建模和动态邻域影响捕捉异质扩散模式和时序动态。

Result: 理论分析证明GODNF能建模多样收敛配置，实验验证其在节点分类和影响力估计任务中的优越性。

Conclusion: GODNF在适应性、深度和理论理解上优于现有GNN，为扩散模型提供了新方向。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [51] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出了一种基于CNN-LSTM混合架构的PM2.5浓度预测模型，结合空间和时间特征，在实验中表现优于传统时间序列模型。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，PM2.5浓度预测对环境保护、公共卫生和城市管理至关重要。

Method: 使用CNN提取局部空间特征，LSTM建模时间依赖性，基于北京工业区2010-2015年的多变量数据集进行6小时平均PM2.5浓度预测。

Result: 模型RMSE为5.236，优于传统时间序列模型，展示了在空气污染预警系统中的潜力。

Conclusion: 模型计算资源需求高，对多变量输入的处理能力需优化，未来将提升可扩展性和支持更复杂的天气预测任务。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [52] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 本文提出了一种改进的交互式投票地图匹配算法，用于高效处理不同采样率的轨迹数据，旨在高精度重建GPS轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决GPS轨迹重建中因输入数据质量不均或路网数据缺失导致的精度问题。

Method: 扩展原算法，集成轨迹插补，采用距离限制的交互投票策略降低计算复杂度，并利用OpenStreetMap资产提升适用性。

Result: 算法在保持原优势的同时，显著提升了处理多样实际场景的能力。

Conclusion: 改进后的算法适用于更广泛的地理区域和复杂场景，提高了GPS轨迹重建的准确性和鲁棒性。

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [53] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS是一种新颖的差异化架构搜索框架，专为加密货币交易设计，通过市场状态感知提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态深度学习模型在动态金融环境中的局限性。

Method: 采用贝叶斯搜索空间、动态激活的神经模块和多目标损失函数。

Result: 在真实加密货币数据上显著优于现有基准，平均绝对误差减少80.3%，收敛速度更快。

Conclusion: 将领域知识（如市场状态）嵌入NAS过程对金融应用至关重要。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [54] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: 论文提出NeMo，一种可扩展且通用的模块化训练方法，适用于Transformer和CNN等架构，显著提升模块分类精度并减少模块大小。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型在现代软件系统中的广泛应用，训练成本高昂成为挑战。现有模块化方法难以适应多样化和大规模模型，尤其是Transformer。

Method: NeMo在神经元级别操作，采用对比学习和复合损失函数，适用于各种DNN架构。

Result: 实验显示，NeMo在模块分类精度上平均提升1.72%，模块大小减少58.10%，适用于CNN和Transformer。

Conclusion: NeMo为可扩展和通用的DNN模块化提供了有效解决方案，具有实际应用潜力。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [55] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种通过提示设计从封闭权重LLM中提取特征并训练轻量级公平分类器的框架，适用于高风险的公平性要求场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法适用于封闭权重LLM（如GPT-4、Gemini等），而这些模型在高风险应用中需要确保群体公平性。

Method: 将LLM视为特征提取器，通过设计提示获取其概率预测的特征，并应用公平算法训练轻量级分类器。

Result: 在五个数据集上验证了框架的有效性，尤其在数据效率和公平性-准确性权衡上优于传统方法。

Conclusion: 该框架为封闭权重LLM提供了一种无需微调即可实现公平分类的解决方案。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [56] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于熵的机制（ETMR和EAR），用于改进测试时强化学习（TTRL）的探索-利用平衡，显著提升了模型在无监督场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现优异，但仍依赖标注数据且无监督适应性不足。TTRL虽能自优化，但面临高推理成本和早期估计偏差等问题。

Method: 引入熵机制，包括ETMR（熵分叉树多数展开）和EAR（基于熵的优势重塑），以优化探索-利用平衡。

Result: 在AIME 2024基准测试中，Llama3.1-8B的Pass at 1指标相对提升68%，且仅消耗60%的展开令牌预算。

Conclusion: 该方法有效平衡了推理效率、多样性和估计鲁棒性，推动了无监督强化学习在开放域推理任务中的应用。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [57] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为RTE的训练框架，通过时间集成提升SNN的对抗鲁棒性，减少对抗扰动的时间传递性。


<details>
  <summary>Details</summary>
Motivation: SNN在能效和类脑计算方面具有潜力，但其对抗扰动的脆弱性尚未被充分研究。

Method: 提出RTE框架，通过统一损失函数和随机采样策略优化各子网络的鲁棒性。

Result: 实验表明RTE在鲁棒性和准确性上优于现有方法，并重塑了SNN的内部鲁棒性景观。

Conclusion: 研究强调了时间结构在对抗学习中的重要性，为构建鲁棒的SNN模型提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [58] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTSM框架通过双分支掩码机制和正交子空间分解，实现了跨被试EEG解码的零样本泛化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试EEG解码中的被试间差异和共享表征稀缺问题。

Method: 采用双分支掩码机制学习个性化和共享时空模式，结合信息论约束分解潜在嵌入。

Result: 在运动想象数据集上实现零样本泛化，性能优于基线。

Conclusion: PTSM通过解耦神经表征，实现了个性化和可迁移的解码。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [59] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 论文提出HS-GPPT模型，通过谱对齐解决图预训练和提示调优中的知识迁移问题，适用于同质性和异质性图。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖同质性低频知识，难以处理真实图中多样谱分布，需解决预训练与下游任务的谱对齐问题。

Method: 采用混合谱滤波器主干和局部-全局对比学习获取谱知识，设计提示图对齐谱分布。

Result: 实验验证了模型在转导和归纳学习中的有效性。

Conclusion: HS-GPPT通过谱对齐实现了高效知识迁移，适用于不同同质性图。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [60] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 论文提出在决策聚焦学习（DFL）中，即使使用可微优化层，最小化替代损失仍能有效降低决策遗憾，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有梯度基DFL方法在LP问题中因梯度为零而效果有限，需改进以提升决策质量。

Method: 提出最小化替代损失，结合高效的可微优化技术DYS-Net，提升训练效率。

Result: 实验显示该方法在决策遗憾和训练时间上均优于现有方法。

Conclusion: 最小化替代损失结合DYS-Net可显著提升DFL效果和效率。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [61] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: CHORD框架通过动态权重统一SFT和RL，避免模型模式破坏和专家数据过拟合，实验证明其稳定高效。


<details>
  <summary>Details</summary>
Motivation: 解决现有SFT和RL结合方法可能破坏模型模式和过拟合专家数据的问题。

Method: 提出CHORD框架，将SFT作为动态加权辅助目标融入RL过程，采用全局系数和token级权重函数。

Result: 实验显示CHORD能稳定高效学习，显著优于基线方法。

Conclusion: CHORD成功统一SFT和RL，为后续研究提供新思路。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [62] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 论文提出TACP方法，通过利用长尾结构减少尾类覆盖率不足的问题，并进一步提出sTACP扩展以平衡各类覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有CP方法在长尾标签分布下，头尾类覆盖率不平衡，尾类覆盖率不足，影响预测集的可靠性。

Method: 提出TACP方法，利用长尾结构缩小头尾覆盖率差距；进一步提出sTACP，通过重加权机制平衡各类覆盖率。

Result: 理论分析表明TACP比标准方法头尾覆盖率差距更小；实验验证了方法的有效性。

Conclusion: TACP和sTACP能有效改善长尾分布下的覆盖率不平衡问题，提升预测集的可靠性。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [63] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 论文提出了一个基于学习理论的框架，用于定义解释算法是否能提供决策函数的信息，并证明许多流行算法对复杂模型无效。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证局部后解释算法是否能真正揭示复杂机器学习模型的行为，并探讨其理论保证的局限性。

Method: 引入一个学习理论框架，定义“信息性解释”为能减少可能决策函数空间复杂性的解释。

Result: 证明许多流行解释算法对复杂模型无效，并推导出不同算法成为信息性解释的条件。

Conclusion: 结论指出解释算法需修改以具备信息性，对AI审计、监管和高风险应用有重要影响。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [64] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle是一种新型自解释图神经网络框架，旨在生成和评估GNN的类级解释，解决了现有方法在类级解释上的不足。


<details>
  <summary>Details</summary>
Motivation: 提升图神经网络（GNNs）的可解释性，确保其安全公平部署，并解决现有自解释GNN模型在类级解释上的局限性。

Method: GraphOracle通过联合学习GNN分类器和一组结构化稀疏子图，采用集成训练和掩码评估策略，高效捕捉图-子图-预测依赖关系。

Result: GraphOracle在多个图分类任务中表现出更高的保真度、可解释性和可扩展性，优于ProtGNN和PGIB等方法。

Conclusion: GraphOracle为GNN提供了一种实用且可靠的类级自解释解决方案，避免了计算瓶颈，提升了训练效率和可扩展性。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [65] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 本文提出了一种全球植树造林项目的数据集，结合卫星图像验证数据可靠性，并引入LDIS评分评估地理信息完整性。


<details>
  <summary>Details</summary>
Motivation: 由于植树造林项目的碳汇效果常依赖自我报告或有限验证，数据可靠性和项目诚信受到质疑，需更透明的方法。

Method: 研究整合了来自45,628个项目的1,289,068个种植点数据，结合卫星图像和其他辅助数据，开发了LDIS评分系统。

Result: 79%的地理参考种植点至少一项LDIS指标不合格，15%的项目缺乏机器可读的地理数据。

Conclusion: 该数据集提升了自愿碳市场的问责性，并可作为计算机视觉任务的训练数据。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [66] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 本文提出了一种名为HXAI（Holistic Explainable Artificial Intelligence）的用户中心框架，旨在将解释嵌入数据分析工作流的每个阶段，并针对用户需求定制解释。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释人工智能方法仅关注单个预测的解释，而忽略了上游决策和下游质量检查，导致用户对AI模型的信任度不足。

Method: HXAI框架将六个组件（数据、分析设置、学习过程、模型输出、模型质量、沟通渠道）统一为一个分类体系，并通过112个问题库和用户调查来满足不同用户的需求。

Result: HXAI提供了一个全面的分类法，减少了术语歧义，并支持对现有工具链的严格覆盖分析。此外，通过嵌入大型语言模型的AI代理，能够将技术成果转化为特定利益相关者的叙述。

Conclusion: HXAI通过跨学科融合和实际项目经验，提出了一种新颖的端到端透明性、可信赖性和负责任AI部署的视角。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [67] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 本文提出了一种名为和谐梯度下降（HGD）的算法，通过平衡不同类别的梯度范数来解决数据流中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据流通常存在类别不平衡问题，现有方法如重采样和重加权效果有限，因此需要一种更高效的训练修改方法。

Method: 提出HGD算法，通过平衡梯度范数来避免次要类别的欠拟合，无需额外参数或先验知识。

Result: 理论分析显示HGD具有次线性遗憾界，实验证明其在多种不平衡数据流场景中优于现有方法。

Conclusion: HGD是一种高效且通用的方法，适用于任何基于梯度下降的模型，能有效处理不平衡数据流。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [68] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA是一种强化学习算法，融合个体奖励和成对偏好，直接利用策略对数概率建模偏好概率，避免单独奖励建模步骤。实验表明DFA在控制环境中表现优于或匹配SAC，且在偏好数据集上优于RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 结合个体奖励和成对偏好，简化偏好建模过程，提升强化学习算法的性能和稳定性。

Method: DFA算法直接利用策略对数概率建模偏好概率，支持人工标注或在线合成的偏好数据，基于Bradley-Terry模型优化偏好损失。

Result: 在六种控制环境中，DFA表现优于或匹配SAC，训练更稳定；在半合成偏好数据集上，DFA优于RLHF基线，接近真实奖励的性能。

Conclusion: DFA通过融合奖励和偏好数据，简化建模步骤，显著提升强化学习性能，适用于多种环境和偏好数据来源。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [69] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 论文提出了一种基于Forman-Ricci曲率的结构提升策略，用于解决图神经网络中信息传递的失真问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂系统（如社交或生物网络）需要更高阶的拓扑结构来表示，而传统图神经网络在处理这类结构时存在信息失真问题。

Method: 利用Forman-Ricci曲率定义边基网络特性，将数据从基本图形式提升到更高阶拓扑结构，以揭示图的局部和全局属性。

Result: 该方法能够有效缓解图学习中长距离信息传递和瓶颈处的信息失真（即过度挤压问题）。

Conclusion: 通过几何和拓扑深度学习方法，提升图神经网络的表达能力，为复杂系统的建模提供了新思路。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [70] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD提出了一种基于共享潜在空间的抗体序列和结构协同设计框架，通过优化潜在代码提高开发性能，显著降低查询成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在原始数据空间中优化CDR，导致搜索效率低下且成本高昂。

Method: LEAD在共享潜在空间中优化序列和结构，并设计了黑盒指导策略以适应非可微分属性评估器。

Result: LEAD在单目标和多目标优化中表现优异，查询消耗减少一半，性能超越基线方法。

Conclusion: LEAD通过潜在空间优化和黑盒策略，显著提升了抗体设计的效率和性能。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [71] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 论文提出了一种通过收缩理论提升卷积神经常微分方程（NODEs）鲁棒性的方法，通过正则化项或权重正则化实现，并在MNIST和FashionMNIST数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的输入噪声和对抗攻击可能导致其脆弱性，因此需要提升其鲁棒性。

Method: 利用收缩理论设计正则化项或权重正则化，使NODEs具有收缩性，从而提升鲁棒性。

Result: 在MNIST和FashionMNIST数据集上，该方法有效减少了噪声和攻击对输出的影响。

Conclusion: 通过收缩理论和正则化方法，可以显著提升NODEs的鲁棒性。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [72] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出了一种名为mCOCO的新框架，利用Reservoir Computing（RC）从BOLD信号中学习群体水平的功能性连接脑模板（CBT），解决了现有方法在可解释性、计算成本和认知能力建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如传统机器学习和图神经网络（GNNs）在生成CBT时存在可解释性差、计算成本高以及忽视认知能力的问题，mCOCO旨在解决这些挑战。

Method: mCOCO框架分为两阶段：(1) 将BOLD信号映射到储备池中，生成个体功能连接组并聚合为群体CBT；(2) 通过认知储备池整合多感官输入，赋予CBT认知特性。

Result: 评估表明，mCOCO在中心性、区分性、拓扑合理性和多感官记忆保留方面显著优于基于GNN的CBT。

Conclusion: mCOCO为功能性连接研究提供了一种高效且可解释的新方法，能够同时捕捉结构和认知特性。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [73] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 论文研究了六种概率机器学习算法在类别概率和不确定性估计中的表现，发现深度学习算法在分布外数据上的不确定性估计不足。


<details>
  <summary>Details</summary>
Motivation: 随着数据模型复杂度增加（如深度学习），不确定性量化变得困难，需要验证现有方法的有效性。

Method: 采用近似贝叶斯推断框架，在合成分类数据集上测试六种算法。

Result: 所有算法校准良好，但深度学习算法在分布外数据上的不确定性估计不足。

Conclusion: 研究为开发新的不确定性估计方法提供了参考。

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [74] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 该研究利用AutoML和可解释AI分析车祸严重性风险因素，构建了高精度预测模型，并识别出关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 全球车祸是伤害和死亡的主要原因，需数据驱动方法理解和减轻其严重性。

Method: 使用JADBio AutoML平台构建预测模型，结合SHAP解释特征贡献，最终采用Ridge Logistic Regression模型。

Result: 模型AUC-ROC达85.6%（训练集）和84.9%（测试集），识别出17个关键影响因素。

Conclusion: 研究提供了可扩展框架，支持Vision Zero目标，强调环境与情境因素的重要性。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [75] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出了一种双空间引导测试框架，通过协调场景参数空间和智能体行为空间，平衡多样性和关键性，提升安全验证效果。


<details>
  <summary>Details</summary>
Motivation: 动态环境中决策智能体的部署增加，安全验证需求上升，现有方法难以平衡多样性和关键性，尤其是高维场景空间中的局部最优问题。

Method: 采用双空间框架：场景参数空间通过分层表示和多维子空间评估定位关键子空间；行为空间利用交互数据量化行为关键性/多样性，支持生成模式切换。

Result: 实验表明，框架在五个决策智能体上平均提升56.23%的关键场景生成，并在新指标下表现更优。

Conclusion: 双空间框架有效解决了多样性与关键性平衡问题，优于现有方法。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [76] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 论文提出了一种用费曼图计算有限宽度修正的方法，以解决无限宽度下NTK无法捕捉训练动态的问题。


<details>
  <summary>Details</summary>
Motivation: 在无限宽度下，NTK无法反映训练中的特征学习或演化，因此需要引入有限宽度修正。

Method: 使用费曼图计算有限宽度修正，简化代数操作，并推导层间递归关系。

Result: 验证了深度网络的稳定性，并证明ReLU等非线性函数在NTK Gram矩阵对角线上无有限宽度修正。

Conclusion: 该方法为有限宽度下的NTK统计提供了有效工具，并通过数值实验验证了其可行性。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [77] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息扩散模型的无监督异常检测方法，用于多元时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列领域已显示出在预测、填补、生成和异常检测中的有效性，但如何结合物理信息提升性能仍需探索。

Method: 使用加权物理信息损失在扩散模型训练中学习物理依赖的时间分布，通过静态权重调度构建损失函数。

Result: 实验表明，物理信息训练提高了异常检测的F1分数，生成数据多样性和对数似然性更好，优于基线方法和之前的工作。

Conclusion: 该方法在合成和真实数据集上表现优异，尤其是在合成数据集和部分真实数据集上超越了纯数据驱动的扩散模型。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [78] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST是一种去中心化联邦图学习框架，通过自适应通信机制解决现有方法在本地子图拓扑信息处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习优化策略未考虑本地子图的独特拓扑信息，而联邦图学习多采用中心化架构，未能发挥去中心化优势。

Method: 提出DFed-SST框架，采用双拓扑自适应通信机制，动态优化客户端间通信拓扑。

Result: 在八个真实数据集上，DFed-SST平均准确率比基线方法提高3.26%。

Conclusion: DFed-SST有效解决了异构性下的模型聚合问题，性能显著优于现有方法。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [79] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的嵌套算子推断方法，用于从高维动力系统的快照数据中学习物理信息的降阶模型。该方法通过利用降阶空间中的层次结构，优先考虑主导模式的相互作用，显著降低了重建误差。


<details>
  <summary>Details</summary>
Motivation: 传统算子推断方法在降阶模型学习中可能无法充分利用数据的层次结构，导致重建误差较大。本文旨在通过嵌套方法改进这一过程。

Method: 采用嵌套算子推断方法，利用降阶空间的层次结构迭代构造初始猜测，优先处理主导模式的相互作用。支持动态基和模型形式的更新。

Result: 在立方热传导问题中，嵌套方法比标准方法误差降低了四倍；在格陵兰冰盖的大规模参数化模型中，平均误差为3%，计算速度提升超过19,000倍。

Conclusion: 嵌套算子推断方法显著提升了降阶模型的精度和计算效率，适用于动态更新和大规模复杂系统。

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [80] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow是一个基于服务器的强化学习框架，解决了工业级RL中的训练与执行解耦和GPU利用率最大化问题。


<details>
  <summary>Details</summary>
Motivation: 工业级强化学习中，训练与复杂执行流程的耦合以及GPU资源利用率低是主要挑战。

Method: 引入数据平面解耦训练与代理执行，采用标签驱动调度和时空复用管道优化资源利用。

Result: 实现了稳定性和高性能，适用于多代理、长周期等复杂RL任务。

Conclusion: SeamlessFlow通过创新设计，为大规模RL部署提供了高效稳定的解决方案。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [81] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 该论文提出了一种基于马尔可夫游戏的框架，用于定量分析不同联盟结构对碳捕获与封存（CCS）项目中各利益相关者目标的影响，并通过多智能体强化学习解决多站点多利益相关者问题。


<details>
  <summary>Details</summary>
Motivation: CCS项目涉及多利益相关者，各自目标不同且地质条件复杂，独立优化难以实现，需研究协作联盟的有效性。

Method: 采用马尔可夫游戏框架和多智能体强化学习，结合安全约束，利用E2C框架的替代模型降低计算成本。

Result: 框架有效解决了多利益相关者参与时的CO2封存优化管理问题。

Conclusion: 协作联盟结构对CCS项目的成功至关重要，提出的方法为多利益相关者协作提供了量化工具。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [82] [Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with Game Theory](https://arxiv.org/abs/2508.11359)
*Jiejun Hu-Bolz,James Stovold*

Main category: cs.GT

TL;DR: 研究探讨人类与生成式AI系统能否通过信息驱动的交互融合为单一个体，使用信息论方法证明其可行性。


<details>
  <summary>Details</summary>
Motivation: 探索人类与生成式AI系统的共生关系，尤其是AI是否可能成为信息寄生虫。

Method: 将人类、生成式AI及环境建模为三玩家随机博弈，利用信息论指标（熵、互信息、转移熵）分析。

Result: 模型显示人类与AI可形成聚合个体，验证了Krakauer等人的理论。

Conclusion: 该模型为人类与AI的共生关系提供了新视角，揭示了AI可能的信息寄生行为。

Abstract: This work asks whether a human interacting with a generative AI system can
merge into a single individual through iterative, information-driven
interactions. We model the interactions between a human, a generative AI
system, and the human's wider environment as a three-player stochastic game. We
use information-theoretic measures (entropy, mutual information, and transfer
entropy) to show that our modelled human and generative AI are able to form an
aggregate individual in the sense of Krakauer et al. (2020). The model we
present is able to answer interesting questions around the symbiotic nature of
humans and AI systems, including whether LLM-driven chatbots are acting as
parasites, feeding on the information provided by humans.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [83] [CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems](https://arxiv.org/abs/2508.11287)
*Xuran Liu,Nan Xue,Rui Bao,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 提出了一种延迟感知调度框架，通过重叠模型加载与计算和通信，减少边缘设备上大型语言模型推理的总延迟。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有方法忽略了按需加载模型导致的冷启动延迟，影响了低延迟和隐私保护的AI服务部署。

Method: 设计了一个动态调整层分区和分配的框架，将问题建模为混合整数非线性规划，并开发动态规划算法优化分区和设备分配。

Result: 实验表明，该方法显著降低了冷启动延迟，优于基线策略。

Conclusion: 提出的框架有效减少了推理延迟，为边缘设备上的大型语言模型部署提供了实用解决方案。

Abstract: While deploying large language models on edge devices promises low-latency
and privacy-preserving AI services, it is hindered by limited device resources.
Although pipeline parallelism facilitates distributed inference, existing
approaches often ignore the cold-start latency caused by on-demand model
loading. In this paper, we propose a latency-aware scheduling framework that
overlaps model loading with computation and communication to minimize total
inference latency. Based on device and model parameters, the framework
dynamically adjusts layer partitioning and allocation to effectively hide
loading time, thereby eliminating as many idle periods as possible. We
formulate the problem as a Mixed-Integer Non-Linear Program and design an
efficient dynamic programming algorithm to optimize model partitioning and
device assignment. Experimental results show that the proposed method
significantly reduces cold-start latency compared to baseline strategies.

</details>


### [84] [Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks](https://arxiv.org/abs/2508.11291)
*Rui Bao,Nan Xue,Yaping Sun,Zhiyong Chen*

Main category: cs.IT

TL;DR: 论文提出了一种动态路由框架，用于在无线边缘设备协作环境中平衡LLM推理质量和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 无线通信与LLM结合面临推理质量与延迟的权衡问题，简单查询卸载延迟高，设备端模型计算能力不足。

Method: 提出动态路由框架，结合设备端轻量模型和边缘服务器强大模型，针对单轮和多轮查询设计成本模型。

Result: 实验显示框架在保持推理质量的同时，平均延迟降低5-15%，大模型调用减少10-20%。

Conclusion: 该框架有效解决了无线边缘环境中LLM部署的质量与延迟矛盾。

Abstract: The integration of wireless communications and Large Language Models (LLMs)
is poised to unlock ubiquitous intelligent services, yet deploying them in
wireless edge-device collaborative environments presents a critical trade-off
between inference quality and end-to-end latency. A fundamental mismatch exists
between task complexity and resource allocation: offloading simple queries
invites prohibitive latency, while on-device models lack the capacity for
demanding computations. To address this challenge, we propose a dynamic,
quality-latency aware routing framework that orchestrates inference between a
lightweight model on the mobile device and a powerful model on the edge server.
Our framework employs two distinct cost models: for single-turn queries, it
fuses a BERT-predicted semantic score with communication and computation
overheads; for multi-turn dialogues, it further quantifies context-aware costs
arising from model switching and KV-cache management. While maintaining full
inference quality, extensive experiments demonstrate that our framework cuts
average response latency by 5-15% and reduces large model invocations by 10-20%
against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

</details>
