{"id": "2510.08883", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.08883", "abs": "https://arxiv.org/abs/2510.08883", "authors": ["Anupam Gupta", "Roie Levin"], "title": "The Online Submodular Cover Problem", "comment": "Original version appeared in SODA 2020. There was a gap in the proof\n  of Theorem 12, which we remedy with an additional assumption (details in\n  Section 5)", "summary": "In the submodular cover problem, we are given a monotone submodular function\n$f$, and we want to pick the min-cost set $S$ such that $f(S) = f(N)$.\nMotivated by problems in network monitoring and resource allocation, we\nconsider the submodular cover problem in an online setting. As a concrete\nexample, suppose at each time $t$, a nonnegative monotone submodular function\n$g_t$ is given to us. We define $f^{(t)} = \\sum_{s \\leq t} g_s$ as the sum of\nall functions seen so far. We need to maintain a submodular cover of these\nsubmodular functions $f^{(1)}, f^{(2)}, \\ldots f^{(T)}$ in an online fashion;\ni.e., we cannot revoke previous choices. Formally, at each time $t$ we produce\na set $S_t \\subseteq N$ such that $f^{(t)}(S_t) = f^{(t)}(N)$ -- i.e., this set\n$S_t$ is a cover -- such that $S_{t-1} \\subseteq S_t$, so previously decisions\nto pick elements cannot be revoked. (We actually allow more general sequences\n$\\{f^{(t)}\\}$ of submodular functions, but this\nsum-of-simpler-submodular-functions case is useful for concreteness.)\n  We give polylogarithmic competitive algorithms for this online submodular\ncover problem. The competitive ratio on an input sequence of length $T$ is\n$O(\\ln n \\ln (T \\cdot f(N) / f_{\\text{min}}))$, where $f_{\\text{min}}$ is the\nsmallest nonzero marginal for functions $f^{(t)}$, and $|N| = n$. For the\nspecial case of online set cover, our competitive ratio matches that of Alon et\nal. [SIAM J. Comp. 03], which are best possible for polynomial-time online\nalgorithms unless $NP \\subseteq BPP$ (see Korman 04). Since existing offline\nalgorithms for submodular cover are based on greedy approaches which seem\ndifficult to implement online, the technical challenge is to (approximately)\nsolve the exponential-sized linear programming relaxation for submodular cover,\nand to round it, both in the online setting."}
{"id": "2510.09002", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09002", "abs": "https://arxiv.org/abs/2510.09002", "authors": ["D Ellis Hershkowitz", "Richard Z Huang"], "title": "Planar Length-Constrained Minimum Spanning Trees", "comment": null, "summary": "In length-constrained minimum spanning tree (MST) we are given an $n$-node\ngraph $G = (V,E)$ with edge weights $w : E \\to \\mathbb{Z}_{\\geq 0}$ and edge\nlengths $l: E \\to \\mathbb{Z}_{\\geq 0}$ along with a root node $r \\in V$ and a\nlength-constraint $h \\in \\mathbb{Z}_{\\geq 0}$. Our goal is to output a spanning\ntree of minimum weight according to $w$ in which every node is at distance at\nmost $h$ from $r$ according to $l$.\n  We give a polynomial-time algorithm for planar graphs which, for any constant\n$\\epsilon > 0$, outputs an $O\\left(\\log^{1+\\epsilon} n\\right)$-approximate\nsolution with every node at distance at most $(1+\\epsilon)h$ from $r$ for any\nconstant $\\epsilon > 0$. Our algorithm is based on new length-constrained\nversions of classic planar separators which may be of independent interest.\nAdditionally, our algorithm works for length-constrained Steiner tree.\nComplementing this, we show that any algorithm on general graphs for\nlength-constrained MST in which nodes are at most $2h$ from $r$ cannot achieve\nan approximation of $O\\left(\\log ^{2-\\epsilon} n\\right)$ for any constant\n$\\epsilon > 0$ under standard complexity assumptions; as such, our results\nseparate the approximability of length-constrained MST in planar and general\ngraphs."}
{"id": "2510.09027", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.09027", "abs": "https://arxiv.org/abs/2510.09027", "authors": ["Katie Clinch", "Serge Gaspers", "Tao Zixu He", "Simon Mackenzie", "Tiankuang Zhang"], "title": "A Faster Randomized Algorithm for Vertex Cover: An Automated Approach", "comment": null, "summary": "This work introduces two techniques for the design and analysis of branching\nalgorithms, illustrated through the case study of the Vertex Cover problem.\nFirst, we present a method for automatically generating branching rules through\na systematic case analysis of local structures. Second, we develop a new\ntechnique for analyzing randomized branching algorithms using the Measure &\nConquer method, offering greater flexibility in formulating branching rules. By\ncombining these innovations with additional techniques, we obtain the fastest\nknown randomized algorithms in different parameters for the Vertex Cover\nproblem on graphs with bounded degree (up to 6) and on general graphs. For\nexample, our algorithm solves Vertex Cover on subcubic graphs in\n$O^*(1.07625^n)$ time and $O^*(1.13132^k)$ time, respectively. For graphs with\nmaximum degree 4, we achieve running times of $O^*(1.13735^n)$ and\n$O^*(1.21103^k)$, while for general graphs we achieve $O^*(1.25281^k)$."}
{"id": "2510.08935", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08935", "abs": "https://arxiv.org/abs/2510.08935", "authors": ["Yingyi Zhang", "Pengyue Jia", "Derong Xu", "Yi Wen", "Xianneng Li", "Yichao Wang", "Wenlin Zhang", "Xiaopeng Li", "Weinan Gan", "Huifeng Guo", "Yong Liu", "Xiangyu Zhao"], "title": "Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) critically depends on effective query\nexpansion to retrieve relevant information. However, existing expansion methods\nadopt uniform strategies that overlook user-specific semantics, ignoring\nindividual expression styles, preferences, and historical context. In practice,\nidentical queries in text can express vastly different intentions across users.\nThis representational rigidity limits the ability of current RAG systems to\ngeneralize effectively in personalized settings. Specifically, we identify two\ncore challenges for personalization: 1) user expression styles are inherently\ndiverse, making it difficult for standard expansions to preserve personalized\nintent. 2) user corpora induce heterogeneous semantic structures-varying in\ntopical focus and lexical organization-which hinders the effective anchoring of\nexpanded queries within the user's corpora space. To address these challenges,\nwe propose Personalize Before Retrieve (PBR), a framework that incorporates\nuser-specific signals into query expansion prior to retrieval. PBR consists of\ntwo components: P-PRF, which generates stylistically aligned pseudo feedback\nusing user history for simulating user expression style, and P-Anchor, which\nperforms graph-based structure alignment over user corpora to capture its\nstructure. Together, they produce personalized query representations tailored\nfor retrieval. Experiments on two personalized benchmarks show that PBR\nconsistently outperforms strong baselines, with up to 10% gains on PersonaBench\nacross retrievers. Our findings demonstrate the value of modeling\npersonalization before retrieval to close the semantic gap in user-adaptive RAG\nsystems. Our code is available at https://github.com/Zhang-Yingyi/PBR-code."}
{"id": "2510.09050", "categories": ["cs.DS", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.09050", "abs": "https://arxiv.org/abs/2510.09050", "authors": ["Dildar Ali", "Rajibul Islam", "Suman Banerjee"], "title": "Multi-product Influence Maximization in Billboard Advertisement", "comment": "This paper has been accepted in ACM IKDD CODS-2025 conference", "summary": "Billboard Advertisement has emerged as an effective out-of-home advertisement\ntechnique where the goal is to select a limited number of slots and play\nadvertisement content over there with the hope that this will be observed by\nmany people, and effectively, a significant number of them will be influenced\ntowards the brand. Given a trajectory and a billboard database and a positive\ninteger $k$, how can we select $k$ highly influential slots to maximize\ninfluence? In this paper, we study a variant of this problem where a commercial\nhouse wants to make a promotion of multiple products, and there is an influence\ndemand for each product. We have studied two variants of the problem. In the\nfirst variant, our goal is to select $k$ slots such that the respective\ninfluence demand of each product is satisfied. In the other variant of the\nproblem, we are given with $\\ell$ integers $k_1,k_2, \\ldots, k_{\\ell}$, the\ngoal here is to search for $\\ell$ many set of slots $S_1, S_2, \\ldots,\nS_{\\ell}$ such that for all $i \\in [\\ell]$, $|S_{i}| \\leq k_i$ and for all $i\n\\neq j$, $S_i \\cap S_j=\\emptyset$ and the influence demand of each of the\nproducts gets satisfied. We model the first variant of the problem as a\nmulti-submodular cover problem and the second variant as its generalization.\nFor solving the first variant, we adopt the bi-criteria approximation\nalgorithm, and for the other variant, we propose a sampling-based approximation\nalgorithm. Extensive experiments with real-world trajectory and billboard\ndatasets highlight the effectiveness and efficiency of the proposed solution\napproach."}
{"id": "2510.08948", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08948", "abs": "https://arxiv.org/abs/2510.08948", "authors": ["Nan Lu", "Yurong Hu", "Jiaquan Fang", "Yan Liu", "Rui Dong", "Yiming Wang", "Rui Lin", "Shaoyi Xu"], "title": "SHERLOCK: Towards Dynamic Knowledge Adaptation in LLM-enhanced E-commerce Risk Management", "comment": null, "summary": "The growth of the e-commerce industry has intensified the adversarial\ndynamics between shadow economy actors and risk management teams. Companies\noften conduct risk investigations into suspicious cases to identify emerging\nfraud patterns, thereby enhancing both preemptive risk prevention and post-hoc\ngovernance. However, the sheer volume of case analyses imposes a substantial\nworkload on risk management analysts, as each case requires the integration of\nlong-term expert experience and meticulous scrutiny across multiple risk\ndimensions. Additionally, individual disparities among analysts hinder the\nestablishment of uniform and high-standard workflows. To address these\nchallenges, we propose the SHERLOCK framework, which leverages the reasoning\ncapabilities of large language models (LLMs) to assist analysts in risk\ninvestigations. Our approach consists of three primary components: (1)\nextracting risk management knowledge from multi-modal data and constructing a\ndomain knowledge base (KB), (2) building an intelligent platform guided by the\ndata flywheel paradigm that integrates daily operations, expert annotations,\nand model evaluations, with iteratively fine-tuning for preference alignment,\nand (3) introducing a Reflect & Refine (R&R) module that collaborates with the\ndomain KB to establish a rapid response mechanism for evolving risk patterns.\nExperiments conducted on the real-world transaction dataset from JD.com\ndemonstrate that our method significantly improves the precision of both\nfactual alignment and risk localization within the LLM analysis results.\nDeployment of the SHERLOCK-based LLM system on JD.com has substantially\nenhanced the efficiency of case investigation workflows for risk managers."}
{"id": "2510.08863", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.08863", "abs": "https://arxiv.org/abs/2510.08863", "authors": ["Deep Bodra", "Sushil Khairnar"], "title": "Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly", "comment": "NoSQL databases, performance benchmarking, cloud computing, Redis;\n  Aerospike, Dragonfly", "summary": "The rise of distributed applications and cloud computing has created a demand\nfor scalable, high-performance key-value storage systems. This paper presents a\nperformance evaluation of three prominent NoSQL key-value stores: Redis,\nAerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)\nframework. We conducted extensive experiments across three distinct workload\npatterns (read-heavy, write-heavy), and balanced while systematically varying\nclient concurrency from 1 to 32 clients. Our evaluation methodology captures\nboth latency, throughput, and memory characteristics under realistic\noperational conditions, providing insights into the performance trade-offs and\nscalability behaviour of each system"}
{"id": "2510.08793", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.08793", "abs": "https://arxiv.org/abs/2510.08793", "authors": ["Ataher Sams", "Simone Di Bari", "Besma Smida", "Natasha Devroye", "Daniela Tuninetti", "Giorgio Taricco"], "title": "On Estimation of Angles of Arrival in Monostatic ISAC Without Instantaneous Transmit CSI", "comment": "7 pages, 5 figures, Accepted at 61st Allerton Conference on\n  Communication, Control, and Computing, 2025", "summary": "This paper explores the fundamental limits of Integrated Sensing and\nCommunication (ISAC) in a more realistic setting compared to previous\nliterature when the Base Staion (BS) has only statistical CSI of the\ncommunication user rather than full CSI. We analyze a monostatic setting where\nthe BS performs multi-target Angle of Arrival (AoA) estimation while\nsimultaneously communicating with one of the targets. We assume that the BS has\nstatistical CSI about all AoAs, with less uncertainty in the AoA of the\ncommunication receiver. The communication receiver is assumed to have perfect\nCSI. Utilizing a Bayesian Cram\\'er-Rao Bound (BCRB) framework to characterize\nthe fundamental limits of sensing under minimum mean square error (MMSE)\ncriteria, we derive achievable BCRB-rate trade-off regions. Our approach\nintroduces a number of transmission strategies that share power across sensing\nand communication beams over a coherence time. Our analysis reveals that beam\nallocation strategies leveraging the principal eigenvectors of the\ntarget-specific sensing matrices minimize individual AoA estimation errors,\nwhile strategies balancing sensing and communication directions optimize joint\nestimation performance at the cost of individual accuracy. We demonstrate that\nleveraging updated BCRB-based sensing information for the communication\nreceiver, due to its lower channel uncertainty, enables significantly improved\ncommunication rates."}
{"id": "2510.08742", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.08742", "abs": "https://arxiv.org/abs/2510.08742", "authors": ["Amir Ban"], "title": "Unending Sequential Auctions", "comment": "Accepted to WINE 2025", "summary": "Sequential auctions for identical items with unit-demand, private-value\nbuyers are common and often occur periodically without end, as new bidders\nreplace departing ones. We model bidder uncertainty by introducing a\nprobability that a bidder must exit the auction in each period. Treating the\nsequential auction as a Markov process, we demonstrate the existence of a\nunique steady state.\n  In the absence of uncertainty, the steady state resembles a posted-price\nmechanism: bidders with values above a threshold almost surely win items by\nrepeatedly bidding the threshold price, while those below the threshold almost\nsurely do not. The equilibrium price corresponds to the threshold value that\nbalances supply (bidders with values above the threshold) and demand (auction\nwinners).\n  When uncertainty is introduced, the threshold value persists but becomes less\nprecise, growing \"fuzzier\" as uncertainty increases. This uncertainty benefits\nlow-value bidders, those below the threshold, by giving them a significant\nchance of winning. Surprisingly, high-value bidders also benefit from\nuncertainty, up to a certain value limit, as it lowers equilibrium bids and\nincreases their expected utility. On the other hand, this bidder uncertainty\noften reduces the auctioneer's utility."}
{"id": "2510.09124", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09124", "abs": "https://arxiv.org/abs/2510.09124", "authors": ["Rasmus Kyng", "Maximilian Probst Gutenberg", "Tim Rieder"], "title": "Random-Shift Revisited: Tight Approximations for Tree Embeddings and L1-Oblivious Routings", "comment": "Accepted to FOCS 2025", "summary": "We present a new and surprisingly simple analysis of random-shift\ndecompositions -- originally proposed by Miller, Peng, and Xu [SPAA'13]: We\nshow that decompositions for exponentially growing scales $D = 2^0, 2^1,\n\\ldots, 2^{\\log_2(\\operatorname{diam}(G))}$, have a tight constant trade-off\nbetween distance-to-center and separation probability on average across the\ndistance scales -- opposed to a necessary $\\Omega(\\log n)$ trade-off for a\nsingle scale.\n  This almost immediately yields a way to compute a tree $T$ for graph $G$ that\npreserves all graph distances with expected $O(\\log n)$-stretch. This gives an\nalternative proof that obtains tight approximation bounds of the seminal result\nby Fakcharoenphol, Rao, and Talwar [STOC'03] matching the $\\Omega(\\log n)$\nlower bound by Bartal [FOCS'96]. Our insights can also be used to refine the\nanalysis of a simple $\\ell_1$-oblivious routing proposed in [FOCS'22], yielding\na tight $O(\\log n)$ competitive ratio.\n  Our algorithms for constructing tree embeddings and $\\ell_1$-oblivious\nroutings can be implemented in the sequential, parallel, and distributed\nsettings with optimal work, depth, and rounds, up to polylogarithmic factors.\nPreviously, fast algorithms with tight guarantees were not known for tree\nembeddings in parallel and distributed settings, and for $\\ell_1$-oblivious\nroutings, not even a fast sequential algorithm was known."}
{"id": "2510.08985", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08985", "abs": "https://arxiv.org/abs/2510.08985", "authors": ["Xuan Lu", "Haohang Huang", "Rui Meng", "Yaohui Jin", "Wenjun Zeng", "Xiaoyu Shen"], "title": "Rethinking Reasoning in Document Ranking: Why Chain-of-Thought Falls Short", "comment": null, "summary": "Document reranking is a key component in information retrieval (IR), aimed at\nrefining initial retrieval results to improve ranking quality for downstream\ntasks. Recent studies--motivated by large reasoning models (LRMs)--have begun\nincorporating explicit chain-of-thought (CoT) reasoning into LLM-based\nrerankers. However, the effectiveness of such reasoning for ranking tasks\nremains underexplored. In this work, we present the first systematic study of\nreasoning in reranking across both pointwise and listwise settings, under both\nsupervised fine-tuning and reinforcement learning. Using diverse benchmarks,\nincluding reasoning-intensive datasets (BRIGHT) and standard IR benchmarks\n(BEIR), we find that reasoning-augmented rerankers consistently underperform\ntheir direct counterparts that predict rankings without CoT, despite\nsubstantially higher inference costs. Our analysis reveals three core\nlimitations: (i) in pointwise rerankers, reasoning breaks calibration and\nbiases models toward the positive class, raising TPR but lowering TNR, which\ninflates false positives and degrades ranking in negative-dominant pools; (ii)\nin listwise rerankers, reasoning improves in-domain fit but increases variance\nand fails to generalize out-of-domain, even when reinforcement learning\nshortens rationales; and (iii) overall, directly fine-tuned rerankers remain\nmore stable, effective, and robust. These findings challenge the assumption\nthat explicit reasoning is universally beneficial for reranking. We conclude by\nhighlighting future directions, including calibration-aware scoring for\npointwise rerankers and the design of concise, targeted reasoning strategies to\nmitigate overfitting and overthinking in listwise rerankers."}
{"id": "2510.08896", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08896", "abs": "https://arxiv.org/abs/2510.08896", "authors": ["Suming Qiu", "Jing Li", "Zhicheng Zhou", "Junjie Huang", "Linyuan Qiu", "Zhijie Sun"], "title": "HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance", "comment": null, "summary": "We present HES-SQL, a novel hybrid training framework that advances\nText-to-SQL generation through the integration of thinking-mode-fused\nsupervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).\nOur approach introduces three key innovations: (1) a skeleton-completeness\nscoring mechanism that enhances preference alignment between generated queries\nand optimal SQL structures; (2) a query-latency-aware reward system that\nincentivizes the generation of computationally efficient SQL queries; (3) a\nself-distillation process for thinking-mode completion that prevents\ndegradation of the model's reasoning capabilities. This framework enables\nhybrid thinking models to switch between reasoning and non-reasoning modes\nwhile improving SQL query accuracy and execution efficiency.\n  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under\ncontrolled single-user conditions, demonstrates that HES-SQL achieves\ncompetitive performance with execution accuracies of 79.14\\% and 54.9\\% on the\nBIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the\nend-to-end execution time of generated queries on the DBMS, averaged over\nmultiple runs to mitigate variance. Efficiency gains range from 11\\% to 20\\%\nrelative to supervised baselines. Our results establish a new paradigm for\nText-to-SQL systems that effectively balances semantic accuracy with\ncomputational efficiency through execution-informed reinforcement learning\n(RL). The proposed methodology has significant implications for developing\nrobust natural language interfaces to databases and can be extended to broader\nstructured generation tasks requiring both correctness and efficiency\noptimization."}
{"id": "2510.08887", "categories": ["cs.IT", "cs.IR", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.08887", "abs": "https://arxiv.org/abs/2510.08887", "authors": ["Zijian Zhang", "Mingyao Cui"], "title": "Observation Matrix Design for Densifying MIMO Channel Estimation via 2D Ice Filling", "comment": "17 pages, 8 figures", "summary": "In recent years, densifying multiple-input multiple-output (MIMO) has\nattracted much attention from the communication community. Thanks to the\nsubwavelength antenna spacing, the strong correlations among densifying\nantennas provide sufficient prior knowledge about channel state information\n(CSI). This inspires the careful design of observation matrices (e.g., transmit\nprecoders and receive combiners), that exploits the CSI prior knowledge, to\nboost channel estimation performance. Aligned with this vision, this work\nproposes to jointly design the combiners and precoders by maximizing the mutual\ninformation between the received pilots and densifying MIMO channels. A\ntwo-dimensional ice-filling (2DIF) algorithm is proposed to efficiently\naccomplish this objective. The algorithm is motivated by the fact that the\neigenspace of MIMO channel covariance can be decoupled into two\nsub-eigenspaces, which are associated with the correlations of transmitter\nantennas and receiver antennas, respectively. By properly setting the precoder\nand the combiner as the eigenvectors from these two sub-eigenspaces, the 2DIF\npromises to generate near-optimal observation matrices. Moreover, we further\nextend the 2DIF method to the popular hybrid combining systems, where a\ntwo-stage 2DIF (TS-2DIF) algorithm is developed to handle the analog combining\ncircuits realized by phase shifters. Simulation results demonstrate that,\ncompared to the state-of-the-art schemes, the proposed 2DIF and TS-2DIF methods\ncan achieve superior channel estimation accuracy."}
{"id": "2510.08788", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.08788", "abs": "https://arxiv.org/abs/2510.08788", "authors": ["Andrey Pudovikov", "Alexandra Khirianova", "Ekaterina Solodneva", "Gleb Molodtsov", "Aleksandr Katrutsa", "Yuriy Dorn", "Egor Samosvat"], "title": "Robust autobidding for noisy conversion prediction models", "comment": null, "summary": "Managing millions of digital auctions is an essential task for modern\nadvertising auction systems. The main approach to managing digital auctions is\nan autobidding approach, which depends on the Click-Through Rate and Conversion\nRate values. While these quantities are estimated with ML models, their\nprediction uncertainty directly impacts advertisers' revenue and bidding\nstrategies. To address this issue, we propose RobustBid, an efficient method\nfor robust autobidding taking into account uncertainty in CTR and CVR\npredictions. Our approach leverages advanced, robust optimization techniques to\nprevent large errors in bids if the estimates of CTR/CVR are perturbed. We\nderive the analytical solution of the stated robust optimization problem, which\nleads to the runtime efficiency of the RobustBid method. The synthetic,\niPinYou, and BAT benchmarks are used in our experimental evaluation of\nRobustBid. We compare our method with the non-robust baseline and the RiskBid\nalgorithm in terms of total conversion volume (TCV) and average cost-per-click\n($CPC_{avg}$) performance metrics. The experiments demonstrate that RobustBid\nprovides bids that yield larger TCV and smaller $CPC_{avg}$ than competitors in\nthe case of large perturbations in CTR/CVR predictions."}
{"id": "2510.09286", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09286", "abs": "https://arxiv.org/abs/2510.09286", "authors": ["Antoine Amarilli", "Mikaël Monet", "Rémi De Pretto"], "title": "Confluence of the Node-Domination and Edge-Domination Hypergraph Rewrite Rules", "comment": "8 pages", "summary": "In this note, we study two rewrite rules on hypergraphs, called\nedge-domination and node-domination, and show that they are confluent. These\nrules are rather natural and commonly used before computing the minimum hitting\nsets of a hypergraph. Intuitively, edge-domination allows us to remove\nhyperedges that are supersets of another hyperedge, and node-domination allows\nus to remove nodes whose incident hyperedges are a subset of that of another\nnode. We show that these rules are confluent up to isomorphism, i.e., if we\napply any sequences of edge-domination and node-domination rules, then the\nresulting hypergraphs can be made isomorphic via more rule applications. This\nin particular implies the existence of a unique minimal hypergraph, up to\nisomorphism."}
{"id": "2510.09129", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09129", "abs": "https://arxiv.org/abs/2510.09129", "authors": ["Yansong Wang", "Qihui Lin", "Junjie Huang", "Tao Jia"], "title": "Generative Data Augmentation in Graph Contrastive Learning for Recommendation", "comment": "The 34th ACM International Conference on Information and Knowledge\n  Management", "summary": "Recommendation systems have become indispensable in various online platforms,\nfrom e-commerce to streaming services. A fundamental challenge in this domain\nis learning effective embeddings from sparse user-item interactions. While\ncontrastive learning has recently emerged as a promising solution to this\nissue, generating augmented views for contrastive learning through most\nexisting random data augmentation methods often leads to the alteration of\noriginal semantic information. In this paper, we propose a novel framework,\nGDA4Rec (Generative Data Augmentation in graph contrastive learning for\nRecommendation) to generate high-quality augmented views and provide robust\nself-supervised signals. Specifically, we employ a noise generation module that\nleverages deep generative models to approximate the distribution of original\ndata for data augmentation. Additionally, GDA4Rec further extracts an item\ncomplement matrix to characterize the latent correlations between items and\nprovide additional self-supervised signals. Lastly, a joint objective that\nintegrates recommendation, data augmentation and contrastive learning is used\nto enforce the model to learn more effective and informative embeddings.\nExtensive experiments are conducted on three public datasets to demonstrate the\nsuperiority of the model. The code is available at:\nhttps://github.com/MrYansong/GDA4Rec."}
{"id": "2510.09050", "categories": ["cs.DS", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.09050", "abs": "https://arxiv.org/abs/2510.09050", "authors": ["Dildar Ali", "Rajibul Islam", "Suman Banerjee"], "title": "Multi-product Influence Maximization in Billboard Advertisement", "comment": "This paper has been accepted in ACM IKDD CODS-2025 conference", "summary": "Billboard Advertisement has emerged as an effective out-of-home advertisement\ntechnique where the goal is to select a limited number of slots and play\nadvertisement content over there with the hope that this will be observed by\nmany people, and effectively, a significant number of them will be influenced\ntowards the brand. Given a trajectory and a billboard database and a positive\ninteger $k$, how can we select $k$ highly influential slots to maximize\ninfluence? In this paper, we study a variant of this problem where a commercial\nhouse wants to make a promotion of multiple products, and there is an influence\ndemand for each product. We have studied two variants of the problem. In the\nfirst variant, our goal is to select $k$ slots such that the respective\ninfluence demand of each product is satisfied. In the other variant of the\nproblem, we are given with $\\ell$ integers $k_1,k_2, \\ldots, k_{\\ell}$, the\ngoal here is to search for $\\ell$ many set of slots $S_1, S_2, \\ldots,\nS_{\\ell}$ such that for all $i \\in [\\ell]$, $|S_{i}| \\leq k_i$ and for all $i\n\\neq j$, $S_i \\cap S_j=\\emptyset$ and the influence demand of each of the\nproducts gets satisfied. We model the first variant of the problem as a\nmulti-submodular cover problem and the second variant as its generalization.\nFor solving the first variant, we adopt the bi-criteria approximation\nalgorithm, and for the other variant, we propose a sampling-based approximation\nalgorithm. Extensive experiments with real-world trajectory and billboard\ndatasets highlight the effectiveness and efficiency of the proposed solution\napproach."}
{"id": "2510.09015", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09015", "abs": "https://arxiv.org/abs/2510.09015", "authors": ["Shota Saito", "Hamdi Joudeh"], "title": "Soft Guessing Under Logarithmic Loss Allowing Errors and Variable-Length Source Coding", "comment": null, "summary": "This paper considers the problem of soft guessing under a logarithmic loss\ndistortion measure while allowing errors. We find an optimal guessing strategy,\nand derive single-shot upper and lower bounds for the minimal guessing moments\nas well as an asymptotic expansion for i.i.d. sources. These results are\nextended to the case where side information is available to the guesser.\nFurthermore, a connection between soft guessing allowing errors and\nvariable-length lossy source coding under logarithmic loss is demonstrated. The\nR\\'enyi entropy, the smooth R\\'enyi entropy, and their conditional versions\nplay an important role."}
{"id": "2510.08869", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.08869", "abs": "https://arxiv.org/abs/2510.08869", "authors": ["Patrick Mesana", "Gilles Caporossi", "Sebastien Gambs"], "title": "Measuring the Hidden Cost of Data Valuation through Collective Disclosure", "comment": null, "summary": "Data valuation methods assign marginal utility to each data point that has\ncontributed to the training of a machine learning model. If used directly as a\npayout mechanism, this creates a hidden cost of valuation, in which\ncontributors with near-zero marginal value would receive nothing, even though\ntheir data had to be collected and assessed. To better formalize this cost, we\nintroduce a conceptual and game-theoretic model, the Information Disclosure\nGame, between a Data Union (sometimes also called a data trust), a member-run\nagent representing contributors, and a Data Consumer (e.g., a platform). After\nfirst aggregating members' data, the DU releases information progressively by\nadding Laplacian noise under a differentially-private mechanism. Through\nsimulations with strategies guided by data Shapley values and multi-armed\nbandit exploration, we demonstrate on a Yelp review helpfulness prediction task\nthat data valuation inherently incurs an explicit acquisition cost and that the\nDU's collective disclosure policy changes how this cost is distributed across\nmembers."}
{"id": "2510.09311", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09311", "abs": "https://arxiv.org/abs/2510.09311", "authors": ["Philip Bille", "Inge Li Gørtz", "Rikke Schjeldrup Jessen"], "title": "Improved Extended Regular Expression Matching", "comment": null, "summary": "An extended regular expression $R$ specifies a set of strings formed by\ncharacters from an alphabet combined with concatenation, union, intersection,\ncomplement, and star operators. Given an extended regular expression $R$ and a\nstring $Q$, the extended regular expression matching problem is to decide if\n$Q$ matches any of the strings specified by $R$. Extended regular expressions\nare a basic concept in formal language theory and a basic primitive for\nsearching and processing data. Extended regular expression matching was\nintroduced by Hopcroft and Ullmann in the 1970s [\\textit{Introduction to\nAutomata Theory, Languages and Computation}, 1979], who gave a simple dynamic\nprogramming solution using $O(n^3m)$ time and $O(n^2m)$ space, where $n$ is the\nlength of $Q$ and $m$ is the length of $R$. Since then, several solutions have\nbeen proposed, but few significant asymptotic improvements have been obtained.\nThe current state-of-the art solution, by Yamamoto and Miyazaki~[COCOON, 2003],\nuses $O(\\frac{n^3k + n^2m}{w} + n + m)$ time and $O(\\frac{n^2k + nm}{w} + n +\nm)$ space, where $k$ is the number of negation and complement operators in $R$\nand $w$ is the number of bits in a word. This roughly replaces the $m$ factor\nwith $k$ in the dominant terms of both the space and time bounds of the\nHopcroft and Ullmann algorithm.\n  We revisit the problem and present a new solution that significantly improves\nthe previous time and space bounds. Our main result is a new algorithm that\nsolves extended regular expression matching in \\[O\\left(n^\\omega k +\n\\frac{n^2m}{\\min(w/\\log w, \\log n)} + m\\right)\\] time and $O(\\frac{n^2 \\log\nk}{w} + n + m) = O(n^2 +m)$ space, where $\\omega \\approx 2.3716$ is the\nexponent of matrix multiplication. Essentially, this replaces the dominant\n$n^3k$ term with $n^\\omega k$ in the time bound, while simultaneously improving\nthe $n^2k$ term in the space to $O(n^2)$."}
{"id": "2510.09136", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09136", "abs": "https://arxiv.org/abs/2510.09136", "authors": ["Marlene Holzleitner", "Stephan Leitner", "Hanna Lind Jorgensen", "Christoph Schmitz", "Jacob Welander", "Dietmar Jannach"], "title": "Controlled Personalization in Legacy Media Online Services: A Case Study in News Recommendation", "comment": null, "summary": "Personalized news recommendations have become a standard feature of large\nnews aggregation services, optimizing user engagement through automated content\nselection. In contrast, legacy news media often approach personalization\ncautiously, striving to balance technological innovation with core editorial\nvalues. As a result, online platforms of traditional news outlets typically\ncombine editorially curated content with algorithmically selected articles - a\nstrategy we term controlled personalization. In this industry paper, we\nevaluate the effectiveness of controlled personalization through an A/B test\nconducted on the website of a major Norwegian legacy news organization. Our\nfindings indicate that even a modest level of personalization yields\nsubstantial benefits. Specifically, we observe that users exposed to\npersonalized content demonstrate higher click-through rates and reduced\nnavigation effort, suggesting improved discovery of relevant content. Moreover,\nour analysis reveals that controlled personalization contributes to greater\ncontent diversity and catalog coverage and in addition reduces popularity bias.\nOverall, our results suggest that controlled personalization can successfully\nalign user needs with editorial goals, offering a viable path for legacy media\nto adopt personalization technologies while upholding journalistic values."}
{"id": "2510.09084", "categories": ["cs.GT", "cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09084", "abs": "https://arxiv.org/abs/2510.09084", "authors": ["Dildar Ali", "Suman Benerjee", "Yamuna Prasad"], "title": "Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising", "comment": "12 Pages", "summary": "In a typical \\emph{billboard advertisement} technique, a number of digital\nbillboards are owned by an \\emph{influence provider}, and several commercial\nhouses approach the influence provider for a specific number of views of their\nadvertisement content on a payment basis. If the influence provider provides\nthe demanded or more influence, then he will receive the full payment else a\npartial payment. In the context of an influence provider, if he provides more\nor less than the advertisers demanded influence, it is a loss for him. This is\nformalized as 'Regret', and naturally, in the context of the influence\nprovider, the goal will be to allocate the billboard slots among the\nadvertisers such that the total regret is minimized. In this paper, we study\nthis problem as a discrete optimization problem and propose two solution\napproaches. The first one selects the billboard slots from the available ones\nin an incremental greedy manner, and we call this method the Budget Effective\nGreedy approach. In the second one, we introduce randomness in the first one,\nwhere we do it for a sample of slots instead of calculating the marginal gains\nof all the billboard slots. We analyze both algorithms to understand their time\nand space complexity. We implement them with real-life datasets and conduct a\nnumber of experiments. We observe that the randomized budget effective greedy\napproach takes reasonable computational time while minimizing the regret."}
{"id": "2510.09039", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09039", "abs": "https://arxiv.org/abs/2510.09039", "authors": ["Wenjun Zhang", "An-An Lu", "Xiqi Gao"], "title": "Low Complexity Detector for XL-MIMO Uplink: A Cross Splitting Based Information Geometry Approach", "comment": null, "summary": "In this paper, we propose the cross splitting based information geometry\napproach (CS-IGA), a novel and low complexity iterative detector for uplink\nsignal recovery in extralarge-scale MIMO (XL-MIMO) systems. Conventional\niterative detectors, such as the approximate message passing (AMP) algorithm\nand the traditional information geometry algorithm (IGA), suffer from a per\niteration complexity that scales with the number of base station (BS) antennas,\ncreating a computational bottleneck. To overcome this, CS-IGA introduces a\nnovel cross matrix splitting of the natural parameter in the a posteriori\ndistribution. This factorization allows the iterative detection based on the\nmatched filter, which reduces per iteration computational complexity.\nFurthermore, we extend this framework to nonlinear detection and propose\nnonlinear CSIGA (NCS-IGA) by seamlessly embedding discrete constellation\nconstraints, enabling symbol-wise processing without external interference\ncancellation loops. Comprehensive simulations under realistic channel\nconditions demonstrate that CS-IGA matches or surpasses the bit error rate\n(BER) performance of Bayes optimal AMP and IGA for both linear and nonlinear\ndetection, while achieving this with fewer iterations and a substantially lower\ncomputational cost. These results establish CS-IGA as a practical and powerful\nsolution for high-throughput signal detection in next generation XL-MIMO\nsystems."}
{"id": "2510.09084", "categories": ["cs.GT", "cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09084", "abs": "https://arxiv.org/abs/2510.09084", "authors": ["Dildar Ali", "Suman Benerjee", "Yamuna Prasad"], "title": "Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising", "comment": "12 Pages", "summary": "In a typical \\emph{billboard advertisement} technique, a number of digital\nbillboards are owned by an \\emph{influence provider}, and several commercial\nhouses approach the influence provider for a specific number of views of their\nadvertisement content on a payment basis. If the influence provider provides\nthe demanded or more influence, then he will receive the full payment else a\npartial payment. In the context of an influence provider, if he provides more\nor less than the advertisers demanded influence, it is a loss for him. This is\nformalized as 'Regret', and naturally, in the context of the influence\nprovider, the goal will be to allocate the billboard slots among the\nadvertisers such that the total regret is minimized. In this paper, we study\nthis problem as a discrete optimization problem and propose two solution\napproaches. The first one selects the billboard slots from the available ones\nin an incremental greedy manner, and we call this method the Budget Effective\nGreedy approach. In the second one, we introduce randomness in the first one,\nwhere we do it for a sample of slots instead of calculating the marginal gains\nof all the billboard slots. We analyze both algorithms to understand their time\nand space complexity. We implement them with real-life datasets and conduct a\nnumber of experiments. We observe that the randomized budget effective greedy\napproach takes reasonable computational time while minimizing the regret."}
{"id": "2510.09334", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09334", "abs": "https://arxiv.org/abs/2510.09334", "authors": ["Peteris Daugulis"], "title": "Optimizing Administrative Divisions: A Vertex $k$-Center Approach for Edge-Weighted Road Graphs", "comment": null, "summary": "Efficient and equitable access to municipal services hinges on well-designed\nadministrative divisions. It requires ongoing adaptation to changing\ndemographics, infrastructure, and economic factors. This article proposes a\nnovel transparent data-driven method for territorial division based on the\nVoronoi partition of edge-weighted road graphs and the vertex $k$-center\nproblem as a special case of the minimax facility location problem. By\nconsidering road network structure and strategic placement of administrative\ncenters, this method seeks to minimize travel time disparities and ensure a\nmore balanced distribution of administrative time burden for the population. We\nshow implementations of this approach in the context of Latvia, a country with\ncomplex geographical features and diverse population distribution."}
{"id": "2510.09167", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09167", "abs": "https://arxiv.org/abs/2510.09167", "authors": ["Minmao Wang", "Xingchen Liu", "Shijie Yi", "Likang Wu", "Hongke Zhao", "Fei Pan", "Qingpeng Cai", "Peng Jiang"], "title": "Hierarchical Semantic RL: Tackling the Problem of Dynamic Action Space for RL-based Recommendations", "comment": null, "summary": "Recommender Systems (RS) are fundamental to modern online services. While\nmost existing approaches optimize for short-term engagement, recent work has\nbegun to explore reinforcement learning (RL) to model long-term user value.\nHowever, these efforts face significant challenges due to the vast, dynamic\naction spaces inherent in recommendation, which hinder stable policy learning.\nTo resolve this bottleneck, we introduce Hierarchical Semantic RL (HSRL), which\nreframes RL-based recommendation over a fixed Semantic Action Space (SAS). HSRL\nencodes items as Semantic IDs (SIDs) for policy learning, and maps SIDs back to\ntheir original items via a fixed, invertible lookup during execution. To align\ndecision-making with SID generation, the Hierarchical Policy Network (HPN)\noperates in a coarse-to-fine manner, employing hierarchical residual state\nmodeling to refine each level's context from the previous level's residual,\nthereby stabilizing training and reducing representation-decision mismatch. In\nparallel, a Multi-level Critic (MLC) provides token-level value estimates,\nenabling fine-grained credit assignment. Across public benchmarks and a\nlarge-scale production dataset from a leading Chinese short-video advertising\nplatform, HSRL consistently surpasses state-of-the-art baselines. In online\ndeployment over a seven-day A/B testing, it delivers an 18.421% CVR lift with\nonly a 1.251% increase in cost, supporting HSRL as a scalable paradigm for\nRL-based recommendation. Our code is released at\nhttps://github.com/MinmaoWang/HSRL."}
{"id": "2510.09057", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09057", "abs": "https://arxiv.org/abs/2510.09057", "authors": ["Ankit Yadav", "Ritumoni Sarma", "Anuj Kumar Bhagat"], "title": "Optimal binary codes from $\\mathcal{C}_{D}$-codes over a non-chain ring", "comment": null, "summary": "In \\cite{shi2022few-weight}, Shi and Li studied $\\mathcal{C}_D$-codes over\nthe ring $\\mathcal{R}:=\\mathbb{F}_2[x,y]/\\langle x^2, y^2, xy-yx\\rangle$ and\ntheir binary Gray images, where $D$ is derived using certain simplicial\ncomplexes. We study the subfield codes $\\mathcal{C}_{D}^{(2)}$ of\n$\\mathcal{C}_{D}$-codes over $\\mathcal{R},$ where $D$ is as in\n\\cite{shi2022few-weight} and more. We find the Hamming weight distribution and\nthe parameters of $\\mathcal{C}_D^{(2)}$ for various $D$, and identify several\ninfinite families of codes that are distance-optimal. Besides, we provide\nsufficient conditions under which these codes are minimal and self-orthogonal.\nTwo families of strongly regular graphs are obtained as an application of the\nconstructed two-weight codes."}
{"id": "2510.09432", "categories": ["cs.DS", "cs.CC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.09432", "abs": "https://arxiv.org/abs/2510.09432", "authors": ["Mats Vroon", "Hans L. Bodlaender"], "title": "On Stable Cutsets in General and Minimum Degree Constrained Graphs", "comment": null, "summary": "A stable cutset is a set of vertices $S$ of a connected graph, that is\npairwise non-adjacent and when deleting $S$, the graph becomes disconnected.\nDetermining the existence of a stable cutset in a graph is known to be\nNP-complete. In this paper, we introduce a new exact algorithm for Stable\nCutset. By branching on graph configurations and using the $O^*(1.3645)$\nalgorithm for the (3,2)-Constraint Satisfaction Problem presented by Beigel and\nEppstein, we achieve an improved running time of $O^*(1.2972^n)$.\n  In addition, we investigate the Stable Cutset problem for graphs with a bound\non the minimum degree $\\delta$. First, we show that if the minimum degree of a\ngraph $G$ is at least $\\frac{2}{3}(n-1)$, then $G$ does not contain a stable\ncutset. Furthermore, we provide a polynomial-time algorithm for graphs where\n$\\delta \\geq \\tfrac{1}{2}n$, and a similar kernelisation algorithm for graphs\nwhere $\\delta = \\tfrac{1}{2}n - k$.\n  Finally, we prove that Stable Cutset remains NP-complete for graphs with\nminimum degree $c$, where $c > 1$. We design an exact algorithm for this\nproblem that runs in $O^*(\\lambda^n)$ time, where $\\lambda$ is the positive\nroot of $x^{\\delta + 2} - x^{\\delta + 1} + 6$. This algorithm can also be\napplied to the \\textsc{3-Colouring} problem with the same minimum degree\nconstraint, leading to an improved exact algorithm as well."}
{"id": "2510.09393", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09393", "abs": "https://arxiv.org/abs/2510.09393", "authors": ["Dakai Zhai", "Jiong Gao", "Boya Du", "Junwei Xu", "Qijie Shen", "Jialin Zhu", "Yuning Jiang"], "title": "ChoirRec: Semantic User Grouping via LLMs for Conversion Rate Prediction of Low-Activity Users", "comment": null, "summary": "Accurately predicting conversion rates (CVR) for low-activity users remains a\nfundamental challenge in large-scale e-commerce recommender systems.Existing\napproaches face three critical limitations: (i) reliance on noisy and\nunreliable behavioral signals; (ii) insufficient user-level information due to\nthe lack of diverse interaction data; and (iii) a systemic training bias toward\nhigh-activity users that overshadows the needs of low-activity users.To address\nthese challenges, we propose ChoirRec, a novel framework that leverages the\nsemantic capabilities of Large Language Models (LLMs) to construct semantic\nuser groups and enhance CVR prediction for low-activity users.With a\ndual-channel architecture designed for robust cross-user knowledge transfer,\nChoirRec comprises three components: (i) a Semantic Group Generation module\nthat utilizes LLMs to form reliable, cross-activity user clusters, thereby\nfiltering out noisy signals; (ii) a Group-aware Hierarchical Representation\nmodule that enriches sparse user embeddings with informative group-level priors\nto mitigate data insufficiency; and (iii) a Group-aware Multi-granularity\nModual that employs a dual-channel architecture and adaptive fusion mechanism\nto ensure effective learning and utilization of group knowledge. We conduct\nextensive offline and online experiments on Taobao, a leading industrial-scale\ne-commerce platform.ChoirRec improves GAUC by 1.16\\% in offline evaluations,\nwhile online A/B testing reveals a 7.24\\% increase in order volume,\nhighlighting its substantial practical value in real-world applications."}
{"id": "2510.09215", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09215", "abs": "https://arxiv.org/abs/2510.09215", "authors": ["Sai Pradeep Muppaneni", "Vineetha Yogesh", "A. Chockalingam"], "title": "A Hybrid I/O Relation Estimation Scheme for Zak-OTFS Receivers", "comment": "Accepted in IEEE Open Journal of the Communications Society", "summary": "In this paper, we consider the problem of estimating the delay-Doppler (DD)\ndomain input-output (I/O) relation in Zak-OTFS modulation, which is needed for\nsignal detection. Two approaches, namely, model-dependent and model-free\napproaches, can be employed for this purpose. The model-dependent approach\nrequires explicit estimation of the physical channel parameters (path delays,\nDopplers, and gains) to obtain the I/O relation. Such an explicit estimation is\nnot required in the model-free approach, where the I/O relation can be\nestimated by reading off the samples in the fundamental DD period of the\nreceived pilot frame. Model-free approach has the advantage of acquiring\nfractional DD channels with simplicity. However, the read-off in the model-free\napproach provides an estimate of the effective channel only over a limited\nregion in the DD plane but it does not provide an estimate for the region\noutside, and this can affect the estimation performance depending on the pulse\nshaping characteristics of the DD pulse shaping filter used. A poorly localized\nDD pulse shape leads to an increased degradation in performance. Motivated by\nthis, in this paper, we propose a novel, yet simple, I/O relation estimation\nscheme that alleviates the above issue in the model-free approach. We achieve\nthis by obtaining a coarse estimate of the effective channel outside the\nmodel-free estimation region using a novel model-dependent scheme and using\nthis estimate along with the model-free estimate to obtain an improved estimate\nof the overall I/O relation. We devise the proposed estimation scheme for both\nexclusive and embedded pilot frames. Our simulation results using Vehicular-A,\nTDL-A and TDL-C channel models with fractional DDs show that the proposed\nhybrid estimation approach achieves superior performance compared to the pure\nmodel-free approach."}
{"id": "2510.09512", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2510.09512", "abs": "https://arxiv.org/abs/2510.09512", "authors": ["Mark Jones", "Jannik Schestag"], "title": "Parameterized Algorithms for Diversity of Networks with Ecological Dependencies", "comment": null, "summary": "For a phylogenetic tree, the phylogenetic diversity of a set A of taxa is the\ntotal weight of edges on paths to A. Finding small sets of maximal diversity is\ncrucial for conservation planning, as it indicates where limited resources can\nbe invested most efficiently. In recent years, efficient algorithms have been\ndeveloped to find sets of taxa that maximize phylogenetic diversity either in a\nphylogenetic network or in a phylogenetic tree subject to ecological\nconstraints, such as a food web. However, these aspects have mostly been\nstudied independently. Since both factors are biologically important, it seems\nnatural to consider them together. In this paper, we introduce decision\nproblems where, given a phylogenetic network, a food web, and integers k, and\nD, the task is to find a set of k taxa with phylogenetic diversity of at least\nD under the maximize all paths measure, while also satisfying viability\nconditions within the food web. Here, we consider different definitions of\nviability, which all demand that a \"sufficient\" number of prey species survive\nto support surviving predators. We investigate the parameterized complexity of\nthese problems and present several fixed-parameter tractable (FPT) algorithms.\nSpecifically, we provide a complete complexity dichotomy characterizing which\ncombinations of parameters - out of the size constraint k, the acceptable\ndiversity loss D, the scanwidth of the food web, the maximum in-degree in the\nnetwork, and the network height h - lead to W[1]-hardness and which admit FPT\nalgorithms. Our primary methodological contribution is a novel algorithmic\nframework for solving phylogenetic diversity problems in networks where\ndependencies (such as those from a food web) impose an order, using a color\ncoding approach."}
{"id": "2510.09510", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09510", "abs": "https://arxiv.org/abs/2510.09510", "authors": ["Siyue Zhang", "Yuan Gao", "Xiao Zhou", "Yilun Zhao", "Tingyu Song", "Arman Cohan", "Anh Tuan Luu", "Chen Zhao"], "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval", "comment": null, "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios."}
{"id": "2510.09220", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09220", "abs": "https://arxiv.org/abs/2510.09220", "authors": ["Marvin Rübenacke", "Sebastian Cammerer", "Michael Sullivan", "Alexander Keller"], "title": "Serial Polar Automorphism Ensemble Decoders for Physical Unclonable Functions", "comment": "7 Pages, 7 Figures, submitted to IEEE for possible publication", "summary": "Physical unclonable functions (PUFs) involve challenging practical\napplications of error-correcting codes (ECCs), requiring extremely low failure\nrates on the order of $10^{-6}$ and below despite raw input bit error rates as\nhigh as 22%. These requirements call for an efficient ultra-low rate code\ndesign. In this work, we propose a novel coding scheme tailored for PUFs based\non Polar codes and a low-complexity version of automorphism ensemble decoding\n(AED). Notably, our serial AED scheme reuses a single successive cancellation\n(SC) decoder across multiple decoding attempts. By introducing cascaded and\nrecursive interleavers, we efficiently scale the number of AED candidates\nwithout requiring expensive large multiplexers. An aggressive quantization\nstrategy of only 3 bits per message further reduces the area requirements of\nthe underlying SC decoder. The resulting coding scheme achieves the same block\nerror rate of $10^{-6}$ as our baseline based on Bose-Ray-Chaudhuri-Hocquenghem\n(BCH) codes while requiring 1.75x fewer codeword bits to encode the same K =\n312 payload bits. This reduction translates directly into 1.75x less helper\ndata storage and, consequently, a smaller overall chip area."}
{"id": "2510.09589", "categories": ["cs.DS", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.09589", "abs": "https://arxiv.org/abs/2510.09589", "authors": ["Aflatoun Amouzandeh", "Klaus Jansen", "Lis Pirotton", "Rob van Stee", "Corinna Wambsganz"], "title": "Minimizing the Weighted Makespan with Restarts on a Single Machine", "comment": "15 pages, 4 figures", "summary": "We consider the problem of minimizing the weighted makespan on a single\nmachine with restarts. Restarts are similar to preemptions but weaker: a job\ncan be interrupted, but then it has to be run again from the start instead of\nresuming at the point of interruption later. The objective is to minimize the\nweighted makespan, defined as the maximum weighted completion time of jobs.\n  We establish a lower bound of 1.4656 on the competitive ratio achievable by\ndeterministic online algorithms. For the case where all jobs have identical\nprocessing times, we design and analyze a deterministic online algorithm that\nimproves the competitive ratio to better than 1.3098. Finally, we prove a lower\nbound of 1.2344 for this case."}
{"id": "2510.09557", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.09557", "abs": "https://arxiv.org/abs/2510.09557", "authors": ["Tzu-Lin Kuo", "Wei-Ning Chiu", "Wei-Yun Ma", "Pu-Jen Cheng"], "title": "Doc2Query++: Topic-Coverage based Document Expansion and its Application to Dense Retrieval via Dual-Index Fusion", "comment": "11 pages, 4 figures", "summary": "Document expansion (DE) via query generation tackles vocabulary mismatch in\nsparse retrieval, yet faces limitations: uncontrolled generation producing\nhallucinated or redundant queries with low diversity; poor generalization from\nin-domain training (e.g., MS MARCO) to out-of-domain data like BEIR; and noise\nfrom concatenation harming dense retrieval. While Large Language Models (LLMs)\nenable cross-domain query generation, basic prompting lacks control, and\ntaxonomy-based methods rely on domain-specific structures, limiting\napplicability. To address these challenges, we introduce Doc2Query++, a DE\nframework that structures query generation by first inferring a document's\nlatent topics via unsupervised topic modeling for cross-domain applicability,\nthen using hybrid keyword selection to create a diverse and relevant keyword\nset per document. This guides LLM not only to leverage keywords, which ensure\ncomprehensive topic representation, but also to reduce redundancy through\ndiverse, relevant terms. To prevent noise from query appending in dense\nretrieval, we propose Dual-Index Fusion strategy that isolates text and query\nsignals, boosting performance in dense settings. Extensive experiments show\nDoc2Query++ significantly outperforms state-of-the-art baselines, achieving\nsubstantial gains in MAP, nDCG@10 and Recall@100 across diverse datasets on\nboth sparse and dense retrieval."}
{"id": "2510.09478", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09478", "abs": "https://arxiv.org/abs/2510.09478", "authors": ["Sina Beyraghi", "Javad Shabanpour", "Giovanni Geraci", "Paul Almasan", "Angel Lozano"], "title": "Site-Specific RIS Deployment in Cellular Networks via Calibrated Ray Tracing", "comment": null, "summary": "This work introduces a fully-automated RIS deployment strategy validated\nthrough a digital twin, powered by Sionna ray tracing, of a UK city. On a scene\ncalibrated with measured data, the method jointly optimizes RIS placement,\norientation, configuration, and BS beamforming across 4G, 5G, and hypothetical\n6G frequencies. Candidate RIS sites are identified via scattering-based rays,\nwhile user clustering reduces deployment overhead. Results show that meaningful\ncoverage enhancement requires dense, large-aperture RIS deployments, raising\nquestions about the practicality and cost of large-scale RIS adoption."}
{"id": "2510.09084", "categories": ["cs.GT", "cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09084", "abs": "https://arxiv.org/abs/2510.09084", "authors": ["Dildar Ali", "Suman Benerjee", "Yamuna Prasad"], "title": "Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising", "comment": "12 Pages", "summary": "In a typical \\emph{billboard advertisement} technique, a number of digital\nbillboards are owned by an \\emph{influence provider}, and several commercial\nhouses approach the influence provider for a specific number of views of their\nadvertisement content on a payment basis. If the influence provider provides\nthe demanded or more influence, then he will receive the full payment else a\npartial payment. In the context of an influence provider, if he provides more\nor less than the advertisers demanded influence, it is a loss for him. This is\nformalized as 'Regret', and naturally, in the context of the influence\nprovider, the goal will be to allocate the billboard slots among the\nadvertisers such that the total regret is minimized. In this paper, we study\nthis problem as a discrete optimization problem and propose two solution\napproaches. The first one selects the billboard slots from the available ones\nin an incremental greedy manner, and we call this method the Budget Effective\nGreedy approach. In the second one, we introduce randomness in the first one,\nwhere we do it for a sample of slots instead of calculating the marginal gains\nof all the billboard slots. We analyze both algorithms to understand their time\nand space complexity. We implement them with real-life datasets and conduct a\nnumber of experiments. We observe that the randomized budget effective greedy\napproach takes reasonable computational time while minimizing the regret."}
{"id": "2510.08887", "categories": ["cs.IT", "cs.IR", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.08887", "abs": "https://arxiv.org/abs/2510.08887", "authors": ["Zijian Zhang", "Mingyao Cui"], "title": "Observation Matrix Design for Densifying MIMO Channel Estimation via 2D Ice Filling", "comment": "17 pages, 8 figures", "summary": "In recent years, densifying multiple-input multiple-output (MIMO) has\nattracted much attention from the communication community. Thanks to the\nsubwavelength antenna spacing, the strong correlations among densifying\nantennas provide sufficient prior knowledge about channel state information\n(CSI). This inspires the careful design of observation matrices (e.g., transmit\nprecoders and receive combiners), that exploits the CSI prior knowledge, to\nboost channel estimation performance. Aligned with this vision, this work\nproposes to jointly design the combiners and precoders by maximizing the mutual\ninformation between the received pilots and densifying MIMO channels. A\ntwo-dimensional ice-filling (2DIF) algorithm is proposed to efficiently\naccomplish this objective. The algorithm is motivated by the fact that the\neigenspace of MIMO channel covariance can be decoupled into two\nsub-eigenspaces, which are associated with the correlations of transmitter\nantennas and receiver antennas, respectively. By properly setting the precoder\nand the combiner as the eigenvectors from these two sub-eigenspaces, the 2DIF\npromises to generate near-optimal observation matrices. Moreover, we further\nextend the 2DIF method to the popular hybrid combining systems, where a\ntwo-stage 2DIF (TS-2DIF) algorithm is developed to handle the analog combining\ncircuits realized by phase shifters. Simulation results demonstrate that,\ncompared to the state-of-the-art schemes, the proposed 2DIF and TS-2DIF methods\ncan achieve superior channel estimation accuracy."}
{"id": "2510.09495", "categories": ["cs.IT", "cs.AI", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09495", "abs": "https://arxiv.org/abs/2510.09495", "authors": ["Srikar Allaparapu", "Michael Baur", "Benedikt Böck", "Michael Joham", "Wolfgang Utschick"], "title": "Precoder Design in Multi-User FDD Systems with VQ-VAE and GNN", "comment": "Submitted to IEEE ICASSP 2026", "summary": "Robust precoding is efficiently feasible in frequency division duplex (FDD)\nsystems by incorporating the learnt statistics of the propagation environment\nthrough a generative model. We build on previous work that successfully\ndesigned site-specific precoders based on a combination of Gaussian mixture\nmodels (GMMs) and graph neural networks (GNNs). In this paper, by utilizing a\nvector quantized-variational autoencoder (VQ-VAE), we circumvent one of the key\ndrawbacks of GMMs, i.e., the number of GMM components scales exponentially to\nthe feedback bits. In addition, the deep learning architecture of the VQ-VAE\nallows us to jointly train the GNN together with VQ-VAE along with pilot\noptimization forming an end-to-end (E2E) model, resulting in considerable\nperformance gains in sum rate for multi-user wireless systems. Simulations\ndemonstrate the superiority of the proposed frameworks over the conventional\nmethods involving the sub-discrete Fourier transform (DFT) pilot matrix and\niterative precoder algorithms enabling the deployment of systems characterized\nby fewer pilots or feedback bits."}
