<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [AskDB: An LLM Agent for Natural Language Interaction with Relational Databases](https://arxiv.org/abs/2511.16131)
*Xuan-Quang Phan,Tan-Ha Mai,Thai-Duy Dinh,Minh-Thuan Nguyen,Lam-Son Lê*

Main category: cs.DB

TL;DR: AskDB是一个基于大型语言模型的智能代理，通过自然语言支持SQL数据库的数据分析和行政操作，集成了动态模式感知提示和任务分解框架。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库交互系统要么专注于自然语言查询，要么只处理狭窄的数据库管理方面，缺乏统一且智能的通用数据库交互界面。

Method: 基于Gemini 2构建，采用动态模式感知提示机制有效整合数据库元数据，以及任务分解框架实现多步骤操作的规划和执行。

Result: 在广泛使用的Text-to-SQL基准测试和精心策划的DBA任务集上表现出色，在分析和行政场景中均展现出强大性能。

Conclusion: AskDB作为关系数据库系统的统一智能代理具有巨大潜力，为终端用户提供直观且易于访问的体验。

Abstract: Interacting with relational databases remains challenging for users across different expertise levels, particularly when composing complex analytical queries or performing administrative tasks. Existing systems typically address either natural language querying or narrow aspects of database administration, lacking a unified and intelligent interface for general-purpose database interaction. We introduce AskDB, a large language model powered agent designed to bridge this gap by supporting both data analysis and administrative operations over SQL databases through natural language. Built on Gemini 2, AskDB integrates two key innovations: a dynamic schema-aware prompting mechanism that effectively incorporates database metadata, and a task decomposition framework that enables the agent to plan and execute multi-step actions. These capabilities allow AskDB to autonomously debug derived SQL, retrieve contextual information via real-time web search, and adaptively refine its responses. We evaluate AskDB on a widely used Text-to-SQL benchmark and a curated set of DBA tasks, demonstrating strong performance in both analytical and administrative scenarios. Our results highlight the potential of AskDB as a unified and intelligent agent for relational database systems, offering an intuitive and accessible experience for end users.

</details>


### [2] [Benchmarking Table Extraction from Heterogeneous Scientific Extraction Documents](https://arxiv.org/abs/2511.16134)
*Marijan Soric,Cécile Gracianne,Ioana Manolescu,Pierre Senellart*

Main category: cs.DB

TL;DR: 提出了一个新的端到端表格提取基准测试，包含37k样本的两个新数据集，评估了多种表格提取方法，发现当前方法在异构数据泛化性、鲁棒性和可解释性方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格提取工具方法多样，用户难以选择合适的工具，需要系统性的评估基准。

Method: 设计了严格的评估流程，包括表格提取各子任务和端到端评估，使用两个新的异构数据集（共37k样本），测试了现成库、软件工具、大视觉语言模型和计算机视觉方法。

Result: 表格提取仍然具有挑战性，当前方法在面对异构数据时缺乏泛化能力，且在鲁棒性和可解释性方面存在局限。

Conclusion: 需要开发更通用、鲁棒和可解释的表格提取方法，该基准为未来研究提供了评估标准。

Abstract: Table Extraction (TE) consists in extracting tables from PDF documents, in a structured format which can be automatically processed. While numerous TE tools exist, the variety of methods and techniques makes it difficult for users to choose an appropriate one. We propose a novel benchmark for assessing end-to-end TE methods (from PDF to the final table). We contribute an analysis of TE evaluation metrics, and the design of a rigorous evaluation process, which allows scoring each TE sub-task as well as end-to-end TE, and captures model uncertainty. Along with a prior dataset, our benchmark comprises two new heterogeneous datasets of 37k samples. We run our benchmark on diverse models, including off-the-shelf libraries, software tools, large vision language models, and approaches based on computer vision. The results demonstrate that TE remains challenging: current methods suffer from a lack of generalizability when facing heterogeneous data, and from limitations in robustness and interpretability.

</details>


### [3] [On 10x Better Scalability: KV Stores Scale Up KV Cache](https://arxiv.org/abs/2511.16138)
*Weiping Yu,Ye Jiarui,He Mengke,Junfeng Liu,Siqiang Luo*

Main category: cs.DB

TL;DR: SGLANG-LSM是一个基于LSM-tree架构的KV缓存管理系统，通过分层设计和自适应优化，显著提升LLM的缓存命中率和首令牌延迟性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于磁盘的KV缓存系统存在文件系统元数据开销、I/O效率低和空间局部性差等可扩展性瓶颈，需要更高效的缓存管理方案。

Method: 采用三层协调组件设计：前缀保持存储引擎、自适应控制器和运行时服务，结合键值分离和LSM-tree架构优化KV缓存管理。

Result: 在大规模动态工作负载评估中，缓存命中率提升高达143%，首令牌延迟降低高达24%，优于现有最优系统。

Conclusion: 这是首次将数据库存储架构系统性地应用于大规模LLM缓存管理，证明了LSM-tree架构在此领域的有效性。

Abstract: Large language models (LLMs) rely on Key-Value (KV) cache to reduce time- to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge- tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch opera- tions and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management.

</details>


### [4] [From Patents to Dataset: Scraping for Oxide Glass Compositions and Properties](https://arxiv.org/abs/2511.16366)
*Gustavo Laranja Thomaello,Thomaz Yeiden Busnardo Aguena,Eric Trevelato Costa,Rafael Baságlia Rosante,Thiago Rodrigo Ramos,Daiane Aparecida Zuanetti,Edgar Dutra Zanotto*

Main category: cs.DB

TL;DR: 使用网络爬虫技术从专利表格中提取玻璃成分和性能数据，构建数据库用于机器学习模型开发新型玻璃。


<details>
  <summary>Details</summary>
Motivation: 现有玻璃数据库信息有限，需要扩展数据量和多样性来改进玻璃建模和预测能力。

Method: 采用网络爬虫技术提取专利表格中的玻璃成分数据，清理并结构化处理，重点关注液相温度、折射率和阿贝数三个关键性能指标。

Result: 成功提取了5,696个液相温度数据、4,298个折射率数据和1,771个阿贝数数据，将现有数据库分别扩展了10.4%、6.6%和4.9%，新数据包含更多钛、镁、锆、铌、铁、锡和钇氧化物成分。

Conclusion: 该方法显著扩展了玻璃成分和性能的可访问空间，新数据库提供了比现有数据库更多样化的成分和性能值，有助于改进玻璃建模应用。

Abstract: In this work, we present web scraping techniques to extract in- formation from patent tables, clean and structure them for future use in predictive machine learning models to develop new glasses. We extracted compositions and three properties relevant to the development of new glasses and structured them into a database to be used together with information from other available datasets. We also analyzed the consistency of the information obtained and what it adds to the existing databases. The extracted liquidus temperatures comprise 5,696 compositions; the second subset includes 4,298 refractive indexes and, finally, 1,771 compositions with Abbe numbers. The extraction performed here increases the available information by approximately 10.4% for liquidus temperature, 6.6% for refractive index, and 4.9% for Abbe number. The impact extends beyond quantity: the newly extracted data introduce compositions with property values that are more diverse than those in existing databases, thereby expanding the accessible compositional and property space for glass modeling applications. We emphasize that the compositions of the new database contain relatively more titanium, magnesium, zirconium, niobium, iron, tin, and yttrium oxides than those of the existing bases.

</details>


### [5] [[Experiment, Analysis, and Benchmark] Systematic Evaluation of Plan-based Adaptive Query Processing](https://arxiv.org/abs/2511.16455)
*Pei Mu,Anderson Chaves Carniel,Antonio Barbalace,Amir Shaikhha*

Main category: cs.DB

TL;DR: 本文首次全面分析了基于计划的AQP策略，发现在不同存储架构（磁盘vs内存数据库）中性能提升的来源存在显著差异：磁盘数据库中主要来自查询计划重排序，而内存数据库中主要来自基数估计精炼。


<details>
  <summary>Details</summary>
Motivation: 不可靠的基数估计仍然是数据库管理系统的关键性能瓶颈。虽然基于计划的自适应查询处理(AQP)策略通过子计划执行的反馈来逐步精炼基数估计，但其在不同存储架构下实际改善性能的原因尚未被探索。

Method: 在磁盘数据库(PostgreSQL)和内存数据库(DuckDB)上实现并评估了最先进的基于计划的AQP策略，使用两个基准测试进行对比分析。

Result: 在磁盘数据库中，性能提升主要来自查询计划重排序，基数更新机制反而引入可测量的开销；在内存数据库中，基数精炼为大多数查询带来显著性能提升。基于计划的AQP相比最先进的基于关系的AQP方法具有显著性能优势。

Conclusion: 这些发现为研究人员提供了关于基于计划的AQP何时以及为何有效的关键见解，并指导数据库系统开发者在实现努力与性能改进之间进行权衡。

Abstract: Unreliable cardinality estimation remains a critical performance bottleneck in database management systems (DBMSs). Adaptive Query Processing (AQP) strategies address this limitation by providing a more robust query execution mechanism. Specifically, plan-based AQP achieves this by incrementally refining cardinality using feedback from the execution of sub-plans. However, the actual reason behind the improvements of plan-based AQP, especially across different storage architectures (on-disk vs. in-memory DBMSs), remains unexplored.
  This paper presents the first comprehensive analysis of state-of-the-art plan-based AQP. We implement and evaluate this strategy on both on-disk and in-memory DBMSs across two benchmarks. Our key findings reveal that while plan-based AQP provides overall speedups in both environments, the sources of improvement differ significantly. In the on-disk DBMS, PostgreSQL, performance gains primarily come from the query plan reorderings, but not the cardinality updating mechanism; in fact, updating cardinalities introduces measurable overhead. Conversely, in the in-memory DBMS, DuckDB, cardinality refinement drives significant performance improvements for most queries. We also observe significant performance benefits of the plan-based AQP compared to a state-of-the-art related-based AQP method. These observations provide crucial insights for researchers on when and why plan-based AQP is effective, and ultimately guide database system developers on the tradeoffs between the implementation effort and performance improvements.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [Sequential testing problem: A follow-up review](https://arxiv.org/abs/2511.15742)
*Tonguç Ünlüyurt*

Main category: cs.DS

TL;DR: 本文对过去20年序列测试问题（STP）的研究进展进行了全面回顾，总结了新的理论成果、问题扩展和应用领域，并讨论了各研究问题之间的关系，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自20年前[1]的综述发表以来，序列测试问题领域涌现了大量新研究，需要对这些进展进行系统性总结和梳理。

Method: 采用文献综述方法，收集和分析过去20年关于序列测试问题的相关研究，识别主要成果并探讨问题间的关联性。

Result: 识别了序列测试问题领域的新理论成果、问题扩展形式以及新的应用场景，建立了不同研究问题之间的联系框架。

Conclusion: 序列测试问题在过去20年取得了显著进展，但仍存在多个有前景的研究方向值得进一步探索。

Abstract: This review aims to provide a comprehensive update on the progress made on the Sequential Testing problem (STP) in the last 20 years after the review, [1] was published. Many studies have provided new theoretical results, extensions of the problem, and new applications. In this review, we pinpoint the main results and discuss the relations between the problems studied. We also provide possible research directions for the problem.

</details>


### [7] [Connectivity-Preserving Important Separators: Enumeration and an Improved FPT Algorithm for Node Multiway Cut-Uncut](https://arxiv.org/abs/2511.15849)
*Batya Kenig*

Main category: cs.DS

TL;DR: 提出了连通性保持重要分隔符框架，将经典重要分隔符概念扩展到包含连通性约束的图分隔问题，显著改进了节点多路割-不割问题的参数化算法复杂度。


<details>
  <summary>Details</summary>
Motivation: 经典重要分隔符方法在处理需要同时满足割约束和连通性约束的图分隔问题时存在局限性，需要开发能够处理复杂连通性约束的新框架。

Method: 引入连通性保持重要分隔符概念，证明其数量上界为2^O(k log k)，并给出枚举算法，应用于节点多路割-不割问题。

Result: 获得了节点多路割-不割问题的参数化算法，运行时间为O(2^O(k log k) · n · m^1+o(1))，将k的依赖从2^O(k^2 log k)改进到2^O(k log k)。

Conclusion: 该框架将重要分隔符范式推广到需要同时满足割约束和连通性约束的分隔问题，为图割-不割问题的固定参数算法设计提供了改进的组合基础。

Abstract: We develop a framework for handling graph separation problems with connectivity constraints. Extending the classical concept of important separators, we introduce and analyze connectivity-preserving important separators, which are important separators that not only disconnect designated terminal sets $A$ and $B$ but also satisfy an arbitrary set of connectivity constraints over the terminals. These constraints can express requirements such as preserving the internal connectivity of each terminal set, enforcing pairwise connections defined by an equivalence relation, or maintaining reachability from a specified subset of vertices. We prove that for any graph $G=(V,E)$, terminal sets $A,B\subseteq V$, and integer $k$, the number of important $A,B$-separators of size at most $k$ satisfying a set of connectivity constraints is bounded by $2^{O(k\log k)}$, and that all such separators can be enumerated within $O(2^{O(k\log k)} \cdot n \cdot T(n,m))$ time, where $T(n,m)$ is the time required to compute a minimum $s,t$-separator. As an application, we obtain a new fixed-parameter-tractable algorithm for the Node Multiway Cut-Uncut (N-MWCU) problem, parameterized by $k$, the size of the separator set. The algorithm runs in $O(2^{O(k\log k)} \cdot n \cdot m^{1+o(1)})$ time for graphs with polynomially-bounded integer weights. This significantly improves the dependence on $k$ from the previous $2^{O(k^2\log k)}$ to $2^{O(k\log k)}$, thereby breaking a long-standing barrier, and simultaneously improves the polynomial factors. Our framework generalises the important-separator paradigm to separation problems in which the deletion set must satisfy both cut and uncut constraints on terminal subsets, thus offering a refined combinatorial foundation for designing fixed-parameter algorithms for cut-uncut problems in graphs.

</details>


### [8] [Robustness of Online Inventory Balancing to Inventory Shocks](https://arxiv.org/abs/2511.16044)
*Yiding Feng,Rad Niazadeh,Amin Saberi*

Main category: cs.DS

TL;DR: 本文提出了一个包含库存冲击的在线商品组合规划模型，设计了Batched Inventory Balancing (BIB)算法，在存在外生和内生库存冲击的情况下实现了渐进最优的竞争比(1-1/e)。


<details>
  <summary>Details</summary>
Motivation: 经典在线资源分配模型未考虑供应侧的库存冲击，而现实世界中的在线商品组合经常面临各种库存冲击，这会显著影响库存平衡算法的性能。

Method: 设计了Batched Inventory Balancing (BIB)算法，使用新颖的随机化原始-对偶方法，将双重构造简化为组合的"区间分配问题"。

Result: BIB算法在存在库存冲击的情况下实现了渐进最优的竞争比(1-1/e)，而原始库存平衡算法在新模型中无法达到最优竞争比。

Conclusion: BIB算法证明了库存平衡型策略对库存冲击的鲁棒性，其竞争比与无冲击情况下的库存平衡算法完全匹配。

Abstract: In classic adversarial online resource allocation problems such as AdWords, customers arrive online while products are given offline with a fixed initial inventory. To ensure revenue guarantees under uncertainty, the decision maker must balance consumption across products. Based on this, the prevalent policy "inventory balancing (IB)" has proved to be optimal or near-optimal competitive in almost all classic settings. However, these models do not capture various forms of inventory shocks on the supply side, which play an important role in real-world online assortment and can significantly impact the revenue performance of the IB algorithm.
  Motivated by this paradigm, we introduce a variant of online assortment planning with inventory shocks. Our model considers adversarial exogenous shocks (where supply increases unpredictably) and allocation-coupled endogenous shocks (where an inventory reduction is triggered by the algorithms and re-adjusted after a usage duration), whose combination leads to non-monotonic inventory fluctuations. As our main result, we show the robustness of IB-type strategies against such shocks by designing a new family of optimal competitive algorithms called "Batched Inventory Balancing (BIB)." Using a novel randomized primal-dual method, we bound the competitive ratio of BIB against optimal offline. We show that with proper choice of a certain parameter, this competitive ratio is asymptotically optimal and converges to (1-1/e) as initial inventories grow, in contrast to the original IB which no longer achieves the optimal ratio in this new model. Moreover, we characterize BIB's competitive ratio parametric by its penalty function and show that it matches exactly the competitive ratio of IB without shocks. Our refined analysis reduces the dual construction to a combinatorial "interval assignment problem" whose algorithmic solution may be of independent interest.

</details>


### [9] [Real Time Proportional Throughput Maximization: How much advance notice should you give your scheduler?](https://arxiv.org/abs/2511.16023)
*Nadim A. Mottu*

Main category: cs.DS

TL;DR: 本文研究了实时吞吐量最大化问题的扩展，引入了t-提前通知概念，证明了存在t/(2t+1)-竞争算法，并证明了该竞争比的最优性。


<details>
  <summary>Details</summary>
Motivation: 扩展实时调度问题，考虑作业提前通知时间对调度性能的影响，探索在有限提前通知下的最优竞争比。

Method: 定义了t-提前通知概念，构建了基于提前通知的调度算法，并通过竞争分析证明算法性能。

Result: 证明了当所有作业具有t-提前通知时，存在t/(2t+1)-竞争算法，且该竞争比是最优的，无论提前通知多少都无法达到1/2-竞争性。

Conclusion: t-提前通知显著影响实时调度算法的性能，t/(2t+1)是最优竞争比，揭示了提前通知对调度性能的根本限制。

Abstract: We will be exploring a generalization of real time scheduling problem sometimes called the real time throughput maximization problem. Our input is a sequence of jobs specified by their release time, deadline and processing time. We assume that jobs are announced before or at their release time. At each time step, the algorithm must decide whether to schedule a job based on the information so far. The goal is to maximize the value of the sum of the processing times of jobs that finish before their deadline, or the total ``active'' time.
  We extend this problem by defining a notion of $t$-advance-notice, a measure of how far in advance each job is given relative to their processing time. We show that there exists a $\frac{t}{2t+1}$-competitive algorithm when all jobs have $t$-advance-notice for $t\in [0,1]$. We also show that this ratio is optimal for all algorithms with $t$-advance-notice and that the upper bound of $\frac{t}{2t+1}$-competitiveness holds for all $t$, in particular that regardless of how much advance-notice is given, no algorithm can reach $\frac{1}{2}$-competitiveness.

</details>


### [10] [Optimal Online Bipartite Matching in Degree-2 Graphs](https://arxiv.org/abs/2511.16025)
*Amey Bhangale,Arghya Chakraborty,Prahladh Harsha*

Main category: cs.DS

TL;DR: 在线二分图匹配问题中，当在线节点的度数限制为2时，确定性分数匹配和随机化整数匹配的最优竞争比不同，分别为0.75和0.717772，表明两者之间存在分离。


<details>
  <summary>Details</summary>
Motivation: 研究在线节点度数限制为2的二分图匹配问题，探索在受限情况下能否获得比经典1-1/e竞争比更好的性能，并比较确定性分数匹配和随机化整数匹配的表现差异。

Method: 对于随机化整数匹配，分析并证明了Half-Half算法的竞争比为η≈0.717772，并给出了匹配的下界证明其最优性。

Result: 确定性分数匹配的最优竞争比为0.75，而随机化整数匹配的最优竞争比为η≈0.717772，两者不同，表明无法获得完美的舍入方案。

Conclusion: 在在线节点度数限制为2的二分图匹配中，确定性分数匹配和随机化整数匹配的最优竞争比存在差异，这证明了两种问题之间的分离性，排除了完美舍入方案的可能性。

Abstract: Online bipartite matching is a classical problem in online algorithms and we know that both the deterministic fractional and randomized integral online matchings achieve the same competitive ratio of $1-\frac{1}{e}$. In this work, we study classes of graphs where the online degree is restricted to $2$. As expected, one can achieve a competitive ratio of better than $1-\frac{1}{e}$ in both the deterministic fractional and randomized integral cases, but surprisingly, these ratios are not the same. It was already known that for fractional matching, a $0.75$ competitive ratio algorithm is optimal. We show that the folklore \textsc{Half-Half} algorithm achieves a competitive ratio of $η\approx 0.717772\dots$ and more surprisingly, show that this is optimal by giving a matching lower-bound. This yields a separation between the two problems: deterministic fractional and randomized integral, showing that it is impossible to obtain a perfect rounding scheme.

</details>


### [11] [Learning-Augmented Online Algorithms for Nonclairvoyant Joint Replenishment Problem with Deadlines](https://arxiv.org/abs/2511.16094)
*Michael Dinitz,Jeremy T. Fineman,Seeun William Umboh*

Main category: cs.DS

TL;DR: 本文提出了一种利用预测来改进在线联合补货与截止期限问题(JRP-D)的算法，在非预知情况下显著降低了竞争比。


<details>
  <summary>Details</summary>
Motivation: 现有的非预知JRP-D算法竞争比为O(√n)，而预知算法为O(1)。本文旨在利用预测的截止期限来桥接这一差距，使算法在预测准确时接近预知性能，在预测不准确时仍保持鲁棒性。

Method: 设计了一种竞争比为O(min(η^{1/3}log^{2/3}(n), √η, √n))的算法，其中η≤n²量化预测错误程度（瞬时物品反转数）。算法在预测无错误时类似预知算法，在预测错误时不会比非预知算法差。

Result: 当预测错误η < o(n^{3/2}/log²(n))时，算法获得比非预知算法渐近更优的竞争比。同时证明了所有合理确定性算法的竞争比下界为Ω(η^{1/3})，表明所提算法在误差度量下几乎最优。

Conclusion: 该算法成功利用预测在JRP-D问题中实现了鲁棒性和一致性，在预测质量足够好时显著优于传统非预知算法，且接近理论最优界。

Abstract: This paper considers using predictions in the context of the online Joint Replenishment Problem with Deadlines (JRP-D). Prior work includes asymptotically optimal competitive ratios of $O(1)$ for the clairvoyant setting and $O(\sqrt{n})$ of the nonclairvoyant setting, where $n$ is the number of items. The goal of this paper is to significantly reduce the competitive ratio for the nonclairvoyant case by leveraging predictions: when a request arrives, the true deadline of the request is not revealed, but the algorithm is given a predicted deadline.
  The main result is an algorithm whose competitive ratio is $O(\min(η^{1/3}\log^{2/3}(n), \sqrtη, \sqrt{n}))$, where $n$ is the number of item types and $η\leq n^2$ quantifies how flawed the predictions are in terms of the number of ``instantaneous item inversions.'' Thus, the algorithm is robust, i.e., it is never worse than the nonclairvoyant solution, and it is consistent, i.e., if the predictions exhibit no inversions, then the algorithm behaves similarly to the clairvoyant algorithm. Moreover, if the error is not too large, specifically $η< o(n^{3/2}/\log^2(n))$, then the algorithm obtains an asymptotically better competitive ratio than the nonclairvoyant algorithm. We also show that all deterministic algorithms falling in a certain reasonable class of algorithms have a competitive ratio of $Ω(η^{1/3})$, so this algorithm is nearly the best possible with respect to this error metric.

</details>


### [12] [Online Graph Coloring for $k$-Colorable Graphs](https://arxiv.org/abs/2511.16100)
*Ken-ichi Kawarabayashi,Hirotaka Yoneda,Masataka Yoneda*

Main category: cs.DS

TL;DR: 本文改进了在线图着色问题的算法，对于k-可着色图，在k≥5时显著降低了所需颜色数量，在k=4时改进了现有上界，在k=2时缩小了随机算法的上下界差距。


<details>
  <summary>Details</summary>
Motivation: 在线图着色是一个经典的计算理论问题，Kierstead在1998年提出的算法至今仍是最好结果。经过近30年，作者希望在这一问题上取得突破性进展。

Method: 针对不同k值设计了不同的确定性在线算法：对于k≥5，使用改进的算法；对于k=4，设计了新算法；对于k=2，分析了随机算法的性能界限。

Result: k≥5时：颜色数从O(n^{1-1/k!})改进到O(n^{1-2/(k(k-1))})；k=4时：从O(n^{5/6})改进到O(n^{14/17})；k=2时：随机算法上下界差距缩小到1.09倍。

Conclusion: 这是在线图着色问题近30年来的首次重大突破，显著改进了多个k值下的颜色数上界，并缩小了随机算法的性能差距。

Abstract: We study the problem of online graph coloring for $k$-colorable graphs. The best previously known deterministic algorithm uses $\tilde{O}(n^{1-1/k!})$ colors for general $k$ and $\tilde{O}(n^{5/6})$ colors for $k = 4$, both given by Kierstead in 1998. In this paper, nearly thirty years later, we have finally made progress. Our results are summarized as follows:
  (1) $k \geq 5$ case. We provide a deterministic online algorithm to color $k$-colorable graphs with $\tilde{O}(n^{1-2/(k(k-1))})$ colors, significantly improving the current upper bound of $\tilde{O}(n^{1-1/k!})$
  (2) $k = 4$ case. We provide a deterministic online algorithm to color $4$-colorable graphs with $\tilde{O}(n^{14/17})$ colors, improving the current upper bound of $\tilde{O}(n^{5/6})$ colors.
  (3) $k = 2$ case. We show that for randomized algorithms, the upper bound is $1.034 \log_2 n + O(1)$ colors and the lower bound is $\frac{91}{96} \log_2 n - O(1)$ colors. This means that we close the gap to $1.09\mathrm{x}$.
  With our algorithm for the $k \geq 5$ case, we also obtain a deterministic online algorithm for graph coloring that achieves a competitive ratio of $O(n / \log \log n)$, which improves the best known result of $O(n \log \log \log n / \log \log n)$ by Kierstead.
  For the bipartite graph case ($k = 2$), the limit of online deterministic algorithms is known: any deterministic algorithm requires $2 \log_2 n - O(1)$ colors. Our results imply that randomized algorithms can perform slightly better but still have a limit.

</details>


### [13] [Scalable and Provable Kemeny Constant Computation on Static and Dynamic Graphs: A 2-Forest Sampling Approach](https://arxiv.org/abs/2511.16356)
*Cheng Li,Meihao Liao,Rong-Hua Li,Guoren Wang*

Main category: cs.DS

TL;DR: 提出了一种基于2-森林采样的Kemeny常数近似方法，相比随机游走方法具有更好的理论保证和效率，并支持动态图更新。


<details>
  <summary>Details</summary>
Motivation: 现有Kemeny常数计算方法主要依赖近似随机游走，需要大样本量且缺乏强理论保证，在大图上计算困难。

Method: 通过路径映射技术建立生成树与2-森林的对应关系，设计基于BIT的高效生成树采样和遍历算法，并提出动态图样本维护策略。

Result: 在10个大型真实数据集上的实验表明，该方法在静态和动态图上均优于现有方法，在效率和精度上表现一致。

Conclusion: 2-森林采样方法为Kemeny常数近似提供了更优的理论边界和实际性能，解决了大图和动态图的计算挑战。

Abstract: Kemeny constant, defined as the expected hitting time of random walks from a source node to a randomly chosen target node, is a fundamental metric in graph data management with many real-world applications. However, computing it exactly on large graphs is highly challenging, as it requires inverting large graph matrices. Existing solutions mainly rely on approximate random-walk-based methods, which still need large sample sizes and lack strong theoretical guarantees. In this paper, we propose a new approach for approximating the Kemeny constant via 2-forest sampling. We first derive an unbiased estimator expressed through spanning trees by introducing a path mapping technique that establishes a direct correspondence between spanning trees and certain classes of 2-forests. Compared to random walk-based estimators, 2-forest-based estimators yield leads to a better theoretical bound. We further design efficient algorithms to sample and traverse spanning trees, leveraging data structures such as the Binary Indexed Tree (BIT) for optimization. Our theoretical analysis shows that the Kemeny constant can be approximated with relative error $ε$ in $O\left(\frac{Δ^2\bar{d}^2}{ε^2}(τ+ n\min(\log n, Δ))\right)$ time, where $τ$ is the tree-sampling time, $\bar{d}$ is the average degree, and $Δ$ is the graph diameter. This complexity is near-linear in practice. Moreover, existing methods largely target static graphs and lack efficient mechanisms for dynamic updates. To address this, we propose two sample maintenance strategies that partially update samples while preserving accuracy on dynamic graphs. Extensive experiments on 10 large real-world datasets demonstrate that our method consistently outperforms state-of-the-art approaches in both efficiency and accuracy on static and dynamic graphs.

</details>


### [14] [A $(2+\varepsilon)$-approximation algorithm for the general scheduling problem in quasipolynomial time](https://arxiv.org/abs/2511.16536)
*Alexander Armbruster,Lars Rohwedder,Andreas Wiese*

Main category: cs.DS

TL;DR: 本文针对一般调度问题(GSP)提出了准多项式时间(2+ε)-近似算法，对于加权延迟目标甚至达到(1+ε)-近似比，显著改进了之前已知的O(log log P)-近似结果。


<details>
  <summary>Details</summary>
Motivation: GSP统一了多个重要的单机调度问题，如加权流时间、加权完成时间和加权延迟作业等。现有最佳结果是多项式时间O(log log P)-近似算法，需要改进近似比。

Method: 通过将问题转化为辅助几何覆盖问题，利用解的结构特性进行猜测和递归分解。虽然缺乏树状结构，但通过变换最优解获得结构性质，从而快速猜测部分解并递归求解剩余问题。

Result: 在作业处理时间为准多项式有界整数的假设下，获得了准多项式时间(2+ε)-近似算法，对于加权延迟目标达到(1+ε)-近似比。

Conclusion: 本文为GSP提供了显著改进的近似算法，特别是在加权延迟目标上取得了突破性进展，展示了通过几何方法和结构分析解决复杂调度问题的有效性。

Abstract: We study the general scheduling problem (GSP) which generalizes and unifies several well-studied preemptive single-machine scheduling problems, such as weighted flow time, weighted sum of completion time, and minimizing the total weight of tardy jobs. We are given a set of jobs with their processing times and release times and seek to compute a (possibly preemptive) schedule for them on one machine. Each job incurs a cost that depends on its completion time in the computed schedule, as given by a separate job-dependent cost function for each job, and our objective is to minimize the total resulting cost of all jobs. The best known result for GSP is a polynomial time $O(\log\log P)$-approximation algorithm [Bansal and Pruhs, FOCS 2010, SICOMP 2014].
  We give a quasi-polynomial time $(2+ε)$-approximation algorithm for GSP, assuming that the jobs' processing times are quasi-polynomially bounded integers. For the special case of the weighted tardiness objective, we even obtain an improved approximation ratio of $1+ε$. For this case, no better result had been known than the mentioned $O(\log\log P)$-approximation for the general case of GSP. Our algorithms use a reduction to an auxiliary geometric covering problem. In contrast to a related reduction for the special case of weighted flow time [Rohwedder, Wiese, STOC 2021][Armbruster, Rohwedder, Wiese, STOC 2023] for GSP it seems no longer possible to establish a tree-like structure for the rectangles to guide an algorithm that solves this geometric problem. Despite the lack of structure due to the problem itself, we show that an optimal solution can be transformed into a near-optimal solution that has certain structural properties. Due to those we can guess a substantial part of the solution quickly and partition the remaining problem in an intricate way, such that we can independently solve each part recursively.

</details>


### [15] [Entrywise Approximate Solutions for SDDM Systems in Almost-Linear Time](https://arxiv.org/abs/2511.16570)
*Angelo Farfan,Mehrdad Ghadiri,Junzhao Yang*

Main category: cs.DS

TL;DR: 提出了一种在Õ(mn^o(1))时间内高概率计算对称对角占优M-矩阵（SDDM）系统Lx=b近似解的算法


<details>
  <summary>Details</summary>
Motivation: 解决大规模图拉普拉斯矩阵系统的高效求解问题，这类系统在科学计算和网络分析中广泛应用

Method: 开发了针对可逆对称对角占优M-矩阵（SDDM）的算法，利用其特殊结构特性进行快速求解

Result: 算法能在Õ(mn^o(1))时间内高概率地计算出系统Lx=b的逐项近似解，其中m是非零元素数，n是系统维度

Conclusion: 该算法为大规模图拉普拉斯系统提供了高效的近似求解方法，显著提升了计算效率

Abstract: We present an algorithm that given any invertible symmetric diagonally dominant M-matrix (SDDM), i.e., a principal submatrix of a graph Laplacian, $\boldsymbol{\mathit{L}}$ and a nonnegative vector $\boldsymbol{\mathit{b}}$, computes an entrywise approximation to the solution of $\boldsymbol{\mathit{L}} \boldsymbol{\mathit{x}} = \boldsymbol{\mathit{b}}$ in $\tilde{O}(m n^{o(1)})$ time with high probability, where $m$ is the number of nonzero entries and $n$ is the dimension of the system.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [16] [Prior-free Collusion-proof Dynamic Mechanisms](https://arxiv.org/abs/2511.15727)
*Endre Csóka*

Main category: cs.GT

TL;DR: 本文提出了TU-GUM和NTU-GUM机制的先验无关版本，特别指出新的先验无关NTU-GUM在重复单商品分配问题中实现了对帕累托效率的1.283近似。


<details>
  <summary>Details</summary>
Motivation: Csóka等人提出的机制依赖于先验信息，本文旨在开发不依赖先验信息的机制版本，使其更具普适性和实用性。

Method: 定义了TU-GUM和NTU-GUM的先验无关变体，特别关注重复单商品分配问题这一特例。

Result: 新的先验无关NTU-GUM机制在重复单商品分配问题中实现了1.283的帕累托效率近似比。

Conclusion: 先验无关机制在动态随机多玩家问题中具有可行性，能够实现近似效率而无需依赖先验信息。

Abstract: For a general class of dynamic stochastic multi-player problems, Csóka, Liu, Rodivilov, and Teytelboym (2024) proposed prior-dependent mechanisms. The Guaranteed Utility Mechanism with transfers (TU-GUM) implements efficiency in a Guaranteed Utility Equilibrium (GUE). Its transfer-free variant (NTU-GUM) implements approximate efficiency in ε-GUE. In this paper, we define prior-free versions of both TU-GUM and NTU-GUM. As a special case, we believe that the new prior-free NTU-GUM implements a 1.283-approximation to Pareto efficiency for the repeated single good allocation problem in Fikioris, Banerjee, and Tardos (2024).

</details>


### [17] [Polynomial-Time Algorithms for Computing the Nucleolus: An Assessment](https://arxiv.org/abs/2511.16517)
*Holger I. Meinhardt*

Main category: cs.GT

TL;DR: 该论文反驳了Maggiorano等人声称的凸博弈核仁强多项式时间组合算法，指出其基于对Davis/Maschler简化博弈性质(RGP)的错误应用，并展示了通过Fenchel-Moreau共轭方法计算预核元素来获得预核仁的替代方法。


<details>
  <summary>Details</summary>
Motivation: 纠正Maggiorano等人关于凸博弈核仁计算的错误主张，证明简化博弈方法无法正确计算核仁，并提供有效的替代计算方法。

Method: 通过分析Davis/Maschler简化博弈性质(RGP)的正确应用，并采用Faigle等人的椭球方法和Meinhardt的Fenchel-Moreau共轭方法来计算预核元素，从而在预核为单点时获得预核仁。

Result: 证明了Maggiorano等人的简化博弈方法存在严重的选择问题，无法正确计算凸博弈的核仁；同时展示了当预核为单点时，通过预核元素计算预核仁的时间复杂度为O(n^3)。

Conclusion: 简化博弈方法不能用于计算凸博弈的核仁，但在预核为单点的博弈中，通过Fenchel-Moreau共轭方法计算预核元素可以获得预核仁，且具有多项式时间复杂度。

Abstract: Recently, Maggiorano et al. (2025) claimed that they have developed a strongly polynomial-time combinatorial algorithm for the nucleolus in convex games that is based on the reduced game approach and submodular function minimization method. Thereby, avoiding the ellipsoid method with its negative side effects in numerical computation completely. However, we shall argue that this is a fallacy based on an incorrect application of the Davis/Maschler reduced game property (RGP). Ignoring the fact that despite the pre-nucleolus, other solutions like the core, pre-kernel, and semi-reactive pre-bargaining set possess this property as well. This causes a severe selection issue, leading to the failure to compute the nucleolus of convex games using the reduced games approach. In order to assess this finding in its context, the ellipsoid method of Faigle et al. (2001) and the Fenchel-Moreau conjugation-based approach from convex analysis of Meinhardt (2013) to compute a pre-kernel element were resumed. In the latter case, it was exploited that for TU games with a single-valued pre-kernel, both solution concepts coincide. Implying that one has computed the pre-nucleolus if one has found the sole pre-kernel element of the game. Though it is a specialized and highly optimized algorithm for the pre-kernel, it assures runtime complexity of O(n^3) for computing the pre-nucleolus whenever the pre-kernel is a single point, which indicates a polynomial-time algorithm for this class of games.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [18] [QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation](https://arxiv.org/abs/2511.15996)
*Amin Bigdeli,Radin Hamidi Rad,Mert Incesu,Negar Arabzadeh,Charles L. A. Clarke,Ebrahim Bagheri*

Main category: cs.IR

TL;DR: QueryGym是一个轻量级、可扩展的Python工具包，为基于大语言模型的查询重写提供统一框架，解决了现有方法实现分散、难以公平比较的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM查询重写方法实现分散，缺乏统一工具包，阻碍了公平比较、快速实验、一致性基准测试和可靠部署。

Method: 提供Python API支持多种LLM方法、检索无关的接口、集中式提示管理系统、内置基准测试支持，以及开源可扩展实现。

Result: 开发了QueryGym工具包，支持与Pyserini和PyTerrier等后端集成，提供版本控制和元数据跟踪的提示管理。

Conclusion: QueryGym填补了LLM查询重写领域工具集的空白，为研究人员提供了统一的实验和比较平台。

Abstract: We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.

</details>


### [19] [Incorporating Token Importance in Multi-Vector Retrieval](https://arxiv.org/abs/2511.16106)
*Archish S,Ankit Garg,Kirankumar Shiragur,Neeraj Kayal*

Main category: cs.IR

TL;DR: 本文提出对ColBERT的Chamfer距离函数进行增强，通过计算查询token贡献的加权和，其中权重反映token重要性，从而提升多向量检索机制的表达能力。


<details>
  <summary>Details</summary>
Motivation: ColBERT的Chamfer距离函数对所有查询token平等对待，但实际中不同token对检索的重要性不同，因此需要引入token重要性权重来增强表达力。

Method: 在保持多向量表示固定的情况下，仅训练token权重，计算查询token贡献的加权和，使用IDF-based权重或few-shot微调来学习权重。

Result: 在BEIR基准测试中，零样本设置下使用IDF权重Recall@10平均提升1.28%，few-shot微调下提升3.66%。

Conclusion: 简单的token权重扩展能显著增强多向量检索机制的表达能力，且仅需训练少量参数即可实现性能提升。

Abstract: ColBERT introduced a late interaction mechanism that independently encodes queries and documents using BERT, and computes similarity via fine-grained interactions over token-level vector representations. This design enables expressive matching while allowing efficient computation of scores, as the multi-vector document representations could be pre-computed offline. ColBERT models distance using a Chamfer-style function: for each query token, it selects the closest document token and sums these distances across all query tokens.
  In our work, we explore enhancements to the Chamfer distance function by computing a weighted sum over query token contributions, where weights reflect the token importance. Empirically, we show that this simple extension, requiring only token-weight training while keeping the multi-vector representations fixed, further enhances the expressiveness of late interaction multi-vector mechanism. In particular, on the BEIR benchmark, our method achieves an average improvement of 1.28\% in Recall@10 in the zero-shot setting using IDF-based weights, and 3.66\% through few-shot fine-tuning.

</details>


### [20] [ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning](https://arxiv.org/abs/2511.16326)
*Jiawei Zhou,Hang Ding,Haiyun Jiang*

Main category: cs.IR

TL;DR: 提出了一种针对检索增强生成(RAG)的答案对齐微调框架，通过课程对比学习训练检索器识别能够生成正确答案的关键文档块，在长文本场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准检索器基于查询-文档相似性优化，但在长上下文场景中难以识别稀疏但关键的证据，无法与生成精确答案的下游目标对齐。

Method: 首先识别能够生成正确答案的高质量正例文档块，然后使用基于知识图谱构建的增强查询进行课程对比学习，挖掘渐进式困难负例来训练检索器。

Result: 在Ultradomain和LongBench基准的10个数据集上达到最先进性能，相比基础模型提升14.5%，同时保持长上下文RAG的高效性。

Conclusion: 该方法为构建真正以答案为中心的检索器提供了稳健有效的解决方案，显著提升了长上下文RAG的效果。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers.

</details>


### [21] [An Efficient LLM-based Evolutional Recommendation with Locate-Forget-Update Paradigm](https://arxiv.org/abs/2511.16414)
*Hao Liu,Le Wu,Min Hou,Han Wu,Kun Zhang,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: EvoRec是一个高效的定位-遗忘-更新框架，专门为基于LLM的推荐系统设计，通过识别与偏好变化相关的少量参数进行精确更新，在节省计算资源的同时保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: LLM推荐系统难以适应随时间变化的用户偏好演化，因为LLM参数过多导致传统重训练或微调方法不实用——重训练计算成本过高，仅用新交互微调会导致不活跃用户的偏好遗忘。

Method: 提出EvoRec框架，通过定位-遗忘-更新机制识别与偏好变化相关的少量参数（仅占LoRA适配器参数的30%）进行精确更新，不引入额外参数。

Result: 在两个真实世界数据集上的实验表明，EvoRec能高效演化LLMRec以适应活跃用户偏好，同时保护不活跃用户的兴趣在演化过程中不受干扰。

Conclusion: EvoRec为LLM推荐系统提供了一种高效的偏好演化方法，在节省计算资源的同时维持了整体推荐性能。

Abstract: Nowadays, Large Language Models (LLMs) have shown exceptional performance in sequential recommendations, and the adoption of LLM-based recommender systems (LLMRec) is becoming increasingly widespread in existing e-commerce platforms. Despite the impressive performance, the constant high volume of new user-item interactions makes it difficult to adapt to the evolution of user preference over time, especially for LLM-based recommender systems. The challenge arises from the large number of parameters in LLMs, which makes traditional evolution methods (i.e., Re-training or Fine-tuning) impractical. Specifically, Re-training with all interactions results in prohibitively high computational costs. On the other hand, fine-tuning with only new interactions leads to preference forgetting among inactive users, ultimately compromising overall performance. To tackle this problem, we propose EvoRec, an efficient Locate-Forget-Update framework designed for LLM-based recommender systems to model the evolution of user preferences. EvoRec identifies a small set of parameters associated with preference changes and updates them precisely, thereby saving computational resources while maintaining strong recommendation performance. Notably, the modified parameters account for only 30\% of LoRA adapter parameters, with no additional parameters introduced. Extensive experiments on two real-world datasets demonstrate that, compared to existing methods, EvoRec not only efficiently evolves LLMRec to adapt to the preferences of active users, but also preserves the interests of inactive users from being disturbed during evolution.

</details>


### [22] [Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation](https://arxiv.org/abs/2511.16478)
*Elena V. Epure,Yashar Deldjoo,Bruno Sguerra,Markus Schedl,Manuel Moussallam*

Main category: cs.IR

TL;DR: 该论文主张随着大语言模型在音乐推荐系统中的应用，需要重新思考评估方法，因为传统的信息检索框架和准确性指标不再适用。


<details>
  <summary>Details</summary>
Motivation: 传统音乐推荐系统主要基于信息检索框架，使用准确性指标评估，但这种方法无法回答"什么是好的推荐"这一核心问题。大语言模型的引入改变了推荐系统的范式，需要新的评估方法。

Method: 首先回顾大语言模型如何重塑用户建模、物品建模和自然语言推荐；然后检查NLP领域的评估实践；最后综合见解，为大语言模型驱动的音乐推荐系统制定结构化的成功和风险维度。

Result: 提出了一个更新的、教学性的跨学科评估视角，重点关注大语言模型提示在音乐推荐系统中的应用。

Conclusion: 大语言模型驱动的音乐推荐系统需要重新设计评估框架，以应对生成式模型带来的挑战和机遇，传统训练/测试协议难以解释大语言模型的行为。

Abstract: Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.
  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.

</details>


### [23] [The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation](https://arxiv.org/abs/2511.16543)
*Jiaheng Zhang,Daqiang Zhang*

Main category: cs.IR

TL;DR: Prism是一个解耦框架，将推荐系统分为专门的排序阶段和解释生成阶段，通过知识蒸馏使用大型教师LLM生成高质量解释知识，再由小型学生模型合成个性化解释，解决了端到端架构中的性能-效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在可解释推荐系统中端到端架构存在的性能-效率权衡问题，避免排序和解释联合优化导致的次优妥协。

Method: 提出Prism解耦框架，使用强大的教师LLM（如FLAN-T5-XXL）作为Oracle生成高保真解释知识，然后由经过微调的紧凑学生模型（如BART-Base）专门合成个性化解释。

Result: 140M参数的Prism模型在忠实度和个性化的人类评估中显著优于11B参数的教师模型，推理速度提升24倍，内存消耗减少10倍。

Conclusion: 解耦结合针对性蒸馏为高质量可解释推荐提供了高效且有效的途径。

Abstract: The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.

</details>


### [24] [PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.16576)
*Alima Subedi,Sankalpa Pokharel,Satish Puri*

Main category: cs.IR

TL;DR: PolyMinHash是一个用于多边形近似相似性搜索的系统，通过将MinHashing适配为2D多边形哈希方案来生成保留相似性的短签名，显著减少查询候选数量。


<details>
  <summary>Details</summary>
Motivation: 随着数据集增大，精确最近邻搜索变得不可行，但现有ANN系统很少关注空间数据库和GIS中的多边形相似性搜索。

Method: 通过计算随机采样点落入多边形内部区域所需的点数来生成MinHash值，保留基于面积的Jaccard相似性。

Result: 与暴力算法相比，哈希机制在查询细化阶段处理的候选数量减少了高达98%。

Conclusion: PolyMinHash系统在搜索精度和运行时间之间提供了良好的权衡，为多边形相似性搜索提供了有效的近似解决方案。

Abstract: Similarity searches are a critical task in data mining. As data sets grow larger, exact nearest neighbor searches quickly become unfeasible, leading to the adoption of approximate nearest neighbor (ANN) searches. ANN has been studied for text data, images, and trajectories. However, there has been little effort to develop ANN systems for polygons in spatial database systems and geographic information systems. We present PolyMinHash, a system for approximate polygon similarity search that adapts MinHashing into a novel 2D polygon-hashing scheme to generate short, similarity-preserving signatures of input polygons. Minhash is generated by counting the number of randomly sampled points needed before the sampled point lands within the polygon's interior area, yielding hash values that preserve area-based Jaccard similarity. We present the tradeoff between search accuracy and runtime of our PolyMinHash system. Our hashing mechanism reduces the number of candidates to be processed in the query refinement phase by up to 98% compared to the number of candidates processed by the brute-force algorithm.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [25] [Fluid Reconfigurable Intelligent Surface (FRIS) Enabling Secure Wireless Communications](https://arxiv.org/abs/2511.15860)
*Xusheng Zhu,Kai-Kit Wong,Boyi Tang,Wen Chen,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: FRIS通过赋予反射元件位置可重构性来增强物理层安全性，相比传统RIS能动态选择最优元素子集，显著提高保密率。


<details>
  <summary>Details</summary>
Motivation: 传统RIS的反射元件位置固定，限制了性能优化。FRIS通过引入位置可重构性作为新的自由度，旨在提升无线通信系统的物理层安全性。

Method: 提出基于交替优化(AO)框架的高效算法：使用广义特征值方法闭式求解波束赋形子问题，采用交叉熵优化(CEO)方法处理元素选择和离散相位设计的组合子问题。

Result: 仿真结果表明，所提出的FRIS设计显著优于传统RIS和其他基线方法，证明了元素位置作为新自由度带来的安全性增益。

Conclusion: FRIS通过引入位置可重构性，为RIS技术提供了新的优化维度，在物理层安全方面具有显著优势，为解决MINLP问题提供了有效方案。

Abstract: The concept of fluid reconfigurable intelligent surface (FRIS) upgrades the conventional reconfigurable intelligent surface (RIS) paradigm by empowering its reflecting elements with positioning reconfigurability. This letter aims to investigate the use of FRIS to enhance physical-layer security in a system, in which a multi-antenna access point (AP) communicates with a legitimate user device in the presence of an eavesdropper. Unlike RIS with fixed-position elements, FRIS can dynamically select an optimal subset of elements from a larger array of candidate locations. We aim to maximize the secrecy rate by jointly optimizing the AP's transmit beamforming, the selection of FRIS activated elements, and their discrete phase shifts. The resulting problem is a challenging mixed-integer nonlinear program (MINLP), which is NP-hard. To address this, we propose an efficient algorithm based on an alternating optimization (AO) framework. Within this framework, the beamforming subproblem is optimally solved in closed form using the generalized eigenvalue method, while the combinatorial subproblem of joint element selection and discrete phase design is handled via the cross-entropy optimization (CEO) method. Simulation results show that the proposed FRIS design significantly outperforms the conventional RIS counterpart and other baselines, demonstrating the substantial security gains by element positioning as the new degree of freedom (DoF).

</details>
