<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.IT](#cs.IT) [Total: 18]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DS](#cs.DS) [Total: 15]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.GT](#cs.GT) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 比较分析用于拟合3D图像数据的镶嵌模型算法策略，评估不同优化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，准确拟合3D图像数据（如多晶体和泡沫）的镶嵌模型对理解微观结构至关重要。

Method: 采用线性/非线性规划、随机优化（交叉熵法）和梯度下降等方法，生成Voronoi、Laguerre和广义平衡功率图（GBPDs）来近似体素结构。

Result: 通过实际数据集评估拟合质量，发现模型复杂度、优化方法复杂度和近似质量之间存在权衡。

Conclusion: 研究为根据数据特征和应用需求选择合适方法提供了指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 论文探讨了深度学习在自动驾驶场景理解中的应用，提出了几种高效模型，并通过BDD100k数据集验证其性能。


<details>
  <summary>Details</summary>
Motivation: 人工智能（尤其是深度学习）在解决复杂任务中表现出色，减少了对人类专业知识的依赖。自动驾驶是其中一个重要应用领域。

Method: 提出了几种基于语义分割的场景理解模型，并尝试了多种Backbone作为编码器。

Result: 实验结果表明，选择合适的Backbone对模型性能有显著影响，模型在准确率、平均IoU和损失函数等指标上均有提升。

Conclusion: 通过优化Backbone选择，可以显著提高语义分割模型的性能，从而更好地理解自动驾驶场景。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种新的基于梯度的测试时适应方法，针对视觉语言模型（VLMs），通过软对比损失改进分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于熵最小化的测试时适应方法（TTA）与VLMs的对比训练目标不一致，导致性能受限和失败模式（如伪标签漂移和类别崩溃）。

Method: 提出CLIPTTA，利用与CLIP预训练目标一致的软对比损失，并通过理论分析证明其批次感知设计能避免崩溃风险。进一步扩展到开放集设置，使用异常对比暴露（OCE）损失改进OOD检测。

Result: 在75个数据集上评估，CLIPTTA表现优于基于熵的目标，并在多个数据集上超越现有TTA方法，展现出更稳定的性能。

Conclusion: CLIPTTA通过对齐预训练目标的适应方法，显著提升了VLMs在分布偏移下的泛化能力。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 论文提出了一种名为注意力聚焦（AF）的机制，通过剪除非信息性标记来优化广义类别发现（GCD）任务中的特征提取。AF由两个组件组成：标记重要性度量（TIME）和标记自适应剪枝（TAP），显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时，模型注意力容易被无关背景区域分散，导致特征提取不理想。

Method: AF机制通过TIME量化多尺度标记重要性，再通过TAP剪除非信息性标记，以聚焦关键对象。

Result: 将AF集成到SimGCD方法中，性能提升高达15.4%，且计算开销极小。

Conclusion: AF是一种轻量级、即插即用的模块，能显著提升GCD任务的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯变形场的3DGS视频流媒体框架，通过混合显著性分块和差异化质量建模，实现了高效压缩和带宽适应。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）视频的数据量大且传输复杂，需要解决其流媒体传输的挑战。

Method: 设计了基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模。

Result: 实验验证了该方法在视频质量、压缩效率和传输速率上的优越性。

Conclusion: 提出的框架有效解决了3DGS视频流媒体传输的挑战，性能优于现有方法。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [6] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 生成超分辨率（GSR）在感知图像质量上领先，但仍存在与低分辨率图像或真实图像不匹配的“幻觉”问题。本文提出一种基于多模态大语言模型（MLLM）的“幻觉评分”（HS）方法，并通过深度特征距离优化GSR模型以减少幻觉。


<details>
  <summary>Details</summary>
Motivation: GSR模型在感知质量上表现优异，但生成的细节可能与输入图像或真实图像不匹配，这种“幻觉”问题限制了其实际应用。本文旨在量化并减少此类问题。

Method: 利用多模态大语言模型（MLLM）构建提示，评估幻觉视觉元素并生成“幻觉评分”（HS）。同时，通过深度特征距离作为可微分奖励函数优化GSR模型。

Result: HS与人类评估高度一致，且为超分辨率模型提供了补充性见解。某些深度特征距离与HS有强相关性。

Conclusion: 提出的HS方法和深度特征优化能有效减少GSR模型的幻觉问题，提升其实际应用价值。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [7] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D高斯溅射视频（3DGS）流媒体的综合解决方案，包括自适应分块技术、质量评估框架和基于元学习的比特率自适应算法。


<details>
  <summary>Details</summary>
Motivation: 3DGS流媒体在提供沉浸式3D视频体验方面表现优异，但仍面临分块、质量评估和比特率自适应等挑战。

Method: 提出自适应3DGS分块技术（基于显著性分析）、新型质量评估框架（联合评估3DGS表示和2D渲染图像质量）和基于元学习的比特率自适应算法。

Result: 实验表明，所提方法显著优于现有技术。

Conclusion: 本文为3DGS流媒体提供了有效的解决方案，解决了关键挑战。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [8] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一种基于深度学习和光流的半自动化工具包，用于B型超声视频中的点跟踪，解决了传统方法中的噪声和运动模糊问题。


<details>
  <summary>Details</summary>
Motivation: B型超声中组织运动的准确跟踪因斑点噪声、低边缘对比度和平面外运动而具有挑战性，影响了临床和研究应用中对组织动态的量化。

Method: 结合深度学习和光流技术，提供高质量、鲁棒的跟踪，并引入图形用户界面和光流滤波技术以减少噪声。

Result: DUSTrack在准确性上优于零样本点跟踪器，与专用方法相当，适用于多种解剖结构和运动模式。

Conclusion: DUSTrack是一个通用且强大的开源工具，适用于临床和生物力学研究中的组织运动量化。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [9] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级机器人动作预测方法，基于InstructPix2Pix模型，仅需单张图像和文本指令即可预测未来10秒的视觉帧，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹对机器人、自主系统等领域至关重要，现有视频预测模型计算成本高且延迟大，需要更高效的解决方案。

Method: 利用InstructPix2Pix模型，结合视觉和文本输入，实现多模态未来帧预测，仅需单张图像和文本指令。

Result: 在RoboTWin数据集上，SSIM和PSNR优于现有基线，计算成本和延迟显著降低。

Conclusion: 该方法在机器人动作预测中高效且轻量，适用于对运动轨迹精度要求高的场景。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [10] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，用于可解释的affordance grounding，通过结合ConceptNet和CLIP的视觉证据，迭代优化预测。


<details>
  <summary>Details</summary>
Motivation: 解决场景理解中对象与动作关联的透明性和可解释性问题。

Method: 整合ConceptNet的常识先验和CLIP的视觉证据，使用基于能量的推理循环迭代优化预测。

Result: 在多对象、无标签设置中提高了准确性和可解释性。

Conclusion: CRAFT为稳健且可信的场景理解提供了方向。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [11] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个针对真实红外图像的多模态大语言模型，基于大规模红外-文本数据集IR-TD，通过双跨模态课程迁移学习策略，在9项任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖合成红外图像的局限性，以及真实红外图像与文本对齐数据稀缺的问题。

Method: 构建IR-TD数据集（26万真实红外图像-文本对），提出双跨模态课程迁移学习策略，从可见光到红外域系统迁移知识。

Result: 在9项任务（如识别、定位）中表现优于现有方法，包括更大规模模型。

Conclusion: IRGPT通过真实数据集和迁移学习策略，显著提升了红外图像的多模态理解能力。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [12] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），用于点云配准中的高质量对应关系识别，通过正交几何一致性和多粒度并行交互提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决点云配准中局部与全局特征融合的挑战，如特征冗余和复杂空间关系。

Method: 利用Gestalt原则设计GPI-Net，包括正交集成策略、Gestalt特征注意力块（GFA）和双路径多粒度并行交互块（DMG）。

Result: 在多个任务中表现优于现有方法。

Conclusion: GPI-Net通过Gestalt原则和并行交互机制有效提升了点云配准的准确性和鲁棒性。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [13] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一个基于Mixture-of-Experts的端到端自动驾驶框架，通过全局专家、场景自适应专家组和双感知路由器实现多样化和鲁棒的驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模式规划方法难以处理多样化交通场景，需要一种能适应复杂环境的自动驾驶框架。

Method: 提出GEMINUS框架，包含全局专家、场景自适应专家组和双感知路由器，动态激活专家模块以应对不同场景。

Result: 在Bench2Drive基准测试中表现优异，驾驶评分和成功率均达最优，单目视觉输入下也有显著提升。

Conclusion: GEMINUS通过专家模块的耦合实现了自适应和鲁棒的驾驶性能，显著优于单专家基线。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [14] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一种抗篡改的可视化图像数据检索框架，通过嵌入元数据链接解决现有方法易受图像裁剪和编辑影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可视化图像中嵌入元数据时易受在线分发中的常见篡改（如裁剪和编辑）影响，缺乏实用性。

Method: VisGuard采用重复数据平铺、可逆信息广播和基于锚点的裁剪定位方案等技术，增强嵌入数据的鲁棒性。

Result: 实验表明，VisGuard在数据检索准确性、嵌入容量和抗篡改安全性方面表现优异。

Conclusion: VisGuard能有效促进和保护可视化传播与信息传递。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [15] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet提出了一种新颖的序列建模框架，结合空间特征提取和时间差分，通过端到端训练提升动态环境中的视觉地点识别性能。


<details>
  <summary>Details</summary>
Motivation: 动态和感知混淆环境中的视觉地点识别（VPR）是长期定位的核心挑战，现有方法多忽略时间连贯性。

Method: 采用轻量级1D卷积编码器和可学习差分时间算子（DSD），结合LSTM和四元组损失，实现端到端序列级嵌入学习。

Result: 在多个公开基准测试中，OptiCorNet在季节和视角变化下优于现有方法。

Conclusion: OptiCorNet通过端到端学习序列级嵌入，显著提升了动态环境中的地点识别效果。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [16] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT提出了一种无需数据的视觉变换器量化方法，通过合成高质量样本和激活校正矩阵，显著提升了量化模型的性能，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成样本时未能充分平衡全局和局部特征，且量化模型与全精度模型的中间层激活分布差异大，导致性能下降。

Method: 按难度递增顺序合成样本，并引入激活校正矩阵以对齐量化模型与全精度模型的中间层激活分布。

Result: DFQ-ViT在性能上显著优于现有DFQ方法，接近真实数据量化的模型，例如3位权重量化的DeiT-T性能提升4.29%。

Conclusion: DFQ-ViT无需微调，降低了计算开销和部署门槛，符合绿色学习原则，适用于资源受限环境。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [17] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强的点云补全框架，通过跨模态检索学习结构先验信息，生成细粒度点云。


<details>
  <summary>Details</summary>
Motivation: 解决不完整点云补全任务中缺乏典型结构特征的问题，并提升生成能力和泛化能力。

Method: 设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），通过双通道控制门和分层特征融合机制整合参考样本的先验信息。

Result: 在多个数据集和真实场景中验证了方法的有效性，能够生成细粒度点云，并具备处理稀疏数据和未见类别的泛化能力。

Conclusion: 提出的框架通过检索增强和跨模态学习，显著提升了点云补全的性能和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [18] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种新型多模态大语言模型，通过令牌压缩技术解决病理学全切片图像（WSI）的视觉问答（VQA）问题，显著降低计算成本并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 病理学全切片图像（WSI）的高分辨率和长上下文长度对多模态大语言模型（MLLM）提出了挑战，现有方法计算资源消耗大且缺乏生成能力。

Method: 提出TCP-LLaVA，通过可训练的压缩令牌聚合视觉和文本信息，仅将压缩后的令牌输入语言模型，减少计算负担。

Result: 在十种TCGA肿瘤亚型的实验中，TCP-LLaVA在VQA准确性上优于现有方法，并大幅降低训练资源消耗。

Conclusion: TCP-LLaVA为WSI的VQA提供了一种高效且准确的解决方案，推动了病理学图像分析的发展。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [19] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 提出了一种基于事件法线流的运动分割和自运动估计框架，适用于神经形态视觉传感器，无需完整光流计算即可实现准确分割和运动估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而神经形态传感器的事件数据稀疏且时间分辨率高，需要一种更高效的方法。

Method: 利用事件数据，结合几何约束，通过优化流程进行事件过分割、残差分析分离运动物体，并基于运动相似性和时间一致性进行层次聚类。

Result: 在EVIMO2v2数据集上验证了方法的准确性，尤其在物体边界表现优异。

Conclusion: 该方法在实时机器人和导航应用中具有显著潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [20] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 综述了基于深度学习的前馈方法在3D重建和视图合成中的应用，分类介绍了不同表示架构，并讨论了关键任务、数据集、评估协议及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算复杂且难以应用于实际场景，前馈方法通过深度学习实现了快速且通用的3D重建和视图合成。

Method: 分类介绍了点云、3D高斯泼溅（3DGS）、神经辐射场（NeRF）等表示架构，并分析了关键任务如无姿态重建、动态3D重建等。

Result: 前馈方法在数字人类、SLAM、机器人等领域有广泛应用，并推动了3D视觉技术的发展。

Conclusion: 前馈方法在3D视觉领域具有巨大潜力，但仍需解决开放研究挑战以进一步推动技术进步。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [21] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为DCHM的框架，通过深度一致性建模和多视角融合，在稀疏视角、大规模和拥挤场景中实现精确的行人检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角行人检测中引入噪声且精度低，依赖昂贵的3D标注，难以泛化到多样化场景。

Method: 采用超像素级高斯泼溅技术，实现全局坐标系中的深度一致性和多视角融合。

Result: 显著减少了建模过程中的噪声，优于现有方法，并在挑战性场景中首次实现行人重建和多视角分割。

Conclusion: DCHM框架在行人建模和定位中表现出色，无需依赖人工标注，适用于复杂场景。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [22] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: ArtiMuse是一个基于多模态大语言模型的图像美学评估模型，结合评分与专家级理解能力，并发布了首个专家标注的数据集ArtiMuse-10K。


<details>
  <summary>Details</summary>
Motivation: 随着教育应用、艺术创作和AI生成内容技术的发展，对图像美学评估的需求增加，现有方法存在模态偏差和缺乏细粒度属性分解的问题。

Method: 提出ArtiMuse模型，具备联合评分和专家级理解能力，并构建ArtiMuse-10K数据集，包含10,000张图像，标注了8维属性和整体评分。

Result: ArtiMuse模型在美学评估中表现出更强的感知和泛化能力，解决了传统方法的局限性。

Conclusion: ArtiMuse模型和数据集将公开，推动图像美学评估领域的发展。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [23] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 提出一种浏览器扩展，将手语实时翻译为视频会议中的字幕，以帮助听障人士与非听障人士沟通。


<details>
  <summary>Details</summary>
Motivation: 解决听障人士与非听障人士之间的沟通障碍，尤其是在疫情期间视频会议成为主要沟通方式的情况下。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展进行实时手语翻译。

Result: 扩展能够自动翻译手语为字幕，提升视频会议中的无障碍沟通。

Conclusion: 该技术有望显著改善听障人士的沟通体验，尤其是在远程会议场景中。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [24] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍了基于Florence模型的视觉问答（VQA）方法，用于胃肠道内窥镜图像分析，通过领域特定增强提升泛化能力，在KASVIR数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决胃肠道内窥镜视觉问答任务，探索大规模多模态模型在医学VQA中的潜力。

Method: 采用Florence模型作为VQA管道主干，结合视觉和文本编码器，并应用领域特定增强技术。

Result: 在KASVIR数据集上微调Florence模型，取得了官方挑战指标的准确回答。

Conclusion: 展示了大规模多模态模型在医学VQA中的潜力，为未来研究提供了强基线。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [25] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 研究探讨了人工神经网络（ANN）决策边界与人类情感感知变异性之间的关系，提出了一种新的感知边界采样方法，并构建了varEmotion数据集，揭示了ANN与人类感知的共同计算原则。


<details>
  <summary>Details</summary>
Motivation: 解决情感认知科学中外部情感刺激与人类内部体验关系建模的挑战，特别是探索ANN在捕捉个体感知差异方面的潜力。

Method: 引入感知边界采样方法生成模糊的ANN分类样本，构建varEmotion数据集，并通过大规模人类行为实验验证。

Result: 发现ANN难以分类的样本同样引发人类感知不确定性，通过行为数据微调ANN，实现了与人类感知模式的对齐。

Conclusion: 建立了ANN决策边界与人类感知变异性的系统性联系，为个性化情感解释建模提供了新视角。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [26] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 开发了一种相机引导系统，帮助用户识别和去除照片中的杂乱内容，提升照片美学质量。


<details>
  <summary>Details</summary>
Motivation: 照片中的杂乱内容会分散注意力，影响情感传达，业余摄影师常因疏忽或经验不足而未能避免。

Method: 系统通过算法评估对象对照片美学的贡献，提供交互式杂乱识别工具，并结合生成对抗网络进行图像修复。

Result: 用户研究表明，系统能帮助用户更高效地识别杂乱并提升照片质量。

Conclusion: 该系统通过灵活界面和精确算法，有效改善了用户的摄影体验和作品质量。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [27] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D提出了一种通过自然语言编码对象间关系的新框架，显著提升了3D场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景-语言模型在对象间关系理解上表现不足，尤其是视觉嵌入无法充分表达对象角色与交互。

Method: Descrip3D通过文本描述增强对象表示，结合嵌入融合和提示级注入的双层集成方法。

Result: 在五个基准数据集上，Descrip3D均优于基线模型。

Conclusion: 语言引导的关系表示能有效提升复杂室内场景的理解能力。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [28] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 论文提出了一种名为LEAD的方法，通过基于logits的网络输出来预测预训练模型在下游任务中的迁移性能，避免了冗长的微调过程。


<details>
  <summary>Details</summary>
Motivation: 预训练模型数量激增，如何高效选择最适合下游任务的模型成为挑战。现有方法在特征空间中建模微调动态，未能准确捕捉优化的非线性特性。

Method: LEAD提出理论框架，用ODE描述logit状态的非线性演化，并设计类感知分解方法考虑不同类别的动态变化。

Result: 在24个预训练模型和10个下游数据集上的实验表明，LEAD表现优异，尤其在低数据场景下适应性广泛。

Conclusion: LEAD通过优化目标对齐和非线性建模，有效解决了预训练模型选择问题，显著提升了迁移性能。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [29] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文比较了GAN、扩散模型和流匹配技术在MRI模态转换中的表现，发现Pix2Pix GAN在结构保真度、图像质量和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的模态图像。

Method: 使用GANs、扩散模型和流匹配技术进行T1w到T2w的2D MRI图像转换，并在三个公开数据集上评估。

Result: Pix2Pix GAN在结构保真度、图像质量和计算效率上表现最佳，流匹配方法在小数据集上容易过拟合。

Conclusion: GAN更适合实际MRI工作流中的模态转换，未来研究可探索更多数据以提升流匹配性能。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [30] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了TensorFlow、PyTorch和JAX在BloodMNIST数据集上的性能，发现不同框架在推理时间和分类准确性上存在差异。


<details>
  <summary>Details</summary>
Motivation: 缺乏对特定深度学习框架在血液图像分类中性能的详细分析。

Method: 使用BloodMNIST数据集，比较TensorFlow、PyTorch和JAX在推理时间和分类准确性上的表现。

Result: JAX和PyTorch的分类准确性接近当前基准，性能受图像分辨率和框架优化影响。

Conclusion: JAX和PyTorch在医学图像分类中表现高效，适合相关应用。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [31] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是首个解决3D开放词汇子概念发现的方法，结合神经场表示和无监督分割，适应场景和用户查询。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务或场景内容，无法同时满足场景和用户查询的需求。

Method: 基于神经场表示，结合无监督分割和弱开放词汇指导。

Result: 在开放词汇子概念发现中表现优异，同时在开放词汇和无监督分割的边缘案例中达到SOTA。

Conclusion: DiSCO-3D为3D语义分割提供了更灵活的解决方案。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [32] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph是一个基于图建模的新框架，用于面部表情识别，结合了视觉变换器和图卷积网络，显著提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互中至关重要，但面部属性的结构变化需要被有效捕捉以提高识别效果。

Method: 使用面部关键点作为图的顶点，基于邻近性和局部外观相似性确定边，结合视觉变换器和图卷积网络捕捉结构依赖关系。

Result: 在Oulu-CASIA、eNTERFACE05和AFEW数据集上分别达到98.09%、79.01%和56.39%的准确率。

Conclusion: Exp-Graph在实验室和真实环境中均表现出强大的泛化能力，适用于实际应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [33] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2是一个高效的适配框架，通过Depthwise-Dilated Adapter增强SAM2在医学视频分割中的多尺度特征提取，减少参数开销。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态特定设计，适应性差，且SAM2在医学视频场景中需要大规模数据集重新训练，计算成本高。

Method: 提出DD-SAM2框架，结合DD-Adapter，以最小参数开销增强多尺度特征提取，适用于有限训练数据的医学视频。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上表现优异，Dice分数分别为0.93和0.97。

Conclusion: DD-SAM2为医学视频分割和跟踪提供了高效的适配器微调方案，代码和模型将公开。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [34] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为BusterX++的新型框架，用于跨模态检测和解释合成媒体，解决了现有单模态检测方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有检测方法因单模态设计而受限。

Method: BusterX++采用强化学习后训练策略，结合多阶段训练、思维奖励和混合推理。

Result: 实验证明BusterX++在性能和泛化能力上表现优异。

Conclusion: BusterX++为跨模态合成媒体检测提供了有效解决方案，并提出了高质量基准GenBuster++。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [35] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion提出了一种基于状态空间模型的双路径参数交互机制，有效解决了多光谱特征融合中的局部互补特征偏好和计算复杂度问题，显著提升了目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现代多光谱特征融合在目标检测中存在两个关键问题：过度偏好局部互补特征而忽视跨模态共享语义，以及感受野大小与计算复杂度之间的权衡。

Method: MS2Fusion通过双路径参数交互机制实现高效融合：一条路径利用跨模态隐藏状态解码挖掘互补信息，另一条路径通过参数共享探索跨模态对齐。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他先进方法，并在RGB-T语义分割和RGBT显著目标检测任务中表现出通用性。

Conclusion: MS2Fusion不仅提升了多光谱目标检测性能，还展示了其在其他多光谱感知任务中的通用性。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [36] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个基于AI的框架，用于提升体育裁判的效率和公平性，特别是在跆拳道中实时头部踢击检测和评分。


<details>
  <summary>Details</summary>
Motivation: 传统裁判系统存在延迟、主观性和不一致性，影响公平性和运动员信任。

Method: 结合计算机视觉、深度学习和边缘推理，实现动作的自动识别与分类。

Result: 将决策时间从分钟缩短到秒，提高一致性和透明度。

Conclusion: FST.ai框架具有鲁棒性、可扩展性，适用于多种体育项目。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [37] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 该研究提出了一种基于计算机视觉的成本效益框架，用于量化机构用餐环境中的食物浪费，通过语义分割分析餐前餐后图像，模型表现良好，为实时监测食物浪费提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 量化机构用餐环境中的食物浪费是制定数据驱动的可持续发展策略的关键。

Method: 研究使用了四种全监督模型（U-Net、U-Net++及其轻量版），通过语义分割分析RGB图像，并采用动态逆频率损失和AdamW优化器进行训练。

Result: 所有模型表现良好，部分模型在特定食物类型上达到或超过90%的DPA指标，轻量模型实现了实时推理。

Conclusion: 该框架为大规模食物服务环境中的实时浪费监测提供了基础，并为减少食物浪费提供了可行方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [38] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通过双路径多级判别增强组织病理学图像与基因表达之间的跨模态对齐，提升基因表达预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用组织病理学图像与基因表达的多层次跨模态对齐，限制了预测性能。

Method: 提出Gene-DML框架，通过双路径多级判别（多尺度实例级判别和跨级实例组判别）增强形态与转录模态的对应关系。

Result: 在公共空间转录组数据集上，Gene-DML实现了最先进的基因表达预测性能。

Conclusion: Gene-DML通过联合建模细粒度和结构级判别，提升了预测准确性和泛化能力。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [39] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 论文提出了Doc-750K数据集和Docopilot模型，解决了多模态大语言模型在复杂文档理解中的不足，避免了检索增强生成方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂文档理解上表现不佳，主要因缺乏高质量的文档级数据集，且现有检索增强生成方法存在上下文碎片化、多阶段误差累积等问题。

Method: 构建了Doc-750K数据集，包含多样文档结构和跨页依赖关系，并开发了原生多模态模型Docopilot，直接处理文档级依赖。

Result: 实验表明，Docopilot在文档理解任务和多轮交互中表现出更高的连贯性、准确性和效率。

Conclusion: Docopilot为文档级多模态理解设立了新基准，相关数据、代码和模型已开源。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [40] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一个协作多代理系统，用于多模态WSI分析，通过任务分配、验证和总结模块提升准确性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在WSI分析中表现不如任务专用模型的问题，并探索协作多代理系统在病理学领域的潜力。

Method: 提出WSI-Agents系统，包含任务分配模块、验证机制和总结模块，结合专家代理和知识库。

Result: 在多种WSI任务上优于现有WSI MLLMs和医疗代理框架。

Conclusion: WSI-Agents通过协作代理系统显著提升了WSI分析的准确性和多功能性。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [41] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 论文提出了一种名为MIPD的框架，通过从多模态大语言模型（MLLM）中蒸馏知识，提升小模型在开放词汇接地情境识别（Ov-GSR）任务中的泛化和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在复杂接地情境识别（GSR）任务中表现不佳且资源消耗大，而传统GSR模型泛化能力不足。

Method: 提出MIPD框架，利用LLM生成判断性理由，并通过场景感知和实例感知提示对齐视觉信息，最终将知识蒸馏到学生模型。

Result: 在Ov-SWiG和HICO-DET数据集上，MIPD在已知、罕见和未知情境中表现优异，并提升了未知检测能力。

Conclusion: MIPD通过知识蒸馏有效提升了小模型的泛化和零样本能力，解决了现有模型的局限性。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [42] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一个名为GTPBD的全球梯田地块数据集，填补了现有研究中梯田地形复杂性和多样性的空白，支持多种任务如语义分割、边缘检测等。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究主要关注中分辨率或平坦农田，缺乏对复杂梯田地形的精细表示，而GTPBD旨在解决这一问题。

Method: 构建了包含47,537张高分辨率图像和三级标注（边界、掩码、地块）的GTPBD数据集，覆盖全球多个地理区域。

Result: GTPBD在多种任务（如语义分割、边缘检测）上进行了基准测试，展示了其挑战性和实用性。

Conclusion: GTPBD为梯田遥感研究提供了基础设施，支持精细农业地形分析和跨场景知识迁移。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [43] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: 提出MultiRetNet，结合视网膜成像、社会经济因素和共病情况，提高糖尿病视网膜病变分期准确性，并整合临床延迟系统，优化早期检测。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球可预防性失明的主要原因，低收入社区患者因筛查机会有限，风险更高。共病条件加速疾病进展，需改进早期检测方法。

Method: 提出MultiRetNet，结合多模态数据（视网膜成像、社会经济因素、共病情况），采用三种融合方法，最终选择全连接层融合。通过对抗性图像和对比学习训练延迟系统，识别需临床复查的样本。

Result: 系统在低质量图像上保持诊断准确性，整合关键健康数据，可提高早期检测率，尤其对服务不足人群。

Conclusion: MultiRetNet有望降低医疗成本，提高早期检测率，减少医疗资源分配不均，促进健康公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [44] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 论文提出InterAct VideoQA数据集，用于评测和提升视频问答模型在交通监控任务中的表现，包含8小时真实交通视频和2.5万QA对，展示了模型在复杂交通场景中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答模型难以处理真实交通场景的复杂性，需领域特定数据集提升性能。

Method: 构建InterAct VideoQA数据集，包含多样交通视频和QA对，评测并微调现有模型。

Result: 模型在数据集上表现不佳，但微调后性能显著提升，验证了领域特定数据的重要性。

Conclusion: InterAct VideoQA为智能交通系统的视频问答模型研究提供了基准数据集。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [45] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过结合因果感知查询优化和细粒度视觉定位，解决了视频问答中关键帧稀疏和因果推理的挑战，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答方法存在任务无关采样和启发式检索的局限性，无法有效处理复杂推理问题。

Method: 利用LLM优化问题-选项对，通过时间定位模型精确检索关键片段，并动态融合证据，最后由MLLM生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA上达到SOTA性能，同时保持计算效率。

Conclusion: LeAdQA通过精确的视觉定位和因果推理，显著提升了复杂视频问答任务的性能。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [46] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS框架通过类特定光谱提示和可学习的[SINK]令牌，实现了高效且可靠的ViT空间-光谱可解释性，解决了HSI数据的高维性和现有显著性方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers（ViTs）在HSI数据中解释性不足的问题，包括现有显著性方法无法捕捉有意义的频谱线索和计算成本过高。

Method: FOCUS框架引入类特定光谱提示和可学习的[SINK]令牌，通过吸引损失训练，无需梯度反向传播或主干修改。

Result: FOCUS提高了波段级IoU 15%，减少注意力崩溃40%以上，且结果与专家标注高度一致。

Conclusion: FOCUS以不到1%的参数开销，为高分辨率ViT可解释性提供了实用解决方案，填补了黑盒建模与可信HSI决策之间的空白。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [47] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息互补的下采样方法HPD，用于解决传统下采样在语义分割任务中丢失关键空间信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法（如最大池化和跨行卷积）在特征聚合、感受野扩展和计算量减少方面表现良好，但在语义分割任务中可能导致关键空间信息丢失，影响像素级预测精度。

Method: 提出Hybrid Pooling Downsampling (HPD)，用MinMaxPooling替代传统方法，通过提取局部区域的最大值信息，有效保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上的实验表明，HPD在分割性能上优于传统方法，平均DSC系数提高0.5%。

Conclusion: HPD模块为语义分割任务提供了一种高效的解决方案。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [48] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EPD的新型ODE求解器，通过并行梯度评估减少截断误差，实现高质量低延迟采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样延迟高，现有加速方法在低延迟下图像质量下降。

Method: EPD利用多个并行梯度评估优化ODE步骤，参数可学习且训练开销小。

Result: 在多个图像合成基准上，EPD在5 NFE延迟下显著优于现有方法（如CIFAR-10 FID 4.47）。

Conclusion: EPD是一种高效插件，可提升现有ODE采样器性能，实现高质量低延迟采样。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [49] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 论文评估了DUSt3R、MASt3R和VGGT等3D重建基础模型在航拍图像上的表现，发现它们能从极稀疏图像集（少于10张）中准确重建密集点云，但高分辨率和大图像集时效果下降。


<details>
  <summary>Details</summary>
Motivation: 研究这些模型在航拍图像上的潜力，填补其在摄影测量领域未探索的空白。

Method: 在UseGeo数据集的航拍块上对预训练模型进行姿态估计和密集3D重建的全面评估。

Result: 模型能从极稀疏图像集重建密集点云，完整性比COLMAP提升50%，VGGT计算效率更高。但高分辨率和大图像集时姿态可靠性下降。

Conclusion: 基于Transformer的方法无法完全替代传统SfM和MVS，但在低分辨率、稀疏场景中可作为补充。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [50] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一种基于视觉提示的统一低层视觉任务处理框架VPIP，通过输入-目标图像对指导模型完成多种任务，并展示了其可扩展性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决低层视觉任务多样化且任务间差异大的问题，提出统一建模方法。

Method: 设计了VPIP框架，包含图像处理主干、提示编码器和提示交互模块，支持灵活架构集成和任务特定表示利用。

Result: 在100多个任务的大规模基准测试中表现优异，增加训练任务数提升了泛化能力，尤其在数据有限的任务中。

Conclusion: VPIP框架有效、可扩展，具备作为通用低层视觉建模基础的潜力。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [51] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类认知的多脸深度伪造视频检测方法HICOM，通过分析人类依赖的四种关键线索，显著提升了检测准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多脸深度伪造视频在自然社交场景中日益普遍，现有方法因缺乏对上下文线索的关注而表现不佳。

Method: 通过人类研究识别四种关键线索，并设计HICOM框架，结合LLM提供可解释性。

Result: HICOM在基准数据集上平均准确率提升3.3%，泛化能力优于现有方法5.8%。

Conclusion: 研究表明，结合人类认知线索可有效提升深度伪造检测能力，并增强结果的可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [52] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant是一种统一的量化框架，通过自适应结合互补技术提升跨模型通用性，解决了扩散模型量化中的通用性和部署兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高，现有后训练量化方法通用性不足，难以适应工业部署需求。

Method: 提出SegQuant框架，包含SegLinear（分段感知的图量化策略）和DualScale（双尺度量化方案），以捕捉结构语义并保持视觉保真度。

Result: SegQuant在多种扩散模型中表现优异，且与主流部署工具兼容。

Conclusion: SegQuant为扩散模型量化提供了高效、通用的解决方案，适用于工业部署。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [53] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench是首个专注于真实世界金融图表的基准测试，包含1,200张图表和7,016个问题。评估25个先进LVLM后，发现开源与闭源模型差距缩小，但模型在指令遵循、空间推理等方面仍有显著局限。


<details>
  <summary>Details</summary>
Motivation: 金融图表因复杂的时间结构和领域术语而未被充分研究，现有LVLM在此领域的表现尚不明确。

Method: 构建FinChart-Bench数据集，包含1,200张金融图表和7,016个问题，评估25个LVLM的性能。

Result: 发现开源与闭源模型差距缩小，升级模型性能下降，指令遵循和空间推理能力不足，LVLM不可靠作为自动评估器。

Conclusion: 当前LVLM在金融图表理解上存在重要局限，FinChart-Bench为未来研究提供了基准。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [54] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet通过将目标域雾霾模式转移到源域无雾图像上，创建特定域微调集，提升去雾模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在未见过的真实雾霾图像上表现不佳，因训练数据有限。

Method: 提出PHATNet，利用物理引导的雾霾转移网络，结合Haze-Transfer-Consistency损失和Content-Leakage损失。

Result: PHATNet显著提升了去雾模型在真实图像数据集上的性能。

Conclusion: PHATNet通过域适应方法有效解决了去雾模型在未见数据上的性能下降问题。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [55] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种用于数字乳腺断层合成（DBT）图像中肿块分割的配对图像生成方法，解决了现有扩散模型在生成质量和标注数据不足方面的问题。


<details>
  <summary>Details</summary>
Motivation: DBT图像中肿块的高隐蔽性导致标注困难且数据稀缺，现有扩散模型生成质量低且无法生成标注，限制了其在监督训练中的应用。

Method: 通过训练额外的扩散引导器，实现无需外部条件的配对图像生成，包括DBT切片和肿块掩码。

Result: 实验表明，该方法提高了生成质量，缓解了标注数据不足问题，并提升了下游任务的性能。

Conclusion: 该方法为DBT图像分割任务提供了一种有效的配对数据生成方案，具有实际应用价值。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [56] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出了一种仅需稀疏深度测量和对应图像的自监督深度补全方法，无需密集标签或多帧图像。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖密集深度标签或多帧图像的局限性，适用于静态或单帧场景。

Method: 利用深度分布特性设计新损失函数，结合视觉基础模型生成的语义分割图提升深度估计。

Result: 实验证明该方法有效。

Conclusion: 新方法在无需额外数据的情况下实现了高质量的深度补全。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [57] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言和基础模型的全能视频修复框架，无需预知退化类型，并在多个新基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法通常需要预知退化类型，限制了灵活性和可解释性。本文旨在通过自然语言和基础模型提供更灵活、可解释的修复指导。

Method: 利用基础模型从视频帧中提取退化感知的语义上下文，并通过自然语言表达。框架在训练和测试时无需退化知识，且推理时无需额外成本。

Result: 在多个新提出的基准测试（包括多退化任务和时变复合退化任务）中，方法性能优于现有技术。

Conclusion: 提出的框架为全能视频修复提供了灵活、可解释的解决方案，并呼吁标准化基准测试以推动领域发展。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [58] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的DETR增强框架，通过建模边界框为高斯分布并引入Gromov-Wasserstein距离，提高了目标检测的定位精度和预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器依赖确定性边界框回归，忽略预测不确定性，限制了模型鲁棒性。本文旨在解决这一问题。

Method: 将边界框建模为多元高斯分布，引入Gromov-Wasserstein距离损失，并基于贝叶斯风险过滤高风险信息。

Result: 在COCO基准测试中表现优异，并在白细胞检测任务中达到SOTA。

Conclusion: 该框架可扩展至通用和特定领域检测任务，验证了其有效性和可扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [59] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于微手势的情感识别方法，通过超图增强的Transformer和混合监督框架重构行为模式，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 微手势作为无意识的身体动作能传达人类情感，但基于微手势的情感建模研究不足。

Method: 采用超图增强的Transformer框架，结合自监督和监督学习，设计编码器和解码器，捕捉微手势的细微运动。

Result: 在两个公开数据集（iMiGUE和SMG）上表现最佳，优于现有方法。

Conclusion: 提出的方法在微手势情感识别中具有显著优势，为情感计算领域提供了新思路。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [60] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出一种无需训练或微调的方法，利用稀疏深度测量将基础模型的相对尺度深度预测转换为度量尺度深度，保持模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在零样本单目深度估计中输出为相对尺度而非度量尺度的问题，避免现有方法的高成本和泛化能力损失。

Method: 采用非学习的方法，利用稀疏深度测量将相对尺度深度预测转换为度量尺度深度，无需额外训练或微调。

Result: 实验证明该方法有效，能在不增加计算成本或牺牲泛化能力的情况下实现相对尺度到度量尺度的转换。

Conclusion: 该方法为实际应用中基础模型的直接部署提供了可行方案，填补了相对尺度与度量尺度之间的差距。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [61] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer结合了手工方法和深度学习的优势，提出了一种轻量级的光谱注意力模型，用于远程光电容积描记术（rPPG）估计，并通过无监督学习提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: rPPG技术需要在大规模数据集上泛化，而手工方法在未见场景中表现更好但性能有限，深度学习则依赖数据但提取能力更强，因此需要结合两者的优势。

Method: 提出BeatFormer模型，结合了放大的正交复数注意力和频域能量测量，并引入无监督的光谱对比学习（SCL）。

Result: 在PURE、UBFC-rPPG和MMPD数据集上验证了BeatFormer的鲁棒性和性能，尤其在运动场景下的跨数据集评估中表现优异。

Conclusion: BeatFormer通过混合方法显著提升了rPPG估计的性能和泛化能力，特别是在复杂条件下。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [62] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于2D预训练多模态网络的统一方法，简化了3D视觉定位任务中的多模态处理，减少了参数量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖多模态分离编码器，导致模型复杂且训练效率低。

Method: 利用2D CLIP双模态模型，通过适配器微调适应三模态任务，并设计GARF模块融合几何特征。

Result: 参数量减少约58%，3D检测任务性能提升6.52%，3D视觉定位任务提升6.25%。

Conclusion: 统一的多模态方法显著简化了架构，提升了效率和性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [63] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 本文提出了一种语义感知表示学习（SARL）方法，用于多标签图像分类，通过语义相关特征学习和最优传输注意力机制提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如注意力机制或图卷积网络）在图像表示中可能包含噪声且无法精确定位对象，因此需要改进。

Method: 1. 使用标签语义相关特征学习模块提取特征；2. 设计基于最优传输的注意力机制；3. 采用区域分数聚合策略进行多标签预测。

Result: 在PASCAL VOC 2007和MS-COCO数据集上的实验表明，SARL优于现有方法。

Conclusion: SARL通过语义对齐的图像表示和区域分数聚合，显著提升了多标签图像分类的性能。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [64] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 提出了一种解耦框架\method，用于高效预测3D高斯，通过局部图像对提取特征并融合，实现无姿态的3D重建，减少资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯几何和外观预测上耦合度高，依赖数据驱动先验且回归速度慢，需解决这些问题以提高效率。

Method: 使用立体视觉主干提取局部图像对特征，通过全局注意力块融合，生成几何和外观的高斯特征，再通过细化网络提升重建质量。

Result: 实现了无姿态的高质量3D重建，减少了对相机参数的依赖，提高了鲁棒性和实用性。

Conclusion: \method为实际3D内容生成提供了高效、可扩展的解决方案，同时保持高质量输出。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [65] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 提出了一种基于多维尺度分析（MDS）和鲁棒优化的方法，用于低温电子显微镜（cryo-EM）中的姿态估计和位移校正，显著提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: 低温电子显微镜（cryo-EM）中由于信噪比（SNR）极低，姿态估计和位移校正是关键挑战，直接影响3D重建的准确性。

Method: 利用MDS技术从二面角对估计旋转矩阵，结合鲁棒优化框架（ℓ₁范数）和迭代位移校正算法，确保单位范数和正交约束。

Result: 该方法在欧拉角精度和重建保真度（通过傅里叶壳层相关性FSC衡量）上均优于现有方法。

Conclusion: 提出的鲁棒优化和全局位移校正方法显著提升了低温电子显微镜中的姿态估计和重建质量。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [66] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种新的概率框架，用于多实例学习（MIL）中的注意力机制，通过估计注意力值的概率分布来捕捉全局和局部交互，并在医学影像分类中取得了最佳预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法将注意力值视为确定性，可能忽略个体实例贡献的不确定性，特别是在医学影像分类中。

Method: 提出了一种概率框架，估计注意力值的概率分布，同时考虑全局和局部交互。

Result: 在三个医学数据集和十一个基线方法中，该方法在不同指标上取得了最佳预测性能，并提供了可解释的不确定性图。

Conclusion: 该概率框架不仅提升了预测性能，还通过不确定性图增强了模型的可解释性。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [67] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出开放集跨模态泛化（OSCMG）任务，扩展了跨模态泛化（CMG）到开放集环境，并提出MICU方法，包含FCMI和CUJP两个组件，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态统一表示工作未考虑开放集环境，而现实应用中常需处理未见类别和新模态，因此提出OSCMG任务以填补这一空白。

Method: 提出MICU方法，包含FCMI（细粒度与粗粒度掩码对比学习）和CUJP（跨模态统一拼图），分别增强多模态对齐和特征多样性。

Result: 在CMG和新提出的OSCMG任务上进行了广泛实验，验证了方法的有效性。

Conclusion: MICU方法通过FCMI和CUJP组件，成功解决了开放集跨模态泛化问题，为现实应用提供了更鲁棒的解决方案。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [68] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一个实时多标签视频分类框架，通过动态激活轻量级适配器（LoRA）降低能耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备上的实时多标签视频分类受限于计算和能源预算，但视频流的结构特性（如标签稀疏性、时间连续性和标签共现）可被利用以提高效率。

Method: Polymorph框架动态选择和组合轻量级LoRA适配器，每个适配器专注于一组共现标签，避免全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph能耗降低40%，mAP提升9个百分点。

Conclusion: Polymorph通过模块化策略显著提升了嵌入式设备上视频分类的效率和性能。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [69] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的分类器，用于解决低重叠点云配准（PCR）任务中的评估问题，显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标在极低内点比例下失效，因此需要重新审视配准结果评估问题，并提出数据驱动的方法。

Method: 基于3DMatch数据集构建数据集，训练深度学习分类器评估配准质量，并将其集成到标准PCR流程中。

Result: 结合GeoTransformer等方法，在3DLoMatch基准上达到86.97%的配准召回率，并在ETH数据集上表现出强泛化能力。

Conclusion: 本文首次通过深度学习框架解决了PCR任务中的评估问题，显著提升了配准性能。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [70] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL提出了一种分层跨模态提示学习框架，通过双向知识流解决模态隔离和层次语义衰减问题，显著提升了预训练视觉语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在下游任务中的适应性和泛化能力仍面临挑战，尤其是模态隔离和层次语义衰减问题。

Method: HiCroPL通过分层知识映射器和轻量级知识代理，实现文本与视觉模态的双向知识流动，优化低层视觉语义和高层文本语义的对齐。

Result: 在四个任务和11个基准测试中，HiCroPL取得了最先进的性能，显著提升了模型的泛化能力。

Conclusion: HiCroPL通过双向知识流和多尺度语义融合，有效解决了预训练视觉语言模型的适应性问题，为未来研究提供了新方向。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [71] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 论文提出RvTC方法，通过灵活的基于分箱的分类替代预设词汇分类，在图像评估任务中实现最佳性能，并证明语义提示能显著提升多模态大语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在图像回归任务中表现有限，预设词汇和通用提示未能利用文本输入的语义理解。

Method: 提出Regression via Transformer-Based Classification (RvTC)，采用基于分箱的灵活分类方法，避免手动设计词汇，并通过数据特异性提示提升性能。

Result: 在四个图像评估数据集上实现最佳性能，AVA数据集中提示加入语义信息后相关性从0.83提升至0.90。

Conclusion: 语义提示信息对多模态回归任务至关重要，RvTC方法为图像评估提供了高效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [72] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于轴对齐几何约束的文档去扭曲方法，通过训练和推理阶段的优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法依赖标注数据，未充分利用文档的几何特性。

Method: 在训练阶段引入轴对齐几何约束，推理阶段采用轴对齐预处理策略。

Result: 在多个基准测试中达到SOTA，AAD指标提升18.2%~34.5%。

Conclusion: 该方法通过几何约束和预处理策略，显著提升了文档去扭曲的效果。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [73] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于B样条曲线拟合的新方法，用于优化FastSAM中的锯齿边缘，提升分割精度和视觉质量，同时保持实时处理能力。


<details>
  <summary>Details</summary>
Motivation: FastSAM虽然实现了实时分割，但其生成的锯齿边缘偏离真实物体形状，影响了分割的准确性和视觉质量。

Method: 采用B样条曲线拟合技术，通过四阶段精炼过程（包括两轮曲线拟合）来平滑锯齿边缘。

Result: 显著提升了边缘的视觉质量和分析准确性，同时保持了实时处理能力。

Conclusion: 该方法增强了FastSAM的实用性，为其在工业自动化、医疗影像等领域的应用提供了更大潜力。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [74] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了Video Thinking Test（Video-TT），用于评估视频大语言模型（video LLMs）在理解和解释真实世界视频时的正确性和鲁棒性，发现其与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型的基准测试未能充分反映其在视频理解中的正确性和鲁棒性与人类智能的差距，因此需要一种新的评估方法。

Method: 设计了Video-TT，包含1000个YouTube Shorts视频，每个视频配有一个开放式问题和四个对抗性问题，以测试视觉和叙事复杂性。

Result: 评估结果显示，视频大语言模型在视频理解和对抗性问题上的表现显著低于人类。

Conclusion: Video-TT揭示了视频大语言模型在复杂视觉叙事理解和鲁棒性方面的不足，为未来改进提供了方向。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [75] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波动方程数据集，旨在填补理论与实际成像应用之间的差距，支持神经算子方法的性能评估和实际医学成像应用。


<details>
  <summary>Details</summary>
Motivation: 传统波动方程数值求解器计算量大且不稳定，限制了实时成像应用；现有神经算子方法因数据集过于简化而效果受限。

Method: 提出OpenBreastUS数据集，包含8,000个解剖学真实的人体乳腺模型和1,600万次频域波模拟，用于评估神经算子的性能和泛化能力。

Result: 首次展示了神经算子求解器在人体乳腺活体成像中的高效应用。

Conclusion: OpenBreastUS为开发创新的神经PDE求解器提供了平台，并推动了其在现实医学成像中的部署。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [76] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI框架通过CLIP嵌入和自适应处理解决水下图像增强中的数据集偏差和高计算成本问题，同时提升透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对海洋保护至关重要，但现有AI模型存在数据集偏差、高计算成本和透明度不足的问题。

Method: EBA-AI利用CLIP嵌入检测和缓解数据集偏差，采用自适应处理优化能效，并结合不确定性估计和可解释性技术。

Result: 实验显示PSNR下降1.0 dB，但计算效率显著提升，适用于实时大规模海洋监测。

Conclusion: EBA-AI在效率、公平性和可解释性方面优于现有方法，为可持续海洋保护提供了新工具。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [77] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON提出了一种无需训练的通用虚拟试穿框架，通过解耦服装和姿势条件，实现跨场景的高保真纹理和姿势一致性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术要么依赖监督学习（高保真但泛化能力差），要么依赖无监督学习（适应性强但受数据偏差限制），缺乏统一的解决方案。

Method: OmniVTON通过服装先验生成机制和连续边界缝合技术保留细节，利用DDIM反演实现姿势对齐，解耦服装和姿势约束。

Result: 实验表明，OmniVTON在多样化数据集、服装类型和应用场景中表现优异，首次实现多人物虚拟试穿。

Conclusion: OmniVTON为虚拟试穿提供了一种无需训练、通用且高效的解决方案，解决了现有技术的局限性。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [78] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 提出PanTiny，一种轻量级单步pan-sharpening框架，通过多数据集联合训练和复合损失函数提升性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前pan-sharpening模型庞大且依赖单一数据集，计算开销高且泛化能力差，需改进。

Method: 设计PanTiny框架，采用多数据集联合训练和复合损失函数。

Result: PanTiny在性能和效率上优于大型专用模型，泛化能力显著提升。

Conclusion: 通过模型设计、训练范式和损失函数的优化，可超越暴力扩展，推动高效、通用模型的发展。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [79] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一种基于视频扩散模型的框架，通过可学习的姿态对齐和身份保持技术，解决了现有方法在身份一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的人类图像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性，因此需要一种更有效的解决方案。

Method: StableAnimator++ 结合了可学习的姿态对齐模块、全局内容感知的人脸编码器和分布感知的身份适配器，并通过 HJB 优化提升推理阶段的面部保真度。

Result: 实验证明，StableAnimator++ 在质量和数量上均优于现有方法。

Conclusion: StableAnimator++ 通过创新的模块设计和优化策略，显著提升了身份一致性和生成视频的质量。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [80] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 评估当前最先进的生成模型在文本图像生成和编辑中的能力，涵盖33个OCR任务，并指出其弱点。


<details>
  <summary>Details</summary>
Motivation: 探讨生成模型是否能掌握文本图像生成和编辑的复杂性，推动其成为通用模型的基础技能。

Method: 选择33个代表性OCR任务，分为五类，评估六个闭源和开源模型。

Result: 发现当前生成模型在OCR任务中的不足，强调需将文本图像生成内化为通用模型的基础能力。

Conclusion: 呼吁将逼真的文本图像生成和编辑作为通用模型的核心能力，而非依赖专用解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [81] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 论文介绍了LASED数据集和可转向CNN，用于解决无人机视觉地点识别中的数据集不足和旋转模糊问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉地点识别面临大规模高空数据集稀缺和图像旋转模糊的挑战，限制了模型的泛化能力。

Method: 提出LASED大规模数据集（约100万张图像）和可转向CNN，以处理旋转方差并提升特征表示。

Result: LASED训练的模型召回率显著提高，可转向CNN比传统CNN平均提升12%召回率。

Conclusion: 结合大规模数据集和旋转等变网络，显著增强了无人机视觉地点识别的鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [82] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 论文提出了首个ESD出血源数据集BleedOrigin-Bench和双阶段检测-跟踪框架BleedOrigin-Net，用于实时定位和跟踪ESD手术中的出血源，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: ESD手术中出血源的实时定位和持续监测存在挑战，现有AI方法仅关注出血区域分割，缺乏对出血源检测和动态跟踪的研究，且缺乏专用数据集。

Method: 提出BleedOrigin-Bench数据集和BleedOrigin-Net框架，结合检测和跟踪技术，从出血起始检测到持续空间跟踪。

Result: BleedOrigin-Net在出血起始检测、初始源检测和点跟踪方面分别达到96.85%、70.24%和96.11%的准确率。

Conclusion: 该研究填补了ESD手术中出血源检测和跟踪的空白，为AI辅助手术提供了有效工具。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [83] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet方法通过多任务ResNet架构和在线少样本学习，解决了SLAM闭环检测的准确性和实时计算问题，并引入新数据集LoopDB。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中闭环检测的准确性和嵌入式硬件实时计算的限制。

Method: 基于多任务ResNet架构，结合在线少样本学习和DISK描述符，优化嵌入式设备性能。

Result: LoopNet在多变条件下表现优于传统方法和手工特征，提供更高的闭环检测准确性。

Conclusion: LoopNet通过创新架构和新数据集，显著提升了SLAM系统的闭环检测性能。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [84] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: 论文提出VideoPlan方法，通过辅助任务增强和多令牌预测解决视觉规划中的数据和目标建模问题，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决长时程视觉规划中数据稀缺和动作空间结构化建模的挑战。

Method: 引入辅助任务增强和多令牌预测技术。

Result: 在COIN和CrossTask数据集上分别超越先前方法7.3%和3.4%，并在Ego4D任务中表现优异。

Conclusion: VideoPlan方法有效提升视觉规划性能，无需依赖特定特征。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [85] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空多图表示方法，用于改进事件传感器的稀疏数据建模，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器数据稀疏且异步，传统方法将其转换为密集张量会丧失优势，而现有图方法对时空动态建模不足。

Method: 构建解耦的空间图（B样条基函数建模全局结构）和时间图（运动向量注意力建模局部动态），替代昂贵的3D核。

Result: 在Gen1和eTraM数据集上，检测精度提升6%，速度提升5倍，参数减少且计算成本不变。

Conclusion: 结构化图建模能有效提升异步视觉任务的性能。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [86] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba是一种基于Mamba-SSMs的神经网络模型，用于高效学习和生成3D关节网格模型，支持超过10,000个顶点的处理。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理大规模3D网格数据时的效率和扩展性问题，特别是捕捉衣物和手部几何形状的需求。

Method: 通过将网格顶点序列化为适合Mamba处理的顺序，并基于身体部位注释或模板网格的3D顶点位置进行排序。设计了MambaDiff3D（生成模型）和Mamba-HMR（恢复模型）。

Result: MambaDiff3D在生成带衣物和手部的密集3D人体网格方面优于现有方法；Mamba-HMR扩展了非参数化人体网格恢复的能力，支持全身（包括面部和手部）且性能接近实时。

Conclusion: MeshMamba通过Mamba-SSMs实现了高效、可扩展的3D网格学习和生成，为3D人体建模任务提供了新的解决方案。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [87] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 论文提出N-JEPA方法，将扩散噪声引入掩码图像建模（MIM），以增强自监督学习（SSL）的表征能力。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在判别任务中表现优异，但生成模型在图像生成和细节增强方面更优。作者希望结合SSL与生成模型的核心原理（如扩散噪声），以提升SSL的表征能力。

Method: 提出N-JEPA方法，通过掩码标记的位置嵌入将扩散噪声引入MIM，并利用多级噪声调度增强模型鲁棒性。

Result: 在分类任务中验证了方法的有效性。

Conclusion: 结合扩散噪声与SSL的N-JEPA方法能有效提升表征能力，代码即将公开。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [88] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种基于层次部分的3D血管生成框架，将全局拓扑与局部几何细节分离，显著提升了复杂血管网络的建模效果。


<details>
  <summary>Details</summary>
Motivation: 准确表示血管的复杂几何和拓扑结构是医学应用中的挑战。

Method: 分三个阶段：关键图生成、血管段生成和层次血管组装。

Result: 在真实数据集上验证，性能优于现有方法。

Conclusion: 首次成功应用基于部分的生成方法，为血管数据生成设定了新基准。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [89] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的可解释性方法，用于分析乳腺影像中的视觉-语言基础模型Mammo-CLIP，以揭示其决策过程。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域如医学影像中，模型决策的可解释性对临床采用至关重要。

Method: 通过训练一个基于Mammo-CLIP的SAE模型，识别与临床相关乳腺概念（如肿块和可疑钙化）相关的潜在特征。

Result: 研究发现SAE潜在空间中的激活神经元与真实区域对齐，并揭示了影响模型决策的混杂因素。

Conclusion: 该研究展示了SAE潜在表示在揭示基础模型内部工作机制方面的潜力。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [90] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 本文提出了一种名为Coalescent Projection（CP）的新方法，结合伪类生成和自监督变换（SSTs），在跨域少样本学习（CD-FSL）中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前CD-FSL方法在更新过多Transformer参数时容易因样本稀缺导致过拟合，需要一种更有效的方法。

Method: 提出CP作为软提示的有效替代，并结合SSTs生成伪类，仅依赖基础域数据为网络应对未见域样本做准备。

Result: 在BSCD-FSL基准测试的极端域偏移场景中，该方法表现出色。

Conclusion: CP与SSTs的结合为解决CD-FSL中的过拟合问题提供了有效方案。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [91] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus是一个无需训练的框架，通过创新的注意力共享机制和改进的扩散变换器（DiT）技术，实现了零样本的高保真主题驱动图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练过程，限制了实际应用，且未能充分利用扩散变换器的零样本潜力。FreeCus旨在解决这一问题。

Method: 提出三种创新：1）注意力共享机制；2）改进的DiT动态偏移分析；3）集成多模态大语言模型（MLLMs）。

Result: 实验表明，FreeCus在零样本条件下实现了与需训练方法相当的性能，且兼容现有修复和控制模块。

Conclusion: FreeCus成功激活了DiT的零样本能力，为高保真主题驱动生成提供了无需训练的解决方案。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [92] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 论文提出了一种基于近似盲PnP的对应学习方法MinCD-PnP，通过最小化学习到的2D和3D关键点之间的Chamfer距离，解决了传统微分PnP对噪声和异常值敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统微分PnP在图像到点云（I2P）配准中对噪声和异常值高度敏感，影响了对应学习的有效性。

Method: 提出MinCD-PnP方法，简化盲PnP为最小化Chamfer距离的任务，并设计轻量级多任务学习模块MinCD-Net。

Result: 在多个数据集上实验表明，MinCD-Net在跨场景和跨数据集设置中均优于现有方法，提高了内点比率（IR）和配准召回率（RR）。

Conclusion: MinCD-PnP和MinCD-Net有效提升了I2P配准的鲁棒性和性能。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [93] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过生成模型从稀疏信号中重建视频，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐重建方面的优势，优化视频压缩的感知质量。

Method: 提出三个关键模块：多粒度条件捕捉静态和动态信息、紧凑表示设计、多条件训练增强鲁棒性。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器，尤其在高压缩比下表现突出。

Conclusion: 条件扩散模型为视频压缩提供了感知优化的有效途径。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [94] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 论文探讨了利用视觉语言模型（VLM）和上下文学习框架检测生物识别系统中的物理和数字攻击，性能优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 随着生物识别系统的发展，攻击技术日益复杂，传统深度学习方法难以适应多样化攻击和环境变化，且数据收集面临隐私和多样性挑战。

Method: 提出基于VLM和上下文学习的框架，系统评估其在安全关键场景中的应用，实验验证其性能。

Result: 实验表明，该方法在物理和数字攻击检测中表现优异，无需资源密集型训练即可超越传统CNN。

Conclusion: 该框架为提高攻击检测的泛化能力提供了有前景的工具。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [95] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为DMD的局部密集表示方法，用于指纹匹配，结合细粒度纹理和细节特征，通过三维张量表示，提升匹配效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在不同采集条件下指纹匹配的鲁棒性和准确性问题。

Method: 基于细节点的局部密集表示（DMD），提取局部图像块的语义特征，形成三维张量，利用前景分割掩码优化匹配。

Result: 在多种指纹数据集上验证了方法的有效性，达到了最先进的准确性和计算效率。

Conclusion: DMD方法在指纹识别中表现出色，具有大规模应用的潜力。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [96] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 论文提出了一种基于通道间相关性的空间-通道状态建模（SCSM）模块，用于提升少样本目标检测（FSOD）中特征通道的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在少样本目标检测中难以准确提取有效特征通道，高权重通道未必有效，低权重通道可能仍有价值。

Method: 设计了SCSM模块，包含空间特征建模（SFM）和基于Mamba的通道状态建模（CSM），以平衡空间与通道关系学习。

Result: 在VOC和COCO数据集上的实验表明，SCSM模块提升了特征表示质量，实现了最先进性能。

Conclusion: SCSM模块通过建模通道相关性，有效提升了少样本目标检测的性能。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [97] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出了BenchDepth，一种通过五个下游代理任务评估深度基础模型（DFMs）的新基准，避免了传统对齐指标的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有深度评估协议存在不一致性，传统基准依赖对齐指标，导致偏见和不公平比较。

Method: 设计了五个下游代理任务（如深度补全、立体匹配等）来评估DFMs的实际应用效果。

Result: 对八种先进DFMs进行了基准测试，并提供了关键发现和分析。

Conclusion: BenchDepth为深度模型评估提供了新思路，有望推动深度估计领域的进一步研究。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [98] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过显式建模双特征分布，解决了工业缺陷检测中单类异常检测的局限性，利用潜在扩散模型生成合成缺陷数据，并通过邻域感知评分机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测系统在单类异常检测范式下面临数据稀缺和异常分布假设不准确的限制，需要一种更有效的方法。

Method: 提出ExDD框架，显式建模正常和异常特征分布，使用潜在扩散模型生成合成缺陷数据，并设计邻域感知评分机制。

Result: 在KSDD2数据集上表现优异（I-AUROC 94.2%，P-AUROC 97.7%），最佳增强效果为100个合成样本。

Conclusion: ExDD通过双分布建模和合成数据生成，显著提升了工业缺陷检测的性能和鲁棒性。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [99] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion通过合成异常生成和双路径特征适应解决路面缺陷检测中的数据稀缺、域偏移和缺陷多样性问题。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临标注数据有限、训练与部署环境间的域偏移以及不同道路条件下缺陷外观的高变异性等挑战。

Method: 提出RoadFusion框架，利用潜在扩散模型生成多样化的合成缺陷，并通过双路径特征适应器分别处理正常和异常输入，结合轻量级判别器进行细粒度缺陷模式识别。

Result: 在六个基准数据集上评估，RoadFusion在分类和定位任务中表现优异，多项指标达到新最优。

Conclusion: RoadFusion通过合成数据生成和特征适应，显著提升了路面缺陷检测的鲁棒性和性能。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [100] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 论文提出了一种使用小型高保真合成数据集训练模型的方法，无需牺牲准确性且效率更高。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型需要大量参数、数据集和计算资源的问题，同时确保数据来源、使用权限和用户同意的合规性。

Method: 利用合成训练数据，提供高细节和完美标签，并通过程序化数据合成控制数据多样性。

Result: 在深度估计、表面法线估计和软前景分割三个任务上，模型表现出高准确性，且训练和推理成本显著降低。

Conclusion: 合成数据集和训练模型在保持高准确性的同时，显著降低了资源需求，并解决了数据公平性问题。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [101] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet提出了一种针对遮挡条件下面部表情识别（FER）的新方法，通过多模态语义引导、多尺度交互模块和动态对抗排斥损失，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有FER模型在面部部分遮挡时难以提取有效特征，导致分类不准确。

Method: 引入多模态语义引导（语义分割图和面部关键点）、多尺度交互模块（MCM）和动态对抗排斥损失（DARELoss）。

Result: 在公开基准和自建数据集Occlu-FER上达到SOTA性能。

Conclusion: ORSANet通过多模态融合和动态损失设计，有效解决了遮挡条件下的FER问题。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [102] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX是一个基于概念的解释框架，旨在提高手术阶段识别模型的可解释性，通过将神经元与相关概念关联来实现。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在手术阶段识别中缺乏可解释性，阻碍了模型的信任和调试。

Method: 提出SurgX框架，包括选择代表性示例序列、构建手术视频数据集的概念集、关联神经元与概念，并识别关键神经元。

Result: 在两个手术阶段识别模型上的实验验证了方法的有效性，并分析了预测解释。

Conclusion: SurgX展示了在解释手术阶段识别方面的潜力，代码已开源。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [103] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出了一种名为EgoPrune的无训练令牌剪枝方法，专为自我运动视频推理设计，显著降低了计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 自我运动视频是具身AI代理的主要视觉输入，现有方法未能充分利用其时空连续性，计算成本高。

Method: EgoPrune包含三个组件：关键帧选择器、视角感知冗余过滤器和基于MMR的令牌选择器。

Result: 在两个基准测试中表现优于现有方法，显著减少FLOPs、内存使用和延迟，并在边缘设备上验证了实用性。

Conclusion: EgoPrune是一种高效且适用于实际部署的自我运动视频推理方法。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [104] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda是一种简单有效的微调方法，通过动态校准融合表示来提升视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视融合表示在决策中的关键作用，RAda旨在填补这一空白。

Method: 使用轻量级注意力层生成学习掩码，动态调整融合表示中每个元素的贡献。

Result: 在不同设置下，RAda表现优异，性能接近当前最佳方法。

Conclusion: RAda是一种通用且高效的微调技术，适用于多种场景。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [105] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 论文探讨了在复杂森林环境中通过众包搜索生成的数据集，用于改进异常检测方法，支持搜捕和救援行动。


<details>
  <summary>Details</summary>
Motivation: 在德国农村发生一起家庭谋杀案后，传统搜索方法因植被密集而失效，需要更有效的异常检测方法。

Method: 利用研究飞机拍摄高分辨率航拍图像，并通过众包搜索生成标注数据集，开发交互式网络界面。

Result: 现有方法在初始基准测试中表现不佳，表明需要上下文感知的异常检测方法。

Conclusion: 该数据集为复杂森林环境中的异常检测提供了基准，并支持动态增长和用户参与。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [106] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种新颖的LiDAR-视觉里程计框架，结合LiDAR点云和图像，通过深度补全和多尺度特征提取实现高精度和鲁棒的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 自主系统的自定位和导航需要高精度的里程计，现有方法在动态环境和遮挡区域表现不佳。

Method: 利用深度补全生成稠密深度图，结合多尺度特征提取网络和注意力机制，通过分层姿态优化模块逐步优化运动估计。

Result: 在KITTI里程计基准测试中，方法表现优于或与现有视觉和LiDAR里程计方法相当。

Conclusion: 提出的LiDAR-视觉融合框架在精度和鲁棒性上优于现有方法，适用于动态环境和遮挡区域。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [107] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR是一个基于不确定性最小化的交互式文本到视频检索框架，通过量化文本模糊性、映射不确定性和帧不确定性，生成针对性澄清问题，显著提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有交互式文本到视频检索方法依赖启发式策略，未明确量化不确定性，限制了其有效性。

Method: 提出UMIVR框架，使用语义熵（TAS）、Jensen-Shannon散度（MUS）和时序质量采样器（TQFS）量化不确定性，并生成澄清问题。

Result: 在MSR-VTT-1k数据集上，经过10轮交互后Recall@1达到69.2%。

Conclusion: UMIVR为交互式文本到视频检索建立了不确定性最小化的基础，显著提升了检索性能。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [108] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer是一种基于Transformer的低光增强方法，通过动态积分图像表示和光照引导的多头自注意力机制，有效解决了非均匀光照场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在非均匀光照场景（如背光和阴影）中表现不佳，存在过曝或亮度恢复不足的问题。

Method: 提出动态积分图像表示建模空间变化光照，构建SAI2E估计器，并引入光照引导的多头自注意力机制（IG-MSA）。

Result: 在五个标准低光数据集和跨域基准（LOL-Blur）上，SAIGFormer在定量和定性指标上均显著优于现有方法。

Conclusion: SAIGFormer在非均匀光照增强中表现优越，并展现出强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [109] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 提出了一种自监督程序学习框架，通过融合Gromov-Wasserstein最优传输和结构先验来解决视频关键步骤发现与排序问题，并通过对比正则化避免退化解。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理顺序变化、背景/冗余帧和重复动作时表现不佳，需要改进。

Method: 结合Gromov-Wasserstein最优传输和结构先验进行帧间映射，并引入对比正则化避免退化解。

Result: 在EgoProceL、ProceL和CrossTask基准测试中表现优于现有方法。

Conclusion: 提出的框架有效解决了自监督程序学习中的关键问题，性能显著提升。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [110] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 论文提出了一种基于图的方法SSG-Com和数据集Endoscapes-SG201，用于更全面地表示手术场景中的工具-动作-目标组合和手部身份，提升了手术场景理解的效果。


<details>
  <summary>Details</summary>
Motivation: 手术场景理解需要全面捕捉工具、解剖结构及其交互的复杂信息，但现有图表示方法未充分探索工具-动作-目标组合和手部身份等关键元素。

Method: 提出了Endoscapes-SG201数据集，标注了工具-动作-目标组合和手部身份，并设计了基于图的SSG-Com方法来学习这些元素。

Result: 实验证明，这些关键元素的整合显著提升了手术场景理解，特别是在关键安全视图评估和动作三元组识别任务中。

Conclusion: 通过引入新的数据集和方法，论文展示了全面表示手术场景关键元素的重要性，为计算机辅助干预系统提供了更有效的工具。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [111] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa提出了一种零样本人-物交互检测方法，通过低秩分解VLM特征适应，提升对未见类别的泛化能力和动作区分能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在区分相同物体涉及的动作或泛化到未见类别时的局限性。

Method: 低秩分解VLM文本特征，生成类共享基特征和可调权重，结合人类-物体标记和LLM动作正则化。

Result: 在HICO-DET上实现了未见动词设置下27.91的mAP，达到新SOTA。

Conclusion: HOLa通过特征分解和权重适应，显著提升了零样本HOI检测的性能。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [112] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Dynamic-Image（DynImg）的创新视频表示方法，通过引入非关键帧作为时间提示，增强快速移动物体的空间特征提取，提升视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法将空间和时间信息分开处理，导致快速移动物体的空间信息难以准确表示，影响时空交互和视频理解。

Method: 引入非关键帧作为时间提示，结合4D视频旋转位置嵌入，突出快速移动物体的空间区域，并保持时空顺序。

Result: 实验表明，DynImg在多个视频理解基准上优于现有方法约2%。

Conclusion: DynImg通过时间提示有效提升了视频理解的准确性，证明了其方法的有效性。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [113] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一个基于GAN的两阶段框架，用于改进传统Mixup在医学图像分类中的不足，通过生成更真实的图像提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统Mixup在医学图像中生成的图像不真实，可能影响学习效果，尤其是在高风险的医疗应用中。

Method: 使用StyleGAN2-ADA生成器，结合Dirichlet和Beta分布生成标签感知的插值图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据显著提升了分类性能，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种无需修改现有训练流程的Mixup替代方案，提供更强的正则化和语义保真度。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [114] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 提出了一种卫星上实时变化检测的端到端框架，解决了数据存储、图像配准和变化检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像变化检测因数据传输和处理延迟无法满足实时或近实时需求。

Method: 采用深度神经网络，包含三个子模块：图像压缩、轻量级配准和高效变化检测模型。

Result: 在低功耗硬件上实现了0.7 Mpixel/s的吞吐量，F1分数表现优异。

Conclusion: 该框架首次在卫星上实现了端到端的变化检测，为实时应用提供了可行方案。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [115] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT是一种基于扩散变换器的皮肤病变分割模型，适用于低成本硬件，结合Rectified Flow提升生成质量并保持快速推理速度，在三个基准数据集上取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变分割对皮肤癌诊断和患者监测至关重要，但现有方法在性能和效率上仍有改进空间。

Method: 提出SegDT模型，基于扩散变换器（DiT）并引入Rectified Flow，优化生成质量和推理速度。

Result: 在三个基准数据集上表现优于现有方法，同时保持快速推理速度。

Conclusion: SegDT为医学图像分析提供了高效、准确的工具，适用于实际医疗应用。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [116] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一个基于大规模人类视频训练的高灵巧性视觉-语言-动作模型（VLA），通过物理指令调优和新颖的数据处理流程，显著提升了复杂操作任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在复杂操作任务中表现不佳，主要依赖合成数据或有限规模的遥操作演示，导致泛化能力差。

Method: 提出物理指令调优范式，结合大规模VLA预训练、物理空间对齐和机器人任务后适应，并引入毫米级重建精度的部分级运动标记化方法。

Result: Being-H0在手部运动生成和指令跟随方面表现出色，并能随模型和数据规模扩展，在真实机器人操作任务中取得显著提升。

Conclusion: 通过利用人类视频数据和物理指令调优，Being-H0显著提升了灵巧性和泛化能力，为机器人操作任务提供了新思路。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种结合SDF和3DGS的混合方法，用于稀疏视图图像的三维重建和新视角渲染，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决SDF方法在细节捕捉和3DGS方法在全局几何一致性上的不足。

Method: 结合SDF的粗几何捕捉和3DGS的渲染能力，通过相互优化提升重建和渲染效果。

Result: 在DTU和MobileBrick数据集上，表面重建和新视角合成效果优于现有方法。

Conclusion: 混合方法有效结合了SDF和3DGS的优势，显著提升了重建和渲染质量。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于圆柱坐标系的隐式表示CylinderPlane，解决了Tri-plane表示中的多面伪影问题，实现了高质量的无伪影360°图像合成。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示因其固有结构问题（如对称区域共享特征导致的多面伪影）限制了360°视图图像的生成能力。

Method: 采用圆柱坐标系设计CylinderPlane表示，通过嵌套圆柱表示多尺度特征，提升复杂几何和高分辨率适应性。

Result: 实验表明，CylinderPlane在合成数据集和真实图像上均优于现有方法。

Conclusion: CylinderPlane解决了Tri-plane的局限性，实现了高质量、多视角一致的360°图像生成，且易于集成到现有神经渲染流程。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了深度神经网络（DNN）在视频分析中效率优化的技术，填补了现有研究主要关注准确性优化的空白。


<details>
  <summary>Details</summary>
Motivation: 视频数据的爆炸式增长对视频分析提出了更高要求，尤其是效率和准确性。尽管DNN已广泛用于确保准确性，但其效率优化仍是一个挑战。

Method: 采用自下而上的方式组织现有方法，涵盖硬件支持、数据处理、操作部署等多方面视角。

Result: 提出了DNN在视频分析中性能优化的框架，并总结了现有工作。

Conclusion: 分析了DNN在视频分析中性能优化的问题和挑战，为未来研究提供了方向。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 论文探讨了在历史音乐手稿中应用主动学习和顺序学习以优化光学音乐识别（OMR）的方法，发现不确定性主动学习效果不佳，需改进方法。


<details>
  <summary>Details</summary>
Motivation: 解决音乐数字化中因标注数据稀缺和历史手稿复杂性导致的光学音乐识别（OMR）限制。

Method: 结合YOLOv8，采用主动学习和顺序学习，选择预测置信度最低的样本进行迭代标注和训练。

Result: 实验表明，仅需少量标注样本即可达到与全监督训练相当的准确率，但不确定性主动学习效果不佳。

Conclusion: 在数据稀缺场景下，需开发更有效的标注方法，不确定性主动学习不适用于当前手稿。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究探讨了彩票假设（LTH）在深度伪造检测中的应用，发现神经网络可以通过高效剪枝保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对信息完整性和社会信任构成挑战，现有检测方法模型庞大且机制不明确，难以在资源有限环境中部署。

Method: 通过MesoNet、CNN-5和ResNet-18架构在OpenForensic和FaceForensics++数据集上实验，应用LTH进行迭代幅度剪枝。

Result: MesoNet在80%稀疏度下保持56.2%准确率（基线为62.6%），仅需3,000参数，且LTH剪枝方法优于一次性剪枝。

Conclusion: LTH剪枝方法能识别关键子网络，保持检测性能，并具有跨数据集的可迁移性，为高效部署深度伪造检测系统提供可能。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为EVA的训练无关方法，通过动态选择中间层来减少多模态大语言模型（MLLMs）中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs在结合视觉和语言理解方面取得进展，但仍存在物体幻觉问题，即模型生成看似合理但实际错误的输出。研究发现先验知识在深层抑制视觉信息，但中间层的机制尚不明确。

Method: 提出EVA方法，动态选择中间层，对比原始输入和纯文本输入的概率分布差异，提取视觉事实知识并修正最终输出。

Result: 实验表明，EVA显著降低了幻觉率，优于基线方法。

Conclusion: EVA是一种简单、模型无关的方法，能有效减少MLLMs中的幻觉问题。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个新的多语言手写文档视觉问答基准，包含1600页手写文档和2400个问答对，旨在解决现有模型在手写文档理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言视觉问答模型在处理多样化手写文档时表现不佳，缺乏真实的多语言手写文档理解基准。

Method: HW-MLVQA提供1600页手写文档和2400个问答对，支持文本、图像及图文结合三种模态的评估，并测试OCR模型性能。

Result: 该基准为多语言手写文档理解提供了严格的评估框架，推动相关技术进步。

Conclusion: HW-MLVQA填补了多语言手写文档理解领域的空白，有望促进该领域的创新和研究。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型知识蒸馏的方法，用于改进图像质量评估（IQA）任务，解决了CLIP模型参数过多和局部失真特征识别不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在IQA任务中参数负担过重和局部失真特征识别能力不足的问题。

Method: 设计质量分级提示模板，微调CLIP，并提出模态自适应知识蒸馏策略。

Result: 在多个IQA数据集上实验表明，该方法显著降低模型复杂度，并优于现有方法。

Conclusion: 该方法展示了在实际部署中的强大潜力。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯散射（3DGS）的双层次视觉重定位框架Hi²-GSLoc，解决了现有方法在精度、计算复杂性和可扩展性上的不足，特别适用于大规模遥感场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重定位方法在精度和计算效率之间存在固有折衷，尤其是在大规模遥感场景中，高海拔变化和领域差距进一步加剧了这些挑战。

Method: 采用3D高斯散射作为场景表示，提出双层次框架Hi²-GSLoc，包含稀疏阶段（基于高斯特定采样和地标检测）和稠密阶段（通过粗到细的匹配迭代优化位姿），并引入分区训练和GPU加速策略。

Result: 在仿真数据、公开数据集和实际飞行实验中，该方法表现出较高的定位精度、召回率和计算效率，并能有效过滤不可靠位姿估计。

Conclusion: Hi²-GSLoc为遥感应用提供了一种高效且可靠的视觉重定位解决方案。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经表示（INR）的无损点云几何压缩方法LINR-PCGC，解决了现有方法对训练数据分布的依赖问题，并显著提升了编码速度和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI点云压缩方法依赖特定训练数据分布，限制了实际应用；INR方法虽解决了分布问题，但仅支持有损压缩且编码时间和解码器尺寸受限。

Method: 设计了点云级编码框架和高效网络初始化策略以减少60%编码时间；提出基于多尺度SparseConv的轻量级网络，实现快速推理和小解码器尺寸。

Result: 实验表明，LINR-PCGC在MVUB数据集上比G-PCC TMC13v23和SparsePCGC分别减少21.21%和21.95%的比特流。

Conclusion: LINR-PCGC首次实现基于INR的无损点云压缩，显著提升了压缩效率和实用性。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS提出了一种基于小波变换的频率正则化方法，通过监督低频子带并自监督高频子带来提升稀疏视图3D高斯泼溅的重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅（3DGS）在重建高质量新视图时容易过拟合训练视图的高频细节，传统傅里叶变换方法参数调优困难且偏向有害高频学习。

Method: DWTGS利用小波空间损失提供额外的空间监督，仅监督低频LL子带，同时自监督高频HH子带的稀疏性。

Result: 实验表明，DWTGS在多个基准测试中优于基于傅里叶的方法，低频策略提高了泛化能力并减少了高频幻觉。

Conclusion: DWTGS通过小波变换的频率正则化方法有效解决了稀疏视图3DGS的高频过拟合问题。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种计算高效的人脸图像质量评估方法，通过教师-学生模型和自训练策略实现轻量化和高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管人脸图像质量评估（FIQA）已有显著进展，但其计算复杂度仍是实际部署的关键问题。本文旨在开发一种计算高效的FIQA方法，便于实际应用。

Method: 采用两阶段方法：1）训练强大的教师模型并通过自训练提升其能力；2）从教师模型中蒸馏出轻量级学生模型。利用伪标签数据增强教师模型并指导学生模型训练。

Result: 学生模型在极低计算开销下达到与教师模型相当的性能，并在ICCV 2025 VQualA FIQA挑战赛中排名第一。

Conclusion: 该方法成功实现了高效且高性能的FIQA，适用于实际部署。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 论文探讨了空间控制图像生成模型的改进，提出了控制令牌预填充作为基线方法，并研究了采样时间增强技术，澄清了适配器方法的动机。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决空间控制图像生成模型中缺乏科学比较的问题，为基于Transformer的系统提供清晰指导。

Method: 在ImageNet上进行受控实验，比较扩散/流模型和自回归模型，提出控制令牌预填充和采样时间增强技术。

Result: 控制令牌预填充表现优异，采样时间增强显著提升控制-生成一致性，适配器方法在有限数据下保持生成质量但一致性较差。

Conclusion: 论文为空间控制图像生成提供了实用见解，澄清了文献中的知识空白，并提出了有效的基线方法和技术改进。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen是一个两阶段框架，通过利用压缩令牌解决长视频生成中的内存和一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成短视频时表现良好，但扩展到长视频时面临内存瓶颈和长期不一致性问题。

Method: 方法分为两阶段：1) To2V模型生成短视频，2) T2To模型生成全局一致的令牌，并通过FIFO-Diffusion策略平滑过渡。

Result: 实验表明，该方法显著提升了长期时间和内容一致性，且计算开销可控。

Conclusion: TokensGen为长视频生成提供了可扩展的模块化解决方案，适用于影视制作和沉浸式模拟。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的方法，通过预测空间自适应双边网格来校正多视角下的光度变化，无需场景特定重新训练，提高了3D高斯泼溅管线的重建质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现代相机管线中的处理（如曝光调整、白平衡等）会导致多视角间的光度不一致，影响新视角合成的质量。现有方法通过联合优化场景表示和每幅图像的外观嵌入来解决，但增加了计算复杂度和训练时间。

Method: 提出了一种基于Transformer的方法，预测空间自适应双边网格来校正光度变化，并将其集成到3D高斯泼溅管线中。

Result: 实验表明，该方法在重建保真度和收敛速度上优于或匹配现有的场景特定优化方法。

Conclusion: 该方法在多视角一致性和跨场景泛化能力上表现优异，同时保持了高效的训练速度。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为HDF的新框架，通过时间-频率分布注意力模块和自适应优化模块，解决了动态面部表情识别中的样本异质性和优化不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多源数据和个体表情变异性导致的样本异质性下性能下降，需要改进。

Method: 设计了时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM），分别增强时间-频率建模和优化平衡。

Result: 在DFEW和FERV39k数据集上，HDF显著提高了识别准确性和鲁棒性，取得了优异的WAR和UAR。

Conclusion: HDF框架在动态面部表情识别中表现出色，具有强泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 论文提出两种基于树结构的语义损失函数，利用标签的层次结构改进医学图像分割，并在稀疏标注下扩展应用。实验表明该方法在两种任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法对所有错误同等惩罚，未能利用标签空间的语义关系，尤其是当标签类别增多且差异细微时。

Method: 提出两种基于树结构的语义损失函数，结合稀疏标注训练方法，利用标签的层次结构优化分割。

Result: 在头部MRI全脑分区和神经外科高光谱成像任务中，方法达到SOTA性能。

Conclusion: 通过利用标签层次结构，提出的语义损失函数显著提升了医学图像分割的准确性和实用性。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 论文提出了一种动态调整低秩适应（LoRA）中固有秩的新方法，用于医学图像分割，通过引入稀疏正则化器自动选择任务适应的秩，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法需要固定秩，难以适应医学图像任务的复杂性和多样性，因此需要一种动态调整秩的方法。

Method: 通过将低秩表示视为奇异值分解，引入l_1稀疏正则化器，并使用近端优化器动态调整秩。

Result: 在少样本微调实验中，该方法显著优于标准LoRA和其他PEFT方法，表现出高效性和鲁棒性。

Conclusion: 动态调整秩的方法在医学图像分割任务中具有显著优势，代码已开源。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 研究了低参数深度神经网络在计算机视觉中的性能，提出了一种减少特征图干扰的方法，并设计了一个名为NoDepth Bottleneck的高效架构。


<details>
  <summary>Details</summary>
Motivation: 探讨低参数深度神经网络中瓶颈架构的性能，特别是特征图干扰问题，以提高小规模网络的扩展性和准确性。

Method: 通过分析不同瓶颈架构，识别减少干扰的关键设计元素，并基于实验提出NoDepth Bottleneck架构。

Result: 在ImageNet数据集上验证了NoDepth Bottleneck的扩展性和准确性，证明了其在小规模网络中的高效性。

Conclusion: 研究为低参数范围的神经网络提供了更高效和可扩展的设计方案，并加深了对计算机视觉中瓶颈架构的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型SEEM生成未标记数据的预测掩码，并通过不确定性校准和自依赖训练策略提升半监督语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务中标注数据稀缺的问题，探索基础分割模型作为标注器的潜力。

Method: 提出ConformalSAM框架，结合SEEM生成掩码和不确定性校准，过滤低置信度标签，并通过自依赖训练策略优化模型。

Result: 在三个标准半监督语义分割基准上，ConformalSAM表现优于现有方法，并可作为插件提升其他方法性能。

Conclusion: ConformalSAM有效利用基础分割模型解决标注稀缺问题，为半监督语义分割提供了新思路。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 论文提出了一种动态注意力重分配（DARA）方法和TrueMICL数据集，以解决多模态大语言模型（MLLMs）在视觉信息利用上的不足，提升真正的多模态上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在多模态上下文学习（MICL）中过度依赖文本模式，忽视视觉线索，导致多模态学习能力受限。这一局限性在不需要视觉理解的任务中被掩盖，亟需解决。

Method: 提出动态注意力重分配（DARA）策略，通过调整视觉和文本令牌的注意力权重，鼓励模型关注视觉内容；并构建TrueMICL数据集，明确要求整合多模态信息。

Result: 实验表明，提出的方法显著提升了多模态上下文学习的真实能力。

Conclusion: DARA和TrueMICL有效解决了MLLMs在视觉信息利用上的不足，推动了多模态上下文学习的实际应用。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中表现出色，优于变分自编码器和生成对抗网络。通过改进扩散后验采样方法，提高了统计稳健性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在多元地下建模和概率反演中的应用，以提升建模能力和计算效率。

Method: 提出对扩散后验采样方法的改进，包括噪声污染似然近似，并测试其在多元地质场景中的性能。

Result: 改进方法显著提高了统计稳健性、后验概率密度采样效率，并降低了计算成本。

Conclusion: 扩散模型在多元地下建模和概率反演中具有高效性和灵活性，适用于多种条件数据。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个评估文本到视频（T2V）生成模型物理常识能力的基准，包含383个精心设计的提示，通过三阶段评估流程间接测试模型的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型在物理常识方面表现不足，常违反因果关系和对象行为等基本直觉，因此需要一种评估方法来填补这一空白。

Method: PhysVidBench采用三阶段评估：基于提示生成物理问题，用视觉语言模型为视频生成字幕，再让语言模型基于字幕回答问题。

Result: 该基准提供了一个结构化、可解释的框架，用于评估生成视频模型的物理常识能力，特别关注工具使用和材料属性等领域。

Conclusion: PhysVidBench填补了T2V评估中物理常识的空白，为未来模型改进提供了明确方向。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种概念驱动的视频对象分割框架（SeC），通过结合视觉语言模型构建高层次对象表示，显著提升了在复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割方法依赖外观匹配，缺乏人类对对象的概念理解能力，导致在视觉变化、遮挡和复杂场景中表现不佳。

Method: SeC框架利用大型视觉语言模型（LVLMs）构建对象的概念先验，结合语义推理和特征匹配，动态调整计算资源。

Result: 在SeCVOS基准测试中，SeC比SAM 2.1提升了11.8个百分点，实现了概念感知视频对象分割的新最佳性能。

Conclusion: SeC通过概念驱动的表示和动态计算调整，显著提升了复杂场景下的视频对象分割能力。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Latent Denoising Tokenizer (l-DeTok)的视觉分词器，通过直接与下游去噪目标对齐，提升生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型共享类似的训练目标（从噪声输入中重建干净信号），但视觉分词器的有效属性尚不明确。

Method: 提出l-DeTok，通过训练分词器从被插值噪声和随机掩码污染的潜在嵌入中重建干净图像。

Result: 在ImageNet 256x256上，l-DeTok在六种代表性生成模型中均优于标准分词器。

Conclusion: 去噪是分词器设计的基本原则，可为未来分词器设计提供新视角。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [142] [Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling](https://arxiv.org/abs/2507.14915)
*Xiaojie Li,Ronghui Li,Shukai Fang,Shuzhao Xie,Xiaoyang Guo,Jiaqing Zhou,Junkun Peng,Zhi Wang*

Main category: cs.MM

TL;DR: SoulDance数据集和SoulNet框架解决了音乐与舞蹈跨模态对齐的挑战，生成高质量、协调的3D舞蹈序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成音乐对齐的全身舞蹈，因缺乏数据集和跨模态对齐的复杂性。

Method: 提出SoulDance数据集和SoulNet框架，包含分层残差向量量化、音乐对齐生成模型和音乐-动作检索模块。

Result: SoulNet显著优于现有方法，生成高质量、音乐协调的舞蹈序列。

Conclusion: SoulNet为音乐对齐舞蹈生成提供了有效解决方案。

Abstract: Well-coordinated, music-aligned holistic dance enhances emotional
expressiveness and audience engagement. However, generating such dances remains
challenging due to the scarcity of holistic 3D dance datasets, the difficulty
of achieving cross-modal alignment between music and dance, and the complexity
of modeling interdependent motion across the body, hands, and face. To address
these challenges, we introduce SoulDance, a high-precision music-dance paired
dataset captured via professional motion capture systems, featuring
meticulously annotated holistic dance movements. Building on this dataset, we
propose SoulNet, a framework designed to generate music-aligned, kinematically
coordinated holistic dance sequences. SoulNet consists of three principal
components: (1) Hierarchical Residual Vector Quantization, which models
complex, fine-grained motion dependencies across the body, hands, and face; (2)
Music-Aligned Generative Model, which composes these hierarchical motion units
into expressive and coordinated holistic dance; (3) Music-Motion Retrieval
Module, a pre-trained cross-modal model that functions as a music-dance
alignment prior, ensuring temporal synchronization and semantic coherence
between generated dance and input music throughout the generation process.
Extensive experiments demonstrate that SoulNet significantly surpasses existing
approaches in generating high-quality, music-coordinated, and well-aligned
holistic 3D dance sequences.

</details>


### [143] [Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval](https://arxiv.org/abs/2507.15491)
*Deyu Zhang,Tingting Long,Jinrui Zhang,Ligeng Chen,Ju Ren,Yaoxue Zhang*

Main category: cs.MM

TL;DR: ProCLIP提出了一种用户中心的文本-视频检索框架，通过动态帧采样和两阶段候选修剪策略，显著提高了效率并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡文本-视频检索的准确性和计算效率方面存在挑战，ProCLIP旨在解决这一问题。

Method: 设计了基于提示的动态帧采样策略和两阶段候选修剪策略，结合轻量级特征提取器和CLIP重排序。

Result: 在基准测试中，ProCLIP实现了75.3%的延迟降低，同时在MSR-VTT数据集上R@1达到49.0。

Conclusion: ProCLIP在效率和准确性上均优于现有方法，适用于边缘端设备。

Abstract: Enabling efficient text-video retrieval on edge-end devices is critical for
real-world applications. Yet, existing methods face a critical challenge in
balancing accuracy and computational efficiency: uniform frame sampling methods
ensure content coverage but incur prohibitive computational costs, while
salient-frame sampling methods reduce overhead but suffer from query-agnostic
frame selection that biases retrieval results. To address this, we propose
ProCLIP, a user-centric framework that achieves state-of-the-art accuracy with
significantly improved efficiency. We design a prompt-aware frame sampling
strategy that dynamically guides lightweight feature extractors using textual
prompts to select semantically relevant frames, overcoming the limitations of
existing salient-frame sampling methods which rely on static, query-agnostic
selection criteria. Moreover, we adopt a two-stage candidate pruning strategy
that combines rapid coarse filtering via a lightweight module with CLIP-powered
fine-grained re-ranking, enhancing retrieval efficiency while preserving
accuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency
reduction versus baselines while maintaining competitive accuracy, i.e.,
R@1=49.0 in MSR-VTT dataset. Code is available at
https://github.com/tiffylong/ProCLIP.

</details>


### [144] [Point Cloud Streaming with Latency-Driven Implicit Adaptation using MoQ](https://arxiv.org/abs/2507.15673)
*Andrew Freeman,Michael Rudolph,Amr Rizk*

Main category: cs.MM

TL;DR: 本文提出了一种基于Media Over QUIC协议的隐式服务器端适应方法，用于点云视频流媒体，以在低延迟和高画质之间实现动态平衡。


<details>
  <summary>Details</summary>
Motivation: 点云视频流媒体因高比特率限制了实时传输的可行性，现有方法依赖客户端显式适应，无法满足动态需求。

Method: 利用Media Over QUIC协议的传输超时特性，根据应用的延迟目标进行隐式服务器端适应。

Result: 实验表明，系统能根据客户端延迟需求动态调整视频质量，低延迟需求的应用获得低画质，反之亦然。

Conclusion: 该方法为点云流媒体提供了一种灵活的服务器端适应策略，优化了延迟与画质的权衡。

Abstract: Point clouds are a promising video representation for next-generation
multimedia experiences in virtual and augmented reality. Point clouds are
notoriously high-bitrate, however, which limits the feasibility of live
streaming systems. Prior methods have adopted traditional HTTP-based protocols
for point cloud streaming, but they rely on explicit client-side adaptation to
maintain low latency under congestion. In this work, we leverage the delivery
timeout feature within the Media Over QUIC protocol to perform implicit
server-side adaptation based on an application's latency target. Through
experimentation with several publisher and network configurations, we
demonstrate that our system unlocks a unique trade-off on a per-client basis:
applications with lower latency requirements will receive lower-quality video,
while applications with more relaxed latency requirements will receive
higher-quality video.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种名为Catalyst正则化的新方法，用于结构化剪枝，确保公平且稳健的剪枝决策。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法（如L1或Group Lasso）在剪枝时存在幅度偏差和决策边界脆弱的问题。

Method: 通过引入辅助催化剂变量，在扩展参数空间中定义新的正则化器，满足剪枝操作保留模型性能的代数条件。

Result: Catalyst剪枝算法在多个数据集和模型上表现优于现有方法，验证了其公平性和稳健性。

Conclusion: Catalyst正则化为结构化剪枝提供了一种理论可靠且实际有效的方法。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [146] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种基于投影空间的新型剪枝策略IPPRO，通过PROscore衡量滤波器剪枝可能性，挑战传统基于幅度的剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于幅度的剪枝方法限制了剪枝决策的灵活性，即使冗余滤波器也可能因幅度较大而保留。

Method: 将滤波器置于投影空间，观察梯度下降运动方向（是否趋向原点）以衡量剪枝可能性，构建PROscore作为重要性评分。

Result: 实验表明，该方法实现了近乎无损的剪枝，减少了性能下降，并在微调后表现优异。

Conclusion: 打破了剪枝中‘大小决定一切’的迷思，从理论和实践上拓展了基于重要性的剪枝方法。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [147] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种通过将语言模型集成到自改进的进化循环中学习程序合成的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在单次尝试中难以解决复杂的程序合成任务，而基于搜索的进化方法受限于生成模型的固定能力。

Method: SOAR结合进化搜索和事后学习，交替使用LLM采样和优化候选解，并通过微调LLM提升搜索能力。

Result: 在ARC-AGI基准测试中，SOAR显著提升了性能，解决了52%的公开测试集问题。

Conclusion: SOAR通过自改进循环和任务间的正向迁移，为程序合成提供了有效的解决方案。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [148] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN是一个用于神经网络模型优化的知识库管道，通过自适应查询任务元数据来改进模型选择。


<details>
  <summary>Details</summary>
Motivation: 传统静态模型选择方法忽略了任务查询与模型架构之间的动态依赖关系，导致匹配不优。

Method: 提出知识编织引擎，将模型优化重构为任务元数据的自适应查询问题，利用图关系知识模式支持细粒度分析。

Result: 在33个数据-任务对中，M-DESIGN在26个案例中提供了最优模型。

Conclusion: M-DESIGN通过动态知识库和自适应查询，显著提升了模型选择的效率和效果。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [149] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 论文研究了潜在空间融合方法在预测抑郁症状中的表现，发现其优于传统融合方法，并强调了多模态数据整合的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型依赖单模态数据或早期融合策略，无法充分捕捉精神数据的复杂性，因此需要更先进的整合技术。

Method: 使用BRIGHTEN临床试验数据，比较了早期融合（随机森林）和潜在空间融合（组合模型）的表现，评估指标为MSE和R2。

Result: 组合模型在所有设置中表现最佳，MSE更低（0.4985 vs. 0.5305），R2更高（0.4695 vs. 0.4356），且泛化能力更强。

Conclusion: 潜在空间融合是多模态心理健康数据预测的强有力方法，未来需关注模型可解释性和个体化预测。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [150] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文提出Predictive Representativity（PR）框架，用于公平性审计，重点关注结果层面的公平性而非数据集组成。通过皮肤病学案例研究，发现AI皮肤癌分类器在深色皮肤人群中表现较差，强调公平性需动态评估。


<details>
  <summary>Details</summary>
Motivation: AI在医疗决策中的应用日益增多，但算法偏见和不公平结果问题突出，尤其是对历史上边缘化群体。研究旨在通过PR框架解决这一问题。

Method: 采用HAM10000数据集和哥伦比亚临床数据集（BOSQUE Test set）训练AI皮肤癌分类器，分析不同皮肤光型的性能差异。

Result: 分类器在深色皮肤人群中表现显著较差，尽管源数据中采样比例均衡。

Conclusion: PR框架为公平性审计提供工具，强调透明度和包容性验证流程的重要性，推动数据驱动医疗中的公平性重新评估。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [151] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究了两层神经网络的训练解，揭示了其隐藏层的平滑激活函数机制，证明了通用逼近性，并丰富了逼近理论。


<details>
  <summary>Details</summary>
Motivation: 理解通过反向传播算法获得的两层神经网络的训练解，特别是隐藏层使用平滑激活函数（如Sigmoid）时的机制。

Method: 采用泰勒级数展开、严格节点偏序、平滑样条实现和平滑连续性限制四种主要原理。

Result: 证明了任意输入维度的通用逼近性，并通过实验验证，揭示了解决方案空间的“黑箱”之谜。

Conclusion: 研究不仅揭示了神经网络解空间的机制，还丰富了逼近理论，为理解神经网络提供了新视角。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [152] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 论文提出了一种多智能体滑雪租赁问题，扩展了经典滑雪租赁困境，考虑了个人和共享成本，并设计了三种竞争比率的优化策略。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体环境下的滑雪租赁问题，解决动态状态下个体与群体决策的挑战。

Method: 定义了三种竞争比率（整体、状态依赖、个体理性），设计了确定性和随机性策略，包括状态感知阈值函数和分布。

Result: 对称策略优于非对称策略，提供了竞争比率上下界，扩展了经典滑雪租赁问题的理论。

Conclusion: 研究为群体决策提供了理论和实践启示，尤其在动态和不确定环境中。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [153] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为FBE的方法，通过约束极端特征来提升OOD检测性能，实验证明其在ImageNet-1k和CIFAR-10上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于距离的OOD检测方法因数据特征分布偏差和极端特征导致性能受限，需改进。

Method: 提出FBE方法，利用数据集的统计特性识别并约束极端特征，扩大分布内外样本的距离。

Result: 在ImageNet-1k和CIFAR-10上实现了最先进的OOD检测性能。

Conclusion: FBE方法简单有效，能显著提升OOD检测能力，理论分析和实验验证了其优越性。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [154] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出了一种新的时间序列异常检测任务Time-RA，将传统判别式任务转化为生成式推理任务，并引入了多模态基准数据集RATs40K。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法通常仅进行二元分类，缺乏详细分类和解释性推理。

Method: 利用大型语言模型（LLMs）将任务转化为生成式推理任务，并开发了包含40,000个样本的多模态数据集RATs40K。

Result: 通过实验验证了当前模型的性能与局限性，强调了监督微调的重要性。

Conclusion: 该任务和数据集为可解释的时间序列异常检测和推理提供了重要进展。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [155] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的激活模式压缩框架，用于高效预测和利用大语言模型中的激活稀疏性，以减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的激活稀疏性为降低计算成本提供了机会，但直接预测神经元级别的激活模式计算开销大。

Method: 通过将相似的激活模式聚类为少量代表性簇，预测簇分配而非单个神经元状态。

Result: 聚类精度达79.34%，困惑度（PPL）最低为12.49，在保持模型质量的同时显著降低计算开销。

Conclusion: 该方法为未来激活模式预测提供了高效基础，有望提升大规模语言模型的推理效率。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [156] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的鲁棒且可解释的波束对齐引擎（BAE），用于毫米波MIMO系统，通过数字孪生和迁移学习减少数据需求，并利用SHAP和DkNN提升透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在6G愿景下，毫米波系统的可解释性和鲁棒性对建立信任和确保性能至关重要。现有深度学习方法面临数据收集开销大、硬件限制、缺乏可解释性和对抗攻击等问题。

Method: 使用宽波束的RSSI测量预测最佳窄波束，减少波束扫描开销；通过数字孪生生成合成数据，结合迁移学习优化模型；利用SHAP和DkNN提升透明度和鲁棒性。

Result: 实验表明，该框架减少70%真实数据需求、62%波束训练开销，异常检测鲁棒性提升8.5倍，接近最优频谱效率。

Conclusion: 提出的框架在减少数据需求、提升透明度和鲁棒性方面表现优异，适用于毫米波MIMO系统。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [157] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦学习框架SSFL-DCSL，通过双重对比损失和软标签解决数据分布差异和标签稀缺问题，提升故障诊断性能。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据且分布不均，标注成本高，数据隐私问题突出。

Method: 结合双重对比损失（局部和全局）和软标签，利用样本加权函数减少伪标签偏差，通过原型共享知识。

Result: 在仅10%标注数据的任务中，准确率比现有方法提升1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据分布差异和标签稀缺问题，同时保护了用户隐私。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [158] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 论文提出B4模型，通过结合价格序列和外部信号，捕捉牛市和熊市动态，提升市场趋势预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场行为复杂，受历史价格和外部叙事（如新闻、社交媒体情绪）影响，投资者偏见使建模困难。本文探索牛市和熊市动态对市场行为的影响。

Method: 提出B4模型，将价格序列和外部信号嵌入共享潜在空间，通过惯性配对和双竞争机制捕捉行为差异和市场异质性。

Result: 实验表明B4在预测市场趋势上表现优异，并提供偏见、行为和市场动态的可解释性。

Conclusion: B4模型有效捕捉市场动态中的偏见和行为不对称，为趋势预测和解释市场行为提供新视角。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [159] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种无需训练的KV缓存优化方法，通过梯形状缓存模式和迭代压缩机制提升LLMs的长距离建模能力。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，LLMs中的KV对数量激增，导致效率瓶颈，需解决长距离建模和连续生成的内存问题。

Method: LaCache采用梯形状KV缓存模式（跨层存储）和基于距离的动态压缩机制。

Result: 实验验证LaCache能有效提升LLMs的长距离能力。

Conclusion: LaCache为LLMs的长距离建模提供了一种高效且无需训练的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [160] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 开发了一种基于深度学习的无障碍设备，用于聋人或听力障碍者，实时定位和识别声源。


<details>
  <summary>Details</summary>
Motivation: 填补当前研究中针对弱势群体的技术空白，利用机器学习提升无障碍设备的性能。

Method: 系统包含三个组件：JerryNet（CNN架构用于声源方向定位）、音频分类（基于CLAP模型）、多模态集成模型（结合音频、视觉和文本数据定位声源）。硬件包括四麦克风阵列和摄像头。

Result: JerryNet方向定位精度91.1%，CLAP模型在自定义和AudioSet数据集上分别达到98.5%和95%准确率，音频-视觉定位模型的cIoU为0.892，AUC为0.658。

Conclusion: 该研究为新一代无障碍设备的发展奠定了基础，具有广阔的应用前景。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [161] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，结合非线性效用聚合和几何感知查询选择，解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，通过更高效的方法建模用户偏好并减少用户交互次数。

Method: 使用Choquet积分建模用户偏好，结合几何感知查询选择和分支定界策略，高效识别决策边界附近的查询。

Result: 在UCI数据集上的实验表明，该方法优于现有方法（如ChoquetRank），以更少的用户交互实现更高的排名准确性。

Conclusion: 该方法有效解决了模式爆炸问题，提高了用户偏好建模的效率和准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [162] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本研究提出了一种基于AI的框架，用于计算绿色氢产量和选址适宜性指数，结合了无监督聚类、监督分类器和SHAP算法，准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 在缺乏直接氢产量数据的情况下，为太阳能丰富的干旱地区提供一种客观、可重复的绿色氢选址方法。

Method: 采用多阶段AI框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，分析气象、地形和时间数据。

Result: 模型预测准确率为98%，显示水源接近度、海拔和季节变化是阿曼绿色氢选址的最关键因素。

Conclusion: 该研究为数据稀缺地区提供了一种可扩展的工具，支持绿色氢基础设施规划和决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [163] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 论文提出了一种多项式时间确定性算法，用于近似计算k-子空间中位数，解决了非凸优化问题，并提供了实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统的k-PCA方法对噪声和异常值敏感，而k-子空间中位数更具鲁棒性，但计算困难。本文旨在解决这一挑战。

Method: 提出了一种多项式时间确定性算法，其近似因子为√d，运行时间与输入规模成多项式关系。

Result: 算法在理论和实验上均表现出色，适用于多种相关问题和实际数据集。

Conclusion: 该技术为k-子空间中位数问题提供了高效解决方案，并有望推广到其他类似问题。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [164] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种新的Pareto最优梯度匹配（POGM）方法，解决梯度方向一致性问题，避免梯度波动并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在梯度匹配中存在梯度波动和高计算开销的问题，需要一种更高效的解决方案。

Method: 利用梯度轨迹作为数据，在元学习器中进行独立训练，最大化梯度内积同时限制梯度偏离经验风险最小化轨迹。

Result: 在DomainBed数据集上表现出竞争力，同时实现计算高效。

Conclusion: POGM方法有效解决了梯度波动和计算开销问题，具有实际应用潜力。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [165] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M是最大的纳米材料-蛋白质相互作用数据集，结合NanoProFormer模型，通过多模态学习显著提升了预测性能，减少了实验依赖。


<details>
  <summary>Details</summary>
Motivation: 纳米材料在医学和环境科学中的应用潜力受限于对蛋白质相互作用的理解，现有模型因数据不足和泛化能力差而进展缓慢。

Method: 提出NanoPro-3M数据集（320万样本，3.7万独特蛋白质），并开发NanoProFormer模型，通过多模态表示学习预测相互作用。

Result: 多模态模型显著优于单模态方法，能处理缺失特征和未知样本，并识别出冠状形成的关键因素。

Conclusion: 该研究为高性能、泛化的纳米材料-蛋白质相互作用预测奠定了基础，加速了体外应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [166] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 该论文提出了在仅观察到配置变化的情况下，高效学习Ising模型结构和参数的算法，解决了之前需要观察所有更新尝试的限制。


<details>
  <summary>Details</summary>
Motivation: 研究Ising模型在更自然的观察模型（仅观察配置变化）下的学习问题，以解决之前强观察模型的局限性。

Method: 提出了一种算法，能够在最大度为d的Ising模型中，以多项式时间恢复依赖图，并在额外时间内恢复参数。

Result: 算法在更弱的观察模型下，性能与i.i.d.设置中的最新成果相当。

Conclusion: 该工作为可逆单站点马尔可夫链提供了更通用的学习框架，扩展了应用范围。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [167] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，通过扩散映射核的线性近似构建，结合了几何直觉与计算效率。


<details>
  <summary>Details</summary>
Motivation: 结合非线性扩散方法的几何优势与线性方法（如PCA）的计算效率和可解释性。

Method: 通过线性近似扩散映射核构建LDM，并在合成和真实数据集上验证其性能。

Result: LDM在具有明显流形结构的数据集上优于PCA，而PCA在噪声或方差主导的场景中更优。

Conclusion: LDM是一种有价值的线性降维技术，具有理论和实际应用的潜力。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [168] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 提出了一种通信和存储高效的联邦分裂学习方法CSE-FSL，通过辅助网络减少客户端与服务器间的通信和存储需求。


<details>
  <summary>Details</summary>
Motivation: 解决联邦分裂学习（FSL）中高通信开销和服务器存储需求大的问题。

Method: 使用辅助网络在客户端本地更新权重，服务器仅维护单一模型，并选择性地传输数据以减少通信量。

Result: 理论分析和实验表明，CSE-FSL在非凸损失函数下收敛，并显著减少通信开销。

Conclusion: CSE-FSL是一种高效的FSL方法，适用于实际联邦学习任务。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [169] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于单轮反馈的多轮强化学习方法（UFO），通过最小化用户反馈提升大模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大模型的多轮问题解决中存在局限性，导致模型无法有效修正答案。

Method: 引入单轮反馈作为观察（UFO），结合奖励结构设计，优化多轮推理。

Result: 实验表明，UFO在保持单轮性能的同时，多轮推理准确率提升14%。

Conclusion: UFO方法有效提升了大模型在多轮问题解决中的反馈响应能力。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [170] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一种基于元学习的动态防御框架，通过实时选择最优聚合规则来应对联邦学习中的模型投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的静态防御方法在对抗自适应攻击或异构数据环境时效果有限，需要一种更灵活、动态的解决方案。

Method: 设计了一个轻量级的上下文赌博代理，根据实时诊断指标动态选择防御规则。

Result: 实验证明，静态规则无法普遍适用，而FedStrategist能在多样场景中学习到更优策略，同时通过风险容忍参数控制安全与性能的权衡。

Conclusion: FedStrategist为构建弹性、智能的去中心化AI系统提供了一种实用且可分析的新方法。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [171] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，用于提升深度伪造检测中的个体公平性，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用带来了显著风险，而现有检测方法在个体公平性方面存在不足，导致特定人群可能受到偏见影响。

Method: 提出了一种可集成到现有深度伪造检测器中的通用框架，以增强个体公平性和泛化能力。

Result: 在多个领先的深度伪造数据集上的实验表明，该方法显著提升了个体公平性，同时保持了强大的检测性能。

Conclusion: 该研究首次揭示了深度伪造检测中个体公平性的关键问题，并提出了有效的解决方案，为未来研究提供了重要参考。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [172] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 该研究开发并验证了四种机器学习模型，用于预测环形几何结构中的临界热通量（CHF），显著优于传统经验模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测临界热通量（CHF）对压水堆和沸水堆的安全分析至关重要。传统经验方法存在局限性，机器学习提供了更精确的预测可能。

Method: 研究使用四种机器学习模型，基于CTF子通道代码，以三种经验模型（Biasi、Bowring、Katto）为基准，利用577个环形几何实验数据点进行训练和测试。

Result: 机器学习模型的平均相对误差低于3.5%，显著优于经验模型的26%以上误差。

Conclusion: 混合机器学习模型在环形几何结构中表现出色，为CHF预测提供了更准确和可靠的方法。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [173] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过共轭梯度近似影响函数过滤噪声数据，以提升语言模型微调性能，实验显示过滤后准确率提升1.5%。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据集通常存在噪声，影响模型性能，因此需要有效方法检测并剔除有害数据。

Method: 使用共轭梯度近似影响函数过滤数据集，并比较梯度相似性与影响函数的效果。

Result: 过滤10%的训练数据后，准确率提升1.5%；梯度相似性在检测有益数据时优于影响函数。

Conclusion: 局部曲率对检测有害数据重要，但对识别有益数据影响较小。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [174] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection是一种新的参数高效微调方法，通过调整解码器块级别的表示而非单个权重矩阵，优于LoRA，并显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 受同伦理论启发，Solo Connection旨在通过可训练的线性变换实现平滑稳定的任务适应，同时解决大型语言模型中长跳跃连接的需求。

Method: Solo Connection在解码器块级别引入长跳跃连接，逐步插值任务特定表示，减少参数并提升适应性。

Result: 在E2E自然语言生成基准测试中优于LoRA，可训练参数减少59%（相比LoRA）和99%以上（相比全微调）。

Conclusion: Solo Connection为大型语言模型的高效微调提供了新思路，尤其在多层架构中表现突出。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [175] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET是一种新型框架，通过增量因果图学习实时检测网络攻击，解决了传统方法在动态数据分布和实时适应中的不足。


<details>
  <summary>Details</summary>
Motivation: 网络攻击对关键基础设施的威胁日益严重，传统检测方法因高数据方差和类别不平衡导致高误报率，且静态因果图方法无法适应实时动态变化。

Method: INCADET框架包含三个模块：早期症状检测、增量因果图学习和因果图分类，利用动态更新因果图和GCN分类系统状态。

Result: 在真实关键基础设施数据集上的实验表明，INCADET在准确性、鲁棒性和适应性上优于静态因果和深度时序基线方法。

Conclusion: INCADET为实时网络攻击检测提供了一种高效、动态且适应性强的解决方案。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [176] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单测试时缩放方法，发现其主要通过限制最大长度实现缩放，而通过追加'Wait'的方式会导致不一致性。与o1类模型不同，简单测试时缩放会限制模型性能上限。


<details>
  <summary>Details</summary>
Motivation: 研究简单测试时缩放方法的有效性，并探讨其与o1类模型在测试时计算缩放行为上的差异。

Method: 分析简单测试时缩放方法，包括限制最大长度和追加'Wait'两种方式，并与o1类模型进行比较。

Result: 限制最大长度是简单测试时缩放的主要方式，而追加'Wait'会导致模型输出不一致。o1类模型通过强化学习自然提升性能，而简单测试时缩放会限制性能上限。

Conclusion: 简单测试时缩放方法仅能模拟缩放行为，而o1类模型通过自然缩放解锁更高性能，目标应是超越原始性能而非仅复制缩放行为。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [177] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 该论文提出了一种结合强化学习和深度学习的方法，通过干预模型解决大规模随机优化问题，并在供应链库存管理中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效应用强化学习解决大规模随机优化问题，特别是在复杂的供应链库存管理中。

Method: 利用预训练的深度学习模型模拟和组合随机过程，并通过约束协调机制预测双重成本。

Result: 该方法在真实数据集上表现出色，通过模块化设计提升了性能。

Conclusion: 通过分解复杂约束为可扩展的深度学习模块，该方法为大规模随机优化问题提供了有效解决方案，并指出了未来研究方向。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [178] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 论文提出ReDiSC模型，通过重参数化掩码扩散模型估计节点标签的联合分布，解决了现有GNN方法中节点标签条件独立的假设问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法假设节点标签条件独立，与图中节点标签实际相关性的直观观察不符。

Method: 提出ReDiSC模型，使用重参数化掩码扩散模型和变分EM框架估计节点标签的联合分布。

Result: ReDiSC在实验中优于或与现有GNN、标签传播和扩散基线方法相当，且能有效扩展到大规模数据集。

Conclusion: ReDiSC在结构化节点分类任务中具有显著优势，解决了计算约束问题并提升了性能。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [179] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 提出了一种联邦强化学习框架FRL-EH，解决环境异质性问题，通过FedRQ算法实现全局策略优化，并在连续状态空间中扩展应用。


<details>
  <summary>Details</summary>
Motivation: 研究联邦强化学习在环境异质性下的挑战，旨在通过协作学习全局策略，同时保护本地轨迹隐私。

Method: 提出FedRQ算法，设计全局目标函数优化策略，并扩展到连续状态空间。

Result: 理论证明FedRQ收敛到最优策略，实验验证其在异质环境中的优越性能。

Conclusion: FRL-EH框架和FedRQ算法在异质环境中表现出高效和鲁棒性，优于现有方法。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [180] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种称为“故障”的新不可靠行为来源，影响具有陡峭决策边界的AI模型可靠性，并提供了形式化定义和算法检测方法。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型的决策可信且可靠，输出在相似输入下保持一致。

Method: 通过形式化定义“故障”，并使用混合整数线性规划（MILP）编码问题，开发算法检测梯度提升决策树（GBDT）模型中的故障。

Result: 证明故障在广泛使用的GBDT模型中普遍存在，且检测问题对深度为4的树集成是NP完全的。

Conclusion: 故障通常指示模型不一致性，算法能有效检测GBDT模型中的故障，提升模型可靠性。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [181] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 论文提出了一种名为GenDD的条件生成框架，用于知识蒸馏（KD），通过Split Tokenization和Distribution Contraction技术解决了高维优化和标签语义监督不足的问题，实验表明其在无监督和监督设置下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）在高维优化和缺乏标签语义监督时面临挑战，需要一种更稳定和高效的方法。

Method: 提出了GenDD框架，结合Split Tokenization策略和Distribution Contraction技术，实现了无监督和监督训练。

Result: 在无监督设置下，GenDD显著优于KL基线（提升16.29%）；在监督设置下，ResNet-50在ImageNet上达到82.28%的top-1准确率。

Conclusion: GenDD框架在知识蒸馏中表现出色，尤其在无监督和监督任务中均取得了显著的性能提升。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [182] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种结构感知的度量函数SDSC，用于时间序列自监督表示学习，解决了传统距离目标（如MSE）的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法（如MSE）对振幅敏感、对波形极性不变且尺度无界，影响了语义对齐和可解释性。SDSC旨在通过量化时间信号的结构一致性来解决这些问题。

Method: SDSC基于Dice相似系数，通过计算有符号振幅的交集来度量结构一致性，并可作为损失函数使用。还提出了一种结合SDSC与MSE的混合损失函数。

Result: 实验表明，SDSC在预测和分类任务中表现优于或与MSE相当，尤其在领域内和低资源场景中。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的有效替代。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [183] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 论文提出了一种利用正样本和无标签样本（PU学习）识别对照组的方法，以解决观察性研究中对照组缺失的问题，并在模拟和真实数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在观察性因果推断中，对照组缺失是一个常见挑战。本文旨在通过PU学习框架，仅利用已知的受处理样本，高置信度地识别潜在对照组。

Method: 采用PU学习方法，基于已知的受处理样本识别潜在对照组，并通过因果图生成合成数据验证方法的可靠性。同时，在可持续农业的真实数据中应用该方法。

Result: 实验表明，PU学习能够成功从无标签数据中识别对照组，并基于此估计接近真实值的平均处理效应（ATE）。

Conclusion: 该方法为观察性因果推断提供了新思路，尤其适用于难以进行随机实验的领域（如农业、环境科学），通过利用现有数据实现准实验设计。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [184] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于最大因果熵逆强化学习的方法，用于无限时域平稳平均场博弈，通过再生核希尔伯特空间建模未知奖励函数，解决了现有方法中奖励函数线性组合的限制。


<details>
  <summary>Details</summary>
Motivation: 现有逆强化学习方法在平均场博弈中通常限制奖励函数为固定基函数的线性组合，无法捕捉非线性结构。本文旨在通过再生核希尔伯特空间建模，直接从专家演示中推断复杂的非线性奖励结构。

Method: 引入拉格朗日松弛将最大因果熵逆强化学习问题转化为无约束对数似然最大化问题，并通过梯度上升算法求解。证明了相关软贝尔曼算子在再生核希尔伯特空间参数下的Fr\'echet可微性。

Result: 在平均场交通路由博弈中，该方法准确恢复了专家行为，验证了其有效性。

Conclusion: 本文方法能够有效推断非线性奖励结构，适用于无限时域平均场博弈，为逆强化学习提供了新的理论框架和实用工具。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [185] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 本文探讨了自注意力机制与更广泛的亲和矩阵计算原则的联系，指出自注意力是无限特征选择（Inf-FS）的特例，并统一了多种机器学习方法的数学基础。


<details>
  <summary>Details</summary>
Motivation: 揭示自注意力机制与亲和矩阵计算原则的共性，统一不同领域的机器学习方法。

Method: 通过分析自注意力与Inf-FS的相似性，比较亲和矩阵的定义和应用方式。

Result: 自注意力是Inf-FS的单跳特例，两者共享基于成对关系的计算结构。

Conclusion: 自注意力是亲和矩阵计算的一种特殊形式，为机器学习提供了统一的数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [186] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN是一种高效、低成本的GNN框架，能在单GPU上处理1000亿规模的图数据，并在用户获取场景中提升13.8%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在效率和准确性之间难以平衡，且面临计算和内存需求高的挑战。

Method: 提出LPS-GNN框架，设计LPMetis图分区算法和子图增强策略。

Result: 在公开和实际数据集上，性能提升8.24%至13.89%，优于现有SOTA模型。

Conclusion: LPS-GNN在效率和性能上表现优异，已成功应用于Tencent平台。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [187] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer-GAN和MILET的新框架，用于无人机飞行状态分类，显著提升准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类方法在动态无人机环境中缺乏鲁棒性和泛化能力，而现有SOTA模型需要大数据集和高计算成本。

Method: 集成Transformer编码器捕捉长期依赖关系，GAN模块生成合成数据增强数据集，MILET聚焦关键输入段以减少噪声和计算开销。

Result: 在DroneDetect和DroneRF数据集上分别达到96.5%和98.6%的准确率，优于其他SOTA方法。

Conclusion: 该框架在计算效率和泛化能力上表现优异，适用于资源受限环境中的实时部署。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [188] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 论文提出Rec-AD框架，结合Tensor Train分解和深度学习推荐模型，提升智能电网中虚假数据注入攻击检测的计算效率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中虚假数据注入攻击检测的计算和内存负担随系统规模和数据维度增加而显著增加，限制了检测效率。

Method: Rec-AD通过嵌入压缩、索引重排序优化数据访问，以及流水线训练机制减少内存通信开销，兼容PyTorch。

Result: 实验表明Rec-AD显著提升计算吞吐量和实时检测性能，缩小攻击窗口并增加攻击成本。

Conclusion: Rec-AD增强了边缘计算能力和可扩展性，为智能电网安全提供技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [189] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为AD-GCL的新型图对比学习框架，旨在解决现有GCL方法在结构不平衡网络中对尾部异常检测的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GCL方法在异常检测中过度关注整体性能，而忽视了对结构不平衡（如幂律分布网络）的鲁棒性，尤其是对低度尾部异常的检测不足。

Method: AD-GCL通过邻居修剪策略过滤噪声边，并通过异常引导的邻居补全扩大尾部节点的感知范围，同时引入原始图与增强图的一致性损失。

Result: 在多个数据集上，AD-GCL在整体、头部和尾部节点的异常检测中均表现出全面优势。

Conclusion: AD-GCL显著提升了图异常检测在结构不平衡网络中的鲁棒性和适用性。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [190] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 提出了一种名为GCC-Spam的新型垃圾文本检测框架，通过字符相似性网络、对比学习和GAN生成伪样本，解决了对抗性攻击和标注数据稀缺问题，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾文本的指数增长带来了信息泄露和社会不稳定等风险，需要有效的检测机制。

Method: 结合字符相似性网络捕捉拼写和语音特征，对比学习优化潜在空间距离，GAN生成伪样本缓解数据稀缺。

Result: 在真实数据集上实验表明，GCC-Spam检测率更高，且需要更少的标注数据。

Conclusion: GCC-Spam框架有效解决了垃圾文本检测中的关键挑战，提升了检测性能和鲁棒性。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [191] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 提出了一种结合空间-时间变换器和课程学习的新框架SST-CL，用于EEG情感识别，解决了非平稳空间-时间神经模式整合和动态情感强度变化的挑战。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在脑机通信系统中至关重要，但面临非平稳空间-时间神经模式整合和动态情感强度变化的挑战。

Method: SST-CL框架包含空间编码器（建模通道间关系）和时间编码器（通过窗口注意力机制捕获多尺度依赖），并结合强度感知课程学习策略。

Result: 在三个基准数据集上表现出最先进的性能，消融研究验证了框架各组件和课程学习机制的必要性。

Conclusion: SST-CL框架有效解决了EEG情感识别的关键挑战，为实际应用提供了鲁棒性。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [192] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 论文提出了一种名为CPAC的新方法，通过原型注意力机制和VAE-GAN结合，改善了欺诈检测中的类别不平衡问题，并提升了潜在空间结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GANs、VAEs）在生成少数类样本时容易导致分类器过度自信和潜在聚类分离不佳，限制了实际检测性能。

Method: 提出CPAC架构，结合原型注意力机制和VAE-GAN，优化潜在空间结构，并与传统过采样方法（如SMOTE）和生成模型进行比较。

Result: CPAC在F1-score（93.14%）和召回率（90.18%）上表现优异，同时改善了潜在聚类分离。

Conclusion: CPAC通过分类器引导的潜在空间优化，显著提升了欺诈检测性能，并提供了对表示学习的深入见解。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [193] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 论文研究了实时生成AI（RTGen）工作负载在异构SoC上的调度问题，评估了不同调度策略对性能的影响，并强调了动态异构调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成AI模型（如LLMs）在实时多模型应用（如视频会议和游戏）中的集成，RTGen工作负载的需求增加，但其在异构SoC上的调度复杂性和性能影响尚未充分研究。

Method: 在AMD Ryzen AI异构SoC上，构建了多模型场景，分析了不同后端上的模型性能，并评估了五种调度策略对实时指标和LLM性能的影响。

Result: 调度决策显著影响性能（如平均41.7%的截止时间违规率差异），表明需要动态异构调度策略以适应工作负载和硬件特性。

Conclusion: 动态异构调度是实现高性能RTGen应用的关键，需结合工作负载动态和硬件异构性。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [194] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 论文介绍了LeanTree，一种白盒工具，用于自动化定理证明，通过分解复杂证明状态为简单分支，并生成数据集，优于黑盒方法。


<details>
  <summary>Details</summary>
Motivation: 自动化定理证明（ATP）因状态和动作空间庞大而具有挑战性，现有的大语言模型（LLMs）缺乏正确性保证，白盒方法发展滞后。

Method: 提出LeanTree，包括基于Lean 4的工具和分解后的中间状态数据集，支持并行搜索和状态重用。

Result: 初步结果表明，白盒方法在某些场景下优于黑盒方法。

Conclusion: 白盒方法在简化评估、丰富训练数据等方面具有优势，为ATP提供了新方向。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [195] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID是一个统一的框架，解决了基于提示的持续学习中的潜在遗忘和提示内存爆炸问题，通过任务感知解码和梯度提示选择策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设任务感知推理并维护任务特定提示列表，限制了可扩展性并隐藏了潜在遗忘。

Method: GRID结合任务感知解码机制（利用代表性输入、自动任务识别和约束解码）和梯度提示选择策略（压缩低信息量提示）。

Result: GRID在短序列、长序列和负迁移基准测试中显著提升后向迁移，减少遗忘任务达80%，优于现有方法。

Conclusion: GRID通过统一框架解决了持续学习中的关键问题，展示了高效和可扩展的终身学习能力。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [196] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 论文研究了可训练有理激活函数在强化学习和持续学习中的表现，发现其灵活性与稳定性之间存在权衡，并提出了一种约束变体以改善稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索可训练有理激活函数在动态环境中的表现，尤其是其灵活性与训练稳定性之间的关系。

Method: 提出了一种约束变体，限制输出缩放以保持适应性，并在MetaWorld和DMC环境中进行了实验。

Result: 约束变体提高了训练稳定性和性能，但在离散动作领域（如Atari）未观察到类似不稳定性。

Conclusion: 研究为动态环境中设计稳健且适应性强的可训练激活函数提供了实用原则。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [197] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA算法通过EKFAC预处理改进Neumann级数迭代，高效近似iHVP，显著提升训练数据归因（TDA）性能。


<details>
  <summary>Details</summary>
Motivation: 梯度基TDA方法（如影响函数和展开微分）需要高效近似iHVP，但现有方法难以实现。

Method: 提出ASTRA算法，利用EKFAC预处理改进Neumann级数迭代，优化iHVP近似。

Result: ASTRA比传统方法更易调参、迭代次数更少且精度更高，显著提升TDA性能。

Conclusion: ASTRA通过改进iHVP近似，为TDA提供了高效且准确的解决方案。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [198] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，通过聚合Rashomon集合中多个近优模型的PDP，生成解释不确定性感知的特征效应视图，提升了模型解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化机器学习系统通常只关注单一最优模型，忽略了解释不确定性，这在以人为中心的可解释AI中至关重要。

Method: 提出Rashomon PDP框架，通过聚合Rashomon集合中多个模型的PDP，捕捉解释变异性并突出分歧区域。

Result: 实验表明，Rashomon PDP在大多数情况下仅覆盖最佳模型PDP的不到70%，凸显单一模型解释的局限性。

Conclusion: Rashomon PDP通过补充额外信息，提高了模型解释的可靠性和可信度，尤其适用于高风险领域。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [199] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 本文介绍了两种高斯过程（GP）的后验采样方法——随机傅里叶特征和路径条件采样，并展示了它们在全局敏感性分析（GSA）和多目标优化中的应用。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真和物理实验成本高昂，限制了其在全局敏感性分析和优化中的应用，因此需要高效的代理模型（如高斯过程）来支持不确定性下的决策。

Method: 提出了随机傅里叶特征和路径条件采样两种方法，用于生成高斯过程的后验样本，并详细描述了其实现和应用。

Result: 通过数值实验验证了这些采样方法在GSA和优化任务中的有效性。

Conclusion: 高斯过程采样方法为工程优化提供了高效的工具，支持不确定性下的决策。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [200] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 论文探讨了方向性作为人工神经网络的归纳偏置的作用，通过剪枝技术在权重绑定的循环神经网络中诱导方向性，证明其并非学习必需但可能是有益的偏置。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环电路的普遍性启发，研究方向性是否对人工神经网络有帮助。

Method: 形式化一个全连接的感知器层（数学上等价于权重绑定的循环神经网络），并通过剪枝技术诱导方向性。

Result: 剪枝方案成功在不影响性能的情况下增加了神经元间信息流的拓扑顺序。

Conclusion: 方向性并非学习必需，但可能是梯度下降和稀疏化可发现的有利归纳偏置。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [201] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 论文研究了自监督特征学习和预训练方法在强化学习（RL）中的应用，特别是基于互信息技能学习（MISL）的方法。通过分析对比后继特征（CSF）方法，证明了其能恢复环境的真实特征，并探讨了互信息目标和熵正则化的影响。


<details>
  <summary>Details</summary>
Motivation: 理解MISL中表示和互信息参数化的理论作用，填补现有研究的空白。

Method: 通过对比后继特征（CSF）方法，分析其如何恢复环境的真实特征，并验证其理论保证。

Result: CSF能恢复环境的真实特征（线性变换内），并揭示了互信息目标和熵正则化的优缺点。

Conclusion: 研究为RL中的表示学习提供了首个可识别性保证，并验证了CSF在状态和像素数据中的有效性。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [202] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一种多模态框架，结合稀疏时间点的胸片和临床数据，预测ICU患者的胸片异常，提前12小时提供高准确性预测。


<details>
  <summary>Details</summary>
Motivation: ICU患者胸片获取不规律，现有工具无法捕捉时间动态，限制了胸片的诊断价值。

Method: 通过视觉编码器生成潜在嵌入，并与高频临床数据对齐，使用Transformer模型预测未来胸片嵌入。

Result: 在2万名ICU患者回顾性研究中，CXR-TFT能提前12小时高准确预测胸片异常。

Conclusion: CXR-TFT通过时间分辨率提升胸片分析，为临床决策提供早期干预依据，改善患者预后。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [203] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 本文探讨了LLMs中的记忆化问题，分析了其与隐私威胁的关系，并提出了一种新的上下文记忆化方法。实验表明，不同记忆化测量方法结果不一致，且最优学习无法完全避免记忆化。


<details>
  <summary>Details</summary>
Motivation: 研究记忆化是否可以通过最优学习避免，以及记忆化对隐私的威胁是否被夸大。

Method: 重新审视现有记忆化测量方法（基于回忆和反事实记忆化），并提出了上下文记忆化方法。通过实验验证了不同方法的差异。

Result: 实验表明：(a) 不同记忆化测量方法结果不一致，(b) 最优学习无法完全避免记忆化，(c) 改进学习会减少上下文和反事实记忆化但增加基于回忆的记忆化，(d) 部分基于回忆的记忆化字符串并不构成隐私威胁。

Conclusion: 记忆化是LLMs学习过程中不可避免的部分，但其隐私威胁可能被高估。上下文记忆化提供了一种更准确的测量方法。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [204] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think是一个统一的强化学习框架，通过结合规则奖励和生成偏好信号，提升LLM在多样化任务中的性能。课程学习策略显著优于联合训练和模型合并。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在泛化性上表现不佳，倾向于记忆而非可迁移学习，因此需要一种更有效的后训练方法。

Method: 提出Omni-Think框架，结合规则奖励和LLM-as-a-Judge生成的偏好信号，采用课程学习策略从结构化任务逐步过渡到开放式任务。

Result: 实验表明，课程学习在四个领域中性能提升5.2%（相比联合训练）和9.1%（相比模型合并）。

Conclusion: 任务感知采样和混合监督对扩展基于RL的后训练方法至关重要，Omni-Think为通用LLM提供了有效的优化框架。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [205] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 论文探讨了利用大语言模型（LLM）对金融知识图谱中的局部子图进行推理，以评估洗钱行为的可疑性。


<details>
  <summary>Details</summary>
Motivation: 洗钱行为的复杂性和实体间的互联性需要基于图数据的推理能力。

Method: 提出了一种轻量级流程，提取感兴趣实体的k跳邻域，将其序列化为结构化文本，并通过少量示例的上下文学习提示LLM进行评估和解释。

Result: 实验表明，LLM能模拟分析师逻辑，识别可疑行为并提供合理解释。

Conclusion: 研究展示了LLM在图推理中的潜力，为可解释的语言驱动金融犯罪分析奠定了基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [206] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 论文提出了一种将等变性网络理论扩展到时间参数化序列变换的方法，解决了传统RNN在动态变换中的局限性，并展示了其在训练速度和泛化能力上的优势。


<details>
  <summary>Details</summary>
Motivation: 数据以连续流的形式到达感官，平滑变换定义了环境中的连续对称性。传统等变性网络仅适用于静态变换和前馈网络，限制了其在序列模型中的应用。

Method: 扩展等变性网络理论到时间参数化的序列变换（“流”），提出改进的RNN模型以实现流等变性。

Result: 改进的模型在训练速度、长度泛化和速度泛化方面显著优于非等变性模型。

Conclusion: 这是构建尊重时间参数化对称性的序列模型的第一步。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [207] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现语言模型可以通过语义无关的数据传递行为特征，称为“潜意识学习”。即使数据过滤了相关特征，学生模型仍能从教师模型生成的数据中学习到这些特征。这种现象在特定条件下普遍存在，可能对AI开发带来潜在风险。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否能够通过看似无关的数据传递行为特征，以及这种现象的普遍性和潜在影响。

Method: 通过教师模型生成仅包含数字序列的数据集，训练学生模型，并观察其是否学习到教师模型的行为特征。实验还扩展到代码和推理痕迹数据。

Result: 学生模型确实从语义无关的数据中学习到了教师模型的行为特征，即使数据被过滤。这种现象在特定条件下普遍存在。

Conclusion: 潜意识学习是一种普遍现象，可能通过蒸馏等方法传播意外特征，对AI开发构成潜在风险。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [208] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 该研究评估了基础模型在电子健康记录（EHRs）处理中的性能、公平性和可解释性，并开发了标准化数据处理流程。


<details>
  <summary>Details</summary>
Motivation: 支持开发有效且可信赖的多模态人工智能系统，以应对临床应用的多样性需求。

Method: 使用MIMIC-IV数据库，系统比较了八种基础模型（单模态和多模态），并开发了标准化数据处理流程。

Result: 多模态数据的整合显著提升了预测性能，且未引入额外偏差。

Conclusion: 该基准测试为开发适用于真实临床场景的多模态AI系统提供了支持。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [209] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 研究了在对比学习框架中引入自适应边界（eMargin）对时间序列表示学习的影响，发现其在无监督聚类指标上表现优异，但在下游分类任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索自适应边界是否能改善时间序列表示学习中相邻但不相似时间步的分离，从而提升下游任务性能。

Method: 在对比损失函数中引入自适应边界（eMargin），基于预定义相似度阈值调整，并在三个基准数据集上评估其对聚类和分类的影响。

Result: eMargin在无监督聚类指标上优于基线，但在下游分类任务中表现不佳。

Conclusion: 无监督聚类指标的高分并不一定意味着嵌入在下游任务中有效，需进一步优化自适应边界的设计。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [210] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR增强AI逻辑任务能力，但可能受限于基础模型支持，无法扩展推理边界，反而可能限制原创解决方案的发现。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是否能真正扩展模型推理边界，还是仅放大已知高奖励输出以提高精度。

Method: 理论与实证研究，分析RLVR的约束条件与熵-奖励权衡。

Result: RLVR提高pass@1，但支持范围缩小，可能忽略正确但低概率解。

Conclusion: RLVR在扩展推理能力上存在潜在限制，未来需创新算法如显式探索机制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [211] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR是一个基于Transformer的框架，通过时间感知注意力机制和预训练语言模型嵌入，解决了EHR数据异质性和复杂时间模式的问题，显著提升了疾病进展预测的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的临床信息，但数据异质性和复杂时间模式使其建模具有挑战性。传统方法难以处理临床事件间的不规则时间间隔。

Method: 提出TALE-EHR框架，结合时间感知注意力机制和预训练语言模型（LLM）生成的嵌入，以捕捉细粒度时间动态和语义信息。

Result: 在MIMIC-IV和PIC数据集上的实验表明，TALE-EHR在疾病进展预测等任务上优于现有基线方法。

Conclusion: TALE-EHR通过结合显式连续时间建模和强语义表示，为EHR分析提供了有效的解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [212] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种基于控制屏障函数（CBFs）的分层多智能体强化学习（HMARL）方法，用于安全关键的多智能体自主系统中安全策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体安全关键系统中，每个智能体在满足安全要求的同时与其他智能体协作完成任务的问题。

Method: 采用分层方法，将强化学习问题分解为高层学习联合协作行为，低层学习基于高层策略的安全个体行为，并提出了基于技能的HMARL-CBF算法。

Result: 在复杂环境中验证，显著提高了安全性（接近完美的成功率/安全率），并在所有环境中提升了性能。

Conclusion: HMARL-CBF方法在多智能体安全关键系统中表现出色，显著优于现有方法。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [213] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: Graph Tsetlin Machine (GraphTM) 是一种基于图结构输入的机器学习方法，通过消息传递构建深度子句，提升解释性和数据利用率，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统 Tsetlin Machine (TM) 在处理图结构数据时存在局限性，GraphTM 旨在扩展其能力，支持序列、网格、关系和多模态输入，同时保持解释性和高效性。

Method: GraphTM 通过消息传递构建嵌套深度子句，识别子图模式，显著减少子句数量，提升效率和解释性。

Result: 在图像分类、动作共指追踪、推荐系统和病毒基因组序列分析中，GraphTM 表现优于卷积 TM 和其他强化学习方法，尤其在噪声容忍度和训练速度上优势明显。

Conclusion: GraphTM 通过结合图表示学习和深度子句，为 TM 学习开辟了新方向，展示了在多领域的潜力。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [214] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 提出了一种增强的重要性度量框架，用于结构化剪枝，以在压缩模型时保持应用特定性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统重要性度量在结构化剪枝中无法保持应用特定性能的问题。

Method: 采用多种策略确定每组的最优剪枝幅度，平衡压缩与任务性能。

Result: 在MNIST图像重建任务中，该方法有效保留了任务相关性能，即使大幅剪枝后仍保持模型可用性。

Conclusion: 提出的方法在模型压缩中成功平衡了剪枝与应用特定性能需求。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [215] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习中模型透明度不足的问题，提出将经典不确定性量化方法映射到量子领域，以增强模型的不确定性感知。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和量子机器学习中模型透明度的缺失导致过拟合和预测过度自信等问题，目前量子机器学习领域对此研究不足。

Method: 基于经典不确定性量化和量子贝叶斯建模的初步探索，理论开发和实证评估将经典方法映射到量子机器学习的技术。

Result: 研究发现，在设计新的量子机器学习模型时，需要借鉴经典不确定性量化的见解以提高不确定性感知。

Conclusion: 论文强调利用经典不确定性量化方法对量子机器学习模型设计的重要性。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [216] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM方法通过动态调整动量解决联邦学习中长尾数据分布导致的收敛问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（非IID）数据，尤其是长尾数据分布下，面临模型偏差和收敛困难的问题。

Method: 提出FedWCM方法，通过全局和每轮数据的动态动量调整，纠正长尾分布带来的方向偏差。

Result: 实验表明，FedWCM解决了非收敛问题，并在处理客户端异构性和数据不平衡时优于现有方法。

Conclusion: FedWCM显著提升了联邦学习在长尾数据分布下的效率和效果。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [217] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedClusAvg的隐私保护联邦学习框架，用于在非独立同分布（Non-IID）和资源受限的环境中检测虚假数据注入攻击（FDIAs）。该框架通过聚类分层采样和分层通信（客户端-子服务器-服务器）提高了模型泛化能力并减少了通信开销。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击（FDIAs）对智能电网构成严重威胁，传统集中式训练方法存在隐私风险、数据共享限制和高传输成本等问题，限制了其扩展性和部署可行性。

Method: 提出FedClusAvg框架，结合聚类分层采样和分层通信机制，支持本地化训练和加权参数聚合，避免集中敏感数据。

Result: 实验结果表明，FedClusAvg在异构数据分布下提高了检测精度，同时显著减少了通信轮次和带宽消耗。

Conclusion: FedClusAvg为大规模分布式电力系统中安全高效的FDIA检测提供了有效解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [218] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBAD的新型Transformer模型，用于检测互联网平台上的不良行为者，并通过局部和全局信息捕捉以及对抗性训练增强模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在检测不良行为者时对输入序列的微小变化敏感，缺乏对抗攻击的鲁棒性。

Method: ROBAD结合Transformer编码器和解码器块，分别捕捉帖子级局部信息和序列级全局信息，并通过对比学习增强分类层。

Result: 在Yelp和Wikipedia数据集上的实验表明，ROBAD能有效抵御最先进的对抗攻击。

Conclusion: ROBAD通过局部-全局信息捕捉和对抗性训练，显著提升了模型在对抗环境下的检测能力。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [219] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 本文探讨了通过强化学习训练流匹配策略以超越原始演示策略性能的方法，提出了两种方案：RWFM和GRPO，并在模拟任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过强化学习提升流匹配策略的性能，以超越由次优策略（如人类操作员）生成的训练演示。

Method: 提出了两种方法：奖励加权流匹配（RWFM）和基于学习奖励替代的组相对策略优化（GRPO），并在模拟单轮动力学任务中进行了测试。

Result: 两种方法均显著优于次优演示策略，其中GRPO方法的成本比简单的模仿学习流匹配（ILFM）方法低50%至85%。

Conclusion: 强化学习可以显著提升流匹配策略的性能，GRPO方法尤其有效。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [220] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 提出了一种名为iQRA的新方法，通过引入随机顺序约束改进点预测集合的概率预测，显著提升了德国日前电力市场价格预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 电力市场价格预测对市场参与者至关重要，但现有机器学习模型缺乏不确定性估计，限制了决策者规避风险的能力。

Method: 基于分位数回归平均（QRA）框架，引入随机顺序约束，提出iQRA方法，改进预测准确性、可靠性和计算成本。

Result: 在德国日前电力市场的广泛预测研究中，iQRA在可靠性和锐度上均优于现有后处理方法，并能生成跨多个置信水平的校准良好的预测区间。

Conclusion: iQRA通过等渗正则化降低了分位数回归问题的复杂性，提供了一种无需超参数选择的变量选择方法，显著提升了预测性能。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [221] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一种新的鲁棒控制理论扩展，通过零和动态博弈处理价值函数梯度的不确定性，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习等应用中价值函数梯度近似带来的不确定性问题。

Method: 提出GU-HJBI方程，证明其适定性，并通过线性二次案例分析和数值研究验证。

Result: 经典二次价值函数假设在梯度不确定性下失效，提出了GURAC算法并验证其稳定性。

Conclusion: 为鲁棒控制提供了新方向，对强化学习和计算金融等领域有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [222] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: 论文提出AnalogFed，一种支持去中心化协作的生成AI框架，用于模拟电路拓扑发现，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计数据具有专有性，限制了生成AI的研究进展，需要一种不共享原始数据的协作方法。

Method: 提出AnalogFed框架，结合联邦学习和隐私保护技术，处理数据异构性并开发生成模型。

Result: 实验表明AnalogFed在性能上与集中式基线相当，同时确保数据隐私，生成模型在拓扑设计上具有高效性和可扩展性。

Conclusion: AnalogFed为模拟电路设计提供了一种安全、高效的协作生成AI解决方案。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [223] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 论文提出了一种分布级遗忘框架，通过移除最小数据集点来消除不需要的分布信号，同时保留目标分布，显著减少删除量。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘工具主要针对单个样本，难以彻底移除整个分布信号，无法满足隐私、法律或质量要求。

Method: 使用Kullback-Leibler散度量化移除和保留效果，提出基于距离的选择规则，减少删除量。

Result: 实验表明，该方法比随机删除减少15-72%的删除量，且对保留性能影响可忽略。

Conclusion: 分布级遗忘是一种高效、模型无关的解决方案，适用于实际部署中的大规模数据删除需求。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [224] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 论文提出U-Cast模型和Time-HD基准，解决高维时间序列预测（HDTSF）中的复杂通道相关性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）研究未充分关注高维场景下的通道相关性，导致现有模型难以应对HDTSF的复杂性和可扩展性问题。

Method: 提出U-Cast模型，利用基于查询的注意力机制学习潜在层次通道结构，并通过全秩正则化解耦高相关通道表示。

Result: 理论证明跨通道信息降低预测风险，实验表明U-Cast在Time-HD基准上优于基线模型。

Conclusion: U-Cast和Time-HD为未来HDTSF研究提供了坚实基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [225] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 提出了一种遗传算法，通过优化分类和回归任务的复杂性指标，生成具有不同难度级别的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 研究社区需要更先进的合成数据生成器来评估机器学习方法的优缺点，本研究旨在通过生成多样化的数据集满足这一需求。

Method: 使用遗传算法优化分类（10个复杂性指标）和回归（4个指标）任务的复杂性，通过线性特征投影调整合成数据集的复杂性。

Result: 实验表明，算法能生成不同难度的数据集，且数据复杂性与识别质量相关。

Conclusion: 该方法为评估机器学习方法提供了灵活的数据集生成工具。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [226] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究多标签分类问题，利用逻辑约束改进模型表现。


<details>
  <summary>Details</summary>
Motivation: 探索在多标签分类中如何利用已知的逻辑约束提高分类效果。

Method: 采用个体分类器结合表达性序列模型，生成联合分布以建模标签相关性。

Result: 实验证明该架构能在训练中利用约束，并在推理时强制执行约束。

Conclusion: 该架构能有效建模标签相关性并利用逻辑约束提升分类性能。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [227] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于共振隧穿二极管（RTD）的神经形态计算架构，用于物理储层计算（RC），并在图像识别任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向实时、边缘和资源受限环境扩展，需要硬件高效的计算模型。

Method: 理论构建并数值实现了基于RTD的RC系统，测试了手写数字分类和Fruit~360数据集的对象识别。

Result: 该架构在性能表现优异的同时，遵循了新一代RC的原则，即用确定性非线性变换替代随机连接。

Conclusion: RTD-based RC架构为资源受限环境提供了一种高效的计算解决方案。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [228] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 论文研究了机器学习决策模型的反事实解释（CFEs）与用户偏好的对齐问题，提出了基于用户研究的AWP模型，显著提升了预测用户偏好CFEs的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CFEs评估指标（如接近性）可能忽视用户实际偏好和约束，需验证其与用户需求的一致性。

Method: 通过两个用户研究（20名和41名参与者）验证现有指标与用户偏好的对齐性，并提出AWP模型。

Result: 用户偏好与现有指标仅63.81%一致，AWP模型预测准确率达84.37%。

Conclusion: AWP模型首次验证了个性化成本模型在CFE生成中的有效性，强调需开发用户中心的自适应评估指标。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [229] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT是一种将GAT应用于多智能体强化学习（MARL）的交通信号控制方法，通过结合邻近智能体信息，平衡了扩展性和增强的接地能力。


<details>
  <summary>Details</summary>
Motivation: 解决MARL在真实交通网络中因环境动态变化导致的性能下降（sim-to-real gap）问题。

Method: 采用分散式GAT方法，结合邻近智能体信息，增强接地能力并保持扩展性。

Result: 在模拟恶劣天气条件下的多种道路网络中验证了JL-GAT的有效性。

Conclusion: JL-GAT成功将GAT扩展至MARL框架，为真实交通网络提供了可扩展且高效的解决方案。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [230] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出利用平均可控性和新型排名编码方法提升GNN在社交网络分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 社交网络中节点特征常因隐私或属性缺失而不可用，影响GNN性能。

Method: 引入平均可控性和中心性指标（NCT-EFA）作为节点级度量，并开发排名编码方法将图论指标转换为固定维特征空间。

Result: 实验表明，平均可控性显著提升GNN性能，排名编码方法优于传统独热编码（ROC AUC从68.7%提升至73.9%）。

Conclusion: 提出的方法能有效生成高效且表达性强的节点表示，提升GNN在社交网络任务中的表现。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [231] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多模态方法LSDGNN，通过长距离和短距离图神经网络分别获取远距离和近距离话语的多模态特征，并通过差异正则化和双仿射模块优化特征交互。此外，提出改进课程学习（ICL）解决数据不平衡问题，实验结果表明模型在IEMOCAP和MELD数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 情感识别在对话中（ERC）是一个实用且具有挑战性的任务，现有方法在长距离和短距离特征交互及数据不平衡问题上存在不足。

Method: 基于有向无环图（DAG）构建长距离和短距离图神经网络，使用差异正则化和双仿射模块优化特征交互，并提出改进课程学习（ICL）解决数据不平衡。

Result: 在IEMOCAP和MELD数据集上，模型表现优于现有基准。

Conclusion: LSDGNN通过优化特征交互和解决数据不平衡问题，显著提升了情感识别在对话中的性能。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [232] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 论文提出了一种精确约束重构方法，用于解决不平衡分类中的直接指标优化问题，包括固定精度优化召回率、固定召回率优化精度以及优化Fβ分数。


<details>
  <summary>Details</summary>
Motivation: 在不平衡分类中，现有方法通常优化平衡准确率，但无法处理类别重要性不同或需要特定指标达到预设水平的情况。

Method: 引入精确约束重构，通过精确惩罚方法解决直接指标优化问题。

Result: 在多个基准数据集上验证了方法的优越性。

Conclusion: 提出的精确重构与优化框架可广泛应用于二元不平衡分类及其他直接指标优化问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [233] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的图神经网络框架，用于捕捉食品配送需求的空间-时间依赖性，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 食品配送平台的需求预测对运营效率至关重要，但空间异质性和时间波动增加了预测难度。

Method: 将配送环境建模为图，节点代表配送区域，边反映空间邻近性和订单流模式；利用注意力机制动态加权邻近区域的影响。

Result: 在真实数据集上验证了模型的高准确性，优于现有方法。

Conclusion: 该框架为城市食品配送提供了可扩展和自适应的解决方案，支持车队定位、资源分配和调度优化。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [234] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS是一种无需训练、模型无关的扩散采样加速框架，通过多核并行实现高效推理，显著提升采样速度且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真生成能力受限于昂贵的推理计算，现有加速方法需重训练或牺牲质量。

Method: 将多核扩散采样视为ODE求解器管道，通过理论支持的核间通信机制，快速求解器逐步修正慢速求解器。

Result: CHORDS在多样大规模图像和视频扩散模型中实现显著加速，四核提速2.1倍，八核提速2.9倍，且无质量损失。

Conclusion: CHORDS为实时高保真扩散生成奠定了坚实基础。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [235] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间基函数模型（TBFM）的闭环神经刺激方法，解决了样本效率、训练时间和延迟等问题，并在非人灵长类动物实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能技术在个性化闭环神经刺激疗法中的应用尚不明确，且存在样本效率低、训练时间长和延迟高等问题。

Method: 采用时间基函数模型（TBFM）进行单次试验的时空前向预测，模拟闭环刺激以驱动神经活动朝向目标模式。

Result: TBF模型在40次实验中表现出高效性（训练时间2-4分钟，延迟0.2ms），预测精度与基线非线性动力学模型相当，优于线性状态空间模型。

Conclusion: TBF模型为复杂AI方法与临床实用闭环刺激方案之间的转化提供了初步桥梁。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [236] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 本文提出了一种流式遗忘学习范式，解决了现有方法在处理流式数据遗忘请求时的效率和性能问题，通过将遗忘问题建模为分布偏移问题，并设计了一种无需原始训练数据的高效算法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通常批量处理遗忘数据，而实际场景中遗忘请求往往是流式的，导致效率和性能下降。本文旨在解决流式遗忘带来的性能维护、效率和数据访问挑战。

Method: 将遗忘问题形式化为分布偏移问题，估计变化后的分布，并提出一种无需原始训练数据的流式遗忘算法。

Result: 理论分析表明，算法在非强凸损失函数条件下实现了$O(√T + V_T)$的误差界限。实验验证了算法在不同模型和数据集上的有效性。

Conclusion: 本文提出的流式遗忘算法在理论和实验上均表现出色，为流式遗忘问题提供了高效解决方案。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [237] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出了一种利用不完整专家演示的强化学习框架，通过映射函数将状态与专家数据的相似性转化为内在奖励，支持灵活探索。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，强化学习需要从无奖励交互和不完整演示中学习，但现有内在动机方法在高维或密集奖励环境中表现不佳。

Method: 采用混合自编码专家模型，将状态与专家数据的相似性映射为内在奖励，支持多样行为学习和缺失信息处理。

Result: 实验表明，该方法在稀疏和密集奖励环境中均表现优异，即使演示不完整。

Conclusion: 为现实环境中缺乏最优数据和精确奖励控制的强化学习提供了实用框架。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [238] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 该论文扩展了PSID方法，引入了最优滤波和平滑技术，以改进多变量时间序列的建模和预测。


<details>
  <summary>Details</summary>
Motivation: 现有PSID方法仅利用过去的主信号数据进行预测，而在离线应用中，结合并发数据（滤波）或所有可用数据（平滑）可以提升估计效果。

Method: 通过引入降秩回归步骤学习最优增益，扩展PSID以支持滤波；并开发了一种前向-后向PSID平滑算法。

Result: 在模拟数据上验证了方法的有效性，能够恢复真实模型参数并实现最优滤波和平滑性能。

Conclusion: 为多变量时间序列的动态交互分析提供了更强大的工具。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [239] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: FG-TS通过乐观奖励解决高维问题中TS探索不足的问题，但在近似后验下表现未充分验证。实验表明FG-TS在线性和逻辑bandits中优于TS，但在神经bandits中较弱。


<details>
  <summary>Details</summary>
Motivation: 解决TS在高维问题中探索不足的问题，并验证FG-TS在近似后验下的表现。

Method: 使用FG-TS及其平滑变体SFG-TS，在11个真实和合成benchmark中比较性能，包括精确和近似后验设置。

Result: FG-TS在线性和逻辑bandits中表现优于TS，但在神经bandits中较弱。实验揭示了奖励规模和采样噪声的权衡。

Conclusion: FG-TS及其变体易于使用且性能优越，推荐作为现代上下文bandit的基准方法。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [240] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT是一种多视图图变换器框架，通过融合SE3不变和SO3等变图表示，显著提升了晶体性质预测的准确性，并在多任务自监督预训练中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉晶体结构的几何和拓扑特征，限制了机器学习在大规模晶体材料模拟中的应用。

Method: 提出MGT框架，结合SE3不变和SO3等变图表示，并采用轻量级专家混合路由器动态调整权重。

Result: MGT在晶体性质预测任务中平均绝对误差降低21%，在迁移学习场景中性能提升高达58%。

Conclusion: MGT是一种有效的晶体材料性质预测模型，具有跨领域扩展潜力，可用于新材料发现。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [241] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，用于安全高效地协作微调大语言模型（LLM），通过区块链信任层和经济激励解决传统联邦学习的中心化问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（FL）的中心化服务器存在单点攻击和投毒攻击的风险，且在大规模LLM（如70B参数）的异构、无信任环境中难以实现。

Method: FLock整合了基于区块链的信任层和经济激励，取代中心化聚合器，提供安全、可审计的合作协议。

Result: 实验表明，FLock能抵御投毒攻击，减少68%以上的对抗攻击成功率，并实现跨领域的知识协同转移。

Conclusion: FLock在去中心化环境中成功验证了70B LLM的微调，展示了优于孤立训练的跨领域泛化能力。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [242] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM是一个统一的、可解释的数学模型，用于分析主动学习（AL）的动态过程，通过四个关键参数预测和比较AL策略的性能。


<details>
  <summary>Details</summary>
Motivation: 传统AL评估方法仅关注最终准确性，无法全面捕捉学习过程的动态变化，因此需要一种更全面的评估框架。

Method: 提出PALM模型，通过四个参数（可达到的准确性、覆盖效率、早期性能和可扩展性）描述AL轨迹，并基于部分观察预测未来性能。

Result: PALM在多个数据集和AL策略上验证有效，能准确预测学习曲线，并揭示AL方法的学习效率、数据覆盖和可扩展性。

Conclusion: PALM为AL的系统化评估提供了基础，支持在研究和实际应用中选择更具成本效益的策略。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [243] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSG的新框架，首次将信道估计与网格化统一起来，通过CSG-AE和PIDA训练方案显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法（如GSG或BSG）依赖不可用的位置数据或错误的信号强度假设，限制了大规模网络优化的效率。

Method: 提出CSG框架，通过联合优化问题仅使用RSRP估计CAPS并划分网格；开发CSG-AE（包含RSRP-to-CAPS编码器、稀疏码本量化器和物理信息解码器）及PIDA训练方案。

Result: 在合成数据上，CSG-AE在CAPS估计和聚类质量上表现优异；在真实数据上，RSRP预测误差显著降低（Active MAE降低30%，Overall MAE降低65%）。

Conclusion: CSG框架通过统一信道估计与网格化，显著提升了网格化性能，推动了大尺度网络优化的发展。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [244] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 论文提出了一种简单算法，证明在log-concave先验假设下能收敛到近端算子，为实践中常用的启发式方法提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有去噪模型在逆问题中作为先验分布的代理使用，但缺乏理论依据，本文旨在填补这一空白。

Method: 提出一种与实践中常用方法相关的简单算法，证明其在log-concave先验下收敛到近端算子，并解释为平滑近端目标的梯度下降。

Result: 算法在理论条件下收敛，为实践中成功的启发式方法提供了理论基础。

Conclusion: 研究为缺乏理论支持的启发式去噪方法提供了收敛性证明，增强了其可信度。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [245] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过拉格朗日优化在令牌空间中为Transformer提供了理论数学背景，将其视为高维单位球上的流映射，并推导了其变分问题。


<details>
  <summary>Details</summary>
Motivation: 为Transformer建立数学框架，填补其在流形上变分计算的研究空白。

Method: 利用拉格朗日优化和流映射理论，推导Transformer的欧拉-拉格朗日方程，并验证其作为变分问题自然求解器的性质。

Result: 提出了Transformer的变分问题框架，证明了其流映射满足变分功能，并推导了新的欧拉-拉格朗日方程。

Conclusion: 论文为Transformer的变分计算奠定了基础，并展示了其在神经网络场景中的适用性。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [246] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出了一种基于自适应随机傅里叶特征（ARFF）的训练算法，用于从快照数据中学习随机微分方程的漂移和扩散分量，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进随机微分方程的数据驱动建模方法，通过自适应随机傅里叶特征提升学习效率和准确性。

Method: 采用自适应随机傅里叶特征（ARFF）结合Metropolis采样和重采样，基于Euler-Maruyama积分的似然损失函数进行训练。

Result: 在多项基准测试中，ARFF方法在损失最小化和收敛速度上均优于传统Adam优化方法。

Conclusion: ARFF方法为随机动力学的数据驱动建模提供了一种高效且性能优越的替代方案。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [247] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo是一个隐私保护的多模态情感识别框架，结合视觉和生理数据，通过联邦学习实现高精度且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决车载情感识别中的模态脆弱性、生理变异性和隐私风险问题。

Method: 使用CNN提取面部视觉特征，随机森林分类生理信号，通过联邦学习和多数投票融合多模态数据。

Result: 联邦CNN准确率77%，随机森林74%，融合后87%，与集中式基线相当，且数据本地化。

Conclusion: FedMultiEmo为车载环境提供了一种实时、隐私保护的情感识别实用方案。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [248] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 论文提出了一种名为Off-Policy Corrected Reward Modeling (OCRM)的方法，通过重要性加权迭代修正奖励模型，解决了RLHF中因分布偏移导致的过优化问题。


<details>
  <summary>Details</summary>
Motivation: 在RLHF训练语言模型时，奖励模型(RM)因分布偏移逐渐变得不准确，导致模型行为偏离人类偏好。

Method: 提出OCRM方法，通过重要性加权迭代修正RM，无需新标签或样本。

Result: 实验表明，OCRM在摘要和聊天机器人任务中显著优于标准RLHF方法。

Conclusion: OCRM有效解决了RLHF中的过优化问题，提升了最终策略的准确性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [249] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 该研究通过测试时适应（TTA）技术解决音频分类中的域偏移问题，比较了TTT、TENT和CoNMix方法，并提出改进版CoNMix，在噪声环境下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中的域偏移问题，特别是在音频分类中由背景噪声引起的性能下降。

Method: 采用TTT、TENT和CoNMix三种TTA方法，并在AudioMNIST和SpeechCommands V1数据集上测试其性能。

Result: 改进版CoNMix在噪声环境下分类准确率最高（如10 dB运动自行车噪声下错误率为5.31%）。

Conclusion: 这是首次将TTA技术应用于音频分类中的域偏移问题，改进版CoNMix表现最优。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [250] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 论文提出了一种名为“数据感知可微分神经架构搜索”的方法，通过同时优化模型架构和输入数据特性，降低TinyML系统的设计复杂度。


<details>
  <summary>Details</summary>
Motivation: 机器学习的高资源消耗限制了其广泛应用，尤其是TinyML系统设计复杂，阻碍了其普及。

Method: 扩展了可微分神经架构搜索的搜索空间，包括数据配置参数和架构选择，实现模型架构与输入数据的协同优化。

Result: 在关键词识别任务中，该方法生成了资源占用低但准确性高的系统。

Conclusion: 该方法为TinyML系统设计提供了一种高效且资源优化的解决方案。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [251] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 研究评估了传统放射组学（CR）和深度学习（DL）MRI放射组学在胶质母细胞瘤预后（生存期≤6 vs >6个月）中的附加价值，发现其相对于临床和分子预测因子的优势有限。


<details>
  <summary>Details</summary>
Motivation: 探讨放射组学在胶质母细胞瘤预后中的实际价值，尤其是与传统临床和分子预测因子相比的附加贡献。

Method: 研究使用了来自五个瑞士中心和一个公共来源的1152例胶质母细胞瘤患者数据，开发了CR和DL模型，并在内部和外部队列中评估了不同特征集（仅影像、仅临床/分子、组合特征）和患者子集的表现。

Result: 在外部验证中，组合特征的CR模型AUC为0.75，略优于仅临床（0.74）和仅影像（0.68）模型。DL模型趋势相似但无统计学意义。组合模型在部分子集中未优于仅临床模型。

Conclusion: 尽管确认了MRI序列的预测价值，但标准CR和DL放射组学方法在胶质母细胞瘤预后中相对于年龄和性别等人口统计学预测因子的附加价值有限。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [252] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym是一个用于评估基于大型语言模型的科学推理能力的基准测试平台，通过控制先验知识水平和问题复杂性来分析代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估大型语言模型在科学发现中应对环境复杂性和利用先验知识能力的专用基准，PhysGym填补了这一空白。

Method: PhysGym提供交互式物理环境模拟，代理需主动探索、收集数据并形成假设，同时支持标准化评估协议和指标。

Result: 通过基线测试展示了PhysGym能区分不同先验知识和任务复杂性下的代理能力。

Conclusion: PhysGym为研究大型语言模型的科学推理能力提供了有效的评估工具。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [253] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文探讨了住院时间（LOS）预测准确性与重新调度灵活性之间的关系，研究了在预测误差下最有效的患者重新调度策略，以防止床位溢出并优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 下游资源（如床位）的可用性对计划选择性手术患者入院至关重要。LOS预测误差可能导致计划不可行，因此需要研究重新调度策略与预测准确性的关系。

Method: 利用模拟机器学习方法评估数据驱动策略，分析不同纠正政策下LOS预测准确性与重新调度灵活性的关系。

Result: 研究发现，在LOS预测误差下，灵活的重新调度策略（如推迟入院、转移患者）能有效防止床位溢出并优化资源利用。

Conclusion: 提高LOS预测准确性虽有益，但结合灵活的重新调度策略更能有效应对预测误差，优化资源管理。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [254] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: 论文探讨了AI在卫星巨型星座管理中的应用，通过强化学习优化数据路由和资源分配，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星星座的快速扩展带来了管理挑战，需要高效、可扩展且稳健的解决方案。

Method: 利用强化学习（RL）优化数据路由（降低延迟）和资源分配（高效利用电池和内存）。

Result: RL在多种星座配置和场景中表现优于传统方法，提供更高的灵活性、可扩展性和通用性。

Conclusion: AI能显著改变卫星星座管理，提供更自适应、稳健且经济的解决方案。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [255] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 论文指出当前异常检测算法的评估方法存在局限，导致进展停滞，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测算法的评估方法未能充分反映实际应用的多样性，限制了算法的发展。

Method: 提出重新思考异常检测的基准测试方法，建议基于共同分类法定义场景，端到端分析流程，并根据场景目标评估算法。

Result: 识别了三个改进方向：基于分类法的场景定义、端到端流程分析和目标导向的评估。

Conclusion: 需要改进异常检测的评估方法，以更好地反映实际应用需求，推动算法发展。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [256] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 提出了一种红队多智能体强化学习框架，通过干扰和探索发现安全关键场景中的极端情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法在安全关键场景中难以捕捉极端情况，需要更高效的解决方案。

Method: 使用红队多智能体强化学习框架，结合约束图表示马尔可夫决策过程，量化威胁并干扰自动驾驶车辆。

Result: 实验表明，该方法显著影响自动驾驶车辆决策安全，并生成多种极端情况。

Conclusion: 该框架为安全关键场景研究提供了新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [257] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出了一种基于C$^2$感知的联邦学习框架，通过优化批量大小控制来降低端到端学习延迟，同时确保收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在6G网络中部署时面临的高维模型更新计算与传输开销，以及设备间通信与计算能力异质性的挑战。

Method: 设计了一种C$^2$感知框架，通过分析收敛性揭示的C$^2$权衡，提出两种批量大小控制策略，适应慢衰落和快衰落场景。

Result: 实验表明，该策略优于不考虑C$^2$权衡或设备异质性的传统批量大小调整方案。

Conclusion: 该框架有效平衡了学习性能和延迟，适用于时间敏感的物联网应用。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [258] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 论文提出了一种深度学习方法，通过结合GRU和Geo-FNO加速HEC-RAS洪水模拟，显著提升计算速度且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理求解器（如HEC-RAS）计算耗时，无法满足洪水事件中的实时决策需求，需在不牺牲精度的情况下加速模拟。

Method: 采用混合自回归架构，结合GRU捕捉短期时间动态和Geo-FNO建模长程空间依赖，从HEC-RAS文件中提取8通道特征向量进行训练。

Result: 模型在密西西比河流域67个河段上测试，中位绝对水位误差为0.31英尺，计算时间从139分钟缩短至40分钟，加速3.5倍。

Conclusion: 数据驱动方法通过特征工程可替代传统水力模型，提升大规模洪水集合预报的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [259] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的模型框架Data Mixing Agent，用于动态调整源领域和目标领域数据的权重，以在持续预训练中平衡性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在小规模任务数据上持续预训练时可能出现的灾难性遗忘问题。

Method: 通过强化学习训练Data Mixing Agent，学习通用的数据混合启发式策略。

Result: 在数学推理任务中表现优于基线方法，并能泛化到未见过的领域和模型。

Conclusion: Data Mixing Agent是一种高效且适应性强的解决方案，能够减少源领域数据需求并保持模型性能。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [260] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 提出了一种可解释的异常检测框架，用于共享单车系统，结合多源数据和隔离森林算法，分析站点级异常及其外部影响因素。


<details>
  <summary>Details</summary>
Motivation: 共享出行系统（如共享单车）在城市交通中至关重要，识别异常有助于优化运营、提高服务可靠性和用户体验。

Method: 基于多源数据（骑行记录、天气、公共交通可用性），采用隔离森林算法进行无监督异常检测，并结合DIFFI算法提供可解释性。

Result: 站点级分析能有效识别异常，外部因素（如恶劣天气和公共交通限制）对异常有显著影响。

Conclusion: 该框架有助于提升共享出行系统的决策能力。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [261] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN是一种神经网络框架，通过显式编码物理定律的几何先验来学习动力学，显著优于现有模型，具有长期稳定性、准确性和能量守恒性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法常忽视物理系统的几何基础，导致预测不稳定，尤其是在高维和混沌系统中。

Method: GeoHNN通过参数化惯性矩阵和使用约束自编码器，强制实施黎曼几何和辛几何结构。

Result: 实验表明，GeoHNN在耦合振子和高维可变形物体等系统中表现优异。

Conclusion: 嵌入物理几何不仅是理论需求，更是构建稳健、通用物理模型的实践必需。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [262] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 该研究提出了一种结合无监督异常检测和可解释人工智能（XAI）的方法，用于电动汽车充电基础设施的异常检测和根因分析。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电站是支持可再生能源交通转型的关键基础设施，但其可靠性和效率需要通过有效的异常检测来识别充电行为中的异常。同时，了解异常的根本原因也至关重要。

Method: 研究采用Isolation Forest进行异常检测，并使用DIFFI方法识别导致异常的关键特征。

Result: 该方法在真实工业案例中进行了评估，证明了其有效性。

Conclusion: 结合无监督异常检测和XAI技术，能够有效检测异常并揭示其根本原因，为充电基础设施的可靠性提供了支持。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [263] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 提出了一种基于多模态感知的毫米波信号遮挡预测框架，结合摄像头、GPS、LiDAR和雷达数据，通过深度学习模型和软加权集成策略，实现了高精度预测。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段的车载通信系统易受动态障碍物（如车辆、行人等）的信号遮挡，需解决这一问题以提高通信可靠性。

Method: 采用多模态感知（摄像头、GPS、LiDAR、雷达），利用模态特定的深度学习模型独立处理各传感器数据，并通过验证性能的软加权集成策略融合输出。

Result: 摄像头单独模型在1.5秒预测范围内F1分数达97.1%，推理时间89.8ms；摄像头+雷达组合进一步提升至97.2% F1分数和95.7ms推理时间。

Conclusion: 多模态感知在毫米波遮挡预测中表现出高效性和准确性，为动态环境中的主动无线通信提供了可行方案。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [264] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA是一种基于变分自编码器的全自动工作流，用于通过拉曼光谱检测植物应激，无需手动预处理，能无偏地识别和量化光谱特征。


<details>
  <summary>Details</summary>
Motivation: 植物应激检测对农业至关重要，传统拉曼分析方法依赖定制化数据处理流程，可能引入偏差和不一致性。

Method: DIVA利用变分自编码器处理原始拉曼光谱（包括荧光背景），无需手动预处理，自动识别和量化光谱特征。

Result: DIVA成功检测了多种植物应激（如非生物和生物应激），展示了其在植物健康评估中的潜力。

Conclusion: DIVA结合深度学习和振动光谱，为AI驱动的植物健康评估提供了新途径，促进农业的可持续性。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [265] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 论文提出了一种名为PRO-DYN的命名法，用于从动力学角度分析现有模型，发现性能不佳的架构仅部分学习动力学，且动力学模块的位置对模型性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在时间序列预测任务中表现不佳，作者假设这是因为模型需要学习数据的底层动力学特性。

Method: 通过系统和实证研究，开发了PRO-DYN命名法分析现有模型，并进行了大量实验验证。

Result: 实验证实，性能不佳的架构仅部分学习动力学，且动力学模块作为最终预测器时效果最佳。

Conclusion: 研究强调了在模型中引入可学习的动力学模块并将其作为最终预测器的重要性。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [266] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein-Rubinstein距离的专家融合模型（WR-EFM），用于解决图节点分类中的类别不平衡问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 在PubMed引文网络数据集中，传统GCN对不同类别的分类性能差异显著（如Category 2的准确率比Category 1低7.5%），因此需要一种方法来解决这种不平衡问题。

Method: WR-EFM结合了针对不同类别的专用GNN模型（如带层归一化和残差连接的GNN用于Categories 0/1，多跳GAT用于Category 2），并通过Wasserstein-Rubinstein距离优化模型间的表示相似性，动态加权融合模型。

Result: WR-EFM在三个类别上的准确率分别为77.8%、78.0%和79.9%，优于单一模型和标准融合方法，且类别间准确率的变异系数降低了77.6%。

Conclusion: WR-EFM通过WR距离引导的融合策略有效捕捉复杂结构模式，为类别不平衡的图分类任务提供了新范式。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [267] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 研究探讨了小规模大语言模型（LLM）是否可以通过带可验证奖励的强化学习（RLVR）获得通用的人类社会智能（如心智理论ToM）。结果表明，小LLM难以发展通用ToM能力，且过度训练会导致模型“破解”训练数据的统计模式，而非真正掌握抽象ToM。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习是否能让小规模LLM获得类似人类的社会智能（如ToM），以验证其泛化能力。

Method: 使用RLVR训练小LLM，并在多个ToM数据集（HiToM、ExploreToM、FANToM）上评估，测试其在未见过数据集（如OpenToM）上的泛化能力。

Result: 小LLM在训练数据上表现提升，但无法泛化到不同特性的未见过ToM任务；过度训练导致模型过拟合训练数据，泛化能力下降。

Conclusion: 小LLM难以通过RLVR获得通用ToM能力，其表现更多是过拟合而非真正掌握抽象ToM。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [268] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 提出了一种结合CNN-LSTM-注意力-Adaboost的混合神经网络模型，通过改进的蛇群优化算法优化超参数，显著提升了4D轨迹预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决中长期4D轨迹预测模型的局限性，提升处理大规模高维轨迹数据的能力。

Method: 使用Adaboost算法划分多个弱学习器，每个子模型结合CNN、LSTM和注意力机制提取特征，并通过改进的蛇群优化算法优化超参数。

Result: 在真实ADS-B数据上测试，SO-CLA-adaboost模型优于传统优化器，预测精度提升39.89%。

Conclusion: 该模型在4D轨迹预测中表现出色，改进的优化算法显著提升了性能。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [269] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文提出了一种优化差分隐私学习算法隐私审计的方法，通过改进审计员的“金丝雀”集，显著提升了隐私参数的下界估计。


<details>
  <summary>Details</summary>
Motivation: 研究黑盒隐私审计，目标是仅通过算法输出（如训练好的模型）来下界估计差分隐私学习算法的隐私参数。

Method: 利用元梯度优化技术优化审计员的金丝雀集，提升隐私审计效果。

Result: 实验表明，优化的金丝雀集可将差分隐私图像分类模型的隐私参数下界提升超过2倍，且该方法具有可迁移性和高效性。

Conclusion: 优化的金丝雀集显著提升了隐私审计的效果，适用于不同规模和隐私级别的模型。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [270] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出一种利用LLM快速生成高质量表格数据的方法，通过分布脚本减少时间和成本负担。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM直接生成数据时的高成本和时间问题，提升大规模合成数据的效率和多样性。

Method: 利用LLM推断字段分布并生成可复用的采样脚本，自动分类字段类型以高效生成数据。

Result: 实验表明该方法在多样性和数据真实性上优于传统方法，显著降低生成成本。

Conclusion: 该方法可加速生产流程测试，缩短开发周期，为合成数据生成提供高效解决方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [271] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 论文提出GUI-G$^2$，一种基于高斯分布的奖励框架，用于改进GUI交互任务中的空间定位问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用稀疏的二元奖励信号，忽略了空间交互的连续性，而人类点击行为自然形成高斯分布。

Method: GUI-G$^2$结合高斯点奖励和覆盖奖励，通过自适应方差机制处理不同尺寸的元素。

Result: 在多个基准测试中，GUI-G$^2$显著优于现有方法，最高提升24.7%。

Conclusion: 连续建模提高了对界面变化的鲁棒性和泛化能力，为GUI交互任务中的空间推理提供了新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [272] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 扩散模型在数据受限情况下优于自回归模型，尤其在计算资源充足时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在数据受限场景下的优势，填补其与自回归模型比较的研究空白。

Method: 系统研究掩码扩散模型在数据受限环境中的表现，分析其性能与计算资源的关系。

Result: 扩散模型在数据稀缺时表现更优，验证损失更低，下游任务性能更好。

Conclusion: 当数据是瓶颈时，扩散模型是自回归模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [273] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO是一个高效处理大规模视频数据中复杂对象查询的系统，通过预训练视觉编码器和多索引结构实现低延迟和高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频分析方法在适应新对象类别和查询延迟方面的不足。

Method: 使用预训练视觉编码器提取特征，构建多索引结构，并通过近似最近邻搜索和跨模态重排优化查询结果。

Result: 在真实数据集上，LOVO在查询准确性和延迟方面显著优于现有方法，搜索延迟降低85倍。

Conclusion: LOVO为视频分析中的复杂对象查询设定了新标准，具有高效、可扩展和动态环境适应性。

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [274] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: 论文研究了捆绑推荐系统中的产品公平性问题，发现传统公平性框架在多层次的捆绑推荐中不适用，需多角度评估。


<details>
  <summary>Details</summary>
Motivation: 捆绑推荐（BR）中的产品公平性问题尚未充分研究，传统推荐系统的公平性框架难以直接应用于多层次场景。

Method: 在三个真实数据集上使用四种先进的BR方法进行可重复性研究，分析捆绑和物品层面的曝光差异。

Result: 发现捆绑和物品层面的曝光模式不同，用户行为对公平性有显著影响。

Conclusion: 研究为构建更公平的捆绑推荐系统提供了实用见解，并为未来研究奠定了基础。

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [275] [RaMen: Multi-Strategy Multi-Modal Learning for Bundle Construction](https://arxiv.org/abs/2507.14361)
*Huy-Son Nguyen,Quang-Huy Nguyen,Duc-Hoang Pham,Duc-Trong Le,Hoang-Quynh Le,Padipat Sitkrongwong,Atsuhiro Takasu,Masoud Mansoury*

Main category: cs.IR

TL;DR: RaMen是一种新颖的多策略方法，通过显式和隐式策略学习，结合内外部信息，优化了捆绑结构的表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖用户反馈或语义信息，未能捕捉真实捆绑结构中的复杂关系，导致表示效果不佳。

Method: RaMen采用显式策略学习（ESL）和隐式策略学习（ISL），结合多模态数据和超图消息传递，学习全面的捆绑表示。

Result: 实验表明RaMen在多个领域优于现有模型，提供了对复杂项目集问题的深入见解。

Conclusion: RaMen通过多策略学习和知识对齐，显著提升了捆绑结构的表示效果。

Abstract: Existing studies on bundle construction have relied merely on user feedback
via bipartite graphs or enhanced item representations using semantic
information. These approaches fail to capture elaborate relations hidden in
real-world bundle structures, resulting in suboptimal bundle representations.
To overcome this limitation, we propose RaMen, a novel method that provides a
holistic multi-strategy approach for bundle construction. RaMen utilizes both
intrinsic (characteristics) and extrinsic (collaborative signals) information
to model bundle structures through Explicit Strategy-aware Learning (ESL) and
Implicit Strategy-aware Learning (ISL). ESL employs task-specific attention
mechanisms to encode multi-modal data and direct collaborative relations
between items, thereby explicitly capturing essential bundle features.
Moreover, ISL computes hyperedge dependencies and hypergraph message passing to
uncover shared latent intents among groups of items. Integrating diverse
strategies enables RaMen to learn more comprehensive and robust bundle
representations. Meanwhile, Multi-strategy Alignment & Discrimination module is
employed to facilitate knowledge transfer between learning strategies and
ensure discrimination between items/bundles. Extensive experiments demonstrate
the effectiveness of RaMen over state-of-the-art models on various domains,
justifying valuable insights into complex item set problems.

</details>


### [276] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 论文探讨了神经IR架构（特别是交叉编码器）的内部机制，提出简单方法可揭示其注意力过程和匹配检测机制。


<details>
  <summary>Details</summary>
Motivation: 神经IR架构的高效性已被证实，但其内部机制尚不明确，现有研究多关注高层次过程，缺乏对匹配过程的描述。

Method: 通过分析注意力过程提取因果见解，并解释匹配检测的机制。

Result: 发现某些注意力头在注意力过程中起关键作用，并揭示了匹配检测的底层机制。

Conclusion: 简单方法也能为神经IR架构的内部机制提供有价值的见解。

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [277] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: 论文提出了一种新的POI推荐框架GDPW，通过结合POI类别信息和时间信息，并利用对比学习解耦这些信息，同时考虑POI的权重因素，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分探索POI类别与时间的关系，且难以捕捉时间的连续性，同时忽略了POI的权重信息（如流行度、转移关系和距离），导致性能不佳。

Method: 提出GDPW框架，通过全局类别图和全局类别-时间图学习类别和时间表示，利用对比学习解耦信息，并基于POI的转移权重和距离关系加权预测结果。

Result: 在两个真实数据集上的实验表明，GDPW优于现有模型，性能提升3%至11%。

Conclusion: GDPW通过综合考虑类别、时间及权重信息，有效提升了POI推荐的准确性和实用性。

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [278] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: 论文提出了一种两阶段框架（检索与重排），通过优化的负例挖掘和定制指标提升法律文档检索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在法律等专业领域中精度和领域知识不足的问题。

Method: 采用两阶段框架：Bi-Encoder快速检索候选文档，Cross-Encoder精确重排，结合负例挖掘优化。

Result: 在SoICT Hackathon 2024中取得前三名，轻量级方法表现竞争力。

Conclusion: 优化的数据处理、损失函数和负采样对法律检索系统至关重要。

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [279] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: 该论文提出了一种名为U-MARVEL的统一框架，用于改进多模态检索任务，通过系统分析嵌入学习和训练策略的关键因素，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的多模态检索方法虽然成功，但其机制未充分探索，可能导致性能不佳和泛化能力有限。

Method: 实现了一个通用的MLLM嵌入学习流程，分析了嵌入生成和训练策略的关键因素，如渐进过渡、难负样本挖掘和重排器蒸馏。

Result: U-MARVEL在M-BEIR基准测试中显著优于现有方法，并在零样本任务中表现出色。

Conclusion: U-MARVEL框架在多模态检索任务中具有强大的泛化能力，为嵌入学习提供了新的方向。

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [280] [User Invariant Preference Learning for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.14925)
*Mingshi Yan,Zhiyong Cheng,Fan Liu,Yingda Lyu,Yahong Han*

Main category: cs.IR

TL;DR: 论文提出了一种用户不变偏好学习（UIPL）方法，用于多行为推荐，通过捕捉用户内在兴趣（不变偏好）来减少噪声干扰，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设用户多行为中存在共享偏好，但忽略了行为的共性和个性，导致某些辅助行为引入噪声，影响目标行为预测。

Method: UIPL利用不变风险最小化范式，通过变分自编码器（VAE）提取用户不变偏好，并构建不同环境增强学习鲁棒性。

Result: 在四个真实数据集上的实验表明，UIPL显著优于现有方法。

Conclusion: UIPL通过捕捉用户不变偏好，有效减少了噪声干扰，提升了多行为推荐的准确性。

Abstract: In multi-behavior recommendation scenarios, analyzing users' diverse
behaviors, such as click, purchase, and rating, enables a more comprehensive
understanding of their interests, facilitating personalized and accurate
recommendations. A fundamental assumption of multi-behavior recommendation
methods is the existence of shared user preferences across behaviors,
representing users' intrinsic interests. Based on this assumption, existing
approaches aim to integrate information from various behaviors to enrich user
representations. However, they often overlook the presence of both
commonalities and individualities in users' multi-behavior preferences. These
individualities reflect distinct aspects of preferences captured by different
behaviors, where certain auxiliary behaviors may introduce noise, hindering the
prediction of the target behavior. To address this issue, we propose a user
invariant preference learning for multi-behavior recommendation (UIPL for
short), aiming to capture users' intrinsic interests (referred to as invariant
preferences) from multi-behavior interactions to mitigate the introduction of
noise. Specifically, UIPL leverages the paradigm of invariant risk minimization
to learn invariant preferences. To implement this, we employ a variational
autoencoder (VAE) to extract users' invariant preferences, replacing the
standard reconstruction loss with an invariant risk minimization constraint.
Additionally, we construct distinct environments by combining multi-behavior
data to enhance robustness in learning these preferences. Finally, the learned
invariant preferences are used to provide recommendations for the target
behavior. Extensive experiments on four real-world datasets demonstrate that
UIPL significantly outperforms current state-of-the-art methods.

</details>


### [281] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: FullRecall是一种新型专利检索方法，通过IPC引导的知识生成信息短语，实现100%召回率，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 专利审查员和发明人在验证发明原创性和非显而易见性时面临巨大压力，专利数据的复杂性加剧了检索挑战，需要可靠的检索策略。

Method: FullRecall利用IPC引导知识生成信息短语，提取关键名词短语构建查询，分阶段检索和排序以平衡精确率和召回率。

Result: 在五个测试案例中，FullRecall实现100%召回率，显著优于基线方法HRR2和ReQ-ReC。

Conclusion: FullRecall通过多阶段检索和排序策略，有效平衡精确率和召回率，提升专利检索的可靠性，减少法律风险。

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [282] [Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations](https://arxiv.org/abs/2507.15113)
*Xiangyu Zeng,Amit Jaspal,Bin Liu,Goutham Panneeru,Kevin Huang,Nicolas Bievre,Mohit Jaggi,Prathap Maniraju,Ankur Jain*

Main category: cs.IR

TL;DR: 论文提出了一种多任务学习方法，通过分离点击A购买A（CABA）和点击A购买B（CABB）的任务，优化电商推荐模型的转化率。


<details>
  <summary>Details</summary>
Motivation: 电商用户行为中常见的点击A购买B（CABB）现象导致传统推荐模型学习偏差，影响转化率。

Method: 采用多任务学习框架，结合分类感知的协同过滤权重方案，通过产品分类树和类别相似性矩阵区分有意义的CABB转换。

Result: 离线评估显示归一化熵降低13.9%，在线A/B测试显示主要业务指标提升0.25%。

Conclusion: 多任务学习方法有效减少了推荐模型的偏差，提升了电商平台的转化率。

Abstract: User journeys in e-commerce routinely violate the one-to-one assumption that
a clicked item on an advertising platform is the same item later purchased on
the merchant's website/app. For a significant number of converting sessions on
our platform, users click product A but buy product B -- the Click A, Buy B
(CABB) phenomenon. Training recommendation models on raw click-conversion pairs
therefore rewards items that merely correlate with purchases, leading to biased
learning and sub-optimal conversion rates. We reframe conversion prediction as
a multi-task problem with separate heads for Click A Buy A (CABA) and Click A
Buy B (CABB). To isolate informative CABB conversions from unrelated CABB
conversions, we introduce a taxonomy-aware collaborative filtering weighting
scheme where each product is first mapped to a leaf node in a product taxonomy,
and a category-to-category similarity matrix is learned from large-scale
co-engagement logs. This weighting amplifies pairs that reflect genuine
substitutable or complementary relations while down-weighting coincidental
cross-category purchases. Offline evaluation on e-commerce sessions reduces
normalized entropy by 13.9% versus a last-click attribution baseline. An online
A/B test on live traffic shows +0.25% gains in the primary business metric.

</details>


### [283] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: SPAR是一个多智能体框架，通过RefChain查询分解和查询演化提升学术文献检索的灵活性和效果，并构建SPARBench基准验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有学术文献检索系统依赖固定流程且推理能力有限，需要更灵活高效的解决方案。

Method: 提出SPAR框架，结合RefChain查询分解和查询演化，并构建SPARBench基准进行系统评估。

Result: SPAR显著优于基线方法，在AutoScholar和SPARBench上分别提升56%和23%的F1分数。

Conclusion: SPAR和SPARBench为学术检索研究提供了可扩展、可解释且高性能的基础。

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [284] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: 论文提出了一种基于LLM的新框架GREAT，用于视频相关搜索中的查询推荐任务（I2Q），并发布了一个大规模数据集KuaiRS。


<details>
  <summary>Details</summary>
Motivation: 解决视频相关搜索中查询推荐任务缺乏学术研究和公开数据集的问题，以及现有方法在语义内容与查询深度交互上的不足。

Method: 提出GREAT框架，利用查询前缀树（trie）指导LLM生成高质量查询，并通过后处理模块优化查询与视频的相关性。

Result: 离线与在线实验证明了GREAT框架的有效性。

Conclusion: GREAT框架显著提升了视频相关搜索中的查询推荐质量，填补了该领域的研究空白。

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [285] [Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.15395)
*Hengyu Zhang,Chunxu Shen,Xiangguo Sun,Jie Tan,Yanchao Tan,Yu Rong,Hong Cheng,Lingling Yi*

Main category: cs.IR

TL;DR: 提出了一种基于信息瓶颈原则的层次图框架（HGIB），用于多行为推荐，解决了行为间分布差异和辅助行为噪声问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多行为推荐中，现有方法面临行为间分布差异和辅助行为噪声导致的负迁移问题，需要更有效的解决方案。

Method: 提出HGIB框架，结合信息瓶颈原则和图细化编码器（GRE），动态修剪冗余边以优化表示学习。

Result: 在三个公开数据集和工业场景中实验，HGIB表现优于现有方法，并通过在线A/B测试验证了实际效果。

Conclusion: HGIB通过信息瓶颈和图细化有效解决了多行为推荐中的关键挑战，具有实际应用价值。

Abstract: In real-world recommendation scenarios, users typically engage with platforms
through multiple types of behavioral interactions. Multi-behavior
recommendation algorithms aim to leverage various auxiliary user behaviors to
enhance prediction for target behaviors of primary interest (e.g., buy),
thereby overcoming performance limitations caused by data sparsity in target
behavior records. Current state-of-the-art approaches typically employ
hierarchical design following either cascading (e.g.,
view$\rightarrow$cart$\rightarrow$buy) or parallel
(unified$\rightarrow$behavior$\rightarrow$specific components) paradigms, to
capture behavioral relationships. However, these methods still face two
critical challenges: (1) severe distribution disparities across behaviors, and
(2) negative transfer effects caused by noise in auxiliary behaviors. In this
paper, we propose a novel model-agnostic Hierarchical Graph Information
Bottleneck (HGIB) framework for multi-behavior recommendation to effectively
address these challenges. Following information bottleneck principles, our
framework optimizes the learning of compact yet sufficient representations that
preserve essential information for target behavior prediction while eliminating
task-irrelevant redundancies. To further mitigate interaction noise, we
introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant
edges through learnable edge dropout mechanisms. We conduct comprehensive
experiments on three real-world public datasets, which demonstrate the superior
effectiveness of our framework. Beyond these widely used datasets in the
academic community, we further expand our evaluation on several real industrial
scenarios and conduct an online A/B testing, showing again a significant
improvement in multi-behavior recommendations. The source code of our proposed
HGIB is available at https://github.com/zhy99426/HGIB.

</details>


### [286] [RankMixer: Scaling Up Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2507.15551)
*Jie Zhu,Zhifang Fan,Xiaoxie Zhu,Yuchen Jiang,Hangyu Wang,Xintian Han,Haoran Ding,Xinmin Wang,Wenlin Zhao,Zhen Gong,Huizhi Yang,Zheng Chai,Zhe Chen,Yuchao Zheng,Qiwei Chen,Feng Zhang,Xun Zhou,Peng Xu,Xiao Yang,Di Wu,Zuotao Liu*

Main category: cs.IR

TL;DR: RankMixer是一种硬件感知的推荐系统模型设计，通过高效的特征交互架构提升模型性能，显著提高MFU并保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决工业推荐系统中训练和服务成本高、传统特征交互模块无法充分利用现代GPU的问题。

Method: 引入RankMixer，结合多头部令牌混合模块和Per-token FFNs，支持稀疏MoE扩展和动态路由策略。

Result: 将MFU从4.5%提升至45%，模型参数扩展100倍且延迟不变，在线A/B测试验证了其普适性。

Conclusion: RankMixer在推荐、广告和搜索场景中表现优异，显著提升用户活跃度和使用时长。

Abstract: Recent progress on large language models (LLMs) has spurred interest in
scaling up recommendation systems, yet two practical obstacles remain. First,
training and serving cost on industrial Recommenders must respect strict
latency bounds and high QPS demands. Second, most human-designed
feature-crossing modules in ranking models were inherited from the CPU era and
fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and
poor scalability. We introduce RankMixer, a hardware-aware model design
tailored towards a unified and scalable feature-interaction architecture.
RankMixer retains the transformer's high parallelism while replacing quadratic
self-attention with multi-head token mixing module for higher efficiency.
Besides, RankMixer maintains both the modeling for distinct feature subspaces
and cross-feature-space interactions with Per-token FFNs. We further extend it
to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic
routing strategy is adapted to address the inadequacy and imbalance of experts
training. Experiments show RankMixer's superior scaling abilities on a
trillion-scale production dataset. By replacing previously diverse handcrafted
low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and
scale our ranking model parameters by 100x while maintaining roughly the same
inference latency. We verify RankMixer's universality with online A/B tests
across three core application scenarios (Recommendation, Advertisement and
Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic
serving without increasing the serving cost, which improves user active days by
0.2% and total in-app usage duration by 0.5%.

</details>


### [287] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: JAM是一个轻量级框架，通过向量翻译和多模态特征聚合实现自然语言音乐推荐，解决了LLMs的高成本和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在音乐推荐系统中的高成本和延迟问题，同时克服检索方法忽略长期用户偏好和需要全模型重训练的局限性。

Method: JAM通过向量翻译建模用户-查询-项目交互，利用跨注意力和稀疏专家混合聚合多模态特征。

Result: JAM在准确性、实用性和易集成性方面表现优异，并提供了一个包含10万+用户-查询-项目三元组的新数据集。

Conclusion: JAM为自然语言音乐推荐提供了一种高效、直观且易于部署的解决方案。

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [288] [Information Theoretic Analysis of a Dual-Band MIMO Cellphone Antenna with ANSYS HFSS SBR+](https://arxiv.org/abs/2507.14704)
*Volodymyr Shyianov,Bamelak Tadele,Vladimir I. Okhmatovski,Amine Mezghani*

Main category: cs.IT

TL;DR: 该论文提出了一种结合信息论和电磁理论的方法，用于评估手机上的双频双极化MIMO天线阵列设计，并通过仿真和概率分析比较了不同方法的性能差异。


<details>
  <summary>Details</summary>
Motivation: 传统天线设计与香农理论分离，本文旨在结合两者，通过信息论方法优化天线阵列设计。

Method: 使用ANSYS HFSS进行电磁仿真，获取天线阵列模型和信道矩阵，通过线性与最优处理估计中断概率曲线，分析分集增益和复用增益。

Result: 研究发现，基于中断概率曲线的方法与传统方法（如包络相关系数或波束耦合矩阵）在分集增益估计上存在显著差异。

Conclusion: 信息论方法为天线设计提供了新的视角，与传统方法相比具有独特优势。

Abstract: Historically, the design of antenna arrays has evolved separately from
Shannon theory. Shannon theory adopts a probabilistic approach in the design of
communication systems, while antenna design approaches have relied on the
deterministic Maxwell theory alone. In this paper, we investigate an
information-theoretic analysis approach which we apply to evaluate the design
of a dual-band, dual-polarized multiple-input multiple-output (MIMO) array on a
cellphone. To this end, we use ANSYS HFSS, a commercial electromagnetic (EM)
simulation software suitable for the numerical optimization of antenna systems.
HFSS is used to obtain an accurate model of the cellphone MIMO antenna array
and HFSS SBR+ is utilized to obtain channel matrices for a large number of
users. Taking advantage of linear and optimal processing at the cellphone, we
estimate the outage probability curves. The curves are then used to determine
the diversity gain in a moderate signal-to-noise ratio (SNR) regime and the
multiplexing gain at a high SNR regime. This approach is then compared with the
method of estimating the diversity gain from the envelope correlation
coefficients or the beam-coupling matrix showing substantial differences in the
two methodologies.

</details>


### [289] [Study of Delay-Calibrated Joint User Activity Detection, Channel Estimation and Data Detection for Asynchronous mMTC Systems](https://arxiv.org/abs/2507.14733)
*Z. Shao,X. Yuan,R. de Lamare*

Main category: cs.IT

TL;DR: 论文提出了一种基于期望最大化方法的延迟校准联合用户活动检测、信道估计和数据检测算法，解决了大规模机器类型通信中的异步传输问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模低功耗设备异步传输时因有限正交前导码导致的碰撞问题。

Method: 采用期望最大化方法，结合近似消息传递原理，开发了一种计算高效的迭代算法。

Result: 数值结果表明，算法在信道和数据符号的归一化均方误差以及误检概率方面表现优异。

Conclusion: 所提算法有效解决了异步大规模机器类型通信中的用户活动检测、信道估计和数据检测问题。

Abstract: This work considers uplink asynchronous massive machine-type communications,
where a large number of low-power and low-cost devices asynchronously transmit
short packets to an access point equipped with multiple receive antennas. If
orthogonal preambles are employed, massive collisions will occur due to the
limited number of orthogonal preambles given the preamble sequence length. To
address this problem, we propose a delay-calibrated joint user activity
detection, channel estimation, and data detection algorithm, and investigate
the benefits of oversampling in estimating continuous-valued time delays at the
receiver. The proposed algorithm is based on the expectation-maximization
method, which alternately estimates the delays and detects active users and
their channels and data by noting that the collided users have different
delays. Under the Bayesian inference framework, we develop a computationally
efficient iterative algorithm using the approximate message passing principle
to resolve the joint user activity detection, channel estimation, and data
detection problem. Numerical results demonstrate the effectiveness of the
proposed algorithm in terms of the normalized mean-squared errors of channel
and data symbols, and the probability of misdetection.

</details>


### [290] [An Information-Theoretic Intersectional Data Valuation Theory](https://arxiv.org/abs/2507.14742)
*Eduardo C. Garrido-Merchán*

Main category: cs.IT

TL;DR: 本文提出了一种基于互信息的定价规则，用于量化交叉隐私损失，并通过Pigouvian式附加费减少有害数据交易。


<details>
  <summary>Details</summary>
Motivation: 在数字市场中，个人数据暴露导致隐私外部性，企业从中获利，而用户面临社会风险和歧视。

Method: 引入一种形式化的定价规则，利用互信息量化交叉隐私损失，并通过附加费内部化成本。

Result: 该方法独立于统计模型，可通过离散化联合概率分布实现，并可根据社会价值观调整附加费。

Conclusion: 该定价规则不仅解决了市场失灵，还为弱势群体提供了再分配保护。

Abstract: In contemporary digital markets, personal data often reveals not just
isolated traits, but complex, intersectional identities based on combinations
of race, gender, disability, and other protected characteristics. This exposure
generates a privacy externality: firms benefit economically from profiling,
prediction, and personalization, while users face hidden costs in the form of
social risk and discrimination. We introduce a formal pricing rule that
quantifies and internalizes this intersectional privacy loss using mutual
information, assigning monetary value to the entropy reduction induced by each
datum. The result is a Pigouvian-style surcharge that discourages harmful data
trades and rewards transparency. Our formulation has the advantage that it
operates independently of the underlying statistical model of the
intersectional variables, be it parametric, nonparametric, or learned, and can
be approximated in practice by discretizing the intersectional joint
probability distributions. We illustrate how regulators can calibrate this
surcharge to reflect different societal values, and argue that it provides not
just a technical fix to market failures, but also a redistributive shield that
empowers vulnerable groups in the face of asymmetric digital power.

</details>


### [291] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 论文研究了分层安全聚合（HSA）中的弱安全需求，提出了一种灵活框架（WS-HSA）以应对异构安全需求，并分析了最优密钥率。


<details>
  <summary>Details</summary>
Motivation: 分层安全聚合（HSA）在大规模用户场景下面临异构安全需求的挑战，例如不同用户群需要不同级别的输入保护。

Method: 提出弱安全HSA（WS-HSA）框架，允许自定义安全输入集和合谋集，以灵活满足异构需求。通过理论分析，确定了最优总密钥率。

Result: 针对不同参数配置，给出了最优密钥率的完整表征或上下界，提供了常数倍最优性保证。

Conclusion: WS-HSA为分层安全聚合中的异构安全需求提供了灵活且高效的解决方案，同时保证了最优密钥率。

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


### [292] [Enhancing Resilience Against Jamming Attacks: A Cooperative Anti-Jamming Method Using Direction Estimation](https://arxiv.org/abs/2507.14775)
*Amir Mehrabian,Georges Kaddoum*

Main category: cs.IT

TL;DR: 本文提出了一种基于多感知节点协作的抗干扰方法（CAJ），通过特征向量（EV）方法估计信道方向，在强干扰下表现优异，且能扩展到多干扰源场景。


<details>
  <summary>Details</summary>
Motivation: 无线通信的固有脆弱性需要增强其安全性，特别是在面对干扰攻击时。

Method: 提出了一种基于特征向量（EV）的方法，通过协作感知节点（SNs）估计信道方向，并分析了其在强干扰下的性能。

Result: 在强干扰下，EV-CAJ方法性能仅下降0.7 dB，且能扩展到多干扰源场景。在快衰落信道中，方法表现出良好的鲁棒性。

Conclusion: 所提出的EV-CAJ方法在抗干扰和信道适应性方面表现出色，适用于复杂无线环境。

Abstract: The inherent vulnerability of wireless communication necessitates strategies
to enhance its security, particularly in the face of jamming attacks. This
paper uses the collaborations of multiple sensing nodes (SNs) in the wireless
network to present a cooperative anti-jamming approach (CAJ) designed to
neutralize the impact of jamming attacks. We propose an eigenvector (EV) method
to estimate the direction of the channel vector from pilot symbols. Through our
analysis, we demonstrate that with an adequate number of pilot symbols, the
performance of the proposed EV method is comparable to the scenario where the
perfect channel state information (CSI) is utilized. Both analytical formulas
and simulations illustrate the excellent performance of the proposed EV-CAJ
under strong jamming signals. Considering severe jamming, the proposed EV-CAJ
method exhibits only a 0.7 dB degradation compared to the case without jamming
especially when the number of SNs is significantly larger than the number of
jamming nodes (JNs). Moreover, the extension of the proposed method can handle
multiple jammers at the expense of degrees of freedom (DoF). We also
investigate the method's ability to remain robust in fast-fading channels with
different coherence times. Our proposed approach demonstrates good resilience,
particularly when the ratio of the channel's coherence time to the time frame
is small. This is especially important in the case of mobile jammers with large
Doppler shifts.

</details>


### [293] [Enhancing Communications and Sensing Simultaneously by Zero-Order Optimization of MTS](https://arxiv.org/abs/2507.14794)
*Wenhai Lai,Kaiming Shen,Zhi-Quan Luo*

Main category: cs.IT

TL;DR: 论文提出一种无需信道状态信息（CSI）的统计方法，通过盲配置优化超表面相位偏移，提升信道强度并实现主动感知。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CSI，但实际中CSI难以获取，因此需要一种无需CSI的优化方法。

Method: 利用接收信号强度（RSS）数据提取无线环境特征，通过条件样本均值实现盲配置优化。

Result: 原型系统验证显示，算法在主动感知任务中优于基准方法（如MUSIC），并将信噪比（SNR）提升约10 dB。

Conclusion: 盲配置方法不仅优化了通信性能，还实现了发射机定位，仅需RSS数据即可实现。

Abstract: Metasurface (MTS) comprises an array of metaatoms, each reflecting and
inducing a phase shift into the incident wireless signal. We seek the optimal
combination of phase shifts across all the meta-atoms to maximize the channel
strength from transmitter to receiver. Unlike many existing works that heavily
rely on channel state information (CSI), this paper proposes a statistical
approach to the phase shift optimization in the absence of CSI, namely blind
configuration or zero-order optimization. The main idea is to extract the key
features of the wireless environment from the received signal strength (RSS)
data via conditional sample mean, with provable performance. Furthermore, as a
windfall profit, we show that the proposed blind configuration method has a
nontrivial connection to phase retrieval which can be utilized for active
sensing. In a nutshell, by configuring a pair of MTSs blindly without channel
estimation, we not only enhance the channel strength to facilitate wireless
communication, but also enable receiver to localize transmitter. All we need is
the RSS data that can be readily measured at receiver. Our algorithm is
verified in prototype systems in the 2.6 GHz spectral band. As shown in field
tests, the proposed algorithm outperforms the benchmarks (e.g., MUSIC) in the
active sensing task, and in the meanwhile raises the signal-to-noise ratio
(SNR) significantly by about 10 dB.

</details>


### [294] [A DPI-PAC-Bayesian Framework for Generalization Bounds](https://arxiv.org/abs/2507.14795)
*Muhan Guan,Farhad Farokhi,Jingge Zhu*

Main category: cs.IT

TL;DR: 提出了一种基于数据处理的PAC-Bayesian框架（DPI-PAC-Bayesian），用于监督学习中的泛化误差界限推导，通过结合数据不等式和变测技术，实现了更紧致的界限。


<details>
  <summary>Details</summary>
Motivation: 旨在通过信息论工具（如数据处理不等式）改进PAC-Bayesian框架，以提供更紧致的泛化误差界限，并消除经典界限中的冗余项。

Method: 将数据处理不等式嵌入变测技术，推导出基于Rényi散度和f-散度的泛化误差界限，并展示了与经典界限的联系。

Result: 提出了三种基于不同散度的界限，并在均匀先验下恢复了Occam's Razor界限，且消除了PAC-Bayes界限中的冗余项。

Conclusion: 该框架为泛化保证提供了灵活的信息论工具，同时连接了数据处理和PAC-Bayesian视角。

Abstract: We develop a unified Data Processing Inequality PAC-Bayesian framework --
abbreviated DPI-PAC-Bayesian -- for deriving generalization error bounds in the
supervised learning setting. By embedding the Data Processing Inequality (DPI)
into the change-of-measure technique, we obtain explicit bounds on the binary
Kullback-Leibler generalization gap for both R\'enyi divergence and any
$f$-divergence measured between a data-independent prior distribution and an
algorithm-dependent posterior distribution. We present three bounds derived
under our framework using R\'enyi, Hellinger \(p\) and Chi-Squared divergences.
Additionally, our framework also demonstrates a close connection with other
well-known bounds. When the prior distribution is chosen to be uniform, our
bounds recover the classical Occam's Razor bound and, crucially, eliminate the
extraneous \(\log(2\sqrt{n})/n\) slack present in the PAC-Bayes bound, thereby
achieving tighter results. The framework thus bridges data-processing and
PAC-Bayesian perspectives, providing a flexible, information-theoretic tool to
construct generalization guarantees.

</details>


### [295] [Rate-Distortion-Perception Trade-off with Strong Realism Constraints: Role of Side Information and Common Randomness](https://arxiv.org/abs/2507.14825)
*Yassine Hamdi,Aaron B. Wagner,Deniz Gündüz*

Main category: cs.IT

TL;DR: 论文探讨了在图像压缩中，生成模型的最新进展揭示了速率与感知质量之间的权衡，并研究了在强完美真实约束下的信息理论极限。


<details>
  <summary>Details</summary>
Motivation: 研究在强完美真实约束下，内存源序列压缩的信息理论极限，特别是在有边信息的情况下。

Method: 考虑了边信息仅在解码器或同时在编码器和解码器的情况，并分析了不同强完美真实约束的定义。

Result: 在边信息同时可用时，揭示了其双重作用；在高斯场景下，强完美真实约束仅在足够公共随机性可用时成立。

Conclusion: 强完美真实约束在边信息可用时对压缩速率有影响，且公共随机性在满足约束中起关键作用。

Abstract: In image compression, with recent advances in generative modeling, existence
of a trade-off between the rate and perceptual quality has been brought to
light, where the perceptual quality is measured by the closeness of the output
and source distributions. We consider the compression of a memoryless source
sequence $X^n=(X_1, \ldots, X_n)$ in the presence of memoryless side
information $Z^n=(Z_1, \ldots, Z_n),$ originally studied by Wyner and Ziv, but
elucidate the impact of a strong perfect realism constraint, which requires the
joint distribution of output symbols $Y^n=(Y_1,...,Y_n)$ to match the
distribution of the source sequence. We consider two cases: when $Z^n$ is
available only at the decoder, or at both the encoder and decoder, and
characterize the information theoretic limits under various scenarios. Previous
works show the superiority of randomized codes under strong perceptual quality
constraints. When $Z^n$ is available at both terminals, we characterize its
dual role, as a source of common randomness, and as a second look on the source
for the receiver. We also study different notions of strong perfect realism
which we call marginal realism, joint realism and near-perfect realism. We
derive explicit solutions when $X$ and $Z$ are jointly Gaussian under the
squared error distortion measure. In traditional lossy compression, having $Z$
only at the decoder imposes no rate penalty in the Gaussian scenario. We show
that, when strong perfect realism constraints are imposed this holds only when
sufficient common randomness is available.

</details>


### [296] [Variable Min-Cut Max-Flow Bounds and Algorithms in Finite Regime](https://arxiv.org/abs/2507.14852)
*Rivka Gitik,Alejandro Cohen*

Main category: cs.IT

TL;DR: 论文提出了一种利用计算几何工具分析异构网络中可变链路容量吞吐量的新框架，推导了新性能界限，并展示了增加链路数量可减少吞吐量变异性。


<details>
  <summary>Details</summary>
Motivation: 实际网络中链路容量常因动态条件波动，限制了源到目的地的最大容量，需要新的分析方法。

Method: 引入计算几何工具分析吞吐量，提出算法强制网络稳定性，并建议使用自适应无速率随机线性网络编码（AR-RLNC）缓解延迟-吞吐量权衡。

Result: 增加链路数量可减少吞吐量变异性近90%；不稳定图可能具有指数级数量的不同最小割集。

Conclusion: 提出的框架和算法有效提升了网络稳定性与性能，AR-RLNC进一步优化了延迟-吞吐量权衡。

Abstract: The maximum achievable capacity from source to destination in a network is
limited by the min-cut max-flow bound; this serves as a converse limit. In
practice, link capacities often fluctuate due to dynamic network conditions. In
this work, we introduce a novel analytical framework that leverages tools from
computational geometry to analyze throughput in heterogeneous networks with
variable link capacities in a finite regime. Within this model, we derive new
performance bounds and demonstrate that increasing the number of links can
reduce throughput variability by nearly $90\%$. We formally define a notion of
network stability and show that an unstable graph can have an exponential
number of different min-cut sets, up to $O(2^{|E|})$. To address this
complexity, we propose an algorithm that enforces stability with time
complexity $O(|E|^2 + |V|)$, and further suggest mitigating the
delay-throughput tradeoff using adaptive rateless random linear network coding
(AR-RLNC).

</details>


### [297] [Reconfigurable Antenna Arrays With Tunable Loads: Expanding Solution Space via Coupling Control](https://arxiv.org/abs/2507.15074)
*Elio Faddoul,Konstantinos Ntougias,Ioannis Krikidis*

Main category: cs.IT

TL;DR: 论文提出两种技术解决可重构天线阵列的互耦和端口选择问题，并通过算法优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法中，可重构天线阵列因互耦和端口选择问题导致性能受限，亟需新方法提升容量和效率。

Method: 1. 利用被动天线和可调负载增强增益；2. 全主动设计中消除互耦。开发贪婪和元启发式算法处理大规模配置。

Result: 数值模拟显示性能显著优于基准方法，并提供了量化负载下的鲁棒设计。

Conclusion: 提出的方法有效解决了互耦和端口选择问题，显著提升了系统性能。

Abstract: The emerging reconfigurable antenna (RA) array technology promises capacity
enhancement through dynamic antenna positioning. Traditional approaches enforce
half-wavelength or greater spacing among RA elements to avoid mutual coupling,
limiting the solution space. Additionally, achieving sufficient spatial channel
sampling requires numerous discrete RA positions (ports), while high-frequency
scenarios with hybrid processing demand many physical RAs to maintain array
gains. This leads to exponential growth in the solution space. We propose two
techniques to address the former challenge: (1) surrounding a limited number of
active RAs with passive ones terminated to tunable analog loads to
\textit{exploit} mutual coupling and increase array gain, and (2) employing
tunable loads on each RA in an all-active design to \textit{eliminate} mutual
coupling in the analog domain. Both methods enable arbitrary RA spacing,
unlocking the full solution space. Regarding the latter challenge, we develop
greedy and meta-heuristic port selection algorithms, alongside low-complexity
heuristic variants, that efficiently handle over $10^{20}$ array
configurations, and optimize the loading values to maximize the sum-rate in a
multiple-input single-output broadcast channel under transmission power
constraints, assuming a heuristic linear precoder. Furthermore, we analyze
performance degradation from quantized loads and propose corresponding robust
designs. Numerical simulations reveal significant performance gains over
benchmarks and provide valuable insights.

</details>


### [298] [Noise Quantification and Control in Circuits via Strong Data-Processing Inequalities](https://arxiv.org/abs/2507.15108)
*Chenyang Sun*

Main category: cs.IT

TL;DR: 本文探讨了强数据处理不等式（SPDI's）在噪声电路计算中的应用，改进了Evans和Schulman以及von Neumann的工作框架，并推广了von Neumann的分析到任意阶多数门。


<details>
  <summary>Details</summary>
Motivation: 研究噪声电路计算中的数据处理不等式，以改进现有框架并推广von Neumann的分析。

Method: 首先建立Evans和Schulman的框架，引入3-多数门控制噪声，并推广到任意阶多数门。

Result: 得出了噪声的上限和深度下限，证明了任意阶多数门的噪声阈值。

Conclusion: 通过改进框架和推广分析，为噪声电路计算提供了更通用的理论支持。

Abstract: This essay explores strong data-processing inequalities (SPDI's) as they
appear in the work of Evans and Schulman \cite{ES} and von Neumann \cite{vN} on
computing with noisy circuits. We first develop the framework in \cite{ES},
which leads to lower bounds on depth and upper bounds on noise that permit
reliable computation. We then introduce the $3$-majority gate, introduced by
\cite{vN} for the purpose of controlling noise, and obtain an upper bound on
noise necessary for its function. We end by generalizing von Neumann's analysis
to majority gates of any order, proving an analogous noise threshold and giving
a sufficient upper bound for order given a desired level of reliability.
  The presentation of material has been modified in a way deemed more natural
by the author, occasionally leading to simplifications of existing proofs.
Furthermore, many computations omitted from the original works have been worked
out, and some new commentary added. The intended audience has a rudimentary
understanding of information theory similar to that of the author.

</details>


### [299] [The Exact Parameters of A Family of BCH Codes](https://arxiv.org/abs/2507.15247)
*Zhonghua Sun*

Main category: cs.IT

TL;DR: 本文确定了窄带BCH码的精确最小距离和维度，解决了Li等人提出的三个开放问题，并对Ding的框架进行了补充。


<details>
  <summary>Details</summary>
Motivation: 尽管BCH码在理论和实践上具有重要意义，但其精确的最小距离和维度对许多家族仍然未知。本文旨在填补这一空白。

Method: 研究了窄带BCH码$\C_{(q, m, \lambda, \ell_0, \ell_1)}$，确定了其最小距离和维度。

Result: 建立了这些码的精确最小距离和维度，解决了Li等人的开放问题。

Conclusion: 本文为BCH码的研究提供了重要进展，解决了开放问题并补充了现有框架。

Abstract: Despite the theoretical and practical significance of BCH codes, the exact
minimum distance and dimension remain unknown for many families. This paper
establishes the precise minimum distance and dimension of narrow-sense BCH
codes $\C_{(q, m, \lambda, \ell_0, \ell_1)}$ over $\gf(q)$ of length
$\frac{q^m-1}{\lambda}$ and designed distance $\frac{(q-\lambda
\ell_0)q^{m-1-\ell_1}-1}{\lambda}$, where $\lambda\mid (q-1)$, $0\leq \ell_0<
\frac{q-1}{\lambda}$, and $0\leq \ell_1\leq m-1$. These results conclusively
resolve the three open problems posed by Li et al. (IEEE Trans. Inf. Theory,
vol. 63, no. 11, pp. 7219-7236, Nov. 2017) while establishing complementary
advances to Ding's seminal framework (IEEE Trans. Inf. Theory, vol. 61, no. 10,
pp. 5322-5330, Oct. 2015).

</details>


### [300] [A Novel Two-Dimensional Smoothing Algorithm](https://arxiv.org/abs/2507.15301)
*Xufeng Chen,Liang Yan,Xiaoshan Gao*

Main category: cs.IT

TL;DR: 提出了一种新的二维平滑（TDS）算法，用于二维序列的平滑和滤波，无需假设噪声分布类型，仅需调整一个参数即可有效提取趋势序列并减少噪声影响。


<details>
  <summary>Details</summary>
Motivation: 传统滤波算法依赖于滤波窗口的选择，限制了其适用性，因此需要一种更通用且简单的方法。

Method: 通过引入损失函数，将趋势序列定义为损失函数最小值时的解，将二维序列分解为趋势序列和波动序列。

Result: TDS算法在数值模拟和图像处理案例中验证了其准确性和有效性。

Conclusion: TDS算法是一种简单、通用且可靠的二维序列平滑和滤波方法。

Abstract: Smoothing and filtering two-dimensional sequences are fundamental tasks in
fields such as computer vision. Conventional filtering algorithms often rely on
the selection of the filtering window, limiting their applicability in certain
scenarios. To this end, we propose a novel Two-Dimensional Smoothing (TDS)
algorithm for the smoothing and filtering problem of two-dimensional sequences.
Typically, the TDS algorithm does not require assumptions about the type of
noise distribution. It is simple and easy to implement compared to conventional
filtering methods, such as 2D adaptive Wiener filtering and Gaussian filtering.
The TDS algorithm can effectively extract the trend contained in the
two-dimensional sequence and reduce the influence of noise on the data by
adjusting only a single parameter. In this work, unlike existing algorithms
that depend on the filtering window, we introduce a loss function, where the
trend sequence is identified as the solution when this loss function takes a
minimum value. Therefore, within the framework of the TDS algorithm, a general
two-dimensional sequence can be innovatively decomposed into a trend sequence
and a fluctuation sequence, in which the trend sequence contains the main
features of the sequence and the fluctuation sequence contains the detailed
features or noise interference of the sequence. To ensure the reliability of
the TDS algorithm, a crucial lemma is first established, indicating that the
trend sequence and fluctuation sequence obtained by the TDS algorithm are
existent and unique when the global smoothing parameter is determined. Three
modified algorithms are then proposed based on the TDS algorithm, with
corresponding lemmas and corollaries demonstrating their reliability. Finally,
the accuracy and effectiveness of the TDS algorithm are further verified
through numerical simulations and image processing cases.

</details>


### [301] [Cross Mutual Information](https://arxiv.org/abs/2507.15372)
*Chetan Gohil,Oliver M Cliff,James M. Shine,Ben D. Fulcher,Joseph T. Lizier*

Main category: cs.IT

TL;DR: 论文提出了一种名为“交叉互信息”的新方法，用于比较非平稳分布下两组样本中变量X和Y的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统互信息（MI）无法直接比较非平稳分布下两组样本的依赖关系，因此需要一种新的度量方法。

Method: 提出“交叉互信息”作为替代度量，并通过模拟研究验证其有效性。

Result: 模拟研究表明该方法能有效比较不同样本集的依赖关系，并与线性回归中的模型拟合度量相关。

Conclusion: 交叉互信息为分析非平稳分布下的依赖关系提供了新工具，未来可应用于神经影像数据分析。

Abstract: Mutual information (MI) is a useful information-theoretic measure to quantify
the statistical dependence between two random variables: $X$ and $Y$. Often, we
are interested in understanding how the dependence between $X$ and $Y$ in one
set of samples compares to another. Although the dependence between $X$ and $Y$
in each set of samples can be measured separately using MI, these estimates
cannot be compared directly if they are based on samples from a non-stationary
distribution. Here, we propose an alternative measure for characterising how
the dependence between $X$ and $Y$ as defined by one set of samples is
expressed in another, \textit{cross mutual information}. We present a
comprehensive set of simulation studies sampling data with $X$-$Y$ dependencies
to explore this measure. Finally, we discuss how this relates to measures of
model fit in linear regression, and some future applications in neuroimaging
data analysis.

</details>


### [302] [Galois equiangular tight frames from Galois self-dual codes](https://arxiv.org/abs/2507.15448)
*Junmin An,Jon-Lark Kim*

Main category: cs.IT

TL;DR: 论文研究了有限域上的框架理论，引入了Galois内积，定义了Galois框架及其相关概念，并构造了Galois等角紧框架。


<details>
  <summary>Details</summary>
Motivation: 扩展实数或复数域上的框架理论到有限域，结合Galois内积，探索新的框架理论。

Method: 定义Galois框架、Galois Gram矩阵、Galois框架算子等概念，并利用Galois自对偶码构造Galois等角紧框架。

Result: 成功构造了Galois等角紧框架，并给出了Galois自对偶码诱导Galois等角紧框架的条件。

Conclusion: 论文为有限域上的框架理论提供了新的工具和方法，扩展了框架理论的应用范围。

Abstract: Greaves et al. (2022) extended frames over real or complex numbers to frames
over finite fields. In this paper, we study the theory of frames over finite
fields by incorporating the Galois inner products introduced by Fan and Zhang
(2017), which generalize the Euclidean and Hermitian inner products. We define
a class of frames, called Galois frames over finite fields, along with related
notions such as Galois Gram matrices, Galois frame operators, and Galois
equiangular tight frames (Galois ETFs). We also characterize when Galois
self-dual codes induce Galois ETFs. Furthermore, we construct explicitly Galois
ETFs from Galois self-dual constacyclic codes.

</details>


### [303] [Estimating Rate-Distortion Functions Using the Energy-Based Model](https://arxiv.org/abs/2507.15700)
*Shitong Wu,Sicheng Xu,Lingyi Chen,Huihui Wu,Wenyi Zhang*

Main category: cs.IT

TL;DR: 论文提出了一种基于能量建模的新框架，用于解决高维率失真（RD）问题，通过连接RD对偶形式和统计物理中的自由能量，有效重建最优条件分布。


<details>
  <summary>Details</summary>
Motivation: 传统Blahut-Arimoto算法在高维场景下计算困难，现有神经方法常忽略最优条件分布重建或依赖不合理假设。

Method: 提出基于能量建模的框架，仅需训练单一神经网络，避免MCMC采样中归一化因子的计算挑战。

Result: 实验证明该算法在高维RD函数估计和最优条件分布重建中显著有效。

Conclusion: 新框架为高维RD问题提供了高效解决方案，避免了传统方法的局限性。

Abstract: The rate-distortion (RD) theory is one of the key concepts in information
theory, providing theoretical limits for compression performance and guiding
the source coding design, with both theoretical and practical significance. The
Blahut-Arimoto (BA) algorithm, as a classical algorithm to compute RD
functions, encounters computational challenges when applied to high-dimensional
scenarios. In recent years, many neural methods have attempted to compute
high-dimensional RD problems from the perspective of implicit generative
models. Nevertheless, these approaches often neglect the reconstruction of the
optimal conditional distribution or rely on unreasonable prior assumptions. In
face of these issues, we propose an innovative energy-based modeling framework
that leverages the connection between the RD dual form and the free energy in
statistical physics, achieving effective reconstruction of the optimal
conditional distribution.The proposed algorithm requires training only a single
neural network and circumvents the challenge of computing the normalization
factor in energy-based models using the Markov chain Monte Carlo (MCMC)
sampling. Experimental results demonstrate the significant effectiveness of the
proposed algorithm in estimating high-dimensional RD functions and
reconstructing the optimal conditional distribution.

</details>


### [304] [Remote Channel Synthesis](https://arxiv.org/abs/2507.15757)
*Yassine Hamdi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 论文研究了在无记忆信道中合成编码器与解码器之间的协调问题，提出了最优压缩和公共随机性速率的单字母表征，并指出低公共随机性率下传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 解决在部分或噪声观测下，编码器与解码器如何协调输出以匹配远程源分布的问题。

Method: 通过无噪声链路传输编码信息，利用公共随机性资源，分析压缩和公共随机性速率的最优组合。

Result: 给出了最优速率单字母表征，证明低公共随机性率下传统方法严格次优。

Conclusion: 协调问题需要新的方法，传统方案在低公共随机性率下效率不足。

Abstract: We consider the problem of synthesizing a memoryless channel between an
unobserved source and a remote terminal. An encoder has access to a partial or
noisy version $Z^n = (Z_1, \ldots, Z_n)$ of a remote source sequence $X^n =
(X_1, \ldots, X_n),$ with $(X_i,Z_i)$ independent and identically distributed
with joint distribution $q_{X,Z}.$ The encoder communicates through a noiseless
link to a decoder which aims to produce an output $Y^n$ coordinated with the
remote source; that is, the total variation distance between the joint
distribution of $X^n$ and $Y^n$ and some i.i.d. target distribution
$q_{X,Y}^{\otimes n}$ is required to vanish as $n$ goes to infinity. The two
terminals may have access to a source of rate-limited common randomness. We
present a single-letter characterization of the optimal compression and common
randomness rates. We also show that when the common randomness rate is small,
then in most cases, coordinating $Z^n$ and $Y^n$ using a standard channel
synthesis scheme is strictly sub-optimal. In other words, schemes for which the
joint distribution of $Z^n$ and $Y^n$ approaches a product distribution
asymptotically are strictly sub-optimal.

</details>


### [305] [The Capacity of Semantic Private Information Retrieval with Colluding Servers](https://arxiv.org/abs/2507.15818)
*Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了语义私有信息检索（Sem-PIR）在多服务器共谋（Sem-TPIR）下的问题，推导了其检索率的上界并设计了一个达到该上界的方案。


<details>
  <summary>Details</summary>
Motivation: 传统PIR假设消息大小和检索概率相同，而Sem-TPIR考虑了消息大小和检索概率不同的情况，并扩展到多服务器共谋场景。

Method: 针对任意$T < N$的共谋参数，设计了达到检索率上界的方案。

Result: 推导了Sem-TPIR的精确容量，并证明了所提方案的最优性。

Conclusion: 本文解决了多服务器共谋下的语义私有信息检索问题，为实际应用提供了理论基础。

Abstract: We study the problem of semantic private information retrieval (Sem-PIR) with
$T$ colluding servers (Sem-TPIR), i.e., servers that collectively share user
queries. In Sem-TPIR, the message sizes are different, and message retrieval
probabilities by any user are not uniform. This is a generalization of the
classical PIR problem where the message sizes are equal and message retrieval
probabilities are identical. The earlier work on Sem-PIR considered the case of
no collusions, i.e., the collusion parameter of $T=1$. In this paper, we
consider the general problem for arbitrary $T < N$. We find an upper bound on
the retrieval rate and design a scheme that achieves this rate, i.e., we derive
the exact capacity of Sem-TPIR.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [306] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA是一个结合大型语言模型和混合检索技术的模式匹配框架，无需标注数据或成对比较即可高效识别候选匹配，显著提升匹配准确性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 模式匹配是集成异构数据源和增强数据集发现的关键，但现有方法复杂且资源密集。

Method: SCHEMORA采用基于提示的方法，结合大型语言模型和混合检索技术，通过丰富模式元数据并利用向量和词汇检索来改进匹配。

Result: 在MIMIC-OMOP基准测试中，SCHEMORA在HitRate@5和HitRate@3上分别提升7.49%和3.75%，达到最新性能。

Conclusion: SCHEMORA是首个基于LLM的开源模式匹配方法，其分析强调了检索的关键作用，并为模型选择提供了实用指导。

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


### [307] [Towards Temporal Knowledge Graph Alignment in the Wild](https://arxiv.org/abs/2507.14475)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Xiang Zhao,Jiuyang Tang,Lei Chen*

Main category: cs.DB

TL;DR: 论文提出了HyDRA方法，用于解决现实世界中多尺度时间知识图谱对齐（TKGA-Wild）的挑战，并通过新数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设时间元素标准统一且结构简化，无法处理现实中的多尺度时间元素纠缠和跨源时间结构不平衡问题。

Method: HyDRA通过多尺度超图检索增强生成任务，并设计尺度交织协同机制，解决时间不完整性和事件密度分布不均问题。

Result: HyDRA在多个基准测试中表现优异，优于24个基线方法，且效率高、可扩展性强。

Conclusion: HyDRA为TKGA-Wild提供了新范式，新数据集BETA和WildBETA更能反映现实挑战。

Abstract: Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent
entities across heterogeneous temporal knowledge graphs (TKGs) for fusion to
improve their completeness. Although some approaches have been proposed to
tackle this task, most assume unified temporal element standards and simplified
temporal structures across different TKGs. They cannot deal with TKGA in the
wild (TKGA-Wild), where multi-scale temporal element entanglement and
cross-source temporal structural imbalances are common. To bridge this gap, we
study the task of TKGA-Wild and propose HyDRA, a new and effective solution.
HyDRA is the first to reformulate the task via multi-scale hypergraph
retrieval-augmented generation to address the challenges of TKGA-Wild.In
addition, we design a new scale-weave synergy mechanism for HyDRA, which
incorporates intra-scale interactions and cross-scale conflict detection. This
mechanism is designed to alleviate the fragmentation caused by multi-source
temporal incompleteness and resolves inconsistencies arising from complex and
uneven temporal event density distributions, thereby enhancing the model
capacity to handle the intricacies of real-world temporal alignment. Finally,
there is no standard benchmark that captures these challenges of TKGA-Wild and
effectively evaluates existing methods. To this end, we formally propose to
benchmark challenges for TKGA-Wild and validate the effectiveness of the method
by establishing two new datasets(BETA and WildBETA). Extensive experiments on
the new datasets and six representative benchmarks show that BETA and WildBETA
better reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm
for TKGA-Wild, consistently outperforming 24 competitive baselines, while
maintaining strong efficiency and scalability.

</details>


### [308] [Opening The Black-Box: Explaining Learned Cost Models For Databases](https://arxiv.org/abs/2507.14495)
*Roman Heinrich,Oleksandr Havrylov,Manisha Luthra,Johannes Wehrstein,Carsten Binnig*

Main category: cs.DB

TL;DR: 论文提出了一种解释学习成本模型（LCMs）的方法，通过AI可解释性技术解决LCMs预测误差大的问题。


<details>
  <summary>Details</summary>
Motivation: LCMs在数据库成本预测中表现优越，但对某些查询计划的预测误差较大，且因其复杂的深度神经网络模型难以理解和调试。

Method: 开发了新的解释技术，扩展了现有AI模型的可解释性方法，并针对LCMs进行了适配。

Result: 提供了一个交互式工具，展示LCMs的可解释性。

Conclusion: 这是使LCMs可调试的第一步，为系统修复LCMs问题铺平了道路。

Abstract: Learned Cost Models (LCMs) have shown superior results over traditional
database cost models as they can significantly improve the accuracy of cost
predictions. However, LCMs still fail for some query plans, as prediction
errors can be large in the tail. Unfortunately, recent LCMs are based on
complex deep neural models, and thus, there is no easy way to understand where
this accuracy drop is rooted, which critically prevents systematic
troubleshooting. In this demo paper, we present the very first approach for
opening the black box by bringing AI explainability approaches to LCMs. As a
core contribution, we developed new explanation techniques that extend existing
methods that are available for the general explainability of AI models and
adapt them significantly to be usable for LCMs. In our demo, we provide an
interactive tool to showcase how explainability for LCMs works. We believe this
is a first step for making LCMs debuggable and thus paving the road for new
approaches for systematically fixing problems in LCMs.

</details>


### [309] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: IDSS是一种基于P2P网络和嵌入式关系数据库的新型大规模数据存储工具，旨在解决传统数据库管理系统在可扩展性和处理异构数据时的低效问题。


<details>
  <summary>Details</summary>
Motivation: 数据生成速度的快速增长对数据管理提出了挑战，传统数据库管理系统在可扩展性和处理大规模异构数据时效率低下。

Method: IDSS利用P2P网络和嵌入式关系数据库架构，支持分布式查询和复杂查询处理，基于通用模式实现高效数据管理。

Result: IDSS提供了一种高效、可扩展的大规模数据存储解决方案，支持复杂分布式查询处理。

Conclusion: IDSS通过结合P2P网络和关系数据库，为大规模异构数据管理提供了一种高效且可扩展的工具。

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


### [310] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: Mayura框架通过Motif-Group Tree（MG-Tree）统一挖掘多个时间图模式，减少冗余计算，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法独立处理每个时间图模式查询，导致冗余计算，Mayura旨在解决这一问题。

Method: 提出MG-Tree层次数据结构，设计共挖掘算法，支持CPU和GPU架构。

Result: 实验显示Mayura在CPU和GPU上分别平均提速2.4倍和1.7倍，保持精确性。

Conclusion: Mayura通过结构共享显著提升时间图模式挖掘效率，适用于高要求应用。

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [311] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: FAMST是一种快速近似最小生成树算法，通过三阶段方法显著降低计算复杂度，适用于大规模高维数据集。


<details>
  <summary>Details</summary>
Motivation: 解决传统最小生成树算法在大规模和高维数据集上的计算挑战。

Method: 采用三阶段方法：近似最近邻图构建、ANN组件间连接和迭代边优化。

Result: 时间复杂度和空间复杂度显著降低，实验显示速度提升高达1000倍，且近似误差低。

Conclusion: FAMST扩展了MST技术的应用范围，使其适用于以前不可行的数据集规模。

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [312] [Tighter Lower Bounds for Single Source Personalized PageRank](https://arxiv.org/abs/2507.14462)
*Xinpeng Jiang,Haoyu Liu,Siqiang Luo,Xiaokui Xiao*

Main category: cs.DS

TL;DR: 论文研究了单源个性化PageRank（SSPPR）查询的近似下界，提出了更紧的下界结果。


<details>
  <summary>Details</summary>
Motivation: 现有下界结果较为宽松，论文旨在填补这一空白，为SSPPR-R和SSPPR-A提供更紧的下界。

Method: 通过理论分析，针对相对误差（SSPPR-R）和加性误差（SSPPR-A）分别推导出新的下界。

Result: 对于SSPPR-R，下界为Ω(min(m, log(1/δ)/δ))；对于SSPPR-A，下界为Ω(min(m, log(1/ϵ)/ϵ))。

Conclusion: 论文成功填补了现有下界结果的空白，为SSPPR查询提供了更精确的理论支持。

Abstract: We study lower bounds for approximating the Single Source Personalized
PageRank (SSPPR) query, which measures the probability distribution of an
$\alpha$-decay random walk starting from a source node $s$. Existing lower
bounds remain loose-$\Omega\left(\min(m, 1/\delta)\right)$ for relative error
(SSPPR-R) and $\Omega\left(\min(n, 1/\epsilon)\right)$ for additive error
(SSPPR-A). To close this gap, we establish tighter bounds for both settings.
For SSPPR-R, we show a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\delta)}{\delta}\right)\right)$ for any $\delta \in (0,1)$. For
SSPPR-A, we prove a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\epsilon)}{\epsilon}\right)\right)$ for any $\epsilon \in (0,1)$,
assuming the graph has $m \in \mathcal{O}(n^{2-\beta})$ edges for any
arbitrarily small constant $\beta \in (0,1)$.

</details>


### [313] [New Algorithms for #2-SAT and #3-SAT](https://arxiv.org/abs/2507.14504)
*Junqiang Peng,Zimo Sheng,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文改进了加权#2-SAT和#3-SAT问题的算法，分别将时间复杂度降至O*(1.1082^m)和O*(1.4423^m)，并引入了新的归约规则和路径分解方法。


<details>
  <summary>Details</summary>
Motivation: 解决加权#2-SAT和#3-SAT问题的效率问题，并改进现有算法的性能。

Method: 引入新的归约规则、优化分支操作分析，并应用公式的原始和对偶图的路径分解。

Result: 加权#2-SAT和#3-SAT的时间复杂度分别降至O*(1.1082^m)和O*(1.4423^m)，显著优于之前的结果。

Conclusion: 新方法在加权和未加权情况下均显著提升了算法效率，为SAT问题的求解提供了更优方案。

Abstract: The #2-SAT and #3-SAT problems involve counting the number of satisfying
assignments (also called models) for instances of 2-SAT and 3-SAT,
respectively. In 2010, Zhou et al. proposed an $\mathcal{O}^*(1.1892^m)$-time
algorithm for #2-SAT and an efficient approach for #3-SAT, where $m$ denotes
the number of clauses. In this paper, we show that the weighted versions of
#2-SAT and #3-SAT can be solved in $\mathcal{O}^*(1.1082^m)$ and
$\mathcal{O}^*(1.4423^m)$ time, respectively. These results directly apply to
the unweighted cases and achieve substantial improvements over the previous
results. These advancements are enabled by the introduction of novel reduction
rules, a refined analysis of branching operations, and the application of path
decompositions on the primal and dual graphs of the formula.

</details>


### [314] [Addressing Bias in Algorithmic Solutions: Exploring Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2507.14509)
*Sheikh Shakil Akhtar,Jayakrishnan Madathil,Pranabendu Misra,Geevarghese Philip*

Main category: cs.DS

TL;DR: 研究组合优化时，通常忽略的“附加信息”可能对用户至关重要。本文提出寻找对特定子群体“无偏”的解。


<details>
  <summary>Details</summary>
Motivation: 现实问题中，被忽略的附加信息可能对某些子群体有重大影响，需要避免算法输出的偏见。

Method: 提出一种方法，寻找在组合优化问题中对指定子群体无偏的解。

Result: 未明确提及具体结果，但目标是实现无偏解。

Conclusion: 强调在组合优化中考虑子群体影响的重要性，以实现更公平的解。

Abstract: A typical goal of research in combinatorial optimization is to come up with
fast algorithms that find optimal solutions to a computational problem. The
process that takes a real-world problem and extracts a clean mathematical
abstraction of it often throws out a lot of "side information" which is deemed
irrelevant. However, the discarded information could be of real significance to
the end-user of the algorithm's output. All solutions of the same cost are not
necessarily of equal impact in the real-world; some solutions may be much more
desirable than others, even at the expense of additional increase in cost. If
the impact, positive or negative, is mostly felt by some specific (minority)
subgroups of the population, the population at large will be largely unaware of
it. In this work we ask the question of finding solutions to combinatorial
optimization problems that are "unbiased" with respect to a collection of
specified subgroups of the total population.

</details>


### [315] [Characterizing and Testing Configuration Stability in Two-Dimensional Threshold Cellular Automata](https://arxiv.org/abs/2507.14569)
*Yonatan Nakar,Dana Ron*

Main category: cs.DS

TL;DR: 论文研究了二维环面上基于阈值规则的细胞自动机稳定配置的特性与测试问题，重点分析了不同阈值规则下的稳定配置结构，并提出了一种测试算法。


<details>
  <summary>Details</summary>
Motivation: 探讨不同阈值规则下细胞自动机稳定配置的特性，并设计高效的测试算法以区分稳定配置与接近稳定的配置。

Method: 首先分析了Threshold-2和Threshold-3规则下的稳定配置结构，随后设计了一种查询复杂度与配置大小无关、与1/ϵ²相关的测试算法。

Result: 成功表征了Threshold-2和Threshold-3规则下的稳定配置结构，并提出了高效的测试算法。

Conclusion: 论文为二维环面上基于阈值规则的细胞自动机稳定配置提供了理论分析和实用测试方法。

Abstract: We consider the problems of characterizing and testing the stability of
cellular automata configurations that evolve on a two-dimensional torus
according to threshold rules with respect to the von-Neumann neighborhood.
While stable configurations for Threshold-1 (OR) and Threshold-5 (AND) are
trivial (and hence easily testable), the other threshold rules exhibit much
more diverse behaviors. We first characterize the structure of stable
configurations with respect to the Threshold-2 (similarly, Threshold-4) and
Threshold-3 (Majority) rules. We then design and analyze a testing algorithm
that distinguishes between configurations that are stable with respect to the
Threshold-2 rule, and those that are $\epsilon$-far from any stable
configuration, where the query complexity of the algorithm is independent of
the size of the configuration and depends quadratically on $1/\epsilon$.

</details>


### [316] [A Black-Box Approach for Exogenous Replenishment in Online Resource Allocation](https://arxiv.org/abs/2507.14812)
*Suho Kang,Ziyang Liu,Rajan Udwani*

Main category: cs.DS

TL;DR: 论文提出了一种黑盒方法，将现有不考虑资源补充的在线资源分配算法扩展为适用于任意（对抗性或随机性）补充过程的算法。


<details>
  <summary>Details</summary>
Motivation: 解决在线资源分配问题中资源随时间补充但补充过程未知的情况，扩展现有算法的适用性。

Method: 引入黑盒方法，将现有算法扩展为支持任意补充过程的算法，同时保持原有竞争比。

Result: 在初始库存较大的情况下，扩展后的算法保持了原有算法的竞争比。

Conclusion: 该方法成功将外生补充过程无缝集成到现有算法中，适用于对抗性和随机性到达模型。

Abstract: In a typical online resource allocation problem, we start with a fixed
inventory of resources and make online allocation decisions in response to
resource requests that arrive sequentially over a finite horizon. We consider
settings where the inventory is replenished over time according to an unknown
exogenous process. We introduce black-box methods that extend any existing
algorithm, originally designed without considering replenishment, into one that
works with an arbitrary (adversarial or stochastic) replenishment process. Our
approach preserves the original algorithm's competitive ratio in regimes with
large initial inventory, thereby enabling the seamless integration of exogenous
replenishment into a large body of existing algorithmic results for both
adversarial and stochastic arrival models.

</details>


### [317] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: 论文提出了一种差分隐私（DP）合成图生成方法，用于近似输入图的三角形子图大小，并提供了误差界限。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保护隐私的同时生成近似原始图三角形子图大小的合成图，应用于图聚类、稀疏化和社会网络分析等领域。

Method: 提出首个多项式时间的(ε,δ)-DP机制，生成合成图，近似所有切割的三角形子图大小，误差为Õ(√(mℓ₃(G))n/ε^(3/2))。

Result: 算法在多项式时间内生成合成图，误差界限为Õ(√(mℓ₃(G))n/ε^(3/2))，并提供了误差下限Ω(√(mn)ℓ₃(G)/ε)。

Conclusion: 该算法适用于加权图，且误差界限适用于任何常数h≥2的K_h子图切割。

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [318] [Predict, Reposition, and Allocate: A Greedy and Flow-Based Architecture for Sustainable Urban Food Delivery](https://arxiv.org/abs/2507.15282)
*Aqsa Ashraf Makhdomi,Iqra Altaf Gillani*

Main category: cs.DS

TL;DR: 本文提出了一种新型环保食品配送优化框架，通过需求预测、配送路径规划和订单分配，减少环境影响并保持服务效率。


<details>
  <summary>Details</summary>
Motivation: 食品配送平台的快速发展增加了温室气体排放，现有优化机制未将环境可持续性纳入目标，导致次优结果。

Method: 利用目标函数的次模性和单调性设计高效贪婪优化算法，将订单分配问题建模为网络流优化模型，并设计三层网络架构匹配订单与配送人员。

Result: 提出的框架减少了车辆数量，构建了可持续的食品配送生态系统。

Conclusion: 该研究为食品配送行业提供了一种兼顾效率与环保的优化方法。

Abstract: The rapid proliferation of food delivery platforms has reshaped urban
mobility but has also contributed significantly to environmental degradation
through increased greenhouse gas emissions. Existing optimization mechanisms
produce sub-optimal outcomes as they do not consider environmental
sustainability their optimization objective. This study proposes a novel
eco-friendly food delivery optimization framework that integrates demand
prediction, delivery person routing, and order allocation to minimize
environmental impact while maintaining service efficiency. Since recommending
routes is NP-Hard, the proposed approach utilizes the submodular and monotone
properties of the objective function and designs an efficient greedy
optimization algorithm. Thereafter, it formulates order allocation problem as a
network flow optimization model, which, to the best of our knowledge, has not
been explored in the context of food delivery. A three-layered network
architecture is designed to match orders with delivery personnel based on
capacity constraints and spatial demand. Through this framework, the proposed
approach reduces the vehicle count, and creates a sustainable food delivery
ecosystem.

</details>


### [319] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: 论文解决了语言生成在极限情况下的联合封闭性问题，并进一步研究了噪声、样本缺失和反馈等变体的性质。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成在极限情况下的联合封闭性，并探索噪声、样本缺失和反馈对生成能力的影响。

Method: 通过构造反例否定联合封闭性，并分析噪声、样本缺失和反馈模型的等价性与分离性。

Result: 证明了联合封闭性不成立，噪声与非噪声生成存在分离，且有限反馈无额外能力而无限反馈更强。

Conclusion: 论文通过技术手段解决了核心问题，并对多种变体提供了精确刻画。

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


### [320] [1.64-Approximation for Chromatic Correlation Clustering via Chromatic Cluster LP](https://arxiv.org/abs/2507.15417)
*Dahoon Lee,Chenglin Fan,Euiwoong Lee*

Main category: cs.DS

TL;DR: 本文提出了一种随机化的1.64近似算法，用于解决Chromatic Correlation Clustering（CCC）问题，显著改进了之前的2.15近似比。


<details>
  <summary>Details</summary>
Motivation: CCC通过引入多类别边关系（颜色）和聚类上的色约束，扩展了传统的相关性聚类，但标准LP松弛的局限性使得改进其近似比困难。

Method: 扩展了聚类LP框架到色约束场景，引入了色聚类LP松弛和结合聚类与贪心策略的舍入算法。

Result: 算法实现了1.64的近似比，突破了标准LP在CCC问题上的积分间隙2的限制。

Conclusion: 展示了聚类LP框架在处理其他聚类问题变体中的潜力。

Abstract: Chromatic Correlation Clustering (CCC) generalizes Correlation Clustering by
assigning multiple categorical relationships (colors) to edges and imposing
chromatic constraints on the clusters. Unlike traditional Correlation
Clustering, which only deals with binary $(+/-)$ relationships, CCC captures
richer relational structures. Despite its importance, improving the
approximation for CCC has been difficult due to the limitations of standard LP
relaxations. We present a randomized $1.64$-approximation algorithm to the CCC
problem, significantly improving the previous factor of $2.15$. Our approach
extends the cluster LP framework to the chromatic setting by introducing a
chromatic cluster LP relaxation and an rounding algorithm that utilizes both a
cluster-based and a greedy pivot-based strategy. The analysis bypasses the
integrality gap of $2$ for the CCC version of standard LP and highlights the
potential of the cluster LP framework to address other variants of clustering
problems.

</details>


### [321] [Job Scheduling under Base and Additional Fees, with Applications to Mixed-Criticality Scheduling](https://arxiv.org/abs/2507.15434)
*Yi-Ting Hsieh,Mong-Jen Kao,Jhong-Yun Liu,Hung-Lung Wang*

Main category: cs.DS

TL;DR: 论文研究了将n个任务调度到m台相同机器上的问题，目标是最小化总机器工作时间。提出了FFD算法实现1.5近似比，并证明了问题存在PTAS。


<details>
  <summary>Details</summary>
Motivation: 解决任务调度中最小化总机器工作时间的优化问题，为实际应用提供高效算法。

Method: 使用First Fit Decreasing (FFD)算法，并证明其近似比为1.5；进一步提出多项式时间近似方案(PTAS)。

Result: FFD算法实现了1.5近似比，问题存在PTAS，且在混合关键性系统调度中应用效果更优。

Conclusion: FFD算法和PTAS为解决任务调度问题提供了高效且近似最优的解决方案，适用于混合关键性系统。

Abstract: We are concerned with the problem of scheduling $n$ jobs onto $m$ identical
machines. Each machine has to be in operation for a prescribed time, and the
objective is to minimize the total machine working time. Precisely, let $c_i$
be the prescribed time for machine $i$, where $i\in[m]$, and $p_j$ be the
processing time for job $j$, where $j\in[n]$. The problem asks for a schedule
$\sigma\colon\, J\to M$ such that $\sum_{i=1}^m\max\{c_i,
\sum_{j\in\sigma^{-1}(i)}p_j\}$ is minimized, where $J$ and $M$ denote the sets
of jobs and machines, respectively. We show that First Fit Decreasing (FFD)
leads to a $1.5$-approximation, and this problem admits a polynomial-time
approximation scheme (PTAS). The idea is further applied to mixed-criticality
system scheduling to yield improved approximation results.

</details>


### [322] [An $n^{O(\log\log n)}$ time approximation scheme for capacitated VRP in the Euclidean plane](https://arxiv.org/abs/2507.15549)
*René Sitters*

Main category: cs.DS

TL;DR: 提出了一种针对欧几里得平面上的容量车辆路径问题（CVRP）的准多项式时间近似方案（Q-PTAS），运行时间为n^{f(ε)·log log n}，显著优于现有最佳结果。


<details>
  <summary>Details</summary>
Motivation: 解决CVRP在欧几里得平面上的高效近似计算问题，推动PTAS的发展。

Method: 首先将CVRP转化为无容量限制的m路径问题，然后为m路径问题设计Q-PTAS。

Result: 实现了运行时间为n^{f(ε)·log log n}的Q-PTAS，为PTAS的实现迈出重要一步。

Conclusion: 该算法为欧几里得CVRP的PTAS提供了潜在途径，m路径问题的PTAS可能直接导致CVRP的PTAS。

Abstract: We present a quasi polynomial time approximation scheme (Q-PTAS) for the
capacitated vehicle routing problem (CVRP) on $n$ points in the Euclidean plane
for arbitrary capacity $c$. The running time is $n^{f(\epsilon)\cdot\log\log
n}$ for any $c$, and where $f$ is a function of $\epsilon$ only. This is a
major improvement over the so far best known running time of
$n^{\log^{O(1/\epsilon)}n}$ time and a big step towards a PTAS for Euclidean
CVRP.
  In our algorithm, we first give a polynomial time reduction of the CVRP in
$\mathbb{R}^d$ (for any fixed $d$) to an uncapacitated routing problem in
$\mathbb{R}^d$ that we call the $m$-paths problem. Here, one needs to find
exactly $m$ paths between two points $a$ and $b$, covering all the given points
in the Euclidean space. We then give a Q-PTAS for the $m$-paths problem in the
pane. Any PTAS for the (arguably easier to handle) Euclidean $m$-paths problem
is most likely to imply a PTAS for the Euclidean CVRP.

</details>


### [323] [Fast Algorithms for Graph Arboricity and Related Problems](https://arxiv.org/abs/2507.15598)
*Ruoxu Cen,Henry Fleischmann,George Z. Li,Jason Li,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 提出了一种在加权无向图中计算树覆盖数（arboricity）的算法，时间复杂度为√n m^(1+o(1))，改进了之前的结果。同时提出了一种计算整个割层次结构的新算法。


<details>
  <summary>Details</summary>
Motivation: 改进现有算法的时间复杂度，特别是在加权和无权图中计算树覆盖数和割层次结构的效率。

Method: 通过调用对数次数的有向全局最小割子程序来优化算法；同时提出了一种新的割层次结构计算方法。

Result: 算法的时间复杂度显著优于之前的结果，且若最小割问题的时间复杂度进一步优化，树覆盖数算法的性能将进一步提升。

Conclusion: 提出的算法在树覆盖数和割层次结构计算中实现了更优的时间复杂度，为相关领域提供了更高效的解决方案。

Abstract: We give an algorithm for finding the arboricity of a weighted, undirected
graph, defined as the minimum number of spanning forests that cover all edges
of the graph, in $\sqrt{n} m^{1+o(1)}$ time. This improves on the previous best
bound of $\tilde{O}(nm)$ for weighted graphs and $\tilde{O}(m^{3/2}) $ for
unweighted graphs (Gabow 1995) for this problem. The running time of our
algorithm is dominated by a logarithmic number of calls to a directed global
minimum cut subroutine -- if the running time of the latter problem improves to
$m^{1+o(1)}$ (thereby matching the running time of maximum flow), the running
time of our arboricity algorithm would improve further to $m^{1+o(1)}$.
  We also give a new algorithm for computing the entire cut hierarchy --
laminar multiway cuts with minimum cut ratio in recursively defined induced
subgraphs -- in $m n^{1+o(1)}$ time. The cut hierarchy yields the ideal edge
loads (Thorup 2001) in a fractional spanning tree packing of the graph which,
we show, also corresponds to a max-entropy solution in the spanning tree
polytope. For the cut hierarchy problem, the previous best bound was
$\tilde{O}(n^2 m)$ for weighted graphs and $\tilde{O}(n m^{3/2})$ for
unweighted graphs.

</details>


### [324] [On zeros and algorithms for disordered systems: mean-field spin glasses](https://arxiv.org/abs/2507.15616)
*Ferenc Bencs,Kuikui Liu,Guus Regts*

Main category: cs.DS

TL;DR: 论文设计了确定性拟多项式时间算法，用于在平均场设置中高精度估计自旋玻璃的配分函数，适用于几乎所有第二矩区域的反温度。


<details>
  <summary>Details</summary>
Motivation: 自旋玻璃在统计物理、计算复杂性和高维统计推断中具有核心地位，研究其配分函数有助于理解这些领域。

Method: 通过研究配分函数的零点位置，设计确定性拟多项式时间算法。

Result: 算法在Sherrington-Kirkpatrick模型的几乎整个复制对称相中成功。

Conclusion: 方法概念简单，适用于球形和伊辛自旋情况。

Abstract: Spin glasses are fundamental probability distributions at the core of
statistical physics, the theory of average-case computational complexity, and
modern high-dimensional statistical inference. In the mean-field setting, we
design deterministic quasipolynomial-time algorithms for estimating the
partition function to arbitrarily high accuracy for nearly all inverse
temperatures in the second moment regime. In particular, for the
Sherrington--Kirkpatrick model, our algorithms succeed for almost the entire
replica-symmetric phase. To achieve this, we study the locations of the zeros
of the partition function. Notably, our methods are conceptually simple, and
apply equally well to the spherical case and the case of Ising spins.

</details>


### [325] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: 研究团队探索未知树的分布式异步算法，提出两种算法，分别实现线性遗憾和竞争比优化，并给出新的下界。


<details>
  <summary>Details</summary>
Motivation: 解决现有集体树探索算法中分布式与异步无法兼顾的问题，提升探索效率。

Method: 提出两种分布式异步算法，分别优化移动次数和竞争比，并通过理论分析证明其性能。

Result: 算法分别实现2n+O(k²2ᵏD)和O(k/log k)(n+kD)的移动次数，竞争比为O(k/log k)，并给出Ω(log²k)的新下界。

Conclusion: 算法在分布式异步环境下表现优异，下界结果改进了现有理论。

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [326] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 论文提出了一种基于量子场论类比的理论框架“自由意志方程”，旨在为AGI决策过程引入受控随机性，以提升适应性和创造性。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，但人类智能具有自发性决策能力，这对创造力和适应性至关重要。

Method: 借鉴量子场论，将AI认知状态视为潜在行动的叠加态，通过类似量子波函数坍缩的机制实现随机决策。

Result: 在非稳态多臂老虎机环境中，该框架的代理表现出更高的奖励和策略多样性。

Conclusion: 该框架为AGI引入受控随机性提供了新思路，可能提升其适应性和创造性。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [327] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM代理实现材料发现的高通量、高保真模拟，减少对人类专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统错误处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划代理和领域专用代理，实现原子结构生成、DFT收敛测试、HPC调度和错误处理。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并量化了功能驱动的不确定性。

Conclusion: DREAMS实现了L3级自动化，显著减少对人类干预的依赖，为材料发现提供了可扩展的路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [328] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 论文介绍了WebGuard数据集，用于评估网络代理行为风险并开发安全措施，发现当前LLM在预测高风险行为上表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自主网络代理快速发展，其潜在风险（如无意或有害行为）亟需有效安全措施，类似人类用户的访问控制。

Method: 提出WebGuard数据集，包含4,939条人工标注行为，覆盖193个网站和22个领域，采用三级风险分类（SAFE、LOW、HIGH），并支持多样化泛化评估。

Result: 前沿LLM在预测行为结果和高风险行为召回率上表现不足（均低于60%），但微调后的Qwen2.5VL-7B模型将准确率从37%提升至80%，高风险召回率从20%提升至76%。

Conclusion: 尽管微调模型显著改进性能，但仍未达到高可靠性部署要求，需进一步优化安全措施。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [329] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为解释性动画，简化复杂STEM主题的可视化教育内容创作。


<details>
  <summary>Details</summary>
Motivation: 解决学习者理解复杂科学和数学概念的困难，同时减少手动创建动态可视化内容的时间和技能门槛。

Method: 通过LLM解析输入文本或PDF生成结构化场景描述，再转换为可执行的Manim Python代码。

Result: 实现了从研究论文到动画的自动化转换，为教育提供了高质量的可视化工具。

Conclusion: Manimator有潜力成为快速创建复杂STEM主题可视化教育内容的工具，推动教育内容的民主化。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [330] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 提出了一种新的本体嵌入方法OnT，结合预训练语言模型和双曲空间几何建模，以同时利用文本信息并保留逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法要么忽略文本信息，要么无法保留逻辑结构，限制了性能和应用潜力。

Method: 通过双曲空间几何建模调整预训练语言模型，结合文本标签并保留EL描述逻辑的类层次和逻辑关系。

Result: 在四个真实本体上的实验表明，OnT在公理预测和推理任务上均优于现有方法，并展示了强大的迁移学习能力。

Conclusion: OnT是一种高效的本体嵌入方法，适用于实际应用，如从SNOMED CT构建新本体。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [331] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，结合大型语言模型（LLM）和专用证明器，显著提升计算效率和准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大型通用模型或小型专用模型，各有局限性，且训练大型专用模型资源消耗大。

Method: ProofCompass通过LLM提供自然语言证明策略和失败分析，指导专用证明器（如DSP-v1.5），实现高效问题分解。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试次数（128 vs 3200）将准确率从54.9%提升至55.3%。

Conclusion: 该方法为形式定理证明提供了同时提升效率和准确性的新途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [332] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: 论文提出Nexus Architect，一种改进的多智能体系统框架，通过自动化工作流合成和迭代提示优化，显著提升推理模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时依赖记忆而非真正推理，泛化能力不足。

Method: 引入Nexus Architect，结合自动化工作流合成和迭代提示优化，生成定制化推理流程。

Result: 实验表明，Nexus Architect在逻辑问题数据集上表现优异，性能远超现有模型。

Conclusion: Nexus Architect通过创新机制显著提升了推理模型的泛化能力和性能。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [333] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家协作，以及引入非推理模型快速筛选问题，显著降低了错误率和延迟，同时节省成本。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，AI模型的错误率需接近0%，而现有推理模型仍存在错误率高和延迟大的问题。

Method: 提出两种方法：1) 推理模型通过推理轨迹长度量化不确定性，将不确定问题转交人类专家；2) 引入非推理模型快速筛选问题，直接转交人类或推理模型。

Result: 错误率从3%降至1%以下，延迟减少40%，成本节省50%，同时保持90%以上的准确率。

Conclusion: 通过系统工程技术，无需修改模型内部，即可显著改善推理模型的错误率和延迟问题。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [334] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，大型推理模型（LRMs）在延长推理长度时性能下降，揭示了测试时间计算与准确性之间的反比关系。


<details>
  <summary>Details</summary>
Motivation: 探讨LRMs在延长推理时的表现，揭示潜在问题。

Method: 构建四类评估任务，分析五种失败模式。

Result: 模型在长推理时出现分心、过拟合、虚假关联等问题。

Conclusion: 需多样化评估推理长度以解决LRMs的问题。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [335] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: 论文提出Routine框架，解决企业环境中代理系统因缺乏领域知识导致的执行不稳定问题，显著提升模型工具调用准确性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统常因缺乏领域知识导致执行不稳定，Routine旨在解决这一问题。

Method: 引入Routine框架，包含清晰结构、明确指令和参数传递，支持多步工具调用任务。

Result: Routine显著提升模型工具调用准确性，GPT-4o从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%。

Conclusion: Routine有效提升代理系统稳定性，加速企业环境中的部署，推动AI流程技术发展。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [336] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion是一个新颖的框架，通过深度协同语义和结构学习，解决了生物医学知识图谱的动态整合问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱对药物发现和疾病理解至关重要，但现有方法难以实现语义理解和结构学习的动态协同。

Method: BioGraphFusion通过张量分解建立全局语义基础，结合LSTM动态优化关系嵌入，并通过查询引导的子图构建和混合评分机制增强学习。

Result: 在三个生物医学任务中，BioGraphFusion优于现有KE、GNN和集成模型，并在CMM1案例中揭示了有生物学意义的通路。

Conclusion: BioGraphFusion通过深度协同语义和结构学习，显著提升了生物医学知识图谱的推理能力。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [337] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，专为嵌入式系统优化的自主代理构建，支持高效运行和跨平台部署。


<details>
  <summary>Details</summary>
Motivation: 现有框架在资源受限或动态环境中表现不佳，依赖云端计算且缺乏持久自主性和环境感知能力。

Method: Amico采用Rust编写，支持WebAssembly，提供事件处理、状态管理和行为执行等抽象模块。

Result: Amico构建了适用于有限计算和间歇性连接环境的弹性交互代理。

Conclusion: Amico为嵌入式系统提供了一种高效、统一的自主代理解决方案。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [338] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（结合文本和视觉输入）在Othello游戏中提升模型性能和内部表征的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，还是需要多模态（如视觉）输入更有效。

Method: 使用VISOTHELLO模型，结合移动历史和棋盘图像进行多模态训练，并与单模态基线对比。

Result: 多模态训练提高了模型性能和内部表征的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表征。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [339] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 论文提出OE-Assist框架，利用大型语言模型（LLM）自动或半自动验证能力问题（CQ），以降低本体评估的成本和错误率。


<details>
  <summary>Details</summary>
Motivation: 传统本体评估方法（如CQ验证）成本高、劳动密集且易出错，需要更高效的解决方案。

Method: 开发OE-Assist框架，利用LLM自动验证CQ，并提供辅助建议。基于1,393个CQ及其对应本体和故事的数据集进行实验。

Result: LLM（o1-preview和o3-mini）的自动评估表现与普通用户相当。

Conclusion: LLM辅助的本体评估具有潜力，可显著提升效率并减少人工干预。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [340] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 论文提出了一种基于几何框架的Coordinate Heart System（CHS），用于AI中的情感表示，通过八种核心情绪坐标实现复杂情感状态的计算。


<details>
  <summary>Details</summary>
Motivation: 传统情感模型在表示复杂情感状态时存在不足，CHS通过几何覆盖和数学计算填补了这一空白。

Method: 将八种核心情绪定位为单位圆上的坐标，通过坐标混合和向量运算实现情感计算，并引入稳定性参数S。

Result: 实验验证了CHS在处理情感冲突和复杂心理场景上的能力，优于传统分类模型。

Conclusion: CHS为AI情感建模提供了新的数学基础，解决了传统模型的局限性。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [341] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的方法，优化对抗性提示后缀以攻击RAG系统，实验表明其攻击成功率优于现有方法，且能绕过检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，因此需要一种梯度无关的方法来优化对抗性提示后缀。

Method: 采用差分进化（DE）算法，将RAG系统视为黑箱，通过演化候选后缀来最大化目标错误文档的检索排名。

Result: 在BEIR QA数据集上的实验显示，DE方法在攻击成功率上优于GGPP和PRADA，且仅需少量标记（≤5个）。

Conclusion: DE生成的对抗性后缀不仅高效，还能绕过检测，验证了其在实际场景中的有效性。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [342] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于比较学习的框架，用于简化敏捷开发中的故事点估算，通过开发者对任务对的比较判断训练模型，效果与传统回归模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统故事点估算方法繁琐且耗时，机器学习虽能减轻负担，但需要项目特定数据。本文旨在通过比较学习框架提高估算效率。

Method: 开发者通过比较任务对的努力程度提供判断，模型基于这些判断预测故事点。

Result: 模型在16个项目中的23,313个估算数据上，预测与真实值的Spearman等级相关系数平均为0.34，效果与回归模型相当。

Conclusion: 比较学习方法在降低开发者认知负担的同时，达到了与传统方法相当的估算效果。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [343] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意合谋中的风险，通过模拟框架展示了去中心化系统在传播虚假信息和电商欺诈中的高效性，并提出改进检测系统的需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主系统的兴起，多智能体系统在复杂现实场景中的潜在危害尚未充分研究，论文旨在填补这一空白。

Method: 提出一个灵活的模拟框架，支持集中式和去中心化协调结构，应用于虚假信息传播和电商欺诈两个高风险领域。

Result: 去中心化系统在执行恶意行为时比集中式系统更高效，能灵活调整策略以规避传统干预手段。

Conclusion: 研究揭示了恶意多智能体系统的运作机制，强调了改进检测系统和应对措施的必要性。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [344] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，通过动态生成多样化的测试输入，显著提高了测试效率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法满足LLM代理的复杂行为评估需求，因此需要一种自动化、动态的测试框架。

Method: Neo结合了问题生成代理和评估代理，通过共享上下文中心模块化地组合领域提示、场景控制和动态反馈，测试输入来自概率状态模型。

Result: 在金融助手聊天机器人测试中，Neo发现边缘案例故障的效率接近人类专家，且测试吞吐量提高了10-12倍。

Conclusion: Neo为可扩展、自演化的LLM质量评估奠定了基础，其框架具有模型无关性和可扩展性。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [345] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估的平台，通过将自然语言安全政策转化为对抗性提示，并使用AI评分器评估模型响应。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在现实应用中的普及，亟需可扩展且严格的安全评估方法。

Method: Aymara AI将自然语言安全政策转化为对抗性提示，并使用AI评分器（经人类判断验证）对模型响应进行评分。

Result: 评估了20个商业LLM在10个安全领域的表现，结果显示性能差异显著，平均安全分数从86.2%到52.4%不等。

Conclusion: LLM安全性具有不一致性和上下文依赖性，需要像Aymara AI这样的可扩展工具来支持负责任的AI开发和监管。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [346] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI与城市规划的结合，提出将城市规划视为生成任务，并指出当前研究的不足及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI（如VAEs、GANs、transformers和扩散模型）如何重塑城市规划，填补现有研究的空白。

Method: 将城市规划概念化为生成式AI任务，分析现有方法并识别关键研究缺口。

Result: 发现当前研究在理论指导、多空间分辨率、数据增强和实际交互方面存在不足。

Conclusion: 提出未来研究方向，包括理论指导生成、数字孪生和人机协同设计，呼吁生成智能与参与式城市规划的新结合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [347] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型（LM）代理和强化学习（RL）的可扩展框架，支持多轮交互和高吞吐训练。


<details>
  <summary>Details</summary>
Motivation: LM代理与强化学习的结合（Agent-RL）缺乏系统性研究，因此开发了AgentFly框架以填补这一空白。

Method: 通过令牌级掩码调整传统RL方法，支持多轮交互；提供装饰器接口定义工具和奖励函数；实现异步执行工具调用和奖励计算；设计集中式资源管理系统。

Result: 成功训练了多个任务的代理，展示了框架的有效性。

Conclusion: AgentFly为LM代理与RL的结合提供了系统化解决方案，具有扩展性和高效性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [348] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 本文提出InsightX Agent，一种基于LMM的框架，用于提高X射线无损检测的可靠性、可解释性和交互性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在工业X射线检测中缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: InsightX Agent利用LMM作为协调器，结合SDMSD检测器和EGR工具，通过主动推理优化检测结果。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的F1分数，显著提升了可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的代理框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [349] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在马尔可夫决策过程中的表现，发现其在简单环境中表现良好，但在复杂场景中需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自主决策中的适用性，尤其是其基于预训练知识的快速适应能力。

Method: 通过在线结构化提示策略，比较LLMs与经典强化学习方法在序列决策任务中的零样本表现。

Result: LLMs在简单环境中初始表现优异，但在复杂场景中规划和推理能力不足；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调和高级记忆整合以提升LLMs的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [350] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 《Endless Tuning》提出了一种基于双重镜像过程的AI可靠部署方法，旨在避免人类被替代并填补责任缺口。通过三个原型应用测试，该方法在用户体验和可控性方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 避免AI替代人类并解决责任缺口问题，推动AI伦理中的不同声音。

Method: 采用双重镜像过程，开发协议并在贷款发放、肺炎诊断和艺术风格识别三个领域进行原型测试。

Result: 用户感知到决策过程中的完全控制，同时建立了责任与赔偿之间的桥梁。

Conclusion: 该方法在用户体验和可控性方面有效，为AI伦理和技术选择提供了新视角。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [351] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的代理人工智能（Agentic AI）在老年护理中的潜力与挑战，包括个性化健康跟踪、认知护理和环境管理，同时强调了数据隐私、伦理保护和透明决策的重要性。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化问题需要创新解决方案，代理AI有望通过自主决策提升老年护理质量。

Method: 分析了LLM驱动的代理AI在老年护理中的应用，探讨其独特能力与限制。

Result: 代理AI可显著改善老年护理，但需解决隐私、伦理和透明性问题。

Conclusion: 需进一步研究以实现以人为本的代理AI整合，填补了相关文献空白。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [352] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文探讨了命题溯因中的精细推理方法，引入“facet”概念以更好地理解解释的异质性，并分析了其在不同设置下的复杂性。


<details>
  <summary>Details</summary>
Motivation: 命题溯因在人工智能和数据库更新中有广泛应用，但复杂的计数和枚举问题使其推理具有挑战性。

Method: 引入“facet”概念（部分解释中出现但非全部解释中出现的文字），并分析其在Post框架中的表现。

Result: 通过“facet”和解释间距离，提供了对解释异质性的更精细理解，并完成了Post框架中的几乎完整分类。

Conclusion: “facet”为命题溯因提供了更深入的理解，同时保持了良好的计算复杂性。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [353] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个基于纯强化学习的框架，通过可验证的安全奖励激励LLMs的潜在安全意识，提升安全对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在浅层拒绝或依赖密集监督的问题，未能充分利用模型内在的安全意识。

Method: 采用双奖励系统：安全奖励鼓励正确拒绝有害查询，帮助性奖励指导高质量响应良性输入。

Result: AlphaAlign简单高效，打破安全与效用的权衡，促进深度对齐。

Conclusion: AlphaAlign通过主动安全推理，显著提升LLMs的安全性和实用性。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [354] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于解决传统模型的局限性，并在真实和模拟数据集上验证了其准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试对人员选拔、职业发展和心理健康评估越来越重要，强制选择测试因其能降低回答失真的风险而被广泛使用。

Method: 通过非线性映射挖掘参与者和项目特征，使用多层神经网络建模其交互，并利用单调性假设提高诊断结果的可解释性。

Result: 实验证明FCNCD在真实和模拟数据集上表现出高准确性、可解释性和鲁棒性。

Conclusion: FCNCD为强制选择测试提供了一种有效且可解释的解决方案，克服了传统模型的不足。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [355] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推理的Causal Action Influence Score (CAIS)作为内在奖励，用于强化学习代理在噪声环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿能有效发现自身的因果效应，而传统强化学习代理在噪声环境中表现脆弱，依赖相关性奖励难以奏效。

Method: CAIS通过计算动作对感官结果分布的1-Wasserstein距离来量化动作的因果影响，从而过滤环境噪声。

Result: 在模拟婴儿-移动环境中，CAIS成功过滤噪声并学习正确策略，同时再现了“消退爆发”现象。

Conclusion: 显式推断因果性是发展鲁棒代理感的关键机制，为自适应自主系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [356] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文提出了一种结合DL-Lite本体和动作条件的新方法，用于自动化规划，并通过多项式编译实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，以提升规划能力。

Method: 结合DL-Lite本体和动作条件，采用一致性更新语义，并通过多项式编译为经典规划问题。

Result: 新方法的复杂度与现有方法相当，且通过编译实现高效性能。

Conclusion: 该方法有效结合了本体知识和规划，性能优越，适用于实际应用。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [357] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 开发了一种名为CSI的人工智能框架，通过模拟专家临床医生的认知过程，诊断118种口腔疾病，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断存在症状重叠的临床挑战，需要超越简单模式匹配的方法，模拟专家推理以提高诊断辅助工具的实用性。

Method: 结合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）进行快速和标准模式诊断。

Result: 在431张内部测试图像上，快速模式准确率为73.4%，标准模式提升至89.5%。

Conclusion: CSI框架通过分层推理显著提升诊断准确性，为临床诊断提供了实用工具。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [358] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了沙特阿拉伯NEOM提出的170公里线性智能城市The Line中人类移动的可行性，通过混合仿真框架验证了自由移动的可能性。


<details>
  <summary>Details</summary>
Motivation: 评估在The Line这种前所未有的城市拓扑结构中，居民是否能自由移动。

Method: 开发了结合基于代理的建模、强化学习、监督学习和图神经网络的混合仿真框架，模拟多模式交通行为。

Result: 实验显示，AI集成架构下，平均通勤时间为7.8至8.4分钟，满意度超过89%，可达性指数超过91%。

Conclusion: 研究表明，在自适应AI系统、可持续基础设施和实时反馈的支持下，The Line中的自由移动不仅是概念上可行，操作上也是现实的。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [359] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用LLM与Lean 4交互生成形式化证明，无需模型微调，在miniF2F-test上达到95.9%成功率。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在形式化证明（如Lean 4）中表现不佳，现有方法需高成本微调。Delta Prover旨在通过代理框架释放通用LLM的潜力。

Method: Delta Prover结合了反射分解与迭代证明修复的算法框架，以及基于Lean 4的DSL，实现交互式证明生成。

Result: 在miniF2F-test上达到95.9%成功率，超越现有方法，并展示更强的测试时扩展性。

Conclusion: 通用LLM在有效代理结构下具备强大定理证明能力，为形式化环境提供高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [360] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种软评估指标和轻量级平衡神经网络，用于解释和提升电弧故障诊断模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有AI电弧故障诊断模型虽准确率高，但缺乏可解释性，难以被信任。

Method: 结合可解释人工智能和真实电弧故障实验，定义电弧故障的正确解释，并提出轻量级平衡神经网络。

Result: 在多个数据集上验证了软评估指标的有效性，模型更易理解和信任。

Conclusion: 该方法提升了电弧故障诊断模型的可解释性和可信度，便于实际应用。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [361] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 本文提出了一种名为DMGC的新型多模态图聚类框架，通过分解混合图并引入双频融合机制，实现了无监督学习中的多模态图聚类。


<details>
  <summary>Details</summary>
Motivation: 多模态图在现实世界中有广泛应用，但在无监督学习中的研究不足，尤其是混合邻域模式（同质性和异质性关系并存）的挑战。

Method: 提出DMGC框架，将混合图分解为同质性增强图和异质性感知图，并引入双频融合机制进行多模态整合。

Result: 在多个数据集上验证了DMGC的优越性能，展示了其有效性和泛化能力。

Conclusion: DMGC为多模态图聚类提供了一种有效的无监督解决方案，填补了研究空白。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [362] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大型语言模型的多智能体框架，旨在解决注塑行业知识传递的挑战，结合文档知识和现场数据，通过检索增强生成和工具调用实现高效任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑行业面临经验工人退休和多语言沟通障碍，导致知识传递困难，需要一种高效解决方案。

Method: IM-Chat采用检索增强生成（RAG）策略和工具调用智能体，结合文档知识和数据驱动的工艺条件生成器，无需微调即可适应任务。

Result: 评估显示，更强大的模型（如GPT-4o）在复杂任务中表现更优，验证了多智能体LLM系统在工业知识工作流中的可行性。

Conclusion: IM-Chat为制造业提供了一个可扩展且通用的AI辅助决策支持方案。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [363] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新的AI系统漏洞类别——认知退化，并提出了Qorvex安全AI框架（QSAF Domain 10）来应对此类问题。


<details>
  <summary>Details</summary>
Motivation: 传统的外部威胁（如提示注入）已不足以应对AI系统的内部故障，如内存不足、规划递归等导致的认知退化问题。

Method: 通过六阶段认知退化生命周期和七种运行时控制（QSAF-BC-001至BC-007）实时监控并缓解问题。

Result: 框架能够检测并缓解认知退化，提升AI系统的行为与认知韧性。

Conclusion: 认知退化是AI系统的新漏洞类别，QSAF框架为跨平台防御提供了首个解决方案。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [364] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO）解决传统多智能体强化学习在拼车平台中依赖准确值函数估计的问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 拼车平台需动态匹配乘客与车辆，传统多智能体强化学习方法因依赖值函数估计在不确定环境中表现不佳。

Method: 1. 将GRPO应用于拼车，用组平均奖励替代PPO基线以减少估计误差；2. 提出OSPO，仅使用一步奖励训练策略。

Result: 在真实曼哈顿拼车数据集上，GRPO和OSPO在大多数场景中表现优越，优化了接载时间和订单数量。

Conclusion: GRPO和OSPO通过避免值函数估计，显著提升了拼车平台的匹配效率和稳定性。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [365] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD方法通过结合非参数检索和扩散生成模型，解决了离线强化学习中数据集稀疏和轨迹重叠不足的问题，提升了长时程规划的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的有效性受限于数据集稀疏性和子优与专家轨迹间缺乏过渡重叠，导致长时程规划困难。

Method: RAD结合非参数检索和扩散生成模型，动态检索高回报状态作为目标，并利用条件引导的扩散模型进行规划。

Result: RAD在多样化基准测试中表现优于基线方法。

Conclusion: RAD通过检索引导的生成方法，显著提升了离线强化学习的泛化能力和性能。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [366] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测效果，解决信息提取和模型构建的挑战。

Method: 结合图注意力网络编码活动及其关系，LSTM处理时间依赖。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在预测流程行为方面具有竞争力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [367] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种基于帕累托优化的方法，通过干预启发式发现业务过程中活动批处理的最优策略，平衡等待时间、处理成本和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 业务过程中，批处理策略需要在成本、处理时间和等待时间之间权衡，但目前缺乏自动发现最优策略的方法。

Method: 采用干预启发式生成批处理策略，结合模拟评估，嵌入三种元启发式（爬山法、模拟退火和强化学习）优化帕累托前沿。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益上优于无启发式引导的基线方法。

Conclusion: 干预启发式结合元启发式能有效发现最优批处理策略，为业务过程优化提供实用工具。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [368] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的图表领域视觉语言模型，通过程序化数据合成和两阶段训练策略（Chart-COT和Chart-RFT）实现复杂图表推理，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在通用多模态数据（如图表）上的优势，解决图表领域缺乏高质量推理数据的问题。

Method: 提出程序化数据合成技术生成高质量图表推理数据，并采用两阶段训练策略（Chart-COT和Chart-RFT）进行模型微调。

Result: Chart-R1在开源基准和自建数据集（ChartRQA）上表现优异，优于现有图表领域方法，甚至接近GPT-4o等大型模型。

Conclusion: Chart-R1通过创新数据合成和训练策略，显著提升了图表推理能力，为多模态推理研究提供了新思路。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [369] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，专注于戏剧创作和实时表演，通过自主决策和物理环境交互提升沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的戏剧生成方法缺乏主动性和物理交互，需详细用户输入，降低了互动性和沉浸感。

Method: 提出HAMLET框架，生成叙事蓝图并支持演员自主决策，通过动作改变场景道具状态，影响其他演员。

Result: 实验评估显示HAMLET能创造表达性和连贯性的戏剧体验。

Conclusion: HAMLET通过多智能体自主性和环境交互，显著提升了戏剧的互动性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [370] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文探讨大型语言模型（LLMs）是否构建内部世界模型或仅依赖统计关联。通过滑轮系统实验，发现LLMs能利用统计关联（如滑轮数量与机械优势的关系），但缺乏对复杂结构连接的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备构建和操作内部世界模型的能力，而非仅依赖统计关联。

Method: 采用认知科学方法，通过三个实验测试LLMs在滑轮系统问题中的表现。

Result: LLMs能利用统计关联（如滑轮数量）估计机械优势，并能区分功能性与杂乱系统，但对复杂结构连接的推理能力有限。

Conclusion: LLMs可能具备初步的世界模型能力，但推理能力有限；认知科学方法有助于评估AI系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [371] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 论文提出了一种利用参数依赖关系提升离线强化学习数据效率的方法，包括参数化SPI算法、基于游戏抽象的预处理技术和基于SMT的预处理技术。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中如何利用额外信息（如参数依赖关系）提升数据效率的问题。

Method: 1. 参数化SPI算法利用分布相关性估计转移动态；2. 基于游戏抽象的预处理技术剪枝冗余动作；3. 基于SMT的预处理技术进一步剪枝动作。

Result: 实验表明，这些技术将SPI的数据效率提升了多个数量级，同时保持相同的可靠性保证。

Conclusion: 通过利用参数依赖关系和预处理技术，显著提升了离线强化学习的数据效率。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [372] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文提出了一种评估多选问题（MCQ）指标的新协议，分析了现有指标与答案波动率的关系，发现最差准确率指标关联性最强。


<details>
  <summary>Details</summary>
Motivation: 现有研究未对评估LLM能力的多选问题指标进行全面评估，且存在答案波动问题。

Method: 提出一种指标评估协议，分析评估方法与波动率和原始性能的关系。

Result: 现有指标与答案波动有强关联，最差准确率指标表现最佳。

Conclusion: 最差准确率指标在多选问题评估中具有最高关联性。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [373] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的战术调节方法，用于《星际争霸II》AI代理，使其能根据高层战术指令调整策略。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽强大，但缺乏根据战术指令动态调整策略的能力。

Method: 冻结预训练策略网络（DI-Star），为每个动作头附加轻量级适配器模块，战术张量编码战略偏好，并通过KL散度约束训练适配器。

Result: 实验表明，该方法能成功调节代理在侵略性、扩张模式和技术偏好等战术维度的行为，同时保持竞争力。

Conclusion: 该方法以最小计算开销实现灵活战术控制，为复杂即时战略游戏提供实用策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [374] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨代理AI在复杂系统中自主检测和响应异常的潜力，强调其改变传统依赖人类的异常管理方法的能力。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人类，效率低且反应慢，代理AI有望提升自主性和效率。

Method: 研究代理AI在复杂系统中的自主异常检测和响应能力。

Result: 代理AI能够有效自主检测和响应异常，减少对人类干预的依赖。

Conclusion: 代理AI在异常管理领域具有变革潜力，可显著提升系统效率和响应速度。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [375] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一个名为g-AMIE的多代理系统框架，用于在医疗诊断对话中实现异步监督，确保AI系统在安全范围内运行，同时提高决策质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，医疗诊断和治疗计划需要由持牌专业人员执行并监督，因此需要一种方法让AI系统在人类监督下安全运行。

Method: 提出g-AMIE系统，通过多代理系统在限定范围内执行病史采集，避免提供个性化医疗建议，并通过临床驾驶舱界面将评估结果提交给监督医生。

Result: 在虚拟临床考试中，g-AMIE在高质量病史采集、病例总结和诊断建议方面优于护士和医生助理组，且监督效率更高。

Conclusion: 异步监督是一种可行的范式，可在专家监督下增强AI系统在现实医疗中的应用。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [376] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，使模型内化推理长度控制能力，减少40.9%的token使用并提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因自由生成链式推理导致token浪费的问题。

Method: 两阶段强化学习：第一阶段学习成功解长度分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 在数学推理基准上，LAPO减少40.9%的token使用，提升2.3%的准确率。

Conclusion: LAPO使模型能根据问题复杂度分配计算资源，实现高效且高质量的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [377] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约的Gas优化，结合了现有模式的兼容性和新模式的自动发现/验证，实现了端到端优化。


<details>
  <summary>Details</summary>
Motivation: 由于智能合约中存在Gas浪费模式，现有解决方案效率低、维护成本高且难以扩展，GasAgent旨在解决这些问题。

Method: GasAgent由四个专门代理（Seeker、Innovator、Executor和Manager）组成，通过闭环协作识别、验证和应用Gas节省改进。

Result: 在100个真实合约中优化了82个，平均节省部署Gas 9.97%；在500个LLM生成的合约中优化了79.8%，Gas节省范围为4.79%至13.93%。

Conclusion: GasAgent证明了其作为LLM辅助智能合约开发的优化层的有效性和广泛适用性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [378] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，通过双视角思维跟踪机制和k-means聚类，实现了复杂服务生态系统的异常涌现分析。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现现象。

Method: EAMI框架采用双视角思维跟踪机制（Inspector Agent和Analysis Agent）提取智能体意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验验证了EAMI在复杂O2O服务系统和Stanford AI Town中的有效性、通用性和高效性。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [379] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了联邦学习（FL）如何满足可信人工智能（TAI）的要求，分析了FL在分布式特性下与TAI对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在敏感和高风险领域的应用增加，确保AI系统符合伦理、法律和技术要求的需求日益迫切。FL作为一种隐私保护技术，如何满足TAI的全面要求成为关键问题。

Method: 以TAI的要求为框架，系统分析了FL在满足这些要求时面临的挑战，并对现有研究、趋势和未解决问题进行了分类和探讨。

Result: 研究揭示了FL与TAI对齐的主要障碍，并总结了当前进展和未来方向。

Conclusion: FL在满足TAI要求方面具有潜力，但仍需解决分布式特性带来的挑战，未来研究需进一步探索这些方向。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [380] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在已知最大定向部分有向无环图（MPDAG）的情况下，如何识别条件因果效应，并提出了三种结果：条件集不受治疗影响时的识别公式、MPDAG设置下的do calculus推广，以及一个完整的算法。


<details>
  <summary>Details</summary>
Motivation: 研究在MPDAG表示的因果模型等价类中识别条件因果效应的问题，填补了现有方法在背景知识限制下的空白。

Method: 提出了三种方法：1）条件集不受治疗影响时的识别公式；2）将do calculus推广到MPDAG设置；3）开发了一个完整的算法用于识别条件效应。

Result: 提供了在MPDAG设置下识别条件因果效应的理论框架和实用工具，扩展了因果推断的应用范围。

Conclusion: 论文为MPDAG背景下的条件因果效应识别提供了全面的解决方案，为因果推断领域做出了重要贡献。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [381] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，优化推理模型的效率和能力，减少60.6%的token使用并提升3.14%的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因统一推理策略导致的低效问题，探索如何在保持能力的同时学习问题特定的推理深度。

Method: 采用分层预算探索，将样本分组并分配不同token预算，结合差异化奖励机制，激励模型根据问题复杂度调整推理深度。

Result: 在四个推理基准测试中，平均token使用减少60.6%，准确性提升3.14%。

Conclusion: HBPO表明推理效率和能力可以同时优化，通过分层训练保持探索多样性，实现自适应推理深度。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [382] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）在时间认知上表现出类似人类的模式，包括主观时间参考点和韦伯-费希纳定律的遵循。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs中未在训练数据中直接指定的认知模式，特别是时间认知。

Method: 通过相似性判断任务、神经元分析、表征层次分析及预训练嵌入模型研究时间认知机制。

Result: 发现LLMs存在主观时间参考点、对数编码机制及层次化时间表征构建过程。

Conclusion: 提出体验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，暗示AI对齐需关注内部构建的引导。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [383] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在国际数学奥林匹克（IMO）问题上的表现，通过优化使用方式，Gemini 2.5 Pro成功解决了5/6的IMO 2025问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在解决高难度数学问题（如IMO）上的潜力，填补其在数学竞赛任务中的表现空白。

Method: 采用Google的Gemini 2.5 Pro模型，结合管道设计和提示工程，避免数据污染，测试其解决IMO 2025问题的能力。

Result: 模型成功解决了5/6的IMO 2025问题，展示了优化使用方法的重要性。

Conclusion: 通过合理设计和优化，LLMs能够有效解决高难度数学问题，但需注意使用方式。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [384] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: 本文研究了通用AI模型的开源定义对市场均衡的影响，分析了监管政策如何影响开发者的经济激励。


<details>
  <summary>Details</summary>
Motivation: 探讨在监管框架下，开源定义对通用AI模型开发者和下游任务优化者之间战略互动的影响。

Method: 构建了一个理论模型，模拟监管者选择开源定义时，开发者的策略和市场均衡。

Result: 发现模型的基础性能决定了监管惩罚与开源阈值对开发者策略的影响程度。

Conclusion: 为AI治理提供了理论依据，支持开源政策的评估与优化。

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [385] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: 论文研究了网络拍卖中的策略证明性，提出了两种单调分配规则（ID-MON和IP-MON），并证明了其存在性和计算可行性。


<details>
  <summary>Details</summary>
Motivation: 现有网络拍卖缺乏通用的策略证明分配规则，导致多单位需求拍卖中策略证明性难以实现。

Method: 提出ID-MON和IP-MON两种单调分配规则，并分析其策略证明支付规则的存在性和条件。

Result: 证明了在ID-MON和IP-MON规则下，存在收益最大化的策略证明支付规则，且计算可行。

Conclusion: 解决了单心智竞标者的组合网络拍卖问题，为网络拍卖提供了理论基础。

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [386] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 论文研究了不可分割物品的公平分配问题，重点探讨了EFX和PMMS问题，提出了新的理论结果和算法。


<details>
  <summary>Details</summary>
Motivation: EFX问题是公平分配领域的核心开放性问题，而PMMS是其更强变体。研究旨在揭示两者之间的关系，并解决特定估值下的公平分配问题。

Method: 通过构造实例和理论证明，分析了EFX和PMMS的存在性，并针对个性化双值估值、二进制估值和配对需求估值提出了多项式时间算法。

Result: 证明了EFX和PMMS在特定估值下的存在性，并构造了PMMS不存在的实例，首次形式化分离了EFX和PMMS。

Conclusion: 研究为公平分配问题提供了新的理论见解和实用算法，扩展了对EFX和PMMS的理解。

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


### [387] [Strategically Robust Game Theory via Optimal Transport](https://arxiv.org/abs/2507.15325)
*Nicolas Lanzetti,Sylvain Fricker,Saverio Bolognani,Florian Dörfler,Dario Paccagnan*

Main category: cs.GT

TL;DR: 论文提出了一种新的均衡概念——战略鲁棒均衡，通过引入可调大小的模糊集来应对对手行为的不确定性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在博弈论中，代理需要面对对手行为的不确定性，这种不确定性可能来自多个方面（如信息不完全、计算能力有限等）。传统方法难以同时处理这些不确定性，因此需要一种新的均衡概念来提供保护。

Method: 提出战略鲁棒均衡，代理在模糊集内考虑最坏情况的行为，模糊集以涌现行为为中心。通过最优运输理论实现这一均衡概念。

Result: 战略鲁棒均衡在存在性上与纳什均衡相同，且计算成本不增加。实验表明，它能有效应对对手行为的不确定性，并可能带来更高的均衡收益。

Conclusion: 战略鲁棒均衡是一种有效的工具，能够在复杂不确定性环境下为代理提供保护，同时可能通过鲁棒化实现协调效应。

Abstract: In many game-theoretic settings, agents are challenged with taking decisions
against the uncertain behavior exhibited by others. Often, this uncertainty
arises from multiple sources, e.g., incomplete information, limited
computation, bounded rationality. While it may be possible to guide the agents'
decisions by modeling each source, their joint presence makes this task
particularly daunting. Toward this goal, it is natural for agents to seek
protection against deviations around the emergent behavior itself, which is
ultimately impacted by all the above sources of uncertainty. To do so, we
propose that each agent takes decisions in face of the worst-case behavior
contained in an ambiguity set of tunable size, centered at the emergent
behavior so implicitly defined. This gives rise to a novel equilibrium notion,
which we call strategically robust equilibrium. Building on its definition, we
show that, when judiciously operationalized via optimal transport,
strategically robust equilibria (i) are guaranteed to exist under the same
assumptions required for Nash equilibria; (ii) interpolate between Nash and
security strategies; (iii) come at no additional computational cost compared to
Nash equilibria. Through a variety of experiments, including bi-matrix games,
congestion games, and Cournot competition, we show that strategic robustness
protects against uncertainty in the opponents' behavior and, surprisingly,
often results in higher equilibrium payoffs - an effect we refer to as
coordination via robustification.

</details>


### [388] [The Root of Revenue Continuity](https://arxiv.org/abs/2507.15735)
*Sergiu Hart,Noam Nisan*

Main category: cs.GT

TL;DR: 论文证明了一个简洁的结论：对于k个加性商品的随机估值X和Y，其Wasserstein距离W(X,Y)满足√Rev(X) - √Rev(Y) ≤ √W(X,Y)，并表明通过“统一折扣”调整的机制对接近X的Y几乎最优。


<details>
  <summary>Details</summary>
Motivation: 研究买家估值分布的小变化对可提取收入的影响，旨在提供一个通用且简洁的理论框架。

Method: 使用Wasserstein距离度量估值分布的差异，并通过数学推导证明√Rev(X) - √Rev(Y) ≤ √W(X,Y)。

Result: 证明了估值分布的小变化仅导致收入的小变化，并提出“统一折扣”机制对接近的分布几乎最优。

Conclusion: 论文提供了一个简洁且通用的理论工具，用于分析估值分布变化对收入的影响，并提出了实用的机制调整方法。

Abstract: In the setup of selling one or more goods, various papers have shown, in
various forms and for various purposes, that a small change in the distribution
of a buyer's valuations may cause only a small change in the possible revenue
that can be extracted. We prove a simple, clean, convenient, and general
statement to this effect: let X and Y be random valuations on k additive goods,
and let W(X,Y) be the Wasserstein (or "earth mover's") distance between them;
then sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a
simple explicit modification of any optimal mechanism for X, namely, "uniform
discounting", is guaranteed to be almost optimal for any Y that is close to X
in the Wasserstein distance.

</details>


### [389] [General Matching Games](https://arxiv.org/abs/2507.15737)
*Felipe Garrido-Lucero,Rida Laraki*

Main category: cs.GT

TL;DR: 本文扩展了Garrido-Lucero和Laraki的匹配博弈模型，涵盖了一对多匹配市场和室友模型，提出了两种框架，确保核心稳定且抗重新谈判的结果存在并可高效计算。


<details>
  <summary>Details</summary>
Motivation: 扩展原有的一对一匹配博弈模型，以适用于更广泛的一对多匹配市场和室友模型，并解决核心稳定性和抗重新谈判的问题。

Method: 通过推广原有模型，提出两种框架，结合延迟接受算法和重新谈判过程，确保结果的稳定性和抗重新谈判性。

Result: 证明了核心稳定且抗重新谈判的结果存在，并提供了高效计算方法。

Conclusion: 扩展后的模型适用于更广泛的匹配市场，为实际应用提供了理论支持。

Abstract: Matching games is a one-to-one two sided market model introduced by
Garrido-Lucero and Laraki, in which coupled agents' utilities are endogenously
determined as the outcome of a strategic game. They refine the classical
pairwise stability by requiring robustness to renegotiation and provide general
conditions under which pairwise stable and renegotiation-proof outcomes exist
as the limit of a deferred acceptance with competitions algorithm together with
a renegotiation process. In this article, we extend their model to a general
setting encompassing most of one-to-many matching markets and roommates models
and specify two frameworks under which core stable and renegotiation-proof
outcomes exist and can be efficiently computed.

</details>
