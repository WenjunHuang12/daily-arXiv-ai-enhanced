<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 10]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 29]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.IT](#cs.IT) [Total: 24]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Equilibrium Refinements Improve Subgame Solving in Imperfect-Information Games](https://arxiv.org/abs/2601.17131)
*Ondrej Kubicek,Viliam Lisy,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 论文提出在非完全信息博弈中，通过改进子博弈求解的均衡选择方法，使用gadget game sequential equilibria替代传统纳什均衡，显著降低策略的可利用性。


<details>
  <summary>Details</summary>
Motivation: 在非完全信息博弈中，现有的子博弈求解方法使用gadget games来确保策略的鲁棒性，但这些gadget games通常包含无限多个纳什均衡。研究发现，虽然这些均衡在gadget game中是等价的，但在完整博弈中面对理性对手时，它们会导致完全不同的性能表现。

Method: 提出gadget game sequential equilibria作为首选的解概念，并引入对序列形式线性规划和反事实遗憾最小化算法的改进，以收敛到这些精炼解，同时仅增加轻微的计算成本。此外，对resolving gadget game优于max-margin gadget game的原因提供了新的理论见解。

Result: 实验在多个标准基准博弈中比较了gadget games的不同纳什均衡，结果显示精炼的均衡始终优于未精炼的纳什均衡，能够将整体策略的可利用性降低超过50%。

Conclusion: 在非完全信息博弈的子博弈求解中，均衡选择至关重要。提出的gadget game sequential equilibria解概念及其计算方法，能够显著提高策略性能并降低可利用性，为大规模博弈求解提供了更有效的技术。

Abstract: Subgame solving is a technique for scaling algorithms to large games by locally refining a precomputed blueprint strategy during gameplay. While straightforward in perfect-information games where search starts from the current state, subgame solving in imperfect-information games must account for hidden states and uncertainty about the opponent's past strategy. Gadget games were developed to ensure that the improved subgame strategy is robust against any possible opponent's strategy in a zero-sum game. Gadget games typically contain infinitely many Nash equilibria. We demonstrate that while these equilibria are equivalent in the gadget game, they yield vastly different performance in the full game, even when facing a rational opponent. We propose gadget game sequential equilibria as the preferred solution concept. We introduce modifications to the sequence-form linear program and counterfactual regret minimization that converge to these refined solutions with only mild additional computational cost. Additionally, we provide several new insights into the surprising superiority of the resolving gadget game over the max-margin gadget game. Our experiments compare different Nash equilibria of gadget games in several standard benchmark games, showing that our refined equilibria consistently outperform unrefined Nash equilibria, and can reduce the exploitability of the overall strategy by more than 50%

</details>


### [2] [Strategic AI in Cournot Markets](https://arxiv.org/abs/2601.17263)
*Sanyukta Deshpande,Sheldon H. Jacobson*

Main category: cs.GT

TL;DR: LLMs在寡头古诺市场中展现出复杂决策能力，既能理解市场动态作为经济规划代理，又会形成持续默契合谋，将价格推高至纳什均衡水平的200%以上。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在竞争性市场中日益自动化决策，理解由此产生的动态并确保公平市场机制至关重要。需要研究LLMs在寡头市场中的行为，以识别AI整合到竞争市场环境中的潜在问题。

Method: 研究LLMs在古诺寡头市场中的多维度决策：1) 决策类型，2) 对手策略，3) 市场构成。通过实验分析这些因素如何影响LLM决策者的竞争性，并测试通过强制最佳响应策略来监管主导代理的有效性。

Result: LLMs不仅理解复杂市场动态，还表现出持续默契合谋行为，将价格推高至纳什均衡水平的200%以上。监管少数主导代理（强制最佳响应策略）能有效破坏合谋，恢复竞争性定价。

Conclusion: AI整合到竞争市场环境存在合谋风险，需要监管政策干预。通过监管少数主导代理可以恢复市场竞争，为自动化时代的监管政策提供建议。

Abstract: As artificial intelligence increasingly automates decision-making in competitive markets, understanding the resulting dynamics and ensuring fair market mechanisms is essential. We investigate the multi-faceted decision-making of large language models (LLMs) in oligopolistic Cournot markets, showing that LLMs not only grasp complex market dynamics--demonstrating their potential as effective economic planning agents--but also engage in sustained tacit collusion, driving prices up to 200% above Nash equilibrium levels. Our analysis examines LLM behavior across three dimensions-(1) decision type, (2) opponent strategies, and (3) market composition--revealing how these factors may shape the competitiveness of LLM-based decision-makers. Furthermore, we show that regulating a few dominant agents by enforcing best-response strategies effectively disrupts collusion and helps restore competitive pricing. Our findings identify potential concerns associated with AI integration in competitive market environments and provide regulatory policy recommendations for the era of automation.

</details>


### [3] [Truth-Revealing Participatory Budgeting](https://arxiv.org/abs/2601.17538)
*Qishen Han,Artem Ivaniuk,Edith Elkind,Lirong Xia*

Main category: cs.GT

TL;DR: 该论文从认知角度研究参与式预算，分析常见预算规则在项目质量未知情况下的表现，发现当项目成本范围缩小时规则表现更好，但策略性投票者很少会诚实投票。


<details>
  <summary>Details</summary>
Motivation: 传统参与式预算研究多从公理角度出发，假设选民完全了解偏好。本文从认知角度出发，考虑项目有无法直接观察的潜在质量水平，选民只能通过噪声信息投票，旨在选举高质量项目集合。

Method: 建立认知框架，分析常见参与式预算规则的表现，测量其结果的期望效用与最优项目集的比较。研究策略性投票者是否有动机诚实传递信息，并进行数值实验验证理论发现。

Result: 发现预算规则的表现随项目成本范围缩小而改善；当项目为单位成本时，常见规则能以趋近于1的概率识别"最佳"集合；策略性投票者只在非常限制的条件下才有动机诚实投票。

Conclusion: 从认知角度研究参与式预算提供了新视角，揭示了预算规则在信息聚合方面的表现，但策略性投票问题限制了信息有效聚合，需要进一步研究激励机制设计。

Abstract: Participatory Budgeting (PB) is commonly studied from an axiomatic perspective, where the aim is to design procedurally fair and economically efficient rules for voters with full information regarding their preferences. In contrast, we take an epistemic perspective and consider a framework where PB projects have different levels of underlying quality, indicating how well the project will take effect, which cannot be directly observed before implementation. Agents with noisy information cast votes to aggregate their information, and aim to elect a high-quality set of projects. We evaluate the performance of common PB rules by measuring the expected utility of their outcomes, compared to the optimal set of projects. We find that the quality of approximation improves as the range of project costs shrinks. When projects have unit cost, these common rules can identify the ``best'' set with probability converging to 1. We also study whether strategic agents have incentives to honestly convey their information in the vote. We find that it happens only under very restrictive conditions. We also run numerical experiments to examine the performance of different rules empirically and support our theoretical findings.

</details>


### [4] [Distances Between Top-Truncated Elections of Different Sizes](https://arxiv.org/abs/2601.17931)
*Piotr Faliszewski,Jitka Mertlová,Pierre Nunn,Stanisław Szufa,Tomasz Wąs*

Main category: cs.GT

TL;DR: 扩展选举地图框架以处理不同规模选举和顶部截断投票，并应用于Preflib数据库可视化


<details>
  <summary>Details</summary>
Motivation: 现有的选举地图框架仅限于候选人数量相同、选民数量相同且所有投票都对所有候选人进行完全排序的情况，这限制了其在现实选举数据集中的应用

Method: 扩展选举地图框架，使其能够处理不同规模的选举（不同候选人数和选民数），并支持顶部截断投票（投票人只对部分候选人排序）

Result: 成功扩展了框架，使其能够处理更广泛的选举数据集，并应用该扩展框架对Preflib数据库的大部分内容进行了可视化展示

Conclusion: 扩展后的选举地图框架能够更全面地处理现实世界中的选举数据，为选举分析和可视化提供了更强大的工具

Abstract: The map of elections framework is a methodology for visualizing and analyzing election datasets. So far, the framework was restricted to elections that have equal numbers of candidates, equal numbers of voters, and where all the (ordinal) votes rank all the candidates. We extend it to the case of elections of different sizes, where the votes can be top-truncated. We use our results to present a visualization of a large fragment of the Preflib database.

</details>


### [5] [Credit Fairness: Online Fairness In Shared Resource Pools](https://arxiv.org/abs/2601.17944)
*Seyed Majid Zahedi,Rupert Freeman*

Main category: cs.GT

TL;DR: 论文提出了一种新的"信用公平"概念，用于解决多轮资源分配中的长期公平性问题，并设计了同时满足信用公平和帕累托效率的机制。


<details>
  <summary>Details</summary>
Motivation: 现有最大最小机制虽然满足单轮公平性（共享激励、策略证明、帕累托效率），但会导致长期资源分配的巨大差异，即使代理具有相同的平均需求。需要一种机制确保早期借出资源的代理能在后期收回资源。

Method: 引入"信用公平"概念，这是对共享激励的强化，确保早期借出资源的代理能在后期收回资源。提出了一种同时满足信用公平和帕累托效率的机制，并在计算资源共享场景中评估其性能。

Result: 研究表明信用公平可以与帕累托效率或策略证明性结合，但不能同时满足三者。提出的机制实现了信用公平和帕累托效率，在计算资源共享场景中表现出良好性能。

Conclusion: 信用公平是解决多轮资源分配中长期公平性的有效概念，提出的机制在保持帕累托效率的同时实现了信用公平，为实际资源分配系统提供了新的解决方案。

Abstract: We consider a setting in which a group of agents share resources that must be allocated among them in each discrete time period. Agents have time-varying demands and derive constant marginal utility from each unit of resource received up to their demand, with zero utility for any additional resources. In this setting, it is known that independently maximizing the minimum utility in each round satisfies sharing incentives (agents weakly prefer participating in the mechanism to not participating), strategyproofness (agents have no incentive to misreport their demands), and Pareto efficiency (Freeman et al. 2018). However, recent work (Vuppalapati et al. 2023) has shown that this max-min mechanism can lead to large disparities in the total resources received by agents, even when they have the same average demand. In this paper, we introduce credit fairness, a strengthening of sharing incentives that ensures agents who lend resources in early rounds are able to recoup them in later rounds. Credit fairness can be achieved in conjunction with either Pareto efficiency or strategyproofness, but not both. We propose a mechanism that is credit fair and Pareto efficient, and we evaluate its performance in a computational resource-sharing setting.

</details>


### [6] [Decentralized Multi-product Pricing: Diagonal Dominance, Nash Equilibrium, and Price of Anarchy](https://arxiv.org/abs/2601.18117)
*Boxiao Chen,Jiashuo Jiang,Stefanus Jasin*

Main category: cs.GT

TL;DR: 论文分析了多产品企业分散定价决策的效率损失，推导出分散收入与集中最优收入比值的紧下界，该界由衡量交叉价格效应强度的单一参数μ决定。


<details>
  <summary>Details</summary>
Motivation: 多产品企业的分散决策可能导致效率损失，因为自主决策者未能内部化产品间的需求交互效应。本文旨在量化这种损失的程度。

Method: 使用线性需求系统建模产品间的替代和互补效应，分析定价博弈中的纳什均衡存在性和唯一性，推导分散收入与集中最优收入比值的下界。

Result: 证明收入比值下界为4(1-μ)/(2-μ)²，其中μ衡量交叉价格效应相对于自身价格敏感性的强度。该界是紧的，并通过对称市场拓扑实现。

Conclusion: 为评估多产品企业集中定价与分散自主权之间的权衡提供了量化框架，效率损失由单一参数μ决定，便于实际应用。

Abstract: Decentralized decision making in multi--product firms can lead to efficiency losses when autonomous decision makers fail to internalize cross--product demand interactions. This paper quantifies the magnitude of such losses by analyzing the Price of Anarchy in a pricing game in which each decision maker independently sets prices to maximize its own product--level revenue. We model demand using a linear system that captures both substitution and complementarity effects across products. We first establish existence and uniqueness of a pure--strategy Nash equilibrium under economically standard diagonal dominance conditions. Our main contribution is the derivation of a tight worst--case lower bound on the ratio between decentralized revenue and the optimal centralized revenue. We show that this efficiency loss is governed by a single scalar parameter, denoted by $μ$, which measures the aggregate strength of cross--price effects relative to own--price sensitivities. In particular, we prove that the revenue ratio is bounded below by $4(1-μ)/(2-μ)^2$, and we demonstrate the tightness of this bound by constructing a symmetric market topology in which the bound is exactly attained. We further refine the analysis by providing an instance--exact characterization of efficiency loss based on the spectral properties of the demand interaction matrix. Together, these results offer a quantitative framework for assessing the trade--off between centralized pricing and decentralized autonomy in multi--product firms.

</details>


### [7] [Dicey Games: Shared Sources of Randomness in Distributed Systems](https://arxiv.org/abs/2601.18303)
*Léonard Brice,Thomas A. Henzinger,K. S. Thejaswini*

Main category: cs.GT

TL;DR: 团队在共享随机源条件下可以突破1/4胜率限制，提出Dicey Games框架研究分布式系统中随机源分配问题


<details>
  <summary>Details</summary>
Motivation: 研究分布式系统中共享随机源对团队协作性能的影响，特别是当团队成员只能成对共享随机源时，能否超越简单的概率限制

Method: 提出Dicey Games形式化框架，分析团队在共享随机源条件下的最优策略存在性、表示方法和计算复杂度，研究有限随机源在团队中的最优分配问题

Result: 发现团队在成对共享随机源条件下可以取得超过1/4的胜率，这是反直觉的结果。建立了Dicey Games的理论框架，刻画了最优策略的特征

Conclusion: 共享随机源的分配方式对团队协作性能有显著影响，Dicey Games为分析分布式系统中随机资源分配提供了有效的理论工具

Abstract: Consider a 4-player version of Matching Pennies where a team of three players competes against the Devil. Each player simultaneously says "Heads" or "Tails". The team wins if all four choices match; otherwise the Devil wins. If all team players randomise independently, they win with probability 1/8; if all players share a common source of randomness, they win with probability 1/2. What happens when each pair of team players shares a source of randomness? Can the team do better than win with probability 1/4? The surprising (and nontrivial) answer is yes! We introduce Dicey Games, a formal framework motivated by the study of distributed systems with shared sources of randomness (of which the above example is a specific instance). We characterise the existence, representation and computational complexity of optimal strategies in Dicey Games, and we study the problem of allocating limited sources of randomness optimally within a team.

</details>


### [8] [Maps of Tournaments: Distances, Experiments, and Data](https://arxiv.org/abs/2601.18348)
*Filip Nikolow,Piotr Faliszewski,Stanisław Szufa*

Main category: cs.GT

TL;DR: 将选举领域的映射框架应用于锦标赛，构建锦标赛地图，通过二维平面上的点表示锦标赛，使欧氏距离反映给定度量下的距离，用于可视化实验结果


<details>
  <summary>Details</summary>
Motivation: 将选举领域的映射可视化方法扩展到锦标赛领域，帮助理解和可视化不同锦标赛结构之间的关系，以及比较随机生成锦标赛与真实锦标赛的差异

Method: 1. 定义锦标赛为完全有向图；2. 识别有用的距离度量；3. 讨论随机锦标赛生成方法；4. 将锦标赛表示为二维平面上的点，使欧氏距离反映度量距离；5. 应用于淘汰赛等场景

Result: 成功构建了锦标赛地图框架，能够有效可视化不同锦标赛之间的关系，比较了随机生成锦标赛与真实锦标赛的差异，展示了该方法在可视化实验结果方面的实用性

Conclusion: 锦标赛地图是有效的可视化工具，能够帮助研究者理解锦标赛结构、比较不同锦标赛、分析实验结果，特别是在淘汰赛等场景中具有实用价值

Abstract: We form a "map of tournaments" by adapting the map framework from the world of elections. By a tournament we mean a complete directed graph where the nodes are the players and an edge points from a winner of a game to the loser (with no ties allowed). A map is a set of tournaments represented as points on a 2D plane, so that their Euclidean distances resemble the distances computed according to a given measure. We identify useful distance measures, discuss ways of generating random tournaments (and compare them to several real-life ones), and show how the maps are helpful in visualizing experimental results (also for knockout tournaments).

</details>


### [9] [Stable Matching with Deviators and Conformists](https://arxiv.org/abs/2601.18573)
*Frederik Glitzner,David Manlove*

Main category: cs.GT

TL;DR: 研究稳定婚姻和室友问题中，当只有部分代理人（deviators）可能发起偏离时，寻找无deviator阻塞匹配的计算复杂性。发现该问题在二分图和非二分图设置中都是NP完全的，但也识别了多项式时间和固定参数可处理的情况。


<details>
  <summary>Details</summary>
Motivation: 在经典的稳定匹配问题中，所有代理人都可能发起偏离形成阻塞对，导致完全稳定的匹配可能不存在或难以计算。但在实际应用中，只有部分代理人（deviators）可能主动发起偏离，而其他代理人（conformists）会遵守给定的匹配。这引发了新的研究问题：能否高效找到没有deviator阻塞的匹配？

Method: 研究在二分图（稳定婚姻问题）和非二分图（稳定室友问题）偏好设置下，当只有部分代理人被标记为deviators时的计算复杂性。通过理论分析证明问题的NP完全性，并识别多项式时间和固定参数可处理的特例。

Result: 发现与经典设置不同，当只有部分代理人是deviators时，判断是否存在无deviator阻塞匹配的问题是NP完全的。这意味着该问题在计算上是困难的。但也识别了一些可处理的情况，包括多项式时间算法和固定参数可处理的案例。

Conclusion: 该研究揭示了在部分代理人可能发起偏离的设定下，稳定匹配问题的计算复杂性发生了根本变化。虽然问题在一般情况下是NP完全的，但识别出的可处理情况为多智能体系统中无法完全保证稳定性时提供了新的算法设计思路。

Abstract: In the fundamental Stable Marriage and Stable Roommates problems, there are inherent trade-offs between the size and stability of solutions. While in the former problem, a stable matching always exists and can be found efficiently using the celebrated Gale-Shapley algorithm, the existence of a stable matching is not guaranteed in the latter problem, but can be determined efficiently using Irving's algorithm. However, the computation of matchings that minimise the instability, either due to the presence of additional constraints on the size of the matching or due to restrictive preference cycles, gives rise to a collection of infamously intractable almost-stable matching problems. In practice, however, not every agent is able or likely to initiate deviations caused by blocking pairs. Suppose we knew, for example, due to a set of requirements or estimates based on historical data, which agents are likely to initiate deviations - the deviators - and which are likely to comply with whatever matching they are presented with - the conformists. Can we decide efficiently whether a matching exists in which no deviator is blocking, i.e., in which no deviator has an incentive to initiate a deviation? Furthermore, can we find matchings in which only a few deviators are blocking? We characterise the computational complexity of this question in bipartite and non-bipartite preference settings. Surprisingly, these problems prove computationally intractable in strong ways: for example, unlike in the classical setting, where every agent is considered a deviator, in this extension, we prove that it is NP-complete to decide whether a matching exists where no deviator is blocking. On the positive side, we identify polynomial-time and fixed-parameter tractable cases, providing novel algorithmics for multi-agent systems where stability cannot be fully guaranteed.

</details>


### [10] [Learning Real-Life Approval Elections](https://arxiv.org/abs/2601.18651)
*Piotr Faliszewski,Łukasz Janeczko,Andrzej Kaczmarczyk,Marcin Kurdziel,Grzegorz Pierczyński,Stanisław Szufa*

Main category: cs.GT

TL;DR: 研究独立批准模型(IAM)及其混合模型在批准选举中的应用，提出学习算法并应用于真实选举数据，发现混合模型比单组件模型更能捕捉现实数据的复杂性。


<details>
  <summary>Details</summary>
Motivation: 独立批准模型(IAM)可以推广多种现有模型（如公正文化、汉明噪声模型、重采样模型），但需要有效算法来从真实选举数据中学习这些模型及其混合形式，以更好地理解和建模现实世界的批准选举行为。

Method: 提出两种学习算法：最大似然估计和贝叶斯学习，用于学习IAM及其混合模型。将这些算法应用于Pabulib数据库中的大量选举数据，评估单组件模型与混合模型的性能差异。

Result: 研究发现单组件模型很少能充分捕捉现实数据的复杂性，而混合模型表现良好。这表明真实选举数据通常需要多个IAM组件的混合才能准确建模。

Conclusion: IAM混合模型是建模真实批准选举数据的有效工具，比单组件模型更能反映现实选举行为的复杂性，为选举分析和预测提供了更准确的建模框架。

Abstract: We study the independent approval model (IAM) for approval elections, where each candidate has its own approval probability and is approved independently of the other ones. This model generalizes, e.g., the impartial culture, the Hamming noise model, and the resampling model. We propose algorithms for learning IAMs and their mixtures from data, using either maximum likelihood estimation or Bayesian learning. We then apply these algorithms to a large set of elections from the Pabulib database. In particular, we find that single-component models are rarely sufficient to capture the complexity of real-life data, whereas their mixtures perform well.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [11] [AI-based System for Transforming text and sound to Educational Videos](https://arxiv.org/abs/2601.17022)
*M. E. ElAlami,S. M. Khater,M. El. R. Rehan*

Main category: cs.MM

TL;DR: 提出基于GAN的教育视频生成系统，通过语音识别、关键词提取与图像生成、视频合成三阶段，从文本或语音输入自动创建教育视频，在视觉质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前从文本或语音等条件输入生成教育视频仍具挑战性，现有方法在视觉质量和语义对齐方面有待改进，需要更有效的教育视频自动生成技术。

Method: 三阶段框架：1) 语音识别转录输入文本或语音；2) 提取关键词并使用CLIP和扩散模型生成相关图像；3) 将生成的图像合成为视频，并集成预录制或合成音频。

Result: 与TGAN、MoCoGAN、TGANS-C等系统相比，获得了28.75%的FID分数，表明视觉质量提升且优于现有方法。

Conclusion: 提出的GAN框架能够有效生成高质量教育视频，在视觉质量和语义对齐方面表现优异，为教育内容自动生成提供了新方法。

Abstract: Technological developments have produced methods that can generate educational videos from input text or sound. Recently, the use of deep learning techniques for image and video generation has been widely explored, particularly in education. However, generating video content from conditional inputs such as text or speech remains a challenging area. In this paper, we introduce a novel method to the educational structure, Generative Adversarial Network (GAN), which develop frame-for-frame frameworks and are able to create full educational videos. The proposed system is structured into three main phases In the first phase, the input (either text or speech) is transcribed using speech recognition. In the second phase, key terms are extracted and relevant images are generated using advanced models such as CLIP and diffusion models to enhance visual quality and semantic alignment. In the final phase, the generated images are synthesized into a video format, integrated with either pre-recorded or synthesized sound, resulting in a fully interactive educational video. The proposed system is compared with other systems such as TGAN, MoCoGAN, and TGANS-C, achieving a Fréchet Inception Distance (FID) score of 28.75%, which indicates improved visual quality and better over existing methods.

</details>


### [12] [Integrating Fine-Grained Audio-Visual Evidence for Robust Multimodal Emotion Reasoning](https://arxiv.org/abs/2601.18321)
*Zhixian Zhao,Wenjie Tian,Xiaohai Tian,Jun Zhang,Lei Xie*

Main category: cs.MM

TL;DR: SABER-LLM是一个用于鲁棒多模态情感推理的框架，通过构建大规模情感推理数据集和结构化证据分解范式，解决现有MLLMs在细粒度感知和跨模态融合方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在细粒度感知方面存在显著限制，主要由于数据稀缺和跨模态融合不足，导致在复杂多模态交互中出现单模态主导和幻觉问题，特别是在视觉和听觉线索微妙、模糊或矛盾的情况下（如讽刺场景）。

Method: 1. 构建SABER数据集：包含60万个视频片段，采用新颖的六维标注模式，联合捕捉视听线索和因果逻辑。2. 提出结构化证据分解范式：强制"感知-然后推理"的分离，以减轻单模态主导。3. 一致性感知直接偏好优化：在模糊或冲突的感知条件下显式鼓励模态间对齐。

Result: 在EMER、EmoBench-M和SABER-Test上的实验表明，SABER-LLM显著优于开源基线，在解码复杂情感动态方面达到与闭源模型竞争的鲁棒性。

Conclusion: SABER-LLM通过大规模数据集和结构化推理框架，有效解决了多模态情感分析中的细粒度感知和跨模态融合问题，为复杂社会情境中的情感推理提供了鲁棒解决方案。

Abstract: Multimodal emotion analysis is shifting from static classification to generative reasoning. Beyond simple label prediction, robust affective reasoning must synthesize fine-grained signals such as facial micro-expressions and prosodic which shifts to decode the latent causality within complex social contexts. However, current Multimodal Large Language Models (MLLMs) face significant limitations in fine-grained perception, primarily due to data scarcity and insufficient cross-modal fusion. As a result, these models often exhibit unimodal dominance which leads to hallucinations in complex multimodal interactions, particularly when visual and acoustic cues are subtle, ambiguous, or even contradictory (e.g., in sarcastic scenery). To address this, we introduce SABER-LLM, a framework designed for robust multimodal reasoning. First, we construct SABER, a large-scale emotion reasoning dataset comprising 600K video clips, annotated with a novel six-dimensional schema that jointly captures audiovisual cues and causal logic. Second, we propose the structured evidence decomposition paradigm, which enforces a "perceive-then-reason" separation between evidence extraction and reasoning to alleviate unimodal dominance. The ability to perceive complex scenes is further reinforced by consistency-aware direct preference optimization, which explicitly encourages alignment among modalities under ambiguous or conflicting perceptual conditions. Experiments on EMER, EmoBench-M, and SABER-Test demonstrate that SABER-LLM significantly outperforms open-source baselines and achieves robustness competitive with closed-source models in decoding complex emotional dynamics. The dataset and model are available at https://github.com/zxzhao0/SABER-LLM.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [13] [Context Lake: A System Class Defined by Decision Coherence](https://arxiv.org/abs/2601.17019)
*Xiaowei Jiang*

Main category: cs.DB

TL;DR: 论文提出"决策一致性定律"，指出AI代理在共享资源上做不可逆决策时需要基于一致的现实表征，现有系统无法满足此要求，因此提出"上下文湖"作为必要的新型系统架构。


<details>
  <summary>Details</summary>
Motivation: 传统数据系统为人类分析周期设计，而AI代理持续运行并做出不可逆的并发决策。当多个代理在共享资源上操作时，它们的行动在能够协调之前就已经相互作用，导致传统的事后正确性保证无法防止冲突。

Method: 提出决策一致性定律，证明现有系统类无法满足此要求，并通过组合不可能定理证明独立推进的系统无法组合提供决策一致性。由此推导出上下文湖作为必要系统类，需满足三个要求：语义操作作为原生能力、所有决策相关状态的事务一致性、以及限制陈旧度和负载下降的操作边界。

Result: 建立了集体代理系统中正确性的理论基础，形式化了架构不变量、执行边界和可接受条件，为AI代理在大规模下建设性操作提供了系统保证要求。

Conclusion: 上下文湖是支持AI代理在共享资源上做出正确不可逆决策的必要系统架构，现有架构无法满足决策一致性要求，需要新的系统保证来确保AI代理在大规模下的建设性操作。

Abstract: AI agents are increasingly the primary consumers of data, operating continuously to make concurrent, irreversible decisions. Traditional data systems designed for human analysis cycles become correctness bottlenecks under this operating regime. When multiple agents operate over shared resources, their actions interact before reconciliation is possible. Correctness guarantees that apply after the decision window therefore fail to prevent conflicts. We introduce the Decision Coherence Law: for agents that take irreversible actions whose effects interact, correctness requires that interacting decisions be evaluated against a coherent representation of reality at the moment they are made. We show that no existing system class satisfies this requirement and prove through the Composition Impossibility Theorem that independently advancing systems cannot be composed to provide Decision Coherence while preserving their native system classes. From this impossibility result, we derive Context Lake as a necessary system class with three requirements: (1) semantic operations as native capabilities, (2) transactional consistency over all decision-relevant state, and (3) operational envelopes bounding staleness and degradation under load. We formalize the architectural invariants, enforcement boundaries, and admissibility conditions required for correctness in collective agent systems. This position paper establishes the theoretical foundation for Context Lakes, identifies why existing architectures fail, and specifies what systems must guarantee for AI agents to operate constructively at scale.

</details>


### [14] [Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs](https://arxiv.org/abs/2601.17058)
*Wei Zhou,Jun Zhou,Haoyu Wang,Zhenghao Li,Qikang He,Shaokun Han,Guoliang Li,Xuanhe Zhou,Yeye He,Chunwei Liu,Zirui Tang,Bin Wang,Shen Tang,Kai Zuo,Yuyu Luo,Zhenzhe Zheng,Conghui He,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: 本文系统综述了LLM增强的数据准备方法，分析了从传统规则驱动到基于提示、上下文感知的智能代理工作流的范式转变，提出了涵盖数据清洗、集成和增强的任务中心分类法，并讨论了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 数据准备对于数据驱动应用至关重要，但传统方法面临局限性。随着对应用就绪数据需求的增长、LLM技术的进步以及灵活代理基础设施的出现，LLM增强方法正在成为数据准备的变革性范式，需要系统性的综述来梳理这一快速发展领域。

Method: 通过调研数百篇近期文献，采用系统综述方法，首先分析范式转变，然后提出任务中心分类法（数据清洗、数据集成、数据增强三大任务），对每类任务调查代表性技术并分析其优缺点，最后分析常用数据集和评估指标。

Result: 系统梳理了LLM增强数据准备的技术现状，识别了当前方法的优势（如改进的泛化能力、语义理解）和局限性（如LLM扩展成本高、幻觉问题、先进方法与弱评估之间的不匹配），并总结了常用数据集和评估实践。

Conclusion: LLM增强的数据准备正在经历范式转变，但仍面临诸多挑战。未来研究方向包括可扩展的LLM-数据系统、可靠的智能代理工作流设计原则以及鲁棒的评估协议，需要跨学科合作来推动这一领域的发展。

Abstract: Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.
  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.

</details>


### [15] [Vidformer: Drop-in Declarative Optimization for Rendering Video-Native Query Results](https://arxiv.org/abs/2601.17221)
*Dominik Winecki,Arnab Nandi*

Main category: cs.DB

TL;DR: Vidformer是一个视频原生查询的渲染加速器，通过透明优化和并行化渲染，将播放时间从分钟级降至亚秒级，实现交互式视频查询


<details>
  <summary>Details</summary>
Motivation: 视频原生查询的瓶颈在于渲染而非查询执行。现有后处理脚本速度慢，导致用户等待时间长，阻碍了交互式视频数据分析

Method: 1) 透明地将现有可视化代码提升为声明式表示；2) 透明地优化和并行化渲染；3) 通过视频点播协议即时提供视频，实现实时分段渲染

Result: 渲染时间减少2-3倍，播放时间降至0.25-0.5秒（提升400倍），解耦了剪辑长度与首帧播放延迟，支持亚秒级交互式视频查询

Conclusion: Vidformer显著加速视频原生查询渲染，实现亚秒级交互体验，并支持基于LLM的交互式对话查询，解决了视频数据分析中的关键瓶颈

Abstract: When interactively exploring video data, video-native querying involves consuming query results as videos, including steps such as compilation of extracted video clips or data overlays. These video-native queries are bottlenecked by rendering, not the execution of the underlying queries. This rendering is currently performed using post-processing scripts that are often slow. This step poses a critical point of friction in interactive video data workloads: even short clips contain thousands of high-definition frames; conventional OpenCV/Python scripts must decode -> transform -> encode the entire data stream before a single pixel appears, leaving users waiting for many seconds, minutes, or hours.
  To address these issues, we present Vidformer, a drop-in rendering accelerator for video-native querying which, (i) transparently lifts existing visualization code into a declarative representation, (ii) transparently optimizes and parallelizes rendering, and (iii) instantly serves videos through a Video on Demand protocol with just-in-time segment rendering. We demonstrate that Vidformer cuts full-render time by 2-3x across diverse annotation workloads, and, more critically, drops time-to-playback to 0.25-0.5s. This represents a 400x improvement that decouples clip length from first-frame playback latency, and unlocks the ability to perform interactive video-native querying with sub-second latencies. Furthermore, we show how our approach enables interactive video-native LLM-based conversational querying as well.

</details>


### [16] [Constant-time Connectivity and 2-Edge Connectivity Querying in Dynamic Graphs](https://arxiv.org/abs/2601.17285)
*Lantian Xu,Junhua Zhang,Dong Wen,Lu Qin,Ying Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: 提出了一种基于生成树和并查树结合的新方法，用于处理全动态图中的连通性查询，实现了常数时间查询复杂度，并在边插入和删除操作上显著提升了理论运行时间


<details>
  <summary>Details</summary>
Motivation: 图处理中的连通性查询是一个基本问题，在现实图应用中边频繁更新，需要研究全动态图（边频繁插入或删除）中的连通性查询处理

Method: 提出了一种基于生成树的新解决方案，通过同时维护一个并查树，结合两种树的优势，实现了常数查询时间，并显著提高了边插入和删除的理论运行时间

Result: 在真实大型数据集上的性能研究表明，该算法相比现有方法有显著改进，并将连通性维护算法扩展到维护2-边连通性

Conclusion: 通过结合生成树和并查树的优势，提出了一种高效的全动态图连通性查询处理方法，实现了常数查询时间，并在边更新操作上取得了理论性能提升

Abstract: Connectivity query processing is a fundamental problem in graph processing. Given an undirected graph and two query vertices, the problem aims to identify whether they are connected via a path. Given frequent edge updates in real graph applications, in this paper, we study connectivity query processing in fully dynamic graphs, where edges are frequently inserted or deleted. A recent solution, called D-tree, maintains a spanning tree for each connected component and applies several heuristics to reduce the depth of the tree. To improve efficiency, we propose a new spanning-tree-based solution by maintaining a disjoint-set tree simultaneously. By combining the advantages of two trees, we achieve the constant query time complexity and also significantly improve the theoretical running time in both edge insertion and edge deletion. In addition, we extend our connectivity maintenance algorithms to maintain 2-edge connectivity. Our performance studies on real large datasets show considerable improvement of our algorithms.

</details>


### [17] [UTune: Towards Uncertainty-Aware Online Index Tuning](https://arxiv.org/abs/2601.18199)
*Chenning Wu,Sifan Chen,Wentao Wu,Yinan Jing,Zhenying He,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: UTune：一个不确定性感知的在线索引调优框架，通过操作符级学习模型和不确定性量化机制，解决在线索引调优中训练数据有限和工作负载漂移的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有学习型索引效益估计器在在线调优场景中存在两个主要挑战：1）可用于训练模型的查询执行反馈有限；2）由于工作负载漂移，不断出现新的未见查询。这两个问题共同限制了现有学习型索引效益估计器的泛化能力。

Method: UTune采用操作符级学习模型，并引入不确定性量化机制来表征模型在有限在线执行反馈下的固有不确定性。通过开发一种新的ε-greedy搜索策略变体，将不确定性信息整合到索引选择和配置枚举中，使用不确定性加权的索引效益。

Result: 实验评估表明，UTune不仅显著改善了与最先进在线索引调优器相比的工作负载执行时间，还减少了索引探索开销，在工作负载相对稳定时实现更快的收敛。

Conclusion: UTune通过不确定性感知的在线索引调优框架，有效解决了学习型索引效益估计器在在线场景中的泛化问题，提高了索引调优的效率和效果。

Abstract: There have been a flurry of recent proposals on learned benefit estimators for index tuning. Although these learned estimators show promising improvement over what-if query optimizer calls in terms of the accuracy of estimated index benefit, they face significant limitations when applied to online index tuning, an arguably more common and more challenging scenario in real-world applications. There are two major challenges for learned index benefit estimators in online tuning: (1) limited amount of query execution feedback that can be used to train the models, and (2) constant coming of new unseen queries due to workload drifts. The combination of the two hinders the generalization capability of existing learned index benefit estimators. To overcome these challenges, we present UTune, an uncertainty-aware online index tuning framework that employs operator-level learned models with improved generalization over unseen queries. At the core of UTune is an uncertainty quantification mechanism that characterizes the inherent uncertainty of the operator-level learned models given limited online execution feedback. We further integrate uncertainty information into index selection and configuration enumeration, the key component of any index tuner, by developing a new variant of the classic $ε$-greedy search strategy with uncertainty-weighted index benefits. Experimental evaluation shows that UTune not only significantly improves the workload execution time compared to state-of-the-art online index tuners but also reduces the index exploration overhead, resulting in faster convergence when the workload is relatively stable.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [18] [Frequency-aware Adaptive Contrastive Learning for Sequential Recommendation](https://arxiv.org/abs/2601.17057)
*Zhikai Wang,Weihua Zhang*

Main category: cs.IR

TL;DR: FACL框架通过频率感知的自适应对比学习，解决序列推荐中数据增强对低频物品和稀疏用户行为的偏见问题，显著提升推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习中的数据增强方法在序列推荐中存在固有偏见，会损害低频物品和稀疏用户行为的完整性，影响推荐系统在真实长尾场景中的性能。

Method: 提出FACL框架：1）微观层面的自适应扰动保护稀有物品完整性；2）宏观层面的重加权放大稀疏和稀有交互序列在训练中的影响。

Result: 在五个公开基准数据集上，FACL持续优于最先进的数据增强和模型增强方法，推荐准确率提升最高达3.8%，显著缓解了低频物品和用户的性能下降。

Conclusion: FACL框架具有强大的意图保持能力和对真实世界长尾推荐场景的优越适用性，为序列推荐中的对比学习提供了更公平有效的解决方案。

Abstract: In this paper, we revisited the role of data augmentation in contrastive learning for sequential recommendation, revealing its inherent bias against low-frequency items and sparse user behaviors. To address this limitation, we proposed FACL, a frequency-aware adaptive contrastive learning framework that introduces micro-level adaptive perturbation to protect the integrity of rare items, as well as macro-level reweighting to amplify the influence of sparse and rare-interaction sequences during training. Comprehensive experiments on five public benchmark datasets demonstrated that FACL consistently outperforms state-of-the-art data augmentation and model augmentation-based methods, achieving up to 3.8% improvement in recommendation accuracy. Moreover, fine-grained analyses confirm that FACL significantly alleviates the performance drop on low-frequency items and users, highlighting its robust intent-preserving ability and its superior applicability to real-world, long-tail recommendation scenarios.

</details>


### [19] [Evaluation on Entity Matching in Recommender Systems](https://arxiv.org/abs/2601.17218)
*Zihan Huang,Rohan Surana,Zhouhang Xie,Junda Wu,Yu Xia,Julian McAuley*

Main category: cs.IR

TL;DR: 提出了Reddit-Amazon-EM数据集，用于评估推荐系统中的实体匹配方法，包含Reddit和Amazon'23数据集中的电影实体对应关系，并评估了多种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统（如对话推荐系统、知识推荐系统）中实体匹配缺乏严格的跨数据集评估框架，阻碍了LLM驱动的对话推荐和知识数据集构建的进展。

Method: 构建Reddit-Amazon-EM数据集，通过人工标注识别Reddit-Movies和Amazon'23中对应的电影实体，并全面评估规则、图、词法、嵌入和LLM等多种实体匹配方法。

Result: 创建了手动标注的实体匹配黄金集，提供了两个数据集间的映射关系（使用实验中最优方法），为推荐系统实体匹配研究提供了有价值的资源。

Conclusion: Reddit-Amazon-EM数据集填补了跨数据集实体匹配评估的空白，促进了推荐系统中实体匹配方法的研究和发展。

Abstract: Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.
  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.
  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.

</details>


### [20] [FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search](https://arxiv.org/abs/2601.17333)
*Lalit Pant,Shivang Nagar*

Main category: cs.IR

TL;DR: 本文提出了一个针对金融知识搜索的现代自然语言查询系统技术蓝图，通过结合NLP、搜索工程和向量数据模型，提升金融数据检索的精确度、召回率和深度洞察能力。


<details>
  <summary>Details</summary>
Motivation: 传统金融知识搜索方法存在精度和召回率不足的问题，难以有效连接分散的金融对象、事件和关系。自然语言查询能够用人类语言与信息系统交互，解决金融数据检索中的发现、相关性排序、数据新鲜度和实体识别等关键挑战。

Method: 结合自然语言处理、搜索工程和向量数据模型的核心构建块，设计了包含离线索引和在线检索的架构组件。详细阐述了金融数据集和文档的NLQ独特需求，并提供了实验方法、数据使用和结果分析。

Result: 提出的系统相比传统方法在知识搜索的精确度和召回率方面都有提升，能够更有效地连接金融对象、事件和关系，为金融服务提供增强的知识搜索实际用例。

Conclusion: 该研究为金融领域的自然语言查询系统提供了全面的技术蓝图和理论支持，展示了NLQ在金融知识搜索中的实际价值，并指出了未来优化的方向。

Abstract: Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.

</details>


### [21] [Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction](https://arxiv.org/abs/2601.17339)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 本文提出一个面向下游应用的QPP评估框架，通过将QPP估计分布作为IR融合的先验，评估QPP在实际IR管道中的实用价值，发现传统相关性评估与下游效果不匹配。


<details>
  <summary>Details</summary>
Motivation: 传统的QPP评估采用集合级相关性度量，无法量化单个查询级别的效果，也不连接下游应用。高相关性的QPP方法在实际IR管道中可能无法有效支持查询特定决策。

Method: 提出下游聚焦的评估框架：将多个排序器检索的top文档列表中QPP估计分布作为IR融合的先验。一方面，这些估计分布与真实检索质量分布的匹配度反映预测器质量；另一方面，作为先验使用反映预测器在IR管道中做出明智决策的能力。

Result: 实验表明：1) QPP估计在加权IR融合中至关重要，相比未加权的CombSUM和RRF融合策略提升超过4.5%；2) QPP的下游效果与标准相关性评估不相关，揭示了新的见解。

Conclusion: 需要重新思考QPP评估方法，从单纯的相关性度量转向考虑实际应用价值的下游评估框架，以更好地评估QPP方法在真实IR系统中的实用性能。

Abstract: The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.

</details>


### [22] [Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework](https://arxiv.org/abs/2601.17359)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 该研究将查询性能预测任务扩展为三个设置：单排序器多查询、多排序器单查询和多排序器多查询，发现不同任务中QPP模型效果差异显著，且预测最佳排序器比预测查询难度更困难。


<details>
  <summary>Details</summary>
Motivation: 传统查询性能预测主要关注单个排序器下不同查询的性能差异，但更细粒度的挑战是确定哪些排序器对特定查询最有效。研究旨在将QPP任务及其评估推广到更全面的框架中。

Method: 将QPP任务形式化为三个设置：1) SRMQ-PP：传统单排序器多查询预测；2) MRSQ-PP：多排序器单查询预测，评估QPP模型选择最佳排序器的能力；3) MRMQ-PP：多排序器多查询预测，考虑所有查询-排序器对的联合预测。

Result: 研究发现：1) QPP模型的相对有效性在不同任务间差异显著；2) 预测特定查询的最佳排序器比预测单个排序器下查询的相对难度要困难得多。

Conclusion: 该研究扩展了QPP任务的评估框架，揭示了不同预测任务间的性能差异，为更全面的查询性能预测研究提供了基础，并强调了预测最佳排序器这一挑战的重要性。

Abstract: The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.

</details>


### [23] [UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization](https://arxiv.org/abs/2601.17438)
*Jialei Li,Yang Zhang,Yimeng Bai,Shuai Zhu,Ziqi Xue,Xiaoyan Zhao,Dingxian Wang,Frank Yang,Andrew Rabinovich,Xiangnan He*

Main category: cs.IR

TL;DR: UniGRec是一个统一的生成式推荐框架，通过可微软项目标识符将分词器和推荐器统一在推荐目标下，解决了训练-推理差异、标识符崩溃和协同信号不足三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法通常将分词与推荐解耦或采用异步交替优化，限制了端到端对齐。需要统一分词器和推荐器以实现完全端到端训练。

Method: 提出UniGRec框架：1) 退火推理对齐平滑连接软训练和硬推理；2) 码字均匀性正则化防止标识符崩溃并促进码本多样性；3) 双重协同蒸馏机制从轻量级教师模型提取协同先验来共同指导分词器和推荐器。

Result: 在真实世界数据集上的广泛实验表明，UniGRec始终优于最先进的基线方法。

Conclusion: UniGRec通过统一分词器和推荐器的端到端训练，有效解决了生成式推荐中的关键挑战，实现了更好的推荐性能。

Abstract: Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.
  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.

</details>


### [24] [Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval](https://arxiv.org/abs/2601.18747)
*Amir Aavani*

Main category: cs.IR

TL;DR: 提出一种基于有向无环图(DAG)的检索语言，能够高效评估多项式时间属性，解决神经符号推理工作流中的效率困境。


<details>
  <summary>Details</summary>
Motivation: 现代信息检索正从简单文档过滤转向复杂的神经符号推理工作流，但现有检索架构在处理逻辑和算术约束时面临效率困境：基于迭代器的引擎无法高效处理复杂嵌套逻辑图，而递归方法虽然能支持这些结构但内存消耗过高。

Method: 提出基于有向无环图(DAG)的形式化检索语言(ℒ_R)，证明其精确捕获复杂度类P。引入ComputePN算法，结合原生DAG遍历和内存高效的"正-负"响应机制，确保ℒ_R中任何查询的高效评估。

Result: 建立了将搜索索引转变为通用计算引擎的理论基础，通过ℒ_R语言和ComputePN算法实现了对多项式时间属性的高效评估，解决了现有检索架构在处理复杂逻辑约束时的效率问题。

Conclusion: 该工作为检索引擎能够直接在其索引上高效评估任何多项式时间属性提供了理论基础，实现了从简单文档过滤到复杂神经符号推理工作流的转变，使搜索索引成为通用计算引擎。

Abstract: Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.

</details>


### [25] [Adversarial Alignment and Disentanglement for Cross-Domain CTR Prediction with Domain-Encompassing Features](https://arxiv.org/abs/2601.17472)
*Junyou He,Lixi Deng,Huichao Guo,Ye Tang,Yong Li,Sulong Xu*

Main category: cs.IR

TL;DR: 提出A²DCDR模型，通过对抗对齐和特征解耦改进跨域推荐，结合域不变特征、非对齐特征和原始上下文数据，提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法通常只使用域不变特征结合目标域特定特征，导致性能不理想，需要更全面的跨域信息捕获方法

Method: 提出A²DCDR模型，包含三个关键组件：1) 使用对抗训练改进MMD以获得更好泛化；2) 特征解耦器和重建机制实现域内解耦；3) 融合域不变特征、非对齐特征和原始上下文数据的新表示方法

Result: 在真实数据集和在线A/B测试中，A²DCDR优于现有方法，验证了其有效性和实际应用价值

Conclusion: A²DCDR通过全面捕获跨域信息（包括域不变和有价值的非对齐特征），显著提升了跨域推荐性能，具有实际应用价值

Abstract: Cross-domain recommendation (CDR) has been increasingly explored to address data sparsity and cold-start issues. However, recent approaches typically disentangle domain-invariant features shared between source and target domains, as well as domain-specific features for each domain. However, they often rely solely on domain-invariant features combined with target domain-specific features, which can lead to suboptimal performance. To overcome the limitations, this paper presents the Adversarial Alignment and Disentanglement Cross-Domain Recommendation ($A^2DCDR$ ) model, an innovative approach designed to capture a comprehensive range of cross-domain information, including both domain-invariant and valuable non-aligned features. The $A^2DCDR$ model enhances cross-domain recommendation through three key components: refining MMD with adversarial training for better generalization, employing a feature disentangler and reconstruction mechanism for intra-domain disentanglement, and introducing a novel fused representation combining domain-invariant, non-aligned features with original contextual data. Experiments on real-world datasets and online A/B testing show that $A^2DCDR$ outperforms existing methods, confirming its effectiveness and practical applicability. The code is provided at https://github.com/youzi0925/A-2DCDR/tree/main.

</details>


### [26] [Towards Fair Large Language Model-based Recommender Systems without Costly Retraining](https://arxiv.org/abs/2601.17492)
*Jin Li,Huilin Gu,Shoujin Wang,Qi Zhang,Shui Yu,Chen Wang,Xiwei Xu,Fang Chen*

Main category: cs.IR

TL;DR: FUDLR提出了一种快速统一的LLM推荐系统去偏方法，将去偏问题重构为高效机器遗忘任务，通过偏差无关掩码识别偏差样本并进行高效参数影响移除。


<details>
  <summary>Details</summary>
Motivation: LLM推荐系统容易无意中延续训练数据中的偏差，导致严重公平性问题。现有去偏方法面临两大挑战：1) 针对特定偏差类型设计，缺乏处理多样化或新兴偏差的通用性；2) 依赖重新训练的方法在LLM巨大参数量下计算不可行。

Method: FUDLR采用两阶段方法：1) 通过新颖的偏差无关掩码识别需要遗忘的偏差诱导样本，优化公平性提升与准确性保持的平衡；2) 通过估计并移除已识别样本对模型参数的影响，实现高效去偏。该方法可适应各种或共存的偏差，只需纳入不同的公平性指标。

Result: 大量实验表明，FUDLR能有效且高效地提升公平性，同时保持推荐准确性，为构建社会责任的LLM推荐系统提供了实用路径。

Conclusion: FUDLR通过将去偏问题重构为高效机器遗忘任务，解决了LLM推荐系统中的公平性问题，提供了一种通用、计算可行的去偏方法，能够适应多种偏差类型，在保持推荐性能的同时显著提升公平性。

Abstract: Large Language Models (LLMs) have revolutionized Recommender Systems (RS) through advanced generative user modeling. However, LLM-based RS (LLM-RS) often inadvertently perpetuates bias present in the training data, leading to severe fairness issues. Addressing these fairness problems in LLM-RS faces two significant challenges. 1) Existing debiasing methods, designed for specific bias types, lack the generality to handle diverse or emerging biases in real-world applications. 2) Debiasing methods relying on retraining are computationally infeasible given the massive parameter scale of LLMs. To overcome these challenges, we propose FUDLR (Fast Unified Debiasing for LLM-RS). The core idea is to reformulate the debiasing problem as an efficient machine unlearning task with two stages. First, FUDLR identifies bias-inducing samples to unlearn through a novel bias-agnostic mask, optimized to balance fairness improvement with accuracy preservation. Its bias-agnostic design allows adaptability to various or co-existing biases simply by incorporating different fairness metrics. Second, FUDLR performs efficient debiasing by estimating and removing the influence of identified samples on model parameters. Extensive experiments demonstrate that FUDLR effectively and efficiently improves fairness while preserving recommendation accuracy, offering a practical path toward socially responsible LLM-RS. The code and data are available at https://github.com/JinLi-i/FUDLR.

</details>


### [27] [To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval](https://arxiv.org/abs/2601.17500)
*Emmanouil Georgios Lionis,Jia-Huei Ju,Angelos Nalmpantis,Casper Thuis,Sean MacAvaney,Andrew Yates*

Main category: cs.IR

TL;DR: 研究发现在稀疏检索中，使用大小写敏感的基础模型默认性能显著差于大小写不敏感模型，但通过文本小写化预处理可以消除这一差距。


<details>
  <summary>Details</summary>
Motivation: 现有LSR方法几乎完全依赖大小写不敏感的基础模型，但最新的SOTA语言模型只有大小写敏感版本。基础模型大小写敏感性对LSR的影响尚未研究，这可能影响方法的未来发展。

Method: 系统评估同一基础模型的大小写敏感和大小写不敏感版本在多个数据集上的表现，并通过文本小写化预处理和token级分析来研究性能差异的原因。

Result: 大小写敏感LSR模型默认性能显著差于大小写不敏感版本，但通过文本小写化预处理可以完全消除性能差距。token级分析显示，在小写化处理下，大小写敏感模型几乎完全抑制了大小写敏感的词汇项，行为类似于大小写不敏感模型。

Conclusion: 这一发现扩展了最新大小写敏感模型在LSR设置中的适用性，促进了更强基础架构在稀疏检索中的集成。通过简单的小写化预处理，大小写敏感模型可以达到与大小写不敏感模型相当的性能。

Abstract: Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR

</details>


### [28] [Pipeline Inspection, Visualization, and Interoperability in PyTerrier](https://arxiv.org/abs/2601.17502)
*Emmanouil Georgios Lionis,Craig Macdonald,Sean MacAvaney*

Main category: cs.IR

TL;DR: PyTerrier是一个声明式信息检索框架，新增了可编程检查、可视化和工具集成功能，旨在帮助研究人员、学生和AI代理更好地理解和使用各种IR管道。


<details>
  <summary>Details</summary>
Motivation: 当前信息检索管道缺乏可编程检查、可视化和与其他工具集成的能力，这使得研究人员、学生和AI代理难以理解和有效使用各种IR管道。

Method: 通过引入模型上下文协议（MCP）和增强的管道操作，使IR管道能够被程序化检查、可视化，并与其他工具集成。

Result: PyTerrier框架现在提供了改进的管道操作，支持程序化检查、可视化，并通过MCP协议实现与其他工具的集成。

Conclusion: 这些新功能使PyTerrier成为更易于理解、使用和集成的信息检索框架，特别有利于研究人员、学生和AI代理的工作。

Abstract: PyTerrier provides a declarative framework for building and experimenting with Information Retrieval (IR) pipelines. In this demonstration, we highlight several recent pipeline operations that improve their ability to be programmatically inspected, visualized, and integrated with other tools (via the Model Context Protocol, MCP). These capabilities aim to make it easier for researchers, students, and AI agents to understand and use a wide array of IR pipelines.

</details>


### [29] [Real-Time Trend Prediction via Continually-Aligned LLM Query Generation](https://arxiv.org/abs/2601.17567)
*Zijing Hui,Wenhan Lyu,Shusen Wang,Li Chen,Chu Wang*

Main category: cs.IR

TL;DR: RTTP是一个实时趋势预测框架，通过LLM直接从新闻内容生成搜索查询，解决低流量搜索环境中的冷启动问题，在Facebook和Meta AI产品中部署后显著提升了长尾趋势检测精度。


<details>
  <summary>Details</summary>
Motivation: 低流量搜索环境中存在冷启动问题，传统基于关键词频率或查询峰值的方法在稀疏设置下反应缓慢且效果不佳，无法及时识别新兴或长尾趋势。

Method: 提出RTTP框架，使用持续学习LLM将新闻帖子转换为搜索式查询，并通过参与度强度和创作者权威性进行评分；提出Mix-Policy DPO方法，结合在线策略稳定性和离线策略新颖性，在模型升级时减轻灾难性遗忘。

Result: 在Facebook和Meta AI产品中部署后，RTTP在尾部趋势检测精度@500上提升了91.4%，查询生成准确率比行业基准提高了19%，并在多周在线训练后保持稳定性能。

Conclusion: 研究表明，当LLM生成的合成搜索信号经过对齐和持续更新时，能够在低流量搜索环境中实现及时的趋势理解，解决了传统方法的延迟问题。

Abstract: Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.

</details>


### [30] [Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts](https://arxiv.org/abs/2601.17601)
*Fangping Lan,Abdullah Aljebreen,Eduard C. Dragut*

Main category: cs.IR

TL;DR: 该研究开发了一个社交媒体超链接意图分类体系，通过众包标注和LLM辅助构建了6个大类26个细分类别的意图分类法，发现广告、争论和分享是最常见的超链接意图。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中约20%的推文包含URL，但以往研究主要关注作者分享链接的动机，这些作者意图在实践中难以观察。为了支持更广泛的下游应用，本研究转向读者视角，探究用户如何理解帖子中超链接的意图。

Method: 采用混合方法：首先通过大规模众包标注进行自下而上的数据驱动过程，然后利用大语言模型辅助生成描述性类别名称和精确定义，最终构建包含6个顶层类别和26个细粒度意图类别的分类体系，并应用于1000个用户帖子的标注分析。

Result: 构建了全面的社交媒体超链接意图分类体系，分析发现广告、争论和分享是最普遍的意图。该分类法为意图感知的信息检索和NLP应用提供了基础。

Conclusion: 该研究开发的读者中心超链接意图分类体系能够更准确地支持社交媒体内容的检索、推荐和理解，为意图感知的信息处理应用提供了实用框架。

Abstract: URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.

</details>


### [31] [Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests](https://arxiv.org/abs/2601.17617)
*Jingjie Ning,João Coelho,Yibo Kong,Yunfan Long,Bruno Martins,João Magalhães,Jamie Callan,Chenyan Xiong*

Main category: cs.IR

TL;DR: 大规模日志分析揭示LLM搜索代理的行为模式：90%多轮会话不超过10步，89%步骤间隔小于1分钟；不同意图行为差异大，事实查询重复率高，推理任务探索更广；代理会跨步骤复用证据，54%新查询词来自累积上下文。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的搜索代理越来越多用于多步信息检索任务，但IR社区缺乏对代理搜索会话如何展开以及检索证据如何使用的实证理解。需要大规模日志分析来揭示这些行为模式。

Method: 基于DeepResearchGym开源搜索API的1444万搜索请求（397万会话）进行大规模日志分析。使用会话化处理，通过LLM标注分配会话级意图和逐步查询重构标签，并提出上下文驱动的术语采纳率（CTAR）来量化新引入查询词是否可追溯到先前检索证据。

Result: 发现三个主要行为模式：1）90%多轮会话不超过10步，89%步骤间隔小于1分钟；2）不同意图行为差异显著，事实查询会话重复率高且随时间增加，推理任务会话维持更广泛探索；3）代理跨步骤复用证据，平均54%新查询词出现在累积证据上下文中，早期步骤贡献超出最近检索。

Conclusion: 代理搜索可能受益于重复感知的早期停止、意图自适应的检索预算和显式的跨步骤上下文跟踪。研究计划发布匿名化日志以支持未来研究。

Abstract: LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.

</details>


### [32] [LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval](https://arxiv.org/abs/2601.17692)
*Yunhan Li,Mingjie Xie,Gaoli Kang,Zihan Gong,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: LegalMALR：一个结合多智能体查询理解系统与零样本大语言模型重排序模块的法律法规检索框架，显著提升对隐式、多问题、口语化法律查询的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的法律查询往往是隐式的、多问题的，并以口语化或未充分说明的形式表达，这使得传统的检索增强生成管道难以准确恢复所需的法规要素。密集检索器主要关注查询的字面形式，而轻量级重排序器缺乏评估法规适用性所需的法律推理能力。

Method: 提出LegalMALR框架：1）多智能体查询理解系统（MAS）生成多样化的、有法律依据的查询重构，并进行迭代密集检索以扩大候选范围；2）使用广义强化策略优化（GRPO）统一优化MAS策略以稳定LLM生成重构的随机行为；3）LLM重排序模块通过自然语言法律推理对累积的候选集进行评估并生成最终排序。

Result: 构建了CSAID数据集（包含118个困难中文法律查询，标注了多个法规标签），并在CSAID和公开STARD基准上评估LegalMALR。实验表明，LegalMALR在分布内和分布外设置中都显著优于强大的检索增强生成基线。

Conclusion: 结合多视角查询解释、基于强化的策略优化和大模型重排序的方法对法规检索是有效的，LegalMALR框架能够显著提升对复杂法律查询的检索性能。

Abstract: Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.

</details>


### [33] [Token-Weighted Multi-Target Learning for Generative Recommenders with Curriculum Learning](https://arxiv.org/abs/2601.17787)
*Wei-Ning Chiu,Chuan-Ju Wang,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: 提出两种基于信息增益的token加权策略（Front-Greater Weighting和Frequency Weighting）和课程学习多目标框架，用于提升基于语义ID的生成式推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统使用标准next-token似然优化，将所有token视为同等重要，这与基于语义ID的生成任务不匹配，需要更精细的token重要性建模。

Method: 1) Front-Greater Weighting：基于条件信息增益，优先考虑能有效减少候选物品不确定性的早期token；2) Frequency Weighting：基于边际信息增益，对稀有token进行加权以对抗流行度偏差；3) 多目标学习框架结合课程学习，联合优化两个加权目标和标准似然。

Result: 在基准数据集上，该方法持续优于强基线模型和现有token加权方法，表现出更好的鲁棒性、对不同语义ID构建的强泛化能力，以及在头部和尾部物品上的显著性能提升。

Conclusion: 提出的token加权策略和多目标学习框架有效解决了语义ID生成式推荐中的token重要性不平衡问题，显著提升了推荐性能，特别是在处理长尾分布和减少流行度偏差方面。

Abstract: Generative recommender systems have recently attracted attention by formulating next-item prediction as an autoregressive sequence generation task. However, most existing methods optimize standard next-token likelihood and implicitly treat all tokens as equally informative, which is misaligned with semantic-ID-based generation. Accordingly, we propose two complementary information-gain-based token-weighting strategies tailored to generative recommendation with semantic IDs. Front-Greater Weighting captures conditional semantic information gain by prioritizing early tokens that most effectively reduce candidate-item uncertainty given their prefixes and encode coarse semantics. Frequency Weighting models marginal information gain under long-tailed item and token distributions, upweighting rare tokens to counteract popularity bias. Beyond individual strategies, we introduce a multi-target learning framework with curriculum learning that jointly optimizes the two token-weighted objectives alongside standard likelihood, enabling stable optimization and adaptive emphasis across training stages. Extensive experiments on benchmark datasets show that our method consistently outperforms strong baselines and existing token-weighting approaches, with improved robustness, strong generalization across different semantic-ID constructions, and substantial gains on both head and tail items. Code is available at https://github.com/CHIUWEINING/Token-Weighted-Multi-Target-Learning-for-Generative-Recommenders-with-Curriculum-Learning.

</details>


### [34] [Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction](https://arxiv.org/abs/2601.17836)
*Weijiang Lai,Beihong Jin,Di Zhang,Siru Chen,Jiongyan Zhang,Yuhang Gou,Jian Dong,Xingxing Wang*

Main category: cs.IR

TL;DR: SparseCTR：一种针对用户长行为序列的高效推荐模型，通过个性化分块和三分支稀疏自注意力机制，在提升效率的同时展现明显的缩放定律现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的成功推动了推荐系统中缩放定律的探索，但标准自注意力机制的高计算复杂度使其难以在工业场景中部署来处理长用户行为序列。现有的稀疏自注意力机制不完全适用于推荐场景，因为用户行为具有个性化和时序特性：不同用户有独特的行为模式，这些模式随时间变化，且数据分布与其他领域显著不同。

Method: 1. 个性化分块：将行为序列按个性化方式分块，避免分离连续行为并支持并行处理。2. 三分支稀疏自注意力机制：联合识别用户的全局兴趣、兴趣转移和短期兴趣。3. 复合相对时序编码：通过可学习的头特定偏置系数，更好地捕捉用户行为间的时序和周期性关系。

Result: 实验结果表明SparseCTR不仅提高了效率，还超越了最先进方法。更重要的是，它在三个数量级的FLOPs范围内展现出明显的缩放定律现象，保持性能提升。在线A/B测试中，CTR提升1.72%，CPM提升1.41%。

Conclusion: SparseCTR是针对用户长行为序列的高效有效模型，通过个性化分块和创新的稀疏注意力机制解决了推荐场景中的计算效率和个性化时序建模问题，在实际工业部署中表现出色。

Abstract: In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\% and CPM by 1.41\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.

</details>


### [35] [Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation](https://arxiv.org/abs/2601.18009)
*Ervin Dervishaj,Maria Maistro,Tuukka Ruotsalo,Christina Lioma*

Main category: cs.IR

TL;DR: 提出一种基于大语言模型的训练后去噪方法，通过移除用户历史交互中的噪声数据来提升协同过滤推荐效果


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据存在固有噪声，传统训练中去噪方法需要额外数据、改变模型架构或训练过程，成本高且数据需求大

Method: 使用LLM进行训练后去噪：向LLM提供用户历史交互、候选物品及其在CF推荐器中的排名，让LLM移除用户历史中的噪声物品以提升候选物品排名

Result: 在3个数据集上使用4种开源和闭源LLM进行实验，与最先进的CF推荐器结合，去噪后推荐效果提升最高达13%

Conclusion: 训练后去噪方法无需改变模型架构或训练过程，也不需额外数据，能有效提升推荐系统性能，为推荐去噪提供了新思路

Abstract: Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.

</details>


### [36] [Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph](https://arxiv.org/abs/2601.18096)
*Yuting Zhang,Ziliang Pei,Chao Wang,Ying Sun,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 提出基于交互集成知识图谱的偏好提示发现模型，通过选择性提取关键属性作为提示，增强LLM推荐效果


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中无法有效捕捉复杂偏好模式，传统推荐嵌入与LLM离散语义空间存在差距，需要解决稀疏交互反映偏好提示不足和属性噪声问题

Method: 设计协作偏好提示提取方案，利用相似用户显式交互的语义知识作为未见项目的提示；开发实例级双重注意力机制量化候选属性偏好可信度；采用扁平化提示组织方法缩短输入长度

Result: 在成对和列表推荐任务上的广泛实验验证了框架有效性，相比基线平均相对改进超过3.02%

Conclusion: 通过偏好提示发现模型有效桥接传统推荐嵌入与LLM语义空间，提升LLM推荐性能

Abstract: LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.

</details>


### [37] [Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking](https://arxiv.org/abs/2601.18146)
*Huizhong Guo,Tianjun Wei,Dongxia Wang,Yingpeng Du,Ziyan Wang,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 提出推理路由框架，通过轻量级路由器头在生成前决定是否对每个实例进行推理，平衡排名效果与计算成本


<details>
  <summary>Details</summary>
Motivation: 尽管推理提示能提升大语言模型在排名任务中的效果，但其收益不稳定且计算成本高昂，因此何时推理与如何推理同样重要

Method: 提出推理路由框架，使用轻量级、即插即用的路由器头，基于生成前的排名感知特征和模型感知难度信号，决定对每个实例采用直接推理还是推理模式

Result: 在三个公开排名数据集和不同规模的开源LLM上实验，一致提升了排名效果并减少了token消耗（如MovieLens上NDCG@10提升6.3%，token减少49.5%）

Conclusion: 推理路由是解决准确性与效率权衡的实用解决方案，能根据系统约束动态分配计算资源到最可能受益于推理的实例

Abstract: Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\% NDCG@10 with -49.5\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.

</details>


### [38] [DMAP: Human-Aligned Structural Document Map for Multimodal Document Understanding](https://arxiv.org/abs/2601.18203)
*ShunLiang Fu,Yanxin Zhang,Yixin Xiang,Xiaoyu Du,Jinhui Tang*

Main category: cs.IR

TL;DR: DMAP：一种显式编码多模态文档层次结构和关系的文档级结构化表示方法，通过结构化语义理解代理构建文档地图，结合反思推理代理进行结构感知推理，显著提升多模态文档问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档问答系统主要依赖扁平化语义检索，将文档表示为不连贯的文本块，忽略了文档内在的层次结构和关系结构。这种扁平化破坏了逻辑和空间依赖关系（如章节组织、图文对应、交叉引用等），而这些正是人类理解文档时自然利用的结构。

Method: 1. 提出文档级结构化文档地图（DMAP），显式编码多模态文档的层次组织和元素间关系；2. 设计结构化语义理解代理，将文本内容与图表等组织成人类对齐的层次模式，捕获语义和布局依赖；3. 基于此表示，反思推理代理执行结构感知和证据驱动的推理，动态评估检索上下文的充分性，并通过与DMAP的定向交互迭代优化答案。

Result: 在MMDocQA基准测试上的广泛实验表明，DMAP能够生成与人类解释模式对齐的文档特定结构化表示，相比传统的基于RAG的方法，显著提高了检索精度、推理一致性和多模态理解能力。

Conclusion: DMAP通过显式建模文档的层次和关系结构，解决了现有多模态文档问答系统扁平化检索的局限性，实现了更符合人类理解模式的结构感知推理，为多模态文档理解提供了有效的结构化表示和推理框架。

Abstract: Existing multimodal document question-answering (QA) systems predominantly rely on flat semantic retrieval, representing documents as a set of disconnected text chunks and largely neglecting their intrinsic hierarchical and relational structures. Such flattening disrupts logical and spatial dependencies - such as section organization, figure-text correspondence, and cross-reference relations, that humans naturally exploit for comprehension. To address this limitation, we introduce a document-level structural Document MAP (DMAP), which explicitly encodes both hierarchical organization and inter-element relationships within multimodal documents. Specifically, we design a Structured-Semantic Understanding Agent to construct DMAP by organizing textual content together with figures, tables, charts, etc. into a human-aligned hierarchical schema that captures both semantic and layout dependencies. Building upon this representation, a Reflective Reasoning Agent performs structure-aware and evidence-driven reasoning, dynamically assessing the sufficiency of retrieved context and iteratively refining answers through targeted interactions with DMAP. Extensive experiments on MMDocQA benchmarks demonstrate that DMAP yields document-specific structural representations aligned with human interpretive patterns, substantially enhancing retrieval precision, reasoning consistency, and multimodal comprehension over conventional RAG-based approaches. Code is available at https://github.com/Forlorin/DMAP

</details>


### [39] [Generative Chain of Behavior for User Trajectory Prediction](https://arxiv.org/abs/2601.18213)
*Chengkai Huang,Xiaodi Chen,Hongtao Huang,Quan Z. Sheng,Lina Yao*

Main category: cs.IR

TL;DR: GCB是一个生成式框架，通过将用户交互建模为多步未来行为的自回归链来预测长期用户行为轨迹，超越了传统的下一项预测方法。


<details>
  <summary>Details</summary>
Motivation: 大多数序列推荐系统专注于下一项预测，忽略了多个未来动作之间的依赖关系。为了理解用户偏好演变并实现主动推荐，需要建模长期用户行为轨迹。

Method: 1. 使用RQ-VAE和k-means细化将物品编码为语义ID，形成保持语义邻近性的离散潜在空间。2. 在该空间上，基于Transformer的自回归生成器以用户历史为条件预测多步未来行为，捕捉长期意图转移并生成连贯轨迹。

Result: 在基准数据集上的实验表明，GCB在多步准确性和轨迹一致性方面持续优于最先进的序列推荐系统。

Conclusion: GCB不仅取得了性能提升，还为捕捉用户偏好演变提供了一个统一的生成式框架，能够建模长期用户行为轨迹和意图转移。

Abstract: Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.

</details>


### [40] [GenCI: Generative Modeling of User Interest Shift via Cohort-based Intent Learning for CTR Prediction](https://arxiv.org/abs/2601.18251)
*Kesha Ou,Zhen Tian,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.IR

TL;DR: GenCI：一种基于生成式用户意图框架的CTR预测方法，通过语义兴趣群组建模动态用户偏好，解决历史特征过拟合和点级排序信息缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法存在两个关键问题：1）判别式范式过度拟合历史主导特征，难以适应快速兴趣变化；2）点级排序范式丢弃了召回集整体的丰富上下文信号，导致长期偏好压制用户即时意图。

Method: 提出GenCI框架：1）使用生成模型（基于下一项预测目标）主动生成候选兴趣群组，作为用户即时意图的显式表示；2）构建分层候选感知网络，通过交叉注意力将上下文信号注入排序阶段，对齐用户历史和目标项目；3）端到端训练整个模型。

Result: 在三个广泛使用的数据集上进行大量实验，证明了该方法的有效性。

Conclusion: GenCI通过生成式用户意图建模和语义兴趣群组，创建了更对齐、更有效的CTR预测流程，解决了现有方法的局限性。

Abstract: Click-through rate (CTR) prediction plays a pivotal role in online advertising and recommender systems. Despite notable progress in modeling user preferences from historical behaviors, two key challenges persist. First, exsiting discriminative paradigms focus on matching candidates to user history, often overfitting to historically dominant features and failing to adapt to rapid interest shifts. Second, a critical information chasm emerges from the point-wise ranking paradigm. By scoring each candidate in isolation, CTR models discard the rich contextual signal implied by the recalled set as a whole, leading to a misalignment where long-term preferences often override the user's immediate, evolving intent. To address these issues, we propose GenCI, a generative user intent framework that leverages semantic interest cohorts to model dynamic user preferences for CTR prediction. The framework first employs a generative model, trained with a next-item prediction (NTP) objective, to proactively produce candidate interest cohorts. These cohorts serve as explicit, candidate-agnostic representations of a user's immediate intent. A hierarchical candidate-aware network then injects this rich contextual signal into the ranking stage, refining them with cross-attention to align with both user history and the target item. The entire model is trained end-to-end, creating a more aligned and effective CTR prediction pipeline. Extensive experiments on three widely used datasets demonstrate the effectiveness of our approach.

</details>


### [41] [Orchestrating Specialized Agents for Trustworthy Enterprise RAG](https://arxiv.org/abs/2601.18267)
*Xincheng You,Qi Sun,Neha Bora,Huayi Li,Shubham Goel,Kang Li,Sean Culatana*

Main category: cs.IR

TL;DR: ADORE是一个用于企业知识工作的代理框架，通过结构化记忆库和迭代检索机制，解决传统RAG在深度合成、可追溯性和完整性验证方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在企业高价值决策场景中表现不佳，存在浅层总结、不一致的根基性和弱完整性验证机制等问题，需要更强大的深度合成、严格可追溯性和对未明确提示的恢复能力。

Method: ADORE采用代理框架，核心是结构化记忆库（Claim-Evidence Graph），包含三个关键技术：记忆锁定的合成、证据覆盖引导的执行、以及分段打包的长上下文根基。

Result: 在DeepResearch Bench上排名第一（52.65分），在DeepConsult上对商业系统的头对头偏好胜率最高（77.2%）。

Conclusion: ADORE通过结构化记忆库和迭代代理协调，显著提升了企业知识工作中深度研究任务的质量、可追溯性和完整性验证能力。

Abstract: Retrieval-Augmented Generation (RAG) shows promise for enterprise knowledge work, yet it often underperforms in high-stakes decision settings that require deep synthesis, strict traceability, and recovery from underspecified prompts. One-pass retrieval-and-write pipelines frequently yield shallow summaries, inconsistent grounding, and weak mechanisms for completeness verification. We introduce ADORE (Adaptive Deep Orchestration for Research in Enterprise), an agentic framework that replaces linear retrieval with iterative, user-steered investigation coordinated by a central orchestrator and a set of specialized agents. ADORE's key insight is that a structured Memory Bank (a curated evidence store with explicit claim-evidence linkage and section-level admissible evidence) enables traceable report generation and systematic checks for evidence completeness. Our contributions are threefold: (1) Memory-locked synthesis - report generation is constrained to a structured Memory Bank (Claim-Evidence Graph) with section-level admissible evidence, enabling traceable claims and grounded citations; (2) Evidence-coverage-guided execution - a retrieval-reflection loop audits section-level evidence coverage to trigger targeted follow-up retrieval and terminates via an evidence-driven stopping criterion; (3) Section-packed long-context grounding - section-level packing, pruning, and citation-preserving compression make long-form synthesis feasible under context limits. Across our evaluation suite, ADORE ranks first on DeepResearch Bench (52.65) and achieves the highest head-to-head preference win rate on DeepConsult (77.2%) against commercial systems.

</details>


### [42] [TopKGAT: A Top-K Objective-Driven Architecture for Recommendation](https://arxiv.org/abs/2601.18432)
*Sirui Chen,Jiawei Chen,Canghong Jin,Sheng Zhou,Jingbang Chen,Wujie Sun,Can Wang*

Main category: cs.IR

TL;DR: TopKGAT：一种直接从top-K指标可微近似推导出的推荐架构，通过前向计算与Precision@K梯度上升动态对齐，提升top-K推荐准确性


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统架构（矩阵分解、深度神经网络、图神经网络）的设计未与top-K目标明确对齐，限制了推荐效果。需要一种直接针对top-K指标优化的推荐架构

Method: 提出TopKGAT架构，从top-K指标的可微近似直接推导。单层TopKGAT的前向计算与Precision@K指标的梯度上升动态内在对齐，结构类似于图注意力网络，可实现高效计算

Result: 在四个基准数据集上的实验表明，TopKGAT持续优于最先进的基线方法，显著提升了top-K推荐准确性

Conclusion: TopKGAT通过将架构设计与top-K目标直接对齐，提供了一种更有效的推荐系统解决方案，代码已开源供研究使用

Abstract: Recommendation systems (RS) aim to retrieve the top-K items most relevant to users, with metrics such as Precision@K and Recall@K commonly used to assess effectiveness. The architecture of an RS model acts as an inductive bias, shaping the patterns the model is inclined to learn. In recent years, numerous recommendation architectures have emerged, spanning traditional matrix factorization, deep neural networks, and graph neural networks. However, their designs are often not explicitly aligned with the top-K objective, thereby limiting their effectiveness.
  To address this limitation, we propose TopKGAT, a novel recommendation architecture directly derived from a differentiable approximation of top-K metrics. The forward computation of a single TopKGAT layer is intrinsically aligned with the gradient ascent dynamics of the Precision@K metric, enabling the model to naturally improve top-K recommendation accuracy. Structurally, TopKGAT resembles a graph attention network and can be implemented efficiently. Extensive experiments on four benchmark datasets demonstrate that TopKGAT consistently outperforms state-of-the-art baselines. The code is available at https://github.com/StupidThree/TopKGAT.

</details>


### [43] [Token-level Collaborative Alignment for LLM-based Generative Recommendation](https://arxiv.org/abs/2601.18457)
*Fake Lin,Binbin Hu,Zhi Zheng,Xi Zhu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Tong Xu*

Main category: cs.IR

TL;DR: TCA4Rec提出了一种模型无关的即插即用框架，通过协同分词器和软标签对齐，在CF监督和LLM生成之间建立显式优化级接口，解决LLM推荐系统中难以有效整合协同过滤信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统难以有效整合协同过滤信号，因为CF基于物品级偏好建模，而LLM基于令牌级下一令牌预测优化，存在根本性不匹配。先前方法将CF视为上下文提示或表示偏差，需要多阶段训练来减少行为语义空间差异，导致CF无法显式调节LLM生成。

Method: 提出TCA4Rec框架，包含：(1)协同分词器：将原始物品级CF对数概率投影到与LLM令牌空间对齐的令牌级分布；(2)软标签对齐：将这些CF信息分布与one-hot监督结合，优化软NTP目标。该设计保留了LLM训练的生成特性，同时实现了与CF模型核心用户偏好的协同对齐。

Result: TCA4Rec与任意传统CF模型兼容，可泛化到广泛的基于解码器的LLM推荐架构。它提供了平衡行为对齐和语义流畅性的显式机制，产生既准确又可控制的生成式推荐。大量实验表明，TCA4Rec在各种CF模型和基于LLM的推荐系统中持续提升推荐性能。

Conclusion: TCA4Rec通过建立CF监督与LLM生成之间的显式优化级接口，有效解决了LLM推荐系统中整合协同过滤信号的挑战，实现了生成式推荐的准确性和可控性平衡，为LLM-based推荐系统提供了通用的协同对齐框架。

Abstract: Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.

</details>


### [44] [Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks](https://arxiv.org/abs/2601.18570)
*Mingzhe Han,Jiahao Liu,Dongsheng Li,Hansu Gu,Peng Zhang,Ning Gu,Tun Lu*

Main category: cs.IR

TL;DR: RQFedRec提出基于特征索引的联邦推荐新范式，通过残差量化将物品表示为离散代码ID，传输代码本而非完整物品嵌入，显著降低通信开销并提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐方法采用ID索引通信范式，传输完整物品嵌入存在三大问题：1) 通信资源消耗不可控，2) 上传的物品信息无法泛化到相关未交互物品，3) 对客户端噪声反馈敏感。需要从根本上改变现有通信范式

Method: 提出特征索引通信范式，使用残差量化(RQ)-Kmeans为每个物品分配离散代码ID列表；客户端基于服务器提供的代码ID生成和训练代码嵌入作为代码本，服务器聚合代码本而非物品嵌入；采用协作-语义双通道聚合策略，配合课程学习策略早期强调语义代码，逐步增加协作代码贡献

Result: 在真实世界数据集上的大量实验表明，RQFedRec在显著降低通信开销的同时，持续优于最先进的联邦推荐基线方法

Conclusion: RQFedRec通过特征索引通信范式成功解决了现有联邦推荐方法的局限性，实现了通信可控、泛化能力强、对噪声鲁棒的联邦推荐系统

Abstract: Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.

</details>


### [45] [FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG](https://arxiv.org/abs/2601.18579)
*Seonho An,Chaejeong Hyun,Min-Soo Kim*

Main category: cs.IR

TL;DR: FastInsight：一种高效的图RAG方法，通过融合图模型搜索和向量图搜索，在保持高效的同时显著提升检索准确性和生成质量


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法依赖耗时的大型语言模型推理过程，无法实现高效检索。现有方法存在拓扑盲区（模型搜索）和语义盲区（图搜索）两个关键限制

Method: 提出FastInsight框架，包含两个新型融合算子：1) Graph-based Reranker (GRanker) - 图模型搜索；2) Semantic-Topological eXpansion (STeX) - 向量图搜索。通过图检索分类学将现有方法分为向量搜索、图搜索和模型搜索三类

Result: 在广泛的检索和生成数据集上，FastInsight相比最先进的基线方法显著提升了检索准确性和生成质量，在效果与效率的权衡中实现了显著的帕累托改进

Conclusion: FastInsight通过融合图模型搜索和向量图搜索，克服了现有方法的拓扑盲区和语义盲区限制，实现了高效且深入的图RAG检索

Abstract: Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.

</details>


### [46] [S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation](https://arxiv.org/abs/2601.18664)
*Zihao Guo,Jian Wang,Ruxin Zhou,Youhua Liu,Jiawei Guo,Jun Zhao,Xiaoxiao Xu,Yongqi Liu,Kaiqiao Zhan*

Main category: cs.IR

TL;DR: S²GR：一种新颖的推理增强生成推荐框架，通过代码本优化建立语义基础，并引入逐步推理机制，在语义ID生成前插入思维标记，实现更平衡的计算分配和可解释的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐方法主要关注从交互序列直接生成语义ID，未能激活类似大语言模型的深层推理能力，限制了性能潜力。当前推理增强方法存在两个关键限制：1）推理和生成步骤的严格顺序分离导致跨层次语义ID代码的计算焦点不平衡；2）生成的推理向量缺乏可解释语义，而推理路径缺乏可验证的监督。

Method: 提出S²GR框架：1）通过代码本优化建立稳健语义基础，整合物品共现关系捕捉行为模式，并采用负载平衡和均匀性目标最大化代码本利用率，强化从粗到细的语义层次；2）核心创新是逐步推理机制，在每个语义ID生成步骤前插入思维标记，每个标记明确表示粗粒度语义，通过对比学习监督，确保物理基础的推理路径和所有语义ID代码的平衡计算焦点。

Result: 大量实验证明了S²GR的优越性，在线A/B测试在大型工业短视频平台上确认了其有效性。

Conclusion: S²GR通过代码本优化和逐步推理机制，解决了现有推理增强生成推荐方法的局限性，实现了更平衡的计算分配、可解释的推理路径和更好的推荐性能。

Abstract: Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [47] [Parallel Algorithm For Finding The Minimum s/t Cut in a Structured 3-Dimensional Proper Order Graph](https://arxiv.org/abs/2601.17026)
*Shridharan Chandramouli*

Main category: cs.DS

TL;DR: 提出两种并行算法计算3D图像分割中最小s-t割：基于Boykov-Kolmogorov的分层合并算法和创新的并行推流重标算法


<details>
  <summary>Details</summary>
Motivation: 解决地震成像中地质层位分割的表面提取问题，这些问题的图结构具有特殊的proper order特性（多列结构），需要高效的并行算法

Method: 1. 基于Boykov-Kolmogorov算法的分层合并变体；2. 创新的并行推流重标算法，采用列分割和处理器亲和性，引入层级同步全局重标机制

Result: 开发了两种并行算法，其中推流重标算法通过消除全局共享队列和实现并发标签更新，提供了更好的并行性能

Conclusion: 针对proper order图结构的最小s-t割问题，提出的并行推流重标算法通过创新的列分割和层级同步机制，实现了高效并行计算

Abstract: We present a parallel algorithm for computing the minimum s-t cut in structured 3-dimensional proper order graphs arising from image segmentation problems. Proper order graphs are multi-column structures where vertices are arranged in parallel columns, with each vertex connected to consecutive vertices in adjacent columns. This graph structure naturally arises in surface extraction problems for geological horizon segmentation in seismic imaging volumes. We develop two parallel approaches: a hierarchical merging variant of the Boykov-Kolmogorov algorithm, and a novel parallel push-relabel algorithm with level synchronized global relabeling. Our primary contribution is the push-relabel variant, which partitions the graph into segments along columns with processor affinity, eliminating the need for a global shared queue. We introduce level synchronized global relabeling that enables concurrent label updates while maintaining correctness through barriers at each frontier level.

</details>


### [48] [Minimizing Completion Times of Stochastic Jobs on Parallel Machines is Hard](https://arxiv.org/abs/2601.17425)
*Benjamin Moseley,Kirk Pruhs,Marc Uetz,Rudy Zhou*

Main category: cs.DS

TL;DR: 该论文证明了随机作业调度问题的计算复杂性：即使对于离散两点分布和单位权重的情况，确定是否存在期望成本不超过给定阈值的调度策略是#P-难的，且评估标准(W)SEPT贪婪策略的期望目标值也是#P-难的。


<details>
  <summary>Details</summary>
Motivation: 虽然随机作业在并行相同机器上调度以最小化期望总加权完成时间是一个经典问题，但过去二十年的近似算法研究仅在非常限制性的输入分布假设下才能获得常数因子性能保证。这种算法困难性令人惊讶，因为缺乏相应的复杂性结果：迄今为止，该问题可能在多项式时间内得到最优解。

Method: 通过复杂性理论分析，证明了该问题的内在难解性。针对离散两点处理时间分布和单位权重的特殊情况，证明了确定是否存在期望成本不超过给定阈值的调度策略是#P-难的。同时证明了评估标准(W)SEPT贪婪策略的期望目标值本身也是#P-难的。

Result: 获得了随机作业调度问题的首个硬度结果：对于离散两点分布和单位权重的情况，决策问题是#P-难的，且评估(W)SEPT策略的期望目标值也是#P-难的。这些结果不依赖于底层确定性对应问题的难解性。

Conclusion: 该论文填补了随机作业调度问题复杂性研究的空白，证明了即使对于简单的离散两点分布和单位权重情况，该问题也是内在难解的，为后续算法设计提供了重要的复杂性理论基础。

Abstract: This paper considers the scheduling of stochastic jobs on parallel identical machines to minimize the expected total weighted completion time. While this is a classical problem with a significant body of research on approximation algorithms over the past two decades, constant-factor performance guarantees are currently known only under very restrictive assumptions on the input distributions, even when all job weights are identical. This algorithmic difficulty is striking given the lack of corresponding complexity results: to date, it is conceivable that the problem could be solved optimally in polynomial time.
  We address this gap with hardness results that demonstrate the problem's inherent intractability. For the special case of discrete two-point processing time distributions and unit weights, we prove that deciding whether there exists a scheduling policy with expected cost at most a given threshold is #P-hard. Furthermore, we show that evaluating the expected objective value of the standard (W)SEPT greedy policy is itself #P-hard. These represent the first hardness results for scheduling independent stochastic jobs and min-sum objective that do not merely rely on the intractability of the underlying deterministic counterparts.

</details>


### [49] [Split Algorithm in Linear Time for the Vehicle Routing Problem with Simultaneous Pickup and Delivery and Time Windows](https://arxiv.org/abs/2601.17572)
*Ethan Gibbons,Mario Ventresca,Beatrice M. Ombuki-Berman*

Main category: cs.DS

TL;DR: 提出线性Split算法的扩展，同时处理带取送货和时间窗的车辆路径问题，将时间复杂度从Θ(n²)降低到Θ(n)


<details>
  <summary>Details</summary>
Motivation: 现有的线性Split算法主要针对基本容量约束的CVRP，难以扩展到更复杂的VRP变体，限制了算法的通用性。需要开发能同时处理取送货和时间窗约束的高效Split算法。

Method: 扩展线性Split算法，使其能同时处理VRPSPD（同时取送货）和VRPTW（时间窗）两种变体。算法保证最优性（假设满足三角不等式），并扩展处理容量惩罚和时间扭曲惩罚函数。

Result: 开发出时间复杂度为Θ(n)的线性Split算法，相比之前的Θ(n²)算法有显著速度提升。通过计算实验验证了速度优势。

Conclusion: 成功扩展了线性Split算法到更复杂的VRP变体，提高了算法的通用性和实用性，为现代VRP求解器提供了更高效的分割工具。

Abstract: For many kinds of vehicle routing problems (VRPs), a popular heuristic approach involves constructing a Traveling Salesman Problem (TSP) solution, referred to as a long tour, then partitioning segments of the solution into routes for different vehicles with respect to problem constraints. Previously, a Split algorithm with a worst-case runtime of $Θ(n)$ was proposed for the capacitated VRP (CVRP) that finds the most cost-efficient partition of customers, given a long tour. This was an improvement over the previously fastest-known Split algorithm with a worst-case runtime of $Θ(n^2)$ that was based on Bellman's shortest path algorithm. While this linear Split has been an integral part of modern state-of-the-art CVRP approaches, little progress has been made in extending this algorithm to handle additional VRP variants, limiting the general applicability of the algorithm. In this work, we propose an extension of the linear Split that handles two cardinal VRP variants simultaneously: (i) simultaneous pickups and deliveries (VRPSPD) and (ii) time windows (VRPTW). The resulting $Θ(n)$ algorithm is guaranteed to be optimal, assuming travel times between nodes satisfy the triangle inequality. Additionally, we extend the linear Split to handle a capacity penalty for the VRPSPD. For the VRPTW, we extend the linear Split to handle the CVRP capacity penalty in conjunction with the popular time warp penalty function. Computational experiments are performed to empirically validate the speed gains of these linear Splits against their $Θ$($n^2$) counterparts.

</details>


### [50] [Sampling Sphere Packings with Continuum Glauber Dynamics](https://arxiv.org/abs/2601.18748)
*Aiya Kuchukova,Santosh Vempala,Daniel J. Zhang*

Main category: cs.DS

TL;DR: 本文为连续硬球模型的Glauber动力学建立了谱隙，扩展了其快速混合的参数范围，并改进了有界域内固定数量球体堆积的采样阈值。


<details>
  <summary>Details</summary>
Motivation: 硬球模型的连续Glauber动力学在统计物理和计算中很重要，但现有理论对快速混合的参数范围有限制。需要扩展这些范围以改进采样算法。

Method: 引入了谱独立性和负场定位的连续扩展，这些技术适用于具有有限范围排斥对势的一般Gibbs点过程。

Result: 在强空间混合假设下，为连续硬球模型的Glauber动力学建立了谱隙，从而扩展了快速混合的参数范围。

Conclusion: 该研究扩展了连续Glauber动力学快速混合的理论保证范围，并改进了有界域内固定数量球体堆积的采样阈值，为更广泛的Gibbs点过程提供了分析工具。

Abstract: We establish a spectral gap for Continuum Glauber dynamics on the hard sphere model assuming strong spatial mixing, thereby extending the range of parameters in which Continuum Glauber is provably rapidly mixing. To do this, we introduce continuous extensions of spectral independence and negative fields localization. Our techniques apply to general Gibbs point processes with finite-range repulsive pair potentials. As a corollary, we improve the threshold up to which packings of a fixed number of spheres can be sampled from a bounded domain.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [51] [High-Rate Quantized Matrix Multiplication: Theory and Practice](https://arxiv.org/abs/2601.17187)
*Or Ordentlich,Yury Polyanskiy*

Main category: cs.IT

TL;DR: 本文研究了量化矩阵乘法问题，分析了通用矩阵乘法和仅权重量化两种场景，提出了基于水填充的WaterSIC量化方案，在信息论极限内实现了接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署中矩阵乘法量化对效率至关重要，但现有量化方案与信息论极限存在差距，需要开发更接近理论最优的量化方法。

Method: 1) 分析通用矩阵乘法和仅权重量化的信息论基础；2) 比较absmax INT和FP量化方案；3) 提出基于加权均方误差源编码的WaterSIC方案，利用水填充原理优化比特分配；4) 与GPTQ等现有方法进行对比分析。

Result: WaterSIC方案仅使用标量INT量化器，其高性能与基选择无关，仅取决于协方差矩阵行列式，在信息论失真极限的2πe/12倍（0.25比特/条目）内。GPTQ在随机旋转下与WaterSIC性能接近（相差约0.1比特）。

Conclusion: 基于水填充的WaterSIC量化方案在理论上接近信息论最优，为LLM量化提供了新的优化方向，同时表明GPTQ在随机旋转下也具有接近最优的性能。

Abstract: This work investigates the problem of quantized matrix multiplication (MatMul), which has become crucial for the efficient deployment of large language models (LLMs). We consider two settings: 1) Generic MatMul, where both matrices must be quantized (weight+activation quantization); and 2) weight-only quantization, where the second matrix is only known through covariance matrix $Σ_X$ of its columns. For each setting, we first review the fundamental information-theoretic tradeoff between quantization rate and distortion (high-rate theory), and then analyze the performance of several popular quantization schemes, comparing them to these fundamental limits. Specifically, we discuss rate loss (compared to information theoretic optima) of absmax INT and floating-point (FP) quantization, for which we also derive remarkably accurate heuristic approximations. Weight-only quantization is related to the problem of weighted mean squared error (WMSE) source coding, whose classical (reverse) waterfilling solution dictates how one should distribute rate between coordinates of the vector. We show how waterfilling can be used to improve practical LLM quantization algorithms (GPTQ), which at present allocate rate equally. This new scheme (termed ``WaterSIC'') only uses scalar INT quantizers, but its high-rate performance is basis free (it depends only on the determinant of $Σ_X$ and, thus, unlike existing schemes, is immune to applying random rotations) and is within a multiplicative factor of $\frac{2πe}{12}$ (or 0.25 bit/entry) of the information-theoretic distortion limit (!). GPTQ's performance is affected by the choice of basis, but for a random rotation and actual $Σ_X$ from Llama-3-8B we find GPTQ to be within 0.1 bit (depending on the layer type) of WaterSIC, suggesting that GPTQ with random rotation is also near optimal (for high-rate quantization).

</details>


### [52] [Joint Uplink-Downlink Fronthaul Bit Allocation in Fronthaul-Limited Massive MU-MIMO Systems](https://arxiv.org/abs/2601.17423)
*Yasaman Khorsandmanesh,Emil Bjornson,Joakim Jalden*

Main category: cs.IT

TL;DR: 该论文优化了大规模多用户MIMO系统中有限容量前传链路的比特分配，通过平衡CSI量化和预编码矩阵量化来最大化系统和速率。


<details>
  <summary>Details</summary>
Motivation: 在集中式基带单元控制的高级天线系统中，前传链路容量有限，需要在信道状态信息量化和下行预编码矩阵量化之间合理分配比特资源，这是提升系统性能的关键挑战。

Method: 基于硬化界推导了系统和速率表达式，针对最大比传输获得了闭式解，揭示了两种量化失真的相对影响，然后构建比特分配优化问题并提出了精确求解算法。

Result: 数值结果表明，分配给CSI和预编码的比特相对重要性随信噪比变化而变化，验证了所提优化算法的有效性。

Conclusion: 该研究为有限前传容量下的大规模MU-MIMO系统提供了有效的比特分配策略，通过动态平衡CSI和预编码量化比特，显著提升了系统和速率性能。

Abstract: This paper optimizes the fronthaul bit allocation in massive multi-user multiple-input multiple-output (MU-MIMO) systems operating with limited-capacity fronthaul links. We consider an advanced antenna system (AAS) controlled by a centralized baseband unit (BBU). In the AAS, multiple antenna elements together with their radio units are integrated into a single unit. In this setup, a key challenge is allocating fronthaul bits between uplink channel state information (CSI) quantization and downlink precoding matrix quantization. We formulate the problem of maximizing the sum spectral efficiency (SE) for a given fronthaul capacity. We develop an SE expression for this scenario based on the hardening bound. We compute the expression in closed form for maximum ratio transmission, which reveals the relative impact of the two types of quantization distortion. We then formulate a bit split optimization problem and propose an algorithm that exactly solves it.
  Numerical results demonstrate how the relative importance of assigning bits to CSI and precoding varies depending on the signal-to-noise ratio.

</details>


### [53] [Double-Cover-Based Analysis of the Bethe Permanent of Block-Structured Positive Matrices](https://arxiv.org/abs/2601.17508)
*Binghong Wu,Pascal O. Vontobel*

Main category: cs.IT

TL;DR: 本文研究了非负矩阵的永久值与Bethe永久值之比，针对块结构矩阵进行了数值分析，发现该比值强烈集中在依赖于少数关键参数的值附近，并使用图覆盖方法解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 虽然矩阵永久值与其Bethe永久值之比在最坏情况下上下界相差指数级，但实际应用中观察到该比值对许多矩阵集合都强烈集中在某个依赖于矩阵大小的值附近。本文旨在研究块结构矩阵集合中这一现象，并解释其背后的原因。

Method: 1. 对块结构矩阵（块内元素取值相同）进行数值研究，分析矩阵永久值与Bethe永久值之比
2. 使用图覆盖（graph-cover）方法解释观察到的行为
3. 量化观测到的比值

Result: 对于块结构矩阵集合，矩阵永久值与Bethe永久值之比同样强烈集中在依赖于少数关键参数的值附近。图覆盖方法能够解释这一行为并量化观测值。

Conclusion: 块结构矩阵的永久值与Bethe永久值之比表现出强烈的集中现象，这一行为可以通过图覆盖方法得到解释和量化，为理解Bethe永久值作为矩阵永久值近似工具的实际性能提供了理论支持。

Abstract: We consider the permanent of a square matrix with non-negative entries. A tractable approximation is given by the so-called Bethe permanent that can be efficiently computed by running the sum-product algorithm on a suitable factor graph. While the ratio of the permanent of a matrix to its Bethe permanent is, in the worst case, upper and lower bounded by expressions that are exponentially far apart in the matrix size, in practice it is observed for many ensembles of matrices of interest that this ratio is strongly concentrated around some value that depends only on the matrix size. In this paper, for an ensemble of block-structured matrices where entries in a block take the same value, we numerically study the ratio of the permanent of a matrix to its Bethe permanent. It is observed that also for this ensemble the ratio is strongly concentrated around some value depending only on a few key parameters of the ensemble. We use graph-cover-based approaches to explain the reasons for this behavior and to quantify the observed value.

</details>


### [54] [Study of Robust Power Allocation for User-Centric Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2601.17632)
*Saeed Mashdour,Saeed Mohammadzadeh,André R. Flores,Shirin Salehi,Rodrigo C. de Lamare,Anke Schmeink*

Main category: cs.IT

TL;DR: 提出一种用于无小区大规模MIMO网络的鲁棒功率分配方法，采用最小二乘框架和Tikhonov正则化来应对信道估计误差，结合迫零预编码实现计算高效且对CSI不完美具有鲁棒性的设计。


<details>
  <summary>Details</summary>
Motivation: 在无小区大规模MIMO网络中，由于不完美的信道状态信息(CSI)导致信道不确定性，需要鲁棒的资源分配来确保可靠的系统性能。

Method: 将功率优化问题表述为最小二乘框架，采用Tikhonov正则化来减轻信道估计误差的不利影响，并与迫零预编码相结合。

Result: 数值结果表明，所提方法优于现有的非鲁棒技术，同时具有低计算开销，适合CSI不确定性下的大规模部署。

Conclusion: 该方法为无小区大规模MIMO网络提供了一种计算高效且对CSI不完美具有鲁棒性的功率分配解决方案，适用于实际部署场景。

Abstract: In cell-free massive multiple-input multiple-output (MIMO) networks, robust resource allocation is critical to ensure reliable system performance in the presence of channel uncertainties resulting from imperfect channel state information (CSI). In this work, we propose a robust power allocation method that formulates the power optimization problem into a least-squares framework, enhanced by Tikhonov regularization to mitigate the adverse effects of channel estimation errors. We integrate our approach with zero-forcing precoding, enabling a design that is both computationally efficient and resilient to CSI imperfections. Numerical results indicate that the proposed method outperforms existing non-robust techniques while benefiting from low computational overhead, making it well-suited for large-scale deployments under CSI uncertainty.

</details>


### [55] [A Model-Driven Lossless Compression Algorithm Resistant to Mismatch](https://arxiv.org/abs/2601.17684)
*Cordelia Hu,Jennifer Tang*

Main category: cs.IT

TL;DR: 提出一种基于下一标记预测的鲁棒压缩算法，能容忍结构化预测不匹配，在认证的不匹配范围内可靠运行，压缩率优于常用压缩方法。


<details>
  <summary>Details</summary>
Motivation: 现代预测模型（如LLM）结合熵编码可实现优于标准压缩算法的压缩率，但需要编码器和解码器产生完全相同的输出分布。复杂预测模型（特别是神经网络）存在非确定性，即使微小不匹配也会导致解码失败。

Method: 提出基于下一标记预测的新压缩算法，能容忍任意大但结构化的预测不匹配。通过形式化不匹配认证证明方案正确性，并分析其理论性能。

Result: 在真实数据集上验证，算法在认证的不匹配范围内可靠运行，同时压缩率超过常用压缩方法。

Conclusion: 该工作解决了预测模型在压缩应用中的非确定性问题，提供了一种鲁棒的压缩方案，能在存在结构化预测不匹配的情况下实现高效压缩。

Abstract: Due to the fundamental connection between next-symbol prediction and compression, modern predictive models, such as large language models (LLMs), can be combined with entropy coding to achieve compression rates that surpass those of standard compression algorithms. However, this approach relies on the assumption that the predictive model produces identical output distributions at both the encoder and decoder, since even small mismatches can cause the decoding to fail. This assumption often fails with complex predictive models, particularly those based on neural networks, a phenomenon referred to as non-determinism.
  In this work, we propose a new compression algorithm based on next-token prediction that is robust to arbitrarily large, but structured, prediction mismatches. We prove the correctness of the proposed scheme under a formal mismatch certification, characterize its theoretical performance, and validate it experimentally on real datasets. Our results demonstrate reliable operation within the certified mismatch regime while achieving compression ratios that exceed those of commonly used compression methods.

</details>


### [56] [A Multi-Modal Fusion Platform for Joint Environment Sensing and Channel Sounding in Highly Dynamic Scenarios](https://arxiv.org/abs/2601.17809)
*Xuejian Zhang,Ruisi He,Mi Yang,Zhengyu Zhang,Ziyi Qi*

Main category: cs.IT

TL;DR: 提出一个多模态感知与信道探测融合平台，用于6G动态场景下的环境-信道联合建模


<details>
  <summary>Details</summary>
Motivation: 6G系统面临复杂传播环境，现有信道探测设备缺乏跨频段能力、动态场景适应性和环境感知能力，限制了环境感知信道模型的发展

Method: 设计多模态感知与信道探测融合平台，实现图像、点云、地理位置信息和多频段多天线信道数据的时空同步采集，采用模块化架构支持快速部署

Result: 平台支持Sub-6 GHz和毫米波频段，带宽达1 GHz，延迟分辨率1 ns，信道切换速率8 ms，环境感知精度达厘米级和360°，定位精度米级

Conclusion: 该平台为6G动态场景下的环境-信道联合建模提供支持，有助于信道模型的分析和优化

Abstract: 6G system is evolving toward full-spectrum coverage,ultra-wide bandwidth, and high mobility, resulting in increasingly complex propagation environments. The deep integration of communication and sensing is widely recognized as a core 6G vision, underscoring the importance of comprehensive environment awareness. Accurate channel modeling forms the foundation of 6G system design and optimization, and channel sounders provide the essential empirical basis. However, existing channel sounders, although supporting wide bandwidth and large antenna arrays in selected bands, generally lack cross-band capability, struggle in dynamic scenarios, and provide limited environmental awareness. The absence of detailed environmental information restricts the development of environment-aware channel models. To address this gap, we propose a multi-modal sensing and channel sounding fusion platform that enables temporally and spatially synchronized acquisition of images, point clouds, geolocation information, and multi-band multi-antenna channel data. The modular architecture facilitates rapid deployment in diverse dynamic environments. The platform supports Sub-6 GHz and mmWave bands with up to 1 GHz bandwidth and 1 ns delay resolution, enabling multi-antenna measurements with a channel switching rate of 8 ms. Moreover, it achieves centimeter-level and 360° environmental sensing accuracy and meter-level positioning accuracy. Key performance metrics of the platform, including dynamic range, phase stability, delay resolution, and multimodal data synchronization, are validated through vehicle-to-infrastructure measurement campaign. The established platform supports environment-channel joint modeling, enabling analysis and optimization of channel models in dynamic 6G scenarios.

</details>


### [57] [On the Extension of Private Distributed Matrix Multiplication Schemes to the Grid Partition](https://arxiv.org/abs/2601.17834)
*Christoph Hofmeister,Razane Tajeddine,Antonia Wachter-Zeh,Rawad Bitar*

Main category: cs.IT

TL;DR: 本文提出了一种将现有OPP（外积划分）私有分布式矩阵乘法方案扩展到GP（网格划分）的扩展方法，并展示了新GP方案在某些参数下优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有私有分布式矩阵乘法（PDMM/SDMM）方案要么专门针对外积划分（OPP），要么针对内积划分（IPP），或者适用于更一般的网格划分（GP）。需要一种方法将大量OPP方案扩展到GP情况，以改进现有方案。

Method: 设计了扩展操作，可以将一大类OPP码设计扩展到GP情况。这些扩展操作应用于现有码设计，并展示了扩展后的GP方案满足额外的组合约束。此外，提出了一个不遵循这些约束的新GP方案。

Result: 扩展操作成功将OPP方案扩展到GP，在某些参数下改进了现有方案。然而，扩展后的GP方案受到组合约束限制。提出的新GP方案不遵循这些约束，在一系列参数范围内优于现有方案。

Conclusion: 通过扩展操作可以将OPP方案扩展到GP，但扩展方案受到组合约束限制。打破这些约束的新GP方案在某些参数下表现更优，为私有分布式矩阵乘法提供了更好的设计选择。

Abstract: We consider polynomial codes for private distributed matrix multiplication (PDMM/SDMM). Existing codes for PDMM are either specialized for the outer product partitioning (OPP), or inner product partitioning (IPP), or are valid for the more general grid partitioning (GP). We design extension operations that can be applied to a large class of OPP code designs to extend them to the GP case. Applying them to existing codes improves upon the state-of-the-art for certain parameters. Additionally, we show that the GP schemes resulting from extension fulfill additional combinatorial constraints, potentially limiting their performance. We illustrate this point by presenting a new GP scheme that does not adhere to these constraints and outperforms the state-of-the-art for a range of parameters.

</details>


### [58] [Phase-Rotated Symbol Spreading for Scalable Rydberg Atomic-MIMO Detection](https://arxiv.org/abs/2601.17838)
*Jiuyu Liu,Yi Ma,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 该论文提出了一种相位旋转符号扩展（PRSS）方法，用于解决Rydberg原子接收机MIMO系统中的非线性信号检测问题，通过在两个连续时隙传输符号并引入π/2相位偏移，实现了有效的线性信号模型重建。


<details>
  <summary>Details</summary>
Motivation: Rydberg原子接收机在MIMO系统中面临非线性信号模型的挑战，这限制了信号检测的可扩展性和性能，需要一种能保持频谱效率同时实现线性化的解决方案。

Method: 提出相位旋转符号扩展（PRSS）方法：将每个符号在两个连续时隙传输，并引入最优的π/2相位偏移，从而重建有效的线性信号模型，使传统RF-MIMO检测算法得以应用。

Result: 仿真结果表明，与当前单次传输方法相比，PRSS在使用最优穷举搜索和低复杂度次优检测方法时，分别实现了超过2.5dB和10dB的误码率改善。

Conclusion: PRSS方法成功解决了Rydberg原子接收机MIMO系统的非线性信号检测问题，在保持频谱效率的同时显著提升了检测性能，为实际应用提供了可行的解决方案。

Abstract: Multiple-input multiple-output (MIMO) systems using Rydberg atomic (RA) receivers face significant scalability challenges in signal detection due to their nonlinear signal models. This letter proposes phase-rotated symbol spreading (PRSS), which transmits each symbol across two consecutive time slots with an optimal π/2 phase offset. PRSS enables reconstruction of an effective linear signal model while maintaining spectral efficiency and facilitating the use of conventional RF-MIMO detection algorithms. Simulation results demonstrate that PRSS achieves greater than 2.5 dB and 10 dB bit error rate improvements compared to current single-transmission methods when employing optimal exhaustive search and low-complexity sub-optimal detection methods, respectively.

</details>


### [59] [Information-Theoretic Secure Aggregation in Decentralized Networks](https://arxiv.org/abs/2601.17970)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: 该论文研究了去中心化安全聚合（DSA）的基本性能极限，证明了计算输入总和的每个比特时，每个用户必须至少向其他用户传输1比特、持有至少1比特密钥，且所有用户总共需要至少K-1个独立密钥比特。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化联邦学习和随机优化中对数据安全需求的增加，需要解决去中心化环境中安全聚合的基本性能极限问题。用户希望在计算所有输入总和的同时，确保即使有最多T个用户串谋，也无法获取超出总和之外的任何信息。

Method: 研究了一个包含K个互连用户的网络，每个用户持有私有输入（如联邦学习中的本地模型更新）。通过信息论方法分析去中心化安全聚合问题，推导最优速率区域，确定最小通信和密钥速率要求。

Result: 表征了最优速率区域：要安全计算输入总和的每个比特，每个用户必须至少传输1比特给其他用户，持有至少1比特密钥，且所有用户总共需要至少K-1个独立密钥比特。这些是DSA的基本性能极限。

Conclusion: 该研究建立了去中心化安全聚合的基本性能极限，为设计可证明安全且通信高效的分布式学习系统协议提供了理论指导和设计洞见。

Abstract: Motivated by the increasing demand for data security in decentralized federated learning (FL) and stochastic optimization, we formulate and investigate the problem of information-theoretic \emph{decentralized secure aggregation} (DSA). Specifically, we consider a network of $K$ interconnected users, each holding a private input, representing, for example, local model updates in FL, who aim to simultaneously compute the sum of all inputs while satisfying the security requirement that no user, even when colluding with up to $T$ others, learns anything beyond the intended sum. We characterize the optimal rate region, which specifies the minimum achievable communication and secret key rates for DSA. In particular, we show that to securely compute one bit of the desired input sum, each user must (i) transmit at least one bit to all other users, (ii) hold at least one bit of secret key, and (iii) all users must collectively hold no fewer than $K - 1$ independent key bits. Our result establishes the fundamental performance limits of DSA and offers insights into the design of provably secure and communication-efficient protocols for distributed learning systems.

</details>


### [60] [Secure Beamforming and Reflection Design for RIS-ISAC Systems under Collusion of Passive and Active Eavesdroppers](https://arxiv.org/abs/2601.18063)
*Tian Zhang,Zhirong Su,Yueyi Dong*

Main category: cs.IT

TL;DR: 研究RIS辅助ISAC系统中的物理层安全，针对主动和被动窃听者协作的场景，通过联合基站波束成形和RIS反射设计，在保证感知性能的同时最大化系统保密速率。


<details>
  <summary>Details</summary>
Motivation: 随着集成感知与通信(ISAC)系统的发展，物理层安全问题日益重要。特别是在存在主动窃听者(AE)和被动窃听者(PE)相互协作的威胁场景下，需要设计有效的安全机制来保护通信安全，同时保证系统的感知性能。

Method: 采用交替优化方案将非凸优化问题分解为三个子问题，结合二次惩罚法和逐次凸逼近(SCA)方法求解，提出联合波束成形和反射设计(JBRD)算法。

Result: 数值结果表明所提出的JBRD算法在保证感知性能的同时，能够有效提升系统保密速率，相比基准方法展现出优越性能。

Conclusion: 该研究为RIS辅助ISAC系统在协作窃听威胁下的物理层安全问题提供了有效的解决方案，提出的JBRD算法通过联合优化基站波束成形和RIS反射设计，实现了安全通信与感知性能的良好平衡。

Abstract: In the paper, the physical-layer security for reconfigurable intelligent surface (RIS) aided integrated sensing and communication (ISAC) system is studied. There is an active eavesdropper (AE) as well as a passive eavesdropper (PE), and they cooperate each other. By joint base station beamforming and RIS reflection design, we aim to achieve the best secure data communications with guaranteed sensing performance. Mathematically, taking the constraints on sensing performance and transmission power in consideration, the system secrecy rate maximization problem is formulated with respect to transmitting beamforming, RIS reflection, and receiving beamforming. The formulated problem is non-convex and is decomposed to three subproblem by applying the alternating optimization scheme. For the decomposed subproblem, we utilize the quadratic penalty method and successive convex approximation (SCA) for the solution derivation. Thereafter, an iterative numerical algorithm, referred to as the joint beamforming and reflection design (JBRD) algorithm, is proposed. Finally, numerical results demonstrate the effectiveness and superiority of the proposed algorithm.

</details>


### [61] [Tail-Latency-Aware Federated Learning with Pinching Antenna: Latency, Participation, and Placement](https://arxiv.org/abs/2601.18097)
*Yushen Lin,Zhiguo Ding*

Main category: cs.IT

TL;DR: PASS系统通过可调天线改变客户端上行延迟，联合优化天线位置和客户端选择，最小化联邦学习达到目标精度的时间，在统计异构性下平衡延迟和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 同步联邦学习中，慢客户端（straggler）是主要时间瓶颈。但在非独立同分布数据下，只选择快速客户端会因统计异构性显著减慢收敛速度。需要平衡延迟和统计多样性。

Method: 提出PASS系统，使用可调辐射夹持天线（PA）沿介质波导任意位置激活，重塑上行延迟。联合优化PA位置和客户端参与，最小化预期达到精度时间，将最大轮延迟的精确预期（通过顺序统计）与异构感知收敛因子耦合。

Result: 推导出一阶最优条件，揭示KKT递归中的显式尾部延迟溢价；在延迟类结构下，获得类内平方根采样定律，建立两阶段相变；对PA位置，证明分段包络导数特性并提供精确断点和根候选枚举程序。

Conclusion: PASS系统能实现更多合格客户端参与，提高时钟精度，在统计异构性下有效平衡延迟和收敛速度，为无线联邦学习提供实用的延迟整形解决方案。

Abstract: Straggler synchronization is a dominant wall-clock bottleneck in synchronous wireless federated learning (FL). Under non-IID data, however, aggressively sampling only fast clients may significantly slow convergence due to statistical heterogeneity. This paper studies PASS-enabled FL, where a radiating pinching antenna (PA) can be activated at an arbitrary position along a dielectric waveguide to reshape uplink latencies. We consider a joint optimization of PA placement and client participation to minimize the expected time-to-accuracy, coupling the exact expected maximum round latency via order statistics with a heterogeneity-aware convergence factor. We derive first-order optimality conditions that reveal an explicit tail-latency premium in the KKT recursion, quantifying how latency gaps are amplified by maximum-order-statistic synchronization. Under a latency-class structure, we obtain a within-class square-root sampling law and establish a two-class phase transition where slow-class participation collapses under an explicit heterogeneity-threshold condition as the per-round sample size grows. For PA placement, we prove a piecewise envelope-derivative characterization and provide an exact breakpoint-and-root candidate-enumeration procedure. Simulation results verify the theoretical findings and show that PASS enables more eligible participation, yielding higher wall-clock accuracy.

</details>


### [62] [Scalable Quantum Message Passing Graph Neural Networks for Next-Generation Wireless Communications: Architectures, Use Cases, and Future Directions](https://arxiv.org/abs/2601.18198)
*Le Tung Giang,Nguyen Xuan Tung,Trinh Van Chien,Lajos Hanzo,Won-Joo Hwang*

Main category: cs.IT

TL;DR: 提出SQM-GNN模型，结合量子计算与图神经网络，通过子图分解和共享参数化量子电路解决无线资源管理中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在无线资源管理中面临大规模密集网络的计算挑战，量子计算与GNN结合可提升计算效率，但现有纯量子消息传递模型受限于量子比特数量，难以扩展到无线系统。

Method: 提出可扩展量子消息传递图神经网络(SQM-GNN)，将图分解为子图，对每个局部子图应用共享参数化量子电路(PQC)，同时整合节点和边特征以完整表示无线图结构。

Result: 在设备到设备(D2D)功率控制任务中，SQM-GNN优于传统GNN和启发式基线方法，证明了其效率。

Conclusion: SQM-GNN为未来无线网络优化提供了有前景的方向，通过量子优势解决了GNN在大规模无线系统中的可扩展性问题。

Abstract: Graph Neural Networks (GNNs) are eminently suitable for wireless resource management, thanks to their scalability, but they still face computational challenges in large-scale, dense networks in classical computers. The integration of quantum computing with GNNs offers a promising pathway for enhancing computational efficiency because they reduce the model complexity. This is achieved by leveraging the quantum advantages of parameterized quantum circuits (PQCs), while retaining the expressive power of GNNs. However, existing pure quantum message passing models remain constrained by the limited number of qubits, hence limiting the scalability of their application to the wireless systems. As a remedy, we conceive a Scalable Quantum Message Passing Graph Neural Network (SQM-GNN) relying on a quantum message passing architecture. To address the aforementioned scalability issue, we decompose the graph into subgraphs and apply a shared PQC to each local subgraph. Importantly, the model incorporates both node and edge features, facilitating the full representation of the underlying wireless graph structure. We demonstrate the efficiency of SQM GNN on a device-to-device (D2D) power control task, where it outperforms both classical GNNs and heuristic baselines. These results highlight SQM-GNN as a promising direction for future wireless network optimization.

</details>


### [63] [Complex-Valued-Matrix Permanents: SPA-based Approximations and Double-Cover Analysis](https://arxiv.org/abs/2601.18232)
*Junda Zhou,Pascal O. Vontobel*

Main category: cs.IT

TL;DR: 将基于因子图和和积算法的永久值近似方法从非负实值矩阵扩展到复值矩阵，通过双边正规因子图实现


<details>
  <summary>Details</summary>
Motivation: 复值矩阵的永久值近似在玻色子采样和概率推断中有重要应用，需要将现有的实值矩阵近似方法扩展到复值领域

Method: 使用双边正规因子图运行和积算法，从算法角度分析和积算法固定点在实值到复值转换中的变化，从分析角度使用图覆盖分析Bethe近似

Result: 提供了复值问题中Bethe近似结构的新见解，明确了这种近似在非负实值设置之外何时仍然有意义

Conclusion: 结合算法和分析视角，为复值矩阵永久值近似提供了新的理论框架，扩展了因子图方法的应用范围

Abstract: Approximating the permanent of a complex-valued matrix is a fundamental problem with applications in Boson sampling and probabilistic inference. In this paper, we extend factor-graph-based methods for approximating the permanent of non-negative-real-valued matrices that are based on running the sum-product algorithm (SPA) on standard normal factor graphs, to factor-graph-based methods for approximating the permanent of complex-valued matrices that are based on running the SPA on double-edge normal factor graphs.
  On the algorithmic side, we investigate the behavior of the SPA, in particular how the SPA fixed points change when transitioning from real-valued to complex-valued matrix ensembles. On the analytical side, we use graph covers to analyze the Bethe approximation of the permanent, i.e., the approximation of the permanent that is obtained with the help of the SPA.
  This combined algorithmic and analytical perspective provides new insight into the structure of Bethe approximations in complex-valued problems and clarifies when such approximations remain meaningful beyond the non-negative-real-valued settings.

</details>


### [64] [A Heterogeneous Massive MIMO Technique for Uniform Service in Cellular Networks](https://arxiv.org/abs/2601.18298)
*Wei Jiang,Hans D. Schotten*

Main category: cs.IT

TL;DR: 提出一种成本效益高的异构大规模MIMO架构，结合集中式基站天线与分布式边缘接入点，在保持用户公平性的同时大幅降低基础设施成本。


<details>
  <summary>Details</summary>
Motivation: 传统蜂窝网络对小区边缘用户服务质量差，而无小区系统虽能提供均匀服务质量但部署成本过高，需要大量接入点站点和大规模光纤网络连接。

Method: 提出异构大规模MIMO架构，将大规模天线策略性地分配在集中式基站和分布式边缘接入点之间，减少所需接入点站点数量和前传连接。

Result: 数值结果表明，该架构在性能和成本平衡方面优于传统蜂窝网络和无细胞系统，在保持与无细胞系统相当的用户公平性的同时大幅降低基础设施成本。

Conclusion: 该异构大规模MIMO架构提供了一种成本效益高的解决方案，有效平衡了网络性能和部署成本，解决了传统蜂窝网络和无细胞系统各自的局限性。

Abstract: Traditional cellular networks struggle with poor quality of service (QoS) for cell-edge users, while cell-free (CF) systems offer uniform QoS but incur high roll-out costs due to acquiring numerous access point (AP) sites and deploying a large-scale optical fiber network to connect them. This paper proposes a cost-effective heterogeneous massive MIMO architecture that integrates centralized co-located antennas at a cell-center base station with distributed edge APs. By strategically splitting massive antennas between centralized and distributed nodes, the system maintains high user fairness comparable to CF systems but reduces infrastructure costs substantially, by minimizing the required number of AP sites and fronthaul connections. Numerical results demonstrate its superiority in balancing performance and costs compared to cellular and CF systems.

</details>


### [65] [Time-Scale-Adaptable Spectrum Sharing for Hybrid Satellite-Terrestrial Networks](https://arxiv.org/abs/2601.18410)
*Yanmin Wang,Wei Feng,Yunfei Chen,Yongxu Zhu,Shidong Zhou,Cheng-Xiang Wang*

Main category: cs.IT

TL;DR: 提出一种基于时标自适应框架的卫星-地面混合网络频谱共享方案，通过链路特征草图辅助的层次化链路聚类和蒙特卡洛逐次逼近功率优化，在仅有统计CSI下最大化网络平均和速率。


<details>
  <summary>Details</summary>
Motivation: 卫星与地面无线网络融合可满足日益增长的泛在通信覆盖需求，但面临频谱稀缺问题。需要设计有效的频谱共享机制，在保证服务质量的同时提升网络性能。

Method: 提出时标自适应频谱共享框架，支持卫星-地面协作时标灵活调整。采用链路特征草图辅助的层次化链路聚类进行链路调度，结合蒙特卡洛逐次逼近法进行功率控制。仅依赖统计CSI，支持全频复用和部分频复用，以及多卫星选择。

Result: 仿真结果表明，通过链路特征草图能充分利用用户空间分布带来的链路多样性。所提方案即使在严格的链路间干扰约束下，仍能实现显著的性能增益。

Conclusion: 提出的时标自适应频谱共享方案能有效解决卫星-地面混合网络的频谱稀缺问题，通过低复杂度算法在仅有统计CSI下实现高性能的联合链路调度和功率控制。

Abstract: Cooperation between satellite and terrestrial wireless networks promises great potential in meeting fast-growing demands for ubiquitous communications coverage. To tackle spectrum scarcity, spectrum sharing is studied for a hybrid satellite-terrestrial network where satellite links share the same group of time-slotted sub-carriers with terrestrial links opportunistically. In particular, with coarse network-wide time synchronization, a time-scale-adaptable spectrum sharing framework is proposed based on a satellite-terrestrial cooperation time scale that can be flexibly adjusted according to practical requirements. For generality, it is assumed that both full and partial frequency reuse could be adopted among the base stations (BSs) and satellite selection is supported when multiple satellites are available. Relying on only statistical channel state information (CSI), joint link scheduling and power control are explored to maximize the average sum rate of the network while ensuring quality of service (QoS) for users. To solve the complicated mixed integer programming (MIP) problem, a low-complexity spectrum sharing scheme is presented based on link-feature-sketching-aided hierarchical link clustering and Monte-Carlo-and-successive-approximation-aided transmit power optimization. Simulation results demonstrate that by link feature sketching, diversity of the links brought by the spatial distribution of the users could be well utilized. The proposed scheme promises a significant performance gain even under strict inter-link interference constraints.

</details>


### [66] [On the Optimal Message Size in PIR Under Arbitrary Collusion Patterns](https://arxiv.org/abs/2601.18440)
*Guru S. Dornadula,Manikya Pant,Gowtham R. Kurri,Prasad Krishnan*

Main category: cs.IT

TL;DR: 本文研究了任意合谋模式下可分解PIR方案的最优消息大小，推导出一般下界，并对特定合谋模式给出了匹配的可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有的PIR方案在无合谋模式下最优消息大小为N-1，但对于更一般的合谋模式还没有类似结果。小消息大小有利于降低实现复杂度和约束，因此需要研究任意合谋模式下可分解PIR方案的最优消息大小。

Method: 首先完全刻画了任意合谋模式下容量可达可分解PIR方案的性质，基于此推导出一般下界，该下界用由合谋模式确定的服务器子集族的命中数表示。然后将下界专门化到几类重要合谋模式：T-合谋、不相交合谋集集合、循环T-连续合谋、不相交循环连续合谋集集合。对后两种模式提出了匹配的可行方案。

Result: 推导出任意合谋模式下容量可达均匀可分解PIR方案最优消息大小的一般下界。对于循环T-连续合谋和不相交循环连续合谋集集合，提出了达到相应下界的可行方案，从而完全刻画了这两种模式的最优消息大小。

Conclusion: 本文首次对任意合谋模式下的可分解PIR方案最优消息大小进行了系统研究，建立了理论下界并对特定合谋模式给出了最优构造，填补了该领域的研究空白。

Abstract: A private information retrieval protocol (PIR) scheme under an arbitrary collusion pattern $\mathcal{P}$ enables a client to retrieve one message from a library of $K$ equal-sized messages duplicated in $N$ servers, while keeping the index of the desired message private from any colluding set in $\mathcal{P}$. Although achieving high rates typically requires sufficiently large message sizes, smaller message sizes also desirable due to reduced implementation complexity and fewer constraints. By characterizing the capacity-achieving schemes, Tian, Sun, and Chen (2019) showed that the optimal message size for uniformly decomposable PIR schemes under no-collusion setting is $N-1$. However, comparable results are not yet available for more general collusion settings.
  In this work, we present a complete characterization of the properties of capacity-achieving decomposable PIR schemes under arbitrary collusion patterns. Building on this characterization, we derive a general lower bound on the optimal message size for capacity-achieving uniformly decomposable PIR schemes under an arbitrary collusion pattern $\mathcal{P}$, expressed in terms of the hitting number of a newly defined family of subsets of servers determined by the collusion pattern $\mathcal{P}$. Finally, we specialize the lower bound to several important classes of collusion patterns, including $T$-collusion, disjoint collections of colluding sets, cyclically $T$-contiguous collusion, and disjoint collections of cyclically contiguous colluding sets. For the last two collusion patterns, we present matching achievable schemes that attain the corresponding bounds, thereby providing a complete characterization of the optimal message size.

</details>


### [67] [Coding Schemes for Document Exchange under Multiple Substring Edits](https://arxiv.org/abs/2601.18441)
*Hrishi Narayanan,Vinayak Ramkumar,Rawad Bitar,Antonia Wachter-Zeh*

Main category: cs.IT

TL;DR: 本文研究了多子串编辑下的文档交换问题，提出了低复杂度的编码方案，编码长度为4t log n + o(log n)比特，优于现有方案的计算复杂度，并在均匀字符串下实现了(4t-1) log n + o(log n)比特的期望编码长度。


<details>
  <summary>Details</summary>
Motivation: 研究多子串编辑下的文档交换问题，现有方案虽然能达到4t log n + O(log log n)比特的编码长度，但计算复杂度很高。需要设计低复杂度的编码方案，并进一步在均匀字符串下优化期望编码长度。

Method: 构造了低复杂度的文档交换方案，编码长度为4t log n + o(log n)比特。针对均匀字符串的情况，开发了期望编码长度为(4t-1) log n + o(log n)比特的方案。相比之前只处理单子串编辑的工作，本文处理了多子串编辑的情况。

Result: 成功设计了低复杂度的文档交换方案，编码长度达到4t log n + o(log n)比特，显著降低了计算复杂度。在均匀字符串下，实现了(4t-1) log n + o(log n)比特的期望编码长度，优于现有方案。

Conclusion: 本文在多子串编辑的文档交换问题上取得了重要进展，提出了低复杂度的编码方案，并在均匀字符串下进一步优化了期望编码长度，为文档交换问题的实际应用提供了更高效的解决方案。

Abstract: We study the document exchange problem under multiple substring edits. A substring edit in a string $\mathbf{x}$ occurs when a substring $\mathbf{u}$ of $\mathbf{x}$ is replaced by an arbitrary string $\mathbf{v}$. The lengths of $\mathbf{u}$ and $\mathbf{v}$ are bounded from above by a fixed constant. Let $\mathbf{x}$ and $\mathbf{y}$ be two binary strings that differ by multiple substring edits. The aim of document exchange schemes is to construct an encoding of $\mathbf{x}$ with small length such that $\mathbf{x}$ can be recovered using $\mathbf{y}$ and the encoding. We construct a low-complexity document exchange scheme with encoding length of $4t\log n+o(\log n)$ bits, where $n$ is the length of the string $\mathbf{x}$. The best known scheme achieves an encoding length of $4t \log n+O(\log\log n)$ bits, but at a much higher computational complexity. Then, we investigate the average length of valid encodings for document exchange schemes with uniform strings $\mathbf{x}$ and develop a scheme with an expected encoding length of $(4t-1) \log n+o(\log n)$ bits. In this setting, prior works have only constructed schemes for a single substring edit.

</details>


### [68] [Finite-Aperture Fluid Antenna Array Design: Analysis and Algorithm](https://arxiv.org/abs/2601.18471)
*Zhentian Zhang,Kai-Kit Wong,Hao Jiang,Farshad Rostami Ghadi,Hyundong Shin,Yangyang Zhang*

Main category: cs.IT

TL;DR: 该论文为固定孔径下的流体天线阵列设计提供通用指导，推导了统一的CRB闭式解，提出了基于梯度的端口位置优化算法，能显著降低CRB和均方误差。


<details>
  <summary>Details</summary>
Motivation: 有限孔径约束使阵列设计变得复杂，可能削弱经典稀疏几何结构的有效性。需要为固定孔径下的流体天线阵列设计提供通用指导。

Method: 推导了统一传统和可重构阵列的闭式CRB，将Fisher信息与端口位置的几何方差显式关联；获得了随机FAA放置下最小间距的闭式概率密度函数；提出了基于梯度的连续端口位置优化算法。

Result: 优化的FAA能实现约30%的CRB降低和42.5%的均方误差降低。

Conclusion: 该研究为固定孔径下的流体天线阵列设计提供了理论分析和实用算法，显著提升了阵列性能。

Abstract: Finite-aperture constraints render array design nontrivial and can undermine the effectiveness of classical sparse geometries. This letter provides universal guidance for fluid antenna array (FAA) design under a fixed aperture. We derive a closed-form Cramér--Rao bound (CRB) that unifies conventional and reconfigurable arrays by explicitly linking the Fisher information to the geometric variance of port locations. We further obtain a closed-form probability density function of the minimum spacing under random FAA placement, which yields a principled lower bound for the minimum-spacing constraint. Building upon these analytical insights, we then propose a gradient-based algorithm to optimize continuous port locations. Utilizing a simple gradient update design, the optimized FAA can achieve about a $30\%$ CRB reduction and a $42.5\%$ reduction in mean-squared error.

</details>


### [69] [Ribbons from Independence Structure: Hypercontractivity, $Φ$-Mutual Information, and Matrix $Φ$-Entropy](https://arxiv.org/abs/2601.18516)
*Chenyu Wang,Amin Gohari*

Main category: cs.IT

TL;DR: 本文研究了具有特定独立结构的联合分布的hypercontractivity ribbon和Φ-ribbon，在基本机制中获得了紧界，提出了新的多部分推广版本和Φ-互信息类似物，并建立了矩阵Φ-ribbon的理论框架。


<details>
  <summary>Details</summary>
Motivation: 研究具有特定独立结构的联合分布的hypercontractivity ribbon和Φ-ribbon，旨在理解在给定独立约束下信息度量的边界特性，为多变量信息理论提供新的分析工具。

Method: 使用超图建模独立结构，提出显式内界描述为关联向量的凸包；建立多部分推广版本和Φ-互信息类似物；提出基于矩阵Φ-熵的矩阵Φ-ribbon概念，并研究其张量化和数据处理性质。

Result: 在基本机制中获得紧界；为一般独立结构提供Φ-ribbon的显式内界；提出新的Zhang-Yeung不等式推广版本；建立了矩阵Φ-ribbon的理论框架，并计算了双对称二进制源的精确矩阵SDPI常数。

Conclusion: 本文为具有独立结构的联合分布提供了hypercontractivity ribbon和Φ-ribbon的系统分析框架，提出了新的推广版本和矩阵扩展，为多变量信息理论中的信息度量边界研究提供了重要工具和结果。

Abstract: We study the hypercontractivity ribbon and the $Φ$-ribbon for joint distributions that obey a given independence structure, obtaining tight bounds in some basic regimes. For general independence structures, modeled as a hypergraph whose hyperedges specify mutually independent subcollections of random variables, we provide an explicit inner bound on the $Φ$-ribbon described by a simple convex hull of incidence vectors. We also provide a new multipartite generalization version and a $Φ$-mutual information analogue of the Zhang--Yeung inequality, which implies nontrivial points in the hypercontractivity ribbon and the $Φ$-ribbon respectively. Finally, we propose the matrix $Φ$-ribbon based on matrix $Φ$-entropy and establish the tensorization and data processing properties, together with the calculation of an exact matrix SDPI constant for the doubly symmetric binary source.

</details>


### [70] [Improvement of the Gilbert-Varshamov Bound for Linear Codes and Quantum Codes](https://arxiv.org/abs/2601.18590)
*Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文改进了经典Gilbert-Varshamov界和量子GV界，通过简洁的概率方法为q元线性码和量子码提供了存在性保证，改进因子为Ω(√n)。


<details>
  <summary>Details</summary>
Motivation: GV界是编码理论的核心基准，但改进困难且技术复杂，现有改进方法难以自然扩展到量子设置。本文旨在开发简洁的概率方法来改进经典和量子GV界。

Method: 开发简洁的概率方法分析q元线性码的存在性，并适应量子设置分析辛自正交结构。使用概率论证和体积计算来建立改进的存在性条件。

Result: 对于相对距离δ<1-1/q的q元线性码，存在性条件改进为(q^k-1)/(q-1) < c_δ√n·q^n/Vol_q(n,d-1)。对于量子码(δ<1-1/q^2)，存在性条件改进为(q^{2n-k}-1)/(q-1) < c_δ√n·q^{2n}/∑_{i=0}^{d-1} C(n,i)(q^2-1)^i，改进因子为Ω(√n)。

Conclusion: 本文成功开发了简洁的概率方法，显著改进了经典和量子GV界，为编码理论提供了更强的存在性保证，特别是在量子设置中取得了重要进展。

Abstract: The Gilbert--Varshamov (GV) bound is a central benchmark in coding theory, establishing existential guarantees for error-correcting codes and serving as a baseline for both Hamming and quantum fault-tolerant information processing. Despite decades of effort, improving the GV bound is notoriously difficult, and known improvements often rely on technically heavy arguments and do not extend naturally to the quantum setting due to additional self-orthogonality constraints.
  In this work we develop a concise probabilistic method that yields an improvement over the classical GV bound for $q$-ary linear codes. For relative distance $δ=d/n<1-1/q$, we show that an $[n,k,d]_q$ linear code exists whenever $\frac{q^{k}-1}{q-1}\;<\;\frac{c_δ\sqrt{n}\, q^{n}}{\mathrm{Vol}_q(n,d-1)}$, for positive constant $c_δ$ depending only on $δ$, where $\mathrm{Vol}_q(n,d-1)$ denotes the volume of a $q$-ary Hamming ball.
  We further adapt this approach to the quantum setting by analyzing symplectic self-orthogonal structures. For $δ<1-1/q^2$, we obtain an improved quantum GV bound: there exists a $q$-ary quantum code $[[n,\,n-k,\,d]]$ provided that $\frac{q^{2n-k}-1}{q-1}<\frac{c_δ\sqrt{n}\cdot q^{2n}}{\sum_{i=0}^{d-1}\binom{n}{i}(q^2-1)^i}$. In particular, our result improves the standard quantum GV bound by an $Ω(\sqrt{n})$ multiplicative factor.

</details>


### [71] [Quantum Rotation Diversity in Displaced Squeezed Binary Phase-Shift Keying](https://arxiv.org/abs/2601.18655)
*Ioannis Krikidis*

Main category: cs.IT

TL;DR: 提出量子旋转分集方案，在Gamma-Gamma湍流信道中使用BPSK位移压缩态和零差检测，通过被动正交旋转耦合连续时隙，实现分集阶数为2，优化旋转角度和能量分配后可达分集阶数4。


<details>
  <summary>Details</summary>
Motivation: 在光学量子通信中，大气湍流信道会导致信号衰落，影响通信性能。需要设计有效的分集方案来提高量子通信系统在湍流信道下的鲁棒性和可靠性。

Method: 提出量子旋转分集方案：使用二进制相移键控位移压缩态和零差检测；通过被动正交旋转耦合连续时隙，重新分配位移振幅；采用联合最大似然检测；推导符号错误率性能分析表达式；优化旋转角度和位移与压缩之间的能量分配。

Result: 在独立衰落和联合检测下获得分集阶数2；当位移振幅和压缩强度随总光子数缩放时，有效分集阶数可达4；推导出渐近分集增益和编码增益；获得最优旋转角度和能量分配的闭式解；数值结果验证了分析和超分集行为。

Conclusion: 量子旋转分集方案能有效提高光学量子通信在Gamma-Gamma湍流信道下的性能，通过优化设计可实现超分集行为，为量子通信系统在衰落信道中的应用提供了有效解决方案。

Abstract: We propose a quantum rotation diversity (QRD) scheme for optical quantum communication using binary phase-shift-keying displaced squeezed states and homodyne detection over Gamma-Gamma turbulence channels. Consecutive temporal modes are coupled by a passive orthogonal rotation that redistributes the displacement amplitude between slots, yielding a diversity order of two under independent fading and joint maximum-likelihood detection. Analytical expressions for the symbol-error rate performance, along with asymptotic results for the diversity and coding gains, are derived. The optimal rotation angle and energy allocation between displacement and squeezing are obtained in closed form. Furthermore, we show that when both the displacement amplitude and the squeezing strength scale with the total photon number, an effective diversity order of four is achieved. Numerical results validate the analysis and demonstrate the super-diversity behaviour of the proposed QRD scheme.

</details>


### [72] [Balancing Privacy and Robustness in Coded Computing Under Profiled Workers](https://arxiv.org/abs/2601.18661)
*Rimpi Borah,J. Harshan,Aaditya Sharma*

Main category: cs.IT

TL;DR: 研究分布式计算中不可信工作节点时，评估索引分配如何影响隐私保护和拜占庭错误定位能力，发现隐私最优和鲁棒性最优的索引分配存在根本性冲突。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，不可信工作节点的评估索引分配对隐私保护和鲁棒性至关重要。现有研究需要明确分析索引分配如何同时影响隐私（对抗合谋好奇节点）和鲁棒性（对抗拜占庭错误），特别是在有限精度算术下。

Method: 基于数值稳定的拉格朗日编码计算（NS-LCC）框架，推导分析边界量化不同评估索引分配对隐私和鲁棒性的影响。构建优化问题识别隐私最优和鲁棒性最优的索引分配，并提出低复杂度贪婪分配策略来平衡两者。

Result: 发现隐私最优和鲁棒性最优的索引分配存在根本性差异：最大化隐私的索引选择会降低错误定位能力，反之亦然。提出的贪婪策略能有效近似隐私与鲁棒性的最优平衡。

Conclusion: 在分布式计算中，隐私保护和错误定位能力之间存在固有的权衡关系。需要设计智能的索引分配策略来平衡这两个关键目标，提出的贪婪方法为此提供了实用解决方案。

Abstract: In distributed computing with untrusted workers, the assignment of evaluation indices plays a critical role in determining both privacy and robustness. In this work, we study how the placement of unreliable workers within the Numerically Stable Lagrange Coded Computing (NS-LCC) framework influences privacy and the ability to localize Byzantine errors. We derive analytical bounds that quantify how different evaluation-index assignments affect privacy against colluding curious workers and robustness against Byzantine corruption under finite-precision arithmetic. Using these bounds, we formulate optimization problems that identify privacy-optimal and robustness-optimal index placements and show that the resulting assignments are fundamentally different. This exposes that index choices that maximizes privacy degrade error-localization, and vice versa. To jointly navigate this trade-off, we propose a low-complexity greedy assignment strategy that closely approximates the optimal balance between privacy and robustness.

</details>


### [73] [A Scanning-Based Indoor Optical Wireless Positioning System with Single VCSEL](https://arxiv.org/abs/2601.18740)
*Yicheng Dong,Rashid Iqbal,Julien Le Kernec,Hanaa Abumarshoud*

Main category: cs.IT

TL;DR: 提出一种基于单个垂直腔面发射激光器的室内可见光定位系统，通过二维扫描实现三维定位，硬件简单且成本效益高


<details>
  <summary>Details</summary>
Motivation: 传统多发射器可见光定位系统硬件复杂且成本高，需要简化系统架构并保持高精度定位能力

Method: 使用单个垂直腔面发射激光器安装在空间天花板中心，以1度分辨率在方位角和仰角两个维度进行空间扫描，结合到达角和接收信号强度进行定位

Result: 仿真结果显示系统在大多数测试点达到亚厘米级精度，在成本效益和简单性方面优于传统多发射器VLP方案

Conclusion: 提出的单发射器扫描式可见光定位系统在保持高精度的同时显著简化了硬件架构，为室内定位提供了一种成本效益高的解决方案

Abstract: This paper presents a novel indoor visible light positioning (VLP) system utilising one vertical-cavity surface-emitting laser installed at the ceiling centre of a space. The system offers three-dimensional localisation by sweeping through space at one-degree resolution in two dimensions (azimuth and elevation), significantly simplifying hardware. Through incorporating the angle of arrival and received signal strength, this system demonstrates excellent precision in indoor positioning. Simulation results verify that the system attains sub-centimetre precision for most test points, outperforming conventional multi-transmitter VLP schemes in cost-efficiency and simplicity.

</details>


### [74] [Multi-Stage Structured Estimators for Information Freshness](https://arxiv.org/abs/2601.18763)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 提出p-MAP估计器，将MAP估计器建模为有限阶段的分段常数函数，以解决信息新鲜度分析中MAP估计器难以分析的问题


<details>
  <summary>Details</summary>
Motivation: 当前信息新鲜度文献主要关注鞅估计器分析，但鞅估计器并非最优，尤其在基于拉取的更新系统中，MAP估计器虽然最优但分析困难

Method: 引入新的p-MAP估计器类别，将MAP估计器建模为具有有限阶段的分段常数函数，从而更接近完全表征信息新鲜度建模中的MAP估计器

Result: 提出了一种新的估计器框架，使得原本难以分析的MAP估计器可以通过有限阶段的分段常数函数来建模，为信息新鲜度分析提供了新的工具

Conclusion: p-MAP估计器填补了信息新鲜度分析中鞅估计器与MAP估计器之间的空白，为解决MAP估计器的分析挑战提供了新途径

Abstract: Most of the contemporary literature on information freshness solely focuses on the analysis of freshness for martingale estimators, which simply use the most recently received update as the current estimate. While martingale estimators are easier to analyze, they are far from optimal, especially in pull-based update systems, where maximum aposteriori probability (MAP) estimators are known to be optimal, but are analytically challenging. In this work, we introduce a new class of estimators called $p$-MAP estimators, which enable us to model the MAP estimator as a piecewise constant function with finitely many stages, bringing us closer to a full characterization of the MAP estimators when modeling information freshness.

</details>
