{"id": "2512.13848", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13848", "abs": "https://arxiv.org/abs/2512.13848", "authors": ["Mufhumudzi Muthivhi", "Terence L van Zyl", "Hairong Wang"], "title": "BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model", "comment": null, "summary": "Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets."}
{"id": "2512.14034", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14034", "abs": "https://arxiv.org/abs/2512.14034", "authors": ["Yifan Shao", "Peilin Zhou"], "title": "Intent-Guided Reasoning for Sequential Recommendation", "comment": "Under Review", "summary": "Sequential recommendation systems aim to capture users' evolving preferences from their interaction histories. Recent reasoningenhanced methods have shown promise by introducing deliberate, chain-of-thought-like processes with intermediate reasoning steps. However, these methods rely solely on the next target item as supervision, leading to two critical issues: (1) reasoning instability--the process becomes overly sensitive to recent behaviors and spurious interactions like accidental clicks, and (2) surface-level reasoning--the model memorizes item-to-item transitions rather than understanding intrinsic behavior patterns. To address these challenges, we propose IGR-SR, an Intent-Guided Reasoning framework for Sequential Recommendation that anchors the reasoning process to explicitly extracted high-level intents. Our framework comprises three key components: (1) a Latent Intent Distiller (LID) that efficiently extracts multi-faceted intents using a frozen encoder with learnable tokens, (2) an Intent-aware Deliberative Reasoner (IDR) that decouples reasoning into intent deliberation and decision-making via a dual-attention architecture, and (3) an Intent Consistency Regularization (ICR) that ensures robustness by enforcing consistent representations across different intent views. Extensive experiments on three public datasets demonstrate that IGR-SR achieves an average 7.13% improvement over state-of-the-art baselines. Critically, under 20% behavioral noise, IGR-SR degrades only 10.4% compared to 16.2% and 18.6% for competing methods, validating the effectiveness and robustness of intent-guided reasoning."}
{"id": "2512.14036", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14036", "abs": "https://arxiv.org/abs/2512.14036", "authors": ["Yifan Shao", "Peilin Zhou", "Shoujin Wang", "Weizhi Zhang", "Xu Cai", "Sunghun Kim"], "title": "DTRec: Learning Dynamic Reasoning Trajectories for Sequential Recommendation", "comment": "Under Review", "summary": "Inspired by advances in LLMs, reasoning-enhanced sequential recommendation performs multi-step deliberation before making final predictions, unlocking greater potential for capturing user preferences. However, current methods are constrained by static reasoning trajectories that are ill-suited for the diverse complexity of user behaviors. They suffer from two key limitations: (1) a static reasoning direction, which uses flat supervision signals misaligned with human-like hierarchical reasoning, and (2) a fixed reasoning depth, which inefficiently applies the same computational effort to all users, regardless of pattern complexity. These rigidity lead to suboptimal performance and significant computational waste. To overcome these challenges, we propose DTRec, a novel and effective framework that explores the Dynamic reasoning Trajectory for Sequential Recommendation along both direction and depth. To guide the direction, we develop Hierarchical Process Supervision (HPS), which provides coarse-to-fine supervisory signals to emulate the natural, progressive refinement of human cognitive processes. To optimize the depth, we introduce the Adaptive Reasoning Halting (ARH) mechanism that dynamically adjusts the number of reasoning steps by jointly monitoring three indicators. Extensive experiments on three real-world datasets demonstrate the superiority of our approach, achieving up to a 24.5% performance improvement over strong baselines while simultaneously reducing computational cost by up to 41.6%."}
{"id": "2512.14041", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14041", "abs": "https://arxiv.org/abs/2512.14041", "authors": ["Mingjia Yin", "Junwei Pan", "Hao Wang", "Ximei Wang", "Shangyu Zhang", "Jie Jiang", "Defu Lian", "Enhong Chen"], "title": "From Feature Interaction to Feature Generation: A Generative Paradigm of CTR Prediction Models", "comment": null, "summary": "Click-Through Rate (CTR) prediction, a core task in recommendation systems, aims to estimate the probability of users clicking on items. Existing models predominantly follow a discriminative paradigm, which relies heavily on explicit interactions between raw ID embeddings. However, this paradigm inherently renders them susceptible to two critical issues: embedding dimensional collapse and information redundancy, stemming from the over-reliance on feature interactions \\emph{over raw ID embeddings}. To address these limitations, we propose a novel \\emph{Supervised Feature Generation (SFG)} framework, \\emph{shifting the paradigm from discriminative ``feature interaction\" to generative ``feature generation\"}. Specifically, SFG comprises two key components: an \\emph{Encoder} that constructs hidden embeddings for each feature, and a \\emph{Decoder} tasked with regenerating the feature embeddings of all features from these hidden representations. Unlike existing generative approaches that adopt self-supervised losses, we introduce a supervised loss to utilize the supervised signal, \\ie, click or not, in the CTR prediction task. This framework exhibits strong generalizability: it can be seamlessly integrated with most existing CTR models, reformulating them under the generative paradigm. Extensive experiments demonstrate that SFG consistently mitigates embedding collapse and reduces information redundancy, while yielding substantial performance gains across various datasets and base models. The code is available at https://github.com/USTC-StarTeam/GE4Rec."}
{"id": "2512.14104", "categories": ["cs.GT", "cs.AR", "cs.CY", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.14104", "abs": "https://arxiv.org/abs/2512.14104", "authors": ["Karthikeyan Sankaralingam"], "title": "The Impact Market to Save Conference Peer Review: Decoupling Dissemination and Credentialing", "comment": "41 pages, 4 figures,", "summary": "Top-tier academic conferences are failing under the strain of two irreconcilable roles: (1) rapid dissemination of all sound research and (2) scarce credentialing for prestige and career advancement. This conflict has created a reviewer roulette and anonymous tribunal model - a zero-cost attack system - characterized by high-stakes subjectivity, turf wars, and the arbitrary rejection of sound research (the equivalence class problem). We propose the Impact Market (IM), a novel three-phase system that decouples publication from prestige. Phase 1 (Publication): All sound and rigorous papers are accepted via a PC review, solving the \"equivalence class\" problem. Phase 2 (Investment): An immediate, scarce prestige signal is created via a futures market. Senior community members invest tokens into published papers, creating a transparent, crowdsourced Net Invested Score (NIS). Phase 3 (Calibration): A 3-year lookback mechanism validates these investments against a manipulation-resistant Multi-Vector Impact Score (MVIS). This MVIS adjusts each investor's future influence (their Investor Rating), imposing a quantifiable cost on bad actors and rewarding accurate speculation. The IM model replaces a hidden, zero-cost attack system with a transparent, accountable, and data-driven market that aligns immediate credentialing with long-term, validated impact. Agent-based simulations demonstrate that while a passive market matches current protocols in low-skill environments, introducing investor agency and conviction betting increases the retrieval of high-impact papers from 28% to over 85% under identical conditions, confirming that incentivized self-selection is the mechanism required to scale peer review."}
{"id": "2512.14390", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.14390", "abs": "https://arxiv.org/abs/2512.14390", "authors": ["Jakub Balabán"], "title": "Finding $b$-colorings Using Feedback Edges", "comment": null, "summary": "A $b$-coloring of a graph is a proper vertex coloring such that each color class contains a vertex that sees all other colors in its neighborhood. The $b$-coloring problem, in which the task is to decide whether a graph admits a $b$-coloring with $k$ colors, is NP-complete in general but polytime solvable on trees. Moreover, it is known that $b$-coloring is in XP but W[$t$]-hard for all $t \\in \\mathbb{N}$ when parameterized by tree-width. In fact, only very few parameters, such as the vertex cover number, were known to admit an FPT algorithm for $b$-coloring. In this paper, we consider a more restrictive parameter measuring similarity to trees than tree-width, namely the feedback edge number, and show that $b$-coloring is fixed-parameter tractable under this parameterization. Our algorithm combines standard techniques used in parameterized algorithmics with the problem-specific ideas used in the polytime algorithm for trees. In addition, we present an FPT algorithm for $b$-coloring parameterized by distance to co-cluster, which is a parameter measuring similarity to complete multipartite graphs. Finally, we make several observations based on known results, including that $b$-coloring is W[$1$]-hard when parameterized by tree-depth."}
{"id": "2512.14022", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14022", "abs": "https://arxiv.org/abs/2512.14022", "authors": ["Hanju Yoo", "Dongha Choi", "Songkuk Kim", "Chan-Byoung Chae", "Robert W. Heath"], "title": "Symbol Distributions in Semantic Communications: A Source-Channel Equilibrium Perspective", "comment": null, "summary": "Semantic communication systems often use an end-to-end neural network to map input data into continuous symbols. These symbols, which are essentially neural network features, usually have fixed dimensions and heavy-tailed distributions. However, due to the end-to-end training nature of the neural network encoder, the underlying reason for the symbol distribution remains underexplored. We propose a new explanation for the semantic symbol distribution: an inherent trade-off between source coding and communications. Specifically, the encoder balances two objectives: allocating power for minimum \\emph{effective codelength} (for source coding) and maximizing mutual information (for communications). We formalize this trade-off via an information-theoretic optimization framework, which yields a Student's $t$-distribution as the resulting symbol distribution. Through extensive studies on image-based semantic systems, we find that our formulation models the learned symbols and predicts how the symbol distribution's shape parameter changes with respect to (i) the use of variable-length coding and (ii) the dataset's entropy variability. Furthermore, we demonstrate how introducing a regularizer that enforces a target symbol distribution, which guides the encoder towards a target prior (e.g., Gaussian), improves training convergence and supports our hypothesis."}
{"id": "2512.14425", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.14425", "abs": "https://arxiv.org/abs/2512.14425", "authors": ["Hosna Hooshyar", "Mattia Fumagalli", "Marco Montali", "Giancarlo Guizzardi"], "title": "Time and Relations into Focus: Ontological Foundations of Object-Centric Event Data", "comment": null, "summary": "Object-centric process mining is a new branch of process mining where events are associated with multiple objects, and where object-to-object interactions are essential to understand the process dynamics. Traditional event data models, also called case-centric, are unable to cope with the complexity introduced by these more refined relationships. Several models have been made to move from case-centric to Object-Centric Event Data (OCED), trying to retain simplicity as much as possible. Still, these suffer from inherent ambiguities, and lack a comprehensive support of essential dimensions related to time and (dynamic) relations. In this work, we propose to fill this gap by leveraging a well-founded ontology of events and bringing ontological foundations to OCED, with a three-step approach. First, we start from key open issues reported in the literature regarding current OCED metamodels, and witness their ambiguity and expressiveness limitations on illustrative and representative examples proposed therein. Second, we consider the OCED Core Model, currently proposed as the basis for defining a new standard for object-centric event data, and we enhance it by grounding it on a lightweight version of UFO-B called gUFO, a well-known foundational ontology tailored to the representation of objects, events, time, and their (dynamic) relations. This results in a new metamodel, which we call gOCED. The third contribution then shows how gOCED at once covers the features of existing metamodels preserving their simplicity, and extends them with the essential features needed to overcome the ambiguity and expressiveness issues reported in the literature."}
{"id": "2512.13904", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13904", "abs": "https://arxiv.org/abs/2512.13904", "authors": ["Amirkia Rafiei Oskooei", "Eren Caglar", "Ibrahim Sahin", "Ayse Kayabay", "Mehmet S. Aktas"], "title": "Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing", "comment": "Accepted manuscript. Published in Applied Sciences, 2025", "summary": "The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($τ< 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms."}
{"id": "2512.14047", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14047", "abs": "https://arxiv.org/abs/2512.14047", "authors": ["Kaike Zhang", "Qi Cao", "Fei Sun", "Xinran Liu"], "title": "AsarRec: Adaptive Sequential Augmentation for Robust Self-supervised Sequential Recommendation", "comment": null, "summary": "Sequential recommender systems have demonstrated strong capabilities in modeling users' dynamic preferences and capturing item transition patterns. However, real-world user behaviors are often noisy due to factors such as human errors, uncertainty, and behavioral ambiguity, which can lead to degraded recommendation performance. To address this issue, recent approaches widely adopt self-supervised learning (SSL), particularly contrastive learning, by generating perturbed views of user interaction sequences and maximizing their mutual information to improve model robustness. However, these methods heavily rely on their pre-defined static augmentation strategies~(where the augmentation type remains fixed once chosen) to construct augmented views, leading to two critical challenges: (1) the optimal augmentation type can vary significantly across different scenarios; (2) inappropriate augmentations may even degrade recommendation performance, limiting the effectiveness of SSL. To overcome these limitations, we propose an adaptive augmentation framework. We first unify existing basic augmentation operations into a unified formulation via structured transformation matrices. Building on this, we introduce AsarRec (Adaptive Sequential Augmentation for Robust Sequential Recommendation), which learns to generate transformation matrices by encoding user sequences into probabilistic transition matrices and projecting them into hard semi-doubly stochastic matrices via a differentiable Semi-Sinkhorn algorithm. To ensure that the learned augmentations benefit downstream performance, we jointly optimize three objectives: diversity, semantic invariance, and informativeness. Extensive experiments on three benchmark datasets under varying noise levels validate the effectiveness of AsarRec, demonstrating its superior robustness and consistent improvements."}
{"id": "2512.14409", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.14409", "abs": "https://arxiv.org/abs/2512.14409", "authors": ["Michelle Döring", "Jannes Malanowski", "Stefan Neubert"], "title": "Cost-Free Neutrality for the River Method", "comment": "appears at AAAI 2026", "summary": "Recently, the River Method was introduced as novel refinement of the Split Cycle voting rule.\n  The decision-making process of River is closely related to the well established Ranked Pairs Method.\n  Both methods consider a margin graph computed from the voters' preferences and eliminate majority cycles in that graph to choose a winner.\n  As ties can occur in the margin graph, a tiebreaker is required along with the preferences.\n  While such a tiebreaker makes the computation efficient, it compromises the fundamental property of neutrality: the voting rule should not favor alternatives in advance.\n  One way to reintroduce neutrality is to use Parallel-Universe Tiebreaking (PUT), where each alternative is a winner if it wins according to any possible tiebreaker.\n  Unfortunately, computing the winners selected by Ranked Pairs with PUT is NP-complete.\n  Given the similarity of River to Ranked Pairs, one might expect River to suffer from the same complexity.\n  Surprisingly, we show the opposite:\n  We present a polynomial-time algorithm for computing River winners with PUT, highlighting significant structural advantages of River over Ranked Pairs.\n  Our Fused-Universe (FUN) algorithm simulates River for every possible tiebreaking in one pass.\n  From the resulting FUN diagram one can then directly read off both the set of winners and, for each winner, a certificate that explains how this alternative dominates the others."}
{"id": "2512.14073", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.14073", "abs": "https://arxiv.org/abs/2512.14073", "authors": ["Xiumei Li", "Xiaotong Sun", "Min Sha"], "title": "Complete weight enumerators and weight hierarchies for linear codes from quadratic forms", "comment": null, "summary": "In this paper, for an odd prime power $q$, we extend the construction of Xie et al. \\cite{XOYM2023} to propose two classes of linear codes $\\mathcal{C}_{Q}$ and $\\mathcal{C}_{Q}'$ over the finite field $\\mathbb{F}_{q}$ with at most four nonzero weights. These codes are derived from quadratic forms through a bivariate construction. We completely determine their complete weight enumerators and weight hierarchies by employing exponential sums. Most of these codes are minimal and some are optimal in the sense that they meet the Griesmer bound. Furthermore, we also establish the weight hierarchies of $\\mathcal{C}_{Q,N}$ and $\\mathcal{C}_{Q,N}'$, which are the descended codes of $\\mathcal{C}_{Q}$ and $\\mathcal{C}_{Q}'$."}
{"id": "2512.14622", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.14622", "abs": "https://arxiv.org/abs/2512.14622", "authors": ["Ostap Vykhopen", "Viktoria Skorik", "Maxim Tereschenko", "Veronika Solopova"], "title": "Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR", "comment": null, "summary": "Large language models can already query databases, yet most existing systems remain reactive: they rely on explicit user prompts and do not actively explore data. We introduce DAR (Data Agnostic Researcher), a multi-agent system that performs end-to-end database research without human-initiated queries. DAR orchestrates specialized AI agents across three layers: initialization (intent inference and metadata extraction), execution (SQL and AI-based query synthesis with iterative validation), and synthesis (report generation with built-in quality control). All reasoning is executed directly inside BigQuery using native generative AI functions, eliminating data movement and preserving data governance. On a realistic asset-incident dataset, DAR completes the full analytical task in 16 minutes, compared to 8.5 hours for a professional analyst (approximately 32x times faster), while producing useful pattern-based insights and evidence-grounded recommendations. Although human experts continue to offer deeper contextual interpretation, DAR excels at rapid exploratory analysis. Overall, this work shifts database interaction from query-driven assistance toward autonomous, research-driven exploration within cloud data warehouses."}
{"id": "2512.14185", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14185", "abs": "https://arxiv.org/abs/2512.14185", "authors": ["Emanuele Artioli", "Farzad Tashtarian", "Christian Timmerer"], "title": "End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach", "comment": "The 35th edition of the Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV '25), March 31-April 4, 2025, Stellenbosch, South Africa", "summary": "The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements."}
{"id": "2512.14277", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14277", "abs": "https://arxiv.org/abs/2512.14277", "authors": ["Panayiotis Smeros", "Vincent Emonet", "Ruijie Wang", "Ana-Claudia Sima", "Tarcisio Mendes de Farias"], "title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions", "comment": "17 pages, 8 figures, 1 table. Under Review", "summary": "The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat."}
{"id": "2512.14457", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.14457", "abs": "https://arxiv.org/abs/2512.14457", "authors": ["Jingyang Zhao", "Mingyu Xiao"], "title": "An Improved Approximation Algorithm for Maximum Weight 3-Path Packing", "comment": null, "summary": "Given a complete graph with $n$ vertices and non-negative edge weights, where $n$ is divisible by 3, the maximum weight 3-path packing problem is to find a set of $n/3$ vertex-disjoint 3-paths such that the total weight of the 3-paths in the packing is maximized. This problem is closely related to the classic maximum weight matching problem. In this paper, we propose a $10/17$-approximation algorithm, improving the best-known $7/12$-approximation algorithm (ESA 2015). Our result is obtained by making a trade-off among three algorithms. The first is based on the maximum weight matching of size $n/2$, the second is based on the maximum weight matching of size $n/3$, and the last is based on an approximation algorithm for star packing. Our first algorithm is the same as the previous $7/12$-approximation algorithm, but we propose a new analysis method -- a charging method -- for this problem, which is not only essential to analyze our second algorithm but also may be extended to analyze algorithms for some related problems."}
{"id": "2512.14105", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.14105", "abs": "https://arxiv.org/abs/2512.14105", "authors": ["Nithin V. Sabu", "Kaushlendra Pandey", "Abhishek K. Gupta", "Sameer S. M"], "title": "Target Detection in Clustered Mobile Nanomachine Networks", "comment": null, "summary": "This work focuses on the development of an analytical framework to study a diffusion-assisted molecular communication-based network of nano-machines (NMs) with a clustered initial deployment to detect a target in a three-dimensional (3D) medium. Leveraging the Poisson cluster process to model the initial locations of clustered NMs, we derive the analytical expression for the target detection probability with respect to time along with relevant bounds. We also investigate a single-cluster scenario. All the derived expressions are validated through extensive particle-based simulations. Furthermore, we analyze the impact of key parameters, such as the mean number of NMs per cluster, the density of the cluster, and the spatial spread, on the detection performance. Our results show that detection probability is greatly influenced by clustering, and different spatial arrangements produce varying performances. The results offer a better understanding of how molecular communication systems should be designed for optimal target detection in nanoscale and biological environments."}
{"id": "2512.14313", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14313", "abs": "https://arxiv.org/abs/2512.14313", "authors": ["Malika Iratni", "Mohand Boughanem", "Taoufiq Dkaki"], "title": "Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias", "comment": null, "summary": "Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the \"lost in the middle\" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines."}
{"id": "2512.14165", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14165", "abs": "https://arxiv.org/abs/2512.14165", "authors": ["Wenzhuo Zou", "Ming-Min Zhao", "An Liu", "Min-Jian Zhao"], "title": "Robust Beamforming for Multiuser MIMO Systems with Unknown Channel Statistics: A Hybrid Offline-Online Framework", "comment": "13 pages, 8 figures", "summary": "Robust beamforming design under imperfect channel state information (CSI) is a fundamental challenge in multiuser multiple-input multiple-output (MU-MIMO) systems, particularly when the channel estimation error statistics are unknown. Conventional model-driven methods usually rely on prior knowledge of the error covariance matrix and data-driven deep learning approaches suffer from poor generalization capability to unseen channel conditions. To address these limitations, this paper proposes a hybrid offline-online framework that achieves effective offline learning and rapid online adaptation. In the offline phase, we propose a shared (among users) deep neural network (DNN) that is able to learn the channel estimation error covariance from observed samples, thus enabling robust beamforming without statistical priors. Meanwhile, to facilitate real-time deployment, we propose a sparse augmented low-rank (SALR) method to reduce complexity while maintaining comparable performance. In the online phase, we show that the proposed network can be rapidly fine-tuned with minimal gradient steps. Furthermore, a multiple basis model-agnostic meta-learning (MB-MAML) strategy is further proposed to maintain multiple meta-initializations and by dynamically selecting the best one online, we can improve the adaptation and generalization capability of the proposed framework under unseen or non-stationary channels. Simulation results demonstrate that the proposed offline-online framework exhibits strong robustness across diverse channel conditions and it is able to significantly outperform state-of-the-art (SOTA) baselines."}
{"id": "2512.14490", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14490", "abs": "https://arxiv.org/abs/2512.14490", "authors": ["Shifu Bie", "Jiangxia Cao", "Zixiao Luo", "Yichuan Zou", "Lei Liang", "Lu Zhang", "Linxun Chen", "Zhaojie Liu", "Xuanping Li", "Guorui Zhou", "Kaiqiao Zhan", "Kun Gai"], "title": "PushGen: Push Notifications Generation with LLM", "comment": "Accepted by WSDM 2026", "summary": "We present PushGen, an automated framework for generating high-quality push notifications comparable to human-crafted content. With the rise of generative models, there is growing interest in leveraging LLMs for push content generation. Although LLMs make content generation straightforward and cost-effective, maintaining stylistic control and reliable quality assessment remains challenging, as both directly impact user engagement. To address these issues, PushGen combines two key components: (1) a controllable category prompt technique to guide LLM outputs toward desired styles, and (2) a reward model that ranks and selects generated candidates. Extensive offline and online experiments demonstrate its effectiveness, which has been deployed in large-scale industrial applications, serving hundreds of millions of users daily."}
{"id": "2512.14424", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14424", "abs": "https://arxiv.org/abs/2512.14424", "authors": ["Yewen Cao", "Yulin Shao"], "title": "Agile Affine Frequency Division Multiplexing", "comment": null, "summary": "The advancement to 6G calls for waveforms that transcend static robustness to achieve intelligent adaptability. Affine Frequency Division Multiplexing (AFDM), despite its strength in doubly-dispersive channels, has been confined by chirp parameters optimized for worst-case scenarios. This paper shatters this limitation with Agile-AFDM, a novel framework that endows AFDM with dynamic, data-aware intelligence. By redefining chirp parameters as optimizable variables for each transmission block based on real-time channel and data information, Agile-AFDM transforms into an adaptive platform. It can actively reconfigure its waveform to minimize peak-to-average power ratio (PAPR) for power efficiency, suppress inter-carrier interference (ICI) for communication reliability, or reduce Cramer-Rao bound (CRLB) for sensing accuracy. This paradigm shift from a static, one-size-fits-all waveform to a context-aware signal designer is made practical by efficient, tailored optimization algorithms. Comprehensive simulations demonstrate that this capability delivers significant performance gains across all metrics, surpassing conventional OFDM and static AFDM. Agile-AFDM, therefore, offers a crucial step forward in the design of agile waveforms for 6G and beyond."}
{"id": "2512.14503", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14503", "abs": "https://arxiv.org/abs/2512.14503", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Mao Zhang", "Wen Chen", "Wenjun Yang", "Yujie Luo", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Binbin Cao", "Changfa Wu", "Dixuan Wang", "Han Wu", "Haoyi Hu", "Kewei Zhu", "Lang Tian", "Lin Yang", "Qiqi Huang", "Siqi Yang", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Xiaowei Huang", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yujin Yuan", "Yuliang Yan", "Zile Zhou"], "title": "RecGPT-V2 Technical Report", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.\n  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility."}
{"id": "2512.14539", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.14539", "abs": "https://arxiv.org/abs/2512.14539", "authors": ["Dan Song", "Ayfer Özgür", "Tsachy Weissman"], "title": "The Performance of Compression-Based Denoisers", "comment": "20 pages, 3 figures", "summary": "We consider a denoiser that reconstructs a stationary ergodic source by lossily compressing samples of the source observed through a memoryless noisy channel. Prior work on compression-based denoising has been limited to additive noise channels. We extend this framework to general discrete memoryless channels by deliberately choosing the distortion measure for the lossy compressor to match the channel conditional distribution. By bounding the deviation of the empirical joint distribution of the source, observation, and denoiser outputs from satisfying a Markov property, we give an exact characterization of the loss achieved by such a denoiser. Consequences of these results are explicitly demonstrated in special cases, including for MSE and Hamming loss. A comparison is made to an indirect rate-distortion perspective on the problem."}
{"id": "2512.14565", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14565", "abs": "https://arxiv.org/abs/2512.14565", "authors": ["Fabian Haak", "Philipp Schaer"], "title": "Pairwise Comparison for Bias Identification and Quantification", "comment": null, "summary": "Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects.\n  The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation"}
