<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.IT](#cs.IT) [Total: 12]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction](https://arxiv.org/abs/2510.25220)
*Zhijie Lin,Zhuofeng Li,Chenglei Dai,Wentian Bao,Shuai Lin,Enyun Yu,Haoxiang Zhang,Liang Zhao*

Main category: cs.IR

TL;DR: GReF是一个统一的生成式高效重排框架，通过Gen-Reranker模型和Rerank-DPO训练方法，解决了传统两阶段重排方法的端到端训练和推理效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段重排方法存在两个主要挑战：生成器和评估器的分离阻碍了端到端训练，自回归生成器存在推理效率问题。

Method: 提出Gen-Reranker模型（双向编码器+动态自回归解码器），通过预训练和Rerank-DPO后训练实现端到端优化，使用有序多令牌预测(OMTP)提高推理效率。

Result: 离线实验显示GReF优于最先进的重排方法，延迟接近非自回归模型，并在快手APP（3亿日活）中显著提升了推荐质量。

Conclusion: GReF成功解决了重排中的端到端训练和推理效率问题，在实际大规模推荐系统中验证了有效性。

Abstract: In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.

</details>


### [2] [TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation](https://arxiv.org/abs/2510.25259)
*Yehjin Shin,Jeongwhan Choi,Seojin Kim,Noseong Park*

Main category: cs.IR

TL;DR: 提出了TV-Rec模型，使用时间变异的卷积滤波器替代固定核和自注意力机制，在序列推荐中捕获位置依赖的时间变化模式，实现了更高的表达能力和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有卷积滤波器在序列推荐中通常需要与自注意力机制结合，因为固定滤波器难以捕获全局交互。需要一种既能捕获局部序列模式又能处理全局交互的解决方案。

Method: 受图信号处理启发，使用时间变异的图滤波器来捕获用户序列中位置依赖的时间变化，替代固定核和自注意力机制。

Result: 在6个公共基准测试中，TV-Rec平均比最先进的基线方法性能提升7.49%，同时减少了计算量并加速了推理。

Conclusion: 时间变异卷积滤波器能够有效捕获序列推荐中的复杂交互模式，无需自注意力机制即可实现更好的性能，为序列推荐提供了更高效的解决方案。

Abstract: Recently, convolutional filters have been increasingly adopted in sequential
recommendation for their ability to capture local sequential patterns. However,
most of these models complement convolutional filters with self-attention. This
is because convolutional filters alone, generally fixed filters, struggle to
capture global interactions necessary for accurate recommendation. We propose
Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a
model inspired by graph signal processing, where time-variant graph filters
capture position-dependent temporal variations in user sequences. By replacing
both fixed kernels and self-attention with time-variant filters, TV-Rec
achieves higher expressive power and better captures complex interaction
patterns in user behavior. This design not only eliminates the need for
self-attention but also reduces computation while accelerating inference.
Extensive experiments on six public benchmarks show that TV-Rec outperforms
state-of-the-art baselines by an average of 7.49%.

</details>


### [3] [Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts](https://arxiv.org/abs/2510.25285)
*Qiushi Pan,Hao Wang,Guoyuan An,Luankang Zhang,Wei Guo,Yong Liu*

Main category: cs.IR

TL;DR: 提出Fuxi-MME框架，通过多嵌入策略和专家混合架构解决推荐系统中物品多面性特征和动态相关性的挑战。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中，物品具有多面性特征，且在不同用户上下文中相关性动态变化，现有序列推荐模型难以有效处理这些问题。

Method: 将传统单一嵌入矩阵分解为多个低维嵌入矩阵，并在Fuxi Block中用MoE层替换相关参数，实现丰富表示的自适应专业化转换。

Result: 在公开数据集上的实验结果表明，该框架优于多个竞争性基线方法。

Conclusion: Fuxi-MME框架通过多嵌入和MoE架构有效提升了推荐模型的性能和可扩展性。

Abstract: In recommendation systems, how to effectively scale up recommendation models
has been an essential research topic. While significant progress has been made
in developing advanced and scalable architectures for sequential
recommendation(SR) models, there are still challenges due to items'
multi-faceted characteristics and dynamic item relevance in the user context.
To address these issues, we propose Fuxi-MME, a framework that integrates a
multi-embedding strategy with a Mixture-of-Experts (MoE) architecture.
Specifically, to efficiently capture diverse item characteristics in a
decoupled manner, we decompose the conventional single embedding matrix into
several lower-dimensional embedding matrices. Additionally, by substituting
relevant parameters in the Fuxi Block with an MoE layer, our model achieves
adaptive and specialized transformation of the enriched representations.
Empirical results on public datasets show that our proposed framework
outperforms several competitive baselines.

</details>


### [4] [Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework](https://arxiv.org/abs/2510.25402)
*Yuqian Chai,Chaochao Wang,Weilei Wang*

Main category: cs.IR

TL;DR: 提出专利质量评估框架，通过合规性、技术一致性和图文一致性检测模块评估专利质量，并在包含人类撰写和AI生成专利的数据集上验证，结果显示AI生成专利存在更多结构性缺陷。


<details>
  <summary>Details</summary>
Motivation: 专利应用激增和AI起草工具出现，但专利内容质量的系统性评估研究有限，需要填补这一空白。

Method: 使用监管合规性、技术一致性和图文一致性检测三个模块评估专利质量，并通过集成模块生成改进建议，在包含80份人类撰写和80份AI生成专利的数据集上进行验证。

Result: 三个检测模块的平衡准确率分别为99.74%、82.12%和91.2%；AI生成专利相比人类撰写专利存在更多结构性缺陷，特别是在图文对齐和交叉引用方面。

Conclusion: 专利质量评估框架有效识别专利缺陷，AI生成专利在结构性质量方面与人类撰写专利存在显著差距，需要特别关注图文一致性和技术细节精确性。

Abstract: Despite the surge in patent applications and emergence of AI drafting tools,
systematic evaluation of patent content quality has received limited research
attention. To address this gap, We propose to evaluate patents using regulatory
compliance, technical coherence, and figure-reference consistency detection
modules, and then generate improvement suggestions via an integration module.
The framework is validated on a comprehensive dataset comprising 80
human-authored and 80 AI-generated patents from two patent drafting tools.
Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\%
respectively across the three detection modules when validated against expert
annotations. Additional analysis was conducted to examine defect distributions
across patent sections, technical domains, and authoring sources. Section-based
analysis indicates that figure-text consistency and technical detail precision
require particular attention. Mechanical Engineering and Construction show more
claim-specification inconsistencies due to complex technical documentation
requirements. AI-generated patents show a significant gap compared to
human-authored ones. While human-authored patents primarily contain
surface-level errors like typos, AI-generated patents exhibit more structural
defects in figure-text alignment and cross-references.

</details>


### [5] [Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report](https://arxiv.org/abs/2510.25428)
*Thang-Long Nguyen-Ho,Minh-Khoi Pham,Hoang-Bao Le*

Main category: cs.IR

TL;DR: 该论文介绍了在Multilingual E-commerce Search Competition中获胜的数据中心化方法，利用LLM在多语言环境下提升电商搜索相关性识别和推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言环境下用户查询与商品项目之间的相关性识别问题，提升电商平台的推荐性能。

Method: 采用以数据为中心的方法，利用大型语言模型(LLMs)及其在其他任务中的能力。

Result: 在竞赛中获得了最高分，优于其他解决方案。

Conclusion: 基于LLM的数据中心化方法在多语言电商搜索任务中表现优异，成功赢得了竞赛。

Abstract: This report details our methodology and results developed for the
Multilingual E-commerce Search Competition. The problem aims to recognize
relevance between user queries versus product items in a multilingual context
and improve recommendation performance on e-commerce platforms. Utilizing Large
Language Models (LLMs) and their capabilities in other tasks, our data-centric
method achieved the highest score compared to other solutions during the
competition. Final leaderboard is publised at
https://alibaba-international-cikm2025.github.io. The source code for our
project is published at https://github.com/nhtlongcs/e-commerce-product-search.

</details>


### [6] [Generalized Pseudo-Relevance Feedback](https://arxiv.org/abs/2510.25488)
*Yiteng Tu,Weihang Su,Yujia Zhou,Yiqun Liu,Fen Lin,Qin Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出了一种名为GPRF的假设放宽框架，用于查询重写，通过基于检索文档的自然语言重写消除模型假设并减少对相关性假设的依赖，使用强化学习训练以确保对噪声反馈的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统查询重写方法如PRF和VPRF依赖两个主要假设：相关性假设（顶部文档相关）和模型假设（需要为特定模型架构设计重写方法）。基于LLM的GRF虽然实现模型无关的重写，但存在严重幻觉或仍依赖相关性假设。

Method: 提出GPRF框架，执行模型无关的自然语言重写，基于检索文档而非仅顶部文档。设计了面向效用的强化学习训练流程，确保对噪声反馈的鲁棒性。

Result: 在多个基准测试和检索器上的广泛实验表明，GPRF始终优于强基线方法。

Conclusion: GPRF是一个有效且可泛化的查询重写框架，通过放宽假设和强化学习训练实现了更好的性能。

Abstract: Query rewriting is a fundamental technique in information retrieval (IR). It
typically employs the retrieval result as relevance feedback to refine the
query and thereby addresses the vocabulary mismatch between user queries and
relevant documents. Traditional pseudo-relevance feedback (PRF) and its
vector-based extension (VPRF) improve retrieval performance by leveraging
top-retrieved documents as relevance feedback. However, they are constructed
based on two major hypotheses: the relevance assumption (top documents are
relevant) and the model assumption (rewriting methods need to be designed
specifically for particular model architectures). While recent large language
models (LLMs)-based generative relevance feedback (GRF) enables model-free
query reformulation, it either suffers from severe LLM hallucination or, again,
relies on the relevance assumption to guarantee the effectiveness of rewriting
quality. To overcome these limitations, we introduce an assumption-relaxed
framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which
performs model-free, natural language rewriting based on retrieved documents,
not only eliminating the model assumption but also reducing dependence on the
relevance assumption. Specifically, we design a utility-oriented training
pipeline with reinforcement learning to ensure robustness against noisy
feedback. Extensive experiments across multiple benchmarks and retrievers
demonstrate that GPRF consistently outperforms strong baselines, establishing
it as an effective and generalizable framework for query rewriting.

</details>


### [7] [MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation](https://arxiv.org/abs/2510.25622)
*Yi Xu,Moyu Zhang,Chaofan Fan,Jinxin Hu,Xiaochen Li,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 提出ADA-SID框架，通过自适应行为-内容对齐和动态行为路由器解决传统语义ID方法在长尾数据中的噪声污染和信号模糊问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容的语义ID方法忽略行为特征的动态特性，且无法处理流行物品与长尾物品之间的信息质量差异，导致噪声污染和信号模糊。

Method: 采用混合量化框架MMQ-v2，包含自适应行为-内容对齐（基于信息丰富度）和动态行为路由器（对语义ID应用不同权重）。

Result: 在公开和大规模工业数据集上的实验表明，ADA-SID在生成式和判别式推荐任务中均表现出显著优势。

Conclusion: ADA-SID通过自适应对齐、去噪和放大机制，有效提升了语义ID在推荐系统中的表达能力和实用性。

Abstract: Industrial recommender systems rely on unique Item Identifiers (ItemIDs).
However, this method struggles with scalability and generalization in large,
dynamic datasets that have sparse long-tail data.Content-based Semantic IDs
(SIDs) address this by sharing knowledge through content quantization. However,
by ignoring dynamic behavioral properties, purely content-based SIDs have
limited expressive power. Existing methods attempt to incorporate behavioral
information but overlook a critical distinction: unlike relatively uniform
content features, user-item interactions are highly skewed and diverse,
creating a vast information gap in quality and quantity between popular and
long-tail items. This oversight leads to two critical limitations: (1) Noise
Corruption: Indiscriminate behavior-content alignment allows collaborative
noise from long-tail items to corrupt their content representations, leading to
the loss of critical multimodal information. (2)Signal Obscurity: The
equal-weighting scheme for SIDs fails to reflect the varying importance of
different behavioral signals, making it difficult for downstream tasks to
distinguish important SIDs from uninformative ones. To tackle these issues, we
propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,
Denoise, and Amplify multimodal information from content and behavior
modalities for semantic IDs learning. The semantic IDs generated by this
framework named ADA-SID. It introduces two innovations: an adaptive
behavior-content alignment that is aware of information richness to shield
representations from noise, and a dynamic behavioral router to amplify critical
signals by applying different weights to SIDs. Extensive experiments on public
and large-scale industrial datasets demonstrate ADA-SID's significant
superiority in both generative and discriminative recommendation tasks.

</details>


### [8] [Retrieval-Augmented Search for Large-Scale Map Collections with ColPali](https://arxiv.org/abs/2510.25718)
*Jamie Mahowald,Benjamin Charles Germain Lee*

Main category: cs.IR

TL;DR: 介绍map-RAS：一个用于历史地图检索增强搜索系统，包含公开演示平台，支持多模态查询和跨集合搜索。


<details>
  <summary>Details</summary>
Motivation: 多模态方法在图书馆、档案馆和博物馆的数字馆藏搜索与导航方面显示出巨大潜力，特别是针对历史地图的检索需求。

Method: 使用ColPali进行多模态查询，Llama 3.2总结搜索结果，支持用户上传自有集合进行跨集合搜索。

Result: 开发了公开演示系统，可搜索美国国会图书馆的101,233张地图图像，为档案管理员、策展人和终端用户提供实用工具。

Conclusion: 该系统在机器学习和数字人文学科领域具有应用前景，为历史地图的搜索和导航提供了创新解决方案。

Abstract: Multimodal approaches have shown great promise for searching and navigating
digital collections held by libraries, archives, and museums. In this paper, we
introduce map-RAS: a retrieval-augmented search system for historic maps. In
addition to introducing our framework, we detail our publicly-hosted demo for
searching 101,233 map images held by the Library of Congress. With our system,
users can multimodally query the map collection via ColPali, summarize search
results using Llama 3.2, and upload their own collections to perform
inter-collection search. We articulate potential use cases for archivists,
curators, and end-users, as well as future work with our system in both machine
learning and the digital humanities. Our demo can be viewed at:
http://www.mapras.com.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [Reviving Thorup's Shortcut Conjecture](https://arxiv.org/abs/2510.24954)
*Aaron Bernstein,Henry Fleischmann,George Z. Li,Bernhard Haeupler,Maximilian Probst Gutenberg,Gary Hoppenworth,Seth Pettie,Thatchaphol Saranurak,Yonggang Jiang,Leon Schiller*

Main category: cs.DS

TL;DR: 本文重新审视Thorup关于可达性捷径的猜想，发现允许添加Steiner顶点后可以打破所有已知的下界限制。作者提出了新的攻击方法，排除了特定厚度限制下的理想捷径，并给出了候选困难实例。


<details>
  <summary>Details</summary>
Motivation: Thorup关于理想大小-直径权衡的可达性捷径猜想虽然被Hesse反驳，但原猜想和后续工作都未考虑允许添加Steiner顶点的情况。本文旨在探索这种扩展后的猜想可能性。

Method: 通过允许在捷径图中添加新的Steiner顶点，作者提出了显式攻击方法来打破已知下界，并分析了捷径的"厚度"限制，排除了特定厚度下的理想捷径存在性。

Result: 1) 允许Steiner顶点时，可以打破所有已知捷径下界；2) 排除了厚度为o(log n/log log n)的理想捷径存在；3) 提出了候选困难实例；4) 展示了在并行算法中的积极应用前景。

Conclusion: 虽然Thorup猜想的原始形式被反驳，但允许Steiner顶点的扩展版本仍有希望。本文为这一研究方向提供了新的见解和工具，并展示了在并行算法设计中的潜在应用价值。

Abstract: We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of
reachability shortcuts with ideal size-diameter tradeoffs. Thorup originally
asked whether, given any graph $G=(V,E)$ with $m$ edges, we can add
$m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$
so that $\text{dist}_{G_+}(u,v) \leq m^{o(1)}$ for all $(u,v)\in E^*$, where
$G_+=(V,E\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03],
followed by significant efforts in the last few years to optimize the lower
bounds.
  In this paper we observe that although Hesse refuted the letter of Thorup's
conjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not
refute the spirit of the conjecture, which should allow $G_+$ to contain both
new (shortcut) edges and new Steiner vertices. Our results are as follows.
  (1) On the positive side, we present explicit attacks that break all known
shortcut lower bounds when Steiner vertices are allowed.
  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size,
$m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\log n/\log \log n)$,
meaning no path can contain $t$ consecutive Steiner vertices.
  (3) We propose a candidate hard instance as the next step toward resolving
the revised version of Thorup's conjecture.
  Finally, we show promising implications. Almost-optimal parallel algorithms
for computing a generalization of the shortcut that approximately preserves
distances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$
depth for exact shortcut paths and exact maximum flow. The state-of-the-art
algorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\v{n}, Haeupler,
Martinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively.

</details>


### [10] [Hedgegraph Polymatroids](https://arxiv.org/abs/2510.25043)
*Karthekeyan Chandrasekaran,Chandra Chekuri,Weihang Wang,Weihao Zhu*

Main category: cs.DS

TL;DR: 本文提出了两种基于划分的连通性度量方法来解决hedgegraphs中割函数非子模性带来的算法挑战，通过引入多拟阵框架获得了新的可解性结果。


<details>
  <summary>Details</summary>
Motivation: Hedgegraphs通过将超边分组到hedge中扩展了超图的表达能力，能够建模超边之间的依赖关系，但其割函数不具有子模性，这阻碍了连通性算法的发展。

Method: 引入两种基于划分的连通性度量方法，研究hedgegraphs中关联的多拟阵结构，而不是直接处理割函数。

Result: 多拟阵框架导出了新的可解性结果，并推广了图和超图中的经典结果。

Conclusion: 通过多拟阵视角，本文克服了hedgegraphs中割函数非子模性的算法障碍，为连通性问题提供了有效的解决方案。

Abstract: Graphs and hypergraphs combine expressive modeling power with algorithmic
efficiency for a wide range of applications. Hedgegraphs generalize hypergraphs
further by grouping hyperedges under a color/hedge. This allows hedgegraphs to
model dependencies between hyperedges and leads to several applications.
However, it poses algorithmic challenges. In particular, the cut function is
not submodular, which has been a barrier to algorithms for connectivity. In
this work, we introduce two alternative partition-based measures of
connectivity in hedgegraphs and study their structural and algorithmic aspects.
Instead of the cut function, we investigate a polymatroid associated with
hedgegraphs. The polymatroidal lens leads to new tractability results as well
as insightful generalizations of classical results on graphs and hypergraphs.

</details>


### [11] [$\{s,t\}$-Separating Principal Partition Sequence of Submodular Functions](https://arxiv.org/abs/2510.25664)
*Kristóf Bérczi,Karthekeyan Chandrasekaran,Tamás Király,Daniel P. Szabo*

Main category: cs.DS

TL;DR: 本文提出了子模函数的{s,t}分离主划分序列理论，证明了其存在性并设计了多项式时间构造算法，应用于{s,t}分离子模k划分问题和超图定向问题。


<details>
  <summary>Details</summary>
Motivation: 受两个应用问题驱动：{s,t}分离子模k划分问题的近似算法和超图定向问题的高效求解。

Method: 定义了{s,t}分离主划分序列，证明其存在性，并设计了多项式时间构造算法。

Result: 成功构建了{s,t}分离主划分序列理论，并应用于两个具体问题：为单调和正模函数提供近似算法，以及解决超图定向问题。

Conclusion: 发展了{s,t}分离主划分序列理论，为子模函数相关优化问题提供了新的理论工具和高效算法。

Abstract: Narayanan and Fujishige showed the existence of the principal partition
sequence of a submodular function, a structure with numerous applications in
areas such as clustering, fast algorithms, and approximation algorithms. In
this work, motivated by two applications, we develop a theory of
$\{s,t\}$-separating principal partition sequence of a submodular function. We
define this sequence, show its existence, and design a polynomial-time
algorithm to construct it. We show two applications: (1) approximation
algorithm for the $\{s,t\}$-separating submodular $k$-partitioning problem for
monotone and posimodular functions and (2) polynomial-time algorithm for the
hypergraph orientation problem of finding an orientation that simultaneously
has strong connectivity at least $k$ and $(s,t)$-connectivity at least $\ell$.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [12] [YTLive: A Dataset of Real-World YouTube Live Streaming Sessions](https://arxiv.org/abs/2510.24769)
*Mojtaba Mozhganfar,Pooya Jamshidi,Seyyed Ali Aghamiri,Mohsen Ghasemi,Mahdi Dolati,Farzad Tashtarian,Ahmad Khonsari,Christian Timmerer*

Main category: cs.MM

TL;DR: YTLive是一个公开的YouTube直播数据集，包含超过50万条记录，追踪了12,156个直播流的实时观众数量变化，填补了直播研究领域缺乏大规模公开数据的空白。


<details>
  <summary>Details</summary>
Motivation: 当前直播研究受限于缺乏大规模公开数据集，无法捕捉实时观众行为模式，因此需要构建一个全面的直播数据集来支持相关研究。

Method: 通过YouTube研究员项目在2024年5-6月收集数据，以5分钟间隔记录并发观众数量，并精确记录广播时长。

Result: 发现周末观众数量更高且更稳定，特别是下午时段；短时长直播吸引更大且更稳定的观众，而长时长直播增长缓慢且变异性更大。

Conclusion: YTLive为直播流媒体研究提供了及时、开放的资源，支持自适应流媒体、资源分配和QoE建模等应用，促进可重复研究和系统级创新。

Abstract: Live streaming plays a major role in today's digital platforms, supporting
entertainment, education, social media, etc. However, research in this field is
limited by the lack of large, publicly available datasets that capture
real-time viewer behavior at scale. To address this gap, we introduce YTLive, a
public dataset focused on YouTube Live. Collected through the YouTube
Researcher Program over May and June 2024, YTLive includes more than 507000
records from 12156 live streams, tracking concurrent viewer counts at
five-minute intervals along with precise broadcast durations. We describe the
dataset design and collection process and present an initial analysis of
temporal viewing patterns. Results show that viewer counts are higher and more
stable on weekends, especially during afternoon hours. Shorter streams attract
larger and more consistent audiences, while longer streams tend to grow slowly
and exhibit greater variability. These insights have direct implications for
adaptive streaming, resource allocation, and Quality of Experience (QoE)
modeling. YTLive offers a timely, open resource to support reproducible
research and system-level innovation in live streaming. The dataset is publicly
available at github.

</details>


### [13] [Hallucination Localization in Video Captioning](https://arxiv.org/abs/2510.25225)
*Shota Nakada,Kazuhiro Saito,Yuchi Ishikawa,Hokuto Munakata,Tatsuya Komatsu,Masayoshi Kondo*

Main category: cs.MM

TL;DR: 提出视频字幕幻觉定位新任务，在词/短语级别识别幻觉，构建HLVC数据集包含1167个视频-字幕对，并建立基于VideoLLM的基线方法进行评测。


<details>
  <summary>Details</summary>
Motivation: 现有句子级幻觉检测任务分析不够细致，需要更细粒度的幻觉定位方法。

Method: 构建HLVC数据集，手动标注视频字幕幻觉，实现基于VideoLLM的基线方法。

Result: 建立了首个视频字幕幻觉定位基准，包含定量和定性评估结果。

Conclusion: 提出了视频字幕幻觉定位任务，为更细致的幻觉分析提供了基准和方法。

Abstract: We propose a novel task, hallucination localization in video captioning,
which aims to identify hallucinations in video captions at the span level (i.e.
individual words or phrases). This allows for a more detailed analysis of
hallucinations compared to existing sentence-level hallucination detection
task. To establish a benchmark for hallucination localization, we construct
HLVC-Dataset, a carefully curated dataset created by manually annotating 1,167
video-caption pairs from VideoLLM-generated captions. We further implement a
VideoLLM-based baseline method and conduct quantitative and qualitative
evaluations to benchmark current performance on hallucination localization.

</details>


### [14] [PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models](https://arxiv.org/abs/2510.25600)
*Zhonghua Jiang,Kunxi Li,Yiyun Zhou,Sihao Liu,Zhaode Wang,Chengfei lv,Shengyu Zhang*

Main category: cs.MM

TL;DR: PureKV是一个即插即用框架，通过联合优化稀疏注意力和KV缓存压缩，解决了VLLMs处理高分辨率输入时的效率问题。该方法实现了5.0倍KV缓存压缩和3.16倍预填充加速，且质量损失可忽略。


<details>
  <summary>Details</summary>
Motivation: VLLMs在处理高分辨率输入时面临效率挑战，注意力机制的二次复杂度和不断增长的KV缓存严重阻碍了预填充和解码阶段。现有方法依赖注意力分数来压缩KV缓存，但与高效注意力机制不兼容，且忽视了稀疏注意力对KV缓存信息结构的影响。

Method: 提出PureKV框架：1）使用低层注意力分数估计高层KV缓存重要性，实现与高效注意力加速器兼容的主动剪枝；2）设计时空稀疏注意力模块（ST-SpAttn），结合空间和时间注意力稀疏性，净化KV缓存中的空间噪声和时间冗余。

Result: 在VLLMs（VideoLLaMA2、Qwen2.5-VL）上的广泛实验表明，PureKV实现了5.0倍KV缓存压缩和3.16倍预填充加速，质量损失可忽略。

Conclusion: PureKV通过联合优化稀疏注意力和KV缓存压缩，有效解决了VLLMs的效率瓶颈，实现了显著的性能提升，同时保持模型质量。

Abstract: Vision-Language Large Models (VLLMs) face significant efficiency challenges
when processing high-resolution inputs. The quadratic complexity in attention
and autoregressive generation, as well as the constantly growing key value (KV)
cache size, severely hinder the prefilling and decoding stages. Recent efforts
have attempted to compress KV cache by identifying and pruning KV cache of less
important tokens, but these methods typically rely on attention scores to
estimate token importance, making them incompatible with efficient attention
mechanisms such as FlashAttention and Sparse Attention, which do not explicitly
compute attention matrices. Moreover, existing methods overlook how sparse
attention, while accelerating the prefilling stage, alters the information
structure of the KV cache, thereby compromising the effectiveness of downstream
KV cache compression strategies. To address this issue, we propose PureKV, a
plug-and-play framework for joint optimization of sparse attention and KV cache
compression. We first introduce a KV cache compression strategy that is fully
compatible with efficient attention accelerators. Our method utilizes lower
layer attention scores to estimate the importance of high layers' KV cache,
enabling active pruning without compromising accuracy. In addition, we have
designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically
tailored for video KV cache compression algorithms. This module combines
spatial and temporal attention sparsity to improve the compression efficiency
of KV cache optimization algorithms by purifying spatial noise and temporal
redundancy in KV cache. At the same time, ST-SpAttn also accelerated the
prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,
Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and
3.16 times prefill acceleration, with negligible quality degradation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [15] [Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications](https://arxiv.org/abs/2510.24763)
*Tingting Huang,Jundong Chen,Huanqiang Zeng,Guofa Cai,Georges Kaddoum*

Main category: cs.IT

TL;DR: 提出了一种基于深度学习的功率域非正交多址混沌移位键控系统，用于解决车载通信中多用户传输的安全性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有车载通信中的多用户混沌通信系统存在频谱效率低、用户连接数有限、计算复杂度高等问题，需要一种更高效、安全的解决方案。

Method: 设计了基于深度神经网络的解调器，采用双域特征提取架构联合处理时域和频域信息，并集成到连续干扰消除框架中。

Result: 系统在频谱效率、能量效率、误码率、安全性和鲁棒性方面表现优异，同时保持较低的计算复杂度。

Conclusion: 所提出的DL-NOMA-CSK系统为安全车载通信提供了实用可行的解决方案。

Abstract: Ensuring secure and efficient multi-user (MU) transmission is critical for
vehicular communication systems. Chaos-based modulation schemes have garnered
considerable interest due to their benefits in physical layer security.
However, most existing MU chaotic communication systems, particularly those
based on non-coherent detection, suffer from low spectral efficiency due to
reference signal transmission, and limited user connectivity under orthogonal
multiple access (OMA). While non-orthogonal schemes, such as sparse code
multiple access (SCMA)-based DCSK, have been explored, they face high
computational complexity and inflexible scalability due to their fixed codebook
designs. This paper proposes a deep learning-assisted power domain
non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for
vehicular communications. A deep neural network (DNN)-based demodulator is
designed to learn intrinsic chaotic signal characteristics during offline
training, thereby eliminating the need for chaotic synchronization or reference
signal transmission. The demodulator employs a dual-domain feature extraction
architecture that jointly processes the time-domain and frequency-domain
information of chaotic signals, enhancing feature learning under dynamic
channels. The DNN is integrated into the successive interference cancellation
(SIC) framework to mitigate error propagation issues. Theoretical analysis and
extensive simulations demonstrate that the proposed system achieves superior
performance in terms of spectral efficiency (SE), energy efficiency (EE), bit
error rate (BER), security, and robustness, while maintaining lower
computational complexity compared to traditional MU-DCSK and existing DL-aided
schemes. These advantages validate its practical viability for secure vehicular
communications.

</details>


### [16] [Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission](https://arxiv.org/abs/2510.25002)
*Zhenyu Liu,Yi Ma,Rahim Tafazolli,Zhi Ding*

Main category: cs.IT

TL;DR: 提出Resi-VidTok框架，通过重要性排序的令牌流实现超低码率视频传输，在恶劣信道条件下保持感知和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 无线网络中实时视频传输在有限带宽和弱连接等恶劣信道条件下仍然极具挑战性，需要解决鲁棒性和质量保真度问题。

Method: 采用弹性1D令牌化管道，结合差分时间令牌编码、步长控制帧稀疏化和轻量级解码器侧插值器，以及信道自适应源信道编码调制方案。

Result: 在低至0.0004的信道带宽比下实现鲁棒的视觉和语义一致性，实时重建速度超过30fps。

Conclusion: Resi-VidTok为能效敏感、延迟敏感和可靠性关键的无线应用提供了实用的视频传输解决方案。

Abstract: Real-time transmission of video over wireless networks remains highly
challenging, even with advanced deep models, particularly under severe channel
conditions such as limited bandwidth and weak connectivity. In this paper, we
propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for
ultra-low-rate and lightweight video transmission that delivers strong
robustness while preserving perceptual and semantic fidelity on commodity
digital hardware. By reorganizing spatio--temporal content into a discrete,
importance-ordered token stream composed of key tokens and refinement tokens,
Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and
graceful quality degradation under constrained channels. A key contribution is
a resilient 1D tokenization pipeline for video that integrates differential
temporal token coding, explicitly supporting reliable recovery from incomplete
token sets using a single shared framewise decoder--without auxiliary temporal
extractors or heavy generative models. Furthermore, stride-controlled frame
sparsification combined with a lightweight decoder-side interpolator reduces
transmission load while maintaining motion continuity. Finally, a
channel-adaptive source--channel coding and modulation scheme dynamically
allocates rate and protection according to token importance and channel
condition, yielding stable quality across adverse SNRs. Evaluation results
indicate robust visual and semantic consistency at channel bandwidth ratios
(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,
demonstrating the practicality of Resi-VidTok for energy-efficient,
latency-sensitive, and reliability-critical wireless applications.

</details>


### [17] [Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder](https://arxiv.org/abs/2510.25181)
*Yixiang Zhou,Tong Wu,Meixia Tao,Jianhua Mo*

Main category: cs.IT

TL;DR: 提出Fed-PELAD联邦学习框架，通过个性化编码器和LoRA适配共享解码器，在保证CSI反馈精度的同时大幅降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中CSI反馈面临的通信开销大、数据异构性和隐私保护等关键挑战。

Method: 采用个性化编码器捕获设备特定特征，共享解码器通过LoRA适配进行全局更新，仅传输紧凑的LoRA参数，并引入交替冻结策略和校准学习率比。

Result: 相比传统方法，上行通信成本降低至42.97%，在异构条件下CSI反馈精度提升1.2 dB。

Conclusion: Fed-PELAD框架在保证性能的同时有效解决了通信开销和隐私问题，适用于大规模MIMO系统的CSI反馈任务。

Abstract: This paper addresses the critical challenges of communication overhead, data
heterogeneity, and privacy in deep learning for channel state information (CSI)
feedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel
federated learning framework that incorporates personalized encoders and a
LoRA-adapted shared decoder. Specifically, personalized encoders are trained
locally on each user equipment (UE) to capture device-specific channel
characteristics, while a shared decoder is updated globally via the
coordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This
design ensures that only compact LoRA adapter parameters instead of full model
updates are transmitted for aggregation. To further enhance convergence
stability, we introduce an alternating freezing strategy with calibrated
learning-rate ratio during LoRA aggregation. Extensive simulations on
3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\%
of the uplink communication cost compared to conventional methods while
achieving a performance gain of 1.2 dB in CSI feedback accuracy under
heterogeneous conditions.

</details>


### [18] [Joint Spatial Registration and Resource Allocation for Transmissive RIS Enabled Cooperative ISCC Networks](https://arxiv.org/abs/2510.25266)
*Ziwei Liu,Wen Chen,Zhendong Li,Qiong Wu*

Main category: cs.IT

TL;DR: 提出了一种新型透射式可重构智能表面(TRIS)收发器驱动的协作集成感知、计算和通信(ISCC)网络，通过联合优化波束成形、时隙分配、感知数据分配和感知波束调度，最小化网络总能耗。


<details>
  <summary>Details</summary>
Motivation: 为了满足多样化网络需求并降低能耗，需要设计一种能够同时完成感知数据采集、通信卸载和计算的协作网络架构。

Method: 采用信号级空间配准算法调整波束宽度，引入多流通信和秩-N约束，使用迭代秩最小化(IRM)方案，并通过块坐标下降(BCD)方案解决非凸优化问题。

Result: 数值仿真结果证实了所提方案在提高整体网络性能和降低网络总能耗方面的优越性。

Conclusion: 提出的TRIS收发器驱动的协作ISCC网络能够有效降低能耗并提升网络性能，为未来多样化网络需求提供了可行解决方案。

Abstract: In this paper, we propose a novel transmissive reconfigurable intelligent
surface (TRIS) transceiver-driven cooperative integrated sensing, computing,
and communication (ISCC) network to meet the requirement for a diverse network
with low energy consumption. The cooperative base stations (BSs) are equipped
with TRIS transceivers to accomplish sensing data acquisition, communication
offloading, and computation in a time slot. In order to obtain higher
cooperation gain, we utilize a signal-level spatial registration algorithm,
which is realized by adjusting the beamwidth. Meanwhile, for more efficient
offloading of the computational task, multistream communication is considered,
and rank-$N$ constraints are introduced, which are handled using an iterative
rank minimization (IRM) scheme. We construct an optimization problem with the
objective function of minimizing the total energy consumption of the network to
jointly optimize the beamforming matrix, time slot allocation, sensing data
allocation and sensing beam scheduling variables. Due to the coupling of the
variables, the proposed problem is a non-convex optimization problem, which we
decouple and solve using a block coordinate descent (BCD) scheme. Finally,
numerical simulation results confirm the superiority of the proposed scheme in
improving the overall network performance and reducing the total energy
consumption of the network.

</details>


### [19] [General Coverage Models: Structure, Monotonicity, and Shotgun Sequencing](https://arxiv.org/abs/2510.25305)
*Yitzchak Grunbaum,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 本文研究了覆盖过程，其中每次抽取显示[n]的子集，目标是确定所有项目至少被看到一次的期望抽取次数。引入了固定长度连续窗口模型，开发了组合工具将覆盖时间计算转化为计数问题，获得了精确表达式和渐近分析。


<details>
  <summary>Details</summary>
Motivation: 受霰弹枪DNA测序的启发，研究每次抽取显示连续窗口的覆盖过程，推广经典的优惠券收集问题。

Method: 开发了统一的组合工具，将覆盖时间计算从概率问题转化为对包含所有项目的[n]子集族的计数问题。分析了循环和非循环窗口模型，以及均匀ℓ-正则模型。

Result: 获得了窗口模型的精确表达式，分析了渐近行为。证明了均匀ℓ-正则模型的通用上界，发现许多采样模型具有相同的主导渐近阶。

Conclusion: 开发了强大的组合工具来精确计算覆盖时间，证明了均匀ℓ-正则模型的通用上界，揭示了不同采样模型在渐近行为上的相似性。

Abstract: We study coverage processes in which each draw reveals a subset of $[n]$, and
the goal is to determine the expected number of draws until all items are seen
at least once. A classical example is the Coupon Collector's Problem, where
each draw reveals exactly one item. Motivated by shotgun DNA sequencing, we
introduce a model where each draw is a contiguous window of fixed length, in
both cyclic and non-cyclic variants. We develop a unifying combinatorial tool
that shifts the task of finding coverage time from probability, to a counting
problem over families of subsets of $[n]$ that together contain all items,
enabling exact calculation. Using this result, we obtain exact expressions for
the window models. We then leverage past results on a continuous analogue of
the cyclic window model to analyze the asymptotic behavior of both models. We
further study what we call uniform $\ell$-regular models, where every draw has
size $\ell$ and every item appears in the same number of admissible draws. We
compare these to the batch sampling model, in which all $\ell$-subsets are
drawn uniformly at random and present upper and lower bounds, which were also
obtained independently by Berend and Sher. We conjecture, and prove for special
cases, that this model maximizes the coverage time among all uniform
$\ell$-regular models. Finally, we prove a universal upper bound on the entire
class of uniform $\ell$-regular models, which illuminates the fact that many
sampling models share the same leading asymptotic order, while potentially
differing significantly in lower-order terms.

</details>


### [20] [Network Oblivious Transfer via Noisy Broadcast Channels](https://arxiv.org/abs/2510.25343)
*Hadi Aghaee,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 本文研究了通过离散无记忆广播信道实现信息论不经意传输，分析了非共谋和共谋两种用户模型，建立了容量区域的上界，并提出了两种具体协议。


<details>
  <summary>Details</summary>
Motivation: 将网络信息论与密码学安全相结合，探索噪声广播信道作为多用户隐私保护通信原语的潜力。

Method: 提出了两种不经意传输协议：第一种针对非共谋接收者利用二进制擦除广播信道结构；第二种针对共谋接收者引入熵共享和隐私放大机制。

Result: 对于非共谋情况，不经意传输容量的上下界一致，完整刻画了可达区域。

Conclusion: 为网络信息论和密码学安全提供了一个统一的理论框架，展示了噪声广播信道在多用户隐私保护通信中的强大潜力。

Abstract: This paper investigates information-theoretic oblivious transfer via a
discrete memoryless broadcast channel with one sender and two receivers. We
analyze both non-colluding and colluding honest-but-curious user models and
establish general upper bounds on the achievable oblivious transfer capacity
region for each case. Two explicit oblivious transfer protocols are proposed.
The first ensures correctness and privacy for independent, non-colluding
receivers by leveraging the structure of binary erasure broadcast channels. The
second protocol, secure even under receiver collusion, introduces additional
entropy-sharing and privacy amplification mechanisms to preserve secrecy
despite information leakage between users. Our results show that for the
non-colluding case, the upper and lower bounds on oblivious transfer capacity
coincide, providing a complete characterization of the achievable region. The
work provides a unified theoretical framework bridging network information
theory and cryptographic security, highlighting the potential of noisy
broadcast channels as powerful primitives for multi-user privacy-preserving
communication.

</details>


### [21] [Joint Beamforming Design and Resource Allocation for IRS-Assisted Full-Duplex Terahertz Systems](https://arxiv.org/abs/2510.25346)
*Chi Qiu,Wen Chen,Qingqing Wu,Fen Hou,Wanming Hao,Ruiqi Liu,Derrick Wing Kwan Ng*

Main category: cs.IT

TL;DR: 提出了一种IRS辅助FD THz通信系统的联合资源分配框架，通过优化IRS反射相位、上下行功率控制、子带带宽分配和子带分配，最大化用户加权最小速率，确保服务质量公平性。


<details>
  <summary>Details</summary>
Motivation: 解决THz频段严重传播损耗、频率相关分子吸收以及FD通信中强残余自干扰等实际部署挑战，满足未来无线网络对超高数据速率和频谱效率的需求。

Method: 开发了两种计算高效算法：一种采用等子带带宽分配简化优化，另一种引入自适应带宽分配以进一步提升频谱利用率和系统灵活性。

Result: 仿真结果验证了所提设计的有效性，表明该方案在频谱效率方面相比基准方案实现了显著提升。

Conclusion: 所提出的联合资源分配框架能够有效平衡系统性能与计算复杂度，为IRS辅助FD THz通信系统的实际部署提供了可行解决方案。

Abstract: Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz
(THz) communication systems have emerged as a promising paradigm to satisfy the
escalating demand for ultra-high data rates and spectral efficiency in future
wireless networks. However, the practical deployment of such systems presents
unique technical challenges, stemming from severe propagation loss,
frequency-dependent molecular absorption in the THz band, and the presence of
strong residual self-interference (SI) inherent to FD communications. To tackle
these issues, this paper proposes a joint resource allocation framework that
aims to maximize the weighted minimum rate among all users, thereby ensuring
fairness in quality of service. Specifically, the proposed design jointly
optimizes IRS reflecting phase shifts, uplink/downlink transmit power control,
sub-band bandwidth allocation, and sub-band assignment, explicitly capturing
the unique propagation characteristics of THz channels and the impact of
residual SI. To strike an balance between system performance and computational
complexity, two computationally efficient algorithms are developed under
distinct spectrum partitioning schemes: one assumes equal sub-band bandwidth
allocation to facilliate tractable optimization, while the other introduces
adaptive bandwidth allocation to further enhance spectral utilization and
system flexibility. Simulation results validate the effectiveness of the
proposed designs and demonstrate that the adopted scheme achieves significant
spectral efficiency improvements over benchmark schemes.

</details>


### [22] [AirCNN via Reconfigurable Intelligent Surfaces: Architecture Design and Implementation](https://arxiv.org/abs/2510.25389)
*Meng Hua,Haotian Wu,Deniz Gündüz*

Main category: cs.IT

TL;DR: AirCNN是一种通过空中模拟计算实现卷积神经网络的新范式，利用可重构智能表面(RIS)和收发器设计，通过无线传播环境模拟CNN层操作。


<details>
  <summary>Details</summary>
Motivation: 利用无线环境模拟CNN计算，减少传统数字计算的开销，通过OTA计算实现高效的神经网络推理。

Method: 提出两种RIS辅助传输架构：MIMO和MISO，用于实现2D卷积和深度可分离卷积。联合优化发射预编码器、接收合并器和RIS相位偏移等参数。

Result: 仿真结果显示AirCNN架构能获得满意的分类性能。Conv2d MISO在各种设置下始终优于Conv2d MIMO，而ConvSD中MISO仅在信道条件差时更优。多RIS配置显著提升性能，特别是在视距主导环境中。

Conclusion: AirCNN证明了通过无线环境模拟CNN计算的可行性，为未来无线通信与人工智能的深度融合提供了新方向。

Abstract: This paper introduces AirCNN, a novel paradigm for implementing convolutional
neural networks (CNNs) via over-the-air (OTA) analog computation. By leveraging
multiple reconfigurable intelligent surfaces (RISs) and transceiver designs, we
engineer the ambient wireless propagation environment to emulate the operations
of a CNN layer. To comprehensively evaluate AirCNN, we consider two types of
CNNs, namely classic two-dimensional (2D) convolution (Conv2d) and light-weight
convolution, i.e., depthwise separable convolution (ConvSD). For Conv2d
realization via OTA computation, we propose and analyze two RIS-aided
transmission architectures: multiple-input multiple-output (MIMO) and
multiple-input single-output (MISO), balancing transmission overhead and
emulation performance. We jointly optimize all parameters, including the
transmitter precoder, receiver combiner, and RIS phase shifts, under practical
constraints such as transmit power budget and unit-modulus phase shift
requirements. We further extend the framework to ConvSD, which requires
distinct transmission strategies for depthwise and pointwise convolutions.
Simulation results demonstrate that the proposed AirCNN architectures can
achieve satisfactory classification performance. Notably, Conv2d MISO
consistently outperforms Conv2d MIMO across various settings, while for ConvSD,
MISO is superior only under poor channel conditions. Moreover, employing
multiple RISs significantly enhances performance compared to a single RIS,
especially in line-of-sight (LoS)-dominated wireless environments.

</details>


### [23] [Several classes of $p$-ary linear codes with few-weights derived from Weil sums](https://arxiv.org/abs/2510.25578)
*Mrinal Kanti Bose,Abhay Kumar Singh*

Main category: cs.IT

TL;DR: 本文提出了几类新的少重量线性码，通过两个特定定义集构造了5类4重量码、1类2重量码，以及使用弱正则弯曲函数构造了2类6重量码、2类8重量码和1类9重量码。


<details>
  <summary>Details</summary>
Motivation: 少重量线性码在编码理论中具有重要研究价值，在秘密共享方案、认证码、关联方案和强正则图等领域有广泛应用。受Cheng和Gao以及Wu、Li和Zeng工作的启发，本文旨在构造新的少重量线性码。

Method: 通过选择两个特定的定义集来构造线性码，第一个定义集产生4重量和2重量码，第二个定义集结合弱正则弯曲函数产生6重量、8重量和9重量码。使用有限域上的Weil和进行详细计算来确定参数和重量分布。

Result: 成功构造了多种少重量线性码：5类4重量码、1类2重量码、2类6重量码、2类8重量码和1类9重量码。其中一类2重量码达到Griesmer界，是最优码。

Conclusion: 本文提出的方法能够有效构造多种少重量线性码，其中包含达到Griesmer界的最优码，为少重量线性码的研究提供了新的构造途径。

Abstract: Linear codes with few weights have been a significant area of research in
coding theory for many years, due to their applications in secret sharing
schemes, authentication codes, association schemes, and strongly regular
graphs. Inspired by the works of Cheng and Gao \cite{P8} and Wu, Li and Zeng
\cite{P12}, in this paper, we propose several new classes of few-weight linear
codes over the finite field $\mathbb{F}_{p}$ through the selection of two
specific defining sets. Consequently, we obtain five classes of $4$-weight
linear codes and one class of $2$-weight linear codes from our first defining
set. Furthermore, by employing weakly regular bent functions in our second
defining set, we derive two classes of $6$-weight codes, two classes of
$8$-weight codes, and one class of $9$-weight codes. The parameters and weight
distributions of all these constructed codes are wholly determined by detailed
calculations on certain Weil sums over finite fields. In addition, we identify
an optimal class of $2$-weight codes that meet the Griesmer bound.

</details>


### [24] [On Multidimensional 2-Weight-Limited Burst-Correcting Codes](https://arxiv.org/abs/2510.25592)
*Hagai Berend,Ohad Elishco,Moshe Schwartz*

Main category: cs.IT

TL;DR: 该论文研究了能够纠正最多2个错误的突发错误的多维码，重点分析了三种不同的错误位置限制情况，并提供了显式编码构造，同时与证明的下界进行了冗余度比较。


<details>
  <summary>Details</summary>
Motivation: 研究多维码在纠正最多2个突发错误时的性能，特别关注错误位置之间的不同距离限制条件，旨在设计高效的纠错编码方案。

Method: 针对三种不同的错误位置限制（L∞距离有界、L1距离有界、轴平行线上距离有界），提供了显式的编码构造方法。

Result: 为每种错误位置限制情况都设计了具体的编码方案，并分析了这些编码方案的冗余度与理论下界的比较结果。

Conclusion: 论文成功构建了针对不同错误位置限制的多维纠错码，并通过冗余度分析验证了所提编码方案的有效性。

Abstract: We consider multidimensional codes capable of correcting a burst error of
weight at most $2$. When two positions are in error, the burst limits their
relative position. We study three such limitations: the $L_\infty$ distance
between the positions is bounded, the $L_1$ distance between the positions is
bounded, or the two positions are on an axis-parallel line with bounded
distance between them. In all cases we provide explicit code constructions, and
compare their excess redundancy to a lower bound we prove.

</details>


### [25] [Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems](https://arxiv.org/abs/2510.25736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文研究了图复制数据库中的对称私有信息检索(SPIR)问题，其中数据库复制由简单图建模，服务器通过边连接共享消息副本。通过开发将PIR方案转换为SPIR方案的算法，建立了容量下界，并对路径图和循环图给出了更紧的上界，特别确定了三顶点路径图的SPIR容量为1/2。


<details>
  <summary>Details</summary>
Motivation: 研究图复制数据库设置中的SPIR问题，旨在量化相比图复制公共随机性设置下SPIR容量的改进，即期望符号数与下载符号数的最大比值。

Method: 开发了一种算法，将一类PIR方案转换为相应的SPIR方案，从而为存在此类方案的图建立容量下界。该方法适用于路径图和循环图等图类。

Result: 为路径图和循环图推导出了比各自PIR容量给出的平凡界限更紧的容量上界。特别地，对于三顶点路径图，确定了SPIR容量为1/2。

Conclusion: 通过提出的转换算法，成功建立了图复制数据库设置中SPIR的容量下界，并对特定图类给出了改进的容量界限，为图复制SPIR问题提供了新的理论结果。

Abstract: We revisit the problem of symmetric private information retrieval (SPIR) in
settings where the database replication is modeled by a simple graph. Here,
each vertex corresponds to a server, and a message is replicated on two servers
if and only if there is an edge between them. To satisfy the requirement of
database privacy, we let all the servers share some common randomness,
independent of the messages. We aim to quantify the improvement in SPIR
capacity, i.e., the maximum ratio of the number of desired and downloaded
symbols, compared to the setting with graph-replicated common randomness.
Towards this, we develop an algorithm to convert a class of PIR schemes into
the corresponding SPIR schemes, thereby establishing a capacity lower bound on
graphs for which such schemes exist. This includes the class of path and cyclic
graphs for which we derive capacity upper bounds that are tighter than the
trivial bounds given by the respective PIR capacities. For the special case of
path graph with three vertices, we identify the SPIR capacity to be
$\frac{1}{2}$.

</details>


### [26] [A mathematical study of the excess growth rate](https://arxiv.org/abs/2510.25740)
*Steven Campbell,Ting-Kam Leonard Wong*

Main category: cs.IT

TL;DR: 该论文从信息论角度研究投资组合理论中的超额增长率，将其与Rényi熵、交叉熵、Helmholtz自由能量等概念联系起来，并提供了三个公理化特征定理。


<details>
  <summary>Details</summary>
Motivation: 研究超额增长率在投资组合理论中的基础作用，探索信息论与量化金融之间的新联系。

Method: 通过公理化方法，从相对熵、Jensen不等式差距和广义Bregman散度三个角度对超额增长率进行特征化。

Result: 建立了超额增长率与信息论概念的系统联系，提供了三个特征定理，并研究了超额增长率最大化问题。

Conclusion: 研究不仅为超额增长率的重要性提供了理论依据，还在信息论与量化金融之间建立了新的联系。

Abstract: We study the excess growth rate -- a fundamental logarithmic functional
arising in portfolio theory -- from the perspective of information theory. We
show that the excess growth rate can be connected to the R\'{e}nyi and cross
entropies, the Helmholtz free energy, L. Campbell's measure of average code
length and large deviations. Our main results consist of three axiomatic
characterization theorems of the excess growth rate, in terms of (i) the
relative entropy, (ii) the gap in Jensen's inequality, and (iii) the
logarithmic divergence that generalizes the Bregman divergence. Furthermore, we
study maximization of the excess growth rate and compare it with the growth
optimal portfolio. Our results not only provide theoretical justifications of
the significance of the excess growth rate, but also establish new connections
between information theory and quantitative finance.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [27] [What Are People's Actual Utility Functions in Budget Aggregation?](https://arxiv.org/abs/2510.24872)
*Ayelet Amster,Lioz Akirav,Rica Gonen,Erel Segal-Halevi*

Main category: cs.GT

TL;DR: 通过实证研究发现，传统预算聚合机制中常用的效用函数模型（如ℓ₁、ℓ₂和Leontief）对人类真实偏好的解释力有限，但发现了对星形偏好和峰值线性效用函数的支持证据。


<details>
  <summary>Details</summary>
Motivation: 验证预算聚合机制中常用的效用函数假设是否准确反映人类真实偏好，因为现有理论缺乏实证证据支持。

Method: 通过结构化民意调查测试人类参与者对不同预算分配的偏好，检验其是否符合常见效用函数模型。

Result: 大多数参与者在不同度量比较中表现出不一致模式，传统假设如项目对称性和符号对称性缺乏实证支持，但发现了对星形偏好和峰值线性效用函数的强烈支持。

Conclusion: 需要替代建模方法，本研究为预算聚合理论提供了系统性的实证测试方法，有助于设计更稳健和现实的机制。

Abstract: While participatory budgeting and budget-aggregation mechanisms require
assumptions about how voters evaluate non-ideal budget allocations, little
empirical evidence exists to validate which utility models accurately capture
human preferences. We conducted structured polls with human participants to
test whether real people's preferences conform to commonly assumed utility
functions such as $\ell_1$, $\ell_2$ and Leontief. Our results suggest that
these models may have limited explanatory power for actual behavior: most
participants showed inconsistent patterns across different metric comparisons,
and standard assumptions of project symmetry and sign symmetry -- core features
of common distance-based metrics -- received little empirical support. However,
we find encouraging evidence for more fundamental preference structures: a
large majority of participants showed consistency with star-shaped preferences,
as well as with peak-linear utility functions, where utility changes
proportionally with distance from the ideal budget. These findings have
important implications for designers of budget aggregation mechanisms. While
theoretical results demonstrate impossibility results for standard distance
metrics regarding truthfulness, Pareto-efficiency, and proportionality, our
evidence suggests alternative modeling approaches may be warranted. More
broadly, this work introduces a systematic methodology to empirically test the
utility function assumptions that underpin budget aggregation theories, paving
the way for more robust and realistic mechanism design.

</details>


### [28] [Fair Indivisible Payoffs through Shapley Value](https://arxiv.org/abs/2510.24906)
*Mikołaj Czarnecki,Michał Korniak,Oskar Skibski,Piotr Skowron*

Main category: cs.GT

TL;DR: 提出了一种在不可分割联盟博弈中公平分配不可分割对象的方法，定义了不可分割Shapley值并研究其性质，通过三个案例研究验证方法，包括在图像分类任务中识别关键区域。


<details>
  <summary>Details</summary>
Motivation: 解决不可分割联盟博弈中的收益分配问题，其中大联盟的价值是自然数，代表不可分割对象（如议会席位、肾脏交换或机器学习模型中的关键特征）的数量分配问题。

Method: 定义了不可分割Shapley值，并研究其数学性质，通过三个案例研究来验证该方法的有效性。

Result: 成功提出了不可分割Shapley值的概念，并在图像分类任务等应用中展示了该方法能够识别关键区域。

Conclusion: 不可分割Shapley值为不可分割对象的公平分配提供了一种有效的数学框架，在多个实际应用中具有实用价值。

Abstract: We consider the problem of payoff division in indivisible coalitional games,
where the value of the grand coalition is a natural number. This number
represents a certain quantity of indivisible objects, such as parliamentary
seats, kidney exchanges, or top features contributing to the outcome of a
machine learning model. The goal of this paper is to propose a fair method for
dividing these objects among players. To achieve this, we define the
indivisible Shapley value and study its properties. We demonstrate our proposed
technique using three case studies, in particular, we use it to identify key
regions of an image in the context of an image classification task.

</details>


### [29] [Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games](https://arxiv.org/abs/2510.25080)
*Will Wolf*

Main category: cs.GT

TL;DR: 该论文介绍了有界单边响应游戏(BORGs)这一新的游戏结构，使用改良版Monopoly Deal作为基准环境，证明CFR算法能有效处理这种动态，并提供了一个完整的研究平台。


<details>
  <summary>Details</summary>
Motivation: 探索游戏中较少研究但策略丰富的有界单边响应结构，这种动态发生在玩家行动短暂转移控制权给对手时，对手必须通过一个或多个连续移动满足固定条件。

Method: 引入改良版Monopoly Deal作为基准环境，使用反事实遗憾最小化(CFR)算法，并开发了一个轻量级全栈研究平台，集成了环境、并行化CFR运行时和可玩网页界面。

Result: CFR算法成功收敛到该领域的有效策略，无需新的算法扩展。研究平台支持高效、可重复的实验，可在单台工作站上运行。

Conclusion: 有界单边响应游戏是策略丰富的结构，CFR能有效处理，提供的研究平台为探索状态表示和策略学习奠定了实用基础。

Abstract: Card games are widely used to study sequential decision-making under
uncertainty, with real-world analogues in negotiation, finance, and
cybersecurity. Typically, these games fall into three categories based on the
flow of control: strictly-sequential (where players alternate single actions),
deterministic-response (where some actions trigger a fixed outcome), and
unbounded reciprocal-response (where alternating counterplays are permitted). A
less-explored but strategically rich structure exists: the bounded one-sided
response. This dynamic occurs when a player's action briefly transfers control
to the opponent, who must satisfy a fixed condition through one or more
sequential moves before the turn resolves. We term games featuring this
mechanism Bounded One-Sided Response Games (BORGs).
  We introduce a modified version of Monopoly Deal as a benchmark environment
that specifically isolates the BORG dynamic, where a Rent action forces the
opponent to sequentially choose payment assets. We demonstrate that the
gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully
converges on effective strategies for this domain without requiring novel
algorithmic extensions. To support efficient, reproducible experimentation, we
present a lightweight, full-stack research platform that unifies the
environment, a parallelized CFR runtime, and a human-playable web interface,
all runnable on a single workstation. This system provides a practical
foundation for exploring state representation and policy learning in bounded
one-sided response settings.
  The trained CFR agent and source code are available at
https://monopolydeal.ai.

</details>


### [30] [Timing Games in Responsive Consensus Protocols](https://arxiv.org/abs/2510.25144)
*Kaya Alpturer,Kushal Babel,Aditya Saraf*

Main category: cs.GT

TL;DR: 该论文分析了区块链共识协议中的时间博弈问题，提出了动态区块奖励机制来激励验证者更快地提出区块，从而在存在自私延迟行为的情况下实现响应性。


<details>
  <summary>Details</summary>
Motivation: 区块链应用激励验证者通过策略性延迟提案来玩时间博弈，因为增加区块时间与更高奖励相关。这使得即使在乐观条件下，响应性在区块链协议中似乎不可能实现。

Method: 开发了响应性共识协议中的时间博弈模型，引入动态区块奖励（随轮次时间减少），并通过投票机制测量延迟，让验证者协调达到合作均衡。

Result: 通过精心设置协议参数，投票机制允许验证者协调并达到合作均衡，通过更高的奖励率使所有人受益。响应性本身可以促进更快的区块提案。

Conclusion: 从静态到动态区块奖励的转变使验证者效用对延迟更敏感，但在理论延迟模型和基于真实世界网络的模拟中，这种影响是轻微的。响应性可以在区块链协议中实现，而不是由于时间博弈而无法达到的属性。

Abstract: Optimistic responsiveness -- the ability of a consensus protocol to operate
at the speed of the network -- is widely used in consensus protocol design to
optimize latency and throughput. However, blockchain applications incentivize
validators to play timing games by strategically delaying their proposals,
since increased block time correlates with greater rewards. Consequently, it
may appear that responsiveness (even under optimistic conditions) is impossible
in blockchain protocols. In this work, we develop a model of timing games in
responsive consensus protocols and find a prisoner's dilemma structure, where
cooperation (proposing promptly) is in the validators' best interest, but
individual incentives encourage validators to delay proposals selfishly. To
attain desirable equilibria, we introduce dynamic block rewards that decrease
with round time to explicitly incentivize faster proposals. Delays are measured
through a voting mechanism, where other validators vote on the current leader's
round time. By carefully setting the protocol parameters, the voting mechanism
allows validators to coordinate and reach the cooperative equilibrium,
benefiting all through a higher rate-of-reward. Thus, instead of responsiveness
being an unattainable property due to timing games, we show that responsiveness
itself can promote faster block proposals. One consequence of moving from a
static to dynamic block reward is that validator utilities become more
sensitive to latency, worsening the gap between the best- and worst-connected
validators. Our analysis shows, however, that this effect is minor in both
theoretical latency models and simulations based on real-world networks.

</details>


### [31] [On Robust Popular Matchings with Tie-Bounded Preferences and Stable Matchings with Two-Sided Ties](https://arxiv.org/abs/2510.25209)
*Koustav De*

Main category: cs.GT

TL;DR: 该论文研究了二分图中稳健流行匹配的存在性问题，分别在单边模型和双边单边平局模型中分析了当只有一个代理改变偏好顺序时，是否存在稳健流行匹配，并提出了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究流行匹配在多个实例中保持稳健性的条件，特别是当实例之间只有单个代理的偏好顺序不同时，探索是否存在始终受欢迎的匹配。

Method: 在单边模型中分析单个代理改变偏好顺序的情况；在双边单边平局模型中给出流行匹配的简单特征描述；针对两种模型都提出了多项式时间算法来判定稳健流行匹配的存在性。

Result: 证明了在双边单边平局模型中，当输入实例仅因单个代理的偏好顺序不同时，存在多项式时间算法可以判定稳健流行匹配的存在性；同时给出了流行匹配的简单特征描述。

Conclusion: 论文成功解决了在特定偏好模型下稳健流行匹配的存在性判定问题，为处理多实例匹配问题提供了有效的算法工具。

Abstract: We are given a bipartite graph $G = \left( A \cup B, E \right)$. In the
one-sided model, every $a \in A$ (often called agents) ranks its neighbours $z
\in N_{a}$ strictly, and no $b \in B$ has any preference order over its
neighbours $y \in N_{b}$, and vertices in $B$ abstain from casting their votes
to matchings. In the two-sided model with one-sided ties, every $a \in A$ ranks
its neighbours $z \in N_{a}$ strictly, and every $b \in B$ puts all of its
neighbours into a single large tie, i.e., $b \in B$ prefers every $y \in N_{b}$
equally. In this two-sided model with one-sided ties, when two matchings
compete in a majority election, $b \in B$ abstains from casting its vote for a
matching when both the matchings saturate $b$ or both leave $b$ unsaturated;
else $b$ prefers the matching where it is saturated. A popular matching $M$ is
\emph{robust} if it remains popular among multiple instances.
  We have analysed the cases when a robust popular matching exists in the
one-sided model where only one agent alters her preference order among the
instances, and we have proposed a polynomial-time algorithm to decide if there
exists a robust popular matching when instances differ only with respect to the
preference orders of a single agent.
  We give a simple characterisation of popular matchings in the two-sided model
with one-sided ties. We show that in the two-sided model with one-sided ties,
if the input instances differ only with respect to the preference orders of a
single agent, there is a polynomial-time algorithm to decide whether there
exists a robust popular matching. We have been able to decide the stable
matching problem in bipartite graphs $G = (A \cup B, E)$ where \textit{both}
sides have weak preferences (ties allowed), with the restriction that every tie
has length at most $k$.

</details>


### [32] [Learning-Augmented Online Bidding in Stochastic Settings](https://arxiv.org/abs/2510.25582)
*Spyros Angelopoulos,Bertrand Simon*

Main category: cs.GT

TL;DR: 本文研究学习增强环境下的在线竞价问题，探索了分布预测和随机算法的能力与限制。


<details>
  <summary>Details</summary>
Motivation: 在线竞价是经典优化问题，在在线决策、可中断系统设计和近似算法分析中有广泛应用。现有研究主要关注不利用预测质量随机信息的预言机和确定性算法。

Method: 第一部分研究基于分布预测的竞价，寻找帕累托最优算法；第二部分研究随机竞价算法的能力与限制，给出一致性/鲁棒性权衡的上下界。

Result: 提出了在算法一致性和鲁棒性之间提供最佳可能权衡的帕累托最优算法，并建立了随机算法在一致性/鲁棒性权衡方面的上下界。

Conclusion: 学习增强设置下的在线竞价研究扩展了传统方法，通过引入随机性在预测预言机或算法本身，提供了更优的性能权衡。

Abstract: Online bidding is a classic optimization problem, with several applications
in online decision-making, the design of interruptible systems, and the
analysis of approximation algorithms. In this work, we study online bidding
under learning-augmented settings that incorporate stochasticity, in either the
prediction oracle or the algorithm itself. In the first part, we study bidding
under distributional predictions, and find Pareto-optimal algorithms that offer
the best-possible tradeoff between the consistency and the robustness of the
algorithm. In the second part, we study the power and limitations of randomized
bidding algorithms, by presenting upper and lower bounds on the
consistency/robustness tradeoffs. Previous works focused predominantly on
oracles that do not leverage stochastic information on the quality of the
prediction, and deterministic algorithms.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [33] [ODataX: A Progressive Evolution of the Open Data Protocol](https://arxiv.org/abs/2510.24761)
*Anirudh Ganesh,Nitin Sood*

Main category: cs.DB

TL;DR: ODataX是OData协议的演进版本，通过简化查询语法、内置性能防护机制和增强缓存来解决OData采用障碍，保持向后兼容性。


<details>
  <summary>Details</summary>
Motivation: OData协议虽然功能强大，但主要局限于企业环境，特别是微软和SAP生态系统，需要解决阻碍其广泛采用的关键障碍。

Method: 开发ODataX协议，在保持OData v4向后兼容的同时，引入渐进式复杂性披露、简化查询语法、基于查询成本估算的性能防护机制和增强缓存机制。

Result: ODataX协议成功解决了OData的采用障碍，在保持企业级查询标准化的同时满足了现代Web开发对简单性的需求。

Conclusion: ODataX有效弥合了企业级查询标准化与现代Web开发简单性需求之间的差距，为OData协议的更广泛采用提供了可行路径。

Abstract: The Open Data Protocol (OData) provides a standardized approach for building
and consuming RESTful APIs with rich query capabilities. Despite its power and
maturity, OData adoption remains confined primarily to enterprise environments,
particularly within Microsoft and SAP ecosystems. This paper analyzes the key
barriers preventing wider OData adoption and introduces ODataX, an evolved
version of the protocol designed to address these limitations. ODataX maintains
backward compatibility with OData v4 while introducing progressive complexity
disclosure through simplified query syntax, built-in performance guardrails via
query cost estimation, and enhanced caching mechanisms. This work aims to
bridge the gap between enterprise-grade query standardization and the
simplicity demanded by modern web development practices.

</details>


### [34] [StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems](https://arxiv.org/abs/2510.25017)
*Qi Lin,Zhenyu Zhang,Viraj Thakkar,Zhenjie Sun,Mai Zheng,Zhichao Cao*

Main category: cs.DB

TL;DR: StorageXTuner是一个基于LLM代理的异构存储引擎自动调优框架，通过四个代理分离关注点，使用洞察驱动的树搜索和分层内存来提升存储系统配置性能。


<details>
  <summary>Details</summary>
Motivation: 存储系统自动配置困难，参数空间大且条件多变，现有启发式和ML调优器通常系统特定、需要手动调整且在变化时性能下降。

Method: 设计四个代理：Executor（沙盒基准测试）、Extractor（性能摘要）、Searcher（洞察引导的配置探索）、Reflector（洞察生成和管理），结合洞察驱动树搜索和分层内存。

Result: 在RocksDB、LevelDB、CacheLib和MySQL InnoDB上测试，相比默认设置和ELMo-Tune，吞吐量最高提升575%和111%，p99延迟最多降低88%和56%，收敛所需试验次数更少。

Conclusion: StorageXTuner框架能够有效自动调优异构存储引擎，显著提升性能并减少收敛时间。

Abstract: Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.

</details>


### [35] [Time-varying Vector Field Compression with Preserved Critical Point Trajectories](https://arxiv.org/abs/2510.25143)
*Mingze Xia,Yuxiao Li,Pu Jiao,Bei Wang,Xin Liang,Hanqi Guo*

Main category: cs.DB

TL;DR: 提出一种高效的有损压缩框架，能够精确保留时变向量场中的所有临界点轨迹，解决了现有压缩方法会破坏这一关键特征的问题。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和观测产生大量时变向量场数据，存储和传输困难。有损压缩是减少数据量的有效方法，但现有方法会扭曲临界点轨迹这一关键特征。

Method: 扩展了空间临界点保留理论到时空临界点轨迹保留，开发了压缩框架；提出半拉格朗日预测器利用平流主导区域的时空相关性，结合传统Lorenzo预测器提高压缩效率。

Result: 在四个真实科学数据集上测试，该方法实现了高达124.48倍的压缩比，同时有效保留所有临界点轨迹，比最佳无损压缩器高出56.07倍。

Conclusion: 该方法在保持高压缩比的同时，能够完全保留临界点轨迹，优于现有有损和无损压缩方法。

Abstract: Scientific simulations and observations are producing vast amounts of
time-varying vector field data, making it hard to store them for archival
purposes and transmit them for analysis. Lossy compression is considered a
promising approach to reducing these data because lossless compression yields
low compression ratios that barely mitigate the problem. However, directly
applying existing lossy compression methods to timevarying vector fields may
introduce undesired distortions in critical-point trajectories, a crucial
feature that encodes key properties of the vector field. In this work, we
propose an efficient lossy compression framework that exactly preserves all
critical-point trajectories in time-varying vector fields. Our contributions
are threefold. First, we extend the theory for preserving critical points in
space to preserving critical-point trajectories in space-time, and develop a
compression framework to realize the functionality. Second, we propose a
semi-Lagrange predictor to exploit the spatiotemporal correlations in
advectiondominated regions, and combine it with the traditional Lorenzo
predictor for improved compression efficiency. Third, we evaluate our method
against state-of-the-art lossy and lossless compressors using four real-world
scientific datasets. Experimental results demonstrate that the proposed method
delivers up to 124.48X compression ratios while effectively preserving all
critical-point trajectories. This compression ratio is up to 56.07X higher than
that of the best lossless compressors, and none of the existing lossy
compressors can preserve all critical-point trajectories at similar compression
ratios.

</details>


### [36] [DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries](https://arxiv.org/abs/2510.25401)
*Jiahao Lou,Quan Yu,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 提出了一种解耦存储架构来改进基于图的近似最近邻搜索系统，通过三阶段查询机制和增量页面级拓扑重排序策略，在保持查询性能的同时显著提升更新效率。


<details>
  <summary>Details</summary>
Motivation: 传统耦合存储方法在索引更新时效率低下，会产生大量冗余向量读写和无效I/O操作，需要解决这一瓶颈问题。

Method: 采用解耦存储架构，设计三阶段查询机制利用多级PQ压缩向量过滤无效I/O和计算，以及增量页面级拓扑重排序策略将新节点插入到包含最相似邻居的页面中。

Result: 解耦架构使插入速度提升10.05倍，删除速度提升6.89倍；三阶段查询和增量重排序使查询效率相比传统耦合架构提升2.66倍。

Conclusion: 解耦存储架构结合优化的查询和重排序策略，能够同时显著提升ANN搜索系统的更新效率和查询性能。

Abstract: On-disk graph-based indexes are widely used in approximate nearest neighbor
(ANN) search systems for large-scale, high-dimensional vectors. However,
traditional coupled storage methods, which store vectors within the index, are
inefficient for index updates. Coupled storage incurs excessive redundant
vector reads and writes when updating the graph topology, leading to
significant invalid I/O. To address this issue, we propose a decoupled storage
architecture. While a decoupled architecture reduces query performance. To
overcome this limitation, we design two tailored strategies: (i) a three-stage
query mechanism that leverages multiple PQ compressed vectors to filter invalid
I/O and computations, and (ii) an incremental page-level topological reordering
strategy that incrementally inserts new nodes into pages containing their most
similar neighbors to mitigate read amplification. Together, these techniques
substantially reduce both I/O and computational overhead during ANN search.
Experimental results show that the decoupled architecture improves update speed
by 10.05x for insertions and 6.89x for deletions, while the three-stage query
and incremental reordering enhance query efficiency by 2.66x compared to the
traditional coupled architecture.

</details>


### [37] [One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans](https://arxiv.org/abs/2510.25684)
*Yujun He,Hangdong Zhao,Simon Frisk,Yifei Yang,Kevin Kristensen,Paraschos Koutris,Xiangyao Yu*

Main category: cs.DB

TL;DR: SplitJoin框架通过引入split作为一等查询操作符，将输入表分区为重和轻部分，允许不同数据分区使用不同的查询计划，以减少中间结果大小。


<details>
  <summary>Details</summary>
Motivation: 最小化中间结果对于多连接查询处理至关重要。虽然Yannakakis算法为无环查询提供了强保证，但环状查询仍然是一个开放挑战。

Method: 提出SplitJoin框架，通过分区输入表为重和轻部分，让不同数据分区使用不同的查询计划，利用现有二元连接引擎减少中间结果大小。系统探索基于分割的优化设计空间，包括阈值选择、分割策略和分割后的连接排序。

Result: 在DuckDB上，SplitJoin完成43个社交网络查询（原生29个），平均运行时间快2.1倍，中间结果小7.9倍（最大分别达13.6倍和74倍）；在Umbra上，完成45个查询（原生35个），平均速度提升1.3倍，中间结果小1.2倍（最大分别达6.1倍和2.1倍）。

Conclusion: SplitJoin框架通过引入split操作符和分区策略，有效减少了多连接查询中的中间结果大小，在现有数据库系统上实现了显著的性能提升。

Abstract: Minimizing intermediate results is critical for efficient multi-join query
processing. Although the seminal Yannakakis algorithm offers strong guarantees
for acyclic queries, cyclic queries remain an open challenge. In this paper, we
propose SplitJoin, a framework that introduces split as a first-class query
operator. By partitioning input tables into heavy and light parts, SplitJoin
allows different data partitions to use distinct query plans, with the goal of
reducing intermediate sizes using existing binary join engines. We
systematically explore the design space for split-based optimizations,
including threshold selection, split strategies, and join ordering after
splits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves
substantial improvements: on DuckDB, SplitJoin completes 43 social network
queries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller
intermediates on average (up to 13.6x and 74x, respectively); on Umbra, it
completes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller
intermediates on average (up to 6.1x and 2.1x, respectively).

</details>
