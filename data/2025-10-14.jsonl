{"id": "2510.09989", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09989", "abs": "https://arxiv.org/abs/2510.09989", "authors": ["Xuyang Sun", "Hussein A. Ammar", "Israfil Bahceci", "Raviraj Adve", "Gary Boudreau", "Zehua Li"], "title": "Remote Interference Mitigation through Null Precoding and Fractional Programming", "comment": null, "summary": "With the rapid deployment of 5G systems, remote interference (RI) caused by\natmospheric ducting has emerged as an occasional, but critical challenge. This\nphenomenon occurs when the downlink (DL) signals from distant base stations\n(BSs) propagate over long distances through tropospheric ducting, severely\ndisrupting uplink (UL) reception at local BSs. To address this challenge, we\nanalyze the effect of RI on network performance, including the channel\nestimation phase. We then develop a solution that identifies the\nangle-of-arrival (AOA) estimation of RI and designs precoders and combiners\nthat mitigate RI. Our approach employs interference cancellation techniques\nthrough null precoding and fractional programming which enhance the performance\nof the network. Interestingly, we show that using our scheme, uplink\ncommunication is possible at low transmit power regimes that were unusable due\nto RI. Our results further show a 5.23~dB reduction in normalized mean square\nerror for channel estimation and achieved data rates around 5.8~bit/s/Hz at the\npreviously unusable low uplink transmit power conditions."}
{"id": "2510.10190", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10190", "abs": "https://arxiv.org/abs/2510.10190", "authors": ["Sina Beyraghi", "Javad Shabanpour", "Giovanni Geraci", "Paul Almasan", "Angel Lozano"], "title": "Data-Driven Deployment of Reconfigurable Intelligent Surfaces in Cellular Networks", "comment": null, "summary": "This paper presents a fully automated, data-driven framework for the\nlarge-scale deployment of reconfigurable intelligent surfaces (RISs) in\ncellular networks. Leveraging physically consistent ray tracing and empirical\ndata from a commercial deployment in the UK, the proposed method jointly\noptimizes RIS placement, orientation, configuration, and base station\nbeamforming in dense urban environments across frequency bands (corresponding\nto 4G, 5G, and a hypothetical 6G system). Candidate RIS locations are\nidentified via reflection- and scattering-based heuristics using calibrated\nelectromagnetic models within the Sionna Ray Tracing (RT) engine. Outage users\nare clustered to reduce deployment complexity, and the tradeoff between\ncoverage gains and infrastructure cost is systematically evaluated. It is shown\nthat achieving meaningful coverage improvement in urban areas requires a dense\ndeployment of large-aperture RIS units, raising questions about\ncost-effectiveness. To facilitate reproducibility and future research, the\ncomplete simulation framework and RIS deployment algorithms are provided as\nopen-source software."}
{"id": "2510.10316", "categories": ["cs.IT", "cs.CR", "math.IT", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.10316", "abs": "https://arxiv.org/abs/2510.10316", "authors": ["Anand D. Sarwate", "Flavio P. Calmon", "Oliver Kosut", "Lalitha Sankar"], "title": "An information theorist's tour of differential privacy", "comment": "16 pages, 8 figures, under review at BITS, the Information Theory\n  Magazine", "summary": "Since being proposed in 2006, differential privacy has become a standard\nmethod for quantifying certain risks in publishing or sharing analyses of\nsensitive data. At its heart, differential privacy measures risk in terms of\nthe differences between probability distributions, which is a central topic in\ninformation theory. A differentially private algorithm is a channel between the\nunderlying data and the output of the analysis. Seen in this way, the\nguarantees made by differential privacy can be understood in terms of\nproperties of this channel. In this article we examine a few of the key\nconnections between information theory and the formulation/application of\ndifferential privacy, giving an ``operational significance'' for relevant\ninformation measures."}
{"id": "2510.10429", "categories": ["cs.IT", "math.AC", "math.CO", "math.IT", "Primary: 94A60, 13P10, Secondary: 05E40, 14M25"], "pdf": "https://arxiv.org/pdf/2510.10429", "abs": "https://arxiv.org/abs/2510.10429", "authors": ["Sergio Da Silva", "Aniya Stewart"], "title": "Quantum-Resistant Cryptography via Universal Gröbner Bases", "comment": "26 pages", "summary": "In this article, we explore the use of universal Gr\\\"obner bases in\npublic-key cryptography by proposing a key establishment protocol that is\nresistant to quantum attacks. By utilizing a universal Gr\\\"obner basis\n$\\mathcal{U}_I$ of a polynomial ideal $I$ as a private key, this protocol\nleverages the computational disparity between generating the universal\nGr\\\"obner basis needed for decryption compared with the single Gr\\\"obner basis\nused for encryption. The security of the system lies in the difficulty of\ndirectly computing the Gr\\\"obner fan of $I$ required to construct\n$\\mathcal{U}_I$. We provide an analysis of the security of the protocol and the\ncomplexity of its various parameters. Additionally, we provide efficient ways\nto recursively generate $\\mathcal{U}_I$ for toric ideals of graphs with\ntechniques which are also of independent interest to the study of these ideals."}
{"id": "2510.11447", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.11447", "abs": "https://arxiv.org/abs/2510.11447", "authors": ["Mizuki Takenawa", "Naoki Sugimoto", "Leslie Wöhler", "Satoshi Ikehata", "Kiyoharu Aizawa"], "title": "Building and Evaluating a Realistic Virtual World for Large Scale Urban Exploration from 360° Videos", "comment": "Multimedia Tools and Applications, Springer (accepted)", "summary": "We propose to build realistic virtual worlds, called 360RVW, for large urban\nenvironments directly from 360{\\deg} videos. We provide an interface for\ninteractive exploration, where users can freely navigate via their own avatars.\n360{\\deg} videos record the entire environment of the shooting location\nsimultaneously leading to highly realistic and immersive representations. Our\nsystem uses 360{\\deg} videos recorded along streets and builds a 360RVW through\nfour main operations: video segmentation by intersection detection, video\ncompletion to remove the videographer, semantic segmentation for virtual\ncollision detection with the avatar, and projection onto a distorted sphere\nthat moves along the camera trajectory following the avatar's movements. Our\ninterface allows users to explore large urban environments by changing their\nwalking direction at intersections or choosing a new location by clicking on a\nmap. Even without a 3D model, the users can experience collision with buildings\nusing metadata produced by semantic segmentation. Furthermore, we stream the\n360{\\deg} videos so users can directly access 360RVW via their web browser. We\nfully evaluate our system, including a perceptual experiment comparing our\napproach to previous exploratory interfaces. The results confirm the quality of\nour system, especially regarding the presence of users and the interactive\nexploration, making it most suitable for a virtual tour of urban environments."}
{"id": "2510.09857", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09857", "abs": "https://arxiv.org/abs/2510.09857", "authors": ["Xiao Yang", "Peifeng Yin", "Abe Engle", "Jinfeng Zhuang", "Ling Leng"], "title": "MTMD: A Multi-Task Multi-Domain Framework for Unified Ad Lightweight Ranking at Pinterest", "comment": "AdKDD 2025", "summary": "The lightweight ad ranking layer, living after the retrieval stage and before\nthe fine ranker, plays a critical role in the success of a cascaded ad\nrecommendation system. Due to the fact that there are multiple optimization\ntasks depending on the ad domain, e.g., Click Through Rate (CTR) for click ads\nand Conversion Rate (CVR) for conversion ads, as well as multiple surfaces\nwhere an ad is served (home feed, search, or related item recommendation) with\ndiverse ad products (shopping or standard ad); it is an essentially challenging\nproblem in industry on how to do joint holistic optimization in the lightweight\nranker, such that the overall platform's value, advertiser's value, and user's\nvalue are maximized.\n  Deep Neural Network (DNN)-based multitask learning (MTL) can handle multiple\ngoals naturally, with each prediction head mapping to a particular optimization\ngoal. However, in practice, it is unclear how to unify data from different\nsurfaces and ad products into a single model. It is critical to learn\ndomain-specialized knowledge and explicitly transfer knowledge between domains\nto make MTL effective. We present a Multi-Task Multi-Domain (MTMD) architecture\nunder the classic Two-Tower paradigm, with the following key contributions: 1)\nhandle different prediction tasks, ad products, and ad serving surfaces in a\nunified framework; 2) propose a novel mixture-of-expert architecture to learn\nboth specialized knowledge each domain and common knowledge shared between\ndomains; 3) propose a domain adaption module to encourage knowledge transfer\nbetween experts; 4) constrain the modeling of different prediction tasks. MTMD\nimproves the offline loss value by 12% to 36%, mapping to 2% online reduction\nin cost per click. We have deployed this single MTMD framework into production\nfor Pinterest ad recommendation replacing 9 production models."}
{"id": "2510.09799", "categories": ["cs.DS", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09799", "abs": "https://arxiv.org/abs/2510.09799", "authors": ["Alessio Maritan", "Luca Schenato"], "title": "Distributed clustering in partially overlapping feature spaces", "comment": null, "summary": "We introduce and address a novel distributed clustering problem where each\nparticipant has a private dataset containing only a subset of all available\nfeatures, and some features are included in multiple datasets. This scenario\noccurs in many real-world applications, such as in healthcare, where different\ninstitutions have complementary data on similar patients. We propose two\ndifferent algorithms suitable for solving distributed clustering problems that\nexhibit this type of feature space heterogeneity. The first is a federated\nalgorithm in which participants collaboratively update a set of global\ncentroids. The second is a one-shot algorithm in which participants share a\nstatistical parametrization of their local clusters with the central server,\nwho generates and merges synthetic proxy datasets. In both cases, participants\nperform local clustering using algorithms of their choice, which provides\nflexibility and personalized computational costs. Pretending that local\ndatasets result from splitting and masking an initial centralized dataset, we\nidentify some conditions under which the proposed algorithms are expected to\nconverge to the optimal centralized solution. Finally, we test the practical\nperformance of the algorithms on three public datasets."}
{"id": "2510.09814", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.09814", "abs": "https://arxiv.org/abs/2510.09814", "authors": ["Emile Martinez", "Felipe Garrido-Lucero", "Umberto Grandi"], "title": "Stability in Online Assignment Games", "comment": null, "summary": "The assignment game models a housing market where buyers and sellers are\nmatched, and transaction prices are set so that the resulting allocation is\nstable. Shapley and Shubik showed that every stable allocation is necessarily\nbuilt on a maximum social welfare matching. In practice, however, stable\nallocations are rarely attainable, as matchings are often sub-optimal,\nparticularly in online settings where eagents arrive sequentially to the\nmarket. In this paper, we introduce and compare two complementary measures of\ninstability for allocations with sub-optimal matchings, establish their\nconnections to the optimality ratio of the underlying matching, and use this\nframework to study the stability performances of randomized algorithms in\nonline assignment games."}
{"id": "2510.09646", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09646", "abs": "https://arxiv.org/abs/2510.09646", "authors": ["Ritesh Chandra", "Sonali Agarwal", "Navjot Singh"], "title": "Real-Time Health Analytics Using Ontology-Driven Complex Event Processing and LLM Reasoning: A Tuberculosis Case Study", "comment": "14 table. 20 figure", "summary": "Timely detection of critical health conditions remains a major challenge in\npublic health analytics, especially in Big Data environments characterized by\nhigh volume, rapid velocity, and diverse variety of clinical data. This study\npresents an ontology-enabled real-time analytics framework that integrates\nComplex Event Processing (CEP) and Large Language Models (LLMs) to enable\nintelligent health event detection and semantic reasoning over heterogeneous,\nhigh-velocity health data streams. The architecture leverages the Basic Formal\nOntology (BFO) and Semantic Web Rule Language (SWRL) to model diagnostic rules\nand domain knowledge. Patient data is ingested and processed using Apache Kafka\nand Spark Streaming, where CEP engines detect clinically significant event\npatterns. LLMs support adaptive reasoning, event interpretation, and ontology\nrefinement. Clinical information is semantically structured as Resource\nDescription Framework (RDF) triples in Graph DB, enabling SPARQL-based querying\nand knowledge-driven decision support. The framework is evaluated using a\ndataset of 1,000 Tuberculosis (TB) patients as a use case, demonstrating\nlow-latency event detection, scalable reasoning, and high model performance (in\nterms of precision, recall, and F1-score). These results validate the system's\npotential for generalizable, real-time health analytics in complex Big Data\nscenarios."}
{"id": "2510.10568", "categories": ["cs.IT", "math.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10568", "abs": "https://arxiv.org/abs/2510.10568", "authors": ["Hua Sun", "Syed A. Jafar"], "title": "On the Capacity of Distributed Quantum Storage", "comment": null, "summary": "A distributed quantum storage code maps a quantum message to N storage nodes,\nof arbitrary specified sizes, such that the stored message is robust to an\narbitrary specified set of erasure patterns. The sizes of the storage nodes,\nand erasure patterns may not be homogeneous. The capacity of distributed\nquantum storage is the maximum feasible size of the quantum message (relative\nto the sizes of the storage nodes), when the scaling of the size of the message\nand all storage nodes by the same scaling factor is allowed. Representing the\ndecoding sets as hyperedges in a storage graph, the capacity is characterized\nfor various graphs, including MDS graph, wheel graph, Fano graph, and\nintersection graph. The achievability is related via quantum CSS codes to a\nclassical secure storage problem. Remarkably, our coding schemes utilize\nnon-trivial alignment structures to ensure recovery and security in the\ncorresponding classical secure storage problem, which leads to similarly\nnon-trivial quantum codes. The converse is based on quantum information\ninequalities, e.g., strong sub-additivity and weak monotonicity of quantum\nentropy, tailored to the topology of the storage graphs."}
{"id": "2510.09897", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09897", "abs": "https://arxiv.org/abs/2510.09897", "authors": ["Wonbin Kweon", "Runchu Tian", "SeongKu Kang", "Pengcheng Jiang", "Zhiyong Lu", "Jiawei Han", "Hwanjo Yu"], "title": "PairSem: LLM-Guided Pairwise Semantic Matching for Scientific Document Retrieval", "comment": null, "summary": "Scientific document retrieval is a critical task for enabling knowledge\ndiscovery and supporting research across diverse domains. However, existing\ndense retrieval methods often struggle to capture fine-grained scientific\nconcepts in texts due to their reliance on holistic embeddings and limited\ndomain understanding. Recent approaches leverage large language models (LLMs)\nto extract fine-grained semantic entities and enhance semantic matching, but\nthey typically treat entities as independent fragments, overlooking the\nmulti-faceted nature of scientific concepts. To address this limitation, we\npropose Pairwise Semantic Matching (PairSem), a framework that represents\nrelevant semantics as entity-aspect pairs, capturing complex, multi-faceted\nscientific concepts. PairSem is unsupervised, base retriever-agnostic, and\nplug-and-play, enabling precise and context-aware matching without requiring\nquery-document labels or entity annotations. Extensive experiments on multiple\ndatasets and retrievers demonstrate that PairSem significantly improves\nretrieval performance, highlighting the importance of modeling multi-aspect\nsemantics in scientific information retrieval."}
{"id": "2510.10039", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.10039", "abs": "https://arxiv.org/abs/2510.10039", "authors": ["Enze Sun", "Zhihao Gavin Tang", "Yifan Wang"], "title": "Combinatorial Philosopher Inequalities", "comment": null, "summary": "In online combinatorial allocation, agents arrive sequentially and items are\nallocated in an online manner. The algorithm designer only knows the\ndistribution of each agent's valuation, while the actual realization of the\nvaluation is revealed only upon her arrival. Against the offline benchmark,\nFeldman, Gravin, and Lucier (SODA 2015) designed an optimal $0.5$-competitive\nalgorithm for XOS agents. An emerging line of work focuses on designing\napproximation algorithms against the (computationally unbounded) optimal online\nalgorithm. The primary goal is to design algorithms with approximation ratios\nstrictly greater than $0.5$, surpassing the impossibility result against the\noffline optimum. Positive results are established for unit-demand agents\n(Papadimitriou, Pollner, Saberi, Wajc, MOR 2024), and for $k$-demand agents\n(Braun, Kesselheim, Pollner, Saberi, EC 2024).\n  In this paper, we extend the existing positive results for agents with\nsubmodular valuations by establishing a $0.5 + \\Omega(1)$ approximation against\na newly constructed online configuration LP relaxation for the combinatorial\nallocation setting. Meanwhile, we provide negative results for agents with XOS\nvaluations by providing a $0.5$ integrality gap for the online configuration\nLP, showing an obstacle of existing approaches."}
{"id": "2510.10335", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10335", "abs": "https://arxiv.org/abs/2510.10335", "authors": ["Jugal Garg", "Eklavya Sharma", "Xiaowei Wu"], "title": "Proportional and Pareto-Optimal Allocation of Chores with Subsidy", "comment": null, "summary": "We consider the problem of allocating $m$ indivisible chores among $n$ agents\nwith possibly different weights, aiming for a solution that is both fair and\nefficient. Specifically, we focus on the classic fairness notion of\nproportionality and efficiency notion of Pareto-optimality. Since proportional\nallocations may not always exist in this setting, we allow the use of subsidies\n(monetary compensation to agents) to ensure agents are\nproportionally-satisfied, and aim to minimize the total subsidy required. Wu\nand Zhou (WINE 2024) showed that when each chore has disutility at most 1, a\ntotal subsidy of at most $n/3 - 1/6$ is sufficient to guarantee\nproportionality. However, their approach is based on a complex technique, which\ndoes not guarantee economic efficiency - a key desideratum in fair division.\n  In this work, we give a polynomial-time algorithm that achieves the same\nsubsidy bound while also ensuring Pareto-optimality. Moreover, both our\nalgorithm and its analysis are significantly simpler than those of Wu and Zhou\n(WINE 2024). Our approach first computes a proportionally-fair competitive\nequilibrium, and then applies a rounding procedure guided by\nminimum-pain-per-buck edges."}
{"id": "2510.10115", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10115", "abs": "https://arxiv.org/abs/2510.10115", "authors": ["Kai Cao", "Yucong Duan", "Wensheng Gan"], "title": "Targeted Sequential Pattern Mining with High Average Utility", "comment": "preprint, 9 figures, 3 tables", "summary": "Incorporating utility into targeted pattern mining can address the practical\nlimitations of traditional frequency-based approaches. However, utility-based\nmethods often suffer from generating a large number of long and complicated\nsequences. To improve pattern relevance and interpretability, average utility\nprovides a more balanced metric by considering both utility and sequence\nlength. Moreover, incorporating user-defined query targets into the mining\nprocess enhances usability and interactivity by retaining only patterns\ncontaining user-specified goals. To address challenges related to mining\nefficiency in large-scale, long-sequence datasets, this study introduces\naverage utility into targeted sequential pattern mining. A novel algorithm,\nTAUSQ-PG, is designed to find targeted high average utility sequential\npatterns. It incorporates efficient filtering and pruning strategies, tighter\nupper bound models, as well as novel specialized evaluation metrics and query\nflags tailored to this task. Extensive comparative experiments on different\ndatasets demonstrate that TAUSQ-PG effectively controls the candidate set size,\nthereby reducing redundant sequence generation and significantly improving\nruntime and memory efficiency."}
{"id": "2510.10674", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10674", "abs": "https://arxiv.org/abs/2510.10674", "authors": ["Marco Origlia", "Marco Secondini"], "title": "Soft-Decoding Reverse Reconciliation in Discrete-Modulation CV-QKD", "comment": "Submitted to IEEE Transactions on Communications", "summary": "In continuous-variable quantum key distribution, information reconciliation\nis required to extract a shared secret key from correlated random variables\nobtained through the quantum channel. Reverse reconciliation (RR) is generally\npreferred, since the eavesdropper has less information about Bob's measurements\nthan about Alice's transmitted symbols. When discrete modulation formats are\nemployed, however, soft information is available only at Bob's side, while\nAlice has access only to hard information (her transmitted sequence). This\nforces her to rely on hard-decision decoding to recover Bob's key.\n  In this work, we introduce a novel RR technique for PAM (and QAM) in which\nBob discloses a carefully designed soft metric to help Alice recover Bob's key,\nwhile leaking no additional information about the key to an eavesdropper. We\nassess the performance of the proposed technique in terms of achievable secret\nkey rate (SKR) and its bounds, showing that the achievable SKR closely\napproaches the upper bound, with a significant gain over hard-decision RR.\nFinally, we implement the scheme at the coded level using binary LDPC codes\nwith belief-propagation decoding, assess its bit-error rate through numerical\nsimulations, compare the observed gain with theoretical predictions from the\nachievable SKR, and discuss the residual gap."}
{"id": "2510.10095", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10095", "abs": "https://arxiv.org/abs/2510.10095", "authors": ["Peiyuan Gong", "Feiran Zhu", "Yaqi Yin", "Chenglei Dai", "Chao Zhang", "Kai Zheng", "Wentian Bao", "Jiaxin Mao", "Yi Zhang"], "title": "CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms", "comment": null, "summary": "Short-video platforms have rapidly become a new generation of information\nretrieval systems, where users formulate queries to access desired videos.\nHowever, user queries, especially long-tail ones, often suffer from spelling\nerrors, incomplete phrasing, and ambiguous intent, resulting in mismatches\nbetween user expectations and retrieved results. While large language models\n(LLMs) have shown success in long-tail query rewriting within e-commerce, they\nstruggle on short-video platforms, where proprietary content such as short\nvideos, live streams, micro dramas, and user social networks falls outside\ntheir training distribution. To address this challenge, we introduce\n\\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific\nknowledge to enhance long-tail query rewriting. For each query, our method\naggregates multi-source knowledge relevant to the query and summarizes it into\nan informative and query-relevant knowledge card. This card then guides the LLM\nto better capture user intent and produce more effective query rewrites. We\noptimize CardRewriter using a two-stage training pipeline: supervised\nfine-tuning followed by group relative policy optimization, with a tailored\nreward system balancing query relevance and retrieval effectiveness. Offline\nexperiments show that CardRewriter substantially improves rewriting quality for\nqueries targeting proprietary content. Online A/B testing further confirms\nsignificant gains in long-view rate (LVR) and click-through rate (CTR), along\nwith a notable reduction in initiative query reformulation rate (IQRR). Since\nSeptember 2025, CardRewriter has been deployed on Kuaishou, one of China's\nlargest short-video platforms, serving hundreds of millions of users daily."}
{"id": "2510.10227", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.10227", "abs": "https://arxiv.org/abs/2510.10227", "authors": ["Greg Bodwin", "Bernhard Haeupler", "D Ellis Hershkowitz", "Zihan Tan"], "title": "Simple Length-Constrained Expander Decompositions", "comment": "@SOSA 2026", "summary": "Length-constrained expander decompositions are a new graph decomposition that\nhas led to several recent breakthroughs in fast graph algorithms. Roughly, an\n$(h, s)$-length $\\phi$-expander decomposition is a small collection of length\nincreases to a graph so that nodes within distance $h$ can route flow over\npaths of length $hs$ while using each edge to an extent at most $1/\\phi$. Prior\nwork showed that every $n$-node and $m$-edge graph admits an $(h, s)$-length\n$\\phi$-expander decomposition of size $\\log n \\cdot s n^{O(1/s)} \\cdot \\phi m$.\n  In this work, we give a simple proof of the existence of $(h, s)$-length\n$\\phi$-expander decompositions with an improved size of $s n^{O(1/s)}\\cdot \\phi\nm$. Our proof is a straightforward application of the fact that the union of\nsparse length-constrained cuts is itself a sparse length-constrained cut. In\nderiving our result, we improve the loss in sparsity when taking the union of\nsparse length-constrained cuts from $\\log ^3 n\\cdot s^3 n^{O(1/s)}$ to $s\\cdot\nn^{O(1/s)}$."}
{"id": "2510.10423", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10423", "abs": "https://arxiv.org/abs/2510.10423", "authors": ["Ehsan Heidari", "Alireza Kaviani", "Masoud Seddighin", "AmirMohammad Shahrezaei"], "title": "Improved Maximin Share Guarantee for Additive Valuations", "comment": null, "summary": "The maximin share ($\\textsf{MMS}$) is the most prominent share-based fairness\nnotion in the fair allocation of indivisible goods. Recent years have seen\nsignificant efforts to improve the approximation guarantees for $\\textsf{MMS}$\nfor different valuation classes, particularly for additive valuations. For the\nadditive setting, it has been shown that for some instances, no allocation can\nguarantee a factor better than $1-\\tfrac{1}{n^4}$ of maximin share value to all\nagents. However, the best currently known algorithm achieves an approximation\nguarantee of $\\tfrac{3}{4} + \\tfrac{3}{3836}$ for $\\textsf{MMS}$. In this work,\nwe narrow this gap and improve the best-known approximation guarantee for\n$\\textsf{MMS}$ to $\\tfrac{10}{13}$."}
{"id": "2510.10123", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10123", "abs": "https://arxiv.org/abs/2510.10123", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet", "Yong Zhang"], "title": "The Hybrid Multimodal Graph Index (HMGI): A Comprehensive Framework for Integrated Relational and Vector Search", "comment": null, "summary": "The proliferation of complex, multimodal datasets has exposed a critical gap\nbetween the capabilities of specialized vector databases and traditional graph\ndatabases. While vector databases excel at semantic similarity search, they\nlack the capacity for deep relational querying. Conversely, graph databases\nmaster complex traversals but are not natively optimized for high-dimensional\nvector search. This paper introduces the Hybrid Multimodal Graph Index (HMGI),\na novel framework designed to bridge this gap by creating a unified system for\nefficient, hybrid queries on multimodal data. HMGI leverages the native graph\ndatabase architecture and integrated vector search capabilities, exemplified by\nplatforms like Neo4j, to combine Approximate Nearest Neighbor Search (ANNS)\nwith expressive graph traversal queries. Key innovations of the HMGI framework\ninclude modality-aware partitioning of embeddings to optimize index structure\nand query performance, and a system for adaptive, low-overhead index updates to\nsupport dynamic data ingestion, drawing inspiration from the architectural\nprinciples of systems like TigerVector. By integrating semantic similarity\nsearch directly with relational context, HMGI aims to outperform pure vector\ndatabases like Milvus in complex, relationship-heavy query scenarios and\nachieve sub-linear query times for hybrid tasks."}
{"id": "2510.10944", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10944", "abs": "https://arxiv.org/abs/2510.10944", "authors": ["Xiaoming Shi", "Yunli Li", "Xiaodan Shao", "Jie Xu", "Rui Zhang"], "title": "Throughput Maximization for Multiuser Communications with Flexible-Sector 6DMA", "comment": null, "summary": "This paper presents a cost-effective and easily-deployable flexible-sector\nsix-dimensional movable antenna (6DMA) architecture for future wireless\ncommunication networks, which enables flexible antenna configurations to match\nusers' spatial distribution for capacity enhancement. Different from\nconventional sectorized base station (BS) with fixed-position antennas (FPAs),\nthe flexible-sector 6DMA-enabled BS employs multiple directional sector antenna\narrays that can flexibly move along a common circular track. By properly moving\nantennas across sectors and rotating all sector antenna arrays synchronously,\nthe flexible-sector BS can adjust the coverage regions of all sectors with\nflexible antenna allocations over them. In particular, we consider the\nmultiuser downlink communication employing the orthogonal multiple access (OMA)\nto serve users in each sector. Under this setup, we jointly optimize the sector\nrotation and the antenna allocation at the flexible-sector BS to maximize the\naverage common throughput achievable for all users based on their spatial\ndistribution. We solve this non-convex optimization problem by deriving\nclosed-form solutions and thereby analyze the effect of users' spatial\ndistribution on the achievable common throughput. It is shown that equal user\ndistribution over sectors is optimal for maximizing the common throughput.\nMotivated by this result, we further develop a low-complexity suboptimal\nsolution for the sector rotation that minimizes the variance of user numbers\nacross sectors. Finally, we provide simulation results to verify our analytical\nresults and validate the performance of our proposed solutions. It is\ndemonstrated that the flexible-sector BS significantly improves the network\nthroughput as compared to other benchmark schemes."}
{"id": "2510.10109", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10109", "abs": "https://arxiv.org/abs/2510.10109", "authors": ["Shuangquan Lyu", "Ming Wang", "Huajun Zhang", "Jiasen Zheng", "Junjiang Lin", "Xiaoxuan Sun"], "title": "Integrating Structure-Aware Attention and Knowledge Graphs in Explainable Recommendation Systems", "comment": null, "summary": "This paper designs and implements an explainable recommendation model that\nintegrates knowledge graphs with structure-aware attention mechanisms. The\nmodel is built on graph neural networks and incorporates a multi-hop neighbor\naggregation strategy. By integrating the structural information of knowledge\ngraphs and dynamically assigning importance to different neighbors through an\nattention mechanism, the model enhances its ability to capture implicit\npreference relationships. In the proposed method, users and items are embedded\ninto a unified graph structure. Multi-level semantic paths are constructed\nbased on entities and relations in the knowledge graph to extract richer\ncontextual information. During the rating prediction phase, recommendations are\ngenerated through the interaction between user and target item representations.\nThe model is optimized using a binary cross-entropy loss function. Experiments\nconducted on the Amazon Books dataset validate the superior performance of the\nproposed model across various evaluation metrics. The model also shows good\nconvergence and stability. These results further demonstrate the effectiveness\nand practicality of structure-aware attention mechanisms in knowledge\ngraph-enhanced recommendation."}
{"id": "2510.10431", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.10431", "abs": "https://arxiv.org/abs/2510.10431", "authors": ["Xue Chen", "Shengtang Huang", "Xin Li"], "title": "Explicit Min-wise Hash Families with Optimal Size", "comment": null, "summary": "We study explicit constructions of min-wise hash families and their extension\nto $k$-min-wise hash families. Informally, a min-wise hash family guarantees\nthat for any fixed subset $X\\subseteq[N]$, every element in $X$ has an equal\nchance to have the smallest value among all elements in $X$; a $k$-min-wise\nhash family guarantees this for every subset of size $k$ in $X$. Min-wise hash\nis widely used in many areas of computer science such as sketching, web page\ndetection, and $\\ell_0$ sampling.\n  The classical works by Indyk and P\\u{a}tra\\c{s}cu and Thorup have shown\n$\\Theta(\\log(1/\\delta))$-wise independent families give min-wise hash of\nmultiplicative (relative) error $\\delta$, resulting in a construction with\n$\\Theta(\\log(1/\\delta)\\log N)$ random bits. Based on a reduction from\npseudorandom generators for combinatorial rectangles by Saks, Srinivasan, Zhou\nand Zuckerman, Gopolan and Yehudayoff improved the number of bits to $O(\\log\nN\\log\\log N)$ for polynomially small errors $\\delta$. However, no construction\nwith $O(\\log N)$ bits (polynomial size family) and sub-constant error was known\nbefore.\n  In this work, we continue and extend the study of constructing ($k$-)min-wise\nhash families from pseudorandomness for combinatorial rectangles and read-once\nbranching programs. Our main result gives the first explicit min-wise hash\nfamilies that use an optimal (up to constant) number of random bits and achieve\na sub-constant (in fact, almost polynomially small) error, specifically, an\nexplicit family of $k$-min-wise hash with $O(k\\log N)$ bits and $2^{-O(\\log\nN/\\log\\log N)}$ error. This improves all previous results for any\n$k=\\log^{O(1)}N$ under $O(k \\log N)$ bits. Our main techniques involve several\nnew ideas to adapt the classical Nisan-Zuckerman pseudorandom generator to fool\nmin-wise hashing with a multiplicative error."}
{"id": "2510.10698", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10698", "abs": "https://arxiv.org/abs/2510.10698", "authors": ["Masoud Seddighin", "Saeed Seddighin"], "title": "Fair Assignment of Indivisible Chores to Asymmetric Agents", "comment": null, "summary": "We consider the problem of assigning indivisible chores to agents with\ndifferent entitlements in the maximin share value (\\MMS) context. While\nconstant-\\MMS\\ allocations/assignments are guaranteed to exist for both goods\nand chores in the symmetric setting, the situation becomes much more complex\nwhen agents have different entitlements. For the allocation of indivisible\ngoods, it has been proven that an $n$-\\WMMS\\ (weighted \\MMS) guarantee is the\nbest one can hope for. For indivisible chores, however, it was recently\ndiscovered that an $O(\\log n)$-\\WMMS\\ assignment is guaranteed to exist. In\nthis work, we improve this upper bound to a constant-\\WMMS\\\nguarantee.\\footnote{We prove the existence of a 20-\\WMMS\\ assignment, but we\ndid not attempt to optimize the constant factor. We believe our methods already\nyield a slightly better bound with a tighter analysis.}"}
{"id": "2510.10243", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10243", "abs": "https://arxiv.org/abs/2510.10243", "authors": ["Jian Zhu", "Zhidong Lin", "Wensheng Gan", "Ruichu Cai", "Zhifeng Hao", "Philip S. Yu"], "title": "Efficient Mining of Low-Utility Sequential Patterns", "comment": "Preprint, 4 tables, 9 figures", "summary": "Discovering valuable insights from rich data is a crucial task for\nexploratory data analysis. Sequential pattern mining (SPM) has found widespread\napplications across various domains. In recent years, low-utility sequential\npattern mining (LUSPM) has shown strong potential in applications such as\nintrusion detection and genomic sequence analysis. However, existing research\nin utility-based SPM focuses on high-utility sequential patterns, and the\ndefinitions and strategies used in high-utility SPM cannot be directly applied\nto LUSPM. Moreover, no algorithms have yet been developed specifically for\nmining low-utility sequential patterns. To address these problems, we formalize\nthe LUSPM problem, redefine sequence utility, and introduce a compact data\nstructure called the sequence-utility chain to efficiently record utility\ninformation. Furthermore, we propose three novel algorithm--LUSPM_b, LUSPM_s,\nand LUSPM_e--to discover the complete set of low-utility sequential patterns.\nLUSPM_b serves as an exhaustive baseline, while LUSPM_s and LUSPM_e build upon\nit, generating subsequences through shrinkage and extension operations,\nrespectively. In addition, we introduce the maximal non-mutually contained\nsequence set and incorporate multiple pruning strategies, which significantly\nreduce redundant operations in both LUSPM_s and LUSPM_e. Finally, extensive\nexperimental results demonstrate that both LUSPM_s and LUSPM_e substantially\noutperform LUSPM_b and exhibit excellent scalability. Notably, LUSPM_e achieves\nsuperior efficiency, requiring less runtime and memory consumption than\nLUSPM_s. Our code is available at https://github.com/Zhidong-Lin/LUSPM."}
{"id": "2510.11418", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11418", "abs": "https://arxiv.org/abs/2510.11418", "authors": ["Daniel Seifert", "Onur Günlü", "Rafael F. Schaefer"], "title": "Forward-Forward Autoencoder Architectures for Energy-Efficient Wireless Communications", "comment": null, "summary": "The application of deep learning to the area of communications systems has\nbeen a growing field of interest in recent years. Forward-forward (FF) learning\nis an efficient alternative to the backpropagation (BP) algorithm, which is the\ntypically used training procedure for neural networks. Among its several\nadvantages, FF learning does not require the communication channel to be\ndifferentiable and does not rely on the global availability of partial\nderivatives, allowing for an energy-efficient implementation. In this work, we\ndesign end-to-end learned autoencoders using the FF algorithm and numerically\nevaluate their performance for the additive white Gaussian noise and Rayleigh\nblock fading channels. We demonstrate their competitiveness with BP-trained\nsystems in the case of joint coding and modulation, and in a scenario where a\nfixed, non-differentiable modulation stage is applied. Moreover, we provide\nfurther insights into the design principles of the FF network, its training\nconvergence behavior, and significant memory and processing time savings\ncompared to BP-based approaches."}
{"id": "2510.10127", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10127", "abs": "https://arxiv.org/abs/2510.10127", "authors": ["Qiya Yang", "Xiaoxi Liang", "Zeping Xiao", "Yingjie Deng", "Yalong Wang", "Yongqi Liu", "Han Li"], "title": "Breaking the Likelihood Trap: Consistent Generative Recommendation with Graph-structured Model", "comment": null, "summary": "Reranking, as the final stage of recommender systems, demands real-time\ninference, accuracy, and diversity. It plays a crucial role in determining the\nfinal exposure, directly influencing user experience. Recently, generative\nreranking has gained increasing attention for its strong ability to model\ncomplex dependencies among items. However, most existing methods suffer from\nthe \"likelihood trap\", where high-likelihood sequences are often perceived as\nlow-quality by humans. These models tend to repeatedly recommend a set of\nhigh-frequency items, resulting in list homogeneity, thereby limiting user\nengagement. In this work, we propose Consistent Graph-structured Generative\nRecommendation (Congrats), a novel generative reranking framework. To break the\nlikelihood trap, we introduce a novel graph-structured decoder that can capture\ndiverse sequences along multiple paths. This design not only expands the\ndecoding space to promote diversity, but also improves prediction accuracy by\nimplicit item dependencies derived from vertex transitions. Furthermore, we\ndesign a differentiable cascade system that incorporates an evaluator, enabling\nthe model to learn directly from user preferences as the training objective.\nExtensive offline experiments validate the superior performance of Congrats\nover state-of-the-art reranking methods. Moreover, Congrats has been evaluated\non a large-scale video-sharing app, Kuaishou, with over 300 million daily\nactive users, demonstrating that our approach significantly improves both\nrecommendation quality and diversity, validating our effectiveness in practical\nindustrial environments."}
{"id": "2510.10665", "categories": ["cs.DS", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.10665", "abs": "https://arxiv.org/abs/2510.10665", "authors": ["Ilias Diakonikolas", "Chao Gao", "Daniel M. Kane", "John Lafferty", "Ankit Pensia"], "title": "Information-Computation Tradeoffs for Noiseless Linear Regression with Oblivious Contamination", "comment": "To appear in NeurIPS 2025", "summary": "We study the task of noiseless linear regression under Gaussian covariates in\nthe presence of additive oblivious contamination. Specifically, we are given\ni.i.d.\\ samples from a distribution $(x, y)$ on $\\mathbb{R}^d \\times\n\\mathbb{R}$ with $x \\sim \\mathcal{N}(0,\\mathbf{I}_d)$ and $y = x^\\top \\beta +\nz$, where $z$ is drawn independently of $x$ from an unknown distribution $E$.\nMoreover, $z$ satisfies $\\mathbb{P}_E[z = 0] = \\alpha>0$. The goal is to\naccurately recover the regressor $\\beta$ to small $\\ell_2$-error. Ignoring\ncomputational considerations, this problem is known to be solvable using\n$O(d/\\alpha)$ samples. On the other hand, the best known polynomial-time\nalgorithms require $\\Omega(d/\\alpha^2)$ samples. Here we provide formal\nevidence that the quadratic dependence in $1/\\alpha$ is inherent for efficient\nalgorithms. Specifically, we show that any efficient Statistical Query\nalgorithm for this task requires VSTAT complexity at least\n$\\tilde{\\Omega}(d^{1/2}/\\alpha^2)$."}
{"id": "2510.10929", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10929", "abs": "https://arxiv.org/abs/2510.10929", "authors": ["Junjie Luo", "Changjun Wang"], "title": "Achieving Coordination in Non-Cooperative Joint Replenishment Games", "comment": null, "summary": "We analyze an infinite-horizon deterministic joint replenishment model from a\nnon-cooperative game-theoretical approach. In this model, a group of retailers\ncan choose to jointly place an order, which incurs a major setup cost\nindependent of the group, and a minor setup cost for each retailer.\nAdditionally, each retailer is associated with a holding cost. Our objective is\nto design cost allocation rules that minimize the long-run average system cost\nwhile accounting for the fact that each retailer independently selects its\nreplenishment interval to minimize its own cost. We introduce a class of cost\nallocation rules that distribute the major setup cost among the associated\nretailers in proportion to their predefined weights. For these rules, we\nestablish a monotonicity property of agent better responses, which enables us\nto prove the existence of a payoff dominant pure Nash equilibrium that can also\nbe computed efficiently. We then analyze the efficiency of these equilibria by\nexamining the price of stability (PoS), the ratio of the best Nash\nequilibrium's system cost to the social optimum, across different information\nsettings. In particular, our analysis reveals that one rule, which leverages\nretailers' own holding cost rates, achieves a near-optimal PoS of 1.25, while\nanother rule that does not require access to retailers' private information\nalso yields a favorable PoS."}
{"id": "2510.10348", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10348", "abs": "https://arxiv.org/abs/2510.10348", "authors": ["Ling Zhang", "Shaleen Deep", "Jignesh M. Patel", "Karthikeyan Sankaralingam"], "title": "Regular Expression Indexing for Log Analysis. Extended Version", "comment": null, "summary": "In this paper, we present the design and architecture of REI, a novel system\nfor indexing log data for regular expression queries. Our main contribution is\nan $n$-gram-based indexing strategy and an efficient storage mechanism that\nresults in a speedup of up to 14x compared to state-of-the-art regex processing\nengines that do not use indexing, using only 2.1% of extra space. We perform a\ndetailed study that analyzes the space usage of the index and the improvement\nin workload execution time, uncovering interesting insights. Specifically, we\nshow that even an optimized implementation of strategies such as inverted\nindexing, which are widely used in text processing libraries, may lead to\nsuboptimal performance for regex indexing on log analysis tasks. Overall, the\nREI approach presented in this paper provides a significant boost when\nevaluating regular expression queries on log data. REI is also modular and can\nwork with existing regular expression packages, making it easy to deploy in a\nvariety of settings. The code of REI is available at\nhttps://github.com/mush-zhang/REI-Regular-Expression-Indexing."}
{"id": "2510.11445", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11445", "abs": "https://arxiv.org/abs/2510.11445", "authors": ["Renaud-Alexandre Pitaval"], "title": "Repeated-and-Offset QPSK for DFT-s-OFDM in Satellite Access", "comment": null, "summary": "Motivated by the convergence of terrestrial cellular networks with satellite\nnetworks, we consider an adaptation of offset quadrature phase shift keying\n(OQPSK), used with single-carrier waveform in traditional satellite systems, to\ndiscrete Fourier transform spread (DFT-s-) orthogonal frequency-division\nmultiplexed (OFDM) waveform employed in the uplink of terrestrial systems. We\nintroduce a new order-one constellation modulation, termed repeated-and-offset\nQPSK (RO-QPSK), derive its basic properties, and compare it with pi/2-BPSK with\nfrequency-domain spectral shaping (FDSS), as supported in 5G. RO-QPSK naturally\nproduces a Hann-window-shaped spectrum, resulting in a very low maximum\npeak-to-average power ratio (PAPR) on the order of 2 dB. Moreover, with\nsingle-tap equalization and symbol combining at the receiver, RO-QSPK can\nimprove the signal-to-interference-plus-noise (SINR) compared to pi/2-BPSK with\nFDSS, in narrowband and/or moderately frequency-selective channels, as\nencountered in satellite communications. A moderate FDSS can also be combined\nwith RO-QSPK to further reduce the PAPR while providing similar performance. Of\nindependent interest, general SINR expressions for DFT-s-OFDM are also\nprovided."}
{"id": "2510.10419", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10419", "abs": "https://arxiv.org/abs/2510.10419", "authors": ["Weiwei Sun", "Keyi Kong", "Xinyu Ma", "Shuaiqiang Wang", "Dawei Yin", "Maarten de Rijke", "Zhaochun Ren", "Yiming Yang"], "title": "ZeroGR: A Generalizable and Scalable Framework for Zero-Shot Generative Retrieval", "comment": null, "summary": "Generative retrieval (GR) reformulates information retrieval (IR) by framing\nit as the generation of document identifiers (docids), thereby enabling an\nend-to-end optimization and seamless integration with generative language\nmodels (LMs). Despite notable progress under supervised training, GR still\nstruggles to generalize to zero-shot IR scenarios, which are prevalent in\nreal-world applications. To tackle this challenge, we propose \\textsc{ZeroGR},\na zero-shot generative retrieval framework that leverages natural language\ninstructions to extend GR across a wide range of IR tasks. Specifically,\n\\textsc{ZeroGR} is composed of three key components: (i) an LM-based docid\ngenerator that unifies heterogeneous documents (e.g., text, tables, code) into\nsemantically meaningful docids; (ii) an instruction-tuned query generator that\ngenerates diverse types of queries from natural language task descriptions to\nenhance corpus indexing; and (iii) a reverse annealing decoding strategy to\nbalance precision and recall during docid generation. We investigate the impact\nof instruction fine-tuning scale and find that performance consistently\nimproves as the number of IR tasks encountered during training increases.\nEmpirical results on the BEIR and MAIR benchmarks demonstrate that\n\\textsc{ZeroGR} outperforms strong dense retrieval and generative baselines in\nzero-shot settings, establishing a new state-of-the-art for instruction-driven\nGR."}
{"id": "2510.10705", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10705", "abs": "https://arxiv.org/abs/2510.10705", "authors": ["Yinhao Dong", "Shan Jiang", "Shi Li", "Pan Peng"], "title": "Learning-Augmented Streaming Algorithms for Correlation Clustering", "comment": "NeurIPS 2025", "summary": "We study streaming algorithms for Correlation Clustering. Given a graph as an\narbitrary-order stream of edges, with each edge labeled as positive or\nnegative, the goal is to partition the vertices into disjoint clusters, such\nthat the number of disagreements is minimized. In this paper, we give the first\nlearning-augmented streaming algorithms for the problem on both complete and\ngeneral graphs, improving the best-known space-approximation tradeoffs. Based\non the works of Cambus et al. (SODA'24) and Ahn et al. (ICML'15), our\nalgorithms use the predictions of pairwise distances between vertices provided\nby a predictor. For complete graphs, our algorithm achieves a better-than-$3$\napproximation under good prediction quality, while using $\\tilde{O}(n)$ total\nspace. For general graphs, our algorithm achieves an $O(\\log |E^-|)$\napproximation under good prediction quality using $\\tilde{O}(n)$ total space,\nimproving the best-known non-learning algorithm in terms of space efficiency.\nExperimental results on synthetic and real-world datasets demonstrate the\nsuperiority of our proposed algorithms over their non-learning counterparts."}
{"id": "2510.11253", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.11253", "abs": "https://arxiv.org/abs/2510.11253", "authors": ["Sayantika Mandal", "Harman Agrawal", "Swaprava Nath"], "title": "Likes, Budgets, and Equilibria: Designing Contests for Socially Optimal Advertising", "comment": "26 pages, under review", "summary": "Firms (businesses, service providers, entertainment organizations, political\nparties, etc.) advertise on social networks to draw people's attention and\nimprove their awareness of the brands of the firms. In all such cases, the\ncompetitive nature of their engagements gives rise to a game where the firms\nneed to decide how to distribute their budget over the agents on a network to\nmaximize their brand's awareness. The firms (players) therefore need to\noptimize how much budget they should put on the vertices of the network so that\nthe spread improves via direct (via advertisements or free promotional offers)\nand indirect marketing (words-of-mouth). We propose a two-timescale model of\ndecisions where the communication between the vertices happen in a faster\ntimescale and the strategy update of the firms happen in a slower timescale. We\nshow that under fairly standard conditions, the best response dynamics of the\nfirms converge to a pure strategy Nash equilibrium. However, such equilibria\ncan be away from a socially optimal one. We provide a characterization of the\ncontest success functions and provide examples for the designers of such\ncontests (e.g., regulators, social network providers, etc.) such that the Nash\nequilibrium becomes unique and social welfare maximizing. Our experiments show\nthat for realistic scenarios, such contest success functions perform fairly\nwell."}
{"id": "2510.10580", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10580", "abs": "https://arxiv.org/abs/2510.10580", "authors": ["Jiahao He", "Yutao Cui", "Cuiping Li", "Jikang Jiang", "Yuheng Hou", "Hong Chen"], "title": "AQORA: A Learned Adaptive Query Optimizer for Spark SQL", "comment": "14 pages, 11 figures", "summary": "Recent studies have identified two main approaches to improve query\noptimization: learned query optimization (LQO), which generates or selects\nbetter query plans before execution based on models trained in advance, and\nadaptive query processing (AQP), which adapts the query plan during execution\nbased on statistical feedback collected at runtime. Although both approaches\nhave shown promise, they also face critical limitations. LQO must commit to a\nfixed plan without access to actual cardinalities and typically rely on a\nsingle end-to-end feedback signal, making learning inefficient. On the other\nhand, AQP depends heavily on rule-based heuristics and lacks the ability to\nlearn from experience. In this paper, we present AQORA, an adaptive query\noptimizer with a reinforcement learning architecture that combines the\nstrengths of both LQO and AQP. AQORA addresses the above challenges through\nfour core strategies: (1) realistic feature encoding, (2) query stage-level\nfeedback and intervention, (3) automatic strategy adaptation, and (4) low-cost\nintegration. Experiments show that AQORA reduces end-to-end execution time by\nup to 90% compared to other learned methods and by up to 70% compared to Spark\nSQL's default configuration with adaptive query execution."}
{"id": "2510.11453", "categories": ["cs.IT", "cs.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11453", "abs": "https://arxiv.org/abs/2510.11453", "authors": ["Chris Peikert", "Alexandra Veliche Hostetler"], "title": "List Decoding Reed--Solomon Codes in the Lee, Euclidean, and Other Metrics", "comment": "26 pages, 1 figure", "summary": "Reed--Solomon error-correcting codes are ubiquitous across computer science\nand information theory, with applications in cryptography, computational\ncomplexity, communication and storage systems, and more. Most works on\nefficient error correction for these codes, like the celebrated\nBerlekamp--Welch unique decoder and the (Guruswami--)Sudan list decoders, are\nfocused on measuring error in the Hamming metric, which simply counts the\nnumber of corrupted codeword symbols. However, for some applications, other\nmetrics that depend on the specific values of the errors may be more\nappropriate.\n  This work gives a polynomial-time algorithm that list decodes (generalized)\nReed--Solomon codes over prime fields in $\\ell_p$ (semi)metrics, for any $0 < p\n\\leq 2$. Compared to prior algorithms for the Lee ($\\ell_1$) and Euclidean\n($\\ell_2$) metrics, ours decodes to arbitrarily large distances (for\ncorrespondingly small rates), and has better distance-rate tradeoffs for all\ndecoding distances above some moderate thresholds. We also prove lower bounds\non the $\\ell_{1}$ and $\\ell_{2}$ minimum distances of a certain natural\nsubclass of GRS codes, which establishes that our list decoder is actually a\nunique decoder for many parameters of interest. Finally, we analyze our\nalgorithm's performance under random Laplacian and Gaussian errors, and show\nthat it supports even larger rates than for corresponding amounts of worst-case\nerror in $\\ell_{1}$ and $\\ell_{2}$ (respectively)."}
{"id": "2510.10440", "categories": ["cs.IR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10440", "abs": "https://arxiv.org/abs/2510.10440", "authors": ["Alex Ayoub", "Samuel Robertson", "Dawen Liang", "Harald Steck", "Nathan Kallus"], "title": "Does Weighting Improve Matrix Factorization for Recommender Systems?", "comment": "In the proceedings of the Web Conference (WWW) 2025 (11 pages)", "summary": "Matrix factorization is a widely used approach for top-N recommendation and\ncollaborative filtering. When implemented on implicit feedback data (such as\nclicks), a common heuristic is to upweight the observed interactions. This\nstrategy has been shown to improve performance for certain algorithms. In this\npaper, we conduct a systematic study of various weighting schemes and matrix\nfactorization algorithms. Somewhat surprisingly, we find that training with\nunweighted data can perform comparably to, and sometimes outperform, training\nwith weighted data, especially for large models. This observation challenges\nthe conventional wisdom. Nevertheless, we identify cases where weighting can be\nbeneficial, particularly for models with lower capacity and specific\nregularization schemes. We also derive efficient algorithms for exactly\nminimizing several weighted objectives that were previously considered\ncomputationally intractable. Our work provides a comprehensive analysis of the\ninterplay between weighting, regularization, and model capacity in matrix\nfactorization for recommender systems."}
{"id": "2510.10989", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.10989", "abs": "https://arxiv.org/abs/2510.10989", "authors": ["Yixiong Gao", "Florian Jaehn", "Minming Li", "Wenhao Ma", "Xinbo Zhang"], "title": "Crane Scheduling Problem with Energy Saving", "comment": null, "summary": "During loading and unloading steps, energy is consumed when cranes lift\ncontainers, while energy is often wasted when cranes drop containers. By\noptimizing the scheduling of cranes, it is possible to reduce energy\nconsumption, thereby lowering operational costs and environmental impacts. In\nthis paper, we introduce a single-crane scheduling problem with energy savings,\nfocusing on reusing the energy from containers that have already been lifted\nand reducing the total energy consumption of the entire scheduling plan. We\nestablish a basic model considering a one-dimensional storage area and provide\na systematic complexity analysis of the problem. First, we investigate the\nconnection between our problem and the semi-Eulerization problem and propose an\nadditive approximation algorithm. Then, we present a polynomial-time Dynamic\nProgramming (DP) algorithm for the case of bounded energy buffer and processing\nlengths. Next, adopting a Hamiltonian perspective, we address the general case\nwith arbitrary energy buffer and processing lengths. We propose an exact DP\nalgorithm and show that the variation of the problem is polynomially solvable\nwhen it can be transformed into a path cover problem on acyclic interval\ndigraphs. We introduce a paradigm that integrates both the Eulerian and\nHamiltonian perspectives, providing a robust framework for addressing the\nproblem."}
{"id": "2510.11255", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.11255", "abs": "https://arxiv.org/abs/2510.11255", "authors": ["Ashwin Goyal", "Drashthi Doshi", "Swaprava Nath"], "title": "Temporal Cooperative Games", "comment": "21 pages, under review", "summary": "Classical cooperative game theory assumes that the worth of a coalition\ndepends only on the set of agents involved, but in practice, it may also depend\non the order in which agents arrive. Motivated by such scenarios, we introduce\ntemporal cooperative games (TCG), where the worth $v$ becomes a function of the\nsequence of agents $\\pi$ rather than just the set $S$. This shift calls for\nrethinking the underlying axioms. A key property in this temporal framework is\nthe incentive for optimal arrival (I4OA), which encourages agents to join in\nthe order maximizing total worth. Alongside, we define two additional\nproperties: online individual rationality (OIR), incentivizing earlier agents\nto invite more participants, and sequential efficiency (SE), ensuring that the\ntotal worth of any sequence is fully distributed among its agents. We identify\na class of reward-sharing mechanisms uniquely characterized by these three\nproperties. The classical Shapley value does not directly apply here, so we\nconstruct its natural analogs in two variants: the sequential world, where\nrewards are defined for each sequence-player pair, and the extended world,\nwhere rewards are defined for each player alone. Properties of efficiency,\nadditivity, and null player uniquely determine these Shapley analogs in both\nworlds. Importantly, the Shapley analogs are disjoint from mechanisms\nsatisfying I4OA, OIR, and SE, and this conflict persists even for restricted\nclasses such as convex and simple TCGs. Our findings thus uncover a fundamental\ntension: when players arrive sequentially, reward-sharing mechanisms satisfying\ndesirable temporal properties must inherently differ from Shapley-inspired\nones, opening new questions for defining fair and efficient solution concepts\nin TCGs."}
{"id": "2510.10858", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10858", "abs": "https://arxiv.org/abs/2510.10858", "authors": ["Guanli Liu", "Renata Borovica-Gajic"], "title": "DriftBench: Defining and Generating Data and Query Workload Drift for Benchmarking", "comment": null, "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."}
{"id": "2510.10511", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10511", "abs": "https://arxiv.org/abs/2510.10511", "authors": ["Xu Zhao", "Xiaopeng Ye", "Chen Xu", "Weiran Shen", "Jun Xu"], "title": "Towards Long-Term User Welfare in Recommender Systems via Creator-Oriented Information Revelation", "comment": null, "summary": "Improving the long-term user welfare (e.g., sustained user engagement) has\nbecome a central objective of recommender systems (RS). In real-world\nplatforms, the creation behaviors of content creators plays a crucial role in\nshaping long-term welfare beyond short-term recommendation accuracy, making the\neffective steering of creator behavior essential to foster a healthier RS\necosystem. Existing works typically rely on re-ranking algorithms that\nheuristically adjust item exposure to steer creators' behavior. However, when\nembedded within recommendation pipelines, such a strategy often conflicts with\nthe short-term objective of improving recommendation accuracy, leading to\nperformance degradation and suboptimal long-term welfare. The well-established\neconomics studies offer us valuable insights for an alternative approach\nwithout relying on recommendation algorithmic design: revealing information\nfrom an information-rich party (sender) to a less-informed party (receiver) can\neffectively change the receiver's beliefs and steer their behavior. Inspired by\nthis idea, we propose an information-revealing framework, named Long-term\nWelfare Optimization via Information Revelation (LoRe). In this framework, we\nutilize a classical information revelation method (i.e., Bayesian persuasion)\nto map the stakeholders in RS, treating the platform as the sender and creators\nas the receivers. To address the challenge posed by the unrealistic assumption\nof traditional economic methods, we formulate the process of information\nrevelation as a Markov Decision Process (MDP) and propose a learning algorithm\ntrained and inferred in environments with boundedly rational creators.\nExtensive experiments on two real-world RS datasets demonstrate that our method\ncan effectively outperform existing fair re-ranking methods and information\nrevealing strategies in improving long-term user welfare."}
{"id": "2510.11266", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11266", "abs": "https://arxiv.org/abs/2510.11266", "authors": ["Kalen Patton"], "title": "Online Allocation with Concave, Diminishing-Returns Objectives", "comment": "Appears at SODA 2026", "summary": "Online resource allocation problems are central challenges in economics and\ncomputer science, modeling situations in which $n$ items arriving one at a time\nmust each be immediately allocated among $m$ agents. In such problems, our\nobjective is to maximize a monotone reward function $f(\\mathbf{x})$ over the\nallocation vector $\\mathbf{x} = (x_{ij})_{i, j}$, which describes the amount of\neach item given to each agent. In settings where $f$ is concave and has\n\"diminishing returns\" (monotone decreasing gradient), several lines of work\nover the past two decades have had great success designing constant-competitive\nalgorithms, including the foundational work of Mehta et al. (2005) on the\nAdwords problem and many follow-ups. Notably, while a greedy algorithm is\n$\\frac{1}{2}$-competitive in such settings, these works have shown that one can\noften obtain a competitive ratio of $1-\\frac{1}{e} \\approx 0.632$ in a variety\nof settings when items are divisible (i.e. allowing fractional allocations).\nHowever, prior works have thus far used a variety of problem-specific\ntechniques, leaving open the general question: Does a\n$(1-\\frac{1}{e})$-competitive fractional algorithm always exist for online\nresource allocation problems with concave, diminishing-returns objectives?\n  In this work, we answer this question affirmatively, thereby unifying and\ngeneralizing prior results for special cases. Our algorithm is one which makes\ncontinuous greedy allocations with respect to an auxiliary objective\n$U(\\mathbf{x})$. Using the online primal-dual method, we show that if $U$\nsatisfies a \"balanced\" property with respect to $f$, then one can bound the\ncompetitiveness of such an algorithm. Our crucial observation is that there is\na simple expression for $U$ which has this balanced property for any $f$,\nyielding our general $(1-\\frac{1}{e})$-competitive algorithm."}
{"id": "2510.11550", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.11550", "abs": "https://arxiv.org/abs/2510.11550", "authors": ["Kristoffer Arnsfelt Hansen", "Xinhao Nie"], "title": "On the Complexity of Stationary Nash Equilibria in Discounted Perfect Information Stochastic Games", "comment": null, "summary": "We study the problem of computing stationary Nash equilibria in discounted\nperfect information stochastic games from the viewpoint of computational\ncomplexity. For two-player games we prove the problem to be in PPAD, which\ntogether with a previous PPAD-hardness result precisely classifies the problem\nas PPAD-complete. In addition to this we give an improved and simpler\nPPAD-hardness proof for computing a stationary epsilon-Nash equilibrium. For\n3-player games we construct games showing that rational-valued stationary Nash\nequilibria are not guaranteed to exist, and we use these to prove\nSqrtSum-hardness of computing a stationary Nash equilibrium in 4-player games."}
{"id": "2510.11011", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11011", "abs": "https://arxiv.org/abs/2510.11011", "authors": ["Farzaneh Zirak", "Farhana Choudhury", "Renata Borovica-Gajic"], "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable Transactional and Analytical Workloads", "comment": "This is a preprint version", "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."}
{"id": "2510.10556", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10556", "abs": "https://arxiv.org/abs/2510.10556", "authors": ["Donglin Zhou", "Weike Pan", "Zhong Ming"], "title": "Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation (SR) models often capture user preferences based on\nthe historically interacted item IDs, which usually obtain sub-optimal\nperformance when the interaction history is limited. Content-based sequential\nrecommendation has recently emerged as a promising direction that exploits\nitems' textual and visual features to enhance preference learning. However,\nthere are still three key challenges: (i) how to reduce the semantic gap\nbetween different content modality representations; (ii) how to jointly model\nuser behavior preferences and content preferences; and (iii) how to design an\neffective training strategy to align ID representations and content\nrepresentations. To address these challenges, we propose a novel model,\nself-supervised representation learning with ID-Content modality alignment,\nnamed SICSRec. Firstly, we propose a LLM-driven sample construction method and\ndevelop a supervised fine-tuning approach to align item-level modality\nrepresentations. Secondly, we design a novel Transformer-based sequential\nmodel, where an ID-modality sequence encoder captures user behavior\npreferences, a content-modality sequence encoder learns user content\npreferences, and a mix-modality sequence decoder grasps the intrinsic\nrelationship between these two types of preferences. Thirdly, we propose a\ntwo-step training strategy with a content-aware contrastive learning task to\nalign modality representations and ID representations, which decouples the\ntraining process of content modality dependency and item collaborative\ndependency. Extensive experiments conducted on four public video streaming\ndatasets demonstrate our SICSRec outperforms the state-of-the-art ID-modality\nsequential recommenders and content-modality sequential recommenders by 8.04%\non NDCG@5 and 6.62% on NDCD@10 on average, respectively."}
{"id": "2510.11368", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11368", "abs": "https://arxiv.org/abs/2510.11368", "authors": ["Kleitos Papadopoulos"], "title": "An $O(n\\log n)$ Algorithm for Single-Item Capacitated Lot Sizing with a One-Breakpoint All-Units Discount and Non-Increasing Prices", "comment": null, "summary": "This paper addresses the single-item capacitated lot sizing problem with a\n1-breakpoint all-units quantity discount in a monotonic setting where the\npurchase prices are non-increasing over the planning horizon. For this case, we\nestablish several novel properties of the optimal solution and develop a hybrid\ndynamic programming approach that maintains a compact representation of the\nsolution space by storing only essential information about the states and using\nlinear equations for intermediate values. Our algorithm runs in \\(O(n\\log n)\\)\ntime, where \\(n\\) denotes the number of periods. Our result is an improvement\nover the previous state-of-the-art algorithm, which has an \\(O(n^2)\\) time\ncomplexity."}
{"id": "2510.11625", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.11625", "abs": "https://arxiv.org/abs/2510.11625", "authors": ["Drew Springham", "Edith Elkind", "Bart de Keijzer", "Maria Polukarov"], "title": "Multiwinner Voting with Interval Preferences under Incomplete Information", "comment": "19 pages, 5 figures", "summary": "In multiwinner approval elections with many candidates, voters may struggle\nto determine their preferences over the entire slate of candidates. It is\ntherefore of interest to explore which (if any) fairness guarantees can be\nprovided under reduced communication. In this paper, we consider voters with\none-dimensional preferences: voters and candidates are associated with points\nin $\\mathbb R$, and each voter's approval set forms an interval of $\\mathbb R$.\nWe put forward a probabilistic preference model, where the voter set consists\nof $\\sigma$ different groups; each group is associated with a distribution over\nan interval of $\\mathbb R$, so that each voter draws the endpoints of her\napproval interval from the distribution associated with her group. We present\nan algorithm for computing committees that provide Proportional Justified\nRepresentation + (PJR+), which proceeds by querying voters' preferences, and\nshow that, in expectation, it makes $\\mathcal{O}(\\log( \\sigma\\cdot k))$ queries\nper voter, where $k$ is the desired committee size."}
{"id": "2510.11166", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.11166", "abs": "https://arxiv.org/abs/2510.11166", "authors": ["Brad Bebee", "Ümit V. Çatalyürek", "Olaf Hartig", "Ankesh Khandelwal", "Simone Rondelli", "Michael Schmidt", "Lefteris Sidirourgos", "Bryan Thompson"], "title": "Poseidon: A OneGraph Engine", "comment": null, "summary": "We present the Poseidon engine behind the Neptune Analytics graph database\nservice. Customers interact with Poseidon using the declarative openCypher\nquery language, which enables requests that seamlessly combine traditional\nquerying paradigms (such as graph pattern matching, variable length paths,\naggregation) with algorithm invocations and has been syntactically extended to\nfacilitate OneGraph interoperability, such as the disambiguation between\nglobally unique IRIs (as exposed via RDF) vs. local identifiers (as encountered\nin LPG data). Poseidon supports a broad range of graph workloads, from simple\ntransactions, to top-k beam search algorithms on dynamic graphs, to whole graph\nanalytics requiring multiple full passes over the data. For example, real-time\nfraud detection, like many other use cases, needs to reflect current committed\nstate of the dynamic graph. If a users cell phone is compromised, then all\nnewer actions by that user become immediately suspect. To address such dynamic\ngraph use cases, Poseidon combines state-of-the-art transaction processing with\nnovel graph data indexing, including lock-free maintenance of adjacency lists,\nsecondary succinct indices, partitioned heaps for data tuple storage with\nuniform placement, and innovative statistics for cost-based query optimization.\nThe Poseidon engine uses a logical log for durability, enabling rapid evolution\nof in-memory data structures. Bulk data loads achieve more than 10 million\nproperty values per second on many data sets while simple transactions can\nexecute in under 20ms against the storage engine."}
{"id": "2510.10564", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10564", "abs": "https://arxiv.org/abs/2510.10564", "authors": ["Liang Li", "Zhou Yang", "Xiaofei Zhu"], "title": "Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation aims to predict the next item based on user\ninterests in historical interaction sequences. Historical interaction sequences\noften contain irrelevant noisy items, which significantly hinders the\nperformance of recommendation systems. Existing research employs unsupervised\nmethods that indirectly identify item-granularity irrelevant noise by\npredicting the ground truth item. Since these methods lack explicit noise\nlabels, they are prone to misidentify users' interested items as noise.\nAdditionally, while these methods focus on removing item-granularity noise\ndriven by the ground truth item, they overlook interest-granularity noise,\nlimiting their ability to perform broader denoising based on user interests. To\naddress these issues, we propose Multi-Granularity Sequence Denoising with\nWeakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSS\nfirst introduces the Multiple Gaussian Kernel Perceptron module to map the\noriginal and enhance sequence into a common representation space and utilizes\nweakly supervised signals to accurately identify noisy items in the historical\ninteraction sequence. Subsequently, it employs the item-granularity denoising\nmodule with noise-weighted contrastive learning to obtain denoised item\nrepresentations. Then, it extracts target interest representations from the\nground truth item and applies noise-weighted contrastive learning to obtain\ndenoised interest representations. Finally, based on the denoised item and\ninterest representations, MGSD-WSS predicts the next item. Extensive\nexperiments on five datasets demonstrate that the proposed method significantly\noutperforms state-of-the-art sequence recommendation and denoising models. Our\ncode is available at https://github.com/lalunex/MGSD-WSS."}
{"id": "2510.11547", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11547", "abs": "https://arxiv.org/abs/2510.11547", "authors": ["Pan Peng", "Christian Sohler", "Yi Xu"], "title": "Sublinear Algorithms for Estimating Single-Linkage Clustering Costs", "comment": "70 pages", "summary": "Single-linkage clustering is a fundamental method for data analysis.\nAlgorithmically, one can compute a single-linkage $k$-clustering (a partition\ninto $k$ clusters) by computing a minimum spanning tree and dropping the $k-1$\nmost costly edges. This clustering minimizes the sum of spanning tree weights\nof the clusters. This motivates us to define the cost of a single-linkage\n$k$-clustering as the weight of the corresponding spanning forest, denoted by\n$\\mathrm{cost}_k$. Besides, if we consider single-linkage clustering as\ncomputing a hierarchy of clusterings, the total cost of the hierarchy is\ndefined as the sum of the individual clusterings, denoted by $\\mathrm{cost}(G)\n= \\sum_{k=1}^{n} \\mathrm{cost}_k$.\n  In this paper, we assume that the distances between data points are given as\na graph $G$ with average degree $d$ and edge weights from $\\{1,\\dots, W\\}$.\nGiven query access to the adjacency list of $G$, we present a sampling-based\nalgorithm that computes a succinct representation of estimates\n$\\widehat{\\mathrm{cost}}_k$ for all $k$. The running time is $\\tilde\nO(d\\sqrt{W}/\\varepsilon^3)$, and the estimates satisfy $\\sum_{k=1}^{n}\n|\\widehat{\\mathrm{cost}}_k - \\mathrm{cost}_k| \\le \\varepsilon\\cdot\n\\mathrm{cost}(G)$, for any $0<\\varepsilon <1$. Thus we can approximate the cost\nof every $k$-clustering upto $(1+\\varepsilon)$ factor \\emph{on average}. In\nparticular, our result ensures that we can estimate $\\cost(G)$ upto a factor of\n$1\\pm \\varepsilon$ in the same running time.\n  We also extend our results to the setting where edges represent similarities.\nIn this case, the clusterings are defined by a maximum spanning tree, and our\nalgorithms run in $\\tilde{O}(dW/\\varepsilon^3)$ time. We futher prove nearly\nmatching lower bounds for estimating the total clustering cost and we extend\nour algorithms to metric space settings."}
{"id": "2510.10828", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10828", "abs": "https://arxiv.org/abs/2510.10828", "authors": ["Zhenghan Tai", "Hanwei Wu", "Qingchen Hu", "Jijun Chi", "Hailin He", "Lei Ding", "Tung Sum Thomas Kwok", "Bohuai Xiao", "Yuchen Hua", "Suyuchen Wang", "Peng Lu", "Muzhi Li", "Yihong Wu", "Liheng Ma", "Jerry Huang", "Jiayi Zhang", "Gonghao Zhang", "Chaolong Jiang", "Jingrui Tian", "Sicheng Lyu", "Zeyu Li", "Boyu Han", "Fengran Mo", "Xinyue Yu", "Yufei Cui", "Ling Zhou", "Xinyu Wang"], "title": "VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is becoming increasingly essential for\nQuestion Answering (QA) in the financial sector, where accurate and\ncontextually grounded insights from complex public disclosures are crucial.\nHowever, existing financial RAG systems face two significant challenges: (1)\nthey struggle to process heterogeneous data formats, such as text, tables, and\nfigures; and (2) they encounter difficulties in balancing general-domain\napplicability with company-specific adaptation. To overcome these challenges,\nwe present VeritasFi, an innovative hybrid RAG framework that incorporates a\nmulti-modal preprocessing pipeline alongside a cutting-edge two-stage training\nstrategy for its re-ranking component. VeritasFi enhances financial QA through\nthree key innovations: (1) A multi-modal preprocessing pipeline that seamlessly\ntransforms heterogeneous data into a coherent, machine-readable format. (2) A\ntripartite hybrid retrieval engine that operates in parallel, combining deep\nmulti-path retrieval over a semantically indexed document corpus, real-time\ndata acquisition through tool utilization, and an expert-curated memory bank\nfor high-frequency questions, ensuring comprehensive scope, accuracy, and\nefficiency. (3) A two-stage training strategy for the document re-ranker, which\ninitially constructs a general, domain-specific model using anonymized data,\nfollowed by rapid fine-tuning on company-specific data for targeted\napplications. By integrating our proposed designs, VeritasFi presents a\ngroundbreaking framework that greatly enhances the adaptability and robustness\nof financial RAG systems, providing a scalable solution for both general-domain\nand company-specific QA tasks. Code accompanying this work is available at\nhttps://github.com/simplew4y/VeritasFi.git."}
{"id": "2510.11627", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11627", "abs": "https://arxiv.org/abs/2510.11627", "authors": ["Sepideh Mahabadi", "Mohammad Roghani", "Jakub Tarnawski", "Ali Vakilian"], "title": "Sublinear Metric Steiner Forest via Maximal Independent Set", "comment": null, "summary": "In this work we consider the Metric Steiner Forest problem in the sublinear\ntime model. Given a set $V$ of $n$ points in a metric space where distances are\nprovided by means of query access to an $n\\times n$ distance matrix, along with\na set of $k$ terminal pairs $(s_1,t_1), \\dots, (s_k,t_k)\\in V\\times V$, the\ngoal is to find a minimum-weight subset of edges that connects each terminal\npair. Although sublinear time algorithms have been studied for estimating the\nweight of a minimum spanning tree in both general and metric settings, as well\nas for the metric Steiner Tree problem, no sublinear time algorithm was known\nfor the metric Steiner Forest problem.\n  Here, we give an $O(\\log k)$-approximation algorithm for the problem that\nruns in time $\\widetilde{O}(n^{3/2})$. Along the way, we provide the first\nsublinear-time algorithm for estimating the size of a Maximal Independent Set\n(MIS). Our algorithm runs in time $\\widetilde{O}(n^{3/2}/\\varepsilon^2)$ under\nthe adjacency matrix oracle model and obtains a purely multiplicative\n$(1+\\varepsilon)$-approximation. Previously, sublinear-time algorithms for MIS\nwere only known for bounded-degree graphs."}
{"id": "2510.10920", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10920", "abs": "https://arxiv.org/abs/2510.10920", "authors": ["Yi Yu", "Zhenxing Hu"], "title": "Comparative Explanations via Counterfactual Reasoning in Recommendations", "comment": null, "summary": "Explainable recommendation through counterfactual reasoning seeks to identify\nthe influential aspects of items in recommendations, which can then be used as\nexplanations. However, state-of-the-art approaches, which aim to minimize\nchanges in product aspects while reversing their recommended decisions\naccording to an aggregated decision boundary score, often lead to factual\ninaccuracies in explanations. To solve this problem, in this work we propose a\nnovel method of Comparative Counterfactual Explanations for Recommendation\n(CoCountER). CoCountER creates counterfactual data based on soft swap\noperations, enabling explanations for recommendations of arbitrary pairs of\ncomparative items. Empirical experiments validate the effectiveness of our\napproach."}
{"id": "2510.11640", "categories": ["cs.DS", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11640", "abs": "https://arxiv.org/abs/2510.11640", "authors": ["Felix Zhou"], "title": "Continual Release of Densest Subgraphs: Privacy Amplification & Sublinear Space via Subsampling", "comment": "to be published in SOSA'26", "summary": "We study the sublinear space continual release model for edge-differentially\nprivate (DP) graph algorithms, with a focus on the densest subgraph problem\n(DSG) in the insertion-only setting. Our main result is the first continual\nrelease DSG algorithm that matches the additive error of the best static DP\nalgorithms and the space complexity of the best non-private streaming\nalgorithms, up to constants. The key idea is a refined use of subsampling that\nsimultaneously achieves privacy amplification and sparsification, a connection\nnot previously formalized in graph DP. Via a simple black-box reduction to the\nstatic setting, we obtain both pure and approximate-DP algorithms with $O(\\log\nn)$ additive error and $O(n\\log n)$ space, improving both accuracy and space\ncomplexity over the previous state of the art. Along the way, we introduce\ngraph densification in the graph DP setting, adding edges to trigger earlier\nsubsampling, which removes the extra logarithmic factors in error and space\nincurred by prior work [ELMZ25]. We believe this simple idea may be of\nindependent interest."}
{"id": "2510.10955", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10955", "abs": "https://arxiv.org/abs/2510.10955", "authors": ["Yu Cui", "Feng Liu", "Jiawei Chen", "Canghong Jin", "Xingyu Lou", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Can Wang"], "title": "HatLLM: Hierarchical Attention Masking for Enhanced Collaborative Modeling in LLM-based Recommendation", "comment": null, "summary": "Recent years have witnessed a surge of research on leveraging large language\nmodels (LLMs) for sequential recommendation. LLMs have demonstrated remarkable\npotential in inferring users' nuanced preferences through fine-grained semantic\nreasoning. However, they also exhibit a notable limitation in effectively\nmodeling collaborative signals, i.e., behavioral correlations inherent in\nusers' historical interactions. Our empirical analysis further reveals that the\nattention mechanisms in LLMs tend to disproportionately focus on tokens within\nthe same item, thereby impeding the capture of cross-item correlations.\n  To address this limitation, we propose a novel hierarchical attention masking\nstrategy for LLM-based recommendation, termed HatLLM. Specifically, in shallow\nlayers, HatLLM masks attention between tokens from different items,\nfacilitating intra-item semantic understanding; in contrast, in deep layers,\nHatLLM masks attention within items, thereby compelling the model to capture\ncross-item correlations. This progressive, layer-wise approach enables LLMs to\njointly model both token-level and item-level dependencies. Extensive\nexperiments on three real-world datasets demonstrate that HatLLM achieves\nsignificant performance gains (9.13% on average) over existing LLM-based\nmethods."}
{"id": "2510.11453", "categories": ["cs.IT", "cs.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11453", "abs": "https://arxiv.org/abs/2510.11453", "authors": ["Chris Peikert", "Alexandra Veliche Hostetler"], "title": "List Decoding Reed--Solomon Codes in the Lee, Euclidean, and Other Metrics", "comment": "26 pages, 1 figure", "summary": "Reed--Solomon error-correcting codes are ubiquitous across computer science\nand information theory, with applications in cryptography, computational\ncomplexity, communication and storage systems, and more. Most works on\nefficient error correction for these codes, like the celebrated\nBerlekamp--Welch unique decoder and the (Guruswami--)Sudan list decoders, are\nfocused on measuring error in the Hamming metric, which simply counts the\nnumber of corrupted codeword symbols. However, for some applications, other\nmetrics that depend on the specific values of the errors may be more\nappropriate.\n  This work gives a polynomial-time algorithm that list decodes (generalized)\nReed--Solomon codes over prime fields in $\\ell_p$ (semi)metrics, for any $0 < p\n\\leq 2$. Compared to prior algorithms for the Lee ($\\ell_1$) and Euclidean\n($\\ell_2$) metrics, ours decodes to arbitrarily large distances (for\ncorrespondingly small rates), and has better distance-rate tradeoffs for all\ndecoding distances above some moderate thresholds. We also prove lower bounds\non the $\\ell_{1}$ and $\\ell_{2}$ minimum distances of a certain natural\nsubclass of GRS codes, which establishes that our list decoder is actually a\nunique decoder for many parameters of interest. Finally, we analyze our\nalgorithm's performance under random Laplacian and Gaussian errors, and show\nthat it supports even larger rates than for corresponding amounts of worst-case\nerror in $\\ell_{1}$ and $\\ell_{2}$ (respectively)."}
{"id": "2510.10978", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10978", "abs": "https://arxiv.org/abs/2510.10978", "authors": ["Bohao Wang", "Jiawei Chen", "Feng Liu", "Changwang Zhang", "Jun Wang", "Canghong Jin", "Chun Chen", "Can Wang"], "title": "Does LLM Focus on the Right Words? Diagnosing Language Bias in LLM-based Recommenders", "comment": null, "summary": "Large language models (LLMs), owing to their extensive open-domain knowledge\nand semantic reasoning capabilities, have been increasingly integrated into\nrecommender systems (RS). However, a substantial gap remains between the\npre-training objectives of LLMs and the specific requirements of recommendation\ntasks. To address this gap, supervised fine-tuning (SFT) is commonly performed\non specially curated recommendation datasets to further enhance their\npredictive ability. Despite its success, SFT exhibits a critical limitation: it\ninduces Language Bias, whereby the model over-relies on auxiliary tokens-such\nas task descriptions and prefix-generated tokens-while underutilizing core user\ninteraction tokens that encode user-specific preferences. This bias not only\nundermines recommendation accuracy but also raises unfairness concerns.\n  To address this issue, we propose Group Distributionally Robust\nOptimization-based Tuning (GDRT), a novel fine-tuning paradigm that enforces\nconsistent model performance across token groups with varying degrees of\nrelevance to auxiliary tokens. By adaptively upweighting underperforming\ngroups, typically those weakly correlated with auxiliary tokens, GDRT shifts\nthe model's attention from superficial auxiliary cues to informative user\ninteraction tokens, thereby mitigating language bias. Extensive experiments\nconducted on three public datasets demonstrate that GDRT effectively mitigates\nlanguage bias, yielding substantial improvements in recommendation accuracy\n(with an average NDCG@10 gain of 24.29%) and significantly enhancing\nrecommendation fairness."}
{"id": "2510.11056", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11056", "abs": "https://arxiv.org/abs/2510.11056", "authors": ["Runze Xia", "Yupeng Ji", "Yuxi Zhou", "Haodong Liu", "Teng Zhang", "Piji Li"], "title": "From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance", "comment": null, "summary": "Query-service relevance prediction in e-commerce search systems faces strict\nlatency requirements that prevent the direct application of Large Language\nModels (LLMs). To bridge this gap, we propose a two-stage reasoning\ndistillation framework to transfer reasoning capabilities from a powerful\nteacher LLM to a lightweight, deployment-friendly student model. In the first\nstage, we address the limitations of general-purpose LLMs by constructing a\ndomain-adapted teacher model. This is achieved through a three-step process:\ndomain-adaptive pre-training to inject platform knowledge, supervised\nfine-tuning to elicit reasoning skills, and preference optimization with a\nmulti-dimensional reward model to ensure the generation of reliable and\npreference-aligned reasoning paths. This teacher can then automatically\nannotate massive query-service pairs from search logs with both relevance\nlabels and reasoning chains. In the second stage, to address the challenges of\narchitectural heterogeneity in standard distillation, we introduce Contrastive\nReasoning Self-Distillation (CRSD). By modeling the behavior of the same\nstudent model under \"standard\" and \"reasoning-augmented\" inputs as a\nteacher-student relationship, CRSD enables the lightweight model to internalize\nthe teacher's complex decision-making mechanisms without needing the explicit\nreasoning path at inference. Offline evaluations and online A/B testing in the\nMeituan search advertising system demonstrate that our framework achieves\nsignificant improvements across multiple metrics, validating its effectiveness\nand practical value."}
{"id": "2510.11066", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11066", "abs": "https://arxiv.org/abs/2510.11066", "authors": ["Alin Fan", "Hanqing Li", "Sihan Lu", "Jingsong Yuan", "Jiandong Zhang"], "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction", "comment": null, "summary": "Modern industrial recommendation systems improve recommendation performance\nby integrating multimodal representations from pre-trained models into ID-based\nClick-Through Rate (CTR) prediction frameworks. However, existing approaches\ntypically adopt modality-centric modeling strategies that process ID-based and\nmultimodal embeddings independently, failing to capture fine-grained\ninteractions between content semantics and behavioral signals. In this paper,\nwe propose Decoupled Multimodal Fusion (DMF), which introduces a\nmodality-enriched modeling strategy to enable fine-grained interactions between\nID-based collaborative representations and multimodal representations for user\ninterest modeling. Specifically, we construct target-aware features to bridge\nthe semantic gap across different embedding spaces and leverage them as side\ninformation to enhance the effectiveness of user interest modeling.\nFurthermore, we design an inference-optimized attention mechanism that\ndecouples the computation of target-aware features and ID-based embeddings\nbefore the attention layer, thereby alleviating the computational bottleneck\nintroduced by incorporating target-aware features. To achieve comprehensive\nmultimodal integration, DMF combines user interest representations learned\nunder the modality-centric and modality-enriched modeling strategies. Offline\nexperiments on public and industrial datasets demonstrate the effectiveness of\nDMF. Moreover, DMF has been deployed on the product recommendation system of\nthe international e-commerce platform Lazada, achieving relative improvements\nof 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead."}
{"id": "2510.11100", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11100", "abs": "https://arxiv.org/abs/2510.11100", "authors": ["Shuwei Chen", "Jiajun Cui", "Zhengqi Xu", "Fan Zhang", "Jiangke Fan", "Teng Zhang", "Xingxing Wang"], "title": "HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction", "comment": "10 pages, 6 figures", "summary": "Click-through rate (CTR) prediction, which models behavior sequence and\nnon-sequential features (e.g., user/item profiles or cross features) to infer\nuser interest, underpins industrial recommender systems. However, most methods\nface three forms of heterogeneity that degrade predictive performance: (i)\nFeature Heterogeneity persists when limited sequence side features provide less\ngranular interest representation compared to extensive non-sequential features,\nthereby impairing sequence modeling performance; (ii) Context Heterogeneity\narises because a user's interest in an item will be influenced by other items,\nyet point-wise prediction neglects cross-item interaction context from the\nentire item set; (iii) Architecture Heterogeneity stems from the fragmented\nintegration of specialized network modules, which compounds the model's\neffectiveness, efficiency and scalability in industrial deployments. To tackle\nthe above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for\nmodeling sequential and set-wise contexts. First, we align sequence side\nfeatures with non-sequential features for accurate sequence modeling and\nfine-grained interest representation. Second, we shift the prediction paradigm\nfrom point-wise to set-wise, facilitating cross-item interaction in a highly\nparallel manner. Third, HoMer's unified encoder-decoder architecture achieves\ndual optimization through structural simplification and shared computation,\nensuring computational efficiency while maintaining scalability with model\nsize. Without arduous modification to the prediction pipeline, HoMer\nsuccessfully scales up and outperforms our industrial baseline by 0.0099 in the\nAUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%.\nAdditionally, HoMer saves 27% of GPU resources via preliminary engineering\noptimization, further validating its superiority and practicality."}
{"id": "2510.11122", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11122", "abs": "https://arxiv.org/abs/2510.11122", "authors": ["Tingqiao Xu", "Shaowei Yao", "Chenhe Dong", "Yiming Jin", "Zerui Huang", "Dan Ou", "Haihong Tang"], "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance", "comment": null, "summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet\nlong-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM\ncoverage. External context (reviews, attribute encyclopedias, UGC) can help but\nis noisy, and single-pass latency and cost forbid any clean-then-summarize\nstep. The model must, per query, judge relevance and decide whether to use,\npartially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG\nframework built on Group Relative Policy Optimization. It trains two rollout\ngroups (no external context vs a single retrieved chunk) and applies\nposterior-driven inter-group advantage scaling that adaptively reweights their\ncontributions by the per-query correctness gap. This teaches when to trust\nretrieval versus fall back to parametric knowledge, without process labels,\nvalue networks, or extra inference passes, preserving single-pass, single-chunk\ndeployment under production latency. Training combines: (1) supervised\ninitialization with a structured rationale that explicitly records the\ncontext-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus\nwhere context choice is most consequential; and (3) an optional lightweight DPO\nwarm start to stabilize with-context calibration. Under a unified\nretrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and\nvanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query\nGoodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's\nproduction relevance system, serving live traffic. To our knowledge, it is\namong the first single-pass RAG solutions for e-commerce relevance, turning\nnoisy external signals into reliable gains without added online complexity."}
{"id": "2510.11317", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11317", "abs": "https://arxiv.org/abs/2510.11317", "authors": ["Chen Gao", "Zixin Zhao", "Lv Shao", "Tong Liu"], "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender Systems by Modeling All-domain Movelines", "comment": null, "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender\nsystems, has been dominated by discriminative models that react to past user\nbehavior rather than proactively modeling user intent. Existing generative\nparadigms attempt to address this but suffer from critical limitations: Large\nLanguage Model (LLM) based methods create a semantic mismatch by forcing\ne-commerce signals into a linguistic space, while ID-based generation is\nconstrained by item memorization and cold-start issues. To overcome these\nlimitations, we propose a novel generative pre-training paradigm. Our model\nlearns to predict the Next Interest Flow, a dense vector sequence representing\na user's future intent, while simultaneously modeling its internal Interest\nDiversity and Interest Evolution Velocity to ensure the representation is both\nrich and coherent. However, this two-stage approach introduces a critical\nobjective mismatch between the generative and discriminative stages. We resolve\nthis via a bidirectional alignment strategy, which harmonizes the two stages\nthrough cross-stage weight initialization and a dynamic Semantic Alignment\nModule for fine-tuning. Additionally, we enhance the underlying discriminative\nmodel with a Temporal Sequential Pairwise (TSP) mechanism to better capture\ntemporal causality. We present the All-domain Moveline Evolution Network\n(AMEN), a unified framework implementing our entire pipeline. Extensive offline\nexperiments validate AMEN's superiority over strong baselines, and a\nlarge-scale online A/B test demonstrates its significant real-world impact,\ndelivering substantial improvements in key business metrics."}
{"id": "2510.11323", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11323", "abs": "https://arxiv.org/abs/2510.11323", "authors": ["Zhe Wang", "Yaming Yang", "Ziyu Guan", "Bin Tong", "Rui Wang", "Wei Zhao", "Hongbo Deng"], "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate Marketing", "comment": null, "summary": "In recent years, affiliate marketing has emerged as a revenue-sharing\nstrategy where merchants collaborate with promoters to promote their products.\nIt not only increases product exposure but also allows promoters to earn a\ncommission. This paper addresses the pivotal yet under-explored challenge in\naffiliate marketing: accurately assessing and predicting the contributions of\npromoters in product promotion. We design a novel metric for evaluating the\nindirect contributions of the promoter, called propagation scale.\nUnfortunately, existing time series forecasting techniques fail to deliver\naccurate predictions due to the propagation scale being influenced by multiple\nfactors and the inherent complexities arising from dynamic scenarios. To\naddress this issue, we decouple the network structure from the node signals and\npropose a two-stage solution: initially, the basic self-sales and network\nstructure prediction are conducted separately, followed by the synthesis of the\npropagation scale. Specifically, we design a graph convolution encoding scheme\nbased on descendant neighbors and incorporate hypergraph convolution to\nefficiently capture complex promotional dynamics. Additionally, three auxiliary\ntasks are employed: self-sales prediction for base estimations, descendant\nprediction to synthesize propagation scale, and promoter activation prediction\nto mitigate high volatility issues. Extensive offline experiments on\nlarge-scale industrial datasets validate the superiority of our method. We\nfurther deploy our model on Alimama platform with over $100,000$ promoters,\nachieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales\nvolume."}
{"id": "2510.11394", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11394", "abs": "https://arxiv.org/abs/2510.11394", "authors": ["Haosheng Qian", "Yixing Fan", "Jiafeng Guo", "Ruqing Zhang", "Qi Chen", "Dawei Yin", "Xueqi Cheng"], "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for\nenhancing the responses of large language models (LLMs) with external knowledge\nsources. Despite the impressive performance in complex question-answering\ntasks, RAG still struggles with hallucinations. Attributing RAG-generated\ncontent through in-line citations has demonstrated potential in reducing\nhallucinations and facilitating human verification. Existing citation\ngeneration methods primarily rely on either fine-tuning the generator or\nemploying post-processing approaches for citation matching. However, the former\napproach demands substantial annotated data and computational resources, while\nthe latter often encounters difficulties in managing multiple citations and\nfrequently produces suboptimal results. In this paper, we introduce a novel\nframework, called VeriCite, designed to rigorously validate supporting evidence\nand enhance answer attribution. Specifically, VeriCite breaks down into a\nthree-stage generation: 1) The initial answer generation first generates a\nresponse based on all available contexts and has its claims verified through\nthe NLI model; 2) the supporting evidence selection assesses the utility of\neach document and extracts useful supporting evidences; 3) the final answer\nrefinement integrates the initial response and collected evidences to produce\nthe final, refined answer.We conduct experiments across five open-source LLMs\nand four datasets, demonstrating that VeriCite can significantly improve\ncitation quality while maintaining the correctness of the answers."}
{"id": "2510.11402", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11402", "abs": "https://arxiv.org/abs/2510.11402", "authors": ["Gregor Meehan", "Johan Pauwels"], "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation", "comment": "Published at ACM RecSys 2025", "summary": "Collaborative filtering (CF) recommender systems struggle with making\npredictions on unseen, or 'cold', items. Systems designed to address this\nchallenge are often trained with supervision from warm CF models in order to\nleverage collaborative and content information from the available interaction\ndata. However, since they learn to replicate the behavior of CF methods,\ncold-start models may therefore also learn to imitate their predictive biases.\nIn this paper, we show that cold-start systems can inherit popularity bias, a\ncommon cause of recommender system unfairness arising when CF models overfit to\nmore popular items, thereby maximizing user-oriented accuracy but neglecting\nrarer items. We demonstrate that cold-start recommenders not only mirror the\npopularity biases of warm models, but are in fact affected more severely:\nbecause they cannot infer popularity from interaction data, they instead\nattempt to estimate it based solely on content features. This leads to\nsignificant over-prediction of certain cold items with similar content to\npopular warm items, even if their ground truth popularity is very low. Through\nexperiments on three multimedia datasets, we analyze the impact of this\nbehavior on three generative cold-start methods. We then describe a simple\npost-processing bias mitigation method that, by using embedding magnitude as a\nproxy for predicted popularity, can produce more balanced recommendations with\nlimited harm to user-oriented cold-start accuracy."}
{"id": "2510.11438", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11438", "abs": "https://arxiv.org/abs/2510.11438", "authors": ["Yujiang Wu", "Shanshan Zhong", "Yubin Kim", "Chenyan Xiong"], "title": "What Generative Search Engines Like and How to Optimize Web Content Cooperatively", "comment": null, "summary": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO$_\\text{API}$, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO."}
{"id": "2510.11483", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11483", "abs": "https://arxiv.org/abs/2510.11483", "authors": ["Heydar Soudani", "Hamed Zamani", "Faegheh Hasibi"], "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning", "comment": null, "summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of\nretrieval-augmented generation (RAG) that employs multiple reasoning steps for\nretrieval and generation. While effective for some complex queries, RAR remains\nvulnerable to errors and misleading outputs. Uncertainty quantification (UQ)\noffers methods to estimate the confidence of systems' outputs. These methods,\nhowever, often handle simple queries with no retrieval or single-step\nretrieval, without properly handling RAR setup. Accurate estimation of UQ for\nRAR requires accounting for all sources of uncertainty, including those arising\nfrom retrieval and generation. In this paper, we account for all these sources\nand introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ\nmethod for RAR. The core idea of R2C is to perturb the multi-step reasoning\nprocess by applying various actions to reasoning steps. These perturbations\nalter the retriever's input, which shifts its output and consequently modifies\nthe generator's input at the next step. Through this iterative feedback loop,\nthe retriever and generator continuously reshape one another's inputs, enabling\nus to capture uncertainty arising from both components. Experiments on five\npopular RAR systems across diverse QA datasets show that R2C improves AUROC by\nover 5% on average compared to the state-of-the-art UQ baselines. Extrinsic\nevaluations using R2C as an external signal further confirm its effectiveness\nfor two downstream tasks: in Abstention, it achieves ~5% gains in both\nF1Abstain and AccAbstain; in Model Selection, it improves the exact match by\n~7% over single models and ~3% over selection methods."}
{"id": "2510.11560", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11560", "abs": "https://arxiv.org/abs/2510.11560", "authors": ["Elisabeth Kirsten", "Jost Grosse Perdekamp", "Mihir Upadhyay", "Krishna P. Gummadi", "Muhammad Bilal Zafar"], "title": "Characterizing Web Search in The Age of Generative AI", "comment": null, "summary": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI."}
{"id": "2510.11589", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11589", "abs": "https://arxiv.org/abs/2510.11589", "authors": ["Shubham Chatterjee", "Jeff Dalton"], "title": "QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking", "comment": "Published in: Proceedings of the 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)", "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches\nleveraging knowledge graphs and multi-vector models capturing fine-grained\nsemantics. We introduce QDER, a neural re-ranking model that unifies these\napproaches by integrating knowledge graph semantics into a multi-vector model.\nQDER's key innovation lies in its modeling of query-document relationships:\nrather than computing similarity scores on aggregated embeddings, we maintain\nindividual token and entity representations throughout the ranking process,\nperforming aggregation only at the final scoring stage - an approach we call\n\"late aggregation.\" We first transform these fine-grained representations\nthrough learned attention patterns, then apply carefully chosen mathematical\noperations for precise matches. Experiments across five standard benchmarks\nshow that QDER achieves significant performance gains, with improvements of 36%\nin nDCG@20 over the strongest baseline on TREC Robust 2004 and similar\nimprovements on other datasets. QDER particularly excels on difficult queries,\nachieving an nDCG@20 of 0.70 where traditional approaches fail completely\n(nDCG@20 = 0.0), setting a foundation for future work in entity-aware\nretrieval."}
{"id": "2510.11592", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11592", "abs": "https://arxiv.org/abs/2510.11592", "authors": ["Shubham Chatterjee"], "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking", "comment": "To be published in: Proceedings of the 2025 Annual International ACM\n  SIGIR Conference on Research and Development in Information Retrieval in the\n  Asia Pacific Region (SIGIR-AP 2025)", "summary": "Current neural re-rankers often struggle with complex information needs and\nlong, content-rich documents. The fundamental issue is not computational--it is\nintelligent content selection: identifying what matters in lengthy,\nmulti-faceted texts. While humans naturally anchor their understanding around\nkey entities and concepts, neural models process text within rigid token\nwindows, treating all interactions as equally important and missing critical\nsemantic signals. We introduce REGENT, a neural re-ranking model that mimics\nhuman-like understanding by using entities as a \"semantic skeleton\" to guide\nattention. REGENT integrates relevance guidance directly into the attention\nmechanism, combining fine-grained lexical matching with high-level semantic\nreasoning. This relevance-guided attention enables the model to focus on\nconceptually important content while maintaining sensitivity to precise term\nmatches. REGENT achieves new state-of-the-art performance in three challenging\ndatasets, providing up to 108% improvement over BM25 and consistently\noutperforming strong baselines including ColBERT and RankVicuna. To our\nknowledge, this is the first work to successfully integrate entity semantics\ndirectly into neural attention, establishing a new paradigm for entity-aware\ninformation retrieval."}
{"id": "2510.11639", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11639", "abs": "https://arxiv.org/abs/2510.11639", "authors": ["Zhanyu Liu", "Shiyao Wang", "Xingmei Wang", "Rongzhou Zhang", "Jiaxin Deng", "Honghui Bao", "Jinghao Zhang", "Wuchao Li", "Pengfei Zheng", "Xiangyu Wu", "Yifei Hu", "Qigen Hu", "Xinchen Luo", "Lejian Ren", "Zixing Zhang", "Qianqian Wang", "Kuo Cai", "Yunfan Wu", "Hongtao Cheng", "Zexuan Cheng", "Lu Ren", "Huanjie Wang", "Yi Su", "Ruiming Tang", "Kun Gai", "Guorui Zhou"], "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation", "comment": null, "summary": "The powerful generative capacity of Large Language Models (LLMs) has\ninstigated a paradigm shift in recommendation. However, existing generative\nmodels (e.g., OneRec) operate as implicit predictors, critically lacking the\ncapacity for explicit and controllable reasoning-a key advantage of LLMs. To\nbridge this gap, we propose OneRec-Think, a unified framework that seamlessly\nintegrates dialogue, reasoning, and personalized recommendation. OneRec-Think\nincorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for\nsemantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate\nLLM reasoning within the recommendation context; and (3) Reasoning Enhancement,\nwhere we design a recommendation-specific reward function that accounts for the\nmulti-validity nature of user preferences. Experiments across public benchmarks\nshow state-of-the-art performance. Moreover, our proposed \"Think-Ahead\"\narchitecture enables effective industrial deployment on Kuaishou, achieving a\n0.159\\% gain in APP Stay Time and validating the practical efficacy of the\nmodel's explicit reasoning capability."}
{"id": "2510.11654", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11654", "abs": "https://arxiv.org/abs/2510.11654", "authors": ["Daniel Berhane Araya", "Duoduo Liao"], "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection", "comment": null, "summary": "Financial markets face growing threats from misinformation that can trigger\nbillions in losses in minutes. Most existing approaches lack transparency in\ntheir decision-making and provide limited attribution to credible sources. We\nintroduce FinVet, a novel multi-agent framework that integrates two\nRetrieval-Augmented Generation (RAG) pipelines with external fact-checking\nthrough a confidence-weighted voting mechanism. FinVet employs adaptive\nthree-tier processing that dynamically adjusts verification strategies based on\nretrieval confidence, from direct metadata extraction to hybrid reasoning to\nfull model-based analysis. Unlike existing methods, FinVet provides\nevidence-backed verdicts, source attribution, confidence scores, and explicit\nuncertainty flags when evidence is insufficient. Experimental evaluation on the\nFinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a\n10.4% improvement over the best individual pipeline (fact-check pipeline) and\n37% improvement over standalone RAG approaches."}
