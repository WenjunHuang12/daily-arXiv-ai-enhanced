{"id": "2510.14777", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.14777", "abs": "https://arxiv.org/abs/2510.14777", "authors": ["Sebastian Haslebacher", "Jonas Lill"], "title": "A Levelset Algorithm for 3D-Tarksi", "comment": null, "summary": "We present a simple new algorithm for finding a Tarski fixed point of a\nmonotone function $F : [N]^3 \\rightarrow [N]^3$. Our algorithm runs in\n$O(\\log^2 N)$ time and makes $O(\\log^2 N)$ queries to $F$, matching the\n$\\Omega(\\log^2 N)$ query lower bound due to Etessami et al.\\ as well as the\nexisting state-of-the-art algorithm due to Fearnley et al."}
{"id": "2510.14887", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14887", "abs": "https://arxiv.org/abs/2510.14887", "authors": ["Sizhe Li", "Nicolas Christianson", "Tongxin Li"], "title": "Prediction-Specific Design of Learning-Augmented Algorithms", "comment": null, "summary": "Algorithms with predictions} has emerged as a powerful framework to combine\nthe robustness of traditional online algorithms with the data-driven\nperformance benefits of machine-learned (ML) predictions. However, most\nexisting approaches in this paradigm are overly conservative, {as they do not\nleverage problem structure to optimize performance in a prediction-specific\nmanner}. In this paper, we show that such prediction-specific performance\ncriteria can enable significant performance improvements over the coarser\nnotions of consistency and robustness considered in prior work. Specifically,\nwe propose a notion of \\emph{strongly-optimal} algorithms with predictions,\nwhich obtain Pareto optimality not just in the worst-case tradeoff between\nrobustness and consistency, but also in the prediction-specific tradeoff\nbetween these metrics. We develop a general bi-level optimization framework\nthat enables systematically designing strongly-optimal algorithms in a wide\nvariety of problem settings, and we propose explicit strongly-optimal\nalgorithms for several classic online problems: deterministic and randomized\nski rental, and one-max search. Our analysis reveals new structural insights\ninto how predictions can be optimally integrated into online algorithms by\nleveraging a prediction-specific design. To validate the benefits of our\nproposed framework, we empirically evaluate our algorithms in case studies on\nproblems including dynamic power management and volatility-based index trading.\nOur results demonstrate that prediction-specific, strongly-optimal algorithms\ncan significantly improve performance across a variety of online\ndecision-making settings."}
{"id": "2510.14918", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.14918", "abs": "https://arxiv.org/abs/2510.14918", "authors": ["Hung Le", "Lazar Milenković", "Shay Solomon", "Cuong Than"], "title": "Tree-Like Shortcuttings of Trees", "comment": "Abstract shortened to meet arXiv limit", "summary": "Sparse shortcuttings of trees -- equivalently, sparse 1-spanners for tree\nmetrics with bounded hop-diameter -- have been studied extensively (under\ndifferent names and settings), since the pioneering works of [Yao82, Cha87,\nAS87, BTS94], initially motivated by applications to range queries, online tree\nproduct, and MST verification, to name a few. These constructions were also\nlifted from trees to other graph families using known low-distortion embedding\nresults. The works of [Yao82, Cha87, AS87, BTS94] establish a tight tradeoff\nbetween hop-diameter and sparsity (or average degree) for tree shortcuttings\nand imply constant-hop shortcuttings for $n$-node trees with sparsity $O(\\log^*\nn)$. Despite their small sparsity, all known constant-hop shortcuttings contain\ndense subgraphs (of sparsity $\\Omega(\\log n)$), which is a significant drawback\nfor many applications.\n  We initiate a systematic study of constant-hop tree shortcuttings that are\n``tree-like''. We focus on two well-studied graph parameters that measure how\nfar a graph is from a tree: arboricity and treewidth. Our contribution is\ntwofold.\n  * New upper and lower bounds for tree-like shortcuttings of trees, including\nan optimal tradeoff between hop-diameter and treewidth for all hop-diameter up\nto $O(\\log\\log n)$. We also provide a lower bound for larger values of $k$,\nwhich together yield $\\text{hop-diameter}\\times \\text{treewidth} =\n\\Omega((\\log\\log n)^2)$ for all values of hop-diameter, resolving an open\nquestion of [FL22, Le23]. [...]"}
{"id": "2510.14752", "categories": ["cs.GT", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.14752", "abs": "https://arxiv.org/abs/2510.14752", "authors": ["Javier Cembrano", "Jose Correa", "Svenja M. Griesbach", "Victor Verdugo"], "title": "Online Proportional Apportionment", "comment": null, "summary": "Traditionally, the problem of apportioning the seats of a legislative body\nhas been viewed as a one-shot process with no dynamic considerations. While\nthis approach is reasonable for some settings, dynamic aspects play an\nimportant role in many others. We initiate the study of apportionment problems\nin an online setting. Specifically, we introduce a framework for proportional\napportionment with no information about the future. In this model, time is\ndiscrete and there are $n$ parties that receive a certain share of the votes at\neach time step. An online algorithm needs to irrevocably assign a prescribed\nnumber of seats at each time, ensuring that each party receives its fractional\nshare rounded up or down, and that the cumulative number of seats allocated to\neach party remains close to its cumulative share up to that time.\n  We study deterministic and randomized online apportionment methods. For\ndeterministic methods, we construct a family of adversarial instances that\nyield a lower bound, linear in $n$, on the worst-case deviation between the\nseats allocated to a party and its cumulative share. We show that this bound is\nbest possible and is matched by a natural greedy method. As a consequence, a\nmethod guaranteeing that the cumulative number of seats assigned to each party\nup to any step equals its cumulative share rounded up or down (global quota)\nexists if and only if $n\\leq 3$. Then, we turn to randomized allocations and\nshow that, for $n\\leq 3$, we can randomize over methods satisfying global quota\nwith the additional guarantee that each party receives, in expectation, its\nproportional share in every step. Our proof is constructive: Any method\nsatisfying these properties can be obtained from a flow on a recursively\nconstructed network. We showcase the applicability of our results to obtain\napproximate solutions in the context of online dependent rounding procedures."}
{"id": "2510.14631", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.14631", "abs": "https://arxiv.org/abs/2510.14631", "authors": ["Uélison Jean Lopes dos Santos", "Alessandro Ferri", "Szilard Nistor", "Riccardo Tommasini", "Carsten Binnig", "Manisha Luthra"], "title": "Towards a Multimodal Stream Processing System", "comment": null, "summary": "In this paper, we present a vision for a new generation of multimodal\nstreaming systems that embed MLLMs as first-class operators, enabling real-time\nquery processing across multiple modalities. Achieving this is non-trivial:\nwhile recent work has integrated MLLMs into databases for multimodal queries,\nstreaming systems require fundamentally different approaches due to their\nstrict latency and throughput requirements. Our approach proposes novel\noptimizations at all levels, including logical, physical, and semantic query\ntransformations that reduce model load to improve throughput while preserving\naccuracy. We demonstrate this with \\system{}, a prototype leveraging such\noptimizations to improve performance by more than an order of magnitude.\nMoreover, we discuss a research roadmap that outlines open research challenges\nfor building a scalable and efficient multimodal stream processing systems."}
{"id": "2510.13846", "categories": ["cs.IT", "cs.AI", "cs.NE", "math.IT", "62M45, 68T05:, 68T10", "I.2.6; I.2.m"], "pdf": "https://arxiv.org/pdf/2510.13846", "abs": "https://arxiv.org/abs/2510.13846", "authors": ["Giuliano Armano"], "title": "Information flow in multilayer perceptrons: an in-depth analysis", "comment": ">30 pages, 8 figures", "summary": "Analysing how information flows along the layers of a multilayer perceptron\nis a topic of paramount importance in the field of artificial neural networks.\nAfter framing the problem from the point of view of information theory, in this\nposition article a specific investigation is conducted on the way information\nis processed, with particular reference to the requirements imposed by\nsupervised learning. To this end, the concept of information matrix is devised\nand then used as formal framework for understanding the aetiology of\noptimisation strategies and for studying the information flow. The underlying\nresearch for this article has also produced several key outcomes: i) the\ndefinition of a parametric optimisation strategy, ii) the finding that the\noptimisation strategy proposed in the information bottleneck framework shares\nstrong similarities with the one derived from the information matrix, and iii)\nthe insight that a multilayer perceptron serves as a kind of \"adaptor\", meant\nto process the input according to the given objective."}
{"id": "2510.14189", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.14189", "abs": "https://arxiv.org/abs/2510.14189", "authors": ["Tatsuro Banno", "Mizuki Takenawa", "Leslie Wöhler", "Satoshi Ikehata", "Kiyoharu Aizawa"], "title": "360CityGML: Realistic and Interactive Urban Visualization System Integrating CityGML Model and 360° Videos", "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "We introduce a novel urban visualization system that integrates 3D urban\nmodel (CityGML) and 360{\\deg} walkthrough videos. By aligning the videos with\nthe model and dynamically projecting relevant video frames onto the geometries,\nour system creates photorealistic urban visualizations, allowing users to\nintuitively interpret geospatial data from a pedestrian view."}
{"id": "2510.14450", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.14450", "abs": "https://arxiv.org/abs/2510.14450", "authors": ["François Durand"], "title": "Why Instant-Runoff Voting Is So Resilient to Coalitional Manipulation: Phase Transitions in the Perturbed Culture", "comment": null, "summary": "Previous studies have shown that Instant-Runoff Voting (IRV) is highly\nresistant to coalitional manipulation (CM), though the theoretical reasons for\nthis remain unclear. To address this gap, we analyze the susceptibility to CM\nof three major voting rules-Plurality, Two-Round System, and IRV-within the\nPerturbed Culture model. Our findings reveal that each rule undergoes a phase\ntransition at a critical value theta\\_c of the concentration of preferences:\nthe probability of CM for large electorates converges exponentially fast to 1\nbelow theta\\_c and to 0 above theta\\_c. We introduce the Super Condorcet Winner\n(SCW), showing that its presence is a key factor of IRV's resistance to\ncoalitional manipulation, both theoretically and empirically. Notably, we use\nthis notion to prove that for IRV, theta\\_c = 0, making it resistant to CM with\neven minimal preference concentration."}
{"id": "2510.14162", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14162", "abs": "https://arxiv.org/abs/2510.14162", "authors": ["Juhyeong Kim", "Yejin Kim", "Youngbin Lee", "Hyunwoo Byun"], "title": "FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API", "comment": "4 pages, 2 figures, accepted at CIKM 2025 FinAI Workshop", "summary": "We present FinAI Data Assistant, a practical approach for natural-language\nquerying over financial databases that combines large language models (LLMs)\nwith the OpenAI Function Calling API. Rather than synthesizing complete SQL via\ntext-to-SQL, our system routes user requests to a small library of vetted,\nparameterized queries, trading generative flexibility for reliability, low\nlatency, and cost efficiency. We empirically study three questions: (RQ1)\nwhether LLMs alone can reliably recall or extrapolate time-dependent financial\ndata without external retrieval; (RQ2) how well LLMs map company names to stock\nticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for\nend-to-end database query processing. Across controlled experiments on prices\nand fundamentals, LLM-only predictions exhibit non-negligible error and show\nlook-ahead bias primarily for stock prices relative to model knowledge cutoffs.\nTicker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high\nfor S\\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and\ncost and higher reliability than a text-to-SQL baseline on our task suite. We\ndiscuss design trade-offs, limitations, and avenues for deployment."}
{"id": "2510.13882", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13882", "abs": "https://arxiv.org/abs/2510.13882", "authors": ["Baigang Chen", "Dongfang Zhao"], "title": "Structure-Preserving Error-Correcting Codes for Polynomial Frames", "comment": null, "summary": "Modern FFT/NTT analytics, coded computation, and privacy-preserving ML\ninterface routinely move polynomial frames across NICs, storage, and\naccelerators. However, even rare silent data corruption (SDC) can flip a few\nring coefficients and cascade through downstream arithmetic. Conventional\ndefenses are ill-matched to current low-latency pipelines:\ndetect-and-retransmit adds RTTs, while byte-stream ECC ignores the algebraic\nstructure and forces format conversions. To that end, we propose a\nstructure-preserving reliability layer that operates in the encoded data's\noriginal polynomial ring, adds a small amount of systematic redundancy, and\ncorrects symbol errors/flagged erasures without round-trip or format changes.\nWe construct two complementary schemes: one for odd length $N_{odd}$ via a\nHensel-lifted BCH ideal with an idempotent encoder, and one for power-of-two\nlength $N_{2^m}$ via a repeated-root negacyclic code with derivative-style\ndecoding. In particular, to stay robust against clustered errors, a ring\nautomorphism provides in-place interleaving to disperse bursts. Implementation\nwise, on four frame sizes $N\\!=\\!1024, 2048, 4096, 8192$, we meet a per-frame\nfailure target of $10^{-9}$ at symbol error rates $10^{-6}\\text{--}10^{-5}$\nwith $t\\!=\\!8\\text{--}9$, incurring only $0.20\\%\\text{--}1.56\\%$ overhead and\ntolerating $\\sim\\!32\\text{--}72$\\,B unknown-error bursts (roughly doubled when\nflagged as erasures) after interleaving. By aligning error correction with ring\nsemantics, we take a practical step toward deployable robustness for\npolynomial-frame computations from an algebraic coding perspective."}
{"id": "2510.14427", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14427", "abs": "https://arxiv.org/abs/2510.14427", "authors": ["Ho Yin Au", "Jie Chen", "Junkun Jiang", "Jingyu Xiang"], "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation", "comment": "Accepted by NeurIPS 2025 (Oral)", "summary": "Recent research on motion generation has shown significant progress in\ngenerating semantically aligned motion with singular semantics. However, when\nemploying these models to create composite sequences containing multiple\nsemantically generated motion clips, they often struggle to preserve the\ncontinuity of motion dynamics at the transition boundaries between clips,\nresulting in awkward transitions and abrupt artifacts. To address these\nchallenges, we present Compositional Phase Diffusion, which leverages the\nSemantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module\n(TPDM) to progressively incorporate semantic guidance and phase details from\nadjacent motion clips into the diffusion process. Specifically, SPDM and TPDM\noperate within the latent motion frequency domain established by the\npre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them\nto learn semantically important and transition-aware phase information from\nvariable-length motion clips during training. Experimental results demonstrate\nthe competitive performance of our proposed framework in generating\ncompositional motion sequences that align semantically with the input\nconditions, while preserving phase transitional continuity between preceding\nand succeeding motion clips. Additionally, motion inbetweening task is made\npossible by keeping the phase parameter of the input motion sequences fixed\nthroughout the diffusion process, showcasing the potential for extending the\nproposed framework to accommodate various application scenarios. Codes are\navailable at https://github.com/asdryau/TransPhase."}
{"id": "2510.14555", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.14555", "abs": "https://arxiv.org/abs/2510.14555", "authors": ["Amal Sakr", "Andrea Araldo", "Tijani Chahed", "Daniel Kofman"], "title": "Co-Investment under Revenue Uncertainty Based on Stochastic Coalitional Game Theory", "comment": null, "summary": "The introduction of new services, such as Mobile Edge Computing (MEC),\nrequires a massive investment that cannot be assumed by a single stakeholder,\nfor instance the Infrastructure Provider (InP). Service Providers (SPs) however\nalso have an interest in the deployment of such services. We hence propose a\nco-investment scheme in which all stakeholders, i.e., the InP and the SPs, form\nthe so-called grand coalition composed of all the stakeholders with the aim of\nsharing costs and revenues and maximizing their payoffs. The challenge comes\nfrom the fact that future revenues are uncertain. We devise in this case a\nnovel stochastic coalitional game formulation which builds upon robust game\ntheory and derive a lower bound on the probability of the stability of the\ngrand coalition, wherein no player can be better off outside of it. In the\npresence of some correlated fluctuations of revenues however, stability can be\ntoo conservative. In this case, we make use also of profitability, in which\npayoffs of players are non-negative, as a necessary condition for\nco-investment. The proposed framework is showcased for MEC deployment, where\ncomputational resources need to be deployed in nodes at the edge of a\ntelecommunication network. Numerical results show high lower bound on the\nprobability of stability when the SPs' revenues are of similar magnitude and\nthe investment period is sufficiently long, even with high levels of\nuncertainty. In the case where revenues are highly variable however, the lower\nbound on stability can be trivially low whereas co-investment is still\nprofitable."}
{"id": "2510.14223", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14223", "abs": "https://arxiv.org/abs/2510.14223", "authors": ["Sudarshan Srinivasa Ramanujam", "Antonio Alonso", "Saurabh Kataria", "Siddharth Dangi", "Akhilesh Gupta", "Birjodh Singh Tiwana", "Manas Somaiya", "Luke Simon", "David Byrne", "Sojeong Ha", "Sen Zhou", "Andrei Akterskii", "Zhanglong Liu", "Samira Sriram", "Crescent Xiong", "Zhoutao Pei", "Angela Shao", "Alex Li", "Annie Xiao", "Caitlin Kolb", "Thomas Kistler", "Zach Moore", "Hamed Firooz"], "title": "Large Scale Retrieval for the LinkedIn Feed using Causal Language Models", "comment": "9 pages, 4 figures", "summary": "In large scale recommendation systems like the LinkedIn Feed, the retrieval\nstage is critical for narrowing hundreds of millions of potential candidates to\na manageable subset for ranking. LinkedIn's Feed serves suggested content from\noutside of the member's network (based on the member's topical interests),\nwhere 2000 candidates are retrieved from a pool of hundreds of millions\ncandidate with a latency budget of a few milliseconds and inbound QPS of\nseveral thousand per second. This paper presents a novel retrieval approach\nthat fine-tunes a large causal language model (Meta's LLaMA 3) as a dual\nencoder to generate high quality embeddings for both users (members) and\ncontent (items), using only textual input. We describe the end to end pipeline,\nincluding prompt design for embedding generation, techniques for fine-tuning at\nLinkedIn's scale, and infrastructure for low latency, cost effective online\nserving. We share our findings on how quantizing numerical features in the\nprompt enables the information to get properly encoded in the embedding,\nfacilitating greater alignment between the retrieval and ranking layer. The\nsystem was evaluated using offline metrics and an online A/B test, which showed\nsubstantial improvements in member engagement. We observed significant gains\namong newer members, who often lack strong network connections, indicating that\nhigh-quality suggested content aids retention. This work demonstrates how\ngenerative language models can be effectively adapted for real time, high\nthroughput retrieval in industrial applications."}
{"id": "2510.14226", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14226", "abs": "https://arxiv.org/abs/2510.14226", "authors": ["Xiao Zheng", "Wenchi Cheng", "Jingqing Wang", "Zhuohui Yao", "Jiangzhou Wang"], "title": "Location-Aided Distributed Beamforming for Near-Field Communications with Element-Wise RIS", "comment": "17 Pages", "summary": "Active reconfigurable intelligent surface (RIS) emerges as an effective\ntechnique to resist the double-fading attenuation of passive RIS. By embedding\nwith power harvesting function, it further evolves to zero-power active RIS,\nwhich can effectively enhance the flexibility of RIS deployment without\nexternal power demand. Nevertheless, existing works neglected the inherent\ndifficulty of channel estimation (CE) for RIS-assisted systems, and the\ndiscrete phase shift constraint in practical deployment. In this paper we\ndesign a new element-wise RIS architecture and propose a distributed\nlocation-aided transmission scheme with low complexity to enhance the reflected\ngain for channel state information (CSI)-limited RIS-assisted near-field\ncommunications. Specifically, the new element-wise RIS provides dynamic element\nselection capability with low hardware resources. Based on Fresnel diffraction\ntheory, we construct the mapping from locations in space-domain to phase\ndistributions of waves in phase-domain and reveal the priority of elements for\nharvesting and reflecting. {Then, the distributed beamforming design with the\nphase of determine-then-align is proposed, where the estimation overhead\nreduction stems from exempted requirements of RIS-associated CE at base station\n(BS).} The asymptotic analysis indicates that the proposed scheme can achieve\nthe optimal gain with a fixed proportion of reflective elements when RIS is\nlarge, followed by simulations to verify its superiority to other protocols."}
{"id": "2510.14645", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.14645", "abs": "https://arxiv.org/abs/2510.14645", "authors": ["Vignesh V Menon", "Adam Wieckowski", "Yiquin Liu", "Benjamin Bross", "Detlev Marpe"], "title": "Block-Partitioning Strategies for Accelerated Multi-rate Encoding in Adaptive VVC Streaming", "comment": "Picture Coding Symposium (PCS), 2025", "summary": "The demand for efficient multi-rate encoding techniques has surged with the\nincreasing prevalence of ultra-high-definition (UHD) video content,\nparticularly in adaptive streaming scenarios where a single video must be\nencoded at multiple bitrates to accommodate diverse network conditions. While\nVersatile Video Coding (VVC) significantly improves compression efficiency, it\nintroduces considerable computational complexity, making multi-rate encoding a\nresource-intensive task. This paper examines coding unit (CU) partitioning\nstrategies to minimize redundant computations in VVC while preserving high\nvideo quality. We propose single- and double-bound approaches, leveraging CU\ndepth constraints from reference encodes to guide dependent encodes across\nmultiple QPs. These methods are evaluated using VVenC with various presets,\ndemonstrating consistent improvements in encoding efficiency. Our methods\nachieve up to 11.69 % reduction in encoding time with minimal bitrate overhead\n(<0.6 %). Comparative Pareto-front (PF) analysis highlights the superior\nperformance of multi-rate approaches over existing configurations. These\nfindings validate the potential of CU-guided strategies for scalable multi-rate\nencoding in adaptive streaming."}
{"id": "2510.14642", "categories": ["cs.GT", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.14642", "abs": "https://arxiv.org/abs/2510.14642", "authors": ["Andrei Seoev", "Leonid Gremyachikh", "Anastasiia Smirnova", "Yash Madhwal", "Alisa Kalacheva", "Dmitry Belousov", "Ilia Zubov", "Aleksei Smirnov", "Denis Fedyanin", "Vladimir Gorgadze", "Yury Yanovich"], "title": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain", "comment": null, "summary": "In blockchain networks, the strategic ordering of transactions within blocks\nhas emerged as a significant source of profit extraction, known as Maximal\nExtractable Value (MEV). The transition from spam-based Priority Gas Auctions\nto structured auction mechanisms like Polygon Atlas has transformed MEV\nextraction from public bidding wars into sealed-bid competitions under extreme\ntime constraints. While this shift reduces network congestion, it introduces\ncomplex strategic challenges where searchers must make optimal bidding\ndecisions within a sub-second window without knowledge of competitor behavior\nor presence. Traditional game-theoretic approaches struggle in this\nhigh-frequency, partially observable environment due to their reliance on\ncomplete information and static equilibrium assumptions. We present a\nreinforcement learning framework for MEV extraction on Polygon Atlas and make\nthree contributions: (1) A novel simulation environment that accurately models\nthe stochastic arrival of arbitrage opportunities and probabilistic competition\nin Atlas auctions; (2) A PPO-based bidding agent optimized for real-time\nconstraints, capable of adaptive strategy formulation in continuous action\nspaces while maintaining production-ready inference speeds; (3) Empirical\nvalidation demonstrating our history-conditioned agent captures 49\\% of\navailable profits when deployed alongside existing searchers and 81\\% when\nreplacing the market leader, significantly outperforming static bidding\nstrategies. Our work establishes that reinforcement learning provides a\ncritical advantage in high-frequency MEV environments where traditional\noptimization methods fail, offering immediate value for industrial participants\nand protocol designers alike."}
{"id": "2510.14257", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14257", "abs": "https://arxiv.org/abs/2510.14257", "authors": ["Lingyu Mu", "Hao Deng", "Haibo Xing", "Kaican Lin", "Zhitong Zhu", "Yu Zhang", "Xiaoyi Zeng", "Zhengxiao Liu", "Zheng Lin", "Jinxin Hu"], "title": "Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation", "comment": null, "summary": "The integration of large language models (LLMs) into recommendation systems\nhas revealed promising potential through their capacity to extract world\nknowledge for enhanced reasoning capabilities. However, current methodologies\nthat adopt static schema-based prompting mechanisms encounter significant\nlimitations: (1) they employ universal template structures that neglect the\nmulti-faceted nature of user preference diversity; (2) they implement\nsuperficial alignment between semantic knowledge representations and behavioral\nfeature spaces without achieving comprehensive latent space integration. To\naddress these challenges, we introduce CoCo, an end-to-end framework that\ndynamically constructs user-specific contextual knowledge embeddings through a\ndual-mechanism approach. Our method realizes profound integration of semantic\nand behavioral latent dimensions via adaptive knowledge fusion and\ncontradiction resolution modules. Experimental evaluations across diverse\nbenchmark datasets and an enterprise-level e-commerce platform demonstrate\nCoCo's superiority, achieving a maximum 8.58% improvement over seven\ncutting-edge methods in recommendation accuracy. The framework's deployment on\na production advertising system resulted in a 1.91% sales growth, validating\nits practical effectiveness. With its modular design and model-agnostic\narchitecture, CoCo provides a versatile solution for next-generation\nrecommendation systems requiring both knowledge-enhanced reasoning and\npersonalized adaptation."}
{"id": "2510.14243", "categories": ["cs.IT", "cs.AI", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14243", "abs": "https://arxiv.org/abs/2510.14243", "authors": ["Caolu Xu", "Zhiyong Chen", "Meixia Tao", "Li Song", "Wenjun Zhang"], "title": "Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network", "comment": "submited to IEEE journal", "summary": "Immersive virtual reality (VR) applications impose stringent requirements on\nlatency, energy efficiency, and computational resources, particularly in\nmulti-user interactive scenarios. To address these challenges, we introduce the\nconcept of spatial computing communications (SCC), a framework designed to meet\nthe latency and energy demands of multi-user VR over distributed mobile edge\ncomputing (MEC) networks. SCC jointly represents the physical space, defined by\nusers and base stations, and the virtual space, representing shared immersive\nenvironments, using a probabilistic model of user dynamics and resource\nrequirements. The resource deployment task is then formulated as a\nmulti-objective combinatorial optimization (MOCO) problem that simultaneously\nminimizes system latency and energy consumption across distributed MEC\nresources. To solve this problem, we propose MO-CMPO, a multi-objective\nconsistency model with policy optimization that integrates supervised learning\nand reinforcement learning (RL) fine-tuning guided by preference weights.\nLeveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates\nPareto-optimal solutions. Simulations with real-world New Radio base station\ndatasets demonstrate that MO-CMPO achieves superior hypervolume performance and\nsignificantly lower inference latency than baseline methods. Furthermore, the\nanalysis reveals practical deployment patterns: latency-oriented solutions\nfavor local MEC execution to reduce transmission delay, while energy-oriented\nsolutions minimize redundant placements to save energy."}
{"id": "2510.14752", "categories": ["cs.GT", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.14752", "abs": "https://arxiv.org/abs/2510.14752", "authors": ["Javier Cembrano", "Jose Correa", "Svenja M. Griesbach", "Victor Verdugo"], "title": "Online Proportional Apportionment", "comment": null, "summary": "Traditionally, the problem of apportioning the seats of a legislative body\nhas been viewed as a one-shot process with no dynamic considerations. While\nthis approach is reasonable for some settings, dynamic aspects play an\nimportant role in many others. We initiate the study of apportionment problems\nin an online setting. Specifically, we introduce a framework for proportional\napportionment with no information about the future. In this model, time is\ndiscrete and there are $n$ parties that receive a certain share of the votes at\neach time step. An online algorithm needs to irrevocably assign a prescribed\nnumber of seats at each time, ensuring that each party receives its fractional\nshare rounded up or down, and that the cumulative number of seats allocated to\neach party remains close to its cumulative share up to that time.\n  We study deterministic and randomized online apportionment methods. For\ndeterministic methods, we construct a family of adversarial instances that\nyield a lower bound, linear in $n$, on the worst-case deviation between the\nseats allocated to a party and its cumulative share. We show that this bound is\nbest possible and is matched by a natural greedy method. As a consequence, a\nmethod guaranteeing that the cumulative number of seats assigned to each party\nup to any step equals its cumulative share rounded up or down (global quota)\nexists if and only if $n\\leq 3$. Then, we turn to randomized allocations and\nshow that, for $n\\leq 3$, we can randomize over methods satisfying global quota\nwith the additional guarantee that each party receives, in expectation, its\nproportional share in every step. Our proof is constructive: Any method\nsatisfying these properties can be obtained from a flow on a recursively\nconstructed network. We showcase the applicability of our results to obtain\napproximate solutions in the context of online dependent rounding procedures."}
{"id": "2510.14321", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14321", "abs": "https://arxiv.org/abs/2510.14321", "authors": ["Jianting Tang", "Dongshuai Li", "Tao Wen", "Fuyu Lv", "Dan Ou", "Linli Xu"], "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm", "comment": null, "summary": "In modern e-commerce search systems, dense retrieval has become an\nindispensable component. By computing similarities between query and item\n(product) embeddings, it efficiently selects candidate products from\nlarge-scale repositories. With the breakthroughs in large language models\n(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs\nfor more accurate text modeling. However, these models still adopt\ndirect-embedding methods, and the semantic accuracy of embeddings remains\ninadequate. Therefore, contrastive learning is heavily employed to achieve\ntight semantic alignment between positive pairs. Consequently, such models tend\nto capture statistical co-occurrence patterns in the training data, biasing\nthem toward shallow lexical and semantic matches. For difficult queries\nexhibiting notable lexical disparity from target items, the performance\ndegrades significantly. In this work, we propose the Large Reasoning Embedding\nModel (LREM), which novelly integrates reasoning processes into representation\nlearning. For difficult queries, LREM first conducts reasoning to achieve a\ndeep understanding of the original query, and then produces a\nreasoning-augmented query embedding for retrieval. This reasoning process\neffectively bridges the semantic gap between original queries and target items,\nsignificantly improving retrieval accuracy. Specifically, we adopt a two-stage\ntraining process: the first stage optimizes the LLM on carefully curated\nQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary\nreasoning and embedding capabilities, and the second stage further refines the\nreasoning trajectories via reinforcement learning (RL). Extensive offline and\nonline experiments validate the effectiveness of LREM, leading to its\ndeployment on China's largest e-commerce platform since August 2025."}
{"id": "2510.14290", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14290", "abs": "https://arxiv.org/abs/2510.14290", "authors": ["M. A. Teeti"], "title": "Reconfigurable Intelligent Surface-Enabled Channel Signature Modulation", "comment": "17 pages, 12 figures, journal", "summary": "This work proposes RIS-enabled channel signature modulation (RIS-CSM), a\nlightweight index modulation scheme for reconfigurable intelligent surfaces\n(RIS). An N-element RIS is partitioned into disjoint groups, each employing\npredetermined binary reflection patterns to generate distinct channel\nsignatures at an $n_R$-antenna receiver, without RIS-side beamforming.\nInformation is embedded in the indices of these signatures, enabling simple\nchannel estimation and scalable spectral efficiency. A closed-form upper bound\non error probability and capacity analysis are derived, revealing diversity\norder $n_R$ and coding gain proportional to N. Simulation results under\nRayleigh fading validate the theoretical analysis. Moreover, simulations\nindicate that spatial correlation among RIS elements can improve system\nperformance at low spectral efficiency."}
{"id": "2510.14907", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14907", "abs": "https://arxiv.org/abs/2510.14907", "authors": ["Geelon So", "Yi-An Ma"], "title": "Learnable Mixed Nash Equilibria are Collectively Rational", "comment": null, "summary": "We extend the study of learning in games to dynamics that exhibit\nnon-asymptotic stability. We do so through the notion of uniform stability,\nwhich is concerned with equilibria of individually utility-seeking dynamics.\nPerhaps surprisingly, it turns out to be closely connected to economic\nproperties of collective rationality. Under mild non-degeneracy conditions and\nup to strategic equivalence, if a mixed equilibrium is not uniformly stable,\nthen it is not weakly Pareto optimal: there is a way for all players to improve\nby jointly deviating from the equilibrium. On the other hand, if it is locally\nuniformly stable, then the equilibrium must be weakly Pareto optimal. Moreover,\nwe show that uniform stability determines the last-iterate convergence behavior\nfor the family of incremental smoothed best-response dynamics, used to model\nindividual and corporate behaviors in the markets. Unlike dynamics around\nstrict equilibria, which can stabilize to socially-inefficient solutions,\nindividually utility-seeking behaviors near mixed Nash equilibria lead to\ncollective rationality."}
{"id": "2510.14330", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14330", "abs": "https://arxiv.org/abs/2510.14330", "authors": ["Yuto Nakamizo", "Ryuhei Miyazato", "Hikaru Tanabe", "Ryuta Yamakura", "Kiori Hatanaka"], "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations", "comment": "5th place solution at Meta KDD Cup 2025", "summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta\nCRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question\nanswering (VQA) dataset focused on factual questions about images, including\negocentric images. The competition was contested based on VQA accuracy, as\njudged by an LLM-based automatic evaluator. Since incorrect answers result in\nnegative scores, our strategy focused on reducing hallucinations from the\ninternal representations of the VLM. Specifically, we trained logistic\nregression-based hallucination detection models using both the hidden_state and\nthe outputs of specific attention heads. We then employed an ensemble of these\nmodels. As a result, while our method sacrificed some correct answers, it\nsignificantly reduced hallucinations and allowed us to place among the top\nentries on the final leaderboard. For implementation details and code, please\nrefer to\nhttps://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit."}
{"id": "2510.14424", "categories": ["cs.IT", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14424", "abs": "https://arxiv.org/abs/2510.14424", "authors": ["Andrea Di Giusto", "Alberto Ravagnani"], "title": "The asymptotic number of equivalence classes of linear codes with given dimension", "comment": null, "summary": "We investigate the asymptotic number of equivalence classes of linear codes\nwith prescribed length and dimension. While the total number of inequivalent\ncodes of a given length has been studied previously, the case where the\ndimension varies as a function of the length has not yet been considered. We\nderive explicit asymptotic formulas for the number of equivalence classes under\nthree standard notions of equivalence, for a fixed alphabet size and increasing\nlength. Our approach also yields an exact asymptotic expression for the sum of\nall q-binomial coefficients, which is of independent interest and answers an\nopen question in this context. Finally, we establish a natural connection\nbetween these asymptotic quantities and certain discrete Gaussian distributions\narising from Brownian motion, providing a probabilistic interpretation of our\nresults."}
{"id": "2510.14626", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14626", "abs": "https://arxiv.org/abs/2510.14626", "authors": ["Zhibo Wu", "Yunfan Wu", "Quan Liu", "Lin Jiang", "Ping Yang", "Yao Hu"], "title": "GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation", "comment": null, "summary": "Multi-interest recommendation has gained attention, especially in industrial\nretrieval stage. Unlike classical dual-tower methods, it generates multiple\nuser representations instead of a single one to model comprehensive user\ninterests. However, prior studies have identified two underlying limitations:\nthe first is interest collapse, where multiple representations homogenize. The\nsecond is insufficient modeling of interest evolution, as they struggle to\ncapture latent interests absent from a user's historical behavior. We begin\nwith a thorough review of existing works in tackling these limitations. Then,\nwe attempt to tackle these limitations from a new perspective. Specifically, we\npropose a framework-level refinement for multi-interest recommendation, named\nGemiRec. The proposed framework leverages interest quantization to enforce a\nstructural interest separation and interest generation to learn the evolving\ndynamics of user interests explicitly. It comprises three modules: (a) Interest\nDictionary Maintenance Module (IDMM) maintains a shared quantized interest\ndictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a\ngenerative model to capture the distribution of user future interests. (c)\nMulti-Interest Retrieval Module (MIRM) retrieves items using multiple\nuser-interest representations. Both theoretical and empirical analyses, as well\nas extensive experiments, demonstrate its advantages and effectiveness.\nMoreover, it has been deployed in production since March 2025, showing its\npractical value in industrial applications."}
{"id": "2510.14574", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14574", "abs": "https://arxiv.org/abs/2510.14574", "authors": ["Jie Feng", "Zhenbing Liu", "Junjie Dai", "Hongbin Chen", "Fangjiong Chen"], "title": "Rotatable Antenna-Enhanced Beamforming: Signal Enhancement and Interference Suppression", "comment": null, "summary": "Conventional beamforming with fixed-orientation antenna (FOA) arrays may\nstruggle to effectively enhance signal and/or suppress interference due to\nsignificant variations in antenna directive gains over different steering\nangles. To break this limitation, we investigate in this paper the rotatable\nantenna (RA)-enhanced single/multi-beam forming by exploiting the new spatial\ndegrees of freedom (DoFs) via antennas' rotation optimization. Specifically,\nthe antenna rotation vector (ARV) and antenna weight vector (AWV) are jointly\noptimized to maximize the minimum array gain over signal directions, subject to\na given constraint on the maximum array gain over interference directions. For\nthe special case of single-beam forming without interference, the optimal ARV\nis derived in closed-form with the maximum ratio combining (MRC) beamformer\napplied to the AWV. For the general case of multi-beam forming, we propose an\nefficient alternating optimization (AO) algorithm to find a high-quality\nsuboptimal solution by iteratively optimizing one of the ARV and AWV with the\nother being fixed. Simulation results demonstrate that the proposed RA-based\nscheme can significantly outperform the traditional FOA-based and isotropic\nantenna (IA)-based schemes in terms of array gain."}
{"id": "2510.14629", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14629", "abs": "https://arxiv.org/abs/2510.14629", "authors": ["Jiani Huang", "Xingchen Zou", "Lianghao Xia", "Qing Li"], "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs", "comment": null, "summary": "The application of Large Language Models (LLMs) in recommender systems faces\nkey challenges in delivering deep personalization and intelligent reasoning,\nespecially for interactive scenarios. Current methods are often constrained by\nlimited context windows and single-turn reasoning, hindering their ability to\ncapture dynamic user preferences and proactively reason over recommendation\ncontexts. To address these limitations, we propose MR.Rec, a novel framework\nthat synergizes memory and reasoning for LLM-based recommendations. To achieve\npersonalization, we develop a comprehensive Retrieval-Augmented Generation\n(RAG) system that efficiently indexes and retrieves relevant external memory to\nenhance LLM personalization capabilities. Furthermore, to enable the synergy\nbetween memory and reasoning, our RAG system goes beyond conventional\nquery-based retrieval by integrating reasoning enhanced memory retrieval.\nFinally, we design a reinforcement learning framework that trains the LLM to\nautonomously learn effective strategies for both memory utilization and\nreasoning refinement. By combining dynamic memory retrieval with adaptive\nreasoning, this approach ensures more accurate, context-aware, and highly\npersonalized recommendations. Extensive experiments demonstrate that MR.Rec\nsignificantly outperforms state-of-the-art baselines across multiple metrics,\nvalidating its efficacy in delivering intelligent and personalized\nrecommendations. We will release code and data upon paper notification."}
{"id": "2510.14649", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14649", "abs": "https://arxiv.org/abs/2510.14649", "authors": ["Gyoseung Lee", "In-soo Kim", "Yonina C. Eldar", "A. Lee Swindlehurst", "Hyeongtaek Lee", "Minje Kim", "Junil Choi"], "title": "Task-Based Quantization for Channel Estimation in RIS Empowered MmWave Systems", "comment": "Accepted to IEEE Transactions on Communications", "summary": "In this paper, we investigate channel estimation for reconfigurable\nintelligent surface (RIS) empowered millimeter-wave (mmWave) multi-user\nsingle-input multiple-output communication systems using low-resolution\nquantization. Due to the high cost and power consumption of analog-to-digital\nconverters (ADCs) in large antenna arrays and for wide signal bandwidths,\ndesigning mmWave systems with low-resolution ADCs is beneficial. To tackle this\nissue, we propose a channel estimation design using task-based quantization\nthat considers the underlying hybrid analog and digital architecture in order\nto improve the system performance under finite bit-resolution constraints. Our\ngoal is to accomplish a channel estimation task that minimizes the mean squared\nerror distortion between the true and estimated channel. We develop two types\nof channel estimators: a cascaded channel estimator for an RIS with purely\npassive elements, and an estimator for the separate RIS-related channels that\nleverages additional information from a few semi-passive elements at the RIS\ncapable of processing the received signals with radio frequency chains.\nNumerical results demonstrate that the proposed channel estimation designs\nexploiting task-based quantization outperform purely digital methods and can\neffectively approach the performance of a system with unlimited resolution\nADCs. Furthermore, the proposed channel estimators are shown to be superior to\nbaselines with small training overhead."}
{"id": "2510.14641", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14641", "abs": "https://arxiv.org/abs/2510.14641", "authors": ["Zhibo Wu", "Yunfan Wu", "Lin Jiang", "Ping Yang", "Yao Hu"], "title": "Causality Enhancement for Cross-Domain Recommendation", "comment": null, "summary": "Cross-domain recommendation forms a crucial component in recommendation\nsystems. It leverages auxiliary information through source domain tasks or\nfeatures to enhance target domain recommendations. However, incorporating\ninconsistent source domain tasks may result in insufficient cross-domain\nmodeling or negative transfer. While incorporating source domain features\nwithout considering the underlying causal relationships may limit their\ncontribution to final predictions. Thus, a natural idea is to directly train a\ncross-domain representation on a causality-labeled dataset from the source to\ntarget domain. Yet this direction has been rarely explored, as identifying\nunbiased real causal labels is highly challenging in real-world scenarios. In\nthis work, we attempt to take a first step in this direction by proposing a\ncausality-enhanced framework, named CE-CDR. Specifically, we first reformulate\nthe cross-domain recommendation as a causal graph for principled guidance. We\nthen construct a causality-aware dataset heuristically. Subsequently, we derive\na theoretically unbiased Partial Label Causal Loss to generalize beyond the\nbiased causality-aware dataset to unseen cross-domain patterns, yielding an\nenriched cross-domain representation, which is then fed into the target model\nto enhance target-domain recommendations. Theoretical and empirical analyses,\nas well as extensive experiments, demonstrate the rationality and effectiveness\nof CE-CDR and its general applicability as a model-agnostic plugin. Moreover,\nit has been deployed in production since April 2025, showing its practical\nvalue in real-world applications."}
{"id": "2510.14843", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14843", "abs": "https://arxiv.org/abs/2510.14843", "authors": ["Ayman Zahr", "Gianluigi Liva"], "title": "Rate-Adaptive Spatially Coupled MacKay-Neal Codes with Thresholds Close to Capacity", "comment": "5 pages, 6 figures. Draft paper", "summary": "We analyze by density evolution the asymptotic performance of rate-adaptive\nMacKay-Neal (MN) code ensembles, where the inner code is a protograph spatially\ncoupled (SC) low-density parity-check code. By resorting to a suitably-defined\nparallel channel model, we compute belief propagation decoding thresholds,\nshowing that SC MN code ensembles can perform within 0.15 dB from the\nbinary-input additive white Gaussian noise capacity over the full [0,1] rate\nrange."}
{"id": "2510.14704", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14704", "abs": "https://arxiv.org/abs/2510.14704", "authors": ["Leonie Winter"], "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?", "comment": "69 pages, 14 figures", "summary": "Offline evaluations in recommender system research depend heavily on\ndatasets, many of which are pruned, such as the widely used MovieLens\ncollections. This thesis examines the impact of data pruning - specifically,\nremoving users with fewer than a specified number of interactions - on both\ndataset characteristics and algorithm performance. Five benchmark datasets were\nanalysed in both their unpruned form and at five successive pruning levels (5,\n10, 20, 50, 100). For each coreset, we examined structural and distributional\ncharacteristics and trained and tested eleven representative algorithms. To\nfurther assess if pruned datasets lead to artificially inflated performance\nresults, we also evaluated models trained on the pruned train sets but tested\non unpruned data. Results show that commonly applied core pruning can be highly\nselective, leaving as little as 2% of the original users in some datasets.\nTraditional algorithms achieved higher nDCG@10 scores when both training and\ntesting on pruned data; however, this advantage largely disappeared when\nevaluated on unpruned test sets. Across all algorithms, performance declined\nwith increasing pruning levels when tested on unpruned data, highlighting the\nimpact of dataset reduction on the performance of recommender algorithms."}
{"id": "2510.14856", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14856", "abs": "https://arxiv.org/abs/2510.14856", "authors": ["Ayman Zahr", "Emna Ben Yacoub", "Balázs Matuz", "Gianluigi Liva"], "title": "Rate-Adaptive Protograph-Based MacKay-Neal Codes", "comment": "Published on IEEE Transactions on Information Theory", "summary": "Rate-adaptive MacKay-Neal (MN) codes based on protographs are analyzed. The\ncode construction employs an outer distribution matcher (DM) to adapt the rate\nof the scheme. The DM is coupled with an inner protograph-based low-density\nparity-check (LDPC) code. The performance achievable by the resulting code\nstructure, that is nonlinear, is studied by means of an equivalent\ncommunication model that reduces the problem to the analysis of the inner\n(linear) LDPC code with transmission that takes place in parallel over the\ncommunication channel, and over a suitably defined binary symmetric channel. A\ndensity evolution analysis of protograph MN code ensembles is outlined, and it\nis complemented by an error floor analysis that relies on the derivation of the\naverage input-output weight distribution of the inner LDPC code ensemble.\nConditions on the shape of the normalized logarithmic asymptotic input-output\nweight distribution are defined, which allow discarding code ensembles with bad\nerror floor properties during the code design phase. Examples of code designs\nare provided, showing how the use of a single LDPC code ensemble allows\noperating within 1 dB from the Shannon limit over a wide range of code rates,\nwhere the code rate is selected by tuning the DM parameters. By enabling rate\nflexibility with a constant blocklength, and with a fixed LDPC code as inner\ncode, the construction provides an appealing solution for very high-throughput\nwireless (optical) links that employ binary-input modulations."}
{"id": "2510.14788", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14788", "abs": "https://arxiv.org/abs/2510.14788", "authors": ["Manjie Xu", "Cheng Chen", "Xin Jia", "Jingyi Zhou", "Yongji Wu", "Zejian Wang", "Chi Zhang", "Kai Zuo", "Yibo Chen", "Xu Tang", "Yao Hu", "Yixin Zhu"], "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale", "comment": "The dataset, code, and models will be released soon", "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms."}
{"id": "2510.14864", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14864", "abs": "https://arxiv.org/abs/2510.14864", "authors": ["Aobo Lyu", "Andrew Clark", "Netanel Raviv"], "title": "The Whole Is Less than the Sum of Parts: Subsystem Inconsistency in Partial Information Decomposition", "comment": null, "summary": "Partial Information Decomposition (PID) was proposed by Williams and Beer in\n2010 as a tool for analyzing fine-grained interactions between multiple random\nvariables, and has since found numerous applications ranging from neuroscience\nto privacy. However, a unified theoretical framework remains elusive due to key\nconceptual and technical challenges. We identify and illustrate a crucial\nproblem: PID violates the set-theoretic principle that the whole equals the sum\nof its parts (WESP). Through a counterexample in a three-variable system, we\ndemonstrate how such violations naturally arise, revealing a fundamental\nlimitation of current lattice-based PID frameworks. To address this issue, we\nintroduce a new axiomatic framework, termed System Information Decomposition\n(SID), specifically tailored for three-variable systems. SID resolves the WESP\nviolation by redefining the summation rules of decomposed information atoms\nbased on synergistic relationships. However, we further show that for systems\nwith four or more variables, no partial summation approach within the existing\nlattice-based structures can fully eliminate WESP inconsistencies. Our results\nthus highlight the inherent inadequacy of (antichain) lattice-based\ndecompositions for general multivariate systems."}
{"id": "2510.14857", "categories": ["cs.IR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.14857", "abs": "https://arxiv.org/abs/2510.14857", "authors": ["Gabriele Barlacchi", "Margherita Lalli", "Emanuele Ferragina", "Fosca Giannotti", "Luca Pappalardo"], "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems", "comment": "12 pages, 4 figures", "summary": "Recommender systems continuously interact with users, creating feedback loops\nthat shape both individual behavior and collective market dynamics. This paper\nintroduces a simulation framework to model these loops in online retail\nenvironments, where recommenders are periodically retrained on evolving\nuser-item interactions. Using the Amazon e-Commerce dataset, we analyze how\ndifferent recommendation algorithms influence diversity, purchase\nconcentration, and user homogenization over time. Results reveal a systematic\ntrade-off: while the feedback loop increases individual diversity, it\nsimultaneously reduces collective diversity and concentrates demand on a few\npopular items. Moreover, for some recommender systems, the feedback loop\nincreases user homogenization over time, making user purchase profiles\nincreasingly similar. These findings underscore the need for recommender\ndesigns that balance personalization with long-term diversity."}
{"id": "2510.14880", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14880", "abs": "https://arxiv.org/abs/2510.14880", "authors": ["Rikiya Takehi", "Benjamin Clavié", "Sean Lee", "Aamir Shakir"], "title": "Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report", "comment": null, "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different\nparameter counts: 17M and 32M. As part of our research, we conduct numerous\nexperiments to improve retrieval and late-interaction models, which we intend\nto distill into smaller models as proof-of-concepts. Our ultimate aim is to\nsupport retrieval at all scales, from large-scale retrieval which lives in the\ncloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a\nmodel that we hope will serve as a solid foundation backbone for all future\nexperiments, representing the first version of a long series of small\nproof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we\nconducted multiple ablation studies, of which we report the results. In terms\nof downstream performance, mxbai-edge-colbert-v0 is a particularly capable\nsmall model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and\nrepresenting a large step forward in long-context tasks, with unprecedented\nefficiency."}
