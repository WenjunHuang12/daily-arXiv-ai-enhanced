{"id": "2511.11649", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11649", "abs": "https://arxiv.org/abs/2511.11649", "authors": ["Jannik Nitschke"], "title": "The Environmental Impact of Ensemble Techniques in Recommender Systems", "comment": "Bachelor Thesis, University of Siegen", "summary": "Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.\n  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.\n  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.\n  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems."}
{"id": "2511.11653", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11653", "abs": "https://arxiv.org/abs/2511.11653", "authors": ["Duolin Sun", "Meixiu Long", "Dan Yang", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Junjie Wang", "Yue Shen", "Peng Wei", "Jian Wang", "Jinjie Gu"], "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning", "comment": null, "summary": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED."}
{"id": "2511.11847", "categories": ["cs.IR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11847", "abs": "https://arxiv.org/abs/2511.11847", "authors": ["Ryan Singh", "Austin Hamilton", "Amanda White", "Michael Wise", "Ibrahim Yousif", "Arthur Carvalho", "Zhe Shan", "Reza Abrisham Baf", "Mohammad Mayyas", "Lora A. Cavuoto", "Fadel M. Megahed"], "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches", "comment": "25 pages, 5 figures", "summary": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments."}
{"id": "2511.12004", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12004", "abs": "https://arxiv.org/abs/2511.12004", "authors": ["Ganlin Xu", "Zhitao Yin", "Linghao Zhang", "Jiaqing Liang", "Weijia Lu", "Xiaodong Zhang", "Zhifei Yang", "Sihang Jiang", "Deqing Yang"], "title": "ComLQ: Benchmarking Complex Logical Queries in Information Retrieval", "comment": "Accepted by AAAI 2026", "summary": "Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \\emph{complex logical queries} involving first-order logic operations such as conjunction ($\\land$), disjunction ($\\lor$), and negation ($\\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \\textbf{ComLQ} for \\textbf{Com}plex \\textbf{L}ogical \\textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \\emph{structure conformity} and \\emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \\textbf{Log-Scaled Negation Consistency} (\\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion."}
{"id": "2511.12057", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12057", "abs": "https://arxiv.org/abs/2511.12057", "authors": ["Ashwin Gerard Colaco", "Martin Boissier", "Sriram Rao", "Shubharoop Ghosh", "Sharad Mehrotra", "Tilmann Rabl"], "title": "GenIE - Simulator-Driven Iterative Data Exploration for Scientific Discovery", "comment": null, "summary": "Physics-based simulators play a critical role in scientific discovery and risk assessment, enabling what-if analyses for events like wildfires and hurricanes. Today, databases treat these simulators as external pre-processing steps. Analysts must manually run a simulation, export the results, and load them into a database before analysis can begin. This linear workflow is inefficient, incurs high latency, and hinders interactive exploration, especially when the analysis itself dictates the need for new or refined simulation data.\n  We envision a new database paradigm, entitled GenIE, that seamlessly integrates multiple simulators into databases to enable dynamic orchestration of simulation workflows. By making the database \"simulation-aware,\" GenIE can dynamically invoke simulators with appropriate parameters based on the user's query and analytical needs. This tight integration allows GenIE to avoid generating data irrelevant to the analysis, reuse previously generated data, and support iterative, incremental analysis where results are progressively refined at interactive speeds.\n  We present our vision for GenIE, designed as an extension to PostgreSQL, and demonstrate its potential benefits through comprehensive use cases: wildfire smoke dispersion analysis using WRF-SFIRE and HYSPLIT, and hurricane hazard assessment integrating wind, surge, and flood models. Our preliminary experiments show how GenIE can transform these slow, static analyses into interactive explorations by intelligently managing the trade-off between simulation accuracy and runtime across multiple integrated simulators. We conclude by highlighting the challenges and opportunities ahead in realizing the full vision of GenIE as a cornerstone for next-generation scientific data analysis."}
{"id": "2511.12072", "categories": ["cs.MM", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.12072", "abs": "https://arxiv.org/abs/2511.12072", "authors": ["Jiahui Sun", "Weining Wang", "Mingzhen Sun", "Yirong Yang", "Xinxin Zhu", "Jing Liu"], "title": "ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation", "comment": null, "summary": "Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency."}
{"id": "2511.12230", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.12230", "abs": "https://arxiv.org/abs/2511.12230", "authors": ["Neal E. Young"], "title": "An improved approximation algorithm for k-Median", "comment": null, "summary": "We give a polynomial-time approximation algorithm for the (not necessarily metric) $k$-Median problem. The algorithm is an $α$-size-approximation algorithm for $α< 1 + 2 \\ln(n/k)$. That is, it guarantees a solution having size at most $α\\times k$, and cost at most the cost of any size-$k$ solution. This is the first polynomial-time approximation algorithm to match the well-known bounds of $H_Δ$ and $1 + \\ln(n/k)$ for unweighted Set Cover (a special case) within a constant factor. It matches these bounds within a factor of 2. The algorithm runs in time $O(k m \\log(n/k) \\log m)$, where $n$ is the number of customers and $m$ is the instance size."}
{"id": "2511.12456", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.12456", "abs": "https://arxiv.org/abs/2511.12456", "authors": ["Sukanya Kudva", "Anil Aswani"], "title": "Collusion-proof Auction Design using Side Information", "comment": null, "summary": "We study the problem of auction design in the presence of bidder collusion. Specifically, we consider a multi-unit auction of identical items with single-minded bidders, where a subset of bidders may collude by coordinating bids and transferring payments and items among themselves. While the classical Vickrey-Clarke-Groves (VCG) mechanism achieves efficient and truthful outcomes, it is highly vulnerable to collusion. In contrast, fully collusion-proof mechanisms are limited to posted-price formats, which fail to guarantee even approximate efficiency. This paper aims to bridge this gap by designing auctions that achieve good welfare and revenue guarantees even when some bidders collude. We first characterize the strategic behavior of colluding bidders under VCG and prove that such bidders optimally bid shade: they never overbid or take additional items, but instead reduce the auction price. This characterization enables a Bulow-Klemperer type result: adding colluding bidders can only improve welfare and revenue relative to running VCG on the non-colluding group alone. We then propose a Hybrid VCG (H-VCG) mechanism that combines VCG applied to non-colluding bidders with a posted-price mechanism for colluding bidders, assuming access to a black-box collusion detection algorithm. We show that H-VCG is ex-post dominant-strategy incentive compatible (DSIC) and derive probabilistic guarantees on expected welfare and revenue under both known and unknown valuation distributions. Numerical experiments across several distributions demonstrate that H-VCG consistently outperforms VCG restricted to non-colluding bidders and approaches the performance of the ideal VCG mechanism assuming universal truthfulness. Our results provide a principled framework for incorporating collusion detection into mechanism design, offering a step toward collusion-resistant auctions."}
{"id": "2511.12108", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.12108", "abs": "https://arxiv.org/abs/2511.12108", "authors": ["Qianfan Wang", "Jifan Liang", "Peihong Yuan", "Ken R. Duffy", "Muriel Médard", "Xiao Ma"], "title": "Guessing Decoding of Short Blocklength Codes", "comment": null, "summary": "Future beyond-5G and 6G systems demand ultra-reliable, low-latency communication with short blocklengths, motivating the development of universal decoding algorithms. Guessing decoding, which infers the noise or codeword candidate in order of decreasing (exact or approximate) likelihood, offers a universal framework applicable to short codes. In this paper, we present a unified treatment of two prominent recent families of guessing decoding: guessing random additive noise decoding (GRAND) and guessing codeword decoding (GCD). For each, we (i) present algorithmic implementations and ordering strategies; (ii) prove maximum-likelihood (ML) optimality under appropriate stopping criteria; (iii) derive saddle-point approximations for the average number of queries; and (iv) validate theoretical predictions with simulations. We further analyze the performance degradation due to limited search budgets relative to ML performance, compare key metrics (worst-case and average complexity, hardware considerations), and highlight how advances in one approach transfer naturally to the other. Our results clarify the operating regimes where GRAND and GCD demonstrate superior performance. This work provides both theoretical insights and practical guidelines for deploying universal guessing decoders in next-generation short-blocklength communications."}
{"id": "2511.12081", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12081", "abs": "https://arxiv.org/abs/2511.12081", "authors": ["Bencheng Yan", "Yuejie Lei", "Zhiyuan Zeng", "Di Wang", "Kaiyi Lin", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction", "comment": null, "summary": "Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics."}
{"id": "2511.12457", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12457", "abs": "https://arxiv.org/abs/2511.12457", "authors": ["Gaurav Jain", "Brandon Baker", "Joe Yin", "Chenwei Xie", "Zihao Ye", "Sidh Kulkarni", "Sara Abdelrahman", "Nova Qi", "Urjeet Shrestha", "Mike Halcrow", "Dave Bailey", "Yuxiong He"], "title": "SEE++: Evolving Snowpark Execution Environment for Modern Workloads", "comment": "4 pages, 4 figures, accepted as a Poster at IEEE BigData 2025", "summary": "Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads."}
{"id": "2511.12404", "categories": ["cs.MM", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.12404", "abs": "https://arxiv.org/abs/2511.12404", "authors": ["Shail Desai", "Aditya Pawar", "Li Lin", "Xin Wang", "Shu Hu"], "title": "SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs", "comment": null, "summary": "Artificial Intelligence (AI) has made it possible for anyone to create images, audio, and video with unprecedented ease, enriching education, communication, and creative expression. At the same time, the rapid rise of AI-generated media has introduced serious risks, including misinformation, identity misuse, and the erosion of public trust as synthetic content becomes increasingly indistinguishable from real media. Although deepfake detection has advanced, many existing tools remain closed-source, limited in modality, or lacking transparency and educational value, making it difficult for users to understand how detection decisions are made. To address these gaps, we introduce SynthGuard, an open, user-friendly platform for detecting and analyzing AI-generated multimedia using both traditional detectors and multimodal large language models (MLLMs). SynthGuard provides explainable inference, unified image and audio support, and an interactive interface designed to make forensic analysis accessible to researchers, educators, and the public. The SynthGuard platform is available at: https://in-engr-nova.it.purdue.edu/"}
{"id": "2511.12714", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.12714", "abs": "https://arxiv.org/abs/2511.12714", "authors": ["George Z. Li", "Jason Li", "Satish Rao", "Junkai Zhang"], "title": "Shortcutting for Negative-Weight Shortest Path", "comment": "14-page STOC submission + new results in appendix", "summary": "Consider the single-source shortest paths problem on a directed graph with real-valued edge weights. We solve this problem in $O(n^{2.5}\\log^{4.5}n)$ time, improving on prior work of Fineman (STOC 2024) and Huang-Jin-Quanrud (SODA 2025, 2026) on dense graphs. Our main technique is an shortcutting procedure that iteratively reduces the number of negative-weight edges along shortest paths by a constant factor."}
{"id": "2511.12523", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12523", "abs": "https://arxiv.org/abs/2511.12523", "authors": ["Adam Dziwoki", "Rostislav Horcik"], "title": "Perturbing Best Responses in Zero-Sum Games", "comment": "Accepted to AAAI 2026", "summary": "This paper investigates the impact of perturbations on the best-response-based algorithms approximating Nash equilibria in zero-sum games, namely Double Oracle and Fictitious Play. More precisely, we assume that the oracle computing the best responses perturbs the utilities before selecting the best response. We show that using such an oracle reduces the number of iterations for both algorithms. For some cases, suitable perturbations ensure the expected number of iterations is logarithmic. Although the utility perturbation is computationally demanding as it requires iterating through all pure strategies, we demonstrate that one can efficiently perturb the utilities in games where pure strategies have further inner structure."}
{"id": "2511.12279", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.12279", "abs": "https://arxiv.org/abs/2511.12279", "authors": ["Shubhransh Singhvi", "Saransh Chopra", "K. V. Rashmi"], "title": "Tight Lower Bounds on the Bandwidth Cost of MDS Convertible Codes in the Split Regime", "comment": null, "summary": "Recent advances in erasure coding for distributed storage systems have demonstrated that adapting redundancy to varying disk failure rates can lead to substantial storage savings. Such adaptation requires code conversion, wherein data encoded under an initial $[k^I + r^I, k^I]$ code is transformed into data encoded under a final $[k^F + r^F, k^F]$ code - an operation that can be resource-intensive. Convertible codes are a class of codes designed to facilitate this transformation efficiently while preserving desirable properties such as the MDS property. In this work, we investigate the fundamental limits on the bandwidth cost of conversion (total amount of data transferred between the storage nodes during conversion) for systematic MDS convertible codes. Specifically, we study the subclass of conversions known as the split regime (a single initial codeword is converted into multiple final codewords).\n  In this setting, prior to this work, the best known lower bounds on the bandwidth cost of conversion for all parameters were derived by Maturana and Rashmi under certain uniformity assumptions on the number of symbols downloaded from each node. Further, these bounds were shown to be tight for the parameter regime where $r^F \\geq k^F$ or $r^I \\leq r^F$. In this work, we derive lower bounds on the bandwidth cost of systematic MDS convertible codes for all parameters in the split regime without the uniformity assumption. Moreover, our bounds are tight for the broader parameter regime where $r^F \\geq k^F$ or $r^I \\leq k^F$. Subsequently, our bounds also partially resolve the conjecture proposed by Maturana and Rashmi. We employ a novel information-theoretic framework, which assumes only that the initial and final codes are systematic and does not rely on any linearity assumptions or the aforementioned uniformity assumptions."}
{"id": "2511.12114", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12114", "abs": "https://arxiv.org/abs/2511.12114", "authors": ["Chengyi Liu", "Xiao Chen", "Shijie Wang", "Wenqi Fan", "Qing Li"], "title": "Continuous-time Discrete-space Diffusion Model for Recommendation", "comment": "Accepted by WSDM 2026", "summary": "In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency."}
{"id": "2511.13059", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.13059", "abs": "https://arxiv.org/abs/2511.13059", "authors": ["Johannes Wehrstein", "Roman Heinrich", "Mihail Stoian", "Skander Krid", "Martin Stemmer", "Andreas Kipf", "Carsten Binnig", "Muhammad El-Hindi"], "title": "Redbench: Workload Synthesis From Cloud Traces", "comment": null, "summary": "Workload traces from cloud data warehouse providers reveal that standard benchmarks such as TPC-H and TPC-DS fail to capture key characteristics of real-world workloads, including query repetition and string-heavy queries. In this paper, we introduce Redbench, a novel benchmark featuring a workload generator that reproduces real-world workload characteristics derived from traces released by cloud providers. Redbench integrates multiple workload generation techniques to tailor workloads to specific objectives, transforming existing benchmarks into realistic query streams that preserve intrinsic workload characteristics. By focusing on inherent workload signals rather than execution-specific metrics, Redbench bridges the gap between synthetic and real workloads. Our evaluation shows that (1) Redbench produces more realistic and reproducible workloads for cloud data warehouse benchmarking, and (2) Redbench reveals the impact of system optimizations across four commercial data warehouse platforms. We believe that Redbench provides a crucial foundation for advancing research on optimization techniques for modern cloud data warehouses."}
{"id": "2511.12854", "categories": ["cs.DS", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12854", "abs": "https://arxiv.org/abs/2511.12854", "authors": ["Alexander Lindermayr", "Kirk Pruhs", "Andréa W. Richa", "Tegan Wilson"], "title": "Indirect Coflow Scheduling", "comment": null, "summary": "We consider routing in reconfigurable networks, which is also known as coflow scheduling in the literature. The algorithmic literature generally (perhaps implicitly) assumes that the amount of data to be transferred is large. Thus the standard way to model a collection of requested data transfers is by an integer demand matrix $D$, where the entry in row $i$ and column $j$ of $D$ is an integer representing the amount of information that the application wants to send from machine/node $i$ to machine/node $j$. A feasible coflow schedule is then a sequence of matchings, which represent the sequence of data transfers that covers $D$. In this work, we investigate coflow scheduling when the size of some of the requested data transfers may be small relative to the amount of data that can be transferred in one round. fractional matchings and/or that employ indirect routing, and compare the relative utility of these options. We design algorithms that perform much better for small demands than the algorithms in the literature that were designed for large data transfers."}
{"id": "2511.12629", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12629", "abs": "https://arxiv.org/abs/2511.12629", "authors": ["Shiyun Lin"], "title": "Bandit Learning in Housing Markets", "comment": "Accepted to AAAI 2026 as oral", "summary": "The housing market, also known as one-sided matching market, is a classic exchange economy model where each agent on the demand side initially owns an indivisible good (a house) and has a personal preference over all goods. The goal is to find a core-stable allocation that exhausts all mutually beneficial exchanges among subgroups of agents. While this model has been extensively studied in economics and computer science due to its broad applications, little attention has been paid to settings where preferences are unknown and must be learned through repeated interactions. In this paper, we propose a statistical learning model within the multi-player multi-armed bandit framework, where players (agents) learn their preferences over arms (goods) from stochastic rewards. We introduce the notion of core regret for each player as the market objective. We study both centralized and decentralized approaches, proving $O(N \\log T / Δ^2)$ upper bounds on regret, where $N$ is the number of players, $T$ is the time horizon and $Δ$ is the minimum preference gap among players. For the decentralized setting, we also establish a matching lower bound, demonstrating that our algorithm is order-optimal."}
{"id": "2511.12430", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.12430", "abs": "https://arxiv.org/abs/2511.12430", "authors": ["Qi Wang", "Xiaoming Chen", "Qiao Qi", "Zhaolin Wang", "Yuanwei Liu"], "title": "Integration of Navigation and Remote Sensing in LEO Satellite Constellations", "comment": "IEEE Transactions on Communications, 2025", "summary": "Low earth orbit (LEO) satellite constellations are becoming a cornerstone of next-generation satellite networks, enabling worldwide high-precision navigation and high-quality remote sensing. This paper proposes a novel dual-function LEO satellite constellation frame structure that effectively integrating navigation and remote sensing. Then, the Cramer-Rao bound (CRB)-based positioning, velocity measurement, and timing (PVT) error and the signal-to-ambiguity-interference-noise ratio (SAINR) are derived as performance metrics for navigation and remote sensing, respectively. Based on it, a joint beamforming design is proposed by minimizing the average weighted PVT error for navigation user equipments (UEs) while ensuring SAINR requirement for remote sensing. Simulation results validate the proposed multi-satellite cooperative beamforming design, demonstrating its effectiveness as an integrated solution for next-generation multi-function LEO satellite constellations."}
{"id": "2511.12495", "categories": ["cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12495", "abs": "https://arxiv.org/abs/2511.12495", "authors": ["Zhen Tao", "Xinke Jiang", "Qingshuai Feng", "Haoyu Zhang", "Lun Du", "Yuchen Fang", "Hao Miao", "Bangquan Xie", "Qingqiang Sun"], "title": "Task-Aware Retrieval Augmentation for Dynamic Recommendation", "comment": "AAAI 2026", "summary": "Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities."}
{"id": "2511.13418", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13418", "abs": "https://arxiv.org/abs/2511.13418", "authors": ["Allaa Boutaleb", "Bernd Amann", "Rafael Angarita", "Hubert Naacke"], "title": "Exploring Multi-Table Retrieval Through Iterative Search", "comment": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025", "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval."}
{"id": "2511.13014", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13014", "abs": "https://arxiv.org/abs/2511.13014", "authors": ["Solon P. Pissis"], "title": "Maximal Palindromes in MPC: Simple and Optimal", "comment": "SOSA 2026", "summary": "In the classical longest palindromic substring (LPS) problem, we are given a string $S$ of length $n$, and the task is to output a longest palindromic substring in $S$. Gilbert, Hajiaghayi, Saleh, and Seddighin [SPAA 2023] showed how to solve the LPS problem in the Massively Parallel Computation (MPC) model in $\\mathcal{O}(1)$ rounds using $\\mathcal{\\widetilde{O}}(n)$ total memory, with $\\mathcal{\\widetilde{O}}(n^{1-ε})$ memory per machine, for any $ε\\in (0,0.5]$.\n  We present a simple and optimal algorithm to solve the LPS problem in the MPC model in $\\mathcal{O}(1)$ rounds. The total time and memory are $\\mathcal{O}(n)$, with $\\mathcal{O}(n^{1-ε})$ memory per machine, for any $ε\\in (0,0.5]$. A key attribute of our algorithm is its ability to compute all maximal palindromes in the same complexities. Furthermore, our new insights allow us to bypass the constraint $ε\\in (0,0.5]$ in the Adaptive MPC model. Our algorithms and the one proposed by Gilbert et al. for the LPS problem are randomized and succeed with high probability."}
{"id": "2511.12863", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12863", "abs": "https://arxiv.org/abs/2511.12863", "authors": ["Xi Zheng", "Yinghui Huang", "Xiangyu Chang", "Ruoxi Jia", "Yong Tan"], "title": "Rethinking Data Value: Asymmetric Data Shapley for Structure-Aware Valuation in Data Markets and Machine Learning Pipelines", "comment": null, "summary": "Rigorous valuation of individual data sources is critical for fair compensation in data markets, informed data acquisition, and transparent development of ML/AI models. Classical Data Shapley (DS) provides a essential axiomatic framework for data valuation but is constrained by its symmetry axiom that assumes interchangeability of data sources. This assumption fails to capture the directional and temporal dependencies prevalent in modern ML/AI workflows, including the reliance of duplicated or augmented data on original sources and the order-specific contributions in sequential pipelines such as federated learning and multi-stage LLM fine tuning. To address these limitations, we introduce Asymmetric Data Shapley (ADS), a structure-aware data valuation framework for modern ML/AI pipelines. ADS relaxes symmetry by averaging marginal contributions only over permutations consistent with an application-specific ordering of data groups. It preserves efficiency and linearity, maintains within group symmetry and directional precedence across groups, and reduces to DS when the ordering collapses to a single group. We develop two complementary computational procedures for ADS: (i) a Monte Carlo estimator (MC-ADS) with finite-sample accuracy guarantees, and (ii) a k-nearest neighbor surrogate (KNN-ADS) that is exact and efficient for KNN predictors. Across representative settings with directional and temporal dependence, ADS consistently outperforms benchmark methods by distinguishing novel from redundant contributions and respecting the sequential nature of training. These results establish ADS as a principled and practical approach to equitable data valuation in data markets and complex ML/AI pipelines."}
{"id": "2511.12469", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.12469", "abs": "https://arxiv.org/abs/2511.12469", "authors": ["Xuehui Dong", "Miyu Feng", "Chen Shao", "Bokai Lai", "Jianan Zhang", "Rujing Xiong", "Kai Wan", "Tiebin Mi", "Robert Caiming Qiu"], "title": "Metasurface-Enabled Superheterodyne Transmitter With Decoupled Harmonic-Free Signal Generation and Precoding", "comment": null, "summary": "The evolution of programmable metasurfaces (PM) from passive beamforming to active information transmission marks a paradigm shift for next-generation wireless systems. However, this transition is hindered by fundamental limitations in conventional metasurface transmitter architectures, including restricted modulation orders, symbol-level spatial inconsistency, and significant harmonic interference. These issues stem from the intrinsic coupling between baseband signal processing and radio-frequency beamforming in monolithic designs reliant on simplistic switching mechanisms. This paper proposes a novel metasurface-enabled superheterodyne architecture (MSA) that fundamentally decouples these functionalities. The MSA introduces a dual-stage up-conversion process, comprising a digital up-conversion module for in-phase/quadrature modulation and baseband-to-intermediate frequency conversion, a precoder module for precoding, and a custom-designed magnitude-phase-decoupled metasurface that acts as a reconfigurable reflective mixer array. This decoupling of harmonic-free waveform generation from spatial precoding overcomes the critical drawbacks of existing approaches. Experimental results from a 5.8 GHz proof-of-concept prototype system validate the MSA's superior performance. The system generates spatially isotropic constellations for arbitrary-order QAM modulations, ensures consistent time-frequency signatures for applications like Doppler-spoofing, and achieves data rates up to 20 Mbps within a linear operating region that minimizes nonlinear distortion. The capability of employing spatial diversity and multi-stream interference cancellation has been demonstrated for the first time in a PM-based transmitter."}
{"id": "2511.12518", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12518", "abs": "https://arxiv.org/abs/2511.12518", "authors": ["Zhongchao Yi", "Kai Feng", "Xiaojian Ma", "Yalong Wang", "Yongqi Liu", "Han Li", "Zhengyang Zhou", "Yang Wang"], "title": "DualGR: Generative Retrieval with Long and Short-Term Interests Modeling", "comment": null, "summary": "In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats \"exposed-but-unclicked\" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval."}
{"id": "2511.13205", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13205", "abs": "https://arxiv.org/abs/2511.13205", "authors": ["Pavel Arkhipov", "Vladimir Kolmogorov"], "title": "Greedy matroid base packings with applications to dynamic graph density and orientations", "comment": null, "summary": "Greedy minimum weight spanning tree packings have proven to be useful in connectivity-related problems. We study the process of greedy minimum weight base packings in general matroids and explore its algorithmic applications.\n  When specialized to bicircular matroids, our results yield an algorithm for the approximate fully-dynamic densest subgraph density $ρ$. We maintain a $(1+\\varepsilon)$-approximation of the density with a worst-case update time $O((ρ\\varepsilon^{-2}+\\varepsilon^{-4})ρ\\log^3 m)$. It improves the dependency on $\\varepsilon$ from the current state-of-the-art worst-case update time complexity $O(\\varepsilon^{-6}\\log^3 n\\logρ)$ [Chekuri, Christiansen, Holm, van der Hoog, Quanrud, Rotenberg, Schwiegelshohn, SODA'24]. We also can maintain an implicit fractional out-orientation with a guarantee that all out-degrees are at most $(1+\\varepsilon)ρ$.\n  Our algorithms above work by greedily packing pseudoforests, and require maintenance of a minimum-weight pseudoforest in a dynamically changing graph. We show that this problem can be solved in $O(\\log n)$ worst-case time per edge insertion or deletion.\n  For general matroids, we observe two characterizations of the limit of the base packings (``the vector of ideal loads''), which imply the characterizations from [Cen, Fleischmann, Li, Li, Panigrahi, FOCS'25], namely, their entropy-minimization theorem and their bottom-up cut hierarchy.\n  Finally, we give combinatorial results on the greedy tree packings. We show that a tree packing of $O(λ^5\\log m)$ trees contains a tree crossing some min-cut once, which improves the bound $O(λ^7\\log^3 m)$ from [Thorup, Combinatorica'07]. We also strengthen the lower bound on the edge load convergence rate from [de Vos, Christiansen, SODA'25], showing that Thorup's upper bound is tight up to a logarithmic factor."}
{"id": "2511.12879", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12879", "abs": "https://arxiv.org/abs/2511.12879", "authors": ["Ashish Kumar Perukari", "Polina Khoroshevskaya"], "title": "Resilient and Efficient Allocation for Large-Scale Autonomous Fleets via Decentralized Coordination", "comment": null, "summary": "Operating large autonomous fleets demands fast, resilient allocation of scarce resources (such as energy and fuel, charger access and maintenance slots, time windows, and communication bandwidth) under uncertainty. We propose a side-information-aware approach for resource allocation at scale that combines distributional predictions with decentralized coordination. Local side information shapes per-agent risk models for consumption, which are coupled through chance constraints on failures. A lightweight consensus-ADMM routine coordinates agents over a sparse communication graph, enabling near-centralized performance while avoiding single points of failure. We validate the framework on real urban road networks with autonomous vehicles and on a representative satellite constellation, comparing against greedy, no-side-information, and oracle central baselines. Our method reduces failure rates by 30-55% at matched cost and scales to thousands of agents with near-linear runtime, while preserving feasibility with high probability."}
{"id": "2511.12718", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.12718", "abs": "https://arxiv.org/abs/2511.12718", "authors": ["Yaniv Fogel", "Meir Feder"], "title": "Leave-One-Out Learning with Log-Loss", "comment": null, "summary": "We study batch learning with log-loss in the individual setting, where the outcome sequence is deterministic. Because empirical statistics are not directly applicable in this regime, obtaining regret guarantees for batch learning has long posed a fundamental challenge. We propose a natural criterion based on leave-one-out regret and analyze its minimax value for several hypothesis classes. For the multinomial simplex over $m$ symbols, we show that the minimax regret is $\\frac{m-1}{N} + o\\!\\left(\\frac{1}{N}\\right)$, and compare it to the stochastic realizable case where it is $\\frac{m-1}{2N} + o\\!\\left(\\frac{1}{N}\\right)$. More generally, we prove that every hypothesis class of VC dimension $d$ is learnable in the individual batch-learning problem, with regret at most $\\frac{d\\log(N)}{N} + o\\!\\left(\\frac{\\log(N)}{N}\\right)$, and we establish matching lower bounds for certain classes. We further derive additional upper bounds that depend on structural properties of the hypothesis class. These results establish, for the first time, that universal batch learning with log-loss is possible in the individual setting."}
{"id": "2511.12597", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12597", "abs": "https://arxiv.org/abs/2511.12597", "authors": ["Mengyao Gao", "Chongming Gao", "Haoyan Liu", "Qingpeng Cai", "Peng Jiang", "Jiajia Chen", "Shuai Yuan", "Xiangnan He"], "title": "MindRec: Mind-inspired Coarse-to-fine Decoding for Generative Recommendation", "comment": null, "summary": "Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose Mind-inspired Recommender (MindRec), a novel generative framework that emulates human thought processes. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling flexible and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\\% average improvement in top-1 recommendation performance over state-of-the-art methods, highlighting its potential to enhance recommendation accuracy. The implementation is available via https://github.com/Mr-Peach0301/MindRec."}
{"id": "2511.13301", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.13301", "abs": "https://arxiv.org/abs/2511.13301", "authors": ["Lisa Lehner", "Christian Komusiewicz", "Luca Pascal Staus"], "title": "A Complexity Analysis of the c-Closed Vertex Deletion Problem", "comment": null, "summary": "A graph is $c$-closed when every pair of nonadjacent vertices has at most $c-1$ common neighbors. In $c$-Closed Vertex Deletion, the input is a graph $G$ and an integer $k$ and we ask whether $G$ can be transformed into a $c$-closed graph by deleting at most $k$ vertices. We study the classic and parameterized complexity of $c$-Closed Vertex Deletion. We obtain, for example, NP-hardness for the case that $G$ is bipartite with bounded maximum degree. We also show upper and lower bounds on the size of problem kernels for the parameter $k$ and introduce a new parameter, the number $x$ of vertices in bad pairs, for which we show a problem kernel of size $\\mathcal{O}(x^3 + x^2\\cdot c))$. Here, a pair of nonadjacent vertices is bad if they have at least $c$ common neighbors. Finally, we show that $c$-Closed Vertex Deletion can be solved in polynomial time on unit interval graphs with depth at most $c+1$ and that it is fixed-parameter tractable with respect to the neighborhood diversity of $G$."}
{"id": "2511.13056", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13056", "abs": "https://arxiv.org/abs/2511.13056", "authors": ["Xin Huang", "Shengwei Zhou"], "title": "An FPTAS for 7/9-Approximation to Maximin Share Allocations", "comment": "29 pages, 6 figures", "summary": "We present a new algorithm that achieves a $\\frac{7}{9}$-approximation for the maximin share (MMS) allocation of indivisible goods under additive valuations, improving the current best ratio of $\\frac{10}{13}$ (Heidari et al., SODA 2026). Building on a new analytical framework, we further obtain an FPTAS that achieves a $\\frac{7}{9}-\\varepsilon$ approximation in $\\tfrac{1}{\\varepsilon} \\cdot \\mathrm{poly}(n,m)$ time. Compared with prior work (Heidari et al., SODA 2026), our algorithm is substantially simpler."}
{"id": "2511.12803", "categories": ["cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12803", "abs": "https://arxiv.org/abs/2511.12803", "authors": ["Yu-Han Huang", "Venugopal V. Veeravalli"], "title": "Finite-Horizon Quickest Change Detection Balancing Latency with False Alarm Probability", "comment": "27 pages, 1 figure, submitted to Sequential Analysis", "summary": "A finite-horizon variant of the quickest change detection (QCD) problem that is of relevance to learning in non-stationary environments is studied. The metric characterizing false alarms is the probability of a false alarm occurring before the horizon ends. The metric that characterizes the delay is \\emph{latency}, which is the smallest value such that the probability that detection delay exceeds this value is upper bounded to a predetermined latency level. The objective is to minimize the latency (at a given latency level), while maintaining a low false alarm probability. Under the pre-specified latency and false alarm levels, a universal lower bound on the latency, which any change detection procedure needs to satisfy, is derived. Change detectors are then developed, which are order-optimal in terms of the horizon. The case where the pre- and post-change distributions are known is considered first, and then the results are generalized to the non-parametric case when they are unknown except that they are sub-Gaussian with different means. Simulations are provided to validate the theoretical results."}
{"id": "2511.12922", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.NE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12922", "abs": "https://arxiv.org/abs/2511.12922", "authors": ["Yu Hou", "Won-Yong Shin"], "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation", "comment": "20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)", "summary": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines."}
{"id": "2511.13573", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13573", "abs": "https://arxiv.org/abs/2511.13573", "authors": ["Joseph", "Naor", "Nitya Raju", "Abhishek Shetty", "Aravind Srinivasan", "Renata Valieva", "David Wajc"], "title": "Dimension-Free Correlated Sampling for the Hypersimplex", "comment": "ITCS 2026", "summary": "Sampling from multiple distributions so as to maximize overlap has been studied by statisticians since the 1950s. Since the 2000s, such correlated sampling from the probability simplex has been a powerful building block in disparate areas of theoretical computer science. We study a generalization of this problem to sampling sets from given vectors in the hypersimplex, i.e., outputting sets of size (at most) some $k$ in $[n]$, while maximizing the sampled sets' overlap. Specifically, the expected difference between two output sets should be at most $α$ times their input vectors' $\\ell_1$ distance. A value of $α=O(\\log n)$ is known to be achievable, due to Chen et al.~(ICALP'17). We improve this factor to $O(\\log k)$, independent of the ambient dimension~$n$. Our algorithm satisfies other desirable properties, including (up to a $\\log^* n$ factor) input-sparsity sampling time, logarithmic parallel depth and dynamic update time, as well as preservation of submodular objectives. Anticipating broader use of correlated sampling algorithms for the hypersimplex, we present applications of our algorithm to online paging, offline approximation of metric multi-labeling and swift multi-scenario submodular welfare approximating reallocation."}
{"id": "2511.13080", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.13080", "abs": "https://arxiv.org/abs/2511.13080", "authors": ["Steven Landers", "Benjamin Marsh"], "title": "MEV in Multiple Concurrent Proposer Blockchains", "comment": null, "summary": "We analyze maximal extractable value in multiple concurrent proposer blockchains, where multiple blocks become data available before their final execution order is determined. This concurrency breaks the single builder assumption of sequential chains and introduces new MEV channels, including same tick duplicate steals, proposer to proposer auctions, and timing races driven by proof of availability latency. We develop a hazard normalized model of delay and inclusion, derive a closed form delay envelope \\(M(τ)\\), and characterize equilibria for censorship, duplication, and auction games. We show how deterministic priority DAG scheduling and duplicate aware payouts neutralize same tick MEV while preserving throughput, identifying simple protocol configurations to mitigate MCP specific extraction without centralized builders."}
{"id": "2511.13347", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13347", "abs": "https://arxiv.org/abs/2511.13347", "authors": ["Shuo Zheng", "Shuowen Zhang"], "title": "Joint Transmit Beamforming and Reflection Optimization for Beyond Diagonal RIS Aided Multi-Cell MIMO Communication", "comment": "submitted for possible publication", "summary": "The sixth-generation (6G) wireless networks will rely on ultra-dense multi-cell deployment to meet the high rate and connectivity demands. However, frequency reuse leads to severe inter-cell interference, particularly for cell-edge users, which limits the communication performance. To overcome this challenge, we investigate a beyond diagonal reconfigurable intelligent surface (BD-RIS) aided multi-cell multi-user downlink MIMO communication system, where a BD-RIS is deployed to enhance desired signals and suppress both intra-cell and inter-cell interference.We formulate the joint optimization problem of the transmit beamforming matrices at the BSs and the BD-RIS reflection matrix to maximize the weighted sum rate of all users, subject to the challenging unitary constraint of the BD-RIS reflection matrix and transmit power constraints at the BSs. To tackle this non-convex and difficult problem, we apply the weighted minimum mean squared error (WMMSE) method to transform the problem into an equivalent tractable form, and propose an efficient alternating optimization (AO) based algorithm to iteratively update the transmit beamforming and BD-RIS reflection using Lagrange duality theory and manifold optimization. Numerical results demonstrate the superiority of the proposed design over various benchmark schemes, and provide useful practical insights on the BD-RIS deployment strategy for multi-cell systems."}
{"id": "2511.12947", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12947", "abs": "https://arxiv.org/abs/2511.12947", "authors": ["Hao Jiang", "Guoquan Wang", "Sheng Yu", "Yang Zeng", "Wencong Zeng", "Guorui Zhou"], "title": "A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation", "comment": null, "summary": "Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items."}
{"id": "2511.13582", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13582", "abs": "https://arxiv.org/abs/2511.13582", "authors": ["Alfonso Cevallos", "Robert Hambrock", "Alistair Stewart"], "title": "The Merkle Mountain Belt", "comment": "36 pages, 15 figures, 5 tables, 1 algorithm", "summary": "Merkle structures are widely used as commitment schemes: they allow a prover to publish a compact commitment to an ordered list $X$ of items, and then efficiently prove to a verifier that $x_i\\in X$ is the $i$-th item in it. We compare different Merkle structures and their corresponding properties as commitment schemes in the context of blockchain applications. Our primary goal is to speed up light client protocols so that, e.g., a user can verify a transaction efficiently from their smartphone.\n  For instance, the Merkle Mountain Range (MMR) yields a succinct scheme: a light client synchronizing for the first time can do so with a complexity sublinear in $|X|$. On the other hand, the Merkle chain, traditionally used to commit to block headers, is not succinct, but it is incremental - a light client resynchronizing frequently can do so with constant complexity - and optimally additive - the structure can be updated in constant time when a new item is appended to list $X$.\n  We introduce new Merkle structures, most notably the Merkle Mountain Belt (MMB), the first to be simultaneously succinct, incremental and optimally additive. A variant called UMMB is also asynchronous: a light client may continue to interact with the network even when out of sync with the public commitment. Our Merkle structures are slightly unbalanced, so that items recently appended to $X$ receive shorter membership proofs than older items. This feature reduces a light client's expected costs, in applications where queries are biased towards recently generated data."}
{"id": "2511.13678", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.13678", "abs": "https://arxiv.org/abs/2511.13678", "authors": ["Haichuan Wang", "Yifan Wu", "Haifeng Xu"], "title": "The Publication Choice Problem", "comment": "Accepted by AAAI 2025 as an oral presentation", "summary": "Researchers strategically choose where to submit their work in order to maximize its impact, and these publication decisions in turn determine venues' impact factors. To analyze how individual publication choices both respond to and shape venue impact, we introduce a game-theoretic framework, coined the Publication Choice Problem, that captures this two-way interplay. We show the existence of a pure-strategy equilibrium in the Publication Choice Problem and its uniqueness under binary researcher types. Our characterizations of the equilibrium properties offer insights about what publication behaviors better indicate a researcher's impact level. Through equilibrium analysis, we further investigate how labeling papers with ``spotlight'' affects the impact factor of venues in the research community. Our analysis shows that competitive venue labeling top papers with ``spotlight'' may decrease the overall impact of other venues in the community, while less competitive venues with ``spotlight'' labeling have the opposite impact."}
{"id": "2511.13482", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13482", "abs": "https://arxiv.org/abs/2511.13482", "authors": ["Shenrui Lin", "Shuowen Zhang"], "title": "On the Capacity of Pixel Antenna based MIMO Communication", "comment": "submitted for possible publication", "summary": "Pixel antenna is a promising technology to enhance the wireless communication data rate by adaptively reconfiguring each antenna's radiation pattern via a so-called antenna coding technique which controls the states of switches connected to multiple pixel ports. This paper studies a multiple-input multiple-output (MIMO) system where both the transmitter and the receiver are equipped with multiple pixel antennas. We aim to characterize the fundamental capacity limit of this MIMO system by jointly optimizing the transmit covariance matrix and the antenna coders at both the transmitter and the receiver. This problem is a mixed-integer non-linear program (MINLP) which is non-convex and particularly challenging to solve due to the binary-valued optimization variables corresponding to the antenna coders. We first propose an exhaustive search based method to obtain the optimal solution to this problem, which corresponds to the fundamental capacity limit. Then, we propose a branch-and-bound based iterative algorithm aiming to find a high-quality suboptimal solution with lower complexity than exhaustive search as the number of pixel ports becomes large. Finally, we devise an alternating optimization (AO) based algorithm with polynomial complexity. Numerical results show that our proposed algorithms achieve a flexible trade-off between performance and complexity. Moreover, equipping the transceivers with pixel antennas can enhance the achievable rate of MIMO communications."}
{"id": "2511.12949", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12949", "abs": "https://arxiv.org/abs/2511.12949", "authors": ["Bokang Fu", "Jiahao Wang", "Xiaojing Liu", "Yuli Liu"], "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior", "comment": null, "summary": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.\n  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems."}
{"id": "2511.13605", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13605", "abs": "https://arxiv.org/abs/2511.13605", "authors": ["Niv Buchbinder", "Joseph", "Naor", "David Wajc"], "title": "Chasing Submodular Objectives, and Submodular Maximization via Cutting Planes", "comment": null, "summary": "We introduce the \\emph{submodular objectives chasing problem}, which generalizes many natural and previously-studied problems: a sequence of constrained submodular maximization problems is revealed over time, with both the objective and available ground set changing at each step. The goal is to maintain solutions of high approximation and low total \\emph{recourse} (number of changes), compared with exact offline algorithms for the same input sequence. For the central cardinality constraint and partition matroid constraints we provide polynomial-time algorithms achieving both optimal $(1-1/e-ε)$-approximation and optimal competitive recourse for \\emph{any} constant-approximation.\n  Key to our algorithm's polynomial time, and of possible independent interest, is a new meta-algorithm for $(1-1/e-ε)$-approximately maximizing the multilinear extension under general constraints, which we call {\\em approximate-or-separate}. Our algorithm relies on an improvement of the round-and-separate method [Gupta-Levin SODA'20], inspired by an earlier proof by [Vondrák, PhD~Thesis'07]. The algorithm, whose guarantees are similar to the influential {\\em continuous greedy} algorithm [Calinescu-Chekuri-Pál-Vondrák SICOMP'11], can use any cutting plane method and separation oracle for the constraints. This allows us to introduce cutting plane methods, used for exact unconstrained submodular minimization since the '80s [Grötschel/Lovász/Schrijver Combinatorica'81], as a useful method for (optimal approximate) constrained submodular maximization. We show further applications of this approach to static algorithms with curvature-sensitive approximation, and to communication complexity protocols."}
{"id": "2511.13601", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.13601", "abs": "https://arxiv.org/abs/2511.13601", "authors": ["Kai Wang"], "title": "A Deterministic Dimension Property of Twisted Goppa Codes", "comment": "This is the first version (v1) of an original research article. 7 pages, containing 1 primary data table", "summary": "This paper presents a large-scale computational study on the dimensional properties of twisted Goppa codes. Through the systematic analysis of over 50,000 parameter sets, we uncover a remarkable deterministic regularity: the actual dimension k of a twisted Goppa code is uniquely determined by a set of macro-parameters (q,m,t,b,u). Specifically, when the order of the finite field q, the extension degree m, the degree t of the Goppa polynomial, the translation parameter b of the automorphism, and the order u of the transformation are fixed, the dimension k of the generated code remains constant."}
{"id": "2511.12959", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12959", "abs": "https://arxiv.org/abs/2511.12959", "authors": ["Jaehyung Lim", "Wonbin Kweon", "Woojoo Kim", "Junyoung Kim", "Dongha Kim", "Hwanjo Yu"], "title": "Personalized Federated Recommendation With Knowledge Guidance", "comment": null, "summary": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg."}
{"id": "2511.13056", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13056", "abs": "https://arxiv.org/abs/2511.13056", "authors": ["Xin Huang", "Shengwei Zhou"], "title": "An FPTAS for 7/9-Approximation to Maximin Share Allocations", "comment": "29 pages, 6 figures", "summary": "We present a new algorithm that achieves a $\\frac{7}{9}$-approximation for the maximin share (MMS) allocation of indivisible goods under additive valuations, improving the current best ratio of $\\frac{10}{13}$ (Heidari et al., SODA 2026). Building on a new analytical framework, we further obtain an FPTAS that achieves a $\\frac{7}{9}-\\varepsilon$ approximation in $\\tfrac{1}{\\varepsilon} \\cdot \\mathrm{poly}(n,m)$ time. Compared with prior work (Heidari et al., SODA 2026), our algorithm is substantially simpler."}
{"id": "2511.13041", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13041", "abs": "https://arxiv.org/abs/2511.13041", "authors": ["Miaomiao Cai", "Min Hou", "Lei Chen", "Le Wu", "Haoyue Bai", "Yong Li", "Meng Wang"], "title": "Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning", "comment": null, "summary": "Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework."}
{"id": "2511.13057", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13057", "abs": "https://arxiv.org/abs/2511.13057", "authors": ["Satyanarayan Pati"], "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact", "comment": "16 pages, 9 figures, 1 table", "summary": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems."}
{"id": "2511.13166", "categories": ["cs.IR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13166", "abs": "https://arxiv.org/abs/2511.13166", "authors": ["Zhaoxin Shen", "Dan Wu"], "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users", "comment": "4 pages, 2 figures", "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs."}
{"id": "2511.13201", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13201", "abs": "https://arxiv.org/abs/2511.13201", "authors": ["Hao Hu", "Yifan Feng", "Ruoxue Li", "Rundong Xue", "Xingliang Hou", "Zhiqiang Tian", "Yue Gao", "Shaoyi Du"], "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation", "comment": "Accepted by AAAI 2026 main conference", "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches."}
{"id": "2511.13389", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13389", "abs": "https://arxiv.org/abs/2511.13389", "authors": ["Zhipeng Ma", "Bo Nørregaard Jørgensen", "Zheng Grace Ma"], "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference", "comment": "Accepted by the Energy Informatics.Academy Conference 2025 (EI.A 2025)", "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions."}
{"id": "2511.13415", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13415", "abs": "https://arxiv.org/abs/2511.13415", "authors": ["Wanqing Cui", "Wei Huang", "Yazhi Guo", "Yibo Hu", "Meiguang Jin", "Junfeng Ma", "Keping Bi"], "title": "Attention Grounded Enhancement for Visual Document Retrieval", "comment": null, "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025."}
{"id": "2511.13418", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13418", "abs": "https://arxiv.org/abs/2511.13418", "authors": ["Allaa Boutaleb", "Bernd Amann", "Rafael Angarita", "Hubert Naacke"], "title": "Exploring Multi-Table Retrieval Through Iterative Search", "comment": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025", "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval."}
{"id": "2511.13523", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13523", "abs": "https://arxiv.org/abs/2511.13523", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Salil Patil", "Swarup Patil", "Vijay Mago"], "title": "Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports", "comment": null, "summary": "Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization."}
