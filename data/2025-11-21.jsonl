{"id": "2511.16131", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16131", "abs": "https://arxiv.org/abs/2511.16131", "authors": ["Xuan-Quang Phan", "Tan-Ha Mai", "Thai-Duy Dinh", "Minh-Thuan Nguyen", "Lam-Son Lê"], "title": "AskDB: An LLM Agent for Natural Language Interaction with Relational Databases", "comment": "15 pages, 10 figures", "summary": "Interacting with relational databases remains challenging for users across different expertise levels, particularly when composing complex analytical queries or performing administrative tasks. Existing systems typically address either natural language querying or narrow aspects of database administration, lacking a unified and intelligent interface for general-purpose database interaction. We introduce AskDB, a large language model powered agent designed to bridge this gap by supporting both data analysis and administrative operations over SQL databases through natural language. Built on Gemini 2, AskDB integrates two key innovations: a dynamic schema-aware prompting mechanism that effectively incorporates database metadata, and a task decomposition framework that enables the agent to plan and execute multi-step actions. These capabilities allow AskDB to autonomously debug derived SQL, retrieve contextual information via real-time web search, and adaptively refine its responses. We evaluate AskDB on a widely used Text-to-SQL benchmark and a curated set of DBA tasks, demonstrating strong performance in both analytical and administrative scenarios. Our results highlight the potential of AskDB as a unified and intelligent agent for relational database systems, offering an intuitive and accessible experience for end users."}
{"id": "2511.16134", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16134", "abs": "https://arxiv.org/abs/2511.16134", "authors": ["Marijan Soric", "Cécile Gracianne", "Ioana Manolescu", "Pierre Senellart"], "title": "Benchmarking Table Extraction from Heterogeneous Scientific Extraction Documents", "comment": null, "summary": "Table Extraction (TE) consists in extracting tables from PDF documents, in a structured format which can be automatically processed. While numerous TE tools exist, the variety of methods and techniques makes it difficult for users to choose an appropriate one. We propose a novel benchmark for assessing end-to-end TE methods (from PDF to the final table). We contribute an analysis of TE evaluation metrics, and the design of a rigorous evaluation process, which allows scoring each TE sub-task as well as end-to-end TE, and captures model uncertainty. Along with a prior dataset, our benchmark comprises two new heterogeneous datasets of 37k samples. We run our benchmark on diverse models, including off-the-shelf libraries, software tools, large vision language models, and approaches based on computer vision. The results demonstrate that TE remains challenging: current methods suffer from a lack of generalizability when facing heterogeneous data, and from limitations in robustness and interpretability."}
{"id": "2511.16138", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16138", "abs": "https://arxiv.org/abs/2511.16138", "authors": ["Weiping Yu", "Ye Jiarui", "He Mengke", "Junfeng Liu", "Siqiang Luo"], "title": "On 10x Better Scalability: KV Stores Scale Up KV Cache", "comment": null, "summary": "Large language models (LLMs) rely on Key-Value (KV) cache to reduce time- to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge- tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch opera- tions and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management."}
{"id": "2511.16366", "categories": ["cs.DB", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.16366", "abs": "https://arxiv.org/abs/2511.16366", "authors": ["Gustavo Laranja Thomaello", "Thomaz Yeiden Busnardo Aguena", "Eric Trevelato Costa", "Rafael Baságlia Rosante", "Thiago Rodrigo Ramos", "Daiane Aparecida Zuanetti", "Edgar Dutra Zanotto"], "title": "From Patents to Dataset: Scraping for Oxide Glass Compositions and Properties", "comment": null, "summary": "In this work, we present web scraping techniques to extract in- formation from patent tables, clean and structure them for future use in predictive machine learning models to develop new glasses. We extracted compositions and three properties relevant to the development of new glasses and structured them into a database to be used together with information from other available datasets. We also analyzed the consistency of the information obtained and what it adds to the existing databases. The extracted liquidus temperatures comprise 5,696 compositions; the second subset includes 4,298 refractive indexes and, finally, 1,771 compositions with Abbe numbers. The extraction performed here increases the available information by approximately 10.4% for liquidus temperature, 6.6% for refractive index, and 4.9% for Abbe number. The impact extends beyond quantity: the newly extracted data introduce compositions with property values that are more diverse than those in existing databases, thereby expanding the accessible compositional and property space for glass modeling applications. We emphasize that the compositions of the new database contain relatively more titanium, magnesium, zirconium, niobium, iron, tin, and yttrium oxides than those of the existing bases."}
{"id": "2511.15727", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.15727", "abs": "https://arxiv.org/abs/2511.15727", "authors": ["Endre Csóka"], "title": "Prior-free Collusion-proof Dynamic Mechanisms", "comment": null, "summary": "For a general class of dynamic stochastic multi-player problems, Csóka, Liu, Rodivilov, and Teytelboym (2024) proposed prior-dependent mechanisms. The Guaranteed Utility Mechanism with transfers (TU-GUM) implements efficiency in a Guaranteed Utility Equilibrium (GUE). Its transfer-free variant (NTU-GUM) implements approximate efficiency in ε-GUE. In this paper, we define prior-free versions of both TU-GUM and NTU-GUM. As a special case, we believe that the new prior-free NTU-GUM implements a 1.283-approximation to Pareto efficiency for the repeated single good allocation problem in Fikioris, Banerjee, and Tardos (2024)."}
{"id": "2511.15860", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.15860", "abs": "https://arxiv.org/abs/2511.15860", "authors": ["Xusheng Zhu", "Kai-Kit Wong", "Boyi Tang", "Wen Chen", "Chan-Byoung Chae"], "title": "Fluid Reconfigurable Intelligent Surface (FRIS) Enabling Secure Wireless Communications", "comment": null, "summary": "The concept of fluid reconfigurable intelligent surface (FRIS) upgrades the conventional reconfigurable intelligent surface (RIS) paradigm by empowering its reflecting elements with positioning reconfigurability. This letter aims to investigate the use of FRIS to enhance physical-layer security in a system, in which a multi-antenna access point (AP) communicates with a legitimate user device in the presence of an eavesdropper. Unlike RIS with fixed-position elements, FRIS can dynamically select an optimal subset of elements from a larger array of candidate locations. We aim to maximize the secrecy rate by jointly optimizing the AP's transmit beamforming, the selection of FRIS activated elements, and their discrete phase shifts. The resulting problem is a challenging mixed-integer nonlinear program (MINLP), which is NP-hard. To address this, we propose an efficient algorithm based on an alternating optimization (AO) framework. Within this framework, the beamforming subproblem is optimally solved in closed form using the generalized eigenvalue method, while the combinatorial subproblem of joint element selection and discrete phase design is handled via the cross-entropy optimization (CEO) method. Simulation results show that the proposed FRIS design significantly outperforms the conventional RIS counterpart and other baselines, demonstrating the substantial security gains by element positioning as the new degree of freedom (DoF)."}
{"id": "2511.15996", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15996", "abs": "https://arxiv.org/abs/2511.15996", "authors": ["Amin Bigdeli", "Radin Hamidi Rad", "Mert Incesu", "Negar Arabzadeh", "Charles L. A. Clarke", "Ebrahim Bagheri"], "title": "QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation", "comment": "4 pages", "summary": "We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym."}
{"id": "2511.15742", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.15742", "abs": "https://arxiv.org/abs/2511.15742", "authors": ["Tonguç Ünlüyurt"], "title": "Sequential testing problem: A follow-up review", "comment": null, "summary": "This review aims to provide a comprehensive update on the progress made on the Sequential Testing problem (STP) in the last 20 years after the review, [1] was published. Many studies have provided new theoretical results, extensions of the problem, and new applications. In this review, we pinpoint the main results and discuss the relations between the problems studied. We also provide possible research directions for the problem."}
{"id": "2511.16455", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16455", "abs": "https://arxiv.org/abs/2511.16455", "authors": ["Pei Mu", "Anderson Chaves Carniel", "Antonio Barbalace", "Amir Shaikhha"], "title": "[Experiment, Analysis, and Benchmark] Systematic Evaluation of Plan-based Adaptive Query Processing", "comment": null, "summary": "Unreliable cardinality estimation remains a critical performance bottleneck in database management systems (DBMSs). Adaptive Query Processing (AQP) strategies address this limitation by providing a more robust query execution mechanism. Specifically, plan-based AQP achieves this by incrementally refining cardinality using feedback from the execution of sub-plans. However, the actual reason behind the improvements of plan-based AQP, especially across different storage architectures (on-disk vs. in-memory DBMSs), remains unexplored.\n  This paper presents the first comprehensive analysis of state-of-the-art plan-based AQP. We implement and evaluate this strategy on both on-disk and in-memory DBMSs across two benchmarks. Our key findings reveal that while plan-based AQP provides overall speedups in both environments, the sources of improvement differ significantly. In the on-disk DBMS, PostgreSQL, performance gains primarily come from the query plan reorderings, but not the cardinality updating mechanism; in fact, updating cardinalities introduces measurable overhead. Conversely, in the in-memory DBMS, DuckDB, cardinality refinement drives significant performance improvements for most queries. We also observe significant performance benefits of the plan-based AQP compared to a state-of-the-art related-based AQP method. These observations provide crucial insights for researchers on when and why plan-based AQP is effective, and ultimately guide database system developers on the tradeoffs between the implementation effort and performance improvements."}
{"id": "2511.16517", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.16517", "abs": "https://arxiv.org/abs/2511.16517", "authors": ["Holger I. Meinhardt"], "title": "Polynomial-Time Algorithms for Computing the Nucleolus: An Assessment", "comment": "32 pages, 1 figure, 1 table", "summary": "Recently, Maggiorano et al. (2025) claimed that they have developed a strongly polynomial-time combinatorial algorithm for the nucleolus in convex games that is based on the reduced game approach and submodular function minimization method. Thereby, avoiding the ellipsoid method with its negative side effects in numerical computation completely. However, we shall argue that this is a fallacy based on an incorrect application of the Davis/Maschler reduced game property (RGP). Ignoring the fact that despite the pre-nucleolus, other solutions like the core, pre-kernel, and semi-reactive pre-bargaining set possess this property as well. This causes a severe selection issue, leading to the failure to compute the nucleolus of convex games using the reduced games approach. In order to assess this finding in its context, the ellipsoid method of Faigle et al. (2001) and the Fenchel-Moreau conjugation-based approach from convex analysis of Meinhardt (2013) to compute a pre-kernel element were resumed. In the latter case, it was exploited that for TU games with a single-valued pre-kernel, both solution concepts coincide. Implying that one has computed the pre-nucleolus if one has found the sole pre-kernel element of the game. Though it is a specialized and highly optimized algorithm for the pre-kernel, it assures runtime complexity of O(n^3) for computing the pre-nucleolus whenever the pre-kernel is a single point, which indicates a polynomial-time algorithm for this class of games."}
{"id": "2511.16106", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16106", "abs": "https://arxiv.org/abs/2511.16106", "authors": ["Archish S", "Ankit Garg", "Kirankumar Shiragur", "Neeraj Kayal"], "title": "Incorporating Token Importance in Multi-Vector Retrieval", "comment": null, "summary": "ColBERT introduced a late interaction mechanism that independently encodes queries and documents using BERT, and computes similarity via fine-grained interactions over token-level vector representations. This design enables expressive matching while allowing efficient computation of scores, as the multi-vector document representations could be pre-computed offline. ColBERT models distance using a Chamfer-style function: for each query token, it selects the closest document token and sums these distances across all query tokens.\n  In our work, we explore enhancements to the Chamfer distance function by computing a weighted sum over query token contributions, where weights reflect the token importance. Empirically, we show that this simple extension, requiring only token-weight training while keeping the multi-vector representations fixed, further enhances the expressiveness of late interaction multi-vector mechanism. In particular, on the BEIR benchmark, our method achieves an average improvement of 1.28\\% in Recall@10 in the zero-shot setting using IDF-based weights, and 3.66\\% through few-shot fine-tuning."}
{"id": "2511.15849", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.15849", "abs": "https://arxiv.org/abs/2511.15849", "authors": ["Batya Kenig"], "title": "Connectivity-Preserving Important Separators: Enumeration and an Improved FPT Algorithm for Node Multiway Cut-Uncut", "comment": null, "summary": "We develop a framework for handling graph separation problems with connectivity constraints. Extending the classical concept of important separators, we introduce and analyze connectivity-preserving important separators, which are important separators that not only disconnect designated terminal sets $A$ and $B$ but also satisfy an arbitrary set of connectivity constraints over the terminals. These constraints can express requirements such as preserving the internal connectivity of each terminal set, enforcing pairwise connections defined by an equivalence relation, or maintaining reachability from a specified subset of vertices. We prove that for any graph $G=(V,E)$, terminal sets $A,B\\subseteq V$, and integer $k$, the number of important $A,B$-separators of size at most $k$ satisfying a set of connectivity constraints is bounded by $2^{O(k\\log k)}$, and that all such separators can be enumerated within $O(2^{O(k\\log k)} \\cdot n \\cdot T(n,m))$ time, where $T(n,m)$ is the time required to compute a minimum $s,t$-separator. As an application, we obtain a new fixed-parameter-tractable algorithm for the Node Multiway Cut-Uncut (N-MWCU) problem, parameterized by $k$, the size of the separator set. The algorithm runs in $O(2^{O(k\\log k)} \\cdot n \\cdot m^{1+o(1)})$ time for graphs with polynomially-bounded integer weights. This significantly improves the dependence on $k$ from the previous $2^{O(k^2\\log k)}$ to $2^{O(k\\log k)}$, thereby breaking a long-standing barrier, and simultaneously improves the polynomial factors. Our framework generalises the important-separator paradigm to separation problems in which the deletion set must satisfy both cut and uncut constraints on terminal subsets, thus offering a refined combinatorial foundation for designing fixed-parameter algorithms for cut-uncut problems in graphs."}
{"id": "2511.16044", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.16044", "abs": "https://arxiv.org/abs/2511.16044", "authors": ["Yiding Feng", "Rad Niazadeh", "Amin Saberi"], "title": "Robustness of Online Inventory Balancing to Inventory Shocks", "comment": "Preliminary version in the Web and Internet Economics (WINE 2025) conference; Journal version in Operations Research (major revision)", "summary": "In classic adversarial online resource allocation problems such as AdWords, customers arrive online while products are given offline with a fixed initial inventory. To ensure revenue guarantees under uncertainty, the decision maker must balance consumption across products. Based on this, the prevalent policy \"inventory balancing (IB)\" has proved to be optimal or near-optimal competitive in almost all classic settings. However, these models do not capture various forms of inventory shocks on the supply side, which play an important role in real-world online assortment and can significantly impact the revenue performance of the IB algorithm.\n  Motivated by this paradigm, we introduce a variant of online assortment planning with inventory shocks. Our model considers adversarial exogenous shocks (where supply increases unpredictably) and allocation-coupled endogenous shocks (where an inventory reduction is triggered by the algorithms and re-adjusted after a usage duration), whose combination leads to non-monotonic inventory fluctuations. As our main result, we show the robustness of IB-type strategies against such shocks by designing a new family of optimal competitive algorithms called \"Batched Inventory Balancing (BIB).\" Using a novel randomized primal-dual method, we bound the competitive ratio of BIB against optimal offline. We show that with proper choice of a certain parameter, this competitive ratio is asymptotically optimal and converges to (1-1/e) as initial inventories grow, in contrast to the original IB which no longer achieves the optimal ratio in this new model. Moreover, we characterize BIB's competitive ratio parametric by its penalty function and show that it matches exactly the competitive ratio of IB without shocks. Our refined analysis reduces the dual construction to a combinatorial \"interval assignment problem\" whose algorithmic solution may be of independent interest."}
{"id": "2511.16326", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16326", "abs": "https://arxiv.org/abs/2511.16326", "authors": ["Jiawei Zhou", "Hang Ding", "Haiyun Jiang"], "title": "ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning", "comment": "Under Review in ARR", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers."}
{"id": "2511.16023", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.16023", "abs": "https://arxiv.org/abs/2511.16023", "authors": ["Nadim A. Mottu"], "title": "Real Time Proportional Throughput Maximization: How much advance notice should you give your scheduler?", "comment": null, "summary": "We will be exploring a generalization of real time scheduling problem sometimes called the real time throughput maximization problem. Our input is a sequence of jobs specified by their release time, deadline and processing time. We assume that jobs are announced before or at their release time. At each time step, the algorithm must decide whether to schedule a job based on the information so far. The goal is to maximize the value of the sum of the processing times of jobs that finish before their deadline, or the total ``active'' time.\n  We extend this problem by defining a notion of $t$-advance-notice, a measure of how far in advance each job is given relative to their processing time. We show that there exists a $\\frac{t}{2t+1}$-competitive algorithm when all jobs have $t$-advance-notice for $t\\in [0,1]$. We also show that this ratio is optimal for all algorithms with $t$-advance-notice and that the upper bound of $\\frac{t}{2t+1}$-competitiveness holds for all $t$, in particular that regardless of how much advance-notice is given, no algorithm can reach $\\frac{1}{2}$-competitiveness."}
{"id": "2511.16414", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16414", "abs": "https://arxiv.org/abs/2511.16414", "authors": ["Hao Liu", "Le Wu", "Min Hou", "Han Wu", "Kun Zhang", "Xin Li", "Si Wei"], "title": "An Efficient LLM-based Evolutional Recommendation with Locate-Forget-Update Paradigm", "comment": null, "summary": "Nowadays, Large Language Models (LLMs) have shown exceptional performance in sequential recommendations, and the adoption of LLM-based recommender systems (LLMRec) is becoming increasingly widespread in existing e-commerce platforms. Despite the impressive performance, the constant high volume of new user-item interactions makes it difficult to adapt to the evolution of user preference over time, especially for LLM-based recommender systems. The challenge arises from the large number of parameters in LLMs, which makes traditional evolution methods (i.e., Re-training or Fine-tuning) impractical. Specifically, Re-training with all interactions results in prohibitively high computational costs. On the other hand, fine-tuning with only new interactions leads to preference forgetting among inactive users, ultimately compromising overall performance. To tackle this problem, we propose EvoRec, an efficient Locate-Forget-Update framework designed for LLM-based recommender systems to model the evolution of user preferences. EvoRec identifies a small set of parameters associated with preference changes and updates them precisely, thereby saving computational resources while maintaining strong recommendation performance. Notably, the modified parameters account for only 30\\% of LoRA adapter parameters, with no additional parameters introduced. Extensive experiments on two real-world datasets demonstrate that, compared to existing methods, EvoRec not only efficiently evolves LLMRec to adapt to the preferences of active users, but also preserves the interests of inactive users from being disturbed during evolution."}
{"id": "2511.16025", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.16025", "abs": "https://arxiv.org/abs/2511.16025", "authors": ["Amey Bhangale", "Arghya Chakraborty", "Prahladh Harsha"], "title": "Optimal Online Bipartite Matching in Degree-2 Graphs", "comment": null, "summary": "Online bipartite matching is a classical problem in online algorithms and we know that both the deterministic fractional and randomized integral online matchings achieve the same competitive ratio of $1-\\frac{1}{e}$. In this work, we study classes of graphs where the online degree is restricted to $2$. As expected, one can achieve a competitive ratio of better than $1-\\frac{1}{e}$ in both the deterministic fractional and randomized integral cases, but surprisingly, these ratios are not the same. It was already known that for fractional matching, a $0.75$ competitive ratio algorithm is optimal. We show that the folklore \\textsc{Half-Half} algorithm achieves a competitive ratio of $η\\approx 0.717772\\dots$ and more surprisingly, show that this is optimal by giving a matching lower-bound. This yields a separation between the two problems: deterministic fractional and randomized integral, showing that it is impossible to obtain a perfect rounding scheme."}
{"id": "2511.16478", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16478", "abs": "https://arxiv.org/abs/2511.16478", "authors": ["Elena V. Epure", "Yashar Deldjoo", "Bruno Sguerra", "Markus Schedl", "Manuel Moussallam"], "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation", "comment": "Under review with the ACM Transactions on Recommender Systems (TORS)", "summary": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.\n  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation."}
{"id": "2511.16044", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.16044", "abs": "https://arxiv.org/abs/2511.16044", "authors": ["Yiding Feng", "Rad Niazadeh", "Amin Saberi"], "title": "Robustness of Online Inventory Balancing to Inventory Shocks", "comment": "Preliminary version in the Web and Internet Economics (WINE 2025) conference; Journal version in Operations Research (major revision)", "summary": "In classic adversarial online resource allocation problems such as AdWords, customers arrive online while products are given offline with a fixed initial inventory. To ensure revenue guarantees under uncertainty, the decision maker must balance consumption across products. Based on this, the prevalent policy \"inventory balancing (IB)\" has proved to be optimal or near-optimal competitive in almost all classic settings. However, these models do not capture various forms of inventory shocks on the supply side, which play an important role in real-world online assortment and can significantly impact the revenue performance of the IB algorithm.\n  Motivated by this paradigm, we introduce a variant of online assortment planning with inventory shocks. Our model considers adversarial exogenous shocks (where supply increases unpredictably) and allocation-coupled endogenous shocks (where an inventory reduction is triggered by the algorithms and re-adjusted after a usage duration), whose combination leads to non-monotonic inventory fluctuations. As our main result, we show the robustness of IB-type strategies against such shocks by designing a new family of optimal competitive algorithms called \"Batched Inventory Balancing (BIB).\" Using a novel randomized primal-dual method, we bound the competitive ratio of BIB against optimal offline. We show that with proper choice of a certain parameter, this competitive ratio is asymptotically optimal and converges to (1-1/e) as initial inventories grow, in contrast to the original IB which no longer achieves the optimal ratio in this new model. Moreover, we characterize BIB's competitive ratio parametric by its penalty function and show that it matches exactly the competitive ratio of IB without shocks. Our refined analysis reduces the dual construction to a combinatorial \"interval assignment problem\" whose algorithmic solution may be of independent interest."}
{"id": "2511.16543", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16543", "abs": "https://arxiv.org/abs/2511.16543", "authors": ["Jiaheng Zhang", "Daqiang Zhang"], "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation", "comment": "11 pages,3 figures", "summary": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.\n  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.\n  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation."}
{"id": "2511.16094", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.16094", "abs": "https://arxiv.org/abs/2511.16094", "authors": ["Michael Dinitz", "Jeremy T. Fineman", "Seeun William Umboh"], "title": "Learning-Augmented Online Algorithms for Nonclairvoyant Joint Replenishment Problem with Deadlines", "comment": null, "summary": "This paper considers using predictions in the context of the online Joint Replenishment Problem with Deadlines (JRP-D). Prior work includes asymptotically optimal competitive ratios of $O(1)$ for the clairvoyant setting and $O(\\sqrt{n})$ of the nonclairvoyant setting, where $n$ is the number of items. The goal of this paper is to significantly reduce the competitive ratio for the nonclairvoyant case by leveraging predictions: when a request arrives, the true deadline of the request is not revealed, but the algorithm is given a predicted deadline.\n  The main result is an algorithm whose competitive ratio is $O(\\min(η^{1/3}\\log^{2/3}(n), \\sqrtη, \\sqrt{n}))$, where $n$ is the number of item types and $η\\leq n^2$ quantifies how flawed the predictions are in terms of the number of ``instantaneous item inversions.'' Thus, the algorithm is robust, i.e., it is never worse than the nonclairvoyant solution, and it is consistent, i.e., if the predictions exhibit no inversions, then the algorithm behaves similarly to the clairvoyant algorithm. Moreover, if the error is not too large, specifically $η< o(n^{3/2}/\\log^2(n))$, then the algorithm obtains an asymptotically better competitive ratio than the nonclairvoyant algorithm. We also show that all deterministic algorithms falling in a certain reasonable class of algorithms have a competitive ratio of $Ω(η^{1/3})$, so this algorithm is nearly the best possible with respect to this error metric."}
{"id": "2511.16576", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16576", "abs": "https://arxiv.org/abs/2511.16576", "authors": ["Alima Subedi", "Sankalpa Pokharel", "Satish Puri"], "title": "PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search", "comment": null, "summary": "Similarity searches are a critical task in data mining. As data sets grow larger, exact nearest neighbor searches quickly become unfeasible, leading to the adoption of approximate nearest neighbor (ANN) searches. ANN has been studied for text data, images, and trajectories. However, there has been little effort to develop ANN systems for polygons in spatial database systems and geographic information systems. We present PolyMinHash, a system for approximate polygon similarity search that adapts MinHashing into a novel 2D polygon-hashing scheme to generate short, similarity-preserving signatures of input polygons. Minhash is generated by counting the number of randomly sampled points needed before the sampled point lands within the polygon's interior area, yielding hash values that preserve area-based Jaccard similarity. We present the tradeoff between search accuracy and runtime of our PolyMinHash system. Our hashing mechanism reduces the number of candidates to be processed in the query refinement phase by up to 98% compared to the number of candidates processed by the brute-force algorithm."}
{"id": "2511.16100", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.16100", "abs": "https://arxiv.org/abs/2511.16100", "authors": ["Ken-ichi Kawarabayashi", "Hirotaka Yoneda", "Masataka Yoneda"], "title": "Online Graph Coloring for $k$-Colorable Graphs", "comment": null, "summary": "We study the problem of online graph coloring for $k$-colorable graphs. The best previously known deterministic algorithm uses $\\tilde{O}(n^{1-1/k!})$ colors for general $k$ and $\\tilde{O}(n^{5/6})$ colors for $k = 4$, both given by Kierstead in 1998. In this paper, nearly thirty years later, we have finally made progress. Our results are summarized as follows:\n  (1) $k \\geq 5$ case. We provide a deterministic online algorithm to color $k$-colorable graphs with $\\tilde{O}(n^{1-2/(k(k-1))})$ colors, significantly improving the current upper bound of $\\tilde{O}(n^{1-1/k!})$\n  (2) $k = 4$ case. We provide a deterministic online algorithm to color $4$-colorable graphs with $\\tilde{O}(n^{14/17})$ colors, improving the current upper bound of $\\tilde{O}(n^{5/6})$ colors.\n  (3) $k = 2$ case. We show that for randomized algorithms, the upper bound is $1.034 \\log_2 n + O(1)$ colors and the lower bound is $\\frac{91}{96} \\log_2 n - O(1)$ colors. This means that we close the gap to $1.09\\mathrm{x}$.\n  With our algorithm for the $k \\geq 5$ case, we also obtain a deterministic online algorithm for graph coloring that achieves a competitive ratio of $O(n / \\log \\log n)$, which improves the best known result of $O(n \\log \\log \\log n / \\log \\log n)$ by Kierstead.\n  For the bipartite graph case ($k = 2$), the limit of online deterministic algorithms is known: any deterministic algorithm requires $2 \\log_2 n - O(1)$ colors. Our results imply that randomized algorithms can perform slightly better but still have a limit."}
{"id": "2511.16356", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.16356", "abs": "https://arxiv.org/abs/2511.16356", "authors": ["Cheng Li", "Meihao Liao", "Rong-Hua Li", "Guoren Wang"], "title": "Scalable and Provable Kemeny Constant Computation on Static and Dynamic Graphs: A 2-Forest Sampling Approach", "comment": null, "summary": "Kemeny constant, defined as the expected hitting time of random walks from a source node to a randomly chosen target node, is a fundamental metric in graph data management with many real-world applications. However, computing it exactly on large graphs is highly challenging, as it requires inverting large graph matrices. Existing solutions mainly rely on approximate random-walk-based methods, which still need large sample sizes and lack strong theoretical guarantees. In this paper, we propose a new approach for approximating the Kemeny constant via 2-forest sampling. We first derive an unbiased estimator expressed through spanning trees by introducing a path mapping technique that establishes a direct correspondence between spanning trees and certain classes of 2-forests. Compared to random walk-based estimators, 2-forest-based estimators yield leads to a better theoretical bound. We further design efficient algorithms to sample and traverse spanning trees, leveraging data structures such as the Binary Indexed Tree (BIT) for optimization. Our theoretical analysis shows that the Kemeny constant can be approximated with relative error $ε$ in $O\\left(\\frac{Δ^2\\bar{d}^2}{ε^2}(τ+ n\\min(\\log n, Δ))\\right)$ time, where $τ$ is the tree-sampling time, $\\bar{d}$ is the average degree, and $Δ$ is the graph diameter. This complexity is near-linear in practice. Moreover, existing methods largely target static graphs and lack efficient mechanisms for dynamic updates. To address this, we propose two sample maintenance strategies that partially update samples while preserving accuracy on dynamic graphs. Extensive experiments on 10 large real-world datasets demonstrate that our method consistently outperforms state-of-the-art approaches in both efficiency and accuracy on static and dynamic graphs."}
{"id": "2511.16536", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.16536", "abs": "https://arxiv.org/abs/2511.16536", "authors": ["Alexander Armbruster", "Lars Rohwedder", "Andreas Wiese"], "title": "A $(2+\\varepsilon)$-approximation algorithm for the general scheduling problem in quasipolynomial time", "comment": "to appear in SODA 2026", "summary": "We study the general scheduling problem (GSP) which generalizes and unifies several well-studied preemptive single-machine scheduling problems, such as weighted flow time, weighted sum of completion time, and minimizing the total weight of tardy jobs. We are given a set of jobs with their processing times and release times and seek to compute a (possibly preemptive) schedule for them on one machine. Each job incurs a cost that depends on its completion time in the computed schedule, as given by a separate job-dependent cost function for each job, and our objective is to minimize the total resulting cost of all jobs. The best known result for GSP is a polynomial time $O(\\log\\log P)$-approximation algorithm [Bansal and Pruhs, FOCS 2010, SICOMP 2014].\n  We give a quasi-polynomial time $(2+ε)$-approximation algorithm for GSP, assuming that the jobs' processing times are quasi-polynomially bounded integers. For the special case of the weighted tardiness objective, we even obtain an improved approximation ratio of $1+ε$. For this case, no better result had been known than the mentioned $O(\\log\\log P)$-approximation for the general case of GSP. Our algorithms use a reduction to an auxiliary geometric covering problem. In contrast to a related reduction for the special case of weighted flow time [Rohwedder, Wiese, STOC 2021][Armbruster, Rohwedder, Wiese, STOC 2023] for GSP it seems no longer possible to establish a tree-like structure for the rectangles to guide an algorithm that solves this geometric problem. Despite the lack of structure due to the problem itself, we show that an optimal solution can be transformed into a near-optimal solution that has certain structural properties. Due to those we can guess a substantial part of the solution quickly and partition the remaining problem in an intricate way, such that we can independently solve each part recursively."}
{"id": "2511.16570", "categories": ["cs.DS", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.16570", "abs": "https://arxiv.org/abs/2511.16570", "authors": ["Angelo Farfan", "Mehrdad Ghadiri", "Junzhao Yang"], "title": "Entrywise Approximate Solutions for SDDM Systems in Almost-Linear Time", "comment": null, "summary": "We present an algorithm that given any invertible symmetric diagonally dominant M-matrix (SDDM), i.e., a principal submatrix of a graph Laplacian, $\\boldsymbol{\\mathit{L}}$ and a nonnegative vector $\\boldsymbol{\\mathit{b}}$, computes an entrywise approximation to the solution of $\\boldsymbol{\\mathit{L}} \\boldsymbol{\\mathit{x}} = \\boldsymbol{\\mathit{b}}$ in $\\tilde{O}(m n^{o(1)})$ time with high probability, where $m$ is the number of nonzero entries and $n$ is the dimension of the system."}
