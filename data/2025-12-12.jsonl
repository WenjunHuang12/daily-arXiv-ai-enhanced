{"id": "2512.10217", "categories": ["cs.DB", "cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.10217", "abs": "https://arxiv.org/abs/2512.10217", "authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Dan Suciu"], "title": "PANDAExpress: a Simpler and Faster PANDA Algorithm", "comment": null, "summary": "PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.\n  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\\tilde O(\\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power."}
{"id": "2512.10621", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.10621", "abs": "https://arxiv.org/abs/2512.10621", "authors": ["Siwoo Song", "Wonseok Shin", "Kunsoo Park", "Giuseppe F. Italiano", "Zhengyi Yang", "Wenjie Zhang"], "title": "Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint", "comment": null, "summary": "A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time."}
{"id": "2512.10354", "categories": ["cs.DS", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.10354", "abs": "https://arxiv.org/abs/2512.10354", "authors": ["Jihoon Jang", "Yehyun Nam", "Kunsoo Park", "Hyunjoon Kim"], "title": "Efficient Defective Clique Enumeration and Search with Worst-Case Optimal Search Space", "comment": "Accepted at SIGMOD 2026. This is the full version", "summary": "A $k$-defective clique is a relaxation of the traditional clique definition, allowing up to $k$ missing edges. This relaxation is crucial in various real-world applications such as link prediction, community detection, and social network analysis. Although the problems of enumerating maximal $k$-defective cliques and searching a maximum $k$-defective clique have been extensively studied, existing algorithms suffer from limitations such as the combinatorial explosion of small partial solutions and sub-optimal search spaces. To address these limitations, we propose a novel clique-first branch-and-bound framework that first generates cliques and then adds missing edges. Furthermore, we introduce a new pivoting technique that achieves a search space size of $\\mathcal{O}(3^{\\frac{n}{3}} \\cdot n^k)$, where $n$ is the number of vertices in the input graph. We prove that the worst-case number of maximal $k$-defective cliques is $Ω(3^{\\frac{n}{3}} \\cdot n^k)$ when $k$ is a constant, establishing that our algorithm's search space is worst-case optimal. Leveraging the diameter-two property of defective cliques, we further reduce the search space size to $\\mathcal{O}(n \\cdot 3^{\\fracδ{3}} \\cdot (δΔ)^k)$, where $δ$ is the degeneracy and $Δ$ is the maximum degree of the input graph. We also propose an efficient framework for maximum $k$-defective clique search based on our branch-and-bound, together with practical techniques to reduce the search space. Experiments on real-world benchmark datasets with more than 1 million edges demonstrate that each of our proposed algorithms for maximal $k$-defective clique enumeration and maximum $k$-defective clique search outperforms the respective state-of-the-art algorithms by up to four orders of magnitude in terms of processing time."}
{"id": "2512.10149", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10149", "abs": "https://arxiv.org/abs/2512.10149", "authors": ["Han Chen", "Steven Zhu", "Yingrui Li"], "title": "STARS: Semantic Tokens with Augmented Representations for Recommendation at Scale", "comment": null, "summary": "Real-world ecommerce recommender systems must deliver relevant items under strict tens-of-milliseconds latency constraints despite challenges such as cold-start products, rapidly shifting user intent, and dynamic context including seasonality, holidays, and promotions. We introduce STARS, a transformer-based sequential recommendation framework built for large-scale, low-latency ecommerce settings. STARS combines several innovations: dual-memory user embeddings that separate long-term preferences from short-term session intent; semantic item tokens that fuse pretrained text embeddings, learnable deltas, and LLM-derived attribute tags, strengthening content-based matching, long-tail coverage, and cold-start performance; context-aware scoring with learned calendar and event offsets; and a latency-conscious two-stage retrieval pipeline that performs offline embedding generation and online maximum inner-product search with filtering, enabling tens-of-milliseconds response times. In offline evaluations on production-scale data, STARS improves Hit@5 by more than 75 percent relative to our existing LambdaMART system. A large-scale A/B test on 6 million visits shows statistically significant lifts, including Total Orders +0.8%, Add-to-Cart on Home +2.0%, and Visits per User +0.5%. These results demonstrate that combining semantic enrichment, multi-intent modeling, and deployment-oriented design can yield state-of-the-art recommendation quality in real-world environments without sacrificing serving efficiency."}
{"id": "2512.09941", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09941", "abs": "https://arxiv.org/abs/2512.09941", "authors": ["Fatemeh Ghasemi", "Swastik Kopparty"], "title": "Fourier Sparsity of Delta Functions and Matching Vector PIRs", "comment": "Full version. Accepted to ITCS 2026", "summary": "In this paper we study a basic and natural question about Fourier analysis of Boolean functions, which has applications to the study of Matching Vector based Private Information Retrieval (PIR) schemes. For integers m and r, define a delta function on {0,1}^r to be a function f: Z_m^r -> C with f(0) = 1 and f(x) = 0 for all nonzero Boolean x. The basic question we study is how small the Fourier sparsity of a delta function can be; namely how sparse such an f can be in the Fourier basis?\n  In addition to being intrinsically interesting and natural, such questions arise naturally when studying \"S-decoding polynomials\" for the known matching vector families. Finding S-decoding polynomials of reduced sparsity, which corresponds to finding delta functions with low Fourier sparsity, would improve the current best PIR schemes.\n  We show nontrivial upper and lower bounds on the Fourier sparsity of delta functions. Our proofs are elementary and clean. These results imply limitations on improving Matching Vector PIR schemes simply by finding better S-decoding polynomials. In particular, there are no S-decoding polynomials that can make Matching Vector PIRs based on the known matching vector families achieve polylogarithmic communication with a constant number of servers. Many interesting questions remain open."}
{"id": "2512.10388", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10388", "abs": "https://arxiv.org/abs/2512.10388", "authors": ["Ziwei Liu", "Yejing Wang", "Qidong Liu", "Zijian Zhang", "Chong Chen", "Wei Huang", "Xiangyu Zhao"], "title": "The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation", "comment": null, "summary": "Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \\textbf{\\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \\name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\\footnote{https://github.com/ziwliu8/H2Rec}."}
{"id": "2512.10223", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.10223", "abs": "https://arxiv.org/abs/2512.10223", "authors": ["Jiewei Feng", "Peihong Yuan", "Ken R. Duffy", "Muriel Médard"], "title": "Improving the decoding performance of CA-polar codes", "comment": null, "summary": "We investigate the use of modern code-agnostic decoders to convert CA-SCL from an incomplete decoder to a complete one. When CA-SCL fails to identify a codeword that passes the CRC check, we apply a code-agnostic decoder that identifies a codeword that satisfies the CRC. We establish that this approach gives gains of up to 0.2 dB in block error rate for CA-Polar codes from the 5G New Radio standard. If, instead, the message had been encoded in a systematic CA-polar code, the gain improves to 0.2 ~ 1dB. Leveraging recent developments in blockwise soft output, we additionally establish that it is possible to control the undetected error rate even when using the CRC for error correction."}
{"id": "2512.10094", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.10094", "abs": "https://arxiv.org/abs/2512.10094", "authors": ["Brian Zhu"], "title": "Does Timeboost Reduce MEV-Related Spam? Theory and Evidence from Layer-2 Transactions", "comment": null, "summary": "Maximal extractable value opportunities often induce spam in Layer-2 blockchains: many identical transactions are submitted near simultaneously, most of which revert, wasting blockspace. We study Timeboost, a mechanism on Arbitrum that auctions a timestamp advantage, crucial under first-come first-served sequencing rules. We develop a game-theoretic model in which users choose the number of transaction copies to submit, and extend upon the baseline setting by modeling the Timeboost auction and subsequent transaction submission behavior. We show that Timeboost reduces spam and increases sequencer/DAO revenue in equilibrium relative to the baseline, transferring user payments from revert costs to auction bids. Empirically, we assemble mempool data from multiple Layer-2 networks, measuring spam via identical transactions submitted in narrow time intervals, and conduct an event study around Timeboost adoption on Arbitrum using other L2s as contemporaneous benchmarks. We find a decline in MEV-related spam and an increase in revenue on Arbitrum post-adoption, consistent with model predictions."}
{"id": "2512.10132", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10132", "abs": "https://arxiv.org/abs/2512.10132", "authors": ["Logan Nye"], "title": "Universal Hirschberg for Width Bounded Dynamic Programs", "comment": "31 pages", "summary": "Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $ω$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(ω\\log T + (\\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small \"middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $Ω(ω)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms."}
{"id": "2512.10688", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10688", "abs": "https://arxiv.org/abs/2512.10688", "authors": ["Lingfeng Liu", "Yixin Song", "Dazhong Shen", "Bing Yin", "Hao Li", "Yanyong Zhang", "Chao Wang"], "title": "Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition", "comment": "Accepted by SIGKDD 2026(First Cycle)", "summary": "Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC."}
{"id": "2512.10678", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.10678", "abs": "https://arxiv.org/abs/2512.10678", "authors": ["Mickaël Beaufils", "Katharina Schleidt", "Hylke van Der Schaaf", "Daniel Ponti", "Neil Chadwick", "Derrick Dasenbrock"], "title": "OGC Geotech Interoperability Experiment Engineering Report", "comment": null, "summary": "This Engineering Report (ER) describes the outcomes of the Open Geospatial Consortium (OGC) Geotech Interoperability Experiment (IE). The objective of this IE was to develop a common conceptual model for describing geotechnical engineering data that bridges existing specifications for encoding those data and which could be integrated across OGC and buildingSMART International Standards, This ER is directly imported from the project wiki found here: https://github.com/opengeospatial/Geotech/wiki. It is also available in html from here: https://docs.ogc.org/per/24-008.html Note that the wiki may be updated after the project end."}
{"id": "2512.10279", "categories": ["cs.GT", "cs.AI", "cs.MA", "econ.TH", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2512.10279", "abs": "https://arxiv.org/abs/2512.10279", "authors": ["Sam Ganzfried"], "title": "Computing Evolutionarily Stable Strategies in Imperfect-Information Games", "comment": null, "summary": "We present an algorithm for computing evolutionarily stable strategies (ESSs) in symmetric perfect-recall extensive-form games of imperfect information. Our main algorithm is for two-player games, and we describe how it can be extended to multiplayer games. The algorithm is sound and computes all ESSs in nondegenerate games and a subset of them in degenerate games which contain an infinite continuum of symmetric Nash equilibria. The algorithm is anytime and can be stopped early to find one or more ESSs. We experiment on an imperfect-information cancer signaling game as well as random games to demonstrate scalability."}
{"id": "2512.10134", "categories": ["cs.DS", "math.CO", "math.PR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.10134", "abs": "https://arxiv.org/abs/2512.10134", "authors": ["Ryan L. Mann", "Gabriel Waite"], "title": "Approximate Counting in Local Lemma Regimes", "comment": "20 pages, 0 figures", "summary": "We establish efficient approximate counting algorithms for several natural problems in local lemma regimes. In particular, we consider the probability of intersection of events and the dimension of intersection of subspaces. Our approach is based on the cluster expansion method. We obtain fully polynomial-time approximation schemes for both the probability of intersection and the dimension of intersection for commuting projectors. For general projectors, we provide two algorithms: a fully polynomial-time approximation scheme under a global inclusion-exclusion stability condition, and an efficient affine approximation under a spectral gap assumption. As corollaries of our results, we obtain efficient algorithms for approximating the number of satisfying assignments of conjunctive normal form formulae and the dimension of satisfying subspaces of quantum satisfiability formulae."}
{"id": "2512.10217", "categories": ["cs.DB", "cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.10217", "abs": "https://arxiv.org/abs/2512.10217", "authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Dan Suciu"], "title": "PANDAExpress: a Simpler and Faster PANDA Algorithm", "comment": null, "summary": "PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.\n  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\\tilde O(\\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power."}
{"id": "2512.10292", "categories": ["cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.10292", "abs": "https://arxiv.org/abs/2512.10292", "authors": ["Vincent Leon", "Iosif Sakos", "Ryann Sim", "Antonios Varvitsiotis"], "title": "Certifying Concavity and Monotonicity in Games via Sum-of-Squares Hierarchies", "comment": "NeurIPS 2025", "summary": "Concavity and its refinements underpin tractability in multiplayer games, where players independently choose actions to maximize their own payoffs which depend on other players' actions. In concave games, where players' strategy sets are compact and convex, and their payoffs are concave in their own actions, strong guarantees follow: Nash equilibria always exist and decentralized algorithms converge to equilibria. If the game is furthermore monotone, an even stronger guarantee holds: Nash equilibria are unique under strictness assumptions. Unfortunately, we show that certifying concavity or monotonicity is NP-hard, already for games where utilities are multivariate polynomials and compact, convex basic semialgebraic strategy sets -- an expressive class that captures extensive-form games with imperfect recall. On the positive side, we develop two hierarchies of sum-of-squares programs that certify concavity and monotonicity of a given game, and each level of the hierarchies can be solved in polynomial time. We show that almost all concave/monotone games are certified at some finite level of the hierarchies. Subsequently, we introduce SOS-concave/monotone games, which globally approximate concave/monotone games, and show that for any given game we can compute the closest SOS-concave/monotone game in polynomial time. Finally, we apply our techniques to canonical examples of imperfect recall extensive-form games."}
{"id": "2512.10354", "categories": ["cs.DS", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.10354", "abs": "https://arxiv.org/abs/2512.10354", "authors": ["Jihoon Jang", "Yehyun Nam", "Kunsoo Park", "Hyunjoon Kim"], "title": "Efficient Defective Clique Enumeration and Search with Worst-Case Optimal Search Space", "comment": "Accepted at SIGMOD 2026. This is the full version", "summary": "A $k$-defective clique is a relaxation of the traditional clique definition, allowing up to $k$ missing edges. This relaxation is crucial in various real-world applications such as link prediction, community detection, and social network analysis. Although the problems of enumerating maximal $k$-defective cliques and searching a maximum $k$-defective clique have been extensively studied, existing algorithms suffer from limitations such as the combinatorial explosion of small partial solutions and sub-optimal search spaces. To address these limitations, we propose a novel clique-first branch-and-bound framework that first generates cliques and then adds missing edges. Furthermore, we introduce a new pivoting technique that achieves a search space size of $\\mathcal{O}(3^{\\frac{n}{3}} \\cdot n^k)$, where $n$ is the number of vertices in the input graph. We prove that the worst-case number of maximal $k$-defective cliques is $Ω(3^{\\frac{n}{3}} \\cdot n^k)$ when $k$ is a constant, establishing that our algorithm's search space is worst-case optimal. Leveraging the diameter-two property of defective cliques, we further reduce the search space size to $\\mathcal{O}(n \\cdot 3^{\\fracδ{3}} \\cdot (δΔ)^k)$, where $δ$ is the degeneracy and $Δ$ is the maximum degree of the input graph. We also propose an efficient framework for maximum $k$-defective clique search based on our branch-and-bound, together with practical techniques to reduce the search space. Experiments on real-world benchmark datasets with more than 1 million edges demonstrate that each of our proposed algorithms for maximal $k$-defective clique enumeration and maximum $k$-defective clique search outperforms the respective state-of-the-art algorithms by up to four orders of magnitude in terms of processing time."}
{"id": "2512.10389", "categories": ["cs.GT", "cond-mat.stat-mech", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2512.10389", "abs": "https://arxiv.org/abs/2512.10389", "authors": ["Kovalenko Aleksandr", "Andrey Leonidov"], "title": "The $k$-flip Ising game", "comment": "31 pages, 15 figures", "summary": "A partially parallel dynamical noisy binary choice (Ising) game in discrete time of $N$ players on complete graphs with $k$ players having a possibility of changing their strategies at each time moment called $k$-flip Ising game is considered. Analytical calculation of the transition matrix of game as well as the first two moments of the distribution of $\\varphi=N^+/N$, where $N^+$ is a number of players adhering to one of the two strategies, is presented. First two moments of the first hitting time distribution for sample trajectories corresponding to transition from a metastable and unstable states to a stable one are considered. A nontrivial dependence of these moments on $k$ for the decay of a metastable state is discussed. A presence of the minima at certain $k^*$ is attributed to a competition between $k$-dependent diffusion and restoring forces."}
{"id": "2512.10532", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.10532", "abs": "https://arxiv.org/abs/2512.10532", "authors": ["Gabriel Cipriani Huete", "Adithya Diddapur", "Pavel Dvořák", "Christian Konrad"], "title": "Semi-Robust Communication Complexity of Maximum Matching", "comment": "To appear at SOSA 2026", "summary": "We study the one-way two-party communication complexity of Maximum Matching in the semi-robust setting where the edges of a maximum matching are randomly partitioned between Alice and Bob, but all remaining edges of the input graph are adversarially partitioned between the two parties.\n  We show that the simple protocol where Alice solely communicates a lexicographically-first maximum matching of their edges to Bob is surprisingly powerful: We prove that it yields a $3/4$-approximation in expectation and that our analysis is tight.\n  The semi-robust setting is at least as hard as the fully robust setting. In this setting, all edges of the input graph are randomly partitioned between Alice and Bob, and the state-of-the-art result is a fairly involved $5/6$-approximation protocol that is based on the computation of edge-degree constrained subgraphs [Azarmehr, Behnezhad, ICALP'23]. Our protocol also immediately yields a $3/4$-approximation in the fully robust setting. One may wonder whether an improved analysis of our protocol in the fully robust setting is possible: While we cannot rule this out, we give an instance where our protocol only achieves a $0.832 < 5/6 = 0.83$-approximation. Hence, while our simple protocol performs surprisingly well, it cannot be used to improve over the state-of-the-art in the fully robust setting."}
{"id": "2512.10551", "categories": ["cs.GT", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10551", "abs": "https://arxiv.org/abs/2512.10551", "authors": ["Chujie Zhao", "Qun Hu", "Shiping Song", "Dagui Chen", "Han Zhu", "Jian Xu", "Bo Zheng"], "title": "LLM-Auction: Generative Auction towards LLM-Native Advertising", "comment": null, "summary": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties."}
{"id": "2512.10621", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.10621", "abs": "https://arxiv.org/abs/2512.10621", "authors": ["Siwoo Song", "Wonseok Shin", "Kunsoo Park", "Giuseppe F. Italiano", "Zhengyi Yang", "Wenjie Zhang"], "title": "Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint", "comment": null, "summary": "A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time."}
{"id": "2512.10614", "categories": ["cs.GT", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.10614", "abs": "https://arxiv.org/abs/2512.10614", "authors": ["Jad Zeroual", "Marianne Akian", "Aurélien Bechler", "Matthieu Chardy", "Stéphane Gaubert"], "title": "Dynamics of multidimensional Simple Clock Auctions", "comment": null, "summary": "Simple Clock Auctions (SCA) are a mechanism commonly used in spectrum auctions to sell lots of frequency bandwidths. We study such an auction with one player having access to perfect information against straightforward bidders. When the opponents' valuations satisfy the ordinary substitutes condition, we show that it is optimal to bid on a fixed lot overtime. In this setting, we consider a continuous-time version of the SCA auction in which the prices follow a differential inclusion with a piecewise-constant dynamics. We show that there exists a unique solution in the sense of Filippov. This guarantees that the continuous-time model coincides with the limit of the discrete-time auction when price increments tend to zero. Moreover, we show that the value function of this limit auction is piecewise linear (though possibly discontinuous). Finally, we illustrate these results by analyzing a simplified version of the multiband Australian spectrum auction of 2017."}
{"id": "2512.10892", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.10892", "abs": "https://arxiv.org/abs/2512.10892", "authors": ["Jugal Garg", "Vishnu V. Narayan", "Yuang Eric Shen"], "title": "Designing Truthful Mechanisms for Asymptotic Fair Division", "comment": "34 pages. To appear at AAAI 2026", "summary": "We study the problem of fairly allocating a set of $m$ goods among $n$ agents in the asymptotic setting, where each item's value for each agent is drawn from an underlying joint distribution. Prior works have shown that if this distribution is well-behaved, then an envy-free allocation exists with high probability when $m=Ω(n\\log{n})$ [Dickerson et al., 2014]. Under the stronger assumption that item values are independently and identically distributed (i.i.d.) across agents, this requirement improves to $m=Ω(n\\log{n}/\\log{\\log{n}})$, which is tight [Manurangsi and Suksompong, 2021]. However, these results rely on non-strategyproof mechanisms, such as maximum-welfare allocation or the round-robin algorithm, limiting their applicability in settings with strategic agents.\n  In this work, we extend the theory to a broader, more realistic class of joint value distributions, allowing for correlations among agents, atomicity, and unequal probabilities of having the highest value for an item. We show that envy-free allocations continue to exist with a high probability when $m=Ω(n\\log{n})$. More importantly, we give a new randomized mechanism that is truthful in expectation, efficiently implementable in polynomial time, and outputs envy-free allocations with high probability, answering an open question posed by [Manurangsi and Suksompong, 2017]. We further extend our mechanism to settings with asymptotic weighted fair division and multiple agent types and good types, proving new results in each case."}
