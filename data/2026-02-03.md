<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 7]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.IR](#cs.IR) [Total: 23]
- [cs.IT](#cs.IT) [Total: 19]
- [cs.MM](#cs.MM) [Total: 5]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Learning in Bayesian Stackelberg Games With Unknown Follower's Types](https://arxiv.org/abs/2602.00771)
*Matteo Bollini,Francesco Bacchiocchi,Samuel Coutts,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: 研究贝叶斯Stackelberg博弈中的在线学习问题，领导者与具有未知私有类型的跟随者重复交互，目标是设计最小化领导者遗憾的算法


<details>
  <summary>Details</summary>
Motivation: 研究最现实的情况：领导者对跟随者类型（可能的跟随者收益）一无所知，这比常见的跟随者收益已知情况带来更多挑战

Method: 首先证明在动作反馈下无法实现无遗憾，因此专注于类型反馈模型，提出了一种无遗憾算法

Result: 在类型反馈设置下，提出的算法实现了$\widetilde{O}(\sqrt{T})$的遗憾界（忽略其他参数依赖）

Conclusion: 首次研究了领导者对跟随者类型完全未知的最现实情况，证明了动作反馈下的负面结果，在类型反馈下设计了有效的无遗憾算法

Abstract: We study online learning in Bayesian Stackelberg games, where a leader repeatedly interacts with a follower whose unknown private type is independently drawn at each round from an unknown probability distribution. The goal is to design algorithms that minimize the leader's regret with respect to always playing an optimal commitment computed with knowledge of the game. We consider, for the first time to the best of our knowledge, the most realistic case in which the leader does not know anything about the follower's types, i.e., the possible follower payoffs. This raises considerable additional challenges compared to the commonly studied case in which the payoffs of follower types are known. First, we prove a strong negative result: no-regret is unattainable under action feedback, i.e., when the leader only observes the follower's best response at the end of each round. Thus, we focus on the easier type feedback model, where the follower's type is also revealed. In such a setting, we propose a no-regret algorithm that achieves a regret of $\widetilde{O}(\sqrt{T})$, when ignoring the dependence on other parameters.

</details>


### [2] [ReACT-TTC: Capacity-Aware Top Trading Cycles for Post-Choice Reassignment in Shared CPS](https://arxiv.org/abs/2602.00859)
*Anurag Satpathy,Arindam Khanda,Chittaranjan Swain,Sajal K. Das*

Main category: cs.GT

TL;DR: 提出一种用于共享资源CPS的用户偏离后重新分配框架，基于改进的TTC机制，支持容量约束和未分配资源，保持帕累托效率、个体理性和策略证明性，并在电动汽车充电案例中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在CPS中，用户可能因个人偏好或本地信息偏离系统分配的资源，导致效率下降。需要轻量级重新分配方案来处理用户不遵从问题。

Method: 提出后偏离重新分配框架，基于改进的Top-Trading-Cycle机制，支持多对一资源容量和未分配资源条件，引入容量感知循环检测规则，并整合前景理论偏好模型。

Result: 证明了算法的终止性，保持帕累托效率、个体理性和策略证明性。在电动汽车充电案例研究中，使用真实数据验证了框架能提高用户满意度和分配质量。

Conclusion: 该框架为共享资源CPS提供了一种有效的后偏离重新分配方案，能处理用户不遵从行为，提高系统效率和用户满意度。

Abstract: Cyber-physical systems (CPS) increasingly manage shared physical resources in the presence of human decision-making, where system-assigned actions must be executed by users or agents in the physical world. A fundamental challenge in such settings is user non-compliance: individuals may deviate from assigned resources due to personal preferences or local information, degrading system efficiency and requiring light-weight reassignment schemes. This paper proposes a post-deviation reassignment framework for shared-resource CPS that operates on top of any initial allocation algorithm and is invoked only when users diverge from prescribed assignments. We advance the Top-Trading-Cycle (TTC) mechanism to enable voluntary, preference-driven exchanges after deviation events, and extend it to handle many-to-one resource capacities and unassigned resource conditions that are not supported by the classical TTC. We formalize these structural cases, introduce capacity-aware cycle-detection rules, and prove termination along with the preservation of Pareto efficiency, individual rationality, and strategy-proofness. A Prospect-Theoretic (PT) preference model is further incorporated to capture realistic user satisfaction behavior. We demonstrate the applicability of this framework on an electric-vehicle (EV) charging case study using real-world data, where it increases user satisfaction and effective assignment quality under non-compliant behavior.

</details>


### [3] [Minimizing Inequity in Facility Location Games](https://arxiv.org/abs/2602.01048)
*Yuhang Guo,Houyu Zhou*

Main category: cs.GT

TL;DR: 本文研究了在线性设施选址游戏中最小化群体不公平的问题，提出了最小化最大群体效应的公平目标，设计了两种单设施机制和扩展了双设施机制，实现了策略防操纵性并获得了紧致的近似保证。


<details>
  <summary>Details</summary>
Motivation: 在设施选址游戏中，代理属于不同群体且可能策略性地行动，需要设计公平机制来最小化群体层面的不公平。现有研究在群体公平目标上存在近似界限的空白，需要统一经典机制到更广泛的公平感知框架中。

Method: 提出了最小化最大群体效应的公平目标框架，该框架推广了经典功利主义、平等主义以及群体公平目标。针对单设施情况设计了BALANCED机制和MAJOR-PHANTOM机制，针对双设施情况扩展了经典的端点机制。

Result: 提出的机制都具有策略防操纵性，并在不同的最大群体效应目标下实现了紧致的近似保证。这些机制不仅填补了Zhou等人(2022)发现的群体公平目标近似界限空白，还将许多经典真实机制统一到公平感知框架中。

Conclusion: 本文为设施选址游戏中的群体公平问题提供了统一的框架和有效的机制设计，所提出的机制在策略防操纵性和近似性能方面都达到了最优，为解决群体层面的公平问题提供了系统性的解决方案。

Abstract: This paper studies the problem of minimizing group-level inequity in facility location games on the real line, where agents belong to different groups and may act strategically. We explore a fairness-oriented objective that minimizes the maximum group effect introduced by Marsh and Schilling (1994). Each group's effect is defined as its total or maximum distance to the nearest facility, weighted by group-specific factors. We show that this formulation generalizes several prominent optimization objectives, including the classical utilitarian (social cost) and egalitarian (maximum cost) objectives, as well as two group-fair objectives, maximum total and average group cost. In order to minimize the maximum group effect, we first propose two novel mechanisms for the single-facility case, the BALANCED mechanism and the MAJOR-PHANTOM mechanism. Both are strategyproof and achieve tight approximation guarantees under distinct formulations of the maximum group effect objective. Our mechanisms not only close the existing gap in approximation bounds for group-fairness objectives identified by Zhou, Li, and Chan (2022), but also unify many classical truthful mechanisms within a broader fairness-aware framework. For the two-facility case, we revisit and extend the classical endpoint mechanism to our generalized setting and demonstrate that it provides tight bounds for two distinct maximum group effect objectives.

</details>


### [4] [Simple and Robust Quality Disclosure: The Power of Quantile Partition](https://arxiv.org/abs/2602.01066)
*Shipra Agrawal,Yiding Feng,Wei Tang*

Main category: cs.GT

TL;DR: 研究在线平台质量信息披露的鲁棒性设计，发现分位数划分策略在对抗性环境下优于阈值划分策略，为平台常用的百分位数徽章提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 在线平台常用简单的百分位数徽章和等级来传达质量信息，这些信息在不同市场环境中保持稳定。研究旨在为这种实践提供理论支持，探讨平台在面对不确定的质量分布、类型分布和估值时，如何设计鲁棒的质量信息披露策略。

Method: 采用鲁棒优化框架，通过最小化竞争比评估信息披露策略。研究分位数划分策略和单调阈值划分策略，建立间接收益函数的函数特征，将鲁棒质量信息披露问题简化为鲁棒信息披露设计程序。

Result: 完全刻画了K-分位数划分策略的鲁棒最优解：最优最坏情况比由一维不动点方程决定，最优阈值遵循后向递归。分位数划分策略的鲁棒比有简单的"分箱最大值"表达式，均匀百分位数桶可实现1+1/K的紧致保证。而有限信号的单调阈值划分策略无法超越因子2的近似。

Conclusion: 分位数划分策略为在线平台的质量信息披露提供了鲁棒的理论基础，解释了为什么实际中常用百分位数徽章，并且上分位数需要更精细的分辨率。相比之下，阈值划分策略在鲁棒性方面存在根本限制。

Abstract: Quality information on online platforms is often conveyed through simple, percentile-based badges and tiers that remain stable across different market environments. Motivated by this empirical evidence, we study robust quality disclosure in a market where a platform commits to a public disclosure policy mapping the seller's product quality into a signal, and the seller subsequently sets a downstream monopoly price. Buyers have heterogeneous private types and valuations that are linear in quality. We evaluate a disclosure policy via a minimax competitive ratio: its worst-case revenue relative to the Bayesian-optimal disclosure-and-pricing benchmark, uniformly over all prior quality distributions, type distributions, and admissible valuations.
  Our main results provide a sharp theoretical justification for quantile-partition disclosure. For K-quantile partition policies, we fully characterize the robust optimum: the optimal worst-case ratio is pinned down by a one-dimensional fixed-point equation and the optimal thresholds follow a backward recursion. We also give an explicit formula for the robust ratio of any quantile partition as a simple "max-over-bins" expression, which explains why the robust-optimal partition allocates finer resolution to upper quantiles and yields tight guarantees such as 1 + 1/K for uniform percentile buckets. In contrast, we show a robustness limit for finite-signal monotone (quality-threshold) partitions, which cannot beat a factor-2 approximation. Technically, our analysis reduces the robust quality disclosure to a robust disclosure design program by establishing a tight functional characterization of all feasible indirect revenue functions.

</details>


### [5] [Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations](https://arxiv.org/abs/2602.01568)
*Hamzah Khan,Dong Ho Lee,Jingqi Li,Tianyu Qiu,Christian Ellis,Jesse Milzman,Wesley Suttle,David Fridovich-Keil*

Main category: cs.GT

TL;DR: 提出一种求解混合层次（Stackelberg-Nash）博弈的高效算法，通过拟策略近似消除高阶导数，实现实时收敛


<details>
  <summary>Details</summary>
Motivation: 多机器人协调中存在混合信息结构（同时决策与层次决策），现有博弈论求解器难以处理这类混合层次博弈

Method: 推导森林结构混合层次博弈的KKT条件，引入拟策略近似消除高阶策略导数，开发不精确牛顿法求解近似KKT系统

Result: 算法在非二次目标和非线性约束的博弈中实现局部指数收敛，Julia库实现展示复杂混合层次结构的实时收敛能力

Conclusion: 提出的拟策略近似和不精确牛顿法有效解决了混合层次博弈的计算难题，为复杂多机器人协调提供了实用工具

Abstract: Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures.

</details>


### [6] [Stable Matching with Predictions: Robustness and Efficiency under Pruned Preferences](https://arxiv.org/abs/2602.02254)
*Samuel McCauley,Benjamin Moseley,Helia Niaparast,Shikha Singh*

Main category: cs.GT

TL;DR: 研究带截断偏好列表的稳定匹配问题，利用预测信息减少偏好列表长度和提案数量，证明经典延迟接受算法对此具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在大规模匹配市场（如NRMP）中，医院无法面试或排序所有住院医师，需要截断偏好列表。基于历史数据，医院可以预测可能的匹配排名，只排序预测区间内的候选人。

Method: 采用算法与预测框架，研究带截断偏好列表的稳定匹配问题。提出两种算法，理论分析和实验评估经典延迟接受算法对截断的鲁棒性。

Result: 即使预测精度一般，也能显著减少偏好列表长度和提案数量。解释了延迟接受算法的实际成功，并将市场设计与算法预测理论联系起来。

Conclusion: 经典延迟接受算法对偏好列表截断具有鲁棒性，结合预测信息可以大幅提高大规模匹配市场的效率，为实际应用提供理论支持。

Abstract: In this paper, we study the fundamental problem of finding a stable matching in two-sided matching markets. In the classic variant, it is assumed that both sides of the market submit a ranked list of all agents on the other side. However, in large matching markets such as the National Resident Matching Program (NRMP), it is infeasible for hospitals to interview or mutually rank each resident. In this paper, we study the stable matching problem with truncated preference lists. In particular, we assume that, based on historical datasets, each hospital has a predicted rank of its likely match and only ranks residents within a bounded interval around that prediction.
  We use the algorithms-with-predictions framework and show that the classic deferred-acceptance (DA) algorithm used to compute stable matchings is robust to such truncation. We present two algorithms and theoretically and empirically evaluate their performance. Our results show that even with reasonably accurate predictions, it is possible to significantly cut down on both instance size (the length of preference lists) as well as the number of proposals made. These results explain the practical success of the DA algorithm and connect market design to the emerging theory of algorithms with predictions.

</details>


### [7] [Carry-Over Lottery Allocation: Practical Incentive-Compatible Drafts](https://arxiv.org/abs/2602.02487)
*Timothy Highley,Tannah Duncan,Ilia Volkov*

Main category: cs.GT

TL;DR: 提出COLA选秀机制：使用多年季后赛结果评估球队质量，所有非季后赛球队获得相同彩票数量，未中奖彩票可累积至未来选秀，消除故意输球动机。


<details>
  <summary>Details</summary>
Motivation: NBA选秀抽签制度旨在帮助弱队，但创造了故意输球（摆烂）的动机，破坏了竞争平衡。需要一种既实用又激励相容的机制来真正帮助弱队。

Method: COLA选秀机制：1) 用多年季后赛结果代替单赛季战绩评估球队质量；2) 所有非季后赛球队获得相同数量彩票；3) 未中奖彩票可累积至未来选秀；4) 季后赛成功或获得高顺位会减少累积彩票；5) 针对特殊年份采用真相揭示机制扩展抽签资格。

Result: COLA机制能消除故意输球的动机，因为淘汰后输更多比赛不会增加彩票数量。同时奖励长期表现差且之前获得较少选秀帮助的球队，保持透明度和球迷参与度。

Conclusion: COLA选秀机制提供了一种实用、激励相容的解决方案，能有效解决NBA摆烂问题，同时考虑了实际实施挑战，包括过渡期、交易选秀权处理以及特殊年份的调整机制。

Abstract: The NBA Draft lottery is designed to promote competitive balance by awarding better draft positions to weaker teams, but it creates incentives to deliberately lose, a practice known as tanking. We propose a draft mechanism that is simultaneously practical, incentive-compatible, and advantages weaker teams. The \textbf{Carry-Over Lottery Allocation (COLA) Draft Mechanism} represents a paradigm shift in evaluating team quality, replacing a single season's standings with playoff outcomes over multiple years. COLA uses a draft lottery where every non-playoff team receives the same number of lottery tickets, removing incentives to lose additional games after elimination. Lottery tickets that do not win a top draft pick carry over to future lotteries, while playoff success or winning a top pick diminishes a team's accumulated tickets. Over time, COLA rewards teams with poor long-term performance and less prior draft assistance. By retaining the lottery format, COLA preserves transparency and fan engagement.
  Real-world implementation challenges are addressed to demonstrate feasibility, including transitioning from the current system, handling traded draft picks, and accommodating draft classes of varying strength. The most significant challenge occurs in years with exceptionally strong draft classes, where teams may prefer missing the playoffs in order to gain lottery access, violating a foundational assumption: that teams prefer playoff success to lottery participation. We provide a solution to this problem, employing a truth-elicitation mechanism to identify such years and expand lottery eligibility to include as many playoff teams as necessary to preserve anti-tanking incentives.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [End Cover for Initial Value Problem: Complete Validated Algorithms with Complexity Analysis](https://arxiv.org/abs/2602.00162)
*Bingwei Zhang,Chee Yap*

Main category: cs.DS

TL;DR: 提出一个完整的验证算法，用于解决常微分方程初值问题的终点覆盖问题，通过覆盖终点集的边界来计算覆盖集，并进行复杂度分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 研究常微分方程初值问题的解在固定时间后的终点集覆盖问题，需要开发能够提供严格数学保证的验证算法，确保计算出的覆盖集既包含所有可能的终点，又不超过给定的误差范围。

Method: 提出一个完整的验证算法，基于覆盖终点集的边界来计算覆盖集。算法处理一阶自治常微分方程，其中向量场是局部Lipschitz的。通过计算盒子B0中所有初值在时间h后的终点集合的覆盖。

Result: 提供了算法的复杂度分析，并展示了实验结果表明该方法的实用性。算法能够计算满足严格包含关系的有限盒子集合，确保终点集被完全覆盖且误差控制在指定范围内。

Conclusion: 提出的验证算法能够有效解决常微分方程初值问题的终点覆盖问题，通过边界覆盖的新技术提供了完整的解决方案，并通过复杂度分析和实验验证了其实用性。

Abstract: We consider the first-order autonomous ordinary differential equation \[ \mathbf{x}' = \mathbf{f}(\mathbf{x}), \] where $\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^n$ is locally Lipschitz. For a box $B_0 \subseteq \mathbb{R}^n$ and $h > 0$, we denote by $\mathrm{IVP}_{\mathbf{f}}(B_0,h)$ the set of solutions $\mathbf{x} : [0,h] \to \mathbb{R}^n$ satisfying \[ \mathbf{x}'(t) = \mathbf{f}(\mathbf{x}(t)), \qquad \mathbf{x}(0) \in B_0 . \]
  We present a complete validated algorithm for the following \emph{End Cover Problem}: given $(\mathbf{f}, B_0, \varepsilon, h)$, compute a finite set $\mathcal{C}$ of boxes such that \[ \mathrm{End}_{\mathbf{f}}(B_0,h) \;\subseteq\; \bigcup_{B \in \mathcal{C}} B \;\subseteq\; \mathrm{End}_{\mathbf{f}}(B_0,h) \oplus [-\varepsilon,\varepsilon]^n , \] where \[ \mathrm{End}_{\mathbf{f}}(B_0,h) = \left\{ \mathbf{x}(h) : \mathbf{x} \in \mathrm{IVP}_{\mathbf{f}}(B_0,h) \right\}. \]
  Moreover, we provide a complexity analysis of our algorithm and introduce a novel technique for computing the end cover $\mathcal{C}$ based on covering the boundary of $\mathrm{End}_{\mathbf{f}}(B_0,h)$. Finally, we present experimental results demonstrating the practicality of our approach.

</details>


### [9] [Hardness and Tractability of T_{h+1}-Free Edge Deletion](https://arxiv.org/abs/2602.00644)
*Ajinkya Gaikwad,Soumen Maity,Leeja R*

Main category: cs.DS

TL;DR: 本文研究了T(h+1)-Free Edge Deletion问题的参数化复杂度，证明了在多个结构参数下该问题是W[1]-难的，但通过结合参数h或使用特定图类可获得固定参数可解性。


<details>
  <summary>Details</summary>
Motivation: 研究T(h+1)-Free Edge Deletion问题的参数化复杂度，该问题要求删除最多k条边使得结果图的每个连通分量大小不超过h。虽然已知该问题在h≥3时是NP完全的，但需要更深入地理解其参数化复杂度，特别是针对各种结构参数。

Method: 通过参数化复杂度理论分析，证明该问题在多个结构参数（如到路径并集的顶点删除距离、到星并集的顶点删除距离、双覆盖数）下是W[1]-难的。同时，通过结合参数h或使用整数线性规划，证明在聚类顶点删除数+h或邻域多样性+h下是固定参数可解的。还提出了参数k下的双准则近似算法，并在分裂图和区间图上建立了固定参数可解算法。

Result: 1) 证明了该问题在多个限制性结构参数下是W[1]-难的，包括到路径并集的顶点删除距离、到星并集的顶点删除距离、双覆盖数等。2) 证明了在聚类顶点删除数+h或邻域多样性+h下是固定参数可解的。3) 提出了参数k下的固定参数可解双准则近似算法。4) 在分裂图和区间图上建立了固定参数可解算法。5) 证明了有向推广版本即使在有向无环图上也是困难的。

Conclusion: T(h+1)-Free Edge Deletion问题在h无界时，对于许多经典结构参数都是固定参数难解的，包括树深、聚类顶点删除数和模块宽度等。然而，通过将h作为额外参数或针对特定图类，可以获得固定参数可解性。该研究统一并扩展了先前关于树宽、路径宽和反馈顶点集的硬度结果，为理解该问题的计算复杂度提供了全面图景。

Abstract: We study the parameterized complexity of the T(h+1)-Free Edge Deletion problem. Given a graph G and integers k and h, the task is to delete at most k edges so that every connected component of the resulting graph has size at most h. The problem is NP-complete for every fixed h at least 3, while it is solvable in polynomial time for h at most 2.
  Recent work showed strong hardness barriers: the problem is W[1]-hard when parameterized by the solution size together with the size of a feedback edge set, ruling out fixed-parameter tractability for many classical structural parameters. We significantly strengthen these negative results by proving W[1]-hardness when parameterized by the vertex deletion distance to a disjoint union of paths, the vertex deletion distance to a disjoint union of stars, or the twin cover number. These results unify and extend known hardness results for treewidth, pathwidth, and feedback vertex set, and show that several restrictive parameters, including treedepth, cluster vertex deletion number, and modular width, do not yield fixed-parameter tractability when h is unbounded.
  On the positive side, we identify parameterizations that restore tractability. We show that the problem is fixed-parameter tractable when parameterized by cluster vertex deletion together with h, and also when parameterized by neighborhood diversity together with h via an integer linear programming formulation. We further present a fixed-parameter tractable bicriteria approximation algorithm parameterized by k. Finally, we show that the problem admits fixed-parameter tractable algorithms on split graphs and interval graphs, and we establish hardness for a directed generalization even on directed acyclic graphs.

</details>


### [10] [Fanciful Figurines flip Free Flood-It -- Polynomial-Time Miniature Painting on Co-gem-free Graphs](https://arxiv.org/abs/2602.00690)
*Christian Rosenke,Mark Scheibner*

Main category: cs.DS

TL;DR: 论文提出"微型绘画"计算问题：根据模板为图顶点着色，目标是用最少的笔触序列实现模板，每个笔触将连通顶点子集覆盖为一种颜色。该问题等价于Free Flood-It游戏的逆问题。


<details>
  <summary>Details</summary>
Motivation: 受微型绘画爱好启发，研究如何高效实现给定颜色模板的图着色问题。该问题与广泛研究的Free Flood-It游戏密切相关，但方向相反，为图着色理论提供新视角。

Method: 建立微型绘画与Free Flood-It游戏的等价关系，利用已知的Free Flood-It复杂度结果。主要贡献是开发了在无诱导co-gem图上的多项式时间算法，这类图严格推广了cographs。

Result: 证明微型绘画在网格、树、分裂图等严重结构限制下是NP难问题。在无诱导co-gem图上给出了多项式时间算法，从而Free Flood-It在co-gem-free图上也是多项式时间可解的。

Conclusion: 微型绘画作为Free Flood-It的逆问题，继承了其复杂性特征。在无诱导co-gem图上可高效求解，为图着色和Flood-It类问题提供了新的算法见解。

Abstract: Inspired by the eponymous hobby, we introduce Miniature Painting as the computational problem to paint a given graph $G=(V,E)$ according to a prescribed template $t \colon V \rightarrow C$, which assigns colors $C$ to the vertices of $G$. In this setting, the goal is to realize the template using a shortest possible sequence of brush strokes, where each stroke overwrites a connected vertex subset with a color in $C$. We show that this problem is equivalent to a reversal of the well-studied Free Flood-It game, in which a colored graph is decolored into a single color using as few moves as possible. This equivalence allows known complexity results for Free Flood-It to be transferred directly to Miniature Painting, including NP-hardness under severe structural restrictions, such as when $G$ is a grid, a tree, or a split graph. Our main contribution is a polynomial-time algorithm for Miniature Painting on graphs that are free of induced co-gems, a graph class that strictly generalizes cographs. As a direct consequence, Free Flood-It is also polynomial-time solvable on co-gem-free graphs, independent of the initial coloring.

</details>


### [11] [Fast $k$-means Seeding Under The Manifold Hypothesis](https://arxiv.org/abs/2602.01104)
*Poojan Shah,Shashwat Agrawal,Ragesh Jaiswal*

Main category: cs.DS

TL;DR: 提出基于流形假设的k-means快速种子算法Qkmeans，利用数据内在维度特性实现优于最坏情况的时间-质量权衡


<details>
  <summary>Details</summary>
Motivation: 现有k-means理论分析基于对最优解的假设，难以在实践中验证。本文提出流形假设（数据在环境维度D中围绕低维内在维度d的流形集中）作为建模实际聚类实例的合理假设。

Method: 利用最优量化理论识别数据集的关键几何特性，这些特性具有理论可预测的缩放规律（依赖于量化指数ε=2/d）。基于这些规律设计快速种子方法Qkmeans，在时间O(nD) + Õ(ε^(1+ρ)ρ^(-1)k^(1+γ))内提供O(ρ^(-2)log k)近似解，其中γ=ε+ρ，ρ<1为输入参数。

Result: 获得了新的运行时间-质量权衡关系。通过跨多个领域的大规模实证研究验证了理论预测和算法性能，弥合了超越最坏情况数据聚类的理论与实践差距。

Conclusion: 流形假设为建模实际k-means实例提供了合理框架，Qkmeans算法利用数据内在维度特性实现了高效聚类，为超越最坏情况分析提供了理论和实证支持。

Abstract: We study beyond worst case analysis for the $k$-means problem where the goal is to model typical instances of $k$-means arising in practice. Existing theoretical approaches provide guarantees under certain assumptions on the optimal solutions to $k$-means, making them difficult to validate in practice. We propose the manifold hypothesis, where data obtained in ambient dimension $D$ concentrates around a low dimensional manifold of intrinsic dimension $d$, as a reasonable assumption to model real world clustering instances. We identify key geometric properties of datasets which have theoretically predictable scaling laws depending on the quantization exponent $\varepsilon = 2/d$ using techniques from optimum quantization theory. We show how to exploit these regularities to design a fast seeding method called $\operatorname{Qkmeans}$ which provides $O(ρ^{-2} \log k)$ approximate solutions to the $k$-means problem in time $O(nD) + \widetilde{O}(\varepsilon^{1+ρ}ρ^{-1}k^{1+γ})$; where the exponent $γ= \varepsilon + ρ$ for an input parameter $ρ< 1$. This allows us to obtain new runtime - quality tradeoffs. We perform a large scale empirical study across various domains to validate our theoretical predictions and algorithm performance to bridge theory and practice for beyond worst case data clustering.

</details>


### [12] [Benchmarking of algorithms for set partitions](https://arxiv.org/abs/2602.01350)
*Arnav Khinvasara,Alexander Pikovski*

Main category: cs.DS

TL;DR: 论文回顾了集合划分问题，提供了计算划分数量的近似公式，评估了多种枚举算法，并推荐Djokić算法用于实际应用。


<details>
  <summary>Details</summary>
Motivation: 集合划分在组合优化等场景中具有重要应用，需要高效的方法来计算划分数量和枚举所有划分方案。

Method: 1. 简要回顾集合划分问题；2. 提供适用于小规模和大规模集合的划分数量近似公式；3. 回顾多种枚举算法；4. 进行基准测试比较算法性能。

Result: 提出了实用的近似公式来计算集合划分数量，通过基准测试发现Djokić等人的算法在性能上表现最佳。

Conclusion: Djokić等人的算法被推荐为实际应用中的首选，因其在枚举所有集合划分时具有最佳性能表现。

Abstract: Set partitions are arrangements of distinct objects into groups. The problem of listing all set partitions arises in a variety of settings, in particular in combinatorial optimization tasks. After a brief review, we give practical approximate formulas for determining the number of set partitions, both for small and large set sizes. Several algorithms for enumerating all set partitions are reviewed, and benchmarking tests were conducted. The algorithm of Djokic et al. is recommended for practical use.

</details>


### [13] [A $5$-Approximation Analysis for the Cover Small Cuts Problem](https://arxiv.org/abs/2602.01462)
*Miles Simmons,Ishan Bansal,Joe Cheriyan*

Main category: cs.DS

TL;DR: 本文改进了Cover Small Cuts问题的近似比，将WGMV原始对偶算法的近似比从6提升到5


<details>
  <summary>Details</summary>
Motivation: Cover Small Cuts问题在组合优化中具有重要意义，先前的研究已经将WGMV算法的近似比改进到16和6，但仍有提升空间。本文旨在通过更强的数学工具进一步改进近似比。

Method: 使用WGMV原始对偶算法，但引入了更强的数学概念：满足对称性和结构次模性的可塑性集合族，替代先前研究中仅满足组合性质的可塑性集合族。

Result: 证明了WGMV算法对Cover Small Cuts问题的近似比为5，优于先前已知的16和6。

Conclusion: 通过引入对称性和结构次模性的更强数学条件，成功将Cover Small Cuts问题的近似比改进到5，这是目前已知的最佳近似比。

Abstract: In the Cover Small Cuts problem, we are given a capacitated (undirected) graph $G=(V,E,u)$ and a threshold value $λ$, as well as a set of links $L$ with end-nodes in $V$ and a non-negative cost for each link $\ell\in L$; the goal is to find a minimum-cost set of links such that each non-trivial cut of capacity less than $λ$ is covered by a link. Bansal, Cheriyan, Grout, and Ibrahimpur (arXiv:2209.11209, Algorithmica 2024) showed that the WGMV primal-dual algorithm, due to Williamson, Goemans, Mihail, and Vazirani (Combinatorica, 1995), achieves approximation ratio $16$ for the Cover Small Cuts problem; their analysis uses the notion of a pliable family of sets that satisfies a combinatorial property. Later, Bansal (arXiv:2308.15714v2, IPCO 2025) and then Nutov (arXiv:2504.03910, MFCS 2025) proved that the same algorithm achieves approximation ratio $6$. We show that the same algorithm achieves approximation ratio $5$, by using a stronger notion, namely, a pliable family of sets that satisfies symmetry and structural submodularity.

</details>


### [14] [A polynomial-time algorithm for recognizing high-bandwidth graphs](https://arxiv.org/abs/2602.01755)
*Luis M. B. Varona*

Main category: cs.DS

TL;DR: 该论文针对图带宽识别问题，为接近最大带宽的大k值情况提供了多项式时间算法，与已知的小k值算法互补，表明当k或n-k较小时问题可在多项式时间内解决。


<details>
  <summary>Details</summary>
Motivation: 现有动态规划方法在k接近0时有效，但当k随n增长时会变得超指数复杂（最坏情况O(n^{n-1})）。需要为接近最大带宽的大k值情况开发更高效的算法。

Method: 将带宽识别问题重新表述为二分图匹配问题（当k ≥ ⌊(n-1)/2⌋时），利用霍尔婚姻定理开发新算法，时间复杂度为O(n^{n-k+1})，辅助空间为O(n)。

Result: 新算法在k接近n-1时具有多项式复杂度，与已知的小k值算法形成互补，表明带宽识别问题在k或n-k较小时都可在多项式时间内解决。

Conclusion: 该研究为图带宽识别问题提供了完整的复杂度分析框架，展示了问题在参数k的两个极端情况下（接近0或接近n-1）都具有多项式时间可解性。

Abstract: An unweighted, undirected graph $G$ on $n$ nodes is said to have \emph{bandwidth} at most $k$ if its nodes can be labelled from $0$ to $n - 1$ such that no two adjacent nodes have labels that differ by more than $k$. It is known that one can decide whether the bandwidth of $G$ is at most $k$ in $O(n^k)$ time and $O(n^k)$ space using dynamic programming techniques. For small $k$ close to $0$, this approach is effectively polynomial, but as $k$ scales with $n$, it becomes superexponential, requiring up to $O(n^{n - 1})$ time (where $n - 1$ is the maximum possible bandwidth). In this paper, we reformulate the problem in terms of bipartite matching for sufficiently large $k \ge \lfloor (n - 1)/2 \rfloor$, allowing us to use Hall's marriage theorem to develop an algorithm that runs in $O(n^{n - k + 1})$ time and $O(n)$ auxiliary space (beyond storage of the input graph). This yields polynomial complexity for large $k$ close to $n - 1$, demonstrating that the bandwidth recognition problem is solvable in polynomial time whenever either $k$ or $n - k$ remains small.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [15] [Updatable Balanced Index for Stable Streaming Similarity Search over Large-Scale Fresh Vectors](https://arxiv.org/abs/2602.00563)
*Yuhui Lai,Shixun Huang,Sheng Wang*

Main category: cs.DB

TL;DR: UBIS是一种可更新的平衡索引，用于处理流式相似性搜索，通过调度并发更新和减少不平衡更新情况，在数据频繁变化时保持索引质量。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能普及，向量成为信息检索和推荐服务的核心数据结构。近似最近邻搜索(ANNS)依赖索引组织大数据集，但数据频繁变化时需要索引支持实时更新。现有方法存在两个问题：1)使用额外缓冲区的方案资源密集且效率低；2)升级内部索引结构的方案在流式工作负载中因更新拥塞和不平衡分布导致性能下降。

Method: 提出UBIS(可更新的平衡索引)，通过调度并发更新解决冲突，并通过减少不平衡更新情况来维持良好的索引质量，特别是在更新频率增加时。

Result: 在真实数据集上的实验结果表明，UBIS相比最先进的索引，在流式工作负载中实现了高达77%的搜索准确率提升和平均45%的更新吞吐量提升。

Conclusion: UBIS通过有效管理并发更新和减少不平衡分布，为流式相似性搜索提供了高效、稳定的可更新索引解决方案，显著提升了搜索准确性和更新性能。

Abstract: As artificial intelligence gains more and more popularity, vectors are one of the most widely used data structures for services such as information retrieval and recommendation. Approximate Nearest Neighbor Search (ANNS), which generally relies on indices optimized for fast search to organize large datasets, has played a core role in these popular services. As the frequency of data shift grows, it is crucial for indices to accommodate new data and support real-time updates. Existing researches adopting two different approaches hold the following drawbacks: 1) approaches using additional buffers to temporarily store new data are resource-intensive and inefficient due to the global rebuilding processes; 2) approaches upgrading the internal index structure suffer from performance degradation because of update congestion and imbalanced distribution in streaming workloads. In this paper, we propose UBIS, an Updatable Balanced Index for stable streaming similarity Search, to resolve conflicts by scheduling concurrent updates and maintain good index quality by reducing imbalanced update cases, when the update frequency grows. Experimental results in the real-world datasets demonstrate that UBIS achieves up to 77% higher search accuracy and 45% higher update throughput on average compared to the state-of-the-art indices in streaming workloads.

</details>


### [16] [Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems](https://arxiv.org/abs/2602.01701)
*Ruyu Li,Tinghui Zhang,Haodi Ma,Daisy Zhe Wang,Yifan Wang*

Main category: cs.DB

TL;DR: Meta Engine是一个"查询系统之上的查询系统"，通过统一集成异构的专用LLM查询系统来解决多模态语义查询中的API碎片化和专业化与通用性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 随着多模态数据的广泛应用，语义查询需求日益增长，但现有LLM语义查询系统存在两个主要问题：1) 不同系统的API碎片化导致集成困难；2) 专用系统在单一模态上表现优异但难以处理多模态数据，而通用系统虽然支持多模态但在特定模态上性能不佳。

Method: 提出Meta Engine统一语义查询引擎，包含五个关键组件：1) 自然语言查询解析器；2) 操作符生成器；3) 查询路由器；4) 适配器集合；5) 结果聚合器。该系统能够集成异构的专用LLM查询系统。

Result: Meta Engine在评估中始终优于所有基线方法，在大多数情况下获得3-6倍的F1分数提升，在特定数据集上甚至达到24倍的性能提升。

Conclusion: Meta Engine成功解决了多模态语义查询中的集成挑战和专业化-通用性权衡问题，为异构LLM查询系统提供了统一的解决方案。

Abstract: With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.
  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some "all-in-one" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.
  This paper introduces Meta Engine, a novel "query system on query systems", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.

</details>


### [17] [ChemDCAT-AP: Enabling Semantic Interoperability with a Contextual Extension of DCAT-AP](https://arxiv.org/abs/2602.01822)
*Philip Stroemert,Hendrik Borgelt,David Linke,Mark Doerr,Bhavin Katabathuni,Oliver Koepler,Norbert Kockmann*

Main category: cs.DB

TL;DR: 提出DCAT-AP PLUS通用应用配置文件，通过上层抽象层增强DCAT的语义表达能力，支持跨领域研究数据集成，并以化学和催化领域为例展示了具体应用。


<details>
  <summary>Details</summary>
Motivation: 跨领域数据集成面临不同学科使用各自元数据模式、领域本体和概念模型的挑战，导致语义互操作性困难。虽然DCAT提供了数据集描述的基础词汇表，但其核心模型过于轻量，现有领域特定应用配置文件（如DCAT-AP）难以满足研究数据的全面表示需求。

Method: 提出DCAT-AP PLUS通用应用配置文件，引入上层抽象层，支持领域特定扩展而不牺牲兼容性。采用LinkML建模框架支持模式继承、生成领域特定子模式，并提供数据类型协调、验证和格式转换机制。以化学和催化领域为例开发ChemDCAT-AP具体配置文件。

Result: DCAT-AP PLUS能够全面表示研究数据的来源和上下文信息，支持跨领域互操作性。通过ChemDCAT-AP展示了化学和催化领域数据集成的潜力，LinkML框架确保了与现有数据基础设施的平滑集成。

Conclusion: DCAT-AP PLUS为跨领域研究数据集成提供了有效的解决方案，通过通用上层抽象层和领域特定扩展的平衡，解决了语义互操作性挑战，促进了跨学科数据重用和知识转移。

Abstract: Cross-domain data integration drives interdisciplinary data reuse and knowledge transfer across domains. However, each discipline maintains its own metadata schemas and domain ontologies, employing distinct conceptual models and application profiles, which complicates semantic interoperability. The W3C Data Catalog Vocabulary (DCAT) offers a widely adopted RDF vocabulary for describing datasets and their distributions, but its core model is intentionally lightweight. Numerous domain-specific application profiles have emerged to enrich DCAT's expressivity, the most well-known DCAT-AP for public data. To facilitate cross-domain interoperability for research data, we propose DCAT-AP PLUS, a DCAT Application Profile (P)roviding additional (L)inks to (U)se-case (S)pecific context (DCAT-AP+). This generic application profile enables a comprehensive representation of the provenance and context of research data generation. DACT-AP+ introduces an upper-level layer that can be specialized by individual domains without sacrificing compatibility. We demonstrate the application of DCAT-AP+ and a specific profile ChemDCAT-AP to showcase the potential of data integration of the neighboring disciplines chemistry and catalysis. We adopt LinkML, a YAML-based modeling framework, to support schema inheritance, generate domain-specific subschemas, and provide mechanisms for data type harmonization, validation, and format conversion, ensuring smooth integration of DCAT-AP+ and ChemDCAT-AP within existing data infrastructures.

</details>


### [18] [Tidehunter: Large-Value Storage With Minimal Data Relocation](https://arxiv.org/abs/2602.01873)
*Andrey Chursin,Lefteris Kokoris-Kogias,Alex Orlov,Alberto Sonnino,Igor Zablotchi*

Main category: cs.DB

TL;DR: Tidehunter是一个存储引擎，通过将WAL作为永久存储而非临时缓冲区，消除了LSM-tree中的值压缩开销，针对大值随机写工作负载提供高性能


<details>
  <summary>Details</summary>
Motivation: LSM-tree在随机工作负载下存在10-30倍的写放大问题，对于大值、均匀分布键的工作负载（如内容寻址存储、去重系统、区块链验证器）尤为严重

Method: 1) 将WAL作为永久存储，值永不覆盖；2) 使用小型懒刷新索引表映射键到WAL位置；3) 无锁写入通过原子分配和并行拷贝饱和NVMe；4) 乐观索引结构利用均匀键分布实现单往返查找；5) 基于epoch的修剪在不阻塞写入的情况下回收空间

Result: 在1TB数据集、1KB值的测试中，Tidehunter达到830K写入/秒，比RocksDB高8.4倍，比BlobDB高2.9倍；点查询提升1.7倍，存在性检查提升15.6倍；在Sui区块链中保持稳定吞吐和延迟

Conclusion: Tidehunter通过消除值压缩显著提升了LSM-tree在大值随机写工作负载下的性能，已准备好用于生产环境，正在Sui区块链中部署

Abstract: Log-Structured Merge-Trees (LSM-trees) dominate persistent key-value storage but suffer from high write amplification from 10x to 30x under random workloads due to repeated compaction. This overhead becomes prohibitive for large values with uniformly distributed keys, a workload common in content-addressable storage, deduplication systems, and blockchain validators. We present Tidehunter, a storage engine that eliminates value compaction by treating the Write-Ahead Log (WAL) as permanent storage rather than a temporary recovery buffer. Values are never overwritten; and small, lazily-flushed index tables map keys to WAL positions. Tidehunter introduces (a) lock-free writes that saturate NVMe drives through atomic allocation and parallel copying, (b) an optimistic index structure that exploits uniform key distributions for single-roundtrip lookups, and (c) epoch-based pruning that reclaims space without blocking writes. On a 1 TB dataset with 1 KB values, Tidehunter achieves 830K writes per second, that is 8.4x higher than RocksDB and 2.9x higher than BlobDB, while improving point queries by 1.7x and existence checks by 15.6x. We validate real-world impact by integrating Tidehunter into Sui, a high-throughput blockchain, where it maintains stable throughput and latency under loads that cause RocksDB-backed validators to collapse. Tidehunter is production-ready and is being deployed in production within Sui.

</details>


### [19] [SQLAgent: Learning to Explore Before Generating as a Data Engineer](https://arxiv.org/abs/2602.01952)
*Wenjia Jiang,Yiwei Wang,Boyan Han,Joey Tianyi Zhou,Chi Zhang*

Main category: cs.DB

TL;DR: 提出一个两阶段LLM框架，将知识获取与查询生成解耦，通过探索阶段构建数据库特定知识库，部署阶段利用知识生成准确SQL查询


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在处理复杂真实数据库时泛化能力不足，因为SQL推理高度依赖特定数据库的独特模式、模糊语义和复杂连接路径

Method: 两阶段框架：探索阶段使用蒙特卡洛树搜索策略自主构建数据库特定知识库（模式片段、可执行查询、自然语言描述三元组）；部署阶段采用双代理系统，利用收集的知识作为上下文示例，迭代检索相关信息并生成SQL查询

Result: 在大规模基准测试中，该方法显著提高了准确性，超越了强基线方法，证明了其有效性和泛化能力

Conclusion: 该框架使LLM能够主动熟悉未见过的数据库并处理复杂的多步推理，为自然语言到关系数据库的接口提供了有效的解决方案

Abstract: Large Language Models have recently shown impressive capabilities in reasoning and code generation, making them promising tools for natural language interfaces to relational databases. However, existing approaches often fail to generalize in complex, real-world settings due to the highly database-specific nature of SQL reasoning, which requires deep familiarity with unique schemas, ambiguous semantics, and intricate join paths. To address this challenge, we introduce a novel two-stage LLM-based framework that decouples knowledge acquisition from query generation. In the Exploration Stage, the system autonomously constructs a database-specific knowledge base by navigating the schema with a Monte Carlo Tree Search-inspired strategy, generating triplets of schema fragments, executable queries, and natural language descriptions as usage examples. In the Deployment Stage, a dual-agent system leverages the collected knowledge as in-context examples to iteratively retrieve relevant information and generate accurate SQL queries in response to user questions. This design enables the agent to proactively familiarize itself with unseen databases and handle complex, multi-step reasoning. Extensive experiments on large-scale benchmarks demonstrate that our approach significantly improves accuracy over strong baselines, highlighting its effectiveness and generalizability.

</details>


### [20] [Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data](https://arxiv.org/abs/2602.02025)
*Serafeim Papadias,Kostas Patroumpas,Dimitrios Skoutas*

Main category: cs.DB

TL;DR: Hippasus是一个用于特征增强的模块化框架，通过结合轻量级统计信号和大型语言模型的语义推理来高效发现和集成多表特征，在保持高运行性能的同时显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型严重依赖特征质量，但有用特征通常分散在多个关系表中。特征增强通过连接操作从相关表中发现和集成特征来丰富基础表，但在具有多表和复杂路径的复杂模式中扩展这一过程仍然具有挑战性。现有方法在有效性和效率之间存在根本性权衡：高准确性需要探索许多候选路径，但穷举探索计算成本过高。

Method: Hippasus采用三个关键贡献：1) 结合轻量级统计信号和大型语言模型的语义推理，在执行前修剪无希望的连接路径；2) 使用优化的多路连接算法并整合来自多个路径的特征，大幅减少执行时间；3) 集成基于LLM的语义理解和统计度量，选择既具有语义意义又具有经验预测性的特征。

Result: 在公开可用数据集上的实验评估表明，Hippasus相比最先进的基线方法，特征增强准确性提高了高达26.8%，同时提供高运行性能。

Conclusion: Hippasus通过创新的路径修剪、优化连接执行和智能特征选择方法，成功解决了特征增强中有效性与效率之间的权衡问题，为复杂关系模式中的特征发现提供了高效且准确的解决方案。

Abstract: Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.

</details>


### [21] [QVCache: A Query-Aware Vector Cache](https://arxiv.org/abs/2602.02057)
*Anıl Eren Göçer,Ioanna Tsakalidou,Hamish Nicholson,Kyoungmin Kim,Anastasia Ailamaki*

Main category: cs.DB

TL;DR: QVCache是首个面向近似最近邻搜索的查询级缓存系统，通过语义感知缓存而非精确匹配，在有限内存下显著降低查询延迟。


<details>
  <summary>Details</summary>
Motivation: 向量数据库已成为现代信息检索的基石，但将近似最近邻搜索扩展到高召回率同时满足严格延迟SLO仍受内存容量和I/O带宽限制。基于磁盘的向量搜索系统在高精度下延迟严重，而全内存方案在十亿级规模下内存成本过高。尽管缓存在传统数据库中起核心作用，但向量搜索缺乏通用的查询级缓存层来分摊重复查询工作。

Method: QVCache利用语义查询重复性，执行相似性感知缓存而非精确匹配查找。它通过在线学习算法动态学习区域特定的距离阈值，在保持召回率的同时实现缓存命中，独立于数据集大小限制查找延迟和内存使用。QVCache作为现有向量数据库的即插即用层，保持兆字节级内存占用。

Result: QVCache实现亚毫秒级缓存命中延迟，与现有ANN系统集成时，端到端查询延迟降低40-1000倍。对于展现时空语义局部性的工作负载，QVCache显著降低延迟，同时保持与底层ANN后端相当的召回率。

Conclusion: QVCache作为首个后端无关的查询级缓存系统，为可扩展向量搜索建立了缺失但必要的缓存层，通过语义感知缓存有效解决了高召回率下的延迟瓶颈问题。

Abstract: Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. However, scaling approximate nearest neighbor (ANN) search to high recall under strict latency SLOs remains fundamentally constrained by memory capacity and I/O bandwidth. Disk-based vector search systems suffer severe latency degradation at high accuracy, while fully in-memory solutions incur prohibitive memory costs at billion-scale. Despite the central role of caching in traditional databases, vector search lacks a general query-level caching layer capable of amortizing repeated query work.
  We present QVCache, the first backend-agnostic, query-level caching system for ANN search with bounded memory footprint. QVCache exploits semantic query repetition by performing similarity-aware caching rather than exact-match lookup. It dynamically learns region-specific distance thresholds using an online learning algorithm, enabling recall-preserving cache hits while bounding lookup latency and memory usage independently of dataset size. QVCache operates as a drop-in layer for existing vector databases. It maintains a megabyte-scale memory footprint and achieves sub-millisecond cache-hit latency, reducing end-to-end query latency by up to 40-1000x when integrated with existing ANN systems. For workloads exhibiting temporal-semantic locality, QVCache substantially reduces latency while preserving recall comparable to the underlying ANN backend, establishing it as a missing but essential caching layer for scalable vector search.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [22] [Disentangled Interest Network for Out-of-Distribution CTR Prediction](https://arxiv.org/abs/2602.00002)
*Yu Zheng,Chen Gao,Jianxin Chang,Yanan Niu,Yang Song,Depeng Jin,Meng Wang,Yong Li*

Main category: cs.IR

TL;DR: DiseCTR：通过因果视角和多兴趣解耦解决CTR预测中的分布外问题，显著提升推荐系统的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法通常假设训练和测试数据来自相同分布，但实际中用户兴趣不断演化导致分布外问题。此外，用户通常有多个兴趣，且演化速度不同，需要更精细的建模方法。

Method: 提出DiseCTR框架：1）从因果视角对CTR预测进行因子分解，包含用户兴趣、曝光模型和点击模型；2）设计稀疏注意力兴趣编码器将原始特征映射到用户兴趣；3）引入弱监督兴趣解耦器学习独立的兴趣嵌入；4）使用注意力兴趣聚合器进行预测。

Result: 在三个真实数据集上，DiseCTR在分布外推荐中达到最佳准确性和鲁棒性，AUC和GAUC提升超过0.02，logloss降低超过13.7%。分析表明DiseCTR成功解耦了用户兴趣。

Conclusion: DiseCTR通过因果视角和多兴趣解耦有效缓解了CTR预测中的分布外问题，用户兴趣解耦是实现分布外泛化的关键。已开源代码和数据。

Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/.

</details>


### [23] [Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts](https://arxiv.org/abs/2602.00003)
*Ye Liu,Xu Chen,Wuji Chen,Mang Li*

Main category: cs.IR

TL;DR: 提出基于LLM的MoE框架解决多国电商搜索中的分布偏移问题，通过专家路由和嵌入融合提升多语言相关性，并优化推理管道降低35%GPU消耗


<details>
  <summary>Details</summary>
Motivation: 多国电商平台面临语言、文化和产品目录的分布偏移，现有单一模型在数据多样性、覆盖范围和推理成本方面存在局限，需要更有效的多语言相关性建模方案

Method: 提出可扩展的LLM-based Mixture-of-Experts框架，动态路由查询到专门化专家，通过拼接融合嵌入；比较基于规则、伪标签和端到端策略，采用端到端硬路由；开发工程优化的离线批处理管道，隐藏内存延迟，提高GPU利用率

Result: 在六个东南亚市场数据集上，MoE相比相同活跃参数的密集基线提升AUC 0.72个百分点；优化管道达到27.6 QPS，吞吐量提升9%，GPU小时消耗降低35%

Conclusion: 提出的MoE框架在保持效率的同时显著提升多语言搜索相关性，为实际电商搜索系统提供了优越的成本效益解决方案

Abstract: In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems.

</details>


### [24] [C$^2$-Cite: Contextual-Aware Citation Generation for Attributed Large Language Models](https://arxiv.org/abs/2602.00004)
*Yue Yu,Ting Bai,HengZhi Lan,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Chuan Shi*

Main category: cs.IR

TL;DR: 提出C²-Cite框架，通过上下文感知的引用生成机制，提升LLM生成文本中引用的语义连贯性和准确性


<details>
  <summary>Details</summary>
Motivation: 现有指令调优的归因LLM在生成文本时，未能正确理解引用符号（如[i]）的上下文语义，导致引用不连贯且检索知识整合不佳

Method: 提出C²-Cite框架，采用上下文引用对齐机制：先将检索文档上下文编码到引用符号表示中，再通过引用路由函数解码对齐标记编号

Result: 在ALCE基准测试的三个数据集上，C²-Cite++平均在引用质量上优于SOTA基线5.8%，在响应正确性上优于17.4%

Conclusion: C²-Cite框架成功将引用标记从通用占位符转变为主动的知识指针，显著提升了LLM生成文本的引用质量和语义连贯性

Abstract: The attribution technique enhances the credibility of LLMs by adding citations to the generated sentences, enabling users to trace back to the original sources and verify the reliability of the output. However, existing instruction-tuned attributed LLMs often fail to properly interpret the contextual semantics of citation symbols (e.g., [i]) during text generation. This shortcoming arises from their insufficient awareness of the context information surrounding citation markers, which in turn leads to disjointed references and poor integration of retrieved knowledge into the generated content. To address this issue, we propose a novel \textbf{C}ontextual-aware \textbf{C}itation generation framework (\textbf{C$^2$}-\textbf{Cite}) that explicitly integrates the semantic relationships between citation markers and their referenced content. Specifically, a contextual citation alignment mechanism is adopted: it first encodes the retrieved document contexts into the symbol representation of citations, then aligns the marker numbers by decoding information from a citation router function. This mechanism enables the transformation of citation markers from generic placeholders into active knowledge pointers that link to the referenced source information. Experimental results on the ALCE benchmark across three datasets validate our framework C$^2$-Cite++: it outperforms the SOTA baseline by an average of 5.8\% in citation quality and 17.4\% in response correctness. The implementation is publicly available at https://github.com/BAI-LAB/c2cite

</details>


### [25] [AutoBool: An Reinforcement-Learning trained LLM for Effective Automated Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2602.00005)
*Shuai Wang,Harrisen Scells,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: AutoBool使用强化学习训练大语言模型生成医学系统综述的高效布尔查询，无需真实查询数据，在多个数据集上超越提示方法，接近专家水平且检索文档量大幅减少。


<details>
  <summary>Details</summary>
Motivation: 医学系统综述中布尔查询是文献检索的主要方式，需要在保持合理精度的同时达到高召回率。现有基于提示的LLM方法难以平衡这一挑战，且缺乏高质量的真实查询数据使得监督微调不可行。

Method: 提出AutoBool强化学习框架，直接使用检索指标优化查询生成，无需目标查询。创建并发布了包含65588个主题的最大数据集，用于训练和评估自动布尔查询生成任务。

Result: 在新数据集和两个现有数据集（CLEF TAR和Seed Collection）上，AutoBool显著优于零样本/少样本提示方法，匹配或超越更大的GPT模型（如GPT-4o、O3）的效果，接近专家编写的查询效果，同时检索文档量减少10-16倍。

Conclusion: AutoBool通过强化学习有效解决了医学系统综述中布尔查询生成的挑战，无需真实查询数据，在多个数据集上表现出色，接近专家水平且大幅减少检索负担。消融研究揭示了模型骨干、大小、解码温度和提示设计的关键作用。

Abstract: We present AutoBool, a reinforcement learning (RL) framework that trains large language models (LLMs) to generate effective Boolean queries for medical systematic reviews. Boolean queries are the primary mechanism for literature retrieval in this domain and must achieve high recall while maintaining reasonable precision - a challenging balance that existing prompt-based LLM approaches often struggle to achieve. A major limitation in this space is the lack of high-quality ground-truth Boolean queries for each topic, which makes supervised fine-tuning impractical. AutoBool addresses this challenge by using RL to directly optimize query generation with retrieval measures, without requiring target queries. To support this effort, we create and release the largest dataset of its kind: 65588 topics in total for training and evaluating the task of automatic Boolean query formulation. Experiments on our new dataset and two established datasets (CLEF TAR and Seed Collection) show that AutoBool significantly outperforms zero shot/few shot prompting and matches or exceeds the effectiveness of much larger GPT-based models (e.g., GPT-4o, O3) using smaller backbones. It also approaches effectiveness of expert-authored queries while retrieving 10 to 16 times fewer documents. Ablation studies reveal the critical roles of model backbone, size, decoding temperature, and prompt design. Code and data are available at https://github.com/ielab/AutoBool.

</details>


### [26] [FDA AI Search: Making FDA-Authorized AI Devices Searchable](https://arxiv.org/abs/2602.00006)
*Arun Kavishwar,William Lotter*

Main category: cs.IR

TL;DR: FDA AI Search是一个网站工具，通过语义查询帮助用户从FDA授权的1200多个AI医疗设备中找到适合特定临床需求的产品，解决了FDA数据库元数据有限和PDF文件不可搜索的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管FDA已授权超过1200个AI医疗设备，但由于FDA数据库仅包含有限的元数据和不可搜索的PDF摘要，临床医生和开发者难以找到适合特定临床需求的设备。

Method: 开发FDA AI Search网站，后端采用基于嵌入的检索系统，使用LLM从授权摘要中提取特征，然后将这些特征与用户查询进行语义比较以找到相关匹配。

Result: 通过定量和定性评估表明，该检索算法相比基于关键词的方法更有效，能够更好地匹配用户查询和FDA授权的AI设备。

Conclusion: 随着FDA授权的AI设备日益普及和应用场景扩展，该工具将帮助医疗提供者找到符合临床需求的设备，并支持开发者构思新的AI应用。

Abstract: Over 1,200 AI-enabled medical devices have received marketing authorization from the U.S. FDA, yet identifying devices suited to specific clinical needs remains challenging because the FDA's databases contain only limited metadata and non-searchable summary PDFs. To address this gap, we developed FDA AI Search, a website that enables semantic querying of FDA-authorized AI-enabled devices. The backend includes an embedding-based retrieval system, where LLM-extracted features from authorization summaries are compared to user queries to find relevant matches. We present quantitative and qualitative evaluation that support the effectiveness of the retrieval algorithm compared to keyword-based methods. As FDA-authorized AI devices become increasingly prevalent and their use cases expand, we envision that the tool will assist healthcare providers in identifying devices aligned with their clinical needs and support developers in formulating novel AI applications.

</details>


### [27] [Front-Loaded or Balanced? The Mechanism through Which Review Order Affects Overall Ratings in Premium Service Settings](https://arxiv.org/abs/2602.00008)
*He Wang,Ziyu Zhou,Hanxiang Liu*

Main category: cs.IR

TL;DR: 研究发现评价顺序（评分优先vs评论优先）影响消费者评分：评分优先界面在高服务质量情境下会提高评分，通过降低认知努力和增强情感启发式实现；低服务质量时则相反。


<details>
  <summary>Details</summary>
Motivation: 在高品质服务环境中，评价界面的顺序（评分优先或评论优先）已成为影响评分真实性和反馈质量的关键因素。现有研究主要关注评论内容和情感，但对评价顺序如何影响评分结果的系统性研究有限。

Method: 通过探索性分析比较Letterboxd（评论优先、评分后置）和Yelp（评分优先、评论后置）的评分分布差异。随后进行三个控制实验，检验评价顺序对消费者评分的影响，并通过机制分析探讨认知努力和情感启发式的双重路径。

Result: Letterboxd的评分分布更集中，极端分数较少；Yelp则呈现明显的双峰分布，评分更极化。实验表明，在高服务质量情境下，评分优先（vs评论优先）界面显著提高消费者总体评分。机制分析显示，评分优先顺序通过降低认知努力和增强情感启发式来提高评分。服务质量调节这一过程：当服务质量低时，评分优先顺序反而导致更低评分。

Conclusion: 研究揭示了评价顺序通过认知和情感路径影响消费者评分的心理机制，扩展了对在线评分形成的理论理解，并为优化平台界面设计以提高评分真实性和可信度提供了实践启示。

Abstract: In the increasingly prevalent landscape of high-quality service contexts, whether consumer evaluation interfaces adopt a rating-first or review-first sequence has become a critical factor shaping rating authenticity and feedback quality. While prior research has primarily examined review content and sentiment, systematic investigation into how evaluation order influences rating outcomes remains limited. Through exploratory analyses, we find that Letterboxd -- which employs a review-first, rating-after mechanism -- exhibits a more centralized rating distribution with fewer extreme scores, whereas Yelp -- which adopts a rating-first, review-after mechanism -- shows a pronounced bimodal distribution with more polarized ratings. Three controlled experiments further demonstrate that in high-quality service contexts, a rating-first (vs. review-first) interface significantly elevates consumers' overall ratings. Mechanism analyses indicate that cognitive effort and affective heuristics serve as dual pathways: a rating-first (vs. review-first) sequence reduces cognitive effort and heightens affective heuristics, thereby increasing rating scores. Moreover, service quality moderates this process. When service quality is low, the rating-first (vs. review-first) sequence instead leads to lower ratings. This research reveals the psychological mechanisms through which evaluation order affects consumer ratings via cognitive and affective pathways. It extends theoretical understanding of online rating formation and offers practical implications for optimizing platform interface design to enhance rating authenticity and credibility.

</details>


### [28] [ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking](https://arxiv.org/abs/2602.00010)
*Mathieu Ciancone,Clovis Varangot-Reille,Marion Schaeffer*

Main category: cs.IR

TL;DR: ChunkNorris：一种基于启发式规则的PDF文档解析与分块优化技术，无需机器学习，计算开销低，在检索增强生成应用中表现优异


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成应用中，信息检索部分至关重要，因为它为大型语言模型提供上下文信息以生成恰当且真实的回答。高质量的解析和分块是关键，因为高效的数据分割直接影响下游任务（信息检索和答案生成）。

Method: ChunkNorris是一种新颖的基于启发式的技术，旨在优化PDF文档的解析和分块。该方法不依赖机器学习，采用一套简单而有效的启发式规则，以最小的计算开销实现高性能。

Result: 通过综合基准测试评估执行时间、能耗和检索准确性，ChunkNorris在性能上优于基线和更先进的技术。研究还提出了一个开放访问的数据集来产生结果。

Conclusion: ChunkNorris为信息检索任务提供了一个实用且高效的替代方案，突出了基于启发式方法在现实世界资源受限的RAG用例中的潜力。

Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases.

</details>


### [29] [Chained Prompting for Better Systematic Review Search Strategies](https://arxiv.org/abs/2602.00011)
*Fatima Nasser,Fouad Trad,Ammar Mohanna,Ghada El-Hajj Fuleihan,Ali Chehab*

Main category: cs.IR

TL;DR: 提出基于大语言模型的链式提示工程框架，用于自动生成系统综述的检索策略，相比传统方法在召回率上有显著提升


<details>
  <summary>Details</summary>
Motivation: 传统手动设计检索策略方法资源密集且易受主观性影响，而启发式和自动化方法在召回率上表现不佳，需要大量专家输入。需要一种更高效、可扩展的自动化方法来生成高质量的检索策略。

Method: 基于大语言模型的链式提示工程框架，复制手动检索设计的过程结构，利用LLM分解综述目标、提取和形式化PICO元素、生成概念表示、扩展术语、合成布尔查询。

Result: 在LEADSInstruct数据集子集上评估，框架达到0.9的平均召回率，显著超过现有方法。在生成结构化PICO元素方面也表现优异。

Conclusion: LLM-based pipelines能够产生透明、可重复且高性能的检索策略，具有作为支持证据合成和循证实践的可扩展工具的潜力。精确的目标规范和术语对齐对优化检索效果至关重要。

Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice.

</details>


### [30] [Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking](https://arxiv.org/abs/2602.00013)
*Vipul Dinesh Pawar*

Main category: cs.IR

TL;DR: 论文提出Linear-PAL框架解决电商排序中的位置偏差问题，通过结构约束和向量化整数哈希技术，在保持高去偏排序质量的同时大幅提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 电商排序中，用户隐式反馈存在严重的位置偏差（用户倾向于与顶部项目交互而不管相关性）。在高度偏差环境下，现有的深度集成方法会出现捷径学习问题，过度拟合排名信号导致排序质量下降。

Method: 提出Linear-PAL框架：1）通过显式特征连接和激进正则化的结构约束强制去偏；2）引入向量化整数哈希技术进行特征生成，用O(N)向量化算术替代字符串操作。

Result: 在大规模数据集（420万样本）上，Linear-PAL实现帕累托优势：去偏排序质量（相关性AUC：0.7626 vs 0.6736）优于深度集成方法，同时训练延迟降低43倍（40秒 vs 1762秒）。

Conclusion: Linear-PAL框架在保持高去偏排序质量的同时显著提升计算效率，支持高频重训练，能够捕捉用户特定的新兴市场趋势，实现近乎实时的个性化排序。

Abstract: In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time.

</details>


### [31] [AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows](https://arxiv.org/abs/2602.00052)
*Ramtin Babaeipour,François Charest,Madison Wright*

Main category: cs.IR

TL;DR: 使用RAG增强的生成式LLM进行临床试验方案信息提取，相比独立LLM准确率更高（87.8% vs 62.6%），AI辅助工作流效率提升40%且用户偏好度更高。


<details>
  <summary>Details</summary>
Motivation: 临床试验方案日益复杂，修订频繁，知识管理困难，给试验团队带来沉重负担。结构化方案内容有望提高效率、改善文档质量和增强合规性。

Method: 采用基于检索增强生成（RAG）的AI系统进行临床试验方案信息自动提取，比较临床试验专用RAG流程与公开可用独立LLM的提取准确性，并评估AI辅助对模拟提取工作流的操作影响。

Result: RAG流程准确率达87.8%，显著高于经过精细提示调优的独立LLM（62.6%）。在模拟工作流中，AI辅助任务完成速度快40%，认知负荷更低，用户强烈偏好AI辅助方式。

Conclusion: 虽然专家监督仍然必要，但AI辅助提取能够实现大规模方案智能化，值得将类似方法整合到真实临床工作流中，进一步验证其对可行性、研究启动和激活后监测的影响。

Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.

</details>


### [32] [SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.00083)
*Yuxin Yang,Gangda Deng,Ömer Faruk Akgül,Nima Chitsazan,Yash Govilkar,Akasha Tigalappanavara,Shi-Xiong Zhang,Sambit Sahu,Viktor Prasanna*

Main category: cs.IR

TL;DR: SPARC-RAG是一个多智能体框架，通过协调顺序和并行推理扩展来解决传统RAG在多跳问答中的局限性，采用共享全局上下文管理和轻量级微调方法，显著提升性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）在多跳问答中面临挑战，传统方法通过顺序深度和并行宽度进行推理扩展，但存在上下文污染和扩展效率低的问题，导致计算增加但收益递减甚至负收益。

Method: 提出SPARC-RAG多智能体框架：1）协调顺序和并行推理扩展，统一上下文管理机制；2）使用专门智能体维护共享全局上下文；3）为每个分支生成针对性互补子查询实现多样化并行探索；4）基于答案正确性和证据基础明确调节退出决策；5）引入轻量级微调方法，通过过程级可验证偏好优化扩展行为。

Result: 在单跳和多跳问答基准测试中，SPARC-RAG始终优于先前RAG基线，平均F1分数提升+6.2，同时推理成本更低。

Conclusion: SPARC-RAG通过协调顺序和并行推理扩展，结合有效的上下文管理和优化微调，解决了传统RAG在多跳问答中的扩展效率问题，实现了性能提升和成本降低的双重优势。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.

</details>


### [33] [RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing](https://arxiv.org/abs/2602.00296)
*Ziqi Wang,Xi Zhu,Shuhang Lin,Haochen Xue,Minghao Guo,Yongfeng Zhang*

Main category: cs.IR

TL;DR: RAGRouter-Bench：首个用于自适应RAG路由的数据集和基准测试，系统评估不同查询-语料库场景下RAG范式的表现，发现没有单一最优范式，强调路由评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究主要关注查询侧复杂性或孤立方法改进，缺乏对不同查询-语料库上下文和效率-效果权衡的系统理解，需要建立系统评估框架。

Method: 提出RAGRouter-Bench数据集和基准测试，从查询-语料库兼容性角度重新审视检索，标准化5种代表性RAG范式，在7,727个查询和21,460个文档上进行系统评估，包含三种典型查询类型和细粒度语义/结构语料库指标。

Result: 实验使用DeepSeek-V3和LLaMA-3.1-8B显示：1）没有单一RAG范式在所有场景下最优；2）范式适用性受查询-语料库交互强烈影响；3）更先进的机制不一定带来更好的效率-效果权衡。

Conclusion: 研究结果强调了路由感知评估的必要性，为自适应、可解释、可泛化的下一代RAG系统奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a core paradigm for grounding large language models with external knowledge. Despite extensive efforts exploring diverse retrieval strategies, existing studies predominantly focus on query-side complexity or isolated method improvements, lacking a systematic understanding of how RAG paradigms behave across different query-corpus contexts and effectiveness-efficiency trade-offs. In this work, we introduce RAGRouter-Bench, the first dataset and benchmark designed for adaptive RAG routing. RAGRouter-Bench revisits retrieval from a query-corpus compatibility perspective and standardizes five representative RAG paradigms for systematic evaluation across 7,727 queries and 21,460 documents spanning diverse domains. The benchmark incorporates three canonical query types together with fine-grained semantic and structural corpus metrics, as well as a unified evaluation for both generation quality and resource consumption. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that no single RAG paradigm is universally optimal, that paradigm applicability is strongly shaped by query-corpus interactions, and that increased advanced mechanism does not necessarily yield better effectiveness-efficiency trade-offs. These findings underscore the necessity of routing-aware evaluation and establish a foundation for adaptive, interpretable, and generalizable next-generation RAG systems.

</details>


### [34] [Equity vs. Equality: Optimizing Ranking Fairness for Tailored Provider Needs](https://arxiv.org/abs/2602.00495)
*Yiteng Tu,Weihang Su,Shuguang Han,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 该论文提出了一种面向公平的排名框架EquityRank，通过建模不同提供商的个性化偏好（如曝光度和销售额），在保持用户侧效果的同时优化提供商侧的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有排名公平性研究主要采用基于平等的视角，关注相似内容的提供商获得相似曝光度。然而，这种方法忽略了现实世界中不同提供商的实际需求差异，他们的效用不仅取决于曝光度，还取决于销售额、参与度等结果。基于曝光的公平性可能无法准确反映具有不同优先级的提供商的真实效用。

Method: 提出了一个面向公平的公平性框架，明确建模每个提供商对关键结果（如曝光度和销售额）的偏好。基于此框架开发了EquityRank算法，这是一种基于梯度的算法，联合优化用户侧效果和提供商侧公平性。

Result: 广泛的离线和在线模拟表明，EquityRank在效果和公平性之间提供了更好的权衡，并且能够适应异构的提供商需求。

Conclusion: 该研究引入了一种新的公平性视角，从平等转向公平，通过考虑提供商的个性化偏好，开发出能够同时满足用户效果需求和提供商公平性需求的排名算法，为信息检索系统中的提供商公平性提供了更全面的解决方案。

Abstract: Ranking plays a central role in connecting users and providers in Information Retrieval (IR) systems, making provider-side fairness an important challenge. While recent research has begun to address fairness in ranking, most existing approaches adopt an equality-based perspective, aiming to ensure that providers with similar content receive similar exposure. However, it overlooks the diverse needs of real-world providers, whose utility from ranking may depend not only on exposure but also on outcomes like sales or engagement. Consequently, exposure-based fairness may not accurately capture the true utility perceived by different providers with varying priorities. To this end, we introduce an equity-oriented fairness framework that explicitly models each provider's preferences over key outcomes such as exposure and sales, thus evaluating whether a ranking algorithm can fulfill these individualized goals while maintaining overall fairness across providers. Based on this framework, we develop EquityRank, a gradient-based algorithm that jointly optimizes user-side effectiveness and provider-side equity. Extensive offline and online simulations demonstrate that EquityRank offers improved trade-offs between effectiveness and fairness and adapts to heterogeneous provider needs.

</details>


### [35] [Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation](https://arxiv.org/abs/2602.00632)
*Hongxun Ding,Keqin Bao,Jizhi Zhang,Yi Fang,Wenxin Xu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 论文提出RISER框架，用强化学习替代长链思维推理来改进推荐系统，解决了推理延迟和用户行为数据缺乏认知模式的问题。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理在推荐系统中存在两个主要问题：过高的推理延迟和用户行为数据缺乏明确的认知推理模式，这使得长链思维不适合顺序推荐领域。

Method: 提出RISER（Reinforced Item Space Exploration）框架，将非可学习的轨迹转化为有效的成对偏好数据进行优化，并采用防止冗余rollout和约束token级更新幅度等策略确保稳定性。

Result: 在三个真实世界数据集上的实验表明，RISER显著优于竞争基线，为RL增强的LLM推荐建立了稳健的范式。

Conclusion: 应该从链式思维结构转向直接利用其底层机制（强化学习）来探索物品空间，RISER框架为此提供了有效的解决方案。

Abstract: While Long Chain-of-Thought (Long CoT) reasoning has shown promise in Large Language Models (LLMs), its adoption for enhancing recommendation quality is growing rapidly. In this work, we critically examine this trend and argue that Long CoT is inherently ill-suited for the sequential recommendation domain. We attribute this misalignment to two primary factors: excessive inference latency and the lack of explicit cognitive reasoning patterns in user behavioral data. Driven by these observations, we propose pivoting away from the CoT structure to directly leverage its underlying mechanism: Reinforcement Learning (RL), to explore the item space. However, applying RL directly faces significant obstacles, notably low sample efficiency-where most actions fail to provide learning signals-and training instability. To overcome these limitations, we propose RISER, a novel Reinforced Item Space Exploration framework for Recommendation. RISER is designed to transform non-learnable trajectories into effective pairwise preference data for optimization. Furthermore, it incorporates specific strategies to ensure stability, including the prevention of redundant rollouts and the constraint of token-level update magnitudes. Extensive experiments on three real-world datasets show that RISER significantly outperforms competitive baselines, establishing a robust paradigm for RL-enhanced LLM recommendation. Our code will be available at https://anonymous.4open.science/r/RISER/.

</details>


### [36] [RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment](https://arxiv.org/abs/2602.00682)
*Yuecheng Li,Hengwei Ju,Zeyu Song,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: RecGOAT是一个用于大模型增强多模态推荐的双重语义对齐框架，通过图注意力网络和双重粒度对齐机制，解决大模型表示与推荐系统ID特征之间的不兼容问题，在公开基准和工业场景中均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐系统虽然整合了用户行为和多模态数据，但在利用大模型时存在根本性表示差异问题。大模型表示针对通用语义任务优化，而推荐模型依赖稀疏的用户/物品ID特征，现有工作忽视了这种表示差异，导致不兼容的多模态表示和次优的推荐性能。

Method: 提出RecGOAT框架：1）使用图注意力网络建模物品-物品、用户-物品、用户-用户关系，利用用户/物品的大模型表示和交互历史丰富协同语义；2）设计双重粒度渐进多模态-ID对齐框架，通过跨模态对比学习实现实例级对齐，通过最优自适应传输实现分布级对齐。

Result: 在三个公开基准测试中实现了最先进的性能，验证了理论洞察。在大型在线广告平台上的部署证实了模型在工业推荐场景中的有效性和可扩展性。

Conclusion: RecGOAT通过双重语义对齐框架成功解决了大模型表示与推荐系统ID特征之间的不兼容问题，提供了理论保证的对齐能力，在学术基准和工业应用中均表现出色，为大模型增强的多模态推荐提供了有效解决方案。

Abstract: Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.

</details>


### [37] [SWGCN: Synergy Weighted Graph Convolutional Network for Multi-Behavior Recommendation](https://arxiv.org/abs/2602.00727)
*Fangda Chen,Yueyang Wang,Chaoli Lou,Min Gao,Qingyu Xiong*

Main category: cs.IR

TL;DR: SWGCN是一种新的多行为推荐模型，通过目标偏好加权器和协同对齐任务来捕捉跨行为协同信号和细粒度行为强度，在多个数据集上显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的多行为推荐方法通常忽视跨行为协同信号和个体行为的细粒度强度，需要克服这些局限性来更好地预测用户的主要转化行为（如购买）。

Method: 提出Synergy Weighted Graph Convolutional Network (SWGCN)，包含两个核心组件：目标偏好加权器（自适应分配用户-物品交互权重）和协同对齐任务（通过辅助偏好评估器指导训练，优先考虑更准确反映用户偏好的协同信号）。

Result: 在Taobao、IJCAI和Beibei三个开源数据集上进行了全面测试。在Taobao数据集上，HR和NDCG分别获得112.49%和156.36%的相对提升，在其他数据集上也表现出一致的性能提升，证明了模型的鲁棒性和泛化能力。

Conclusion: SWGCN通过有效捕捉跨行为协同信号和细粒度行为强度，显著提升了多行为推荐性能，代码已开源供研究社区使用。

Abstract: Multi-behavior recommendation paradigms have emerged to capture diverse user activities, forecasting primary conversions (e.g., purchases) by leveraging secondary signals like browsing history. However, current graph-based methods often overlook cross-behavioral synergistic signals and fine-grained intensity of individual actions. Motivated by the need to overcome these shortcomings, we introduce Synergy Weighted Graph Convolutional Network (SWGCN). SWGCN introduces two novel components: a Target Preference Weigher, which adaptively assigns weights to user-item interactions within each behavior, and a Synergy Alignment Task, which guides its training by leveraging an Auxiliary Preference Valuator. This task prioritizes interactions from synergistic signals that more accurately reflect user preferences. The performance of our model is rigorously evaluated through comprehensive tests on three open-source datasets, specifically Taobao, IJCAI, and Beibei. On the Taobao dataset, SWGCN yields relative gains of 112.49% and 156.36% in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), respectively. It also yields consistent gains on IJCAI and Beibei, confirming its robustness and generalizability across various datasets. Our implementation is open-sourced and can be accessed via https://github.com/FangdChen/SWGCN.

</details>


### [38] [Towards Trustworthy Multimodal Recommendation](https://arxiv.org/abs/2602.00730)
*Zixuan Li*

Main category: cs.IR

TL;DR: 提出可插拔的模态级修正组件，通过软匹配抑制不可信模态特征，提升多模态推荐的鲁棒性；同时分析交互级可信度，发现伪交互和传播图伪边在噪声下的双重影响。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态推荐面临可信度问题：电商平台的多模态内容（如图片、标题）可能具有误导性或不可靠，这些不可信信号会注入多模态表示中，使现有推荐系统在模态损坏下变得脆弱。

Method: 提出可插拔的模态级修正组件，通过学习物品与多模态特征之间的软对应关系来缓解不可信模态特征。使用轻量级投影和基于Sinkhorn的软匹配，抑制不匹配的模态信号同时保持语义一致性，无需修改现有推荐器架构。同时分析交互级可信度，研究训练集伪交互和传播图伪边在噪声信号下的影响。

Result: 在多个数据集和骨干网络上，在不同损坏水平下的广泛实验表明，模态修正提高了鲁棒性，并验证了交互级观察：伪交互的性能影响取决于先验信号对齐；传播图伪边可能帮助或损害鲁棒性，因为消息传递可能放大不对齐。

Conclusion: 该工作从方法和分析两个角度推进可信多模态推荐：模态级修正组件能有效抑制不可信模态特征；交互级分析揭示了伪交互和传播图伪边在噪声下的复杂影响，为构建更鲁棒的多模态推荐系统提供了实用见解。

Abstract: Recent advances in multimodal recommendation have demonstrated the effectiveness of incorporating visual and textual content into collaborative filtering. However, real-world deployments raise an increasingly important yet underexplored issue: trustworthiness. On modern e-commerce platforms, multimodal content can be misleading or unreliable (e.g., visually inconsistent product images or click-bait titles), injecting untrustworthy signals into multimodal representations and making existing recommenders brittle under modality corruption. In this work, we take a step towards trustworthy multimodal recommendation from both a method and an analysis perspective. First, we propose a plug-and-play modality-level rectification component that mitigates untrustworthy modality features by learning soft correspondences between items and multimodal features. Using lightweight projections and Sinkhorn-based soft matching, the rectification suppresses mismatched modality signals while preserving semantic consistency, and can be integrated into existing multimodal recommenders without architectural modifications. Second, we present two practical insights on interaction-level trustworthiness under noisy collaborative signals: (i) training-set pseudo interactions can help or hurt performance under noise depending on prior-signal alignment; and (ii) propagation-graph pseudo edges can also help or hurt robustness, as message passing may amplify misalignment. Extensive experiments on multiple datasets and backbones under varying corruption levels demonstrate improved robustness from modality rectification and validate the above interaction-level observations.

</details>


### [39] [Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training](https://arxiv.org/abs/2602.00805)
*Yunhan Li,Mingjie Xie,Zihan Gong,Zeyang Shi,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: 本文提出针对工业法律检索系统中共享检索骨干的多阶段优化框架，通过组件级混合阶段配置而非单一最优检查点来提升检索质量。


<details>
  <summary>Details</summary>
Motivation: 在工业系统中，单个检索骨干通常被多个下游应用共享，检索质量直接制约系统性能和可扩展性，同时耦合了跨应用的模型选择、部署和回滚决策。

Method: 采用多阶段优化框架，对密集检索器和重排序器进行优化，发现不同检索组件呈现阶段依赖的权衡，从而提出组件级混合阶段配置方法。

Result: 通过端到端评估验证了优化后的检索骨干，并将其部署为支持多个工业应用的共享检索服务。

Conclusion: 在共享检索骨干系统中，组件级混合阶段配置比依赖单一均匀最优检查点更有效，能够更好地平衡不同检索组件的阶段依赖权衡。

Abstract: Recent advances in embedding-based retrieval have enabled dense retrievers to serve as core infrastructure in many industrial systems, where a single retrieval backbone is often shared across multiple downstream applications. In such settings, retrieval quality directly constrains system performance and extensibility, while coupling model selection, deployment, and rollback decisions across applications.
  In this paper, we present empirical findings and a system-level solution for optimizing retrieval components deployed as a shared backbone in production legal retrieval systems. We adopt a multi-stage optimization framework for dense retrievers and rerankers, and show that different retrieval components exhibit stage-dependent trade-offs. These observations motivate a component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint. The resulting backbone is validated through end-to-end evaluation and deployed as a shared retrieval service supporting multiple industrial applications.

</details>


### [40] [Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment](https://arxiv.org/abs/2602.01023)
*Kai Yuan,Anthony Zheng,Jia Hu,Divyanshu Sheth,Hemanth Velaga,Kylee Kim,Matteo Guarrera,Besim Avci,Xuetao Yin,Rajyashree Mukherjee,Sean Suchter*

Main category: cs.IR

TL;DR: 提出一个统一的查询自动补全框架，通过检索增强生成和多目标直接偏好优化，将QAC重新定义为端到端列表生成，在商业搜索平台上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有查询自动补全方法存在根本性挑战：传统检索-排序流水线覆盖长尾查询有限且需要大量特征工程，而生成方法存在幻觉和安全风险。

Method: 1) 将QAC重新定义为端到端列表生成并进行多目标优化；2) 部署基于规则、模型和LLM作为评判者的验证器，结合RAG、多目标DPO和迭代批判-修订生成高质量合成数据；3) 混合服务架构在严格延迟约束下实现高效生产部署。

Result: 大规模商业搜索平台评估显示：离线指标在所有维度均有提升，人工评估获得+0.40到+0.69偏好分数，在线实验减少5.44%击键次数和增加3.46%建议采纳率。

Conclusion: 通过大语言模型、RAG和多目标对齐驱动的端到端生成代表了范式转变，建立了经过生产验证的框架，可惠及更广泛的搜索和推荐行业。

Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\% reduction in keystrokes and 3.46\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.

</details>


### [41] [GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm](https://arxiv.org/abs/2602.01865)
*Shaopeng Chen,Chuyue Xie,Huimin Ren,Shaozong Zhang,Han Zhang,Ruobing Cheng,Zhiqiang Cao,Zehao Ju,Gao Yu,Jie Ding,Xiaodong Chen,Xuewu Jiao,Shuanglong Li,Liu Lin*

Main category: cs.IR

TL;DR: 提出GRAB框架，使用因果动作感知多通道注意力机制改进CTR预测，相比传统DLRMs显著提升收入和点击率


<details>
  <summary>Details</summary>
Motivation: 传统深度学习推荐模型在性能和效率上面临瓶颈，难以处理泛化和长序列建模问题。受大语言模型扩展成功的启发，需要开发更有效的CTR预测框架。

Method: 提出GRAB（Generative Ranking for Ads at Baidu）端到端生成式框架，采用新颖的因果动作感知多通道注意力（CamA）机制，有效捕捉用户行为序列中的时间动态和特定动作信号。

Result: 大规模在线部署显示，GRAB显著优于现有DLRMs，带来3.05%的收入增长和3.49%的CTR提升。模型表现出良好的扩展性：随着使用更长的交互序列，其表达能力呈现单调且近似线性的提升。

Conclusion: GRAB框架成功将生成式方法应用于CTR预测，通过创新的注意力机制解决了传统推荐模型的局限性，在实际部署中取得了显著的业务效果提升。

Abstract: Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized.

</details>


### [42] [Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation](https://arxiv.org/abs/2602.02024)
*Clémence Réda,Tomas Rigaux,Hiba Bederina,Koh Takeuchi,Hisashi Kashima,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 提出B-DivRec算法，结合行列式点过程和模糊去重技术，在推荐系统中平衡相关性与多样性，并自适应调整质量-多样性权衡


<details>
  <summary>Details</summary>
Motivation: 推荐系统需要同时提供高度相关和多样化的项目，既要个性化又要让用户走出舒适区，但实际应用中存在避免推荐过于相似项目、计算成本高等挑战

Method: 1. 在用户反馈模型已知情况下，提出B-DivRec算法，结合行列式点过程和模糊去重技术调整项目多样性程度；2. 提出自适应方法，根据用户反馈动态调整质量-多样性权衡

Result: 在合成数据和真实数据集（电影推荐和药物再利用）上验证了B-DivRec的性能和通用性

Conclusion: B-DivRec算法能有效平衡推荐系统的相关性与多样性，通过自适应调整策略优化用户体验

Abstract: A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing.

</details>


### [43] [Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs](https://arxiv.org/abs/2602.02338)
*Yu Liang,Zhongjin Zhang,Yuxuan Zhu,Kerui Zhang,Zhiluohan Guo,Wenhang Zhou,Zonqi Yang,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Jianxin Wang,Jiazhi Xia*

Main category: cs.IR

TL;DR: ReSID是一个推荐原生的语义ID框架，通过场感知掩码自编码和全局对齐正交量化，在不依赖LLM的情况下提升序列推荐的性能并大幅降低token化成本。


<details>
  <summary>Details</summary>
Motivation: 现有语义ID推荐方法存在两个问题：1）语义嵌入与协同预测弱耦合；2）通用量化方案在减少序列不确定性方面效率低下，与生成式推荐目标不匹配。

Method: ReSID包含两个核心组件：1）场感知掩码自编码（FAMAE），从结构化特征学习预测充分的物品表示；2）全局对齐正交量化（GAOQ），通过联合减少语义模糊性和前缀条件不确定性来生成紧凑可预测的语义ID序列。

Result: 在10个数据集上的实验表明，ReSID平均超过强序列和语义ID基线10%以上，同时将token化成本降低高达122倍。

Conclusion: ReSID提供了一个不依赖LLM的推荐原生语义ID框架，通过信息保留和序列可预测性视角重新思考表示学习和量化，在性能和效率上均取得显著提升。

Abstract: Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.

</details>


### [44] [RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval](https://arxiv.org/abs/2602.02444)
*Tyler Skow,Alexander Martin,Benjamin Van Durme,Rama Chellappa,Reno Kriz*

Main category: cs.IR

TL;DR: RANKVIDEO是一个基于推理的视频检索重排序模型，通过两阶段课程训练和推理密集型数据合成，在MultiVENT 2.0基准上实现了31%的平均nDCG@10提升。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在文本重排序方面取得了快速进展，但基于推理的视频检索重排序仍然未被充分探索。现有视频检索系统缺乏能够对查询-视频对进行显式推理的重排序模型。

Method: 提出RANKVIDEO模型，采用两阶段课程训练：1) 感知基础监督微调；2) 结合点对点、成对和教师置信度蒸馏目标的重排序训练。同时开发了用于构建推理密集型查询-视频对的数据合成流程。

Result: 在大型MultiVENT 2.0基准测试中，RANKVIDEO在两级检索框架中持续提升检索性能，nDCG@10平均提升31%，优于纯文本和视觉语言重排序替代方案，同时更高效。

Conclusion: RANKVIDEO填补了基于推理的视频检索重排序的研究空白，通过显式推理视频内容来评估相关性，显著提升了视频检索性能，为视频检索系统提供了有效的重排序解决方案。

Abstract: Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [45] [Information Propagation and Encoding in Solids: A Quantitative Approach Towards Mechanical Intelligence](https://arxiv.org/abs/2602.00140)
*Peerasait Prachaseree,Emma Lejeune*

Main category: cs.IT

TL;DR: 该研究提出了一种量化弹性体中信息传播的框架，将物理结构与信息处理能力联系起来，为机械智能设计提供可比较的指标。


<details>
  <summary>Details</summary>
Motivation: 传统工程系统将机械功能与信息处理分离，而生物系统可以利用物理结构进行信息处理。研究旨在探索如何将信息处理能力直接嵌入机械结构，但目前缺乏量化评估框架。

Method: 使用弹性体作为模型系统，应用信息论工具将弹性域视为信息编码器，量化信息从施加载荷到离散传感器位置的传播。将信息度量与经典力学现象（如圣维南效应和主应力线）联系起来。

Result: 建立了弹性体中信息传播的量化框架，展示了如何通过几何形状和结构化材料来调节信息传输，使弹性域能够传输或阻断信息。

Conclusion: 这项工作为机械智能提供了可量化的指标和基准任务，支持机械嵌入式信息处理的比较设计，弥合了物理结构与计算能力之间的差距。

Abstract: Engineered systems typically separate mechanical function from information processing, whereas biological systems can exploit physical structure as a medium for information processing and computation. Motivated by this contrast, recent work in mechanics has explored embedding information-processing capabilities directly into mechanical structures. However, quantitative frameworks for evaluating such capabilities remain limited. Here we address a foundational question: how does information propagate through a solid body? Using elastic bodies as a model system, we apply information-theoretic tools to treat an elastic domain as an information encoder and quantify how information transmits from applied loads to discrete sensor locations. We further connect these measures to familiar mechanical phenomena, including Saint-Venant's effect and principal stress lines. Moving toward design, we show how geometry and architected materials can tune transmission, enabling elastic domains to either transmit or block information. Overall, this work advances quantifiable metrics and benchmark tasks for mechanical intelligence, supporting comparable designs of mechanically embodied information processing.

</details>


### [46] [Semantic-Aware Command and Control Transmission for Multi-UAVs](https://arxiv.org/abs/2602.00142)
*Boya Li,Xiaonan Liu,Dongzhu Liu,Dusit Niyato,Zhu Han*

Main category: cs.IT

TL;DR: 提出基于语义感知的多无人机C&C传输框架，利用语义相似度实现组播传输，优化资源分配，提升传输效率和QoS


<details>
  <summary>Details</summary>
Motivation: 随着无人机数量增加和无线数据爆炸式增长，传统的比特导向通信网络已接近香农容量极限，无法满足无人机C&C传输对超可靠低延迟通信的QoS要求

Method: 1) 利用语义相似度衡量无人机C&C消息在连续传输时间间隔内的变化，并捕捉无人机间的消息相关性以实现组播传输；2) 基于语义相似度和命令重要性设计触发函数量化QoS；3) 开发PPO算法联合确定传输模式（单播/组播/空闲）和有限资源块分配

Result: 实验结果表明，提出的语义感知框架相比比特导向的无人机传输显著提高了传输效率和有效性

Conclusion: 语义感知的C&C传输框架能够有效解决有限无线资源下的多无人机通信问题，通过利用语义相似度实现组播机会，优化资源分配，满足URLLC要求

Abstract: Uncrewed aerial vehicles (UAVs) have played an important role in the low-altitude economy and have been used in various applications. However, with the increasing number of UAVs and explosive wireless data, the existing bit-oriented communication network has approached the Shannon capacity, which cannot satisfy the quality of service (QoS) with ultra-reliable low-latency communication (URLLC) requirements for command and control (C\&C) transmission in bit-oriented UAV communication networks. To address this issue, we propose a novel semantic-aware C\&C transmission for multi-UAVs under limited wireless resources. Specifically, we leverage semantic similarity to measure the variation in C\&C messages for each UAV over continuous transmission time intervals (TTIs) and capture the correlation of C\&C messages among UAVs, enabling multicast transmission. Based on the semantic similarity and the importance of UAV commands, we design a trigger function to quantify the QoS of UAVs. Then, to maximize the long-term QoS and exploit multicast opportunities of C\&C messages induced by semantic similarity, we develop a proximal policy optimization (PPO) algorithm to jointly determine the transmission mode (unicast/multicast/idle) and the allocation of limited resource blocks (RBs) between a base station (BS) and UAVs. Experimental results show that our proposed semantic-aware framework significantly increases transmission efficiency and improves effectiveness compared with bit-oriented UAV transmission.

</details>


### [47] [The structure and enumeration of periodic binary sequences with high nonlinear complexity](https://arxiv.org/abs/2602.01134)
*Qin Yuan,Chunlei Li,Xiangyong Zeng*

Main category: cs.IT

TL;DR: 本文研究了非线性复杂度≥3n/4的n周期二进制序列的结构特征，并推导出此类序列的精确计数公式


<details>
  <summary>Details</summary>
Motivation: 非线性复杂度是评估序列随机性的重要指标，定义为能够生成给定序列的最短反馈移位寄存器的长度。研究高非线性复杂度序列的结构特性对于密码学和序列设计具有重要意义

Method: 首先刻画了非线性复杂度≥3n/4的n周期二进制序列的结构特征，然后基于这些结构特性推导出精确的枚举公式

Result: 获得了此类序列的精确计数公式，为高非线性复杂度序列的定量分析提供了理论依据

Conclusion: 本文成功描述了高非线性复杂度周期序列的结构特征，并给出了精确的计数公式，这对序列设计和密码分析具有重要价值

Abstract: Nonlinear complexity, as an important measure for assessing the randomness of sequences, is defined as the length of the shortest feedback shift registers that can generate a given sequence. In this paper, the structure of n-periodic binary sequences with nonlinear complexity larger than or equal to 3n/4 is characterized. Based on their structure, an exact enumeration formula for the number of such periodic sequences is determined.

</details>


### [48] [On the Palindromic/Reverse-Complement Duplication Correcting Codes](https://arxiv.org/abs/2602.01151)
*Yubo Sun,Gennian Ge*

Main category: cs.IT

TL;DR: 该论文研究用于纠正DNA存储中重复错误的编码，包括反向互补重复和回文重复，提出了多种编码构造方案和理论界限。


<details>
  <summary>Details</summary>
Motivation: 受体内DNA存储应用的启发，研究纠正重复错误的编码问题，特别是反向互补重复和回文重复这两种DNA序列中常见的错误类型。

Method: 1. 构造单冗余符号编码，可纠正任意数量的反向互补/回文重复（要求重复长度k≥3⌈log_q n⌉且不相交）；2. 推导Gilbert-Varshamov界限；3. 针对q≥4，提出两种纠正t个长度1反向互补重复的显式编码构造。

Result: 1. 单冗余符号编码能纠正任意数量的长重复错误；2. 最优冗余上界为2log_q n + log_q log_q n + O(1)；3. 两种长度1重复纠正编码：第一种冗余2t log_q n + O(log_q log_q n)，编解码复杂度较低；第二种冗余(2t-1)log_q n + O(log_q log_q n)，但复杂度较高。

Conclusion: 该论文为DNA存储中的重复错误纠正提供了系统的编码理论和构造方法，包括理论界限和实际可行的编码方案，对DNA数据存储技术的发展具有重要意义。

Abstract: Motivated by applications in in-vivo DNA storage, we study codes for correcting duplications. A reverse-complement duplication of length $k$ is the insertion of the reversed and complemented copy of a substring of length $k$ adjacent to its original position, while a palindromic duplication only inserts the reversed copy without complementation. We first construct an explicit code with a single redundant symbol capable of correcting an arbitrary number of reverse-complement duplications (respectively, palindromic duplications), provided that all duplications have length $k \ge 3\lceil \log_q n \rceil$ and are disjoint. Next, we derive a Gilbert-Varshamov bound for codes that can correct a reverse-complement duplication (respectively, palindromic duplication) of arbitrary length, showing that the optimal redundancy is upper bounded by $2\log_q n + \log_q\log_q n + O(1)$. Finally, for $q \ge 4$, we present two explicit constructions of codes that can correct $t$ length-one reverse-complement duplications. The first construction achieves a redundancy of $2t\log_q n + O(\log_q\log_q n)$ with encoding complexity $O(n)$ and decoding complexity $O\big(n(\log_2 n)^4\big)$. The second construction achieves an improved redundancy of $(2t-1)\log_q n + O(\log_q\log_q n)$, but with encoding and decoding complexities of $O\big(n \cdot \mathrm{poly}(\log_2 n)\big)$.

</details>


### [49] [A class of pseudorandom sequences From Function Fields](https://arxiv.org/abs/2602.01154)
*Xiaofeng Liu,Jun Zhang,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文利用代数函数域上的指数和界，研究了一类p元序列的周期、线性复杂度、线性复杂度轮廓、r-模式分布、周期相关性和非线性复杂度等性质，推广了先前文献中的序列构造。


<details>
  <summary>Details</summary>
Motivation: 受Hu等人（2007）在循环椭圆函数域上构造伪随机序列和Xing等人（2003）从函数域构造低相关、大线性跨度二进制序列的启发，本文旨在推广这些构造并研究更广泛的p元序列的密码学性质。

Method: 利用Weil和Deligne推导的代数函数域上指数和的界，分析一类p元序列的密码学性质。这些序列推广了[IEEE Trans. Inf. Theory, 49(6), 2003]和[IEEE Trans. Inf. Theory, 53(7), 2007]中的构造。

Result: 研究了该p元序列的周期、线性复杂度、线性复杂度轮廓、r-模式分布、周期相关性和非线性复杂度等密码学性质，得到了相应的理论结果。

Conclusion: 通过应用代数函数域上的指数和理论，成功分析了一类推广序列的多种密码学性质，为伪随机序列设计提供了理论依据。

Abstract: Motivated by the constructions of pseudorandom sequences over the cyclic elliptic function fields by Hu \textit{et al.} in \text{[IEEE Trans. Inf. Theory, 53(7), 2007]} and the constructions of low-correlation, large linear span binary sequences from function fields by Xing \textit{et al.} in \text{[IEEE Trans. Inf. Theory, 49(6), 2003]}, we utilize the bound derived by Weil \text{[Basic Number Theory, Grund. der Math. Wiss.,
  Bd 144]} and Deligne \text{[ Lecture Notes in Mathematics, vol. 569 (Springer, Berlin, 1977)]} for the exponential sums over the general algebraic function fields and study the periods, linear complexities, linear complexity profiles, distributions of $r-$patterns, period correlation and nonlinear complexities for a class of $p-$ary sequences that generalize the constructions in \text{[IEEE Trans. Inf. Theory, 49(6), 2003]} and [IEEE Trans. Inf. Theory, 53(7), 2007].

</details>


### [50] [Reducing ORBGRAND Latency via Partial Gaussian Elimination](https://arxiv.org/abs/2602.01174)
*Li Wan,Wenyi Zhang*

Main category: cs.IT

TL;DR: 提出一种基于消除辅助的ORBGRAND解码方案，通过整合RMRE位和部分高斯消除过滤机制，显著减少需要测试的错误模式数量，降低平均和最坏情况延迟，同时保持纠错性能。


<details>
  <summary>Details</summary>
Motivation: ORBGRAND虽然通过利用LLR排序实现了并行解码，但在不利信道条件下仍存在高尾部延迟问题，限制了其在实时系统中的应用。需要一种方法来减少解码延迟，同时保持纠错性能。

Method: 提出消除辅助的ORBGRAND方案，整合RMRE（最可靠错误位排名）与部分高斯消除过滤机制。方案将共享相同RMRE的错误模式分组并联合验证，一旦识别出有效错误模式，就恢复ORBGRAND搜索。通过利用先前的高斯消除步骤过滤不必要的猜测，显著减少需要测试的错误模式数量。

Result: 仿真结果显示，与原始ORBGRAND相比，消除辅助的ORBGRAND过滤掉超过50%的错误模式，相应降低整体计算复杂度，且块错误率没有损失。该方法适用于超可靠低延迟通信场景。

Conclusion: 消除辅助的ORBGRAND方案通过整合RMRE和高斯消除过滤机制，有效减少解码延迟和计算复杂度，同时保持纠错性能，适用于实时通信系统。

Abstract: Guessing Random Additive Noise Decoding (GRAND) is a universal framework for decoding all block codes by testing candidate error patterns (EPs). Ordered Reliability Bits GRAND (ORBGRAND) facilitates parallel implementation of GRAND by exploiting log-likelihood ratio (LLR) rankings but still suffers from high tail latency under unfavorable channel conditions, limiting its use in real-time systems.
  We propose an elimination-aided ORBGRAND scheme that reduces decoding latency by integrating the Rank of the Most Reliable Erroneous (RMRE) bit with a partial Gaussian-elimination (GE) filtering mechanism. The scheme groups and jointly verifies EPs that share the same RMRE, and once a valid EP is identified, the ORBGRAND search is resumed. By leveraging prior GE steps to filter out unnecessary guesses, this approach significantly reduces the number of EPs to be tested, thereby lowering both average and worst-case latency while maintaining error-correction performance.
  Simulation results show that compared to the original ORBGRAND, the elimination-aided ORBGRAND filters out more than 50\% of EPs and correspondingly reduce overall computational complexity, all with no loss in block error rate. This demonstrates that this approach is suitable for ultra-reliable low-latency communication scenarios.

</details>


### [51] [L-Moment-Based LOS and NLOS Channel Characterization via Four-parameter Kappa Distribution for AoA BLE CTE Measurements](https://arxiv.org/abs/2602.01229)
*Hamed Talebian,Aamir Mahmood,Mikael Gidlund*

Main category: cs.IT

TL;DR: 该论文通过BLE CTE传输收集IQ样本，分析LOS和NLOS条件下的统计特性差异，使用L-moment方法验证NLOS具有更重的尾部分布，并通过自监督聚类实现更好的可分离性。


<details>
  <summary>Details</summary>
Motivation: 现有BLE方向查找数据集缺乏严格控制的LOS/NLOS配对IQ测量，且常用的平坦衰落模型在室内多径环境中不足。需要更准确的统计模型来区分LOS和NLOS条件。

Method: 使用商用BLE模块进行配对几何测量，收集132,000个标记CTE数据包。通过稳健预处理去除异常值，使用假设检验验证特征可分离性，计算L-moment比率并在LMRD中分析，最后应用自监督聚类到L-moment统计量。

Result: 特征检验显示LOS和NLOS具有强可分离性，所有均值差异显著，92%的方差差异显著。NLOS子集显示更重的尾部和更强的不对称性。Kappa分布拟合提供更好的拟合优度，自监督聚类相比乘积矩产生更可分离的表示。

Conclusion: L-moment方法能有效表征BLE CTE IQ样本在LOS/NLOS条件下的统计差异，为室内定位系统提供更准确的传播模型，自监督聚类进一步提升了特征可分离性。

Abstract: Bluetooth Low Energy (BLE) CTE transmissions provide in-phase and quadrature (IQ) samples whose empirical statistics are strongly governed by the propagation regime. in particular, the distributions differ markedly between line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. In NLOS, multipath-induced distortions typically degrade Angle-of-Arrivial (AoA) estimation accuracy. Existing BLE direction finding datasets rarely provide tightly controlled, IQ-level paired LOS and NLOS measurements with rigorous statistical validation, and commonly used flat-fading models can be inadequate for cluttered indoor environments exhibiting heavy-tailed power distributions. To address these limitations, we conduct a paired-geometry BLE AoA measurement campaign using an off-the-shelf module, collecting 132000 labeled CTE packets under matched anchor-tag conditions. A robust preprocessing stage removes anomalous CTEs using combined univariate and multivariate criteria. Feature-wise hypothesis tests on IQ-derived power features confirm strong LOS and NLOS separability. All mean differences are statistically significant; additionally, 92 percent of feature-wise variance differences are significant. We further compute L-moment ratios (LMRs) and analyze them in the L-moment Ratio Diagram (LMRD), showing that NLOS subsets exhibit markedly heavier tails and stronger asymmetry than LOS. Kappa-family distributions fitted from LMRs provide substantially improved dual scored L--moment goodness-of-fit (GoF), Specifically, for NLOS, which is the smallest discrepancy in the LMRD and a near-zero standardized L-kurtosis deviation. As a practice, we apply a self-supervised clustering to L-moment statistics, achieving a more separable representation, compared to product moments.

</details>


### [52] [MDS matrices from skew polynomials with automorphisms and derivations](https://arxiv.org/abs/2602.01383)
*Atif Ahmad Khan,Shakir Ali,Elif Segah Oztas,Abhishek Kesarwani*

Main category: cs.IT

TL;DR: 本文提出了一种使用斜多项式环构造MDS矩阵的新方法，引入了δθ-循环矩阵的概念，并构建了准递归MDS矩阵，这些矩阵具有对合性质，改进了现有文献中的准对合构造。


<details>
  <summary>Details</summary>
Motivation: MDS矩阵在编码理论和对称密钥密码学中因其最优扩散特性而至关重要。现有构造方法有限，需要探索新的数学框架来构建具有更好性质的MDS矩阵，特别是对合MDS矩阵在密码学应用中具有重要价值。

Method: 使用斜多项式环F_q[X;θ,δ]作为数学框架，其中θ是自同构，δ是θ-导子。引入δθ-循环矩阵的概念，研究其结构性质。推导这些矩阵成为对合且满足MDS性质的充要条件。在斜多项式环F_q[X;θ]中构造与伴随矩阵相关的准递归MDS矩阵。

Result: 成功构建了δθ-循环MDS矩阵，这是经典构造在无θ-导子情况下的推广。主要贡献是构造了准递归MDS矩阵，这些矩阵被证明是对合的，严格改进了文献中先前报道的准对合构造。提供了多个说明性结果和示例。

Conclusion: 斜多项式环为构造MDS矩阵提供了强大的数学框架，特别是δθ-循环矩阵和准递归MDS矩阵的构造。这些构造不仅推广了经典结果，而且在对合性质方面实现了严格改进，为密码学和编码理论中的MDS矩阵设计提供了新的工具和方法。

Abstract: Maximum Distance Separable (MDS) matrices play a central role in coding theory and symmetric-key cryptography due to their optimal diffusion properties. In this paper, we present a construction of MDS matrices using skew polynomial rings \( \mathbb{F}_q[X;θ,δ] \), where \( θ\) is an automorphism and \( δ\) is a \( θ\)-derivation on \( \mathbb{F}_q \). We introduce the notion of \( δ_θ \)-circulant matrices and study their structural properties. Necessary and sufficient conditions are derived under which these matrices are involutory and satisfy the MDS property. The resulting $δ_θ$-circulant matrix can be viewed as a generalization of classical constructions obtained in the absence of $θ$-derivations. One of the main contribution of this work is the construction of quasi recursive MDS matrices. In the setting of the skew polynomial ring $\mathbb{F}_q[X;θ]$, we construct quasi recursive MDS matrices associated with companion matrices.
  These matrices are shown to be involutory, yielding a strict improvement over the quasi-involutory constructions previously reported in the literature. Several illustrative results and examples are also provided.

</details>


### [53] [Design of Root Protograph LDPC Codes Simultaneously Achieving Full Diversity and High Coding Gain](https://arxiv.org/abs/2602.01555)
*Inki Kim,Hyuntae Ahn,Yongjune Kim,Hee-Youl Kwak,Dae-Young Yun,Sang-Hyo Kim*

Main category: cs.IT

TL;DR: 提出一种新的原图LDPC码设计框架，在块衰落信道中实现全分集，在高斯白噪声信道中实现接近容量性能


<details>
  <summary>Details</summary>
Motivation: 现有LDPC码设计通常在块衰落信道（追求分集）和高斯白噪声信道（追求容量）之间存在性能折衷，需要一种能同时在这两种信道环境中表现优异的编码方案

Method: 1. 基于布尔近似的分集演化分析推导广义根校验结构约束；2. 为两区块块衰落信道设计原图模板；3. 使用密度演化指导的遗传算法优化原图边连接以获得优异AWGN性能

Result: 提出的编码方案在块衰落信道中实现全分集，在AWGN信道中实现接近容量性能，有效弥合了分集导向和容量导向设计之间的差距

Conclusion: 该框架成功设计出在两种不同信道环境下都具有鲁棒性能的原图LDPC码，为跨信道环境的编码设计提供了有效解决方案

Abstract: This paper presents a novel design framework for protograph-based LDPC codes that simultaneously achieves full diversity in block-fading channels (BFCs) and nearcapacity performance in additive white Gaussian noise channels (AWGNCs). By leveraging a Boolean approximation-based analysis--Diversity Evolution (DivE)--we derive structural constraints for generalized rootchecks that guarantee full diversity. Based on these constraints, we propose a protograph template tailored for two-block BFCs. Furthermore, we employ a genetic algorithm guided by density evolution to optimize the protograph edges within this template for superior AWGNC performance. The resulting codes effectively bridge the gap between diversityoriented and capacity-oriented designs, exhibiting robust performance across both channel environments.

</details>


### [54] [On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations](https://arxiv.org/abs/2602.01582)
*Haoyu Lei,Mohammad Jalali,Chin Wa Lau,Farzan Farnia*

Main category: cs.IT

TL;DR: AI纠错解码器在AWGN信道下表现优于传统BP解码，但研究发现其对分布偏移的鲁棒性较差，存在潜在的鲁棒性代价


<details>
  <summary>Details</summary>
Motivation: 探究AI解码器性能提升的来源及其代价，特别是研究其对信道输出分布偏移的鲁棒性

Method: 通过对抗性扰动（FGM和投影梯度法）和通用对抗性扰动，在ℓ₂约束下评估AI解码器的鲁棒性

Result: AI解码器（如ECCT和CrossMPT）在对抗性扰动下性能显著下降，对抗性扰动在AI解码器间转移性强，但对BP解码器转移性弱

Conclusion: AI解码器的性能提升可能以鲁棒性为代价，对信道分布变化更敏感，这揭示了其性能增益背后的潜在脆弱性

Abstract: Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, and what cost is paid to achieve them? In this work, we study this question through the lens of robustness to distributional shifts at the channel output. We evaluate both input-dependent adversarial perturbations (FGM and projected gradient methods under $\ell_2$ constraints) and universal adversarial perturbations that apply a single norm-bounded shift to all received vectors. Our results show that recent AI decoders, including ECCT and CrossMPT, could suffer significant performance degradation under such perturbations, despite superior nominal performance under i.i.d. AWGN. Moreover, adversarial perturbations transfer relatively strongly between AI decoders but weakly to BP-based decoders, and universal perturbations are substantially more harmful than random perturbations of equal norm. These numerical findings suggest a potential robustness cost and higher sensitivity to channel distribution underlying recent AI decoding gains.

</details>


### [55] [Spectral-Aligned Pruning for Universal Error-Correcting Code Transformers](https://arxiv.org/abs/2602.01602)
*Sanghyeon Cho,Taewoo Park,Seong-Joon Park,Dae-Young Yun,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim*

Main category: cs.IT

TL;DR: 提出SAP（谱对齐剪枝）框架，用于对通用信道解码器FECCT进行结构化剪枝，通过利用二分图谱实现跨码重用剪枝掩码，结合LoRA进行码特定恢复，显著降低计算复杂度和内存占用。


<details>
  <summary>Details</summary>
Motivation: 尽管FECCT作为通用信道解码器在多种码族上表现出色，但基于Transformer的架构存在计算复杂度高、参数量大的问题，限制了实际部署。需要一种既能保持性能又能降低资源消耗的剪枝方法。

Method: 提出SAP（谱对齐剪枝）框架：1）利用二分图谱实现结构化剪枝掩码的跨码重用；2）使用参数高效的LoRA（低秩适应）进行码特定恢复；3）在共享剪枝骨干网络基础上仅存储小型码特定适配器参数。

Result: 在多种码型上的实验表明，SAP能够达到与专用逐码剪枝相当的译码性能，同时通过内核级结构化剪枝显著降低计算成本和模型内存占用。

Conclusion: SAP框架成功解决了FECCT部署中的计算复杂度和内存问题，实现了跨码剪枝掩码重用和参数高效恢复，为通用信道解码器的实际应用提供了可行方案。

Abstract: Recently, the Foundation Error Correction Code Transformer (FECCT) has emerged as a promising universal channel decoder, achieving competitive decoding performance across diverse code families by relying on a single shared model backbone, optionally followed by code-specific retraining. Despite this flexibility, the high computational complexity and large parameter footprint of transformer-based decoders present substantial obstacles to practical deployment. To address these challenges, we investigate structured pruning for FECCT and propose Spectral-Aligned Pruning (SAP), a structure-aware framework that enables cross-code reuse of structured pruning masks across codes by leveraging the spectrum of the corresponding bipartite graph. After pruning, SAP performs per-code recovery via parameter-efficient low-rank adaptation (LoRA), enabling a shared pruned backbone while storing only small code-specific adapter parameters. Experiments across diverse codes show that SAP achieves decoding performance comparable to dedicated per-code pruning, while enabling substantial reductions in computational cost and model memory footprint through kernel-level structured pruning.

</details>


### [56] [Low-Complexity Multi-Agent Continual Learning for Stacked Intelligent Metasurface-Assisted Secure Communications](https://arxiv.org/abs/2602.01653)
*Enyu Shi,Yiyang Zhu,Jiayi Zhang,Ziheng Liu,Jiakang Zheng,Jiancheng An,Derrick Wing Kwan Ng,Bo Ai,Chau Yuen*

Main category: cs.IT

TL;DR: 提出了一种基于堆叠智能超表面(SIM)的多用户MIMO系统物理层安全增强方案，采用流形增强的异构多智能体持续学习框架优化波束赋形，显著降低硬件开销和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统多用户MIMO系统需要复杂的基带数字预编码，硬件开销大。堆叠智能超表面(SIM)技术可以在电磁域执行波束赋形，但如何优化SIM相位和基站功率分配以最大化加权和保密率是一个高维非凸优化难题。

Method: 提出SIMHACL框架：1) 每个基站天线传输单用户流，多层SIM在电磁域执行波束赋形；2) 采用流形增强的异构多智能体持续学习(MHACL)框架，结合梯度表示和双尺度策略优化；3) 将相位协调嵌入乘积流形结构，将指数搜索空间降至线性复杂度。

Result: 仿真验证：1) 在SIM辅助系统中实现毫秒级每轮训练；2) 显著优于多种基线方案；3) SIMHACL达到与MHACL相当的加权和保密率，同时减少30%计算时间。

Conclusion: SIM与MHACL框架的结合为未来无线通信安全提供了高效解决方案，通过电磁域波束赋形避免了复杂数字预编码，同时利用智能学习算法有效解决了高维非凸优化问题，实现了性能与复杂度的良好平衡。

Abstract: Stacked intelligent metasurfaces (SIMs), composed of multiple layers of reconfigurable transmissive metasurfaces, are gaining prominence as a transformative technology for future wireless communication security. This paper investigates the integration of SIM into multi-user multiple-input multiple-output (MIMO) systems to enhance physical layer security. A novel system architecture is proposed, wherein each base station (BS) antenna transmits a dedicated single-user stream, while a multi-layer SIM executes wave-based beamforming in the electromagnetic domain, thereby avoiding the need for complex baseband digital precoding and significantly reducing hardware overhead. To maximize the weighted sum secrecy rate (WSSR), we formulate a joint precoding optimization problem over BS power allocation and SIM phase shifts, which is high-dimensional and non-convex due to the complexity of the objective function and the coupling among optimization variables. To address this, we propose a manifold-enhanced heterogeneous multi-agent continual learning (MHACL) framework that incorporates gradient representation and dual-scale policy optimization to achieve robust performance in dynamic environments with high demands for secure communication. Furthermore, we develop SIM-MHACL (SIMHACL), a low-complexity learning template that embeds phase coordination into a product manifold structure, reducing the exponential search space to linear complexity while maintaining physical feasibility. Simulation results validate that the proposed framework achieves millisecond-level per-iteratio ntraining in SIM-assisted systems, significantly outperforming various baseline schemes, with SIMHACL achieving comparable WSSR to MHACL while reducing computation time by 30\%.

</details>


### [57] [Decoding Golay Codes and their Related Lattices: A PAC Code Perspective](https://arxiv.org/abs/2602.01657)
*Yujun Ji,Ling Liu,Shanxiang Lyu,Chao Chen,Tao Dai,Baoming Bai*

Main category: cs.IT

TL;DR: 提出一种基于极化调整卷积码（PAC码）的Golay码解码方法，通过Forney的立方构造和生成器G*(8,7)/(8,4)找到Golay码的不同构造方式，实现高效的并行列表解码算法，性能接近最大似然。


<details>
  <summary>Details</summary>
Motivation: 现有Golay码解码方法通常需要索引置换和码字打孔操作，这些操作增加了计算复杂度。本文旨在开发一种更高效的解码方法，避免这些复杂操作，同时将方法扩展到相关格结构（如Leech格）的解码。

Method: 利用Forney的Golay码立方构造和生成器G*(8,7)/(8,4)，从PAC码的角度重新构造Golay码。基于这些新构造，开发了并行列表解码算法，该算法不需要索引置换和码字打孔操作。

Result: 提出的解码方法实现了接近最大似然性能，相比现有方法消除了索引置换和码字打孔的需求。该方法还能高效解码相关格结构，如Leech格Λ24及其主子格H24。

Conclusion: 从PAC码角度提出的Golay码解码方法提供了一种高效且性能优越的替代方案，能够扩展到相关格结构的解码，为Golay码及其相关结构的解码提供了新的思路。

Abstract: In this work, we propose a decoding method of Golay codes from the perspective of Polarization Adjusted Convolutional (PAC) codes. By invoking Forney's cubing construction of Golay codes and their generators $G^*(8,7)/(8,4)$, we found different construction methods of Golay codes from PAC codes, which result in an efficient parallel list decoding algorithm with near-maximum likelihood performance. Compared with existing methods, our method can get rid of index permutation and codeword puncturing. Using the new decoding method, some related lattices, such as Leech lattice $Λ_{24}$ and its principal sublattice $H_{24}$, can be also decoded efficiently.

</details>


### [58] [Performance Guarantees of Cellular Networks with Hardcore Regulation and Scheduling](https://arxiv.org/abs/2602.01802)
*Ke Feng,François Baccelli,Catherine Rosenberg*

Main category: cs.IT

TL;DR: 本文研究了蜂窝网络中空间硬核调控与基站调度对性能保证的影响，通过空间网络演算分析干扰上界，识别调度优于始终激活的场景，为网络设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现代通信网络需要提供性能保证，空间调控（如基站间距约束）已被证明对建立可证明的无线链路级保证至关重要。本文旨在研究蜂窝网络下行链路中，在硬核空间调控下，基站调度如何影响性能保证。

Method: 采用空间网络演算框架，首先为空间调控的蜂窝网络提供总干扰功率的上界，然后识别基站调度比基站始终激活能提供更好链路级速率保证的机制。特别分析了六边形蜂窝网络作为特例。

Result: 获得了空间调控蜂窝网络的干扰上界，确定了调度优于始终激活的机制。六边形网络分析提供了具体案例。结果揭示了需要何种空间调控、何时选择调度以及如何降低网络功耗以满足特定性能保证目标。

Conclusion: 空间硬核调控与基站调度的结合能够改善蜂窝网络的性能保证。研究结果为网络设计提供了重要见解：明确了所需的空间调控类型、调度策略的选择时机以及降低功耗以实现目标性能保证的潜在方法。

Abstract: Providing performance guarantees is one of the {critical} objectives of {recent and future} communication networks, toward which regulations, {i.e., constraints on key system parameters,} have played an indispensable role. This is the case for large wireless communication networks, where spatial regulations (e.g., constraints on intercell distance) have recently been shown, through a spatial network calculus, to be essential for establishing provable wireless link-level guarantees. In this work, we focus on performance guarantees for {the downlink of} cellular networks where we impose a hardcore (spatial) regulation on base station (BS) locations and evaluate {how BS scheduling (which controls which BSs can transmit at a given time) impacts performance}. Hardcore regulation is the simplest form of spatial regulation that enforces a minimal distance between any pair of transmitters in the network. Within this framework of spatial network calculus, we first provide an upper bound on the power of total interference for a spatially regulated cellular network, and then, identify the regimes where scheduling BSs yields {better} link-level rate guarantees compared to scenarios where base stations are always active. The hexagonal cellular network is analyzed as a special case. The results offer insights into what spatial regulations are needed, when to choose scheduling, and how to potentially reduce the network power consumption {to provide a certain target performance guarantee}.

</details>


### [59] [Zero-Shot Knowledge Base Resizing for Rate-Adaptive Digital Semantic Communication](https://arxiv.org/abs/2602.01829)
*Shumin Yao,Hui Du,Lifeng Xie,Yaping Sun,Hao Chen,Nan Ma,Xiaodong Xu*

Main category: cs.IT

TL;DR: 提出一种零样本知识库大小调整方法，通过揭示语义层次结构实现VQ-VAE语义通信系统的动态速率适配，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE语义通信系统中，传输速率由知识库大小决定，但传统方法需要为每个不同大小的知识库单独训练和存储模型，计算和存储成本过高，无法实现精细的速率控制。

Method: 1) 将知识库向量嵌入双曲空间以揭示层次关系；2) 使用最小生成树算法构建主语义树；3) 通过迭代剪枝最不重要的叶节点实现即时大小调整。

Result: 该方法的重建质量与从头训练的专业知识库几乎相同，计算成本仅为一小部分，且在极低速率下表现出更强的鲁棒性，避免了传统知识库的灾难性失败。

Conclusion: 解决了VQ-VAE语义通信系统的基本限制，为灵活、速率自适应的语义通信提供了实用高效的路径。

Abstract: Digital semantic communication systems, which often leverage the Vector Quantized Variational Autoencoder (VQ-VAE) framework, are pivotal for future wireless networks. In a VQ-VAE-based semantic communication system, the transmission rate is directly governed by the size of a discrete codebook known as knowledge base (KB). However, the KB size is a fixed hyperparameter, meaning that adapting the rate requires training and storing a separate model for each desired size -- a practice that is too computationally and storage-prohibitive to achieve truly granular rate control. To address this, we introduce a principled, zero-shot KB resizing method that enables on-the-fly rate adaptation without any retraining. Our approach establishes a global importance ranking for all vectors within a single, large parent KB by uncovering its inherent semantic hierarchy. This is achieved via a three-step framework: 1) embedding KB vectors into hyperbolic space to reveal their hierarchical relationships; 2) constructing a master semantic tree using a minimum spanning tree algorithm; 3) enabling instant resizing by iteratively pruning the least important leaf nodes. Extensive simulations demonstrate that our method achieves reconstruction quality nearly identical to that of dedicated KBs trained from scratch, while demanding only a fraction of the computational budget. Moreover, our approach exhibits superior robustness at very low rates, where conventional KBs suffer from catastrophic failure. Our work resolves a fundamental limitation of VQ-VAE-based semantic communication systems, offering a practical and efficient path toward flexible and rate-adaptive semantic communication.

</details>


### [60] [Two-Stage Coded-Sliding Beam Training and QoS-Constrained Sum-Rate Maximization for SIM-Assisted Wireless Communications](https://arxiv.org/abs/2602.02131)
*Qian Zhang,Ju Liu,Yao Ge,Yufei Zhao,Wali Ullah Khan,Zheng Dong,Yong Liang Guan,Chau Yuen*

Main category: cs.IT

TL;DR: 提出一个用于SIM辅助通信系统的低复杂度算法统一框架，包括广义两步码本构造、两阶段编码滑动波束训练和可变解耦块连续上界最小化算法，以解决信道状态信息获取和相移优化问题。


<details>
  <summary>Details</summary>
Motivation: 堆叠智能超表面为大规模天线通信提供了经济高效的解决方案，但高效的信道状态信息获取和相移优化仍然是关键挑战，需要低复杂度算法来解决这些问题。

Method: 1) 广义两步码本构造方法：利用二维角度域解耦将平面阵列波束形成器设计转化为两个独立的一维线性阵列设计问题，通过Gerchberg-Saxton算法和提出的基于主最小化的近端距离算法求解；2) 两阶段编码滑动波束训练方法：第一阶段嵌入纠错码增强抗噪性，第二阶段在匹配角度样本周围进行滑动采样提高角度分辨率；3) 可变解耦块连续上界最小化算法：通过闭式迭代更新直接求解QoS约束的和速率最大化问题。

Result: 仿真结果表明，所提方法能够实现精确的波束模式实现、改进的波束训练精度和角度分辨率，以及增强的和速率性能。

Conclusion: 提出的统一框架为SIM辅助通信系统提供了有效的低复杂度解决方案，成功解决了信道状态信息获取和相移优化的关键挑战，在多径用户信道中具有良好的扩展性。

Abstract: Stacked intelligent metasurfaces (SIM) provide a cost-effective and scalable solution for large-scale antenna communications.However, efficient channel state information acquisition and phase shift optimization remain critical challenges. In this paper, we develop a unified framework of low-complexity algorithms for SIM-assisted communication systems to address these issues. Specifically, we propose a generalized two-step codebook construction (TSCC) method that leverages two-dimensional angular-domain decoupling to transform planar array beamformer design into two independent one-dimensional linear array beamformer design problems, efficiently solved via the Gerchberg-Saxton algorithm and our proposed majorization-minimization-based proximal distance (PDMM) algorithm. We further develop a two-stage coded-sliding beam training (TSCSBT) method for low-overhead and high-accuracy beam training, where error-correcting codes are embedded in the first-stage training to enhance robustness against noise, and sliding sampling is subsequently performed around the matched angular samples to improve angular resolution. The proposed framework is further extended to multi-path user channels. Finally, a variable decoupling-based block successive upper bound minimization (VD-BSUM) algorithm is proposed to directly solve the QoS-constrained sum-rate maximization problem through closed-form iterative updates with substantially reduced computational complexity. Simulation results demonstrate the effectiveness of the proposed methods in achieving precise beam pattern realization, improved beam training accuracy and angular resolution, and enhanced sum-rate performance.

</details>


### [61] [Preemptive Scheduling for Age of Job Minimization in Task-Specific Machine Networks](https://arxiv.org/abs/2602.02435)
*Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出基于作业年龄的调度策略，包括WI、WIMWF和NGM策略，在不同系统规模下各有优势


<details>
  <summary>Details</summary>
Motivation: 研究多用户多网络作业分配系统中的调度问题，由于资源限制无法同时服务所有作业，需要设计有效的调度策略来优化作业时效性

Method: 引入作业年龄作为时效性指标，提出max-weight策略、Whittle index策略（几何分布）、WIMWF策略（一般分布）和Net-gain最大化策略

Result: WIMWF策略在一般作业完成时间分布下表现最佳；小系统中max-weight策略优于NGM策略，大系统中NGM策略渐近更优；几何服务时间下WI策略在所有系统规模中表现最好

Conclusion: 针对不同作业完成时间分布和系统规模，提出了相应的优化调度策略，WIMWF策略在一般分布下具有最佳性能，系统规模对策略选择有重要影响

Abstract: We consider a time-slotted job-assignment system consisting of a central server, $N$ task-specific networks of machines, and multiple users. Each network specializes in executing a distinct type of task. Users stochastically generate jobs of various types and forward them to the central server, which routes each job to the appropriate network of machines. Due to resource constraints, the server cannot serve all users' jobs simultaneously, which motivates the design of scheduling policies with possible preemption. To evaluate scheduling performance, we introduce a novel timeliness metric, the age of job, inspired by the well-known metric, the age of information. We study the problem of minimizing the long-term weighted average age of job. We first propose a max-weight policy by minimizing the one-step Lyapunov drift and then derive the Whittle index (WI) policy when the job completion times of the networks of machines follow geometric distributions. For general job completion time distributions, we introduce a Whittle index with max-weight fallback (WIMWF) policy. We also investigate the Net-gain maximization (NGM) policy. Numerically, we show that the proposed WIMWF policy achieves the best performance in the general job completion time setting. We also observe a scaling trend: two different max-weight policies can outperform the NGM policy in small systems, whereas the NGM policy improves as we scale the system size and becomes asymptotically better than max-weight policies. For geometric service times, the WI policy yields the lowest age across all considered system sizes.

</details>


### [62] [Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation](https://arxiv.org/abs/2602.02469)
*Ahmed M. Elshazly,Ahmed Arafa*

Main category: cs.IT

TL;DR: 提出了一种无需设备端信道状态信息的联邦学习方法，利用多天线MRC检测和AgeTop-k坐标选择策略，在单OFDM符号内传输模型更新，降低延迟。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中，多设备同时发送模型更新时面临信道状态信息获取困难、正交子载波有限导致传输延迟增加的问题，需要设计高效的空中计算方案。

Method: 提出年龄感知的边缘盲空中联邦学习方法：1) 设备端无需CSI；2) 参数服务器使用多天线和MRC基于估计的信道增益和检测更新；3) 采用AgeTop-k选择策略，优先选择最大幅度且等待时间最长的k个坐标，确保单OFDM符号传输。

Result: 实验表明：1) 更多PS天线显著提高精度和收敛速度；2) 在较好信道条件下，AgeTop-k优于随机选择；3) 最优k值取决于信道条件，噪声环境下较小的k更好。

Conclusion: 该方法通过多天线MRC和AgeTop-k坐标选择，在无需设备端CSI的情况下实现了高效的无线联邦学习，平衡了压缩误差和信道噪声的影响，显著降低了延迟。

Abstract: We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.

</details>


### [63] [Secure Multi-User Linearly-Separable Distributed Computing](https://arxiv.org/abs/2602.02489)
*Amir Masoud Jafarpisheh,Ali Khalesi,Petros Elia*

Main category: cs.IT

TL;DR: 该论文在线性可分离分布式计算框架中引入信息论保密性，提出了两个充要保密准则，并设计了适用于有限域和实数域的通用方案，在保持性能的同时实现完美保密或可任意小的互信息泄露。


<details>
  <summary>Details</summary>
Motivation: 现有的多用户线性可分离分布式计算框架虽然能提供接近最优的性能，但其线性特性引发了数据保密性问题。论文旨在建立信息论保密框架，确保每个用户只能学习到自己请求的函数，无法获取其他用户的信息。

Method: 提出了两个充要保密准则：1）每个用户观察到的服务器响应中，可见的公共随机性必须恰好跨越α_k-1维子空间；2）从矩阵D中移除用户观察的服务器对应列后，矩阵秩至少为K-1。基于这些条件，设计了一个通用方案：在矩阵E后附加Null(D)的基，并精心注入共享随机性。

Result: 该方案在保持原有性能的同时，在有限域情况下能实现完美信息论保密，在实数域情况下能提供明确的互信息界限，通过增加高斯公共随机性的方差可以使互信息泄露任意小。

Conclusion: 论文成功解决了线性可分离分布式计算框架的保密性问题，提出的保密准则和设计方案具有通用性，适用于有限域和实数域，在许多情况下无需额外成本即可实现保密性保障。

Abstract: The introduction of the new multi-user linearly-separable distributed computing framework, has recently revealed how a parallel treatment of users can yield large parallelization gains with relatively low computation and communication costs. These gains stem from a new approach that converts the computing problem into a sparse matrix factorization problem; a matrix $F$ that describes the users' requests, is decomposed as \(F = DE\), where a \(γ\)-sparse \(E\) defines the task allocation across $N$ servers, and a \(δ\)-sparse \(D\) defines the connectivity between \(N\) servers and \(K\) users as well as the decoding process. While this approach provides near-optimal performance, its linear nature has raised data secrecy concerns.
  We here adopt an information-theoretic secrecy framework, seeking guarantees that each user can learn nothing more than its own requested function. In this context, our main result provides two necessary and sufficient secrecy criteria; (i) for each user \(k\) who observes $α_k$ server responses, the common randomness visible to that user must span a subspace of dimension exactly $α_k-1$,
  and (ii) for each user, removing from \(\mathbf{D}\) the columns corresponding to the servers it observes must leave a matrix of rank at least \(K-1\). With these conditions in place, we design a general scheme -- that applies to finite and non-finite fields alike -- which is based on appending to \(\mathbf{E}\) a basis of \(\mathrm{Null}(\mathbf{D})\) and by carefully injecting shared randomness. In many cases, this entails no additional costs. The scheme, while maintaining performance, guarantees perfect information-theoretic secrecy in the case of finite fields, while in the real case, the conditions yield an explicit mutual-information bound that can be made arbitrarily small by increasing the variance of Gaussian common randomness.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [64] [Divide and Conquer: Multimodal Video Deepfake Detection via Cross-Modal Fusion and Localization](https://arxiv.org/abs/2602.00209)
*Qingcao Li,Miao He,Liang Yi,Qing Wen,Yitao Zhang,Hongshuo Jin,Peng Cheng,Zhongjie Ba,Li Lu,Kui Ren*

Main category: cs.MM

TL;DR: 该论文提出了一种用于检测视频深度伪造内容的两阶段系统，包含单模态检测和多模态分数融合，在DDL挑战赛Track 2中取得了0.5528的最终得分。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展，检测音频-视频伪造内容变得日益重要。DDL挑战赛Track 2旨在开发能够准确检测和定位视频中伪造内容的技术，需要同时处理音频和视觉模态的伪造检测。

Method: 采用两阶段框架：1）单模态检测阶段包含音频深度伪造检测模块、音频定位模块、图像深度伪造检测和定位模块；2）多模态分数融合阶段，通过分析训练和评估数据集，探索多种分数计算和融合策略，有效整合不同模态的互补信息。

Result: 在挑战赛测试集上，最终融合系统取得了AUC 0.87、AP 0.55、AR 0.23的成绩，最终得分为0.5528，表明多模态融合策略能有效提升系统鲁棒性。

Conclusion: 提出的两阶段多模态融合系统在视频深度伪造检测任务中表现良好，通过整合音频和视觉模态的互补信息，提高了检测和定位的准确性，为深度伪造检测提供了有效的技术方案。

Abstract: This paper presents a system for detecting fake audio-visual content (i.e., video deepfake), developed for Track 2 of the DDL Challenge. The proposed system employs a two-stage framework, comprising unimodal detection and multimodal score fusion. Specifically, it incorporates an audio deepfake detection module and an audio localization module to analyze and pinpoint manipulated segments in the audio stream. In parallel, an image-based deepfake detection and localization module is employed to process the visual modality. To effectively leverage complementary information across different modalities, we further propose a multimodal score fusion strategy that integrates the outputs from both audio and visual modules. Guided by a detailed analysis of the training and evaluation dataset, we explore and evaluate several score calculation and fusion strategies to improve system robustness. Overall, the final fusion-based system achieves an AUC of 0.87, an AP of 0.55, and an AR of 0.23 on the challenge test set, resulting in a final score of 0.5528.

</details>


### [65] [MTAVG-Bench: A Comprehensive Benchmark for Evaluating Multi-Talker Dialogue-Centric Audio-Video Generation](https://arxiv.org/abs/2602.00607)
*Yang-Hao Zhou,Haitian Li,Rexar Lin,Heyan Huang,Jinxing Zhou,Changsen Yuan,Tian Lan,Ziqin Zhou,Yudong Li,Jiajun Xu,Jingyun Liao,Yi-Ming Cheng,Xuefeng Chen,Xian-Ling Mao,Yousheng Feng*

Main category: cs.MM

TL;DR: MTAVG-Bench：首个用于评估多说话者对话视频生成的基准，包含1.8k生成视频和2.4k人工标注QA对，从四个层次评估模型性能


<details>
  <summary>Details</summary>
Motivation: 现有评估基准主要针对真人录制视频或单说话者场景，无法有效捕捉生成多说话者对话视频中的潜在错误（如身份漂移、不自然的对话转换、音视频不对齐等）

Method: 通过半自动流程构建：使用多个流行模型生成1.8k视频，设计精心设计的提示词，人工标注2.4k QA对，从四个层次评估：音视频信号保真度、时间属性一致性、社交互动、电影表达

Result: 在12个专有和开源全模态模型上测试，Gemini 3 Pro表现最强，领先的开源模型在信号保真度和一致性方面保持竞争力

Conclusion: MTAVG-Bench能够进行细粒度失败分析，支持严格的模型比较和有针对性的视频生成改进

Abstract: Recent advances in text-to-audio-video (T2AV) generation have enabled models to synthesize audio-visual videos with multi-participant dialogues. However, existing evaluation benchmarks remain largely designed for human-recorded videos or single-speaker settings. As a result, potential errors that occur in generated multi-talker dialogue videos, such as identity drift, unnatural turn transitions, and audio-visual misalignment, cannot be effectively captured and analyzed. To address this issue, we introduce MTAVG-Bench, a benchmark for evaluating audio-visual multi-speaker dialogue generation. MTAVG-Bench is built via a semi-automatic pipeline, where 1.8k videos are generated using multiple popular models with carefully designed prompts, yielding 2.4k manually annotated QA pairs. The benchmark evaluates multi-speaker dialogue generation at four levels: audio-visual signal fidelity, temporal attribute consistency, social interaction, and cinematic expression. We benchmark 12 proprietary and open-source omni-models on MTAVG-Bench, with Gemini 3 Pro achieving the strongest overall performance, while leading open-source models remain competitive in signal fidelity and consistency. Overall, MTAVG-Bench enables fine-grained failure analysis for rigorous model comparison and targeted video generation refinement.

</details>


### [66] [Cross-Modal Binary Attention: An Energy-Efficient Fusion Framework for Audio-Visual Learning](https://arxiv.org/abs/2602.00701)
*Mohamed Saleh,Zahra Ahmadi*

Main category: cs.MM

TL;DR: CMQKA是一种线性复杂度的跨模态融合机制，结合SNNergy框架实现高效能的多模态融合，在多个音频-视觉基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉融合方法面临根本性权衡：基于注意力的方法能有效建模跨模态关系但计算复杂度高（二次方），而高效融合策略依赖简单拼接无法提取互补的跨模态信息。需要一种既能捕获复杂跨模态依赖又计算可扩展的融合机制。

Method: 提出CMQKA（跨模态查询-键注意力）机制，通过高效二进制操作实现线性复杂度，采用双向跨模态Query-Key注意力提取互补时空特征，使用可学习残差融合保持模态特异性特征。在此基础上构建SNNergy框架，采用分层架构处理多尺度输入，使用事件驱动的二进制脉冲操作实现高能效。

Result: 在CREMA-D、AVE和UrbanSound8K-AV等挑战性音频-视觉基准测试中显著优于现有多模态融合基线，建立了新的SOTA结果，同时实现了卓越的能效。

Conclusion: CMQKA和SNNergy框架通过引入可扩展的融合机制，实现了分层跨模态集成，为实际音频-视觉智能系统提供了实用的能效解决方案，推动了多模态融合的发展。

Abstract: Effective multimodal fusion requires mechanisms that can capture complex cross-modal dependencies while remaining computationally scalable for real-world deployment. Existing audio-visual fusion approaches face a fundamental trade-off: attention-based methods effectively model cross-modal relationships but incur quadratic computational complexity that prevents hierarchical, multi-scale architectures, while efficient fusion strategies rely on simplistic concatenation that fails to extract complementary cross-modal information. We introduce CMQKA, a novel cross-modal fusion mechanism that achieves linear O(N) complexity through efficient binary operations, enabling scalable hierarchical fusion previously infeasible with conventional attention. CMQKA employs bidirectional cross-modal Query-Key attention to extract complementary spatiotemporal features and uses learnable residual fusion to preserve modality-specific characteristics while enriching representations with cross-modal information. Building upon CMQKA, we present SNNergy, an energy-efficient multimodal fusion framework with a hierarchical architecture that processes inputs through progressively decreasing spatial resolutions and increasing semantic abstraction. This multi-scale fusion capability allows the framework to capture both local patterns and global context across modalities. Implemented with event-driven binary spike operations, SNNergy achieves remarkable energy efficiency while maintaining fusion effectiveness and establishing new state-of-the-art results on challenging audio-visual benchmarks, including CREMA-D, AVE, and UrbanSound8K-AV, significantly outperforming existing multimodal fusion baselines. Our framework advances multimodal fusion by introducing a scalable fusion mechanism that enables hierarchical cross-modal integration with practical energy efficiency for real-world audio-visual intelligence systems.

</details>


### [67] [Seeing, Hearing, and Knowing Together: Multimodal Strategies in Deepfake Videos Detection](https://arxiv.org/abs/2602.01284)
*Chen Chen,Dion Hoe-Lian Goh*

Main category: cs.MM

TL;DR: 研究通过195名参与者的实验，发现人们在识别真实视频和深度伪造视频时使用不同线索，视觉、声音和直觉的多模态组合对成功识别至关重要，为媒体素养工具设计提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造视频越来越难以识别，了解人类使用的识别策略对于设计有效的媒体素养干预措施至关重要。研究旨在探索人们在判断真实与伪造视频时依赖的线索及其组合。

Method: 研究招募195名21-40岁参与者，让他们判断真实和深度伪造视频，评估自信度，并报告依赖的视觉、音频和知识策略线索。使用关联规则挖掘分析线索组合如何影响识别表现。

Result: 参与者识别真实视频比深度伪造更准确，对真实内容的预期校准误差更低。通过关联规则挖掘发现，视觉外观、声音特征和直觉经常共同出现于成功识别中，表明多模态方法在人类检测中的重要性。

Conclusion: 研究揭示了哪些线索有助于或阻碍深度伪造检测，为设计引导有效线索使用的媒体素养工具提供了方向。基于这些见解可以帮助人们提高识别技能，增强对欺骗性数字媒体的抵抗力。

Abstract: As deepfake videos become increasingly difficult for people to recognise, understanding the strategies humans use is key to designing effective media literacy interventions. We conducted a study with 195 participants between the ages of 21 and 40, who judged real and deepfake videos, rated their confidence, and reported the cues they relied on across visual, audio, and knowledge strategies. Participants were more accurate with real videos than with deepfakes and showed lower expected calibration error for real content. Through association rule mining, we identified cue combinations that shaped performance. Visual appearance, vocal, and intuition often co-occurred for successful identifications, which highlights the importance of multimodal approaches in human detection. Our findings show which cues help or hinder detection and suggest directions for designing media literacy tools that guide effective cue use. Building on these insights can help people improve their identification skills and become more resilient to deceptive digital media.

</details>


### [68] [Mixture of Disentangled Experts with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.01833)
*Xiang Li,Xiaoming Zhang,Dezhuang Miao,Xianfu Cheng,Dawei Li,Honggui Han,Zhoujun Li*

Main category: cs.MM

TL;DR: DERL是一个解耦专家表示学习框架，通过混合专家自适应地将多模态输入解耦到正交的私有和共享表示空间，使用多级重建策略增强表示的表达性和鲁棒性，在模态缺失条件下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界噪声常导致多模态情感分析中数据缺失或损坏，现有特征解耦方法难以处理异构信息在不确定缺失情况下的内部变化，难以从退化模态中学习有效的多模态表示。

Method: 提出DERL框架：1) 使用混合专家自适应地将多模态输入解耦到正交的私有和共享表示空间；2) 开发多级重建策略提供协同监督；3) 解耦特征作为具有不同角色的模态专家生成重要性感知的融合结果。

Result: 在两个MSA基准测试中，DERL在各种缺失模态条件下优于最先进方法。例如，在MOSI数据集上，在模态内缺失情况下，Acc-2提升2.47%，MAE提升2.25%。

Conclusion: DERL通过解耦专家表示学习有效处理多模态情感分析中的模态缺失问题，增强了表示的表达性和鲁棒性，在现实噪声条件下表现出优越性能。

Abstract: Multimodal Sentiment Analysis (MSA) integrates multiple modalities to infer human sentiment, but real-world noise often leads to missing or corrupted data. However, existing feature-disentangled methods struggle to handle the internal variations of heterogeneous information under uncertain missingness, making it difficult to learn effective multimodal representations from degraded modalities. To address this issue, we propose DERL, a Disentangled Expert Representation Learning framework for robust MSA. Specifically, DERL employs hybrid experts to adaptively disentangle multimodal inputs into orthogonal private and shared representation spaces. A multi-level reconstruction strategy is further developed to provide collaborative supervision, enhancing both the expressiveness and robustness of the learned representations. Finally, the disentangled features act as modality experts with distinct roles to generate importance-aware fusion results. Extensive experiments on two MSA benchmarks demonstrate that DERL outperforms state-of-the-art methods under various missing-modality conditions. For instance, our method achieves improvements of 2.47% in Acc-2 and 2.25% in MAE on MOSI under intra-modal missingness.

</details>
