{"id": "2511.10804", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10804", "abs": "https://arxiv.org/abs/2511.10804", "authors": ["Pål Grønås Drange", "Fedor V. Fomin", "Petr Golovach", "Danil Sagunov"], "title": "Discounted Cuts: A Stackelberg Approach to Network Disruption", "comment": "Accepted to AAAI 2026", "summary": "We study a Stackelberg variant of the classical Most Vital Links problem, modeled as a one-round adversarial game between an attacker and a defender. The attacker strategically removes up to $k$ edges from a flow network to maximally disrupt flow between a source $s$ and a sink $t$, after which the defender optimally reroutes the remaining flow. To capture this attacker--defender interaction, we introduce a new mathematical model of discounted cuts, in which the cost of a cut is evaluated by excluding its $k$ most expensive edges. This model generalizes the Most Vital Links problem and uncovers novel algorithmic and complexity-theoretic properties.\n  We develop a unified algorithmic framework for analyzing various forms of discounted cut problems, including minimizing or maximizing the cost of a cut under discount mechanisms that exclude either the $k$ most expensive or the $k$ cheapest edges. While most variants are NP-complete on general graphs, our main result establishes polynomial-time solvability for all discounted cut problems in our framework when the input is restricted to bounded-genus graphs, a relevant class that includes many real-world networks such as transportation and infrastructure networks. With this work, we aim to open collaborative bridges between artificial intelligence, algorithmic game theory, and operations research."}
{"id": "2511.10823", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.10823", "abs": "https://arxiv.org/abs/2511.10823", "authors": ["Tim Randolph", "Karol Węgrzycki"], "title": "Beating Meet-in-the-Middle for Subset Balancing Problems", "comment": "43 pages, 1 figure", "summary": "We consider exact algorithms for Subset Balancing, a family of related problems that generalizes Subset Sum, Partition, and Equal Subset Sum. Specifically, given as input an integer vector $\\vec{x} \\in \\mathbb{Z}^n$ and a constant-size coefficient set $C \\subset \\mathbb{Z}$, we seek a nonzero solution vector $\\vec{c} \\in C^n$ satisfying $\\vec{c} \\cdot \\vec{x} = 0$.\n  For $C = \\{-d,\\ldots,d\\}$, $d > 1$ and $C = \\{-d,\\ldots,d\\}\\setminus\\{0\\}$, $d > 2$, we present algorithms that run in time $O(|C|^{(0.5 - ε)n})$ for a constant $ε> 0$ that depends only on $C$. These are the first algorithms that break the $O(|C|^{n/2})$-time ``Meet-in-the-Middle barrier'' for these coefficient sets in the worst case. This improves on the result of Chen, Jin, Randolph and Servedio (SODA 2022), who broke the Meet-in-the-Middle barrier on these coefficient sets in the average-case setting. We also improve the best exact algorithm for Equal Subset Sum (Subset Balancing with $C = \\{-1,0,1\\}$), due to Mucha, Nederlof, Pawlewicz, and Węgrzycki (ESA 2019), by an exponential margin. This positively answers an open question of Jin, Williams, and Zhang (ESA 2025). Our results leave two natural cases in which we cannot yet break the Meet-in-the-Middle barrier: $C = \\{-2, -1, 1, 2\\}$ and $C = \\{-1, 1\\}$ (Partition).\n  Our results bring the representation technique of Howgrave-Graham and Joux (CRYPTO 2010) from average-case to worst-case inputs for many $C$. This requires a variety of new techniques: we present strategies for (1) achieving good ``mixing'' with worst-case inputs, (2) creating flexible input representations for coefficient sets without 0, and (3) quickly recovering compatible solution pairs from sets of vectors containing ``pseudosolution pairs''. These techniques may find application to other algorithmic problems on integer sums or be of independent interest."}
{"id": "2511.10851", "categories": ["cs.DS", "cs.CC", "math.NT"], "pdf": "https://arxiv.org/pdf/2511.10851", "abs": "https://arxiv.org/abs/2511.10851", "authors": ["Chris Umans", "Siki Wang"], "title": "A number-theoretic conjecture implying faster algorithms for polynomial factorization and integer factorization", "comment": null, "summary": "The fastest known algorithm for factoring a degree $n$ univariate polynomial over a finite field $\\mathbb{F}_q$ runs in time $O(n^{3/2 + o(1)}\\text{polylog } q)$, and there is a reason to believe that the $3/2$ exponent represents a ''barrier'' inherent in algorithms that employ a so-called baby-steps-giant-steps strategy. In this paper, we propose a new strategy with the potential to overcome the $3/2$ barrier. In doing so we are led to a number-theoretic conjecture, one form of which is that there are sets $S, T$ of cardinality $n^β$, consisting of positive integers of magnitude at most $\\exp(n^α)$, such that every integer $i \\in [n]$ divides $s-t$ for some $s \\in S, t \\in T$. Achieving $α+ β\\le 1 + o(1)$ is trivial; we show that achieving $α, β< 1/2$ (together with an assumption that $S, T$ are structured) implies an improvement to the exponent 3/2 for univariate polynomial factorization. Achieving $α= β= 1/3$ is best-possible and would imply an exponent 4/3 algorithm for univariate polynomial factorization. Interestingly, a second consequence would be a reduction of the current-best exponent for deterministic (exponential) algorithms for factoring integers, from $1/5$ to $1/6$."}
{"id": "2511.10961", "categories": ["cs.DS", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.10961", "abs": "https://arxiv.org/abs/2511.10961", "authors": ["Fan Wang", "Sandy Irani"], "title": "Cycle Basis Algorithms for Reducing Maximum Edge Participation", "comment": null, "summary": "We study the problem of constructing cycle bases of graphs with low maximum edge participation, defined as the maximum number of basis cycles that share any single edge. This quantity, though less studied than total weight or length, plays a critical role in quantum fault tolerance because it directly impacts the overhead of lattice surgery procedures used to implement an almost universal quantum gate set. Building on a recursive algorithm of Freedman and Hastings, we introduce a family of load-aware heuristics that adaptively select vertices and edges to minimize edge participation throughout the cycle basis construction. Our approach improves empirical performance on random regular graphs and on graphs derived from small quantum codes. We further analyze a simplified balls-into-bins process to establish lower bounds on edge participation. While the model differs from the cycle basis algorithm on real graphs, it captures what can be proven for our heuristics without using complex graph theoretic properties related to the distribution of cycles in the graph. Our analysis suggests that the maximum load of our heuristics grows on the order of (log n)^2. Our results indicate that careful cycle basis construction can yield significant practical benefits in the design of fault-tolerant quantum systems. This question also carries theoretical interest, as it is essentially identical to the basis number of a graph, defined as the minimum possible maximum edge participation over all cycle bases."}
{"id": "2511.10962", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.10962", "abs": "https://arxiv.org/abs/2511.10962", "authors": ["Xintian Han", "Honggang Chen", "Quan Lin", "Jingyue Gao", "Xiangyuan Ren", "Lifei Zhu", "Zhisheng Ye", "Shikang Wu", "XiongHang Xie", "Xiaochu Gan", "Bingzheng Wei", "Peng Xu", "Zhe Wang", "Yuchao Zheng", "Jingjian Lin", "Di Wu", "Junfeng Ge"], "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation", "comment": null, "summary": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios."}
{"id": "2511.10718", "categories": ["cs.GT", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.10718", "abs": "https://arxiv.org/abs/2511.10718", "authors": ["Daniele Bracale", "Moulinath Banerjee", "Cong Shi", "Yuekai Sun"], "title": "Online Price Competition under Generalized Linear Demands", "comment": null, "summary": "We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $λ_i(\\mathbf{p}) = μ_i(\\langle \\boldsymbolθ_{i,0}, \\mathbf{p} \\rangle)$, with known increasing link $μ_i$ and unknown parameter $\\boldsymbolθ_{i,0}$, where the vector $\\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\\sqrt{T}\\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment."}
{"id": "2511.10777", "categories": ["cs.IT", "cs.DM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.10777", "abs": "https://arxiv.org/abs/2511.10777", "authors": ["Xiaxin Li", "Arya Mazumdar"], "title": "Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time", "comment": null, "summary": "The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\\in \\mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\\text{sign}(Ax)$, where $A\\in \\mathbb{R}^{m\\times n}$ is a sensing matrix and $|\\text{supp}(x)|\\leq k, k \\ll n$. Under this setting, most preexisting works have a recovery runtime $Ω(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1.i): For the universal exact support recovery, a scheme of $m=O(k^2\\log(n/k)\\log n)$ measurements and runtime $D=O(km)$. (1.ii): For the universal $ε$-approximate support recovery, the same scheme with $m=O(kε^{-1}\\log(n/k)\\log n)$ and runtime $D=O(ε^{-1}m)$, improving the runtime significantly with an extra $O(\\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\\frac{\\log k}{\\log\\log k}\\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025."}
{"id": "2511.11106", "categories": ["cs.MM", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11106", "abs": "https://arxiv.org/abs/2511.11106", "authors": ["Zhonghua Jiang", "Kui Chen", "Kunxi Li", "Keting Yin", "Yiyun Zhou", "Zhaode Wang", "Chengfei Lv", "Shengyu Zhang"], "title": "AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization", "comment": null, "summary": "Recent advancements in Audio-Video Large Language Models (AV-LLMs) have enhanced their capabilities in tasks like audio-visual question answering and multimodal dialog systems. Video and audio introduce an extended temporal dimension, resulting in a larger key-value (KV) cache compared to static image embedding. A naive optimization strategy is to selectively focus on and retain KV caches of audio or video based on task. However, in the experiment, we observed that the attention of AV-LLMs to various modalities in the high layers is not strictly dependent on the task. In higher layers, the attention of AV-LLMs shifts more towards the video modality. In addition, we also found that directly integrating temporal KV of audio and spatial-temporal KV of video may lead to information confusion and significant performance degradation of AV-LLMs. If audio and video are processed indiscriminately, it may also lead to excessive compression or reservation of a certain modality, thereby disrupting the alignment between modalities. To address these challenges, we propose AccKV, an Adaptive-Focusing and Cross-Calibration KV cache optimization framework designed specifically for efficient AV-LLMs inference. Our method is based on layer adaptive focusing technology, selectively focusing on key modalities according to the characteristics of different layers, and enhances the recognition of heavy hitter tokens through attention redistribution. In addition, we propose a Cross-Calibration technique that first integrates inefficient KV caches within the audio and video modalities, and then aligns low-priority modalities with high-priority modalities to selectively evict KV cache of low-priority modalities. The experimental results show that AccKV can significantly improve the computational efficiency of AV-LLMs while maintaining accuracy."}
{"id": "2511.11088", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.11088", "abs": "https://arxiv.org/abs/2511.11088", "authors": ["Puyun Hu", "Wei Pan", "Xun Jian", "Zeqi Ma", "Tianjie Li", "Yang Shen", "Chengzhi Han", "Yudong Zhao", "Zhanhuai Li"], "title": "ResBench: A Comprehensive Framework for Evaluating Database Resilience", "comment": null, "summary": "Existing database benchmarks primarily focus on performance under ideal running environments. However, in real-world scenarios, databases probably face numerous adverse events. Quantifying the ability to cope with these events from a comprehensive perspective remains an open problem. We provide the definition of database resilience to describe its performance when facing adversity and propose ResBench, a benchmark for evaluating database resilience. This framework achieves automation, standardization, and visualization of the testing process through clear hierarchical decoupling. ResBench simulates adverse events and injects them during normal transaction processing, utilizing a module to gather multiple metrics for the evaluation model. We assess database resilience across eight dimensions: throughput, latency, stability, resistance, recovery, disturbance period, adaptation capability and metric deviation. All the results are presented to users via a user-friendly graphical interface. We demonstrate the execution process and result interpretation of ResBench using two types of adversity datasets."}
{"id": "2511.11057", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.11057", "abs": "https://arxiv.org/abs/2511.11057", "authors": ["Kotaro Kimura", "Tomohiro I"], "title": "R-enum Revisited: Speedup and Extension for Context-Sensitive Repeats and Net Frequencies", "comment": null, "summary": "Nishimoto and Tabei [CPM, 2021] proposed r-enum, an algorithm to enumerate various characteristic substrings, including maximal repeats, in a string $T$ of length $n$ in $O(r)$ words of compressed working space, where $r \\le n$ is the number of runs in the Burrows-Wheeler transform (BWT) of $T$. Given the run-length encoded BWT (RLBWT) of $T$, r-enum runs in $O(n\\log\\log_{w}(n/r))$ time in addition to the time linear to the number of output strings, where $w=Θ(\\log n)$ is the word size. In this paper, we improve the $O(n\\log\\log_{w}(n/r))$ term to $O(n)$. We also extend r-enum to compute other context-sensitive repeats such as near-supermaximal repeats (NSMRs) and supermaximal repeats, and the context diversity for every maximal repeat in the same complexities. Furthermore, we study the occurrences that witness NSMRs, which have recently attracted attention under the name of net occurrences: An occurrence of a repeat is called a net occurrence if it is not covered by another repeat, and the net frequency of a repeat is the number of its net occurrences. With this terminology, an NSMR is defined to be a repeat with a positive net frequency. Given the RLBWT of $T$, we show how to compute the set $S^{nsmr}$ of all NSMRs in $T$ together with their net frequency/occurrences in $O(n)$ time and $O(r)$ space. We also show that an $O(r)$-space data structure can be built from the RLBWT to support queries of computing the net frequency of any query pattern $P$ in $O(|P|)$ time. The data structure is built in $O(r)$ space and in $O(n)$ time with high probability or deterministic $O(n+|S^{nsmr}|\\log\\log\\min(σ,|S^{nsmr}|))$ time, where $σ\\le r$ is the alphabet size of $T$. To achieve this, we prove that the total number of net occurrences is less than $2r$. We also get a new upper bound $2r$ of the number of minimal unique substrings in $T$, which may be of independent interest."}
{"id": "2511.11010", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.11010", "abs": "https://arxiv.org/abs/2511.11010", "authors": ["Kyle Deeds", "Ying-Hsiang Huang", "Claire Gong", "Shreya Shaji", "Alison Yan", "Leslie Harka", "Samuel J Klein", "Shannon Zejiang Shen", "Mark Phillips", "Trevor Owens", "Benjamin Charles Germain Lee"], "title": "GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs", "comment": "10 pages, 5 figures, 2 tables", "summary": "Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as \"redacted documents\" or \"pie charts.\" We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net."}
{"id": "2511.10845", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10845", "abs": "https://arxiv.org/abs/2511.10845", "authors": ["Natan Doubez", "Pascal Lenzner", "Marcus Wunderlich"], "title": "Optimal Welfare in Noncooperative Network Formation under Attack", "comment": "Accepted at AAAI 2026 -- full version", "summary": "Communication networks are essential for our economy and our everyday lives. This makes them lucrative targets for attacks. Today, we see an ongoing battle between criminals that try to disrupt our key communication networks and security professionals that try to mitigate these attacks. However, today's networks, like the Internet or peer-to-peer networks among smart devices, are not controlled by a single authority, but instead consist of many independently administrated entities that are interconnected. Thus, both the decisions of how to interconnect and how to secure against potential attacks are taken in a decentralized way by selfish agents.\n  This strategic setting, with agents that want to interconnect and potential attackers that want to disrupt the network, was captured via an influential game-theoretic model by Goyal, Jabbari, Kearns, Khanna, and Morgenstern (WINE 2016). We revisit this model and show improved tight bounds on the achieved robustness of networks created by selfish agents. As our main result, we show that such networks can resist attacks of a large class of potential attackers, i.e., these networks maintain asymptotically optimal welfare post attack. This improves several bounds and resolves an open problem. Along the way, we show the counter-intuitive result, that attackers that aim at minimizing the social welfare post attack do not actually inflict the greatest possible damage."}
{"id": "2511.11148", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11148", "abs": "https://arxiv.org/abs/2511.11148", "authors": ["Yanze Zhu", "Qingqing Wu", "Xinrong Guan", "Ziyuan Zheng", "Honghao Wang", "Wen Chen", "Yang Liu", "Yuan Guo"], "title": "Joint Beamforming and Position Optimization for IRS-Aided SWIPT with Movable Antennas", "comment": "13 pages, 7 figures, submitted to IEEE journal for possible publication", "summary": "Simultaneous wireless information and power transfer (SWIPT) has been envisioned as a promising technology to support ubiquitous connectivity and reliable sustainability in Internet-of-Things (IoT) networks, which, however, generally suffers from severe attenuation caused by long distance propagation, leading to inefficient wireless power transfer (WPT) for energy harvesting receivers (EHRs). This paper proposes to introduce emerging intelligent reflecting surface (IRS) and movable antenna (MA) technologies into SWIPT systems aiming at enhancing information transmission for information decoding receivers (IDRs) and improving receive power of EHRs. We consider to maximize the weighted sum-rate of IDRs via jointly optimizing the active and passive beamforming at the base station (BS) and IRS, respectively, as well as the positions of MAs, while guaranteeing the requirements of all EHRs. To tackle this challenging task due to the non-convexity of associated optimization, we develop an efficient algorithm combining weighted minimal mean square error (WMMSE), block coordinate descent (BCD), majorization-minimization (MM), and penalty duality decomposition (PDD) frameworks. Besides, we present a feasibility characterization method to examine the achievability of EHRs' requirements. Simulation results demonstrate the significant benefits of our proposed solutions. Particularly, the optimized IRS configuration may exhibit higher performance gain than MA counterpart under our considered scenario."}
{"id": "2511.11399", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11399", "abs": "https://arxiv.org/abs/2511.11399", "authors": ["Rosario Napoli", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database", "comment": "Accepted at the 30th IEEE Symposium on Computers and Communications (ISCC) 2025", "summary": "Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.\n  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.\n  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis."}
{"id": "2511.11237", "categories": ["cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.11237", "abs": "https://arxiv.org/abs/2511.11237", "authors": ["Daniel Blankenburg", "Antonia Ellerbrock", "Thomas Kesselheim", "Jens Vygen"], "title": "An Efficient Algorithm for Minimizing Ordered Norms in Fractional Load Balancing", "comment": "40 pages", "summary": "We study the problem of minimizing an ordered norm of a load vector (indexed by a set of $d$ resources), where a finite number $n$ of customers $c$ contribute to the load of each resource by choosing a solution $x_c$ in a convex set $X_c \\subseteq \\mathbb{R}^d_{\\geq 0}$; so we minimize $||\\sum_{c}x_c||$ for some fixed ordered norm $||\\cdot||$. We devise a randomized algorithm that computes a $(1+\\varepsilon)$-approximate solution to this problem and makes, with high probability, $\\mathcal{O}((n+d) (\\varepsilon^{-2}+\\log\\log d)\\log (n+d))$ calls to oracles that minimize linear functions (with non-negative coefficients) over $X_c$. While this has been known for the $\\ell_{\\infty}$ norm via the multiplicative weights update method, existing proof techniques do not extend to arbitrary ordered norms. Our algorithm uses a resource price mechanism that is motivated by the follow-the-regularized-leader paradigm, and is expressed by smooth approximations of ordered norms. We need and show that these have non-trivial stability properties, which may be of independent interest. For each customer, we define dynamic cost budgets, which evolve throughout the algorithm, to determine the allowed step sizes. This leads to non-uniform updates and may even reject certain oracle solutions. Using non-uniform sampling together with a martingale argument, we can guarantee sufficient expected progress in each iteration, and thus bound the total number of oracle calls with high probability."}
{"id": "2511.11172", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11172", "abs": "https://arxiv.org/abs/2511.11172", "authors": ["Mubaraka Sani Ibrahim", "Isah Charles Saidu", "Lehel Csato"], "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition", "comment": "((1) African University of Science and Technology (Abuja, Nigeria), (2) Baze University (Abuja, Nigeria), (3) Babes-Bolyai University (Cluj-Napoca, Romania))", "summary": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data."}
{"id": "2511.10854", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.10854", "abs": "https://arxiv.org/abs/2511.10854", "authors": ["Vade Shah", "Jason R. Marden"], "title": "Playing with Peaks: A Game-Theoretic Comparison of Electricity Pricing Mechanisms", "comment": null, "summary": "As electricity consumption grows, reducing peak demand--the maximum load on the grid--has become critical for preventing infrastructure strain and blackouts. Pricing mechanisms that incentivize consumers with flexible loads to shift consumption away from high-demand periods have emerged as effective tools, yet different mechanisms are used in practice with unclear relative performance. This work compares two widely implemented approaches: anytime peak pricing (AP), where consumers pay for their individual maximum consumption, and coincident peak pricing (CP), where consumers pay for their consumption during the system-wide peak period. To compare these mechanisms, we model the electricity market as a strategic game and characterize the peak demand in equilibrium under both AP and CP. Our main result demonstrates that with perfect information, equilibrium peak demand under CP never exceeds that under AP; on the other hand, with imperfect information, the coordination introduced by CP can backfire and induce larger equilibrium peaks than AP. These findings demonstrate that potential gains from coupling users' costs (as done in CP) must be weighed against these miscoordination risks. We conclude with preliminary results indicating that progressive demand cost structures--rather than per-unit charges--may mitigate these risks while preserving coordination benefits, achieving desirable performance in both deterministic and stochastic settings."}
{"id": "2511.11225", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11225", "abs": "https://arxiv.org/abs/2511.11225", "authors": ["Zhaolin Wang", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Yuanwei Liu"], "title": "Mutual Coupling in Continuous Aperture Arrays: Physical Modeling and Beamforming Design", "comment": "13 pages, 11 figures", "summary": "The phenomenon of mutual coupling in continuous aperture arrays (CAPAs) is studied. First, a general physical model for the phenomenon that accounts for both polarization and surface dissipation losses is developed. Then, the unipolarized coupling kernel is characterized, revealing that polarization induces anisotropic coupling and invalidates the conventional half-wavelength spacing rule for coupling elimination. Next, the beamforming design problem for CAPAs with coupling is formulated as a functional optimization problem, leading to the derivation of optimal beamforming structures via the calculus of variations. To address the challenge of inverting the coupling kernel in the optimal structure, two methods are proposed: 1) the kernel approximation method, which yields a closed-form solution via wavenumber-domain transformation and GaussLegendre quadrature, and 2) the conjugate gradient method, which addresses an equivalent quadratic functional optimization problem iteratively. Furthermore, the optimal array gain and beampattern are analyzed at the large-aperture limit. Finally, the proposed continuous mutual coupling model is extended to spatially discrete arrays (SPDAs), and comprehensive numerical results are provided, demonstrating that: 1) coupled SPDA performance correctly converges to the CAPA limit, while uncoupled models are shown to violate physics, 2) polarization results in anisotropic array gain behavior, and 3) the coupled beampattern exhibits higher directivity than the uncoupled beampattern."}
{"id": "2511.11319", "categories": ["cs.DS", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11319", "abs": "https://arxiv.org/abs/2511.11319", "authors": ["Quentin Hillebrand", "Pasin Manurangsi", "Vorapong Suppakitpaisarn", "Phanu Vajanopath"], "title": "Improved Differentially Private Algorithms for Rank Aggregation", "comment": "To appear in AAAI 2026", "summary": "Rank aggregation is a task of combining the rankings of items from multiple users into a single ranking that best represents the users' rankings. Alabi et al. (AAAI'22) presents differentially-private (DP) polynomial-time approximation schemes (PTASes) and $5$-approximation algorithms with certain additive errors for the Kemeny rank aggregation problem in both central and local models. In this paper, we present improved DP PTASes with smaller additive error in the central model. Furthermore, we are first to study the footrule rank aggregation problem under DP. We give a near-optimal algorithm for this problem; as a corollary, this leads to 2-approximation algorithms with the same additive error as the $5$-approximation algorithms of Alabi et al. for the Kemeny rank aggregation problem in both central and local models."}
{"id": "2511.11255", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11255", "abs": "https://arxiv.org/abs/2511.11255", "authors": ["Wencai Ye", "Mingjie Sun", "Shuhang Chen", "Wenjin Wu", "Peng Jiang"], "title": "Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform."}
{"id": "2511.11023", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11023", "abs": "https://arxiv.org/abs/2511.11023", "authors": ["Yaoxin Ge", "Yao Zhang", "Dengji Zhao"], "title": "Fair Incentives for Early Arrival in 0-1 Cooperative Games", "comment": "Accepted by AAAI2026", "summary": "Incentives for early arrival (I4EA) was recently proposed for studying online cooperative games. In an online cooperative game, players arrive in an unknown order, and the value increase after each player arrived should be distributed immediately among all the arrived players. Although there is only one arriving order in the game, we also hope that the value distribution is equal to their Shapley value in expectation. To achieve these goals, the early solutions ignored the fairness in each single arriving order. More specifically, an important player may receive nothing in a game, which seems unfair in reality. To combat this, we propose refined fairness in this paper and design new solutions in 0-1 value games. Specifically, we compute the distance of the distribution in each order to the Shapley value and aim to minimize it. We propose a new mechanism called Egalitarian Value-Sharing (EVS) to do so. We also show that the mechanism can maximize the egalitarian welfare among all the players who made contributions."}
{"id": "2511.11256", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11256", "abs": "https://arxiv.org/abs/2511.11256", "authors": ["Jingyu Lin", "Li Chen", "Xiaoqian Ye"], "title": "SCL Decoding of Non-Binary Linear Block Codes", "comment": null, "summary": "Non-binary linear block codes (NB-LBCs) are an important class of error-correcting codes that are especially competent in correcting burst errors. They have broad applications in modern communications and storage systems. However, efficient soft-decision decoding of these codes remains challenging. This paper proposes successive cancellation list (SCL) decoding for NB-LBCs that are defined over a finite field of characteristic two, i.e., F_{2^r}, where r is the extension degree. By establishing a one-to-r mapping between the binary composition of each non-binary codeword and r binary polar codewords, SCL decoding of the r polar codes can be performed with a complexity that is sub-quadratic in the codeword length. An r-step decoding path sorting strategy is further proposed to facilitate the decoding. Simulation results on extended Reed-Solomon (eRS) and non-binary extended BCH (NB-eBCH) codes show that SCL decoding can outperform their state-of-the-art soft-decision decoding with fewer finite field arithmetic operations. For length-16 eRS codes, their maximum-likelihood (ML) decoding performances can be approached with a moderate list size."}
{"id": "2511.11498", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11498", "abs": "https://arxiv.org/abs/2511.11498", "authors": ["Renato Ferreira Pinto", "Cassandra Marcussen", "Elchanan Mossel", "Shivam Nadimpalli"], "title": "Learning and Testing Convex Functions", "comment": "43 pages", "summary": "We consider the problems of \\emph{learning} and \\emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.\n  In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:\n  - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\\varepsilon$ using $n^{O(1/\\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\\mathrm{poly}(1/\\varepsilon)}$ samples in the \\emph{correlational statistical query (CSQ)} model.\n  - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\\sqrt{n}/\\varepsilon)^n$ samples."}
{"id": "2511.11305", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11305", "abs": "https://arxiv.org/abs/2511.11305", "authors": ["Chenghan Fu", "Daoze Zhang", "Yukang Lin", "Zhanheng Nie", "Xiang Zhang", "Jianyu Liu", "Yueran Liu", "Wanxian Guan", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising", "comment": "31 pages, 12 figures", "summary": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences."}
{"id": "2511.11282", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11282", "abs": "https://arxiv.org/abs/2511.11282", "authors": ["Erwan Christian Escudie", "Matthia Sabatelli", "Olivier Buffet", "Jilles Steeve Dibangoye"], "title": "ε-Optimally Solving Two-Player Zero-Sum POSGs", "comment": null, "summary": "We present a novel framework for ε-optimally solving two-player zero-sum partially observable stochastic games (zs-POSGs). These games pose a major challenge due to the absence of a principled connection with dynamic programming (DP) techniques developed for two-player zero-sum stochastic games (zs-SGs). Prior attempts at transferring solution methods have lacked a lossless reduction, defined here as a transformation that preserves value functions, equilibrium strategies, and optimality structure, thereby limiting generalisation to ad-hoc algorithms. This work introduces the first lossless reduction from zs-POSGs to transition-independent zs-SGs, enabling the principled application of a broad class of DP-based methods. We show empirically that point-based value iteration (PBVI) algorithms, applied via this reduction, produce ε-optimal strategies across a range of benchmark domains, consistently matching or outperforming existing state-of-the-art methods. Our results open a systematic pathway for algorithmic and theoretical transfer from SGs to partially observable settings."}
{"id": "2511.11495", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11495", "abs": "https://arxiv.org/abs/2511.11495", "authors": ["Zhengwei Jiang", "Yufeng Zhou", "Xusheng Zhu", "Wen Chen", "Qingqing Wu", "Kai-Kit Wong"], "title": "Joint Optimization for Multi-User Transmissive RIS-MIMO Systems", "comment": null, "summary": "Transmissive reconfigurable intelligent surfaces (RIS) represent a transformative architecture for future wireless networks, enabling a paradigm shift from traditional costly base stations to low-cost, energy-efficient transmitters. This paper explores a downlink multi-user MIMO system where a transmissive RIS, illuminated by a single feed antenna, forms the core of the transmitter. The joint optimization of the RIS coefficient vector, power allocation, and receive beamforming in such a system is critical for performance but poses significant challenges due to the non-convex objective, coupled variables, and constant modulus constraints. To address these challenges, we propose a novel optimization framework. Our approach involves reformulating the sum-rate maximization problem into a tractable equivalent form and developing an efficient alternating optimization (AO) algorithm. This algorithm decomposes the problem into subproblems for the RIS coefficients, receive beamformers, and power allocation, each solved using advanced techniques including convex approximation and difference-of-convex programming. Simulation results demonstrate that our proposed method converges rapidly and achieves substantial sum-rate gains over conventional schemes, validating the effectiveness of our approach and highlighting the potential of transmissive RIS as a key technology for next-generation wireless systems."}
{"id": "2511.11499", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.11499", "abs": "https://arxiv.org/abs/2511.11499", "authors": ["Prashanti Anderson", "Samuel B. Hopkins", "Amit Rajaraman", "David Steurer"], "title": "Faster MAX-CUT on Bounded Threshold Rank Graphs", "comment": "20 pages", "summary": "We design new algorithms for approximating 2CSPs on graphs with bounded threshold rank, that is, whose normalized adjacency matrix has few eigenvalues larger than $\\varepsilon$, smaller than $-\\varepsilon$, or both. Unlike on worst-case graphs, 2CSPs on bounded threshold rank graphs can be $(1+O(\\varepsilon))$-approximated efficiently. Prior approximation algorithms for this problem run in time exponential in the threshold rank and $1/\\varepsilon$. Our algorithm has running time which is polynomial in $1/\\varepsilon$ and exponential in the threshold rank of the label-extended graph, and near-linear in the input size. As a consequence, we obtain the first $(1+O(\\varepsilon))$ approximation for MAX-CUT on bounded threshold rank graphs running in $\\mathrm{poly}(1/\\varepsilon)$ time. We also improve the state-of-the-art running time for 2CSPs on bounded threshold-rank graphs from polynomial in input size to near-linear via a new comparison inequality between the threshold rank of the label-extended graph and base graph. Our algorithm is a simple yet novel combination of subspace enumeration and semidefinite programming."}
{"id": "2511.11370", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11370", "abs": "https://arxiv.org/abs/2511.11370", "authors": ["Jiahao Wang", "Bokang Fu", "Yu Zhu", "Yuli Liu"], "title": "SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation", "comment": null, "summary": "LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.\n  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop \"assess-validate-reflect\" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks."}
{"id": "2511.11365", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11365", "abs": "https://arxiv.org/abs/2511.11365", "authors": ["Piotr Faliszewski", "Stanislaw Kazmierowski", "Grzegorz Lisowski", "Ildiko Schlotter", "Paolo Turrini"], "title": "Computing Equilibrium Nominations in Presidential Elections", "comment": null, "summary": "We study strategic candidate nomination by parties in elections decided by Plurality voting. Each party selects a nominee before the election, and the winner is chosen from the nominated candidates based on the voters' preferences. We introduce a new restriction on these preferences, which we call party-aligned single-peakedness: all voters agree on a common ordering of the parties along an ideological axis, but may differ in their perceptions of the positions of individual candidates within each party. The preferences of each voter are single-peaked with respect to their own axis over the candidates, which is consistent with the global ordering of the parties. We present a polynomial-time algorithm for recognizing whether a preference profile satisfies party-aligned single-peakedness. In this domain, we give polynomial-time algorithms for deciding whether a given party can become the winner under some (or all) nominations, and whether this can occur in some pure Nash equilibrium. We also prove a tight result about the guaranteed existence of pure strategy Nash equilibria for elections with up to three parties for single-peaked and party-aligned single-peaked preference profiles."}
{"id": "2511.10777", "categories": ["cs.IT", "cs.DM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.10777", "abs": "https://arxiv.org/abs/2511.10777", "authors": ["Xiaxin Li", "Arya Mazumdar"], "title": "Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time", "comment": null, "summary": "The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\\in \\mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\\text{sign}(Ax)$, where $A\\in \\mathbb{R}^{m\\times n}$ is a sensing matrix and $|\\text{supp}(x)|\\leq k, k \\ll n$. Under this setting, most preexisting works have a recovery runtime $Ω(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1.i): For the universal exact support recovery, a scheme of $m=O(k^2\\log(n/k)\\log n)$ measurements and runtime $D=O(km)$. (1.ii): For the universal $ε$-approximate support recovery, the same scheme with $m=O(kε^{-1}\\log(n/k)\\log n)$ and runtime $D=O(ε^{-1}m)$, improving the runtime significantly with an extra $O(\\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\\frac{\\log k}{\\log\\log k}\\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025."}
{"id": "2511.11399", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11399", "abs": "https://arxiv.org/abs/2511.11399", "authors": ["Rosario Napoli", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database", "comment": "Accepted at the 30th IEEE Symposium on Computers and Communications (ISCC) 2025", "summary": "Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.\n  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.\n  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis."}
{"id": "2511.11371", "categories": ["cs.GT", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.11371", "abs": "https://arxiv.org/abs/2511.11371", "authors": ["Daniel Ebert", "Antonia Ellerbrock"], "title": "Nucleolus, Happy Nucleolus, and Vehicle Routing", "comment": "14 pages, 8 figures", "summary": "We study the recently introduced fair division concept of the happy nucleolus for cost allocation among players in a cooperative game, with special focus on its computation. The happy nucleolus applies the same fairness criterion as the well-established nucleolus but with reduced total value. Still, we show that the relation between the two concepts is quite involved, and intuitive properties do not hold - e.g., the entry of a player in the happy nucleolus can be larger than the entry of the same player in the nucleolus, even for monotone and subadditive games. This refutes conjectures of Meir, Rosenschein and Malizia (2011).\n  Further, we study the separation problem of the linear programs appearing in the MPS scheme for computing the (happy) nucleolus. It includes linear subspace avoidance constraints, which can be handled efficiently for problems with a certain dynamic programming formulation due to Köhnemann and Toth (2020). We show how to get rid of these constraints for all monotone games if we allow for an arbitrarily small error of epsilon, thus conserving known approximation guarantees for the same problem without subspace avoidance.\n  Finally, we focus on practical results at the example of vehicle routing games by designing an efficient heuristic based on our previous insights and past work, and demonstrate its power."}
{"id": "2511.11475", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.11475", "abs": "https://arxiv.org/abs/2511.11475", "authors": ["Argyrios Deligkas", "Gregory Gutin", "Mark Jones", "Philip R. Neary", "Anders Yeo"], "title": "Public Goods Games in Directed Networks with Constraints on Sharing", "comment": null, "summary": "In a public goods game, every player chooses whether or not to buy a good that all neighboring players will have access to. We consider a setting in which the good is indivisible, neighboring players are out-neighbors in a directed graph, and there is a capacity constraint on their number, k, that can benefit from the good. This means that each player makes a two-pronged decision: decide whether or not to buy and, conditional on buying, choose which k out-neighbors to share access. We examine both pure and mixed Nash equilibria in the model from the perspective of existence, computation, and efficiency. We perform a comprehensive study for these three dimensions with respect to both sharing capacity (k) and the network structure (the underlying directed graph), and establish sharp complexity dichotomies for each."}
{"id": "2511.11475", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.11475", "abs": "https://arxiv.org/abs/2511.11475", "authors": ["Argyrios Deligkas", "Gregory Gutin", "Mark Jones", "Philip R. Neary", "Anders Yeo"], "title": "Public Goods Games in Directed Networks with Constraints on Sharing", "comment": null, "summary": "In a public goods game, every player chooses whether or not to buy a good that all neighboring players will have access to. We consider a setting in which the good is indivisible, neighboring players are out-neighbors in a directed graph, and there is a capacity constraint on their number, k, that can benefit from the good. This means that each player makes a two-pronged decision: decide whether or not to buy and, conditional on buying, choose which k out-neighbors to share access. We examine both pure and mixed Nash equilibria in the model from the perspective of existence, computation, and efficiency. We perform a comprehensive study for these three dimensions with respect to both sharing capacity (k) and the network structure (the underlying directed graph), and establish sharp complexity dichotomies for each."}
{"id": "2511.11531", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.11531", "abs": "https://arxiv.org/abs/2511.11531", "authors": ["Valentin Zech", "Martin Bullinger"], "title": "Deviation Dynamics in Cardinal Hedonic Games", "comment": "Appears in the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Computing stable partitions in hedonic games is a challenging task because there exist games in which stable outcomes do not exist. Even more, these No-instances can often be leveraged to prove computational hardness results. We make this impression rigorous in a dynamic model of cardinal hedonic games by providing meta theorems. These imply hardness of deciding about the possible or necessary convergence of deviation dynamics based on the mere existence of No-instances. Our results hold for additively separable, fractional, and modified fractional hedonic games (ASHGs, FHGs, and MFHGs). Moreover, they encompass essentially all reasonable stability notions based on single-agent deviations. In addition, we propose dynamics as a method to find individually rational and contractually individual stable (CIS) partitions in ASHGs. In particular, we find that CIS dynamics from the singleton partition possibly converge after a linear number of deviations but may require an exponential number of deviations in the worst case."}
{"id": "2511.11531", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.11531", "abs": "https://arxiv.org/abs/2511.11531", "authors": ["Valentin Zech", "Martin Bullinger"], "title": "Deviation Dynamics in Cardinal Hedonic Games", "comment": "Appears in the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Computing stable partitions in hedonic games is a challenging task because there exist games in which stable outcomes do not exist. Even more, these No-instances can often be leveraged to prove computational hardness results. We make this impression rigorous in a dynamic model of cardinal hedonic games by providing meta theorems. These imply hardness of deciding about the possible or necessary convergence of deviation dynamics based on the mere existence of No-instances. Our results hold for additively separable, fractional, and modified fractional hedonic games (ASHGs, FHGs, and MFHGs). Moreover, they encompass essentially all reasonable stability notions based on single-agent deviations. In addition, we propose dynamics as a method to find individually rational and contractually individual stable (CIS) partitions in ASHGs. In particular, we find that CIS dynamics from the singleton partition possibly converge after a linear number of deviations but may require an exponential number of deviations in the worst case."}
{"id": "2511.11545", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11545", "abs": "https://arxiv.org/abs/2511.11545", "authors": ["Irmak Sağlam", "Mahdi Nazeri", "Alessandro Abate", "Sadegh Soudjani", "Anne-Kathrin Schmuck"], "title": "Incremental Data-Driven Policy Synthesis via Game Abstractions", "comment": "To be presented at the 40th Annual AAAI Conference on Artificial Intelligence AAAI'26 (Oral)", "summary": "We address the synthesis of control policies for unknown discrete-time stochastic dynamical systems to satisfy temporal logic objectives. We present a data-driven, abstraction-based control framework that integrates online learning with novel incremental game-solving. Under appropriate continuity assumptions, our method abstracts the system dynamics into a finite stochastic (2.5-player) game graph derived from data. Given a requirement over time on this graph, we compute the winning region -- i.e., the set of initial states from which the objective is satisfiable -- in the resulting game, together with a corresponding control policy.\n  Our main contribution is the construction of abstractions, winning regions and control policies incrementally, as data about the system dynamics accumulates. Concretely, our algorithm refines under- and over-approximations of reachable sets for each state-action pair as new data samples arrive. These refinements induce structural modifications in the game graph abstraction -- such as the addition or removal of nodes and edges -- which in turn modify the winning region. Crucially, we show that these updates are inherently monotonic: under-approximations can only grow, over-approximations can only shrink, and the winning region can only expand.\n  We exploit this monotonicity by defining an objective-induced ranking function on the nodes of the abstract game that increases monotonically as new data samples are incorporated. These ranks underpin our novel incremental game-solving algorithm, which employs customized gadgets (DAG-like subgames) within a rank-lifting algorithm to efficiently update the winning region. Numerical case studies demonstrate significant computational savings compared to the baseline approach, which resolves the entire game from scratch whenever new data samples arrive."}
