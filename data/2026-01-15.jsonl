{"id": "2601.08927", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.08927", "abs": "https://arxiv.org/abs/2601.08927", "authors": ["Pavan Kumar", "Shayan Srinivasa Garani"], "title": "Two-dimensional Entanglement-assisted Quantum Quasi-cyclic Low-density Parity-check Codes", "comment": "9 pages, 4 figures", "summary": "For any positive integer $g \\ge 2$, we derive general conditions for the existence of a $2g$-cycle in the Tanner graph of two-dimensional ($2$-D) classical quasi-cyclic (QC) low-density parity-check (LDPC) codes. Based on these conditions, we construct a family of $2$-D classical QC-LDPC codes with girth greater than $4$ by stacking $p \\times p \\times p$ tensors, where $p$ is an odd prime. Furthermore, for composite values of $p$, we propose two additional families of $2$-D classical LDPC codes obtained via similar tensor stacking. In this case, one family achieves girth greater than $4$, while the other attains girth greater than $6$. All the proposed $2$-D classical QC-LDPC codes exhibit an erasure correction capability of at least $p \\times p$. Based on the constructed classical $2$-D QC-LDPC codes, we derive two families of $2$-D entanglement-assisted (EA) quantum low-density parity-check (QLDPC) codes. The first family of $2$-D EA-QLDPC codes is obtained from a pair of binary $2$-D classical LDPC codes and is designed such that the unassisted part of the Tanner graph of the resulting EA-QLDPC code is free of cycles of length four, while requiring only a single ebit to be shared across the quantum transceiver. The second family is constructed from a single $2$-D classical LDPC code whose Tanner graph is free from $4$-cycles. Moreover, the constructed EA-QLDPC codes inherit an erasure correction capability of $p \\times p$, as the underlying classical codes possess the same erasure correction property."}
{"id": "2601.08929", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.08929", "abs": "https://arxiv.org/abs/2601.08929", "authors": ["Zachary Roberston"], "title": "A Local Characterization of $f$-Divergences Yielding PSD Mutual-Information Matrices", "comment": null, "summary": "We study when the variable-indexed matrix of pairwise \\(f\\)-mutual informations \\(M^{(f)}_{ij}=I_f(X_i;X_j)\\) is positive semidefinite (PSD). Let \\(f:(0,\\infty)\\to\\mathbb{R}\\) be convex with \\(f(1)=0\\), finite in a neighborhood of \\(1\\), and with \\(f(0)<\\infty\\) so that diagonal terms are finite. We give a sharp \\emph{local} characterization around independence: there exists \\(δ=δ(f)>0\\) such that for every \\(n\\) and every finite-alphabet family \\((X_1,\\ldots,X_n)\\) whose pairwise joint-to-product ratios lie in \\((1-δ,1+δ)\\), the matrix \\(M^{(f)}\\) is PSD if and only if \\(f\\) is analytic at \\(1\\) with a convergent expansion \\(f(t)=\\sum_{m=2}^{\\infty} a_m (t-1)^m\\) and \\(a_m\\ge 0\\) on a neighborhood of \\(1\\). Consequently, any negative Taylor coefficient yields an explicit finite-alphabet counterexample under arbitrarily weak dependence, and non-analytic convex divergences (e.g.\\ total variation) are excluded. This PSD requirement is distinct from Hilbertian/metric properties of divergences between distributions (e.g.\\ \\(\\sqrt{\\mathrm{JS}}\\)): we study PSD of the \\emph{variable-indexed} mutual-information matrix. The proof combines a replica embedding that turns monomial terms into Gram matrices with a replica-forcing reduction to positive-definite dot-product kernels, enabling an application of the Schoenberg--Berg--Christensen--Ressel classification."}
{"id": "2601.08986", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.08986", "abs": "https://arxiv.org/abs/2601.08986", "authors": ["Sara Saeidian"], "title": "On the Information Leakage Envelope of the Gaussian Mechanism", "comment": null, "summary": "We study the pointwise maximal leakage (PML) envelope of the Gaussian mechanism, which characterizes the smallest information leakage bound that holds with high probability under arbitrary post-processing. For the Gaussian mechanism with a Gaussian secret, we derive a closed-form expression for the deterministic PML envelope for sufficiently small failure probabilities. We then extend this result to general unbounded secrets by identifying a sufficient condition under which the envelope coincides with the Gaussian case. In particular, we show that strongly log-concave priors satisfy this condition via an application of the Brascamp-Lieb inequality."}
{"id": "2601.09039", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09039", "abs": "https://arxiv.org/abs/2601.09039", "authors": ["Mete Erdogan", "Abhiram Gorle", "Shubham Chandak", "Mert Pilanci", "Tsachy Weissman"], "title": "An Information-Theoretic Perspective on LLM Tokenizers", "comment": null, "summary": "Large language model (LLM) tokenizers act as structured compressors: by mapping text to discrete token sequences, they determine token count (and thus compute and context usage) and the statistical structure seen by downstream models. Despite their central role in LLM pipelines, the link between tokenization, compression efficiency and induced structure is not well understood. We empirically demonstrate that tokenizer training scale redistributes entropy: as training data grows, the token stream becomes more diverse in aggregate (higher unigram entropy) yet markedly more predictable in-context (lower higher-order conditional entropies), indicating that tokenization absorbs substantial short-range regularity although these gains degrade under train-test domain mismatch. To ground these observations, we first benchmark i) pretrained GPT-family tokenizers as black-box compressors across various domains, and ii) learned tokenizers across configurations spanning vocabulary size, training scale, and domain. Next, we study tokenization as a transform for universal compression and introduce a compression-aware BPE variant. Finally, we adopt a channel lens and introduce capacity-utilization metrics to analyze tokenizer behaviour and outline implications for downstream modeling. Put together, our results expose various trade-offs between compression, induced structure, and robustness under domain shift, and motivate principled, compression-aware tokenizer design."}
{"id": "2601.09299", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.09299", "abs": "https://arxiv.org/abs/2601.09299", "authors": ["Ziheng Chen", "Bo Li", "Zihan Luo", "Jialin Zhang"], "title": "On the Fair Allocation to Asymmetric Agents with Binary XOS Valuations", "comment": "AAMAS 2026", "summary": "We study the problem of allocating $m$ indivisible goods among $n$ agents, where each agent's valuation is fractionally subadditive (XOS). With respect to AnyPrice Share (APS) fairness, Kulkarni et al. (2024) showed that, when agents have binary marginal values, a $0.1222$-APS allocation can be found in polynomial time, and there exists an instance where no allocation is better than $0.5$-approximate APS. Very recently, Feige and Grinberg (2025) extended the problem to the asymmetric case, where agents may have different entitlements, and improved the approximation ratio to $1/6$ for general XOS valuations. In this work, we focus on the asymmetric setting with binary XOS valuations, and further improve the approximation ratio to $1/2$, which matches the known upper bound. We also present a polynomial-time algorithm to compute such an allocation. Beyond APS fairness, we also study the weighted maximin share (WMMS) fairness. Farhadi et al. (2019) showed that, a $1/n$-WMMS allocation always exists for agents with general additive valuations, and that this approximation ratio is tight. We extend this result to general XOS valuations, where a $1/n$-WMMS allocation still exists, and this approximation ratio cannot be improved even when marginal values are binary. This shows a sharp contrast to binary additive valuations, where an exact WMMS allocation exists and can be found in polynomial time."}
{"id": "2601.09216", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.09216", "abs": "https://arxiv.org/abs/2601.09216", "authors": ["Xinyuan Zhang", "Zijian Wang", "Chang Dao", "Juexiao Zhou"], "title": "Honesty-Aware Multi-Agent Framework for High-Fidelity Synthetic Data Generation in Digital Psychiatric Intake Doctor-Patient Interactions", "comment": null, "summary": "Data scarcity and unreliable self-reporting -- such as concealment or exaggeration -- pose fundamental challenges to psychiatric intake and assessment. We propose a multi-agent synthesis framework that explicitly models patient deception to generate high-fidelity, publicly releasable synthetic psychiatric intake records. Starting from DAIC-WOZ interviews, we construct enriched patient profiles and simulate a four-role workflow: a \\emph{Patient} completes self-rated scales and participates in a semi-structured interview under a topic-dependent honesty state; an \\emph{Assessor} selects instruments based on demographics and chief complaints; an \\emph{Evaluator} conducts the interview grounded in rater-administered scales, tracks suspicion, and completes ratings; and a \\emph{Diagnostician} integrates all evidence into a diagnostic summary. Each case links the patient profile, self-rated and rater-administered responses, interview transcript, diagnostic summary, and honesty state. We validate the framework through four complementary evaluations: diagnostic consistency and severity grading, chain-of-thought ablations, human evaluation of clinical realism and dishonesty modeling, and LLM-based comparative evaluation. The resulting corpus spans multiple disorders and severity levels, enabling controlled study of dishonesty-aware psychiatric assessment and the training and evaluation of adaptive dialogue agents."}
{"id": "2601.08989", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.08989", "abs": "https://arxiv.org/abs/2601.08989", "authors": ["Matteo Caporrella", "Stefano Leucci"], "title": "An Almost-Optimal Upper Bound on the Push Number of the Torus Puzzle", "comment": "22 pages, 8 figures", "summary": "We study the Torus Puzzle, a solitaire game in which the elements of an input $m \\times n$ matrix need to be rearranged into a target configuration via a sequence of unit rotations (i.e., circular shifts) of rows and/or columns. Amano et al.\\ proposed a more permissive variant of the above puzzle, where each row and column rotation can shift the involved elements by any amount of positions. The number of rotations needed to solve the puzzle in the original and in the permissive variants of the puzzle are respectively known as the \\emph{push number} and the \\emph{drag number}, where the latter is always smaller than or equal to the former and admits an existential lower bound of $Ω(mn)$. While this lower bound is matched by an $O(mn)$ upper bound, the push number is not so well understood. Indeed, to the best of our knowledge, only an $O(mn \\cdot \\max\\{ m, n \\})$ upper bound is currently known. In this paper, we provide an algorithm that solves the Torus Puzzle using $O(mn \\cdot \\log \\max \\{m, n\\})$ unit rotations in a model that is more restricted than that of the original puzzle. This implies a corresponding upper bound on the push number and reduces the gap between the known upper and lower bounds from $Θ(\\max\\{m,n\\})$ to $Θ(\\log \\max\\{m, n\\})$."}
{"id": "2601.08901", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08901", "abs": "https://arxiv.org/abs/2601.08901", "authors": ["Yuexi Shen", "Minqian Liu", "Dawei Zhou", "Lifu Huang"], "title": "Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas", "comment": "21 pages, 6 tables", "summary": "Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery."}
{"id": "2601.09057", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09057", "abs": "https://arxiv.org/abs/2601.09057", "authors": ["Xiaoli Xu", "Yong Zeng"], "title": "Hybrid Mono- and Bi-static OFDM-ISAC via BS-UE Cooperation: Closed-Form CRLB and Coverage Analysis", "comment": "BS-UE cooperative ISAC, OFDM ISAC, hybrid mono- and bi-static sensing, CRLB", "summary": "This paper proposes a hybrid mono- and bi-static sensing framework, by leveraging the base station (BS) and user equipment (UE) cooperation in integrated sensing and communication (ISAC) systems. This scheme is built on 3GPP-supported sensing modes, and it does not incur any extra spectrum cost or inter-cell coordination. To reveal the fundamental performance limit of the proposed hybrid sensing mode, we derive closed-form Cramér-Rao lower bound (CRLB) for sensing target localization and velocity estimation, as functions of target and UE positions. The results reveal that significant performance gains can be achieved over the purely mono- or bi-static sensing, especially when the BS-target-UE form a favorable geometry, which is close to a right triangle. The analytical results are validated by simulations using effective parameter estimation algorithm and weighted mean square error (MSE) fusion method. Based on the derived sensing bound, we further analyze the sensing coverage by varying the UE positions, which shows that sensing coverage first improves then degrades as the BS-UE separation increases. Furthermore, the sensing accuracy for a potential target with best UE selection is derived as a function of the UE density in the network."}
{"id": "2601.09354", "categories": ["cs.GT", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.09354", "abs": "https://arxiv.org/abs/2601.09354", "authors": ["Jonathan Carrero", "Ismael Rodriguez", "Fernando Rubio"], "title": "Measuring the benefits of lying in MARA under egalitarian social welfare", "comment": "This paper was published in 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC). The present version is the author's accepted manuscript", "summary": "When some resources are to be distributed among a set of agents following egalitarian social welfare, the goal is to maximize the utility of the agent whose utility turns out to be minimal. In this context, agents can have an incentive to lie about their actual preferences, so that more valuable resources are assigned to them. In this paper we analyze this situation, and we present a practical study where genetic algorithms are used to assess the benefits of lying under different situations."}
{"id": "2601.09404", "categories": ["cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09404", "abs": "https://arxiv.org/abs/2601.09404", "authors": ["Jun-Peng Zhu", "Boyan Niu", "Peng Cai", "Zheming Ni", "Kai Xu", "Jiajun Huang", "Shengbo Ma", "Bing Wang", "Xuan Zhou", "Guanglei Bao", "Donghui Zhang", "Liu Tang", "Qi Liu"], "title": "TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models", "comment": "4 pages, 5 figures", "summary": "The SQL-based exploratory data analysis has garnered significant attention within the data analysis community. The emergence of large language models (LLMs) has facilitated the paradigm shift from manual to automated data exploration. However, existing methods generally lack the ability for cross-domain analysis, and the exploration of LLMs capabilities remains insufficient. This paper presents TiInsight, an SQL-based automated cross-domain exploratory data analysis system. First, TiInsight offers a user-friendly GUI enabling users to explore data using natural language queries. Second, TiInsight offers a robust cross-domain exploratory data analysis pipeline: hierarchical data context (i.e., HDC) generation, question clarification and decomposition, text-to-SQL (i.e., TiSQL), and data visualization (i.e., TiChart). Third, we have implemented and deployed TiInsight in the production environment of PingCAP and demonstrated its capabilities using representative datasets. The demo video is available at https://youtu.be/JzYFyYd-emI."}
{"id": "2601.09081", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.09081", "abs": "https://arxiv.org/abs/2601.09081", "authors": ["Zekun Wang", "Binghao Yue", "Weitao Pan", "Jianyi Shi", "Yue Hao"], "title": "A Grouped Sorting Queue Supporting Dynamic Updates for Timer Management in High-Speed Network Interface Cards", "comment": null, "summary": "With the hardware offloading of network functions, network interface cards (NICs) undertake massive stateful, high-precision, and high-throughput tasks, where timers serve as a critical enabling component. However, existing timer management schemes suffer from heavy software load, low precision, lack of hardware update support, and overflow. This paper proposes two novel operations for priority queues--update and group sorting--to enable hardware timer management. To the best of our knowledge, this work presents the first hardware priority queue to support an update operation through the composition and propagation of basic operations to modify the priorities of elements within the queue. The group sorting mechanism ensures correct timing behavior post-overflow by establishing a group boundary priority to alter the sorting process and element insertion positions. Implemented with a hybrid architecture of a one-dimension (1D) systolic array and shift registers, our design is validated through packet-level simulations for flow table timeout management. Results demonstrate that a 4K-depth, 16-bit timer queue achieves over 500 MHz (175 Mpps, 12 ns precision) in a 28nm process and over 300 MHz (116 Mpps) on an FPGA. Critically, it reduces LUTs and FFs usage by 31% and 25%, respectively, compared to existing designs."}
{"id": "2601.08919", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08919", "abs": "https://arxiv.org/abs/2601.08919", "authors": ["Sourav Saha", "Mandar Mitra"], "title": "Fine Grained Evaluation of LLMs-as-Judges", "comment": null, "summary": "A good deal of recent research has focused on how Large Language Models\n  (LLMs) may be used as `judges' in place of humans to evaluate the quality\n  of the output produced by various text / image processing systems. Within\n  this broader context, a number of studies have investigated the specific\n  question of how effectively LLMs can be used as relevance assessors for\n  the standard ad hoc task in Information Retrieval (IR). We extend these\n  studies by looking at additional questions. Most importantly, we use a\n  Wikipedia based test collection created by the INEX initiative, and\n  prompt LLMs to not only judge whether documents are relevant /\n  non-relevant, but to highlight relevant passages in documents that it\n  regards as useful. The human relevance assessors involved in creating\n  this collection were given analogous instructions, i.e., they were asked\n  to highlight all passages within a document that respond to the\n  information need expressed in a query. This enables us to evaluate the\n  quality of LLMs as judges not only at the document level, but to also\n  quantify how often these `judges' are right for the right reasons.\n  Our findings suggest that LLMs-as-judges work best under human\n  supervision."}
{"id": "2601.09098", "categories": ["cs.IT", "eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.09098", "abs": "https://arxiv.org/abs/2601.09098", "authors": ["Yifeng Qin", "Jing Chen", "Zhi Hao Jiang", "Zhi Ning Chen", "Yongming Huang"], "title": "Overcoming the Shadow: Bending Airy Beams for Radiative Near-Field Multi-User Access in Half-Space Blockage Scenarios", "comment": null, "summary": "The move to next-generation wireless communications with extremely large-scale antenna arrays (ELAAs) brings the communications into the radiative near-field (RNF) region, where distance-aware focusing is feasible. However, high-frequency RNF links are highly vulnerable to blockage in indoor environments dominated by half-space obstacles (walls, corners) that create knife-edge shadows. Conventional near-field focused beams offer high gain in line-of-sight (LoS) scenarios but suffer from severe energy truncation and effective-rank collapse in shadowed regions, making hardware remedies such as reconfigurable intelligent surfaces (RIS) impractical. We propose a beamforming strategy that exploits the auto-bending property of Airy beams to mitigate half-space blockage without additional hardware. The Airy beam is designed to ``ride'' the diffraction edge, accelerating its main lobe into the shadow to restore connectivity. Our contributions are threefold: (i) a Green's function-based RNF multi-user channel model that analytically reveals singular-value collapse behind knife-edge obstacles; (ii) an Airy analog beamforming scheme that optimizes the bending trajectory to recover the effective channel rank; and (iii) an Airy null-steering method that aligns oscillatory nulls with bright-region users to suppress interference in mixed shadow/bright scenarios. Simulations show that the proposed edge-riding Airy strategy achieves an SNR improvement of over 20 dB and restores full-rank connectivity in shadowed links compared to conventional RNF focusing, virtually eliminating outage in geometric shadows and increasing multi-user spectral efficiency by approximately 35\\% under typical indoor ELAA configurations. These results demonstrate robust RNF multi-user access in half-space blockage scenarios without relying on RIS."}
{"id": "2601.09139", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.09139", "abs": "https://arxiv.org/abs/2601.09139", "authors": ["Gramoz Goranci", "Monika Henzinger", "Peter Kiss", "Ali Momeni", "Gernot Zöcklein"], "title": "Dynamic Hierarchical $j$-Tree Decomposition and Its Applications", "comment": "SODA 2026", "summary": "We develop a new algorithmic framework for designing approximation algorithms for cut-based optimization problems on capacitated undirected graphs that undergo edge insertions and deletions. Specifically, our framework dynamically maintains a variant of the hierarchical $j$-tree decomposition of [Madry FOCS'10], achieving a poly-logarithmic approximation factor to the graph's cut structure and supporting edge updates in $O(n^ε)$ amortized update time, for any arbitrarily small constant $ε\\in (0,1)$.\n  Consequently, we obtain new trade-offs between approximation and update/query time for fundamental cut-based optimization problems in the fully dynamic setting, including all-pairs minimum cuts, sparsest cut, multi-way cut, and multi-cut. For the last three problems, these trade-offs give the first fully-dynamic algorithms achieving poly-logarithmic approximation in sub-linear time per operation.\n  The main technical ingredient behind our dynamic hierarchy is a dynamic cut-sparsifier algorithm that can handle vertex splits with low recourse. This is achieved by white-boxing the dynamic cut sparsifier construction of [Abraham et al. FOCS'16], based on forest packing, together with new structural insights about the maintenance of these forests under vertex splits. Given the versatility of cut sparsification in both the static and dynamic graph algorithms literature, we believe this construction may be of independent interest."}
{"id": "2601.09159", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09159", "abs": "https://arxiv.org/abs/2601.09159", "authors": ["Zhibo Zhang", "Yang Xu", "Kai Ming Ting", "Cam-Tu Nguyen"], "title": "LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval", "comment": null, "summary": "Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \\emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness."}
{"id": "2601.09124", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09124", "abs": "https://arxiv.org/abs/2601.09124", "authors": ["Rachel St. Clair", "John Austin Cook", "Peter Sutor", "Victor Cavero", "Garrett Mindt"], "title": "The .serva Standard: One Primitive for All AI Cost Reduced, Barriers Removed", "comment": null, "summary": "Artificial Intelligence (AI) infrastructure faces two compounding crises. Compute payload - the unsustainable energy and capital costs of training and inference - threatens to outpace grid capacity and concentrate capability among a handful of organizations. Data chaos - the 80% of project effort consumed by preparation, conversion, and preprocessing - strangles development velocity and locks datasets to single model architectures. Current approaches treat these as separate problems, managing each with incremental optimization while increasing ecosystem complexity. This paper presents ServaStack: a universal data format (.serva) paired with a universal AI compute engine (Chimera). The .serva format achieves lossless compression by encoding information using laser holography principles, while Chimera converts compute operations into a representational space where computation occurs directly on .serva files without decompression. The result is automatic data preprocessing. The Chimera engine enables any existing model to operate on .serva data without retraining, preserving infrastructure investments while revamping efficiency. Internal benchmarks demonstrate 30-374x energy efficiency improvements (96-99% reduction), 4x-34x lossless storage compression, and 68x compute payload reduction without accuracy loss when compared to RNN, CNN, and MLP models on FashionMNIST and MNIST datasets. At hyperscale with one billion daily iterations, these gains translate to $4.85M savings per petabyte per training cycle. When any data flows to any model on any hardware, the AI development paradigm shifts. The bottleneck moves from infrastructure to imagination."}
{"id": "2601.09289", "categories": ["cs.DS", "cs.CC", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.09289", "abs": "https://arxiv.org/abs/2601.09289", "authors": ["Takashi Horiyama", "Takehiro Ito", "Jun Kawahara", "Shin-ichi Minato", "Akira Suzuki", "Ryuhei Uehara", "Yutaro Yamaguchi"], "title": "Computational Complexity of Swish", "comment": "10 pages, 5 figures", "summary": "Swish is a card game in which players are given cards having symbols (hoops and balls), and find a valid superposition of cards, called a \"swish.\" Dailly, Lafourcade, and Marcadet (FUN 2024) studied a generalized version of Swish and showed that the problem is solvable in polynomial time with one symbol per card, while it is NP-complete with three or more symbols per card. In this paper, we resolve the previously open case of two symbols per card, which corresponds to the original game. We show that Swish is NP-complete for this case. Specifically, we prove the NP-hardness when the allowed transformations of cards are restricted to a single (horizontal or vertical) flip or 180-degree rotation, and extend the results to the original setting allowing all three transformations. In contrast, when neither transformation is allowed, we present a polynomial-time algorithm. Combining known and our results, we establish a complete characterization of the computational complexity of Swish with respect to both the number of symbols per card and the allowed transformations."}
{"id": "2601.09286", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09286", "abs": "https://arxiv.org/abs/2601.09286", "authors": ["Hanze Guo", "Jianxun Lian", "Xiao Zhou"], "title": "Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models", "comment": "25 pages, 6 figures", "summary": "Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD."}
{"id": "2601.09137", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09137", "abs": "https://arxiv.org/abs/2601.09137", "authors": ["Mingyu Hu", "Nan Liu", "Wei Kang"], "title": "Movable Antenna Assisted Dual-Polarized Multi-Cell Cooperative AirComp: An Alternating Optimization Approach", "comment": null, "summary": "Over-the-air computation (AirComp) is a key enabler for distributed optimization, since it leverages analog waveform superposition to perform aggregation and thereby mitigates the communication bottleneck caused by iterative information exchange. However, AirComp is sensitive to wireless environment and conventional systems with fixed single-polarized base-station arrays cannot fully exploit spatial degrees of freedom while also suffering from polarization mismatch. To overcome these limitations, this paper proposes a multi-cell cooperative air-computation framework assisted by dual-polarized movable antennas (D-PMA), and formulates a mean squared error (MSE) minimization problem by jointly optimizing the combining matrix, polarization vectors, antenna positions, and user transmit coefficients. The resulting problem is highly nonconvex, so an alternating algorithm is developed in which closed-form updates are obtained for the combining matrix and transmit coefficients. Then a method based on successive convex approximation (SCA) and semidefinite relaxation (SDR) is proposed to refine polarization vectors, and the antenna positions are updated using a gradient-based method. In addition, we develop a statistical-channel-based scheme for optimizing the antenna locations, and we further present the corresponding algorithm to efficiently obtain the solution. Numerical results show that the proposed movable dual-polarized scheme consistently outperforms movable single-polarized and fixed-antenna baselines under both instantaneous and statistical channels."}
{"id": "2601.09477", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.09477", "abs": "https://arxiv.org/abs/2601.09477", "authors": ["Joel Andersson", "Matti Karppa"], "title": "Engineering Compressed Matrix Multiplication with the Fast Walsh-Hadamard Transform", "comment": "23 pages", "summary": "We present an implementation of Pagh's compressed matrix multiplication algorithm, a randomized algorithm that constructs sketches of matrices to compute an unbiased estimate of their product. By leveraging fast polynomial multiplication via the FFT, the algorithm achieves high performance when the product matrix is sparse or contains only a small number of entries with magnitudes significantly larger than the rest. We show empirically that the algorithm is practical and can outperform state-of-the-art DGEMM implementations when the product matrix has few nonzero entries or is otherwise dominated by a small subset of elements with large magnitude. As a minor theoretical contribution, we replace the FFT with the Fast Walsh-Hadamard Transform (FWHT) in sketched multiplication, preserving all correctness and variance guarantees of the original algorithm.\n  Experiments with our carefully engineered multithreaded CPU implementation for dense double-precision matrices on 64-core CPU nodes across a range of synthetic benchmarks, exhibiting variable sparsity patterns, show that the FWHT variant is up to 4 times faster than the FFT-based version. Under favorable sparsity and magnitude patterns in the product matrix, our FWHT-based implementation achieves a speedup of up to 40 over DGEMM from Intel MKL, with low probability of error in the estimates. Our implementation is released as free software and comes with NumPy-compatible Python bindings."}
{"id": "2601.09306", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09306", "abs": "https://arxiv.org/abs/2601.09306", "authors": ["Xin Xia", "Hongzhi Yin", "Shane Culpepper"], "title": "On-Device Large Language Models for Sequential Recommendation", "comment": "WSDM'26", "summary": "On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs."}
{"id": "2601.09188", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09188", "abs": "https://arxiv.org/abs/2601.09188", "authors": ["Yaqian Zhang", "Jingke Xu"], "title": "Reducing The Sub-packetization Level of Optimal-Access Cooperative MSR Codes", "comment": null, "summary": "Cooperative MSR codes are a kind of storage codes which enable optimal-bandwidth repair of any $h\\geq2$ node erasures in a cooperative way, while retaining the minimum storage as an $[n,k]$ MDS code. Each code coordinate (node) is assumed to store an array of $\\ell$ symbols, where $\\ell$ is termed as sub-packetization. Large sub-packetization tends to induce high complexity, large input/output in practice. To address the disk IO capability, a cooperative MSR code is said to have optimal-access property, if during node repair, the amount of data accessed at each helper node meets a theoretical lower bound.\n  In this paper, we focus on reducing the sub-packetization of optimal-access cooperative MSR codes with two erasures. At first, we design two crucial MDS array codes for repairing a specific repair pattern of two erasures with optimal access. Then, using the two codes as building blocks and by stacking up of the two codes for several times, we obtain an optimal-access cooperative MSR code with two erasures. The derived code has sub-packetization $\\ell=r^{\\binom{n}{2}-\\lfloor\\frac{n}{r}\\rfloor(\\binom{r}{2}-1)}$ where $r=n-k$, and it reduces $\\ell$ by a fraction of $1/r^{\\lfloor\\frac{n}{r}\\rfloor(\\binom{r}{2}-1)}$ compared with the state of the art ($\\ell=r^{\\binom{n}{2}}$)."}
{"id": "2601.09489", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.09489", "abs": "https://arxiv.org/abs/2601.09489", "authors": ["Peyman Afshani", "Rezaul Chowdhury", "Inge Li Gørtz", "Mayank Goswami", "Francesco Silvestri", "Mariafiore Tognon"], "title": "How many users have been here for a long time? Efficient solutions for counting long aggregated visits", "comment": null, "summary": "This paper addresses the Counting Long Aggregated Visits problem, which is defined as follows. We are given $n$ users and $m$ regions, where each user spends some time visiting some regions. For a parameter $k$ and a query consisting of a subset of $r$ regions, the task is to count the number of distinct users whose aggregate time spent visiting the query regions is at least $k$. This problem is motivated by queries arising in the analysis of large-scale mobility datasets. We present several exact and approximate data structures for supporting counting long aggregated visits, as well as conditional and unconditional lower bounds. First, we describe an exact data structure that exhibits a space-time tradeoff, as well as efficient approximate solutions based on sampling and sketching techniques. We then study the problem in geometric settings where regions are points in $\\mathbb{R}^d$ and queries are hyperrectangles, and derive exact data structures that achieve improved performance in these structured spaces."}
{"id": "2601.09366", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09366", "abs": "https://arxiv.org/abs/2601.09366", "authors": ["Jana Isabelle Friese", "Andreas Konstantin Kruff", "Philipp Schaer", "Norbert Fuhr", "Nicola Ferro"], "title": "LISP -- A Rich Interaction Dataset and Loggable Interactive Search Platform", "comment": null, "summary": "We present a reusable dataset and accompanying infrastructure for studying human search behavior in Interactive Information Retrieval (IIR). The dataset combines detailed interaction logs from 61 participants (122 sessions) with user characteristics, including perceptual speed, topic-specific interest, search expertise, and demographic information. To facilitate reproducibility and reuse, we provide a fully documented study setup, a web-based perceptual speed test, and a framework for conducting similar user studies. Our work allows researchers to investigate individual and contextual factors affecting search behavior, and to develop or validate user simulators that account for such variability. We illustrate the datasets potential through an illustrative analysis and release all resources as open-access, supporting reproducible research and resource sharing in the IIR community."}
{"id": "2601.09196", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09196", "abs": "https://arxiv.org/abs/2601.09196", "authors": ["K V Harsha", "Jithin Ravi", "Tobias Koch"], "title": "Second-Order Asymptotics of Two-Sample Tests", "comment": "Submitted to the 2026 IEEE International Symposium on Information Theory", "summary": "In two-sampling testing, one observes two independent sequences of independent and identically distributed random variables distributed according to the distributions $P_1$ and $P_2$ and wishes to decide whether $P_1=P_2$ (null hypothesis) or $P_1\\neq P_2$ (alternative hypothesis). The Gutman test for this problem compares the empirical distributions of the observed sequences and decides on the null hypothesis if the Jensen-Shannon (JS) divergence between these empirical distributions is below a given threshold. This paper proposes a generalization of the Gutman test, termed \\emph{divergence test}, which replaces the JS divergence by an arbitrary divergence. For this test, the exponential decay of the type-II error probability for a fixed type-I error probability is studied. First, it is shown that the divergence test achieves the optimal first-order exponent, irrespective of the choice of divergence. Second, it is demonstrated that the divergence test with an invariant divergence achieves the same second-order asymptotics as the Gutman test. In addition, it is shown that the Gutman test is the GLRT for the two-sample testing problem, and a connection between two-sample testing and robust goodness-of-fit testing is established."}
{"id": "2601.09577", "categories": ["cs.DS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09577", "abs": "https://arxiv.org/abs/2601.09577", "authors": ["MD Nazmul Alam Shanto", "Md. Tanzeem Rahat", "Md. Manzurul Hasan"], "title": "Permutation Matching Under Parikh Budgets: Linear-Time Detection, Packing, and Disjoint Selection", "comment": "12 pages (Excluding reference)", "summary": "We study permutation (jumbled/Abelian) pattern matching over a general alphabet $Σ$. Given a pattern P of length m and a text T of length n, the classical task is to decide whether T contains a length-m substring whose Parikh vector equals that of P . While this existence problem admits a linear-time sliding-window solution, many practical applications require optimization and packing variants beyond mere detection. We present a unified sliding-window framework based on maintaining the Parikh-vector difference between P and the current window of T , enabling permutation matching in O(n + σ) time and O(σ) space, where σ = |Σ|. Building on this foundation, we introduce a combinatorial-optimization variant that we call Maximum Feasible Substring under Pattern Supply (MFSP): find the longest substring S of T whose symbol counts are component-wise bounded by those of P . We show that MFSP can also be solved in O(n + σ) time via a two-pointer feasibility maintenance algorithm, providing an exact packing interpretation of P as a resource budget. Finally, we address non-overlapping occurrence selection by modeling each permutation match as an equal-length interval and proving that a greedy earliest-finishing strategy yields a maximum-cardinality set of disjoint matches, computable in linear time once all matches are enumerated. Our results provide concise, provably correct algorithms with tight bounds, and connect frequency-based string matching to packing-style optimization primitives."}
{"id": "2601.09459", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09459", "abs": "https://arxiv.org/abs/2601.09459", "authors": ["Pei-Chi Lo", "Thomas Y. Lu"], "title": "Dissecting Judicial Reasoning in U.S. Copyright Damage Awards", "comment": "Presented in SIGKDD'25 SciSoc LLM Workshop: Large Language Models for Scientific and Societal Advances", "summary": "Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law."}
{"id": "2601.09222", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09222", "abs": "https://arxiv.org/abs/2601.09222", "authors": ["Ling Liu", "Qi Cao", "Liping Li", "Baoming Bai"], "title": "On Polar Coding with Feedback", "comment": "7 pages, 7 figures and 2 tables; A short version will be submitted to IEEE for possible publication", "summary": "In this work, we investigate the performance of polar codes with the assistance of feedback in communication systems. Although it is well known that feedback does not improve the capacity of memoryless channels, we show that the finite length performance of polar codes can be significantly improved as feedback enables genie-aided decoding and allows more flexible thresholds for the polar coding construction. To analyze the performance under the new construction, we then propose an accurate characterization of the distribution of the error event under the genie-aided successive cancellation (SC) decoding. This characterization can be also used to predict the performance of the standard SC decoding of polar codes with rates close to capacity."}
{"id": "2601.09478", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09478", "abs": "https://arxiv.org/abs/2601.09478", "authors": ["Renqiang Luo", "Dong Zhang", "Yupeng Gao", "Wen Shi", "Mingliang Hou", "Jiaying Liu", "Zhe Wang", "Shuo Yu"], "title": "Bridging Semantic Understanding and Popularity Bias with LLMs", "comment": "10 pages, 4 figs, WWW 2026 accepted", "summary": "Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as \"diversity\" or \"debiasing\", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM."}
{"id": "2601.09254", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09254", "abs": "https://arxiv.org/abs/2601.09254", "authors": ["Changshuo Wang", "Zijian Liang", "Kai Niu", "Ping Zhang"], "title": "A Theoretical Framework for Rate-Distortion Limits in Learned Image Compression", "comment": "14 pages, 6 figures. arXiv version", "summary": "We present a novel systematic theoretical framework to analyze the rate-distortion (R-D) limits of learned image compression. While recent neural codecs have achieved remarkable empirical results, their distance from the information-theoretic limit remains unclear. Our work addresses this gap by decomposing the R-D performance loss into three key components: variance estimation, quantization strategy, and context modeling. First, we derive the optimal latent variance as the second moment under a Gaussian assumption, providing a principled alternative to hyperprior-based estimation. Second, we quantify the gap between uniform quantization and the Gaussian test channel derived from the reverse water-filling theorem. Third, we extend our framework to include context modeling, and demonstrate that accurate mean prediction yields substantial entropy reduction. Unlike prior R-D estimators, our method provides a structurally interpretable perspective that aligns with real compression modules and enables fine-grained analysis. Through joint simulation and end-to-end training, we derive a tight and actionable approximation of the theoretical R-D limits, offering new insights into the design of more efficient learned compression systems."}
{"id": "2601.09496", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09496", "abs": "https://arxiv.org/abs/2601.09496", "authors": ["Jujia Zhao", "Zihan Wang", "Shuaiqun Pan", "Suzan Verberne", "Zhaochun Ren"], "title": "Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning", "comment": null, "summary": "Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \\textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \\textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness."}
{"id": "2601.09300", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09300", "abs": "https://arxiv.org/abs/2601.09300", "authors": ["Minhan Gao", "Kenneth Shum"], "title": "Regenerating codes with minimal disk I/O cost achieving optimal tradeoff between storage and repair bandwidth", "comment": null, "summary": "There are multiple performance metrics in the design of coding schemes for distributed storage systems. The first metric is called repair bandwidth, which measures the network resources required during the repair process. Another critical metric for repair efficiency is disk I/O cost, defined as the amount of data packets accessed at helper nodes to repair the failed node. In an encoding scheme with optimal I/O cost, the number of packets sent to the newcomer is exactly the same as the number of packets read from memory. This mode of repair is referred to as uncoded repair, as no coding operations are performed at the helper node. In addition to minimizing disk I/O cost, an uncoded repair mechanism has the advantage of incurring minimal computational overhead at the helper node. In this paper, we demonstrate that for single node failures, if all surviving nodes participate in the repair of the failed node, we can achieve all points on the fundamental tradeoff curve between storage and repair bandwidth. The design of the proposed encoding scheme is based on the theory of gammoids, a specialized class of graph-based matroids. We prove that this scheme can tolerate an unlimited number of node repair iterations over a field of fixed size."}
{"id": "2601.09523", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09523", "abs": "https://arxiv.org/abs/2601.09523", "authors": ["Abdelrahman Abdallah", "Mohammed Ali", "Muhammad Abdul-Mageed", "Adam Jatowt"], "title": "TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval", "comment": null, "summary": "Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/."}
{"id": "2601.09308", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.09308", "abs": "https://arxiv.org/abs/2601.09308", "authors": ["Peter Harremoës"], "title": "An Information Theoretic Proof of the Radon-Nikodym Theorem", "comment": "7 pages", "summary": "The Radon-Nikodym theorem plays a significant role in the definition of Shannon entropy, f-divergences, and other basic quantities in information theory. The existence of Radon Nikodym derivates appear in many text books in measure theory but in text books on probability or information theory it is often omitted because the proof is often considered to be too difficult."}
{"id": "2601.09530", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09530", "abs": "https://arxiv.org/abs/2601.09530", "authors": ["Bingde Hu", "Enhao Pan", "Wanjing Zhou", "Yang Gao", "Zunlei Feng", "Hao Zhong"], "title": "SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval", "comment": null, "summary": "Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems."}
{"id": "2601.09328", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.09328", "abs": "https://arxiv.org/abs/2601.09328", "authors": ["Adrien Vandenbroucque", "Amedeo Roberto Esposito", "Michael Gastpar"], "title": "Contraction of Rényi Divergences for Discrete Channels: Properties and Applications", "comment": null, "summary": "This work explores properties of Strong Data-Processing constants for Rényi Divergences. Parallels are made with the well-studied $\\varphi$-Divergences, and it is shown that the order $α$ of Rényi Divergences dictates whether certain properties of the contraction of $\\varphi$-Divergences are mirrored or not. In particular, we demonstrate that when $α>1$, the contraction properties can deviate quite strikingly from those of $\\varphi$-Divergences. We also uncover specific characteristics of contraction for the $\\infty$-Rényi Divergence and relate it to $\\varepsilon$-Local Differential Privacy. The results are then applied to bound the speed of convergence of Markov chains, where we argue that the contraction of Rényi Divergences offers a new perspective on the contraction of $L^α$-norms commonly studied in the literature."}
{"id": "2601.09543", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09543", "abs": "https://arxiv.org/abs/2601.09543", "authors": ["Jason Carpenter", "Faaiq Bilal", "Eman Ramadan", "Zhi-Li Zhang"], "title": "Examining DOM Coordinate Effectiveness For Page Segmentation", "comment": null, "summary": "Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation."}
{"id": "2601.09329", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09329", "abs": "https://arxiv.org/abs/2601.09329", "authors": ["Jun Su", "Guangyue Han", "Shlomo Shamai"], "title": "Generalized Schalkwijk-Kailath Coding for Autoregressive Gaussian Channels", "comment": null, "summary": "We propose a Gaussian random coding scheme for AR($p$) Gaussian channels that generalizes the celebrated Schalkwijk-Kailath (SK) coding scheme. This constructive coding scheme, termed the SK(2) coding scheme, yields a closed-form characterization for the corresponding achievable rate. Among many others, this result shows that the celebrated SK coding scheme is not universally optimal, and therefore, disprove the conjecture proposed by Butman in \\cite{butman1976linear}."}
{"id": "2601.09562", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09562", "abs": "https://arxiv.org/abs/2601.09562", "authors": ["Abdelrahman Abdallah", "Mohamed Darwish Mounis", "Mahmoud Abdalla", "Mahmoud SalahEldin Kasem", "Mostafa Farouk Senussi", "Mohamed Mahmoud", "Mohammed Ali", "Adam Jatowt", "Hyun-Soo Kang"], "title": "MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval", "comment": null, "summary": "Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/."}
{"id": "2601.09347", "categories": ["cs.IT", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.09347", "abs": "https://arxiv.org/abs/2601.09347", "authors": ["Pierre Jean-Claude Robert Bertrand"], "title": "A Constructive Method to Minimize the Index of Coincidence under Marginal Constraints", "comment": null, "summary": "We consider the problem of minimizing the index of coincidence of a joint distribution under fixed marginal constraints. This objective is motivated by several applications in information theory, where the index of coincidence naturally arises. A closed-form solution is known when the marginals satisfy a strong feasibility condition, but this condition is rarely met in practice. We first show that the measure of the set of marginals for which condition applies vanishes as the dimension grows. We then characterize the structure of the optimal coupling in the general case, proving that it exhibits a monotone staircase of zero entries. Based on this structure, we propose an explicit iterative construction and prove that it converges in finitely many steps to a minimizer. Main result of the paper is a complete constructive solution of index-of-coincidence minimization."}
{"id": "2601.09362", "categories": ["cs.IT", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.09362", "abs": "https://arxiv.org/abs/2601.09362", "authors": ["Yuto Mizunuma", "Yuichiro Fujiwara"], "title": "Asymptotic Rate Bounds and Constructions for the Inclusive Variant of Disjunct Matrices", "comment": "9 pages, 2 figures", "summary": "Disjunct matrices, also known as cover-free families and superimposed codes, are combinatorial arrays widely used in group testing. Among their variants, those that satisfy an additional combinatorial property called inclusiveness form a special class suitable for computationally efficient and highly error-tolerant group testing under the general inhibitor complex model, a broad framework that subsumes practical settings such as DNA screening. Despite this relevance, the asymptotic behavior of the inclusive variant of disjunct matrices has remained largely unexplored. In particular, it was not previously known whether this variant can achieve an asymptotically positive rate, a requirement for scalable group testing designs. In this work, we establish the first nontrivial asymptotic lower bound on the maximum achievable rate of the inclusive variant, which matches the strongest known upper bound up to a logarithmic factor. Our proof is based on the probabilistic method and yields a simple and efficient randomized construction. Furthermore, we derandomize this construction to obtain a deterministic polynomial-time construction. These results clarify the asymptotic potential of robust and scalable group testing under the general inhibitor complex model."}
{"id": "2601.09390", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09390", "abs": "https://arxiv.org/abs/2601.09390", "authors": ["Devansh Jain", "Lakshmi Prasad Natarajan"], "title": "On Decoding First- and Second-Order BiD Codes", "comment": "11 pages, 4 figures", "summary": "BiD codes, which are a new family of algebraic codes of length $3^m$, achieve the erasure channel capacity under bit-MAP decoding and offer asymptotically larger minimum distance than Reed-Muller (RM) codes. In this paper we propose fast maximum-likelihood (ML) and max-log-MAP decoders for first-order BiD codes. For second-order codes, we identify their minimum-weight parity checks and ascertain a code property known as 'projection' in the RM coding literature. We use these results to design a belief propagation decoder that performs within 1 dB of ML decoder for block lengths 81 and 243."}
{"id": "2601.09406", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09406", "abs": "https://arxiv.org/abs/2601.09406", "authors": ["Akira Kamatsuka", "Takahiro Yoshida"], "title": "A Generalized Leakage Interpretation of Alpha-Mutual Information", "comment": null, "summary": "This paper presents a unified interpretation of $α$-mutual information ($α$-MI) in terms of generalized $g$-leakage. Specifically, we present a novel interpretation of $α$-MI within an extended framework for quantitative information flow based on adversarial generalized decision problems. This framework employs the Kolmogorov-Nagumo mean and the $q$-logarithm to characterize adversarial gain. Furthermore, we demonstrate that, within this framework, the parameter $α$ can be interpreted as a measure of the adversary's risk aversion."}
{"id": "2601.09498", "categories": ["cs.IT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.09498", "abs": "https://arxiv.org/abs/2601.09498", "authors": ["Leonhard Grosse", "Sara Saeidian", "Tobias J. Oechtering", "Mikael Skoglund"], "title": "Dobrushin Coefficients of Private Mechanisms Beyond Local Differential Privacy", "comment": null, "summary": "We investigate Dobrushin coefficients of discrete Markov kernels that have bounded pointwise maximal leakage (PML) with respect to all distributions with a minimum probability mass bounded away from zero by a constant $c>0$. This definition recovers local differential privacy (LDP) for $c\\to 0$. We derive achievable bounds on contraction in terms of a kernels PML guarantees, and provide mechanism constructions that achieve the presented bounds. Further, we extend the results to general $f$-divergences by an application of Binette's inequality. Our analysis yields tighter bounds for mechanisms satisfying LDP and extends beyond the LDP regime to any discrete kernel."}
{"id": "2601.09519", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09519", "abs": "https://arxiv.org/abs/2601.09519", "authors": ["Henrique K. Miyamoto", "Sheng Yang"], "title": "Error Exponents for Randomised List Decoding", "comment": "12 pages, 1 figure", "summary": "This paper studies random-coding error exponents of randomised list decoding, in which the decoder randomly selects $L$ messages with probabilities proportional to the decoding metric of the codewords. The exponents (or bounds) are given for mismatched, and then particularised to matched and universal decoding metrics. Two regimes are studied: for fixed list size, we derive an ensemble-tight random-coding error exponent, and show that, for the matched metric, it does not improve the error exponent of ordinary decoding. For list sizes growing exponentially with the block-length, we provide a non-trivial lower bound to the error exponent that is tight at high rates under the matched metric."}
{"id": "2601.09550", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09550", "abs": "https://arxiv.org/abs/2601.09550", "authors": ["Roberto Bruno", "Adrien Vandenbroucque", "Amedeo Roberto Esposito"], "title": "A Finite-Sample Strong Converse for Binary Hypothesis Testing via (Reverse) Rényi Divergence", "comment": "An extended version, with proofs, of a paper submitted to ISIT 2026", "summary": "This work investigates binary hypothesis testing between $H_0\\sim P_0$ and $H_1\\sim P_1$ in the finite-sample regime under asymmetric error constraints. By employing the ``reverse\" Rényi divergence, we derive novel non-asymptotic bounds on the Type II error probability which naturally establish a strong converse result. Furthermore, when the Type I error is constrained to decay exponentially with a rate $c$, we show that the Type II error converges to 1 exponentially fast if $c$ exceeds the Kullback-Leibler divergence $D(P_1\\|P_0)$, and vanishes exponentially fast if $c$ is smaller. Finally, we present numerical examples demonstrating that the proposed converse bounds strictly improve upon existing finite-sample results in the literature."}
{"id": "2601.09554", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09554", "abs": "https://arxiv.org/abs/2601.09554", "authors": ["Rayan Chouity", "Charbel Hannoun", "Jihad Fahs", "Ibrahim Abou-Faycal"], "title": "On Linear Estimators for some Stable Vectors", "comment": null, "summary": "We consider the estimation problem for jointly stable random variables. Under two specific dependency models: a linear transformation of two independent stable variables and a sub-Gaussian symmetric $α$-stable (S$α$S) vector, we show that the conditional mean estimator is linear in both cases. Moreover, we find dispersion optimal linear estimators. Interestingly, for the sub-Gaussian (S$α$S) vector, both estimators are identical generalizing the well-known Gaussian result of the conditional mean being the best linear minimum-mean square estimator."}
{"id": "2601.09564", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09564", "abs": "https://arxiv.org/abs/2601.09564", "authors": ["Barış Nakiboğlu"], "title": "The Spectral Representations Of The Simple Hypothesis Testing Problem", "comment": "16 Pages", "summary": "The convex conjugate (i.e., the Legendre transform) of Type II error probability (volume) as a function of Type I error probability (volume) is determined for the hypothesis testing problem with randomized detectors. The derivation relies on properties of likelihood ratio quantiles and is general enough to extend to the case of $σ$-finite measures in all non-trivial cases. The convex conjugate of the Type II error volume, called the primitive entropy spectrum, is expressed as an integral of the complementary distribution function of the likelihood ratio using a standard spectral identity. The resulting dual characterization of the Type II error volume leads to state of the art bounds for the case of product measures via Berry--Esseen theorem through a brief analysis relying on properties of the Gaussian Mills ratio, both with and without tilting."}
{"id": "2601.09581", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09581", "abs": "https://arxiv.org/abs/2601.09581", "authors": ["Dorsa Fathollahi", "V. Arvind Rameshwar", "V. Lalitha"], "title": "On the Error Probability of RPA Decoding of Reed-Muller Codes over BMS Channels", "comment": "7 pages, to be submitted to the IEEE for possible publication", "summary": "We analyze the performance of the Recursive Projection-Aggregation (RPA) decoder of Ye and Abbe (2020), for Reed-Muller (RM) codes, over general binary memoryless symmetric (BMS) channels. Our work is a significant generalization of a recent result of Rameshwar and Lalitha (2025) that showed that the RPA decoder provably achieves vanishing error probabilities for \"low-rate\" RM codes, over the binary symmetric channel (BSC). While a straightforward generalization of the proof strategy in that paper will require additional, restrictive assumptions on the BMS channel, our technique, which employs an equivalence between the RPA projection operation and a part of the \"channel combining\" phase in polar codes, requires no such assumptions. Interestingly, such an equivalence allows for the use of a generic union bound on the error probability of the first-order RM code (the \"base case\" of the RPA decoder), under maximum-likelihood decoding, which holds for any BMS channel. We then exploit these observations in the proof strategy outlined in the work of Rameshwar and Lalitha (2025), and argue that, much like in the case of the BSC, one can obtain vanishing error probabilities, in the large $n$ limit (where $n$ is the blocklength), for RM orders that scale roughly as $\\log \\log n$, for all BMS channels."}
{"id": "2601.09640", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09640", "abs": "https://arxiv.org/abs/2601.09640", "authors": ["David Miller", "Rémi A. Chou"], "title": "Secret sharing with additive access structures from correlated random variables", "comment": "7 pages, to be submitted to ISIT 2026", "summary": "We generalize secret-sharing models that rely on correlated randomness and public communication, originally designed for a fixed access structure, to support a sequence of dynamic access structures, which we term an Additive Access Structure. Specifically, the access structure is allowed to monotonically grow by having any subset of participants added to it at a given time step, and the dealer only learns of these changes to the access structure on the time step that they occur. For this model, we prove the existence of a secret sharing strategy that achieves the same secret rate at each time step as the best known strategy for the fixed access structure version of this model. We also prove that there exists a strategy that is capacity-achieving at any time step where the access structure is a threshold access structure."}
{"id": "2601.09674", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.09674", "abs": "https://arxiv.org/abs/2601.09674", "authors": ["Lei Huang"], "title": "Counting and Entropy Bounds for Structure-Avoiding Spatially-Coupled LDPC Constructions", "comment": null, "summary": "Designing large coupling memory quasi-cyclic spatially-coupled LDPC (QC-SC-LDPC) codes with low error floors requires eliminating specific harmful substructures (e.g., short cycles) induced by edge spreading and lifting. Building on our work~\\cite{r15} that introduced a Clique Lovász Local Lemma (CLLL)-based design principle and a Moser--Tardos (MT)-type constructive approach, this work quantifies the size and structure of the feasible design space. Using the quantitative CLLL, we derive explicit lower bounds on the number of partition matrices satisfying a given family of structure-avoidance constraints, and further obtain bounds on the number of non-equivalent solutions under row/column permutations. Moreover, via Rényi-entropy bounds for the MT distribution, we provide a computable lower bound on the number of distinct solutions that the MT algorithm can output, giving a concrete diversity guarantee for randomized constructions. Specializations for eliminating 4-cycle candidates yield closed-form bounds as functions of system parameters, offering a principled way to size memory/lifting and to estimate the remaining search space."}
{"id": "2601.09679", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.09679", "abs": "https://arxiv.org/abs/2601.09679", "authors": ["Adel Javanmard", "David P. Woodruff"], "title": "Progress on the Courtade-Kumar Conjecture: Optimal High-Noise Entropy Bounds and Generalized Coordinate-wise Mutual Information", "comment": "16 pages", "summary": "The Courtade-Kumar conjecture posits that dictatorship functions maximize the mutual information between the function's output and a noisy version of its input over the Boolean hypercube. We present two significant advancements related to this conjecture. First, we resolve an open question posed by Courtade and Kumar, proving that for any Boolean function (regardless of bias), the sum of mutual information between the function's output and the individual noisy input coordinates is bounded by $1-H(α)$, where $α$ is the noise parameter of the Binary Symmetric Channel. This generalizes their previous result which was restricted to balanced Boolean functions. Second, we advance the study of the main conjecture in the high noise regime. We establish an optimal error bound of $O(λ^2)$ for the asymptotic entropy expansion, where $λ= (1-2α)^2$, improving upon the previous best-known bounds. This refined analysis leads to a sharp, linear Fourier concentration bound for highly informative functions and significantly extends the range of the noise parameter $λ$ for which the conjecture is proven to hold."}
