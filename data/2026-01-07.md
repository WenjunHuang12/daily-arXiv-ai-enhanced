<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 20]
- [cs.DS](#cs.DS) [Total: 4]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Breaking Rank -- A Novel Unscented Kalman Filter for Parameter Estimations of a Lumped-Parameter Cardiovascular Model](https://arxiv.org/abs/2601.02390)
*Alex Thornton,Ian Halliday,Harry Saxton,Xu Xu*

Main category: cs.IT

TL;DR: 提出改进的UKF算法，解决心血管模型参数估计中的秩亏问题，实现10个参数的高精度估计


<details>
  <summary>Details</summary>
Motivation: 传统UKF在参数估计中存在秩亏问题，只能估计少量参数，需要固定非关键参数，限制了心血管模型参数估计的临床应用

Method: 对UKF进行改进，克服秩亏问题，使算法能够同时估计所有参数，包括影响较小的参数，无需先验参数分布知识

Result: 改进的UKF在50个10参数样本的挑战性数据集上，90%情况下能恢复98%以上精度的几乎所有参数，显著优于原始UKF

Conclusion: 改进的UKF极大提升了参数估计的实用性，实现了心血管模型几乎所有参数的完全可识别性，具有优异的鲁棒性

Abstract: We make modifications to the unscented Kalman filter (UKF) which bestow almost complete practical identifiability upon a lumped-parameter cardiovascular model with 10 parameters and 4 output observables - a highly non-linear, stiff problem of clinical significance. The modifications overcome the challenging problems of rank deficiency when applying the UKF to parameter estimation. Rank deficiency usually means only a small subset of parameters can be estimated. Traditionally, pragmatic compromises are made, such as selecting an optimal subset of parameters for estimation and fixing non-influential parameters. Kalman filters are typically used for dynamical state tracking, to facilitate the control u at every time step. However, for the purpose of parameter estimation, this constraint no longer applies. Our modification has transformed the utility of UKF for the parameter estimation purpose, including minimally influential parameters, with excellent robustness (i.e., under severe noise corruption, challenging patho-physiology, and no prior knowledge of parameter distributions). The modified UKF algorithm is robust in recovering almost all parameters to over 98% accuracy, over 90% of the time, with a challenging target data set of 50, 10-parameter samples. We compare this to the original implementation of the UKF algorithm for parameter estimation and demonstrate a significant improvement.

</details>


### [2] [Weights on finite fields and failures of the MacWilliams identities](https://arxiv.org/abs/2601.02608)
*Jay A. Wood*

Main category: cs.IT

TL;DR: 论文探讨了线性码的权重枚举器与其对偶码权重枚举器之间的关系，指出汉明权重枚举器具有对称性，但存在一类权重其枚举器表现出相反行为。


<details>
  <summary>Details</summary>
Motivation: 研究线性码与其对偶码之间权重枚举器关系的普遍性，探索MacWilliams定理之外的特殊情况，理解哪些权重类型会导致对偶码权重枚举器的不对称性。

Method: 通过理论分析和构造反例，证明存在一类有限域上的权重，其权重枚举器不遵循MacWilliams定理的对称性，即两个具有相同权重枚举器的线性码，它们的对偶码却有不同的权重枚举器。

Result: 发现并证明存在一类权重，其权重枚举器表现出与汉明权重枚举器相反的行为：两个线性码可以有相同的权重枚举器，但它们的对偶码却有不同的权重枚举器。

Conclusion: MacWilliams定理中汉明权重枚举器的对称性不是普遍性质，存在一类权重其枚举器关系表现出相反行为，这扩展了对线性码权重枚举器理论的理解。

Abstract: In the 1960s, MacWilliams proved that the Hamming weight enumerator of a linear code over a finite field completely determines, and is determined by, the Hamming weight enumerator of its dual code. In particular, if two linear codes have the same Hamming weight enumerator, then their dual codes have the same Hamming weight enumerator.
  In contrast, there is a wide class of weights on finite fields whose weight enumerators have the opposite behavior: there exist two linear codes having the same weight enumerator, but their dual codes have different weight enumerators.

</details>


### [3] [State-Dependent Fading Gaussian Channel with Common Reconstruction Constraints](https://arxiv.org/abs/2601.02802)
*Viswanathan Ramachandran*

Main category: cs.IT

TL;DR: 本文研究了在高斯衰落信道中同时传输消息和联合重构信道状态的通信问题，其中状态干扰已知于发送端，衰落系数完全已知于收发双方，接收端需解码消息并在共同重构约束下估计状态，给出了最优率失真权衡区域的完整表征。


<details>
  <summary>Details</summary>
Motivation: 研究在衰落高斯信道中同时实现消息传输和信道状态联合重构的问题。传统通信系统主要关注消息传输，但在许多实际场景中（如传感通信、状态估计等），接收端不仅需要解码消息，还需要准确估计信道状态。本文考虑发送端非因果已知状态干扰、收发双方完全已知瞬时衰落系数的场景，探索消息传输与状态估计之间的最优权衡。

Method: 采用信息论方法分析高斯衰落模型，其中状态为独立同分布高斯序列，状态干扰非因果已知于发送端，瞬时衰落系数完美已知于收发双方。接收端需解码传输消息并在共同重构约束下估计状态，确保其估计与发送端一致。通过理论分析推导最优率失真权衡区域。

Result: 完整表征了该场景下的最优率失真权衡区域，这是本文的主要理论贡献。通过数值示例验证了理论结果，展示了率失真和功率失真之间的权衡关系。

Conclusion: 本文成功解决了衰落高斯信道中消息传输与状态联合重构的优化问题，给出了最优率失真权衡区域的完整理论表征，为同时实现可靠通信和准确状态估计的系统设计提供了理论基础。

Abstract: The task of jointly communicating a message and reconstructing a common estimate of the channel state is examined for a fading Gaussian model with additive state interference. The state is an independent and identically distributed Gaussian sequence known noncausally at the transmitter, and the instantaneous fading coefficient is perfectly known at both the transmitter and the receiver. The receiver is required to decode the transmitted message and, in addition, reconstruct the state under a common reconstruction constraint ensuring that its estimate coincides with that at the transmitter. A complete characterization of the optimal rate distortion tradeoff region for this setting is the main result of our work. The analytical results are also validated through numerical examples illustrating the rate distortion and power distortion tradeoffs.

</details>


### [4] [DeepFP: Deep-Unfolded Fractional Programming for MIMO Beamforming](https://arxiv.org/abs/2601.02822)
*Jianhang Zhu,Tsung-Hui Chang,Liyao Xiang,Kaiming Shen*

Main category: cs.IT

TL;DR: 提出一种混合学习和优化的方法来解决MIMO无线网络中的加权和速率波束成形问题，通过将深度展开网络集成到FastFP算法中进行步长优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如FP和WMMSE）计算量大，需要大矩阵求逆和拉格朗日乘子调优。FastFP方法消除了这些问题，但步长选择困难。因此需要一种更高效的方法。

Method: 将深度展开网络集成到FastFP算法中，用于优化更新步长，形成混合学习和优化的方法。

Result: 数值实验表明，所提方法比基于WMMSE算法的学习方法效率更高。

Conclusion: 提出的混合方法有效解决了多小区MIMO网络中加权和速率波束成形问题，通过深度展开网络优化FastFP步长，显著提高了计算效率。

Abstract: This work proposes a mixed learning-based and optimization-based approach to the weighted-sum-rates beamforming problem in a multiple-input multiple-output (MIMO) wireless network. The conventional methods, i.e., the fractional programming (FP) method and the weighted minimum mean square error (WMMSE) algorithm, can be computationally demanding for two reasons: (i) they require inverting a sequence of matrices whose sizes are proportional to the number of antennas; (ii) they require tuning a set of Lagrange multipliers to account for the power constraints. The recently proposed method called the reduced WMMSE addresses the above two issues for a single cell. In contrast, for the multicell case, another recent method called the FastFP eliminates the large matrix inversion and the Lagrange multipliers by using an improved FP technique, but the update stepsize in the FastFP can be difficult to decide. As such, we propose integrating the deep unfolding network into the FastFP for the stepsize optimization. Numerical experiments show that the proposed method is much more efficient than the learning method based on the WMMSE algorithm.

</details>


### [5] [Context-aware Privacy Bounds for Linear Queries](https://arxiv.org/abs/2601.02855)
*Heng Zhao,Sara Saeidian,Tobias J. Oechtering*

Main category: cs.IT

TL;DR: 该论文通过点态最大泄漏(PML)视角重新分析拉普拉斯机制的隐私保护，发现差分隐私(DP)框架的分布无关定义通常导致过度噪声添加。通过引入先验分布假设，作者推导出更紧致的上下文感知泄漏边界，能在保持隐私保证的同时减少所需噪声规模。


<details>
  <summary>Details</summary>
Motivation: 差分隐私(DP)作为最流行的隐私保护框架，采用与数据生成分布无关的上下文无关定义。这种定义方式往往要求添加过多噪声，导致分析结果精度下降。作者希望利用数据先验知识，在保证隐私的同时减少噪声添加。

Method: 作者从点态最大泄漏(PML)视角重新分析拉普拉斯机制，引入先验分布假设（限定单个记录属于特定类别的概率下界），推导出适用于一般线性查询的紧致、上下文感知泄漏边界，并证明该边界严格优于标准DP保证。

Result: 理论证明推导出的上下文感知泄漏边界严格优于标准DP保证，且当概率下界趋近于零时收敛到DP保证。数值评估表明，利用先验知识可以在保持隐私保证的同时显著减少所需噪声规模。

Conclusion: 通过结合先验分布知识，可以设计更高效的隐私保护机制，在保证隐私的同时减少噪声添加，提高线性查询结果的实用性。这为在实际应用中平衡隐私保护与数据分析精度提供了新思路。

Abstract: Linear queries, as the basis of broad analysis tasks, are often released through privacy mechanisms based on differential privacy (DP), the most popular framework for privacy protection. However, DP adopts a context-free definition that operates independently of the data-generating distribution. In this paper, we revisit the privacy analysis of the Laplace mechanism through the lens of pointwise maximal leakage (PML). We demonstrate that the distribution-agnostic definition of the DP framework often mandates excessive noise. To address this, we incorporate an assumption about the prior distribution by lower-bounding the probability of any single record belonging to any specific class. With this assumption, we derive a tight, context-aware leakage bound for general linear queries, and prove that our derived bound is strictly tighter than the standard DP guarantee and converges to the DP guarantee as this probability lower bound approaches zero. Numerical evaluations demonstrate that by exploiting this prior knowledge, the required noise scale can be reduced while maintaining privacy guarantees.

</details>


### [6] [Dualities for finite abelian groups and applications to coding theory](https://arxiv.org/abs/2601.03126)
*Jay A. Wood*

Main category: cs.IT

TL;DR: 研究有限阿贝尔群与其特征群之间对偶性的选择如何影响加法码的对偶码定义，延续Delsarte(1973)和Dougherty等人的工作


<details>
  <summary>Details</summary>
Motivation: 有限阿贝尔群与其特征群之间存在多种同构（对偶性）选择，不同的选择会导致不同的对偶码定义。本文旨在系统研究这些对偶性的性质及其对加法码对偶码的影响，深化对编码理论中这一基础问题的理解。

Method: 采用代数编码理论方法，研究有限阿贝尔群与其特征群之间的对偶性（同构）。分析不同对偶性选择如何定义加法码的对偶码，并系统研究这些对偶性和对偶码的数学性质。

Result: 建立了有限阿贝尔群对偶性的系统理论框架，揭示了不同对偶性选择对加法码对偶码性质的影响。可能发现了某些对偶性具有更好的代数性质，或者在某些应用场景下更为适用。

Conclusion: 有限阿贝尔群与其特征群之间的对偶性选择不是唯一的，这种选择会影响加法码对偶码的定义和性质。该研究为编码理论提供了更深入的理论基础，有助于设计更有效的编码方案。

Abstract: The choice of an isomorphism, a duality, between a finite abelian group $A$ and its character group allows one to define dual codes of additive codes over $A$. Properties of dualities and dual codes are studied, continuing work of Delsarte from 1973 and more recent work of Dougherty and his collaborators.

</details>


### [7] [On the Euclidean duals of the cyclic codes generated by cyclotomic polynomials](https://arxiv.org/abs/2601.03165)
*Anuj Kumar Bhagat,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 证明了循环码C_n的欧几里得对偶码的最小距离为2^ω(n)，其中ω(n)是n的不同素因子个数，验证了先前提出的猜想。


<details>
  <summary>Details</summary>
Motivation: 研究由第n个分圆多项式生成的循环码的欧几里得对偶码的最小距离特性，验证先前提出的猜想。

Method: 使用数论和编码理论方法，分析由分圆多项式生成的循环码的结构特性，推导其对偶码的最小距离。

Result: 证明了对于所有与q互质的正整数n，循环码C_n的欧几里得对偶码的最小距离为2^ω(n)，其中ω(n)是n的不同素因子个数。

Conclusion: 成功确定了由分圆多项式生成的循环码的欧几里得对偶码的最小距离公式，验证了先前提出的猜想，为这类码的纠错能力提供了理论保证。

Abstract: In this article, we determine the minimum distance of the Euclidean dual of the cyclic code $\mathcal{C}_n$ generated by the $n$th cyclotomic polynomial $Q_n(x)$ over $\mathbb{F}_q$, for every positive integer $n$ co-prime to $q$. In particular, we prove that the minimum distance of $\mathcal{C}_{n}^{\perp}$ is a function of $n$, namely $2^{ω(n)}$. This was precisely the conjecture posed by us in \cite{BHAGAT2025}.

</details>


### [8] [On the Capacity Region of Individual Key Rates in Vector Linear Secure Aggregation](https://arxiv.org/abs/2601.03241)
*Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文解决了向量线性安全聚合问题中最小个体密钥速率的开放问题，提出了新的可达速率区域，并证明并非所有用户都需要持有密钥。


<details>
  <summary>Details</summary>
Motivation: 解决Yuan-Sun在ISIT 2025提出的开放问题：确定向量线性安全聚合问题中每个用户所需的最小个体密钥速率。现有研究对密钥分配要求较高，需要探索更高效的密钥分配方案。

Method: 通过分析满足"秩增量条件"的索引集I，构造多面体可达速率区域。该区域的顶点由二进制速率分配(R1,...,RK) = (1(1∈I),...,1(K∈I))表征，其中I满足rank([F_I;G_I]) = rank(F_I) + N。

Result: 提出了严格大于现有最佳可达区域的新可达速率区域，揭示了并非所有用户都需要持有密钥的重要新事实。当最小化持有密钥的用户数量时，该方案是最优的。

Conclusion: 该研究解决了向量线性安全聚合中的关键开放问题，通过秩增量条件表征了最优密钥分配，显著改进了现有方案，为分布式安全计算提供了新的理论基础。

Abstract: We provide new insights into an open problem recently posed by Yuan-Sun [ISIT 2025], concerning the minimum individual key rate required in the vector linear secure aggregation problem. Consider a distributed system with $K$ users, where each user $k\in [K]$ holds a data stream $W_k$ and an individual key $Z_k$. A server aims to compute a linear function $\mathbf{F}[W_1;\ldots;W_K]$ without learning any information about another linear function $\mathbf{G}[W_1;\ldots;W_K]$, where $[W_1;\ldots;W_K]$ denotes the row stack of $W_1,\ldots,W_K$. The open problem is to determine the minimum required length of $Z_k$, denoted as $R_k$, $k\in [K]$. In this paper, we characterize a new achievable region for the rate tuple $(R_1,\ldots,R_K)$. The region is polyhedral, with vertices characterized by a binary rate assignment $(R_1,\ldots,R_K) = (\mathbf{1}(1 \in \mathcal{I}),\ldots,\mathbf{1}(K\in \mathcal{I}))$, where $\mathcal{I}\subseteq [K]$ satisfies the \textit{rank-increment condition}: $\mathrm{rank}\left(\bigl[\mathbf{F}_{\mathcal{I}};\mathbf{G}_{\mathcal{I}}\bigr]\right) =\mathrm{rank}\bigl(\mathbf{F}_{\mathcal{I}}\bigr)+N$. Here, $\mathbf{F}_\mathcal{I}$ and $\mathbf{G}_\mathcal{I}$ are the submatrices formed by the columns indexed by $\mathcal{I}$. Our results uncover the novel fact that it is not necessary for every user to hold a key, thereby strictly enlarging the best-known achievable region in the literature. Furthermore, we provide a converse analysis to demonstrate its optimality when minimizing the number of users that hold keys.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [9] [Listen to the Unexpected: Self-Supervised Surprise Detection for Efficient Viewport Prediction](https://arxiv.org/abs/2601.02629)
*Arman Nik Khah,Ravi Prakash*

Main category: cs.MM

TL;DR: 提出自学习框架检测音频"惊喜"事件，结合视觉线索提升360度视频视口预测，减少比特率浪费18%


<details>
  <summary>Details</summary>
Motivation: 当前360度视频自适应流媒体主要依赖视觉显著性或历史注视模式，忽略了空间音频在引导用户注意力方面的作用。音频中的"惊喜"事件（偏离时间预期的时刻）可能对预测用户视口有重要价值。

Method: 采用自学习框架检测音频惊喜事件，结合SE(3)-等变图神经网络和循环时间建模，通过双重自监督目标训练。关键特性是自然建模时间注意力衰减：事件开始时惊喜度高，随着听众适应而减弱。

Result: 在AVTrack360数据集上的实验表明，将音频惊喜与视觉线索结合，相比纯视觉方法可减少比特率浪费高达18%。

Conclusion: 空间音频中的惊喜事件是预测360度视频视口的重要线索，整合音频和视觉信息能显著提升自适应流媒体的带宽分配效率。

Abstract: Adaptive streaming of 360-degree video relies on viewport prediction to allocate bandwidth efficiently. Current approaches predominantly use visual saliency or historical gaze patterns, neglecting the role of spatial audio in guiding user attention. This paper presents a self-learning framework for detecting "surprising" auditory events -- moments that deviate from learned temporal expectations -- and demonstrates their utility for viewport prediction. The proposed architecture combines $SE(3)$-equivariant graph neural networks with recurrent temporal modeling, trained via a dual self-supervised objective. A key feature is the natural modeling of temporal attention decay: surprise is high at event onset but diminishes as the listener adapts. Experiments on the AVTrack360 dataset show that integrating audio surprise with visual cues reduces bitrate waste by up to 18% compared to visual-only methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [Case Count Metric for Comparative Analysis of Entity Resolution Results](https://arxiv.org/abs/2601.02824)
*John R. Talburt,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Onais Khan Mohammed,Mahboob Khan Mohammed,Khizer Syed,Leon Claasssens*

Main category: cs.DB

TL;DR: 开发了Case Count Metric System (CCMS)系统，用于在真实链接未知的情况下，系统比较和分析两个不同ER聚类过程在同一数据集上的结果，通过四种转换场景统计集群变化情况。


<details>
  <summary>Details</summary>
Motivation: 在实体解析(ER)中，当不知道真实链接关系时，需要一种系统方法来比较不同聚类过程的结果，以评估它们如何转换集群结构。

Method: 开发CCMS系统和软件，基于四种转换场景统计集群变化：保持不变、合并为更大集群、分割为更小集群、与多个集群重叠。系统可以分析模式显示详细变化信息。

Result: CCMS能够系统量化两个ER聚类过程之间的差异，提供详细的统计计数，并在大学和工业研究中得到应用验证。

Conclusion: CCMS为比较未知真实链接情况下的ER聚类结果提供了有效的系统方法，有助于评估聚类过程的质量和差异。

Abstract: This paper describes a new process and software system, the Case Count Metric System (CCMS), for systematically comparing and analyzing the outcomes of two different ER clustering processes acting on the same dataset when the true linking (labeling) is not known. The CCMS produces a set of counts that describe how the clusters produced by the first process are transformed by the second process based on four possible transformation scenarios. The transformations are that a cluster formed in the first process either remains unchanged, merges into a larger cluster, is partitioned into smaller clusters, or otherwise overlaps with multiple clusters formed in the second process. The CCMS produces a count for each of these cases, accounting for every cluster formed in the first process. In addition, when run in analysis mode, the CCMS program can assist the user in evaluating these changes by displaying the details for all changes or only for certain types of changes. The paper includes a detailed description of the CCMS process and program and examples of how the CCMS has been applied in university and industry research.

</details>


### [11] [Accurate Table Question Answering with Accessible LLMs](https://arxiv.org/abs/2601.03137)
*Yangfan Jiang,Fei Wei,Ergute Bao,Yaliang Li,Bolin Ding,Yin Yang,Xiaokui Xiao*

Main category: cs.DB

TL;DR: Orchestra：一种多智能体方法，通过协调多个小型开源LLM完成简单任务，解决复杂表格问答问题，降低使用成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 当前表格问答（TQA）任务依赖昂贵的大型专有LLM，存在显著经济障碍。小型开源LLM虽然成本低，但能力较弱，现有方法导致性能大幅下降

Method: 提出Orchestra多智能体方法，协调一组LLM智能体，每个负责相对简单的任务，通过结构化分层工作流程解决复杂TQA问题，降低每个智能体面临的提示复杂度

Result: 在多个TQA基准测试中，使用Qwen2.5-14B达到72.1%准确率（接近GPT-4的75.3%）；使用更大的Qwen、Llama或DeepSeek模型时，超越所有先前方法，在所有基准测试中建立新的最先进结果

Conclusion: Orchestra通过多智能体协作显著提升了小型开源LLM在表格问答任务中的性能，实现了高质量、经济高效的解决方案，为资源受限环境提供了可行方案

Abstract: Given a table T in a database and a question Q in natural language, the table question answering (TQA) task aims to return an accurate answer to Q based on the content of T. Recent state-of-the-art solutions leverage large language models (LLMs) to obtain high-quality answers. However, most rely on proprietary, large-scale LLMs with costly API access, posing a significant financial barrier. This paper instead focuses on TQA with smaller, open-weight LLMs that can run on a desktop or laptop. This setting is challenging, as such LLMs typically have weaker capabilities than large proprietary models, leading to substantial performance degradation with existing methods.
  We observe that a key reason for this degradation is that prior approaches often require the LLM to solve a highly sophisticated task using long, complex prompts, which exceed the capabilities of small open-weight LLMs. Motivated by this observation, we present Orchestra, a multi-agent approach that unlocks the potential of accessible LLMs for high-quality, cost-effective TQA. Orchestra coordinates a group of LLM agents, each responsible for a relatively simple task, through a structured, layered workflow to solve complex TQA problems -- akin to an orchestra. By reducing the prompt complexity faced by each agent, Orchestra significantly improves output reliability.
  We implement Orchestra on top of AgentScope, an open-source multi-agent framework, and evaluate it on multiple TQA benchmarks using a wide range of open-weight LLMs. Experimental results show that Orchestra achieves strong performance even with small- to medium-sized models. For example, with Qwen2.5-14B, Orchestra reaches 72.1% accuracy on WikiTQ, approaching the best prior result of 75.3% achieved with GPT-4; with larger Qwen, Llama, or DeepSeek models, Orchestra outperforms all prior methods and establishes new state-of-the-art results across all benchmarks.

</details>


### [12] [SpANNS: Optimizing Approximate Nearest Neighbor Search for Sparse Vectors Using Near Memory Processing](https://arxiv.org/abs/2601.03229)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.DB

TL;DR: SpANNS是一个基于CXL Type-2近内存处理架构的稀疏近似最近邻搜索系统，通过混合倒排索引和查询优化，相比CPU基线实现15.2-21.6倍的加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏近似最近邻搜索（ANNS）目前受限于CPU实现，而密集ANNS已有专用硬件加速。随着混合检索系统（结合稀疏和密集嵌入）成为信息检索标准，稀疏ANNS的性能瓶颈日益严重，需要硬件加速解决方案。

Method: 提出SpANNS近内存处理架构，基于CXL Type-2平台构建。采用混合倒排索引，专用控制器处理查询解析和聚类过滤，计算使能的DIMM在数据附近执行索引遍历和距离计算，结合高效的查询管理和运行时优化。

Result: SpANNS相比最先进的CPU基线实现了15.2倍到21.6倍的执行速度提升，为稀疏向量搜索提供了可扩展且高效的解决方案。

Conclusion: SpANNS通过近内存处理架构成功解决了稀疏ANNS的性能瓶颈，为混合检索系统中的稀疏向量搜索提供了硬件加速方案，显著提升了搜索效率和可扩展性。

Abstract: Approximate Nearest Neighbor Search (ANNS) is a fundamental operation in vector databases, enabling efficient similarity search in high-dimensional spaces. While dense ANNS has been optimized using specialized hardware accelerators, sparse ANNS remains limited by CPU-based implementations, hindering scalability. This limitation is increasingly critical as hybrid retrieval systems, combining sparse and dense embeddings, become standard in Information Retrieval (IR) pipelines. We propose SpANNS, a near-memory processing architecture for sparse ANNS. SpANNS combines a hybrid inverted index with efficient query management and runtime optimizations. The architecture is built on a CXL Type-2 near-memory platform, where a specialized controller manages query parsing and cluster filtering, while compute-enabled DIMMs perform index traversal and distance computations close to the data. It achieves 15.2x to 21.6x faster execution over the state-of-the-art CPU baselines, offering scalable and efficient solutions for sparse vector search.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [13] [GCRank: A Generative Contextual Comprehension Paradigm for Takeout Ranking Model](https://arxiv.org/abs/2601.02361)
*Ziheng Ni,Congcong Liu,Cai Shang,Yiming Sun,Junjie Li,Zhiwei Fang,Guangpeng Chen,Jian Li,Zehua Zhang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.IR

TL;DR: 提出生成式上下文理解框架，通过统一架构建模异构信号，解决广告排序中用户意图理解不足的问题，在食品配送广告平台取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有广告排序模型依赖碎片化模块和手工特征，难以理解复杂用户意图，特别是在食品配送等位置服务中，用户决策受动态空间、时间和个体上下文影响。

Method: 提出生成式框架，将排序重构为上下文理解任务，包含生成式上下文编码器(GCE)和生成式上下文融合(GCF)。GCE包含个性化上下文增强器(PCE)、集体上下文增强器(CCE)和动态上下文增强器(DCE)，GCF通过低秩适配无缝集成这些表示。

Result: 实验证明该方法在关键业务指标上取得显著提升，包括点击率和平台收入，已成功部署在大型食品配送广告平台，展示了实际应用价值。

Conclusion: 该工作开创了生成式推荐的新视角，突出了其在工业广告系统中的实际潜力，通过统一架构建模异构信号，有效解决了广告排序中的上下文理解挑战。

Abstract: The ranking stage serves as the central optimization and allocation hub in advertising systems, governing economic value distribution through eCPM and orchestrating the user-centric blending of organic and advertising content. Prevailing ranking models often rely on fragmented modules and hand-crafted features, limiting their ability to interpret complex user intent. This challenge is further amplified in location-based services such as food delivery, where user decisions are shaped by dynamic spatial, temporal, and individual contexts. To address these limitations, we propose a novel generative framework that reframes ranking as a context comprehension task, modeling heterogeneous signals in a unified architecture. Our architecture consists of two core components: the Generative Contextual Encoder (GCE) and the Generative Contextual Fusion (GCF). The GCE comprises three specialized modules: a Personalized Context Enhancer (PCE) for user-specific modeling, a Collective Context Enhancer (CCE) for group-level patterns, and a Dynamic Context Enhancer (DCE) for real-time situational adaptation. The GCF module then seamlessly integrates these contextual representations through low-rank adaptation. Extensive experiments confirm that our method achieves significant gains in critical business metrics, including click-through rate and platform revenue. We have successfully deployed our method on a large-scale food delivery advertising platform, demonstrating its substantial practical impact. This work pioneers a new perspective on generative recommendation and highlights its practical potential in industrial advertising systems.

</details>


### [14] [The Impact of LLM-Generated Reviews on Recommender Systems: Textual Shifts, Performance Effects, and Strategic Platform Control](https://arxiv.org/abs/2601.02362)
*Itzhak Ziv,Moshe Unger,Hilah Geva*

Main category: cs.IR

TL;DR: 研究AI生成评论对推荐系统的影响，发现人类评论质量更高，平台控制AI内容生成策略很重要


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，推荐系统面临AI生成内容与人类创作内容并存的新环境，需要研究AI内容如何影响推荐系统性能和商业结果

Method: 使用TripAdvisor酒店评论数据集，通过LLM生成合成评论，分析用户中心（用户用AI优化评论）和平台中心（平台从结构化元数据生成评论）两种AI内容引入路径，评估其对推荐系统训练和部署阶段的影响

Result: AI生成评论在多个文本维度上与人类评论存在系统性差异；虽然AI评论相比无文本数据的模型能提升推荐性能，但人类评论训练的模型表现最佳；人类训练模型能很好泛化到AI内容，而AI训练模型在两种内容上都表现不佳；基于语气的框架策略（鼓励性、建设性、批判性）能显著提升平台生成评论的效果

Conclusion: 平台在控制AI生成评论的生成和整合方面具有战略重要性，需要确保合成内容能够补充推荐系统的鲁棒性和可持续商业价值

Abstract: The rise of generative AI technologies is reshaping content-based recommender systems (RSes), which increasingly encounter AI-generated content alongside human-authored content. This study examines how the introduction of AI-generated reviews influences RS performance and business outcomes. We analyze two distinct pathways through which AI content can enter RSes: user-centric, in which individuals use AI tools to refine their reviews, and platform-centric, in which platforms generate synthetic reviews directly from structured metadata. Using a large-scale dataset of hotel reviews from TripAdvisor, we generate synthetic reviews using LLMs and evaluate their impact across the training and deployment phases of RSes. We find that AI-generated reviews differ systematically from human-authored reviews across multiple textual dimensions. Although both user- and platform-centric AI reviews enhance RS performance relative to models without textual data, models trained on human reviews consistently achieve superior performance, underscoring the quality of authentic human data. Human-trained models generalize robustly to AI content, whereas AI-trained models underperform on both content types. Furthermore, tone-based framing strategies (encouraging, constructive, or critical) substantially enhance platform-generated review effectiveness. Our findings highlight the strategic importance of platform control in governing the generation and integration of AI-generated reviews, ensuring that synthetic content complements recommendation robustness and sustainable business value.

</details>


### [15] [Towards Trustworthy LLM-Based Recommendation via Rationale Integration](https://arxiv.org/abs/2601.02364)
*Chung Park,Taesan Kim,Hyeongjun Yun,Dongjoon Hong,Junui Hong,Kijung Park,MinCheol Cho,Mira Myong,Jihoon Oh,Min sung Choi*

Main category: cs.IR

TL;DR: 提出基于LLM的推荐系统LLM-Rec，通过生成逻辑合理的推荐理由来提升透明度和推荐性能，采用理由优先的指令调优策略


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统过度关注准确性和短期参与度，忽视了透明度和可信度。虽然亚马逊和Instagram等平台开始提供推荐理由，但现有系统大多将其视为事后产物，缺乏与推荐过程的深度融合

Method: 提出LLM-Rec系统，采用自标注理由数据集和理由优先的指令调优格式，模型先生成解释再输出推荐项目，并使用思维链风格表示理由

Result: 在Amazon Review数据集的时尚和科学领域实验中，相比现有基线方法取得了显著改进，同时公开发布了包含用户历史、理由和推荐项目的增强数据集

Conclusion: LLM-Rec通过生成逻辑合理的推荐理由，同时增强了推荐系统的可解释性和推荐性能，为可解释推荐系统研究提供了新方向

Abstract: Traditional recommender systems (RS) have been primarily optimized for accuracy and short-term engagement, often overlooking transparency and trustworthiness. Recently, platforms such as Amazon and Instagram have begun providing recommendation rationales to users, acknowledging their critical role in fostering trust and enhancing engagement; however, most existing systems still treat them as post-hoc artifacts. We propose an LLM-based recommender (LLM-Rec) that not only predicts items but also generates logically grounded rationales. Our approach leverages a self-annotated rationale dataset and instruction tuning in a rationale-first format, where the model generates an explanation before outputting the recommended item. By adopting this strategy and representing rationales in a chain-of-thought (CoT) style, LLM-Rec strengthens both interpretability and recommendation performance. Experiments on the Fashion and Scientific domains of the Amazon Review dataset demonstrate significant improvements over well-established baselines. To encourage reproducibility and future research, we publicly release a rationale-augmented recommendation dataset containing user histories, rationales, and recommended items.

</details>


### [16] [FUSE : Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation](https://arxiv.org/abs/2601.02365)
*Tushar Vatsa,Vibha Belavadi,Priya Shanmugasundaram,Suhas Suresha,Dewang Sultania*

Main category: cs.IR

TL;DR: FUSE提出了一种基于失败感知的多模态搜索推荐系统，通过紧凑的GDR表示和七种上下文预算策略优化检索质量，其中上下文压缩策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多模态创意助手在检索过程中存在多个失败点（意图理解、内容类型选择、候选召回、结果排序），同时原始图像处理成本高昂，需要更高效的解决方案。

Method: FUSE系统使用紧凑的GDR表示替代原始图像，实现七种上下文预算策略（基线提示、上下文压缩、思维链推理、小样本优化、检索增强上下文、两阶段处理、零样本最小化），并建立管道归因层监控系统性能。

Result: 在788个评估查询上，上下文压缩策略在所有管道阶段表现最优：意图准确率93.3%，路由成功率86.8%（含回退），召回率99.4%，NDCG@5为88.5%。

Conclusion: 策略性的上下文总结优于全面和最小化的上下文策略，FUSE通过紧凑表示和智能上下文管理有效解决了多模态检索中的失败点和成本问题。

Abstract: Multimodal creative assistants decompose user goals and route tasks to subagents for layout, styling, retrieval, and generation. Retrieval quality is pivotal, yet failures can arise at several stages: understanding user intent, choosing content types, finding candidates (recall), or ranking results. Meanwhile, sending and processing images is costly, making naive multimodal approaches impractical. We present FUSE: Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation. FUSE replaces most raw-image prompting with a compact Grounded Design Representation (GDR): a selection aware JSON of canvas elements (image, text, shape, icon, video, logo), structure, styles, salient colors, and user selection provided by the Planner team. FUSE implements seven context budgeting strategies: comprehensive baseline prompting, context compression, chain-of-thought reasoning, mini-shot optimization, retrieval-augmented context, two-stage processing, and zero-shot minimalism. Finally, a pipeline attribution layer monitors system performance by converting subagent signals into simple checks: intent alignment, content-type/routing sanity, recall health (e.g., zero-hit and top-match strength), and ranking displacement analysis. We evaluate the seven context budgeting variants across 788 evaluation queries from diverse users and design templates (refer Figure 3). Our systematic evaluation reveals that Context Compression achieves optimal performance across all pipeline stages, with 93.3% intent accuracy, 86.8% routing success(with fallbacks), 99.4% recall, and 88.5% NDCG@5. This approach demonstrates that strategic context summarization outperforms both comprehensive and minimal contextualization strategies.

</details>


### [17] [TextBridgeGNN: Pre-training Graph Neural Network for Cross-Domain Recommendation via Text-Guided Transfer](https://arxiv.org/abs/2601.02366)
*Yiwen Chen,Yiqing Wu,Huishi Luo,Fuzhen Zhuang,Deqing Wang*

Main category: cs.IR

TL;DR: TextBridgeGNN：一个使用文本作为语义桥梁的图推荐预训练框架，通过多级图传播连接不同领域，解决ID嵌入不可迁移和异构图结构不兼容问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于ID嵌入的图推荐模型难以迁移到新领域，主要面临两个挑战：1) ID嵌入因领域特定ID空间隔离而不可迁移；2) 跨领域异构交互图的结构不兼容。需要构建可预训练的图推荐模型。

Method: 提出TextBridgeGNN框架：1) 使用文本作为语义桥梁连接不同领域；2) 预训练阶段利用文本信息打破数据孤岛，设计分层GNN学习领域特定和全局知识；3) 微调阶段提出相似性迁移机制，通过语义相关节点初始化目标域ID嵌入。

Result: 实验表明TextBridgeGNN在跨领域、多领域和无训练设置下优于现有方法，能够有效整合预训练语言模型的语义与基于图的协同过滤，无需昂贵的语言模型微调或实时推理开销。

Conclusion: TextBridgeGNN成功解决了图推荐模型的迁移问题，通过文本语义桥梁实现了知识在不同领域间的有效传递，为构建可预训练的图推荐模型提供了有效解决方案。

Abstract: Graph-based recommendation has achieved great success in recent years. The classical graph recommendation model utilizes ID embedding to store essential collaborative information. However, this ID-based paradigm faces challenges in transferring to a new domain, making it hard to build a pre-trained graph recommendation model. This phenomenon primarily stems from two inherent challenges: (1) the non-transferability of ID embeddings due to isolated domain-specific ID spaces, and (2) structural incompatibility between heterogeneous interaction graphs across domains.
  To address these issues, we propose TextBridgeGNN, a pre-training and fine-tuning framework that can effectively transfer knowledge from a pre-trained GNN to downstream tasks. We believe the key lies in how to build the relationship between domains. Specifically, TextBridgeGNN uses text as a semantic bridge to connect domains through multi-level graph propagation. During the pre-training stage, textual information is utilized to break the data islands formed by multiple domains, and hierarchical GNNs are designed to learn both domain-specific and domain-global knowledge with text features, ensuring the retention of collaborative signals and the enhancement of semantics. During the fine-tuning stage, a similarity transfer mechanism is proposed. This mechanism initializes ID embeddings in the target domain by transferring from semantically related nodes, successfully transferring the ID embeddings and graph pattern.
  Experiments demonstrate that TextBridgeGNN outperforms existing methods in cross-domain, multi-domain, and training-free settings, highlighting its ability to integrate Pre-trained Language Model (PLM)-driven semantics with graph-based collaborative filtering without costly language model fine-tuning or real-time inference overhead.

</details>


### [18] [Distillation-based Scenario-Adaptive Mixture-of-Experts for the Matching Stage of Multi-scenario Recommendation](https://arxiv.org/abs/2601.02368)
*Ruibing Wang,Shuhan Guo,Haotong Du,Quanming Yao*

Main category: cs.IR

TL;DR: DSMOE通过场景自适应投影模块和跨架构知识蒸馏，解决多场景推荐匹配阶段中MMOE的专家崩溃和头部场景参数主导问题，显著提升长尾场景检索质量。


<details>
  <summary>Details</summary>
Motivation: 多场景推荐中，MMOE在排序阶段表现良好，但在匹配阶段面临两个主要问题：独立双塔架构的盲目优化，以及头部场景参数主导导致长尾场景表现不佳。

Method: 提出DSMOE框架，包含两个核心组件：1) 场景自适应投影模块(SAP)，生成轻量级、场景特定的参数，防止专家崩溃；2) 跨架构知识蒸馏框架，使用交互感知的教师模型指导双塔学生模型学习复杂匹配模式。

Result: 在真实数据集上的大量实验表明，DSMOE在多场景推荐匹配阶段表现优异，特别是在数据稀疏、代表性不足的长尾场景中显著提升了检索质量。

Conclusion: DSMOE通过解决多场景推荐匹配阶段的结构和分布瓶颈，有效提升了整体推荐性能，特别是在长尾场景中表现出色，为多场景推荐系统提供了有效的解决方案。

Abstract: Multi-scenario recommendation is pivotal for optimizing user experience across diverse contexts. While Multi-gate Mixture-of-Experts (MMOE) thrives in ranking, its transfer to the matching stage is hindered by the blind optimization inherent to independent two-tower architectures and the parameter dominance of head scenarios. To address these structural and distributional bottlenecks, we propose Distillation-based Scenario-Adaptive Mixture-of-Experts (DSMOE). Specially, we devise a Scenario-Adaptive Projection (SAP) module to generate lightweight, context-specific parameters, effectively preventing expert collapse in long-tail scenarios. Concurrently, we introduce a cross-architecture knowledge distillation framework, where an interaction-aware teacher guides the two-tower student to capture complex matching patterns. Extensive experiments on real-world datasets demonstrate DSMOE's superiority, particularly in significantly improving retrieval quality for under-represented, data-sparse scenarios.

</details>


### [19] [Improving News Recommendations through Hybrid Sentiment Modelling and Reinforcement Learning](https://arxiv.org/abs/2601.02372)
*Eunice Kingenga,Mike Wa Nkongolo*

Main category: cs.IR

TL;DR: 该研究提出了一种结合混合情感分析和强化学习的自适应情感感知新闻推荐框架，通过Q-learning优化推荐策略，提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统新闻推荐系统在情感分析方面存在局限性：处理歧义性差、词典不一致、上下文理解有限，特别是在多源新闻环境中。现有模型通常将情感作为次要特征，难以适应用户的情感偏好。

Method: 1. 使用BBC News数据集
2. 构建混合情感分析模型：结合VADER、AFINN、TextBlob和SentiWordNet四种工具生成文章级情感评分
3. 将文章分类为积极、消极或中性
4. 将情感状态嵌入Q-learning架构中，让智能体学习最优推荐策略
5. 通过迭代Q-learning更新持续改进个性化推荐

Result: 提出的系统能够有效识别并推荐情感对齐的文章，通过混合情感建模与强化学习的结合，为以用户为中心的新闻推荐提供了可行、可解释且自适应的解决方案。

Conclusion: 结合混合情感分析和强化学习的方法为新闻推荐系统提供了一种有效的情感感知框架，能够更好地适应用户的情感偏好，提升个性化推荐质量。

Abstract: News recommendation systems rely on automated sentiment analysis to personalise content and enhance user engagement. Conventional approaches often struggle with ambiguity, lexicon inconsistencies, and limited contextual understanding, particularly in multi-source news environments. Existing models typically treat sentiment as a secondary feature, reducing their ability to adapt to users' affective preferences. To address these limitations, this study develops an adaptive, sentiment-aware news recommendation framework by integrating hybrid sentiment analysis with reinforcement learning. Using the BBC News dataset, a hybrid sentiment model combines VADER, AFINN, TextBlob, and SentiWordNet scores to generate robust article-level sentiment estimates. Articles are categorised as positive, negative, or neutral, and these sentiment states are embedded within a Q-learning architecture to guide the agent in learning optimal recommendation policies. The proposed system effectively identifies and recommends articles with aligned emotional profiles while continuously improving personalisation through iterative Q-learning updates. The results demonstrate that coupling hybrid sentiment modelling with reinforcement learning provides a feasible, interpretable, and adaptive approach for user-centred news recommendation.

</details>


### [20] [A Lay User Explainable Food Recommendation System Based on Hybrid Feature Importance Extraction and Large Language Models](https://arxiv.org/abs/2601.02374)
*Melissa Tessa,Diderot D. Cidjeu,Rachele Carli,Sarah Abchiche,Ahmad Aldarwishd,Igor Tchappi,Amro Najjar*

Main category: cs.IR

TL;DR: 使用LLM结合SHAP为食品推荐系统生成更详细、动态且易于理解的后处理解释，提升用户信任和透明度


<details>
  <summary>Details</summary>
Motivation: 现有食品推荐系统的解释通常不够详细或难以理解，特别是对于普通用户。需要一种方法能够提供更全面、动态且易于理解的解释，以增强用户对推荐结果的信任和系统透明度。

Method: 提出一种后处理方法，结合大型语言模型(LLM)和SHAP（SHapley Additive exPlanations）的混合关键变量提取技术。通过LLM生成动态、有说服力的解释，同时利用SHAP识别影响推荐结果的关键变量。

Result: 相比文献中的现有方法，该方法能为普通用户提供更全面、动态且令人信服的解释。这些解释使复杂的推荐结果更容易理解，从而增强了用户信任和系统透明度。

Conclusion: LLM与SHAP的结合为食品推荐系统提供了一种有效的后处理解释方法，能够生成更详细、易于理解的解释，显著提升用户体验和系统可信度。

Abstract: Large Language Models (LLM) have experienced strong development in recent years, with varied applications. This paper uses LLMs to develop a post-hoc process that provides more elaborated explanations of the results of food recommendation systems. By combining LLM with a hybrid extraction of key variables using SHAP, we obtain dynamic, convincing and more comprehensive explanations to lay user, compared to those in the literature. This approach enhances user trust and transparency by making complex recommendation outcomes easier to understand for a lay user.

</details>


### [21] [TAG-HGT: A Scalable and Cost-Effective Framework for Inductive Cold-Start Academic Recommendation](https://arxiv.org/abs/2601.02381)
*Zhexiang Li*

Main category: cs.IR

TL;DR: TAG-HGT是一个神经符号框架，通过解耦"语义优先、结构优化"范式，将冻结大语言模型的语义知识蒸馏到轻量级异构图Transformer中，解决了学术平台冷启动推荐中生成模型推理延迟高、成本大的问题。


<details>
  <summary>Details</summary>
Motivation: 工业学术平台面临冷启动推荐挑战，每天有大量新学者加入但无历史交互记录。现有生成图模型虽然语义能力强，但推理延迟高（13分钟/1000请求）、计算成本大，无法满足实时、百万级应用需求。

Method: 采用解耦的"语义优先、结构优化"范式：1) 使用冻结的DeepSeek-V3作为离线语义工厂；2) 通过跨视图对比学习将LLM知识蒸馏到轻量级异构图Transformer中；3) 结合LLM的全局召回能力和结构信号的局部区分能力。

Result: 在OpenAlex数据集上，TAG-HGT达到SOTA System Recall@10为91.97%，比纯结构基线提升20.7%。推理延迟降低5个数量级（从780秒降至1.73毫秒），推理成本从1.50美元降至<0.001美元/1000查询，成本降低99.9%。

Conclusion: TAG-HGT成功平衡了生成质量和工业可扩展性，通过神经符号框架实现了高精度学术推荐，大幅降低了推理延迟和成本，使高质量学术推荐更加民主化。

Abstract: Inductive cold-start recommendation remains the "Achilles' Heel" of industrial academic platforms, where thousands of new scholars join daily without historical interaction records. While recent Generative Graph Models (e.g., HiGPT, OFA) demonstrate promising semantic capabilities, their prohibitive inference latency (often exceeding 13 minutes per 1,000 requests) and massive computational costs render them practically undeployable for real-time, million-scale applications. To bridge this gap between generative quality and industrial scalability, we propose TAG-HGT, a cost-effective neuro-symbolic framework. Adopting a decoupled "Semantics-First, Structure-Refined" paradigm, TAG-HGT utilizes a frozen Large Language Model (DeepSeek-V3) as an offline semantic factory and distills its knowledge into a lightweight Heterogeneous Graph Transformer (HGT) via Cross-View Contrastive Learning (CVCL). We present a key insight: while LLM semantics provide necessary global recall, structural signals offer the critical local discrimination needed to distinguish valid collaborators from semantically similar but socially unreachable strangers in dense embedding spaces. Validated under a strict Time-Machine Protocol on the massive OpenAlex dataset, TAG-HGT achieves a SOTA System Recall@10 of 91.97%, outperforming structure-only baselines by 20.7%. Most significantly, from an industrial perspective, TAG-HGT reduces inference latency by five orders of magnitude ($4.5 \times 10^{5}\times$) compared to generative baselines (from 780s down to 1.73 ms), and slashes inference costs from $\sim$$1.50 to $<$$0.001 per 1k queries. This 99.9% cost reduction democratizes high-precision academic recommendation.

</details>


### [22] [Tree of Preferences for Diversified Recommendation](https://arxiv.org/abs/2601.02386)
*Hanyang Yuan,Ning Tang,Tongya Zheng,Jiarong Xu,Xintong Hu,Renhong Huang,Shunyu Liu,Jiacong Hu,Jiawei Chen,Mingli Song*

Main category: cs.IR

TL;DR: 本文提出一种利用大语言模型从数据偏差角度解决推荐系统多样性问题的新方法，通过构建偏好树揭示用户未充分探索的兴趣，并生成合成交互数据来训练推荐模型。


<details>
  <summary>Details</summary>
Motivation: 现有多样化推荐方法主要从观测到的用户反馈推断用户偏好多样性，但由于数据偏差的存在，观测数据可能无法完全反映用户兴趣，导致未充分探索的偏好被淹没或未显现，从而造成推荐多样性不足。

Method: 1. 提出偏好树结构，从粗到细建模用户偏好，使LLM能够系统推理用户行为背后的逻辑；2. 采用数据为中心的方法，识别匹配用户偏好的候选物品并生成反映未充分探索偏好的合成交互；3. 将这些交互整合训练通用推荐器实现多样化；4. 通过动态选择有影响力的用户优化整体效率。

Result: 在多样性和相关性方面的广泛评估表明，该方法在大多数情况下优于现有方法，在其他情况下达到接近最优的性能，且具有合理的推理延迟。

Conclusion: 从数据偏差角度研究多样化推荐是有效的，利用LLM的世界知识和零样本推理能力可以揭示用户未充分探索的偏好，从而提供更相关且多样化的推荐。

Abstract: Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items. Existing approaches predominantly aim to infer the diversity of user preferences from observed user feedback. Nonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap, this work aims to study diversified recommendation from a data-bias perspective. Inspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations. To achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences. To guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences. These interactions are integrated to train a general recommender for diversification. Moreover, we scale up overall efficiency by dynamically selecting influential users during optimization. Extensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency.

</details>


### [23] [Socially-Aware Recommender Systems Mitigate Opinion Clusterization](https://arxiv.org/abs/2601.02412)
*Lukas Schüepp,Carmen Amo Alonso,Florian Dörfler,Giulia De Pasquale*

Main category: cs.IR

TL;DR: 该论文提出了一种考虑社交网络拓扑结构的推荐系统，通过利用用户社交关系来平衡个性化与内容多样性，从而缓解过滤气泡和意见极化问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统、内容创作者和用户之间形成复杂的反馈循环：推荐系统影响用户偏好，用户偏好又影响创作者内容，创作者内容再影响推荐结果。这种循环导致过滤气泡和意见极化问题。现有推荐系统未能充分考虑用户社交网络在缓解这些问题中的作用。

Method: 开发了一种社交网络感知的推荐系统，明确考虑用户-创作者反馈互动，并战略性地利用用户自身社交网络的拓扑结构来促进内容多样化。该方法将社交网络信息整合到推荐算法设计中。

Result: 理论上证明了意见聚类与推荐内容对用户意见的影响力呈正相关。提出的方法展示了社交感知推荐系统在对抗意见极化和聚类现象方面的有效性，能够平衡内容多样性与个性化。

Conclusion: 在推荐系统设计中考虑并利用用户社交网络对于缓解过滤气泡效应至关重要。社交感知的推荐系统能够有效平衡内容多样性与个性化，对抗意见极化和聚类现象，为构建更健康的在线生态系统提供了重要思路。

Abstract: Recommender systems shape online interactions by matching users with creators content to maximize engagement. Creators, in turn, adapt their content to align with users preferences and enhance their popularity. At the same time, users preferences evolve under the influence of both suggested content from the recommender system and content shared within their social circles. This feedback loop generates a complex interplay between users, creators, and recommender algorithms, which is the key cause of filter bubbles and opinion polarization. We develop a social network-aware recommender system that explicitly accounts for this user-creators feedback interaction and strategically exploits the topology of the user's own social network to promote diversification. Our approach highlights how accounting for and exploiting user's social network in the recommender system design is crucial to mediate filter bubble effects while balancing content diversity with personalization. Provably, opinion clusterization is positively correlated with the influence of recommended content on user opinions. Ultimately, the proposed approach shows the power of socially-aware recommender systems in combating opinion polarization and clusterization phenomena.

</details>


### [24] [A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance](https://arxiv.org/abs/2601.02428)
*Okan Bursa*

Main category: cs.IR

TL;DR: ARM是一种动态记忆RAG框架，通过选择性记忆和遗忘机制替代静态向量索引，实现高效检索增强生成。


<details>
  <summary>Details</summary>
Motivation: 传统RAG使用静态向量索引存在效率问题，需要更智能的记忆管理机制来平衡检索质量、延迟和内存效率。

Method: 采用动态记忆基板，基于认知巩固和遗忘原理：频繁检索的项目被巩固保护，很少使用的项目逐渐衰减。还实现了动态选择性检索策略和可配置的嵌入权重优化。

Result: 在轻量级检索基准测试中，ARM仅用约2200万参数达到接近SOTA性能（NDCG@5≈0.940，Recall@5=1.000）。Llama 3.1+静态RAG获得最高关键词覆盖率（67.2%），GPT-4o+动态策略获得最快响应（平均8.2秒）。

Conclusion: ARM在质量、延迟和内存效率之间提供了实用的权衡，具有竞争性准确性、自调节内存增长和可解释的保留动态，无需重新训练生成器。

Abstract: We introduce \emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\approx$ 0.940, Recall@5 $=1.000$) with only $\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings.
  ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.

</details>


### [25] [CREAM: Continual Retrieval on Dynamic Streaming Corpora with Adaptive Soft Memory](https://arxiv.org/abs/2601.02708)
*HuiJeong Son,Hyeongu Kang,Sunho Kim,Subeen Ho,SeongKu Kang,Dongha Lee,Susik Yoon*

Main category: cs.IR

TL;DR: CREAM是一个用于动态数据流信息检索的自监督持续学习框架，通过软记忆结构适应已见和未见主题，无需真实标签。


<details>
  <summary>Details</summary>
Motivation: 动态数据流中的分布漂移会降低AI信息检索系统的性能。现有基于记忆的持续学习方法依赖固定查询集和真实相关文档，限制了泛化能力，无法适应现实世界未见查询和文档。

Method: 提出CREAM自监督框架，通过细粒度相似性估计、正则化聚类原型和分层核心集采样三种关键技术，将流式查询和文档的演化语义捕获到动态结构化的软记忆中。

Result: 在两个基准数据集上，CREAM在无标签设置下比最强方法平均提升27.79%的Success@5和44.5%的Recall@10，性能达到甚至超过监督方法。

Conclusion: CREAM通过自监督方式有效解决了动态数据流信息检索中的分布漂移问题，能够适应未见主题，为现实应用提供了更实用的解决方案。

Abstract: Information retrieval (IR) in dynamic data streams is emerging as a challenging task, as shifts in data distribution degrade the performance of AI-powered IR systems. To mitigate this issue, memory-based continual learning has been widely adopted for IR. However, existing methods rely on a fixed set of queries with ground-truth relevant documents, which limits generalization to unseen queries and documents, making them impractical for real-world applications. To enable more effective learning with unseen topics of a new corpus without ground-truth labels, we propose CREAM, a self-supervised framework for memory-based continual retrieval. CREAM captures the evolving semantics of streaming queries and documents into dynamically structured soft memory and leverages it to adapt to both seen and unseen topics in an unsupervised setting. We realize this through three key techniques: fine-grained similarity estimation, regularized cluster prototyping, and stratified coreset sampling. Experiments on two benchmark datasets demonstrate that CREAM exhibits superior adaptability and retrieval accuracy, outperforming the strongest method in a label-free setting by 27.79\% in Success@5 and 44.5\% in Recall@10 on average, and achieving performance comparable to or even exceeding that of supervised methods.

</details>


### [26] [Ahead of the Spread: Agent-Driven Virtual Propagation for Early Fake News Detection](https://arxiv.org/abs/2601.02750)
*Bincheng Gu,Min Gao,Junliang Yu,Zongwei Wang,Zhiyi Liu,Kai Shu,Hongyu Zhang*

Main category: cs.IR

TL;DR: AVOID：基于LLM智能体驱动虚拟传播的早期假新闻检测新范式，通过主动模拟而非被动观察传播信号来生成证据，显著提升早期检测性能。


<details>
  <summary>Details</summary>
Motivation: 早期假新闻检测面临挑战，因为传播初期缺乏可观测的传播信号。传统方法依赖内容分析或被动等待传播数据，无法在早期有效检测。需要一种能在传播早期主动生成传播证据的方法。

Method: 提出AVOID框架：1）使用LLM驱动的智能体，赋予不同角色和数据驱动的人格特征；2）主动模拟早期传播行为，无需真实传播数据；3）生成虚拟传播轨迹作为补充社交证据；4）采用去噪引导的融合策略，将模拟传播与内容语义对齐。

Result: 在基准数据集上的广泛实验表明，AVOID始终优于最先进的基线方法，证明了虚拟传播增强对早期假新闻检测的有效性和实用价值。

Conclusion: AVOID将早期检测重新定义为证据生成的新范式，通过主动模拟传播信号克服了早期阶段缺乏观测数据的限制，为早期假新闻检测提供了创新且有效的解决方案。

Abstract: Early detection of fake news is critical for mitigating its rapid dissemination on social media, which can severely undermine public trust and social stability. Recent advancements show that incorporating propagation dynamics can significantly enhance detection performance compared to previous content-only approaches. However, this remains challenging at early stages due to the absence of observable propagation signals. To address this limitation, we propose AVOID, an \underline{a}gent-driven \underline{v}irtual pr\underline{o}pagat\underline{i}on for early fake news \underline{d}etection. AVOID reformulates early detection as a new paradigm of evidence generation, where propagation signals are actively simulated rather than passively observed. Leveraging LLM-powered agents with differentiated roles and data-driven personas, AVOID realistically constructs early-stage diffusion behaviors without requiring real propagation data. The resulting virtual trajectories provide complementary social evidence that enriches content-based detection, while a denoising-guided fusion strategy aligns simulated propagation with content semantics. Extensive experiments on benchmark datasets demonstrate that AVOID consistently outperforms state-of-the-art baselines, highlighting the effectiveness and practical value of virtual propagation augmentation for early fake news detection. The code and data are available at https://github.com/Ironychen/AVOID.

</details>


### [27] [Netflix Artwork Personalization via LLM Post-training](https://arxiv.org/abs/2601.02764)
*Hyunji Nam,Sejoon Oh,Emma Kong,Yesu Feng,Moumita Bhattacharya*

Main category: cs.IR

TL;DR: 本文提出使用LLM进行个性化艺术品推荐，针对用户异质性选择最适合用户偏好的标题视觉表示，相比Netflix生产模型提升3-5%


<details>
  <summary>Details</summary>
Motivation: 在Netflix等娱乐平台上，用户偏好多样，同一标题的不同艺术品（视觉表示）对不同用户吸引力不同。现有的一刀切方法无法满足个性化需求，需要根据用户偏好推荐最合适的艺术品来提升用户满意度和参与度。

Method: 对预训练LLM进行后训练，使其能够根据用户偏好为每个用户选择最合适的标题视觉表示。使用Llama 3.1 8B模型，在11万数据点上训练，在5千个保留的用户-标题对上评估。

Result: 后训练的LLM相比Netflix生产模型取得了3-5%的性能提升，证明了使用LLM进行细粒度个性化推荐的可行性。

Conclusion: LLM在个性化艺术品推荐方面具有潜力，能够根据用户异质性提供更精准的视觉表示选择，为细粒度个性化推荐开辟了有前景的方向。

Abstract: Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.

</details>


### [28] [COFFEE: COdesign Framework for Feature Enriched Embeddings in Ads-Ranking Systems](https://arxiv.org/abs/2601.02807)
*Sohini Roychowdhury,Doris Wang,Qian Ge,Joy Mu,Srihari Reddy*

Main category: cs.IR

TL;DR: 提出一个三维框架来增强广告推荐系统中的用户-广告表示，通过整合多源事件、延长用户历史、丰富事件属性和多模态嵌入，显著提升模型性能而不增加推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 商业广告推荐模型需要多样化和丰富的数据源来准确评估用户兴趣。虽然扩展的用户参与历史可以改善用户兴趣预测，但同样重要的是嵌入来自多个来源的活动序列，以确保用户和广告表示的新鲜度，遵循扩展定律原则。

Method: 提出一个新颖的三维框架：第一维度考察整合不同事件来源的影响；第二维度考虑更长用户历史的好处；第三维度专注于用额外事件属性和多模态嵌入来丰富数据。通过比较有机用户参与来源（如内容浏览）与广告展示来源来评估投资回报率。

Result: 该方法可以将广告展示来源的AUC和扩展曲线斜率提升1.56到2倍（相比有机使用来源），即使在线序列长度仅为100到10K。使用丰富的广告展示事件来源时，CTR预测的AUC比基线生产广告推荐系统提高0.56%，并为更长和离线的用户-广告表示改进了序列扩展分辨率。

Conclusion: 提出的三维源丰富框架能有效增强用户-广告表示，显著提升广告推荐性能，同时不增加模型推理或服务复杂度，为商业广告推荐系统提供了实用的改进方案。

Abstract: Diverse and enriched data sources are essential for commercial ads-recommendation models to accurately assess user interest both before and after engagement with content. While extended user-engagement histories can improve the prediction of user interests, it is equally important to embed activity sequences from multiple sources to ensure freshness of user and ad-representations, following scaling law principles. In this paper, we present a novel three-dimensional framework for enhancing user-ad representations without increasing model inference or serving complexity. The first dimension examines the impact of incorporating diverse event sources, the second considers the benefits of longer user histories, and the third focuses on enriching data with additional event attributes and multi-modal embeddings. We assess the return on investment (ROI) of our source enrichment framework by comparing organic user engagement sources, such as content viewing, with ad-impression sources. The proposed method can boost the area under curve (AUC) and the slope of scaling curves for ad-impression sources by 1.56 to 2 times compared to organic usage sources even for short online-sequence lengths of 100 to 10K. Additionally, click-through rate (CTR) prediction improves by 0.56% AUC over the baseline production ad-recommendation system when using enriched ad-impression event sources, leading to improved sequence scaling resolutions for longer and offline user-ad representations.

</details>


### [29] [HarmonRank: Ranking-aligned Multi-objective Ensemble for Live-streaming E-commerce Recommendation](https://arxiv.org/abs/2601.02955)
*Boyang Xia,Zhou Yu,Zhiliang Zhu,Hanxiao Sun,Biyun Han,Jun Wang,Runnan Liu,Wenwu Ou*

Main category: cs.IR

TL;DR: 提出HarmonRank框架，解决直播电商推荐中多目标排序的优化方向错位和跨目标对齐问题，通过可微分排序技术和关系感知集成方案提升效果。


<details>
  <summary>Details</summary>
Motivation: 直播电商推荐需要平衡购买和用户-主播互动等多个目标，传统集成模型使用多个独立的二分类损失存在两个问题：1) 二分类任务的优化方向与排序任务（以AUC评估）不一致；2) 忽视了目标间的相关性（如评论和购买行为的部分依赖关系）。

Method: 提出HarmonRank框架：1) 将排序指标AUC公式化为秩和问题，使用可微分排序技术进行面向排序的优化；2) 将原始的一步集成范式改为两步关系感知集成方案，学习不同目标间对齐的排序能力。

Result: 在两个工业数据集上的离线实验和在线实验表明，该方法显著优于现有最先进方法。已在快手直播电商推荐平台（4亿日活用户）全面部署，带来超过2%的购买增益。

Conclusion: HarmonRank通过同时实现与排序任务的对齐和跨目标间的对齐，有效解决了直播电商推荐中的多目标平衡问题，在实际工业场景中取得了显著效果提升。

Abstract: Recommendation for live-streaming e-commerce is gaining increasing attention due to the explosive growth of the live streaming economy. Different from traditional e-commerce, live-streaming e-commerce shifts the focus from products to streamers, which requires ranking mechanism to balance both purchases and user-streamer interactions for long-term ecology. To trade off multiple objectives, a popular solution is to build an ensemble model to integrate multi-objective scores into a unified score. The ensemble model is usually supervised by multiple independent binary classification losses of all objectives. However, this paradigm suffers from two inherent limitations. First, the optimization direction of the binary classification task is misaligned with the ranking task (evaluated by AUC). Second, this paradigm overlooks the alignment between objectives, e.g., comment and buy behaviors are partially dependent which can be revealed in labels correlations. The model can achieve better trade-offs if it learns the aligned parts of ranking abilities among different objectives.
  To mitigate these limitations, we propose a novel multi-objective ensemble framework HarmonRank to fulfill both alignment to the ranking task and alignment among objectives. For alignment to ranking, we formulate ranking metric AUC as a rank-sum problem and utilize differentiable ranking techniques for ranking-oriented optimization. For inter-objective alignment, we change the original one-step ensemble paradigm to a two-step relation-aware ensemble scheme.
  Extensive offline experiments results on two industrial datasets and online experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods. The proposed method has been fully deployed in Kuaishou's live-streaming e-commerce recommendation platform with 400 million DAUs, contributing over 2% purchase gain.

</details>


### [30] [Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation](https://arxiv.org/abs/2601.02962)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 提出一种通过递归算法询问构建建议树的新方法，用于识别搜索查询建议中的偏见，特别针对政治领域人物相关搜索的主题群体偏见。


<details>
  <summary>Details</summary>
Motivation: 搜索查询建议在在线信息搜索中扮演重要角色，但相关研究较少。主要挑战在于上下文稀疏性和数据基础有限（每个查询最多10条建议），这使得识别搜索查询建议中的偏见变得困难。

Method: 采用递归算法询问技术，创建建议树，从而获取更多潜意识的搜索查询建议。基于这些建议，研究政治领域人物相关搜索中的主题群体偏见。

Result: 该方法能够深化偏见分析的数据基础，提供了一种新的替代方法来识别搜索查询建议中的偏见。

Conclusion: 通过递归算法询问构建建议树的方法，为解决搜索查询建议偏见识别中的数据稀疏性问题提供了有效途径，特别适用于政治领域人物搜索的主题群体偏见分析。

Abstract: Despite their important role in online information search, search query suggestions have not been researched as much as most other aspects of search engines. Although reasons for this are multi-faceted, the sparseness of context and the limited data basis of up to ten suggestions per search query pose the most significant problem in identifying bias in search query suggestions. The most proven method to reduce sparseness and improve the validity of bias identification of search query suggestions so far is to consider suggestions from subsequent searches over time for the same query. This work presents a new, alternative approach to search query bias identification that includes less high-level suggestions to deepen the data basis of bias analyses. We employ recursive algorithm interrogation techniques and create suggestion trees that enable access to more subliminal search query suggestions. Based on these suggestions, we investigate topical group bias in person-related searches in the political domain.

</details>


### [31] [Parallel Latent Reasoning for Sequential Recommendation](https://arxiv.org/abs/2601.03153)
*Jiakai Tang,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: PLR提出并行潜在推理框架，通过同时探索多个不同推理轨迹来解决序列推荐中稀疏行为序列的复杂用户偏好建模问题，相比现有仅依赖深度扩展的方法，实现了宽度级别的计算扩展。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法仅依赖单一轨迹的深度扩展，随着推理深度增加会出现收益递减问题。需要新的方法来更有效地捕捉稀疏行为序列中的复杂用户偏好。

Method: PLR框架通过连续潜在空间中的可学习触发令牌构建并行推理流，通过全局推理正则化保持流间多样性，并通过混合推理流聚合自适应合成多流输出。

Result: 在三个真实世界数据集上的实验表明，PLR显著优于最先进的基线方法，同时保持实时推理效率。理论分析进一步验证了并行推理在提升泛化能力方面的有效性。

Conclusion: PLR为序列推荐中的推理能力增强开辟了新途径，超越了现有的深度扩展方法，展示了宽度级别计算扩展的潜力。

Abstract: Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose \textbf{Parallel Latent Reasoning (PLR)}, a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.

</details>


### [32] [Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers](https://arxiv.org/abs/2601.03211)
*Yue Kang,Zhuoyi Huang,Benji Schussheim,Diana Licon,Dina Atia,Shixing Cao,Jacob Danovitch,Kunho Kim,Billy Norcilien,Jonah Karpman,Mahmound Sayed,Mike Taylor,Tao Sun,Pavel Metrikov,Vipul Agarwal,Chris Quirk,Ye-Yi Wang,Nick Craswell,Irene Shaffer,Tianwei Chen,Sulaiman Vesal,Soundar Srinivasan*

Main category: cs.IR

TL;DR: 本文提出一种高效方法，通过合成数据生成和小语言模型微调，实现企业搜索中高质量、高吞吐量的相关性标注，性能媲美大语言模型但成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 企业搜索中构建高质量数据集面临核心挑战：难以获取标注数据。现有方法依赖大语言模型进行相关性标注，但成本高、吞吐量低，难以满足企业级规模化需求。

Method: 1. 使用LLM从种子文档合成真实企业查询；2. 应用BM25检索困难负样本；3. 使用教师LLM分配相关性分数；4. 将生成的数据集蒸馏到小型语言模型，生成紧凑的相关性标注器。

Result: 在923个企业查询-文档对的人工标注基准上，蒸馏后的SLM与人类判断的一致性达到或超过教师LLM水平。吞吐量提高17倍，成本效益提高19倍。

Conclusion: 该方法为企业级检索应用提供了可扩展、成本效益高的相关性标注解决方案，支持现实场景中的快速离线评估和迭代，解决了企业搜索中数据标注的规模化难题。

Abstract: In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [33] [A $O^*((2 + ε)^k)$ Time Algorithm for Cograph Deletion Using Unavoidable Subgraphs in Large Prime Graphs](https://arxiv.org/abs/2601.02532)
*Manuel Lafond,Francis Sarrazin*

Main category: cs.DS

TL;DR: 该论文提出了基于模分解的新方法解决Cograph Deletion问题，将运行时间从O*(2.303^k)改进到O*((2+ε)^k)，并首次应用了素数图的结构特征。


<details>
  <summary>Details</summary>
Motivation: Cograph Deletion问题在计算生物学和社交网络分析中有应用。现有参数化算法都采用相似的策略，通过寻找P4并探索其局部结构进行递归分支，最佳算法运行时间为O*(2.303^k)，且由于分支情况复杂需要自动搜索。由于难以进一步改进现有策略，需要新的方法。

Method: 采用模分解的新方法：1) 将图分解为模块和商图，分别独立解决；2) 将问题简化为在素数图上求解（所有模块都是平凡的）；3) 利用Chudnovsky等人的特征化结果：任何足够大的素数图都包含七种结构之一作为诱导子图；4) 这些结构都有大量P4（数量随图大小线性增长），从而允许递归分支算法达到O*((2+ε)^k)的运行时间。

Result: 实现了运行时间O*((2+ε)^k)的参数化算法，显著优于现有的O*(2.303^k)。这是素数图特征化的首次算法应用，并为其他修改问题的改进打开了大门。

Conclusion: 通过模分解和素数图结构特征化的新方法，显著改进了Cograph Deletion问题的参数化算法性能。该方法可推广到其他图修改问题，论文还确定了哪些图类H的H-free编辑问题可以利用这种约简到素数图的方法。

Abstract: We study the parameterized complexity of the Cograph Deletion problem, which asks whether one can delete at most $k$ edges from a graph to make it $P_4$-free. This is a well-known graph modification problem with applications in computation biology and social network analysis.
  All current parameterized algorithms use a similar strategy, which is to find a $P_4$ and explore the local structure around it to perform an efficient recursive branching.
  The best known algorithm achieves running time $O^*(2.303^k)$ and requires an automated search of the branching cases due to their complexity.
  Since it appears difficult to further improve the current strategy, we devise a new approach using modular decompositions. We solve each module and the quotient graph independently, with the latter being the core problem. This reduces the problem to solving on a prime graph, in which all modules are trivial. We then use a characterization of Chudnovsky et al. stating that any large enough prime graph has one of seven structures as an induced subgraph. These all have many $P_4$s, with the quantity growing linearly with the graph size, and we show that these allow a recursive branch tree algorithm to achieve running time $O^*((2 + ε)^k)$ for any $ε> 0$.
  This appears to be the first algorithmic application of the prime graph characterization and it could be applicable to other modification problems. Towards this goal, we provide the exact set of graph classes $\H$ for which the $\H$-free editing problem can make use of our reduction to a prime graph, opening the door to improvements for other modification problems.

</details>


### [34] [A Practical 73/50 Approximation for Contiguous Monotone Moldable Job Scheduling](https://arxiv.org/abs/2601.02836)
*Klaus Jansen,Felix Ohnesorge*

Main category: cs.DS

TL;DR: 提出了一种用于单调可塑作业调度的新算法，近似比为≈1.4593+ε，时间复杂度为O(nm log(1/ε))，相比之前算法在效率和实用性上有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的单调可塑作业调度算法存在以下问题：1）PTAS算法时间复杂度极高（Ω(n^{g(1/ε)})，其中g是超指数函数）；2）完全多项式近似方案仅适用于m≥8n/ε的特殊情况；3）3/2近似算法时间复杂度为O(nm log(mn))。需要开发更实用、高效的算法。

Method: 提出了一种新的近似算法，核心思想是通过更高效的调度策略和优化技术，在保证近似比的同时显著降低时间复杂度。算法采用O(nm log(1/ε))的时间复杂度，适用于连续变体问题。

Result: 算法达到≈(1.4593+ε)的近似比，时间复杂度为O(nm log(1/ε))。实际实现表明，算法的实际性能明显优于理论最坏情况近似比。

Conclusion: 该研究提出了一种在理论和实践上都高效的单调可塑作业调度算法，在近似比和时间复杂度之间取得了良好平衡，相比现有算法有显著改进，具有实际应用价值。

Abstract: In moldable job scheduling, we are provided $m$ identical machines and $n$ jobs that can be executed on a variable number of machines. The execution time of each job depends on the number of machines assigned to execute that job. For the specific problem of monotone moldable job scheduling, jobs are assumed to have a processing time that is non-increasing in the number of machines.
  The previous best-known algorithms are: (1) a polynomial-time approximation scheme with time complexity $Ω(n^{g(1/\varepsilon)})$, where $g(\cdot)$ is a super-exponential function [Jansen and Thöle '08; Jansen and Land '18], (2) a fully polynomial approximation scheme for the case of $m \geq 8\frac{n}{\varepsilon}$ [Jansen and Land '18], and (3) a $\frac{3}{2}$ approximation with time complexity $O(nm\log(mn))$ [Wu, Zhang, and Chen '23].
  We present a new practically efficient algorithm with an approximation ratio of $\approx (1.4593 + \varepsilon)$ and a time complexity of $O(nm \log \frac{1}{\varepsilon})$. Our result also applies to the contiguous variant of the problem. In addition to our theoretical results, we implement the presented algorithm and show that the practical performance is significantly better than the theoretical worst-case approximation ratio.

</details>


### [35] [Hardness of Regular Expression Matching with Extensions](https://arxiv.org/abs/2601.03020)
*Taisei Nogami,Tachio Terauchi*

Main category: cs.DS

TL;DR: 正则表达式匹配问题的时间复杂度下界研究：对于包含反向引用、交集和补集扩展的正则表达式，在OVC假设下无法达到O(n^{2-ε})时间；对于ERE（含补集）匹配，在k-Clique假设下无法达到O(n^{ω-ε})时间，组合算法无法达到O(n^{3-ε})时间。


<details>
  <summary>Details</summary>
Motivation: 研究正则表达式匹配问题的时间复杂度下界，特别是对于包含反向引用、交集和补集等扩展功能的情况。虽然Thompson算法能在O(nm)时间内解决基本正则表达式匹配，且lookaround扩展也能保持相同复杂度，但其他扩展的时间复杂度下界尚不清楚。

Method: 基于计算复杂性理论中的假设（正交向量猜想OVC和k-Clique假设），通过归约方法证明时间复杂度的下界。对于ERE匹配问题，分别考虑了使用快速矩阵乘法的算法和组合算法两种情况。

Result: 1. 对于包含反向引用、交集或补集扩展的正则表达式匹配，在OVC假设下无法在O(n^{2-ε} poly(m))时间内解决（反向引用即使限制为一个捕获组）。
2. 对于ERE（含补集）匹配，在k-Clique假设下无法在O(n^{ω-ε} poly(m))时间内解决（ω为矩阵乘法指数）。
3. 组合ERE匹配算法在组合k-Clique假设下无法在O(n^{3-ε} poly(m))时间内解决。

Conclusion: 该研究揭示了正则表达式扩展功能对匹配问题时间复杂度的根本影响，解释了为什么ERE匹配问题的时间复杂度在45年来难以改进。Hopcroft和Ullman的O(n³m)算法以及Bille等人的O(n^ωm)改进已经接近最优，这为理解正则表达式匹配的计算复杂性提供了重要理论依据。

Abstract: The regular expression matching problem asks whether a given regular expression of length $m$ matches a given string of length $n$. As is well known, the problem can be solved in $O(nm)$ time using Thompson's algorithm. Moreover, recent studies have shown that the matching problem for regular expressions extended with a practical extension called lookaround can be solved in the same time complexity. In this work, we consider three well-known extensions to regular expressions called backreference, intersection and complement, and we show that, unlike in the case of lookaround, the matching problem for regular expressions extended with any of the three (for backreference, even when restricted to one capturing group) cannot be solved in $O(n^{2-\varepsilon} \mathrm{poly}(m))$ time for any constant $\varepsilon > 0$ under the Orthogonal Vectors Conjecture. Moreover, we study the matching problem for regular expressions extended with complement in more detail, which is also known as extended regular expression (ERE) matching. We show that there is no ERE matching algorithm that runs in $O(n^{ω-\varepsilon} \mathrm{poly}(m))$ time ($2 \le ω< 2.3716$ is the exponent of square matrix multiplication) for any constant $\varepsilon > 0$ under the $k$-Clique Hypothesis, and there is no combinatorial ERE matching algorithm that runs in $O(n^{3-\varepsilon} \mathrm{poly}(m))$ time for any constant $\varepsilon > 0$ under the Combinatorial $k$-Clique Hypothesis. This shows that the $O(n^3 m)$-time algorithm introduced by Hopcroft and Ullman in 1979 and recently improved by Bille et al. to run in $O(n^ωm)$ time using fast matrix multiplication was already optimal in a sense, and sheds light on why the theoretical computer science community has struggled to improve the time complexity of ERE matching with respect to $n$ and $m$ for more than 45 years.

</details>


### [36] [Density Matters: A Complexity Dichotomy of Deleting Edges to Bound Subgraph Density](https://arxiv.org/abs/2601.03129)
*Matthias Bentert,Tom-Lukas Breitkopf,Vincent Froese,Anton Herrmann,André Nichterlein*

Main category: cs.DS

TL;DR: τ-BDED问题的复杂性完全分类：当2τ为整数或τ<2/3时多项式可解，否则NP难。同时给出了树宽参数下的FPT算法和整数τ时的随机算法。


<details>
  <summary>Details</summary>
Motivation: 研究τ-有界密度边删除问题的计算复杂性，该问题要求删除最少边使得图中所有子图密度不超过τ。之前已知部分τ值的复杂性，需要完整的复杂性分类。

Method: 通过将问题规约到新的受限网络流问题，根据τ值分别使用最大s-t流或一般因子匹配算法求解。对于NP难情况，提供树宽参数下的固定参数可解算法。

Result: 建立了完整的复杂性二分：当2τ∈ℕ或τ<2/3时多项式可解，否则NP难。对于整数τ∈ℕ，给出了随机O(m^{1+o(1)})时间算法，并在树宽参数下获得FPT算法。

Conclusion: 完全解决了τ-BDED问题的复杂性分类，建立了流与匹配变体之间的新联系，这些技术具有独立的研究价值。

Abstract: We study $τ$-Bounded-Density Edge Deletion ($τ$-BDED), where given an undirected graph $G$, the task is to remove as few edges as possible to obtain a graph $G'$ where no subgraph of $G'$ has density more than $τ$. The density of a (sub)graph is the number of edges divided by the number of vertices. This problem was recently introduced and shown to be NP-hard for $τ\in \{2/3, 3/4, 1 + 1/25\}$, but polynomial-time solvable for $τ\in \{0,1/2,1\}$ [Bazgan et al., JCSS 2025]. We provide a complete dichotomy with respect to the target density $τ$:
  1. If $2τ\in \mathbb{N}$ (half-integral target density) or $τ< 2/3$, then $τ$-BDED is polynomial-time solvable.
  2. Otherwise, $τ$-BDED is NP-hard.
  We complement the NP-hardness with fixed-parameter tractability with respect to the treewidth of $G$. Moreover, for integral target density $τ\in \mathbb{N}$, we show $τ$-BDED to be solvable in randomized $O(m^{1 + o(1)})$ time. Our algorithmic results are based on a reduction to a new general flow problem on restricted networks that, depending on $τ$, can be solved via Maximum s-t-Flow or General Factors. We believe this connection between these variants of flow and matching to be of independent interest.

</details>
