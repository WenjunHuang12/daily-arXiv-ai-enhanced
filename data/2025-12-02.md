<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 13]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.IT](#cs.IT) [Total: 14]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [An O(1) Space Algorithm for N-Dimensional Tensor Rotation: A Generalization of the Reversal Method](https://arxiv.org/abs/2512.00111)
*Dexin Chen*

Main category: cs.DS

TL;DR: 本文提出了一种名为"2^n+1反转算法"的N维张量原地旋转方法，将经典的一维数组三反转算法推广到任意维度，实现了O(1)辅助空间和线性时间复杂度的张量旋转。


<details>
  <summary>Details</summary>
Motivation: 多维数组（张量）旋转是计算机科学中的基本操作，在数据处理和科学计算中广泛应用。虽然存在多种方法，但实现原地旋转（即使用O(1)辅助空间）是一个重要的算法挑战。经典的一维三反转算法提供了O(1)空间解决方案，但需要将其推广到N维情况。

Method: 提出"2^n+1反转算法"，这是对一维三反转算法的N维推广。算法首先形式化定义N维张量反转操作，然后通过一系列反转操作实现张量旋转。对于n维张量，算法需要执行2^n+1次反转操作（如一维需要3次，二维需要5次）。

Result: 算法实现了原地张量旋转，仅需O(1)辅助空间，时间复杂度与元素数量呈线性关系。提供了详细的伪代码和严格的正确性证明，验证了从一维（3次反转）和二维（5次反转）观察到的模式适用于任意维度。

Conclusion: 成功将经典的三反转算法推广到任意维度，提出了通用的"2^n+1反转算法"，解决了N维张量原地旋转问题，为多维数据处理提供了高效的空间优化解决方案。

Abstract: The rotation of multi-dimensional arrays, or tensors, is a fundamental operation in computer science with applications ranging from data processing to scientific computing. While various methods exist, achieving this rotation in-place (i.e., with O(1) auxiliary space) presents a significant algorithmic challenge. The elegant three-reversal algorithm provides a well-known O(1) space solution for one-dimensional arrays. This paper introduces a generalization of this method to N dimensions, resulting in the "$2^n+1$ reversal algorithm". This algorithm achieves in-place tensor rotation with O(1) auxiliary space and a time complexity linear in the number of elements. We provide a formal definition for N-dimensional tensor reversal, present the algorithm with detailed pseudocode, and offer a rigorous proof of its correctness, demonstrating that the pattern observed in one dimension ($2^1+1=3$ reversals) and two dimensions ($2^2+1=5$ reversals) holds for any arbitrary number of dimensions.

</details>


### [2] [Maximum-Flow and Minimum-Cut Sensitivity Oracles for Directed Graphs](https://arxiv.org/abs/2512.00153)
*Mridul Ahi,Keerti Choudhary,Shlok Pande,Pushpraj,Lakshay Saggi*

Main category: cs.DS

TL;DR: 该论文提出了在有向图中处理边故障的流和割的灵敏度数据结构，包括故障容忍流族、最大流灵敏度查询器和最小割灵敏度查询器，特别关注小λ值的情况。


<details>
  <summary>Details</summary>
Motivation: 在有向图中，当边发生故障时，需要快速查询最大流值和最小割值的变化。传统方法空间复杂度高，特别是对于小最大流值λ的图，需要更紧凑的数据结构来高效处理单边和双边故障查询。

Method: 1. 构造故障容忍流族：对于任何最大流值为λ的图，构建包含2λ+1个(s,t)-流的族B，使得对于每条边e，B包含G-e的一个(s,t)-最大流。
2. 最大流灵敏度查询器：构建空间复杂度O(λn)的单边和双边故障查询器，能在O(n)时间内报告故障后的最大流值。
3. 最小割灵敏度查询器：针对小λ值，构建空间复杂度O(λn)的双边故障查询器，常数时间回答最小割大小查询，O(n)时间报告割划分。
4. 扩展到k边故障：构建空间复杂度O_{λ,k}(n log n)的k故障容忍最小割查询器，在O_{λ,k}(log n)时间内回答查询。

Result: 1. 成功构造了故障容忍流族，包含2λ+1个流，覆盖所有单边故障情况。
2. 实现了紧凑的最大流灵敏度查询器，空间O(λn)，查询时间O(n)。
3. 针对小λ图，构建了更紧凑的最小割查询器，空间O(λn)，常数时间回答割大小查询。
4. 扩展到k故障情况，实现了空间O_{λ,k}(n log n)、查询时间O_{λ,k}(log n)的通用查询器。

Conclusion: 该论文为有向图中的边故障灵敏度查询提供了高效的数据结构，特别优化了小最大流值图的情况。提出的故障容忍流族概念具有独立的理论价值，各种查询器在空间效率和查询时间上取得了良好平衡，扩展了现有工作并解决了实际应用中的关键问题。

Abstract: Given a digraph $G = (V, E)$ with a designated source $s$, sink $t$, and an $(s,t)$-max-flow of value $λ$, we present constructions for max-flow and min-cut sensitivity oracles, and introduce the concept of a fault-tolerant flow family, which may be of independent interest. Our main contributions are as follows.
  1. Fault-Tolerant Flow Family: For any graph $G$ with $(s,t)$-max-flow value $λ$, we construct a family $B$ of $2λ+1$ $(s,t)$-flows such that for every edge $e$, $B$ contains an $(s,t)$-max-flow of $G-e$.
  2. Max-Flow Sensitivity Oracle: We construct a single as well as dual-edge sensitivity oracle for $(s,t)$-max-flow that requires only $O(λn)$ space. Given any set $F$ of up to two failing edges, the oracle reports the updated max-flow value in $G-F$ in $O(n)$ time. Additionally, for the single-failure case, the oracle can determine in constant time whether the flow through an edge $x$ changes when another edge $e$ fails.
  3. Min-Cut Sensitivity Oracle for Dual Failures: Recently, Baswana et al. (ICALP'22) designed an $O(n^2)$-sized oracle for answering $(s,t)$-min-cut size queries under dual edge failures in constant time. We extend this by focusing on graphs with small min-cut values $λ$, and present a more compact oracle of size $O(λn)$ that answers such min-cut size queries in constant time and reports the corresponding $(s,t)$-min-cut partition in $O(n)$ time.
  4. Min-Cut Sensitivity Oracle for Multiple Failures: We extend our results to the general case of $k$ edge failures. For any graph with $(s,t)$-min-cut of size $λ$, we construct a $k$-fault-tolerant min-cut oracle with space complexity $O_{λ,k}(n \log n)$ that answers min-cut size queries in $O_{λ,k}(\log n)$ time.

</details>


### [3] [Approximating Directed Connectivity in Almost-Linear Time](https://arxiv.org/abs/2512.00176)
*Kent Quanrud*

Main category: cs.DS

TL;DR: 提出随机算法，在加权有向图中用O(log⁴n/ε)和O(log⁵n/ε)次单商品流计算(1+ε)-近似最小全局边割和顶点割，实现几乎线性时间近似方案。


<details>
  <summary>Details</summary>
Motivation: 传统计算有向图最小割的算法复杂度较高，需要开发更高效的近似算法，特别是在大规模图上实现几乎线性时间的解决方案。

Method: 基于"收缩包装"的分治技术，针对根Steiner连通性问题，通过流算法验证根到终端的连通性，对未验证终端生成包含最小割汇分量的r-割，实现终端集的分治处理。

Result: 边割算法复杂度为O(log⁴n/ε)次单商品流，顶点割为O(log⁵n/ε)次，结合CKL+22的几乎线性时间流算法，得到几乎线性时间近似方案，对小的顶点连通性提供更快的精确算法。

Conclusion: 通过收缩包装分治技术，实现了有向图最小割的高效近似算法，显著改进了计算复杂度，为大规模图分析提供了实用工具。

Abstract: We present randomized algorithms that compute $(1+ε)$-approximate minimum global edge and vertex cuts in weighted directed graphs in $O(\log^4(n) / ε)$ and $O(\log^5(n)/ε)$ single-commodity flows, respectively. With the almost-linear time flow algorithm of [CKL+22], this gives almost linear time approximation schemes for edge and vertex connectivity. By setting $ε$ appropriately, this also gives faster exact algorithms for small vertex connectivity.
  At the heart of these algorithms is a divide-and-conquer technique called "shrink-wrapping" for a certain well-conditioned rooted Steiner connectivity problem. Loosely speaking, for a root $r$ and a set of terminals, shrink-wrapping uses flow to certify the connectivity from a root $r$ to some of the terminals, and for the remaining uncertified terminals, generates an $r$-cut where the sink component both (a) contains the sink component of the minimum $(r,t)$-cut for each uncertified terminal $t$ and (b) has size proportional to the number of uncertified terminals. This yields a divide-and-conquer scheme over the terminals where we can divide the set of terminals and compute their respective minimum $r$-cuts in smaller, contracted subgraphs.

</details>


### [4] [Expected Cost Analysis of Online Facility Assignment on Regular Polygons](https://arxiv.org/abs/2512.00506)
*Md Rawha Siddiqi Riad,Md Manzurul Hasan*

Main category: cs.DS

TL;DR: 本文分析几何环境中的在线设施分配问题：设施位于正n边形顶点，顾客按均匀随机位置顺序到达，需立即分配到最近可用设施，研究期望分配成本的递归特征和计算方法。


<details>
  <summary>Details</summary>
Motivation: 研究在线设施分配问题在几何环境中的表现，特别是当设施位于正多边形顶点、顾客随机到达时，需要立即分配且未来到达未知的情况下，如何分析期望分配成本。

Method: 提出期望成本的递归特征：对于任何占用状态S，期望剩余成本V(S)等于所有边缘位置立即分配成本加上分配后期望未来成本的平均值。开发了积分方程解法、离散化动态规划方法和蒙特卡洛模拟。

Result: 证明了积分方程可以计算解决方案，并给出了小n值（n=3,4,5）的期望值。对于更大的n和期望成本，开发了高效的数值计算方法。

Conclusion: 该工作为多边形环境中的在线分配问题建立了基本的概率分析方法，为几何设置中的在线设施分配提供了理论基础和实用计算工具。

Abstract: This paper analyzes the online facility assignment problem in a geometric setting where facilities with unit capacity are positioned at the vertices of a regular $n$-gon. Customers arrive sequentially at uniformly random positions along the edges. They must be assigned immediately to the nearest available facility, with ties broken by coin toss. The sequential nature and unknown future arrivals require a probabilistic analysis of the expected assignment cost. Our main contribution is a recursive characterization of the expected cost: for any occupancy state $S$, the expected remaining cost $V(S)$ equals the average over all edge positions of the immediate assignment cost plus the expected future cost after assignment. We prove that this integral equation can calculate a solution and provide the expected value for small $n$ ($n = 3, 4, 5$). For larger values of $n$ and expected cost, we develop efficient numerical methods, including a discretized dynamic programming approach and Monte Carlo simulation. The work establishes a fundamental probabilistic approach for online assignment in polygonal environments.

</details>


### [5] [Perfect $L_p$ Sampling with Polylogarithmic Update Time](https://arxiv.org/abs/2512.00632)
*William Swartworth,David P. Woodruff,Samson Zhou*

Main category: cs.DS

TL;DR: 首次实现了在最优空间复杂度下具有多项式对数更新时间的完美Lp采样器，解决了之前方法更新时间过慢的问题


<details>
  <summary>Details</summary>
Motivation: Jayaram和Woodruff提出的完美Lp采样器虽然达到了最优的O(log² n)空间复杂度，但更新时间为n^C，效率极低。需要在不牺牲空间效率的前提下显著改善更新时间的性能

Method: 通过高效模拟截断指数随机变量幂次倒数的和，使用Gil-Pelaez反演公式近似其特征函数，并应用梯形公式的变体来快速计算近似值

Result: 首次实现了在0<p<2范围内，具有相同最优内存消耗但仅需poly(log n)更新时间的完美Lp采样器

Conclusion: 成功解决了完美Lp采样器中更新时间与空间效率之间的权衡问题，为流算法中的采样操作提供了更实用的解决方案

Abstract: Perfect $L_p$ sampling in a stream was introduced by Jayaram and Woodruff (FOCS 2018) as a streaming primitive which, given turnstile updates to a vector $x \in \{-\text{poly}(n), \ldots, \text{poly}(n)\}^n$, outputs an index $i^* \in \{1, 2, \ldots, n\}$ such that the probability of returning index $i$ is exactly \[\Pr[i^* = i] = \frac{|x_i|^p}{\|x\|_p^p} \pm \frac{1}{n^C},\] where $C > 0$ is an arbitrarily large constant. Jayaram and Woodruff achieved the optimal $\tilde{O}(\log^2 n)$ bits of memory for $0 < p < 2$, but their update time is at least $n^C$ per stream update. Thus an important open question is to achieve efficient update time while maintaining optimal space. For $0 < p < 2$, we give the first perfect $L_p$-sampler with the same optimal amount of memory but with only $\text{poly}(\log n)$ update time. Crucial to our result is an efficient simulation of a sum of reciprocals of powers of truncated exponential random variables by approximating its characteristic function, using the Gil-Pelaez inversion formula, and applying variants of the trapezoid formula to quickly approximate it.

</details>


### [6] [A Fast Algorithm for Finding Minimum Weight Cycles in Mining Cyclic Graph Topologies](https://arxiv.org/abs/2512.01049)
*Heman Shakeri,Torben Amtoft,Behnaz Moradi-Jamei,Nathan Albin,Pietro Poggi-Corradini*

Main category: cs.DS

TL;DR: 提出一种基于复合距离度量的确定性最小权重环算法，通过节点丢弃和动态图剪枝优化搜索效率，并应用于循环模数计算加速。


<details>
  <summary>Details</summary>
Motivation: 图中循环结构对网络分析至关重要，但最小权重环的高效计算仍具挑战性，需要比最短路径计算更优的算法。

Method: 基于Dijkstra算法框架，引入复合距离度量将全局环搜索转化为节点中心优化，结合节点丢弃技术和动态图剪枝启发式方法加速搜索。

Result: 算法能有效找到最小权重环，通过剪枝技术显著提升实际运行速度，并在循环模数计算中大幅减少迭代约束查找时间。

Conclusion: 该确定性算法为循环拓扑分析提供了高效核心原语，可支持更复杂的网络循环特性分析应用。

Abstract: Cyclic structures are fundamental topological features in graphs, playing critical roles in network robustness, information flow, community structure, and various dynamic processes. Algorithmic tools that can efficiently probe and analyze these cyclic topologies are increasingly vital for tasks in graph mining, network optimization, bioinformatics, and social network analysis. A core primitive for quantitative analysis of cycles is finding the Minimum Weight Cycle (MWC), representing the shortest cyclic path in a weighted graph. However, computing the MWC efficiently remains a challenge, particularly compared to shortest path computations. This paper introduces a novel deterministic algorithm for finding the MWC in general weighted graphs. Our approach adapts the structure of Dijkstra's algorithm by introducing and minimizing a \textit{composite distance} metric, effectively translating the global cycle search into an iterative node-centric optimization. We provide a rigorous proof of correctness based on loop invariants. We detail two mechanisms for accelerating the search: a provable node discarding technique based on intermediate results, and a highly effective graph pruning heuristic. This heuristic dynamically restricts the search to relevant subgraphs, leveraging the principle of locality often present in complex networks to achieve significant empirical speedups, while periodic resets ensure global optimality is maintained. The efficiency of the proposed MWC algorithm enables its use as a core component in more complex analyses focused on cyclic properties. We illustrate this through a detailed application case study: accelerating the computation of the Loop Modulus, a measure of cycle richness used in advanced network characterization. Our algorithm dramatically reduces the runtime of the iterative constraint-finding bottleneck in this computation.

</details>


### [7] [Beware of the Classical Benchmark Instances for the Traveling Salesman Problem with Time Windows](https://arxiv.org/abs/2512.01064)
*Francisco J. Soulignac*

Main category: cs.DS

TL;DR: 提出了一种简单精确的启发式搜索方法，能在10秒内解决50+客户的TSPTW-M经典基准实例，并警告这些基准已不再适合评估算法性能


<details>
  <summary>Details</summary>
Motivation: 解决TSPTW-M（带时间窗的旅行商问题，以制造周期为目标）及其Duration变体的基准测试问题，发现现有基准实例过于简单，无法有效评估算法性能

Method: 提出了一种简单而精确的启发式搜索方法，能够快速求解TSPTW-M问题，并将其作为现成方法应用于Duration目标变体

Result: 该方法在10秒内解决了所有50+客户的经典基准实例，并且除一个实例外，所有实例的Duration目标变体也都被成功解决

Conclusion: 这些经典基准实例已不再适合用于评估TSPTW-M及其Duration变体，因为它们可以被"破解"得到看似出色的结果；同时警告在设计机器学习算法的硬训练集时需要谨慎

Abstract: We propose a simple and exact informed search method for the Traveling Salesman Problem with Time Windows and Makespan objective (TSPTW-M) that solves all instances of the classical benchmark with 50 or more customers in less than ten seconds each. Applying this algorithm as an off-the-shelf method, we also solve all but one of these instances for the Duration objective. Our main conclusion is that these instances should no longer be employed for evaluating the TSPTW-M and its Duration variant: they can be ``hacked'' to yield results that seem outstanding at first sight. Additionally, caution is advised when designing hard training sets for machine learning algorithms.

</details>


### [8] [A practical algorithm for 3-admissibility](https://arxiv.org/abs/2512.01121)
*Christine Awofeso,Patrick Greaves,Oded Lachish,Felix Reidl*

Main category: cs.DS

TL;DR: 本文设计了一种计算图3-可容许性的算法，该算法在时间和空间上都是线性复杂度，并首次实现了3-可容许性的显式计算。


<details>
  <summary>Details</summary>
Motivation: 3-可容许性是识别具有算法友好结构的现实世界网络的有前景的度量。尽管3-可容许性比2-可容许性具有更好的算法特性，但现有研究缺乏计算3-可容许性的显式算法。

Method: 设计了一种算法，决定输入图G的3-可容许性是否至多为p。该算法采用"乐观"设计哲学，在时间和空间复杂度上都与输入大小呈线性关系（时间复杂度O(m+n)，空间复杂度O(n)）。

Result: 算法在\corpussize个现实世界网络上进行了实验评估，结果表明大多数现实世界网络的3-可容许性并不比2-可容许性大很多，尽管前者具有更好的算法特性。

Conclusion: 这是第一个计算3-可容许性的显式算法，其线性的时间和空间复杂度使其具有实用性。实验发现现实世界网络的3-可容许性通常接近2-可容许性，这为利用3-可容许性的算法优势提供了实际可能性。

Abstract: The $3$-admissibility of a graph is a promising measure to identify real-world networks that have an algorithmically favourable structure.
  We design an algorithm that decides whether the $3$-admissibility of an input graph~$G$ is at most~$p$ in time~\runtime and space~\memory, where $m$ is the number of edges in $G$ and $n$ the number of vertices. To the best of our knowledge, this is the first explicit algorithm to compute the $3$-admissibility.
  The linear dependence on the input size in both time and space complexity, coupled with an `optimistic' design philosophy for the algorithm itself, makes this algorithm practicable, as we demonstrate with an experimental evaluation on a corpus of \corpussize real-world networks.
  Our experimental results show, surprisingly, that the $3$-admissibility of most real-world networks is not much larger than the $2$-admissibility, despite the fact that the former has better algorithmic properties than the latter.

</details>


### [9] [Near-Optimal Sparsifiers for Stochastic Knapsack and Assignment Problems](https://arxiv.org/abs/2512.01240)
*Shaddin Dughmi,Yusuf Hakan Kalayci,Xinyu Liu*

Main category: cs.DS

TL;DR: 论文提出了一个多面体稀疏化框架，用于解决背包类约束下的不确定信息收集问题，通过度量查询集在多面体中的嵌入程度而非基数，实现了与问题维度无关的(1-ε)近似稀疏化。


<details>
  <summary>Details</summary>
Motivation: 现有基于基数的稀疏化方法在处理背包类约束时失效，因为可行集结构变化剧烈。需要一种能自然捕捉冗余而不依赖基数的新框架。

Method: 引入多面体稀疏化框架，将稀疏度定义为将查询集嵌入缩放可行性多面体所需的最小标量。通过按相似权重分组物品，并使用充电论证：当查询集错过最优物品时，要么用同组查询物品替代，要么利用该组的超额贡献补偿损失。

Result: 证明背包、多重背包和广义分配问题都允许具有多项式稀疏度(1/ρ和1/ε的多项式)的(1-ε)近似稀疏化器，且与问题维度无关。揭示了复杂度分离：虽然多重背包问题没有FPTAS，广义分配是APX难的，但它们的稀疏化对应问题允许高效近似算法。

Conclusion: 多面体稀疏化框架成功解决了背包类约束的稀疏化问题，实现了与维度无关的高效近似。提出了开放问题：这种稀疏化能否扩展到一般整数线性规划，同时保持与问题维度无关的稀疏度。

Abstract: When uncertainty meets costly information gathering, a fundamental question emerges: which data points should we probe to unlock near-optimal solutions? Sparsification of stochastic packing problems addresses this trade-off. The existing notions of sparsification measure the level of sparsity, called degree, as the ratio of queried items to the optimal solution size. While effective for matching and matroid-type problems with uniform structures, this cardinality-based approach fails for knapsack-type constraints where feasible sets exhibit dramatic structural variation. We introduce a polyhedral sparsification framework that measures the degree as the smallest scalar needed to embed the query set within a scaled feasibility polytope, naturally capturing redundancy without relying on cardinality.
  Our main contribution establishes that knapsack, multiple knapsack, and generalized assignment problems admit (1 - epsilon)-approximate sparsifiers with degree polynomial in 1/p and 1/epsilon -- where p denotes the independent activation probability of each element -- remarkably independent of problem dimensions. The key insight involves grouping items with similar weights and deploying a charging argument: when our query set misses an optimal item, we either substitute it with a queried item from the same group or leverage that group's excess contribution to compensate for the loss. This reveals an intriguing complexity-theoretic separation -- while the multiple knapsack problem lacks an FPTAS and generalized assignment is APX-hard, their sparsification counterparts admit efficient (1 - epsilon)-approximation algorithms that identify polynomial-degree query sets. Finally, we raise an open question: can such sparsification extend to general integer linear programs with degree independent of problem dimensions?

</details>


### [10] [Separator Theorem for Minor-Free Graphs in Linear Time](https://arxiv.org/abs/2512.01587)
*Édouard Bonnet,Tuukka Korhonen,Hung Le,Jason Li,Tomáš Masařík*

Main category: cs.DS

TL;DR: 该论文解决了在O(n)时间内为minor-free图找到大小为O(√n)的平衡分隔符这一长期开放问题，提出了一种基于顶点加权BFS的简单算法。


<details>
  <summary>Details</summary>
Motivation: 平面图分隔定理(Lipton-Tarjan)可以在线性时间内找到大小为O(√n)的平衡分隔符，但将其推广到minor-free图时，Alon等人的算法需要O(n^{3/2})时间。二十多年来，在minor-free图中在线性时间内找到大小为O(√n)的平衡分隔符一直是一个主要开放问题。

Method: 提出了一种非常简单的算法：在输入图上运行顶点加权变体的广度优先搜索(BFS)常数次。关键技术贡献是一个指导搜索平衡分隔符的顶点加权方案，建立了平衡分隔符大小与团minor模型存在之间的新联系。

Result: 成功解决了这一开放问题，为minor-free图提供了在线性时间内找到大小为O(√n)的平衡分隔符的算法。该算法简单高效，仅需常数次加权BFS。

Conclusion: 该论文肯定地回答了长期存在的开放问题，为minor-free图提供了线性时间的平衡分隔符算法。提出的加权方案可能具有独立的研究价值，为平面图算法向更一般图类的推广铺平了道路。

Abstract: The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that any minor-free graph admits a balanced separator of size $O(\sqrt{n})$ that can be found in $O(n^{3/2})$ time. The superlinear running time in their separator theorem is a key bottleneck for generalizing algorithmic results from planar to minor-free graphs. Despite extensive research for more than two decades, finding a balanced separator of size $O(\sqrt{n})$ in (linear) $O(n)$ time for minor-free graphs remains a major open problem. Known algorithms either give a separator of size much larger than $O(\sqrt{n})$ or have superlinear running time, or both.
  In this paper, we answer the open problem affirmatively. Our algorithm is very simple: it runs a vertex-weighted variant of breadth-first search (BFS) a constant number of times on the input graph. Our key technical contribution is a weighting scheme on the vertices to guide the search for a balanced separator, offering a new connection between the size of a balanced separator and the existence of a clique-minor model. We believe that our weighting scheme may be of independent interest.

</details>


### [11] [JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford](https://arxiv.org/abs/2512.01802)
*Xin Wang,Xi Chen*

Main category: cs.DS

TL;DR: JFR是一个基于Bellman-Ford的优化框架，通过前沿收缩和抽象多跳传播加速最短路径计算，在稀疏、稠密和负边图中减少25-99%的松弛操作，保持正确性。


<details>
  <summary>Details</summary>
Motivation: 传统最短路径算法（如Bellman-Ford）在大规模图上的计算开销较大，需要减少松弛操作次数以降低内存访问开销和计算工作量，提高吞吐量和能效。

Method: 采用前沿收缩和抽象多跳传播技术，在保持算法正确性的前提下，显著减少不必要的松弛操作，加速最短路径计算过程。

Result: 在最多20,000个节点和2.95亿条边的超大规模图上，JFR相比SPFA-SLF算法实现了25-99%的松弛操作减少，运行时间相当或更好，在各种图规模和密度下表现稳健。

Conclusion: JFR通过减少松弛操作有效降低了计算开销，适合高吞吐量和能效敏感场景。未来工作将集成高性能队列结构、自适应前沿策略和缓存感知技术以进一步提升性能。

Abstract: We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.

</details>


### [12] [Tight Bounds for Feedback Vertex Set Parameterized by Clique-width](https://arxiv.org/abs/2512.01900)
*Narek Bojikian,Stefan Kratsch*

Main category: cs.DS

TL;DR: 本文提出了一种新的标记图无环性表示方法，并给出了三个应用：基于k-团表达式的反馈顶点集计数算法、基于树分解的反馈顶点集计数算法，以及连通反馈顶点集决策算法，均达到了SETH下的紧界。


<details>
  <summary>Details</summary>
Motivation: 解决反馈顶点集计数问题的复杂性，特别是模2计数问题。之前的研究使用割-计数技术通过计数其他对象获得决策问题的紧界，但模2计数的复杂性一直未解决。同时，连通反馈顶点集决策问题的复杂性也需要研究。

Method: 1. 引入新的标记图无环性表示概念
2. 对于k-团表达式：设计O(6^kn^c)时间算法，通过复杂的合并子程序处理表达式中的并节点
3. 对于树分解：设计O(3^kn^c)时间算法
4. 对于连通反馈顶点集：设计O(18^kn^c)时间的单边错误蒙特卡洛算法
5. 所有算法均提供SETH下的匹配下界证明

Result: 1. 首次解决了基于k-团表达式的反馈顶点集模2计数问题，达到SETH紧界
2. 解决了基于树分解的反馈顶点集模2计数问题，匹配已知决策问题的紧界
3. 解决了连通反馈顶点集决策问题，达到SETH紧界
4. 填补了文献中的多个开放问题

Conclusion: 本文通过引入新的无环性表示方法，系统解决了反馈顶点集相关的计数和决策问题，为多个开放问题提供了完整的答案，并在SETH下达到了最优的时间复杂度。

Abstract: We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the decision problem in the same time. We complement these by a matching lower bound under the Strong Exponential-Time Hypothesis (SETH). This closes an open question that appeared multiple times in the literature [ESA 23, ICALP 24, IPEC 25].
  We also present an algorithm that, given a graph $G$ and a tree decomposition of width $k$ of $G$, in time $O(3^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. This matches the known SETH-tight bound for the decision version, which was obtained using the celebrated cut-and-count technique [FOCS 11, TALG 22]. Unlike other applications of cut-and-count, which use the isolation lemma to reduce a decision problem to counting solutions modulo $2$, this bound was obtained via counting other objects, leaving the complexity of counting solutions modulo $2$ open.
  Finally, we present a one-sided error Monte-Carlo algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(18^kn^c)$ decides the existence of a connected feedback vertex set of size $b$ in $G$. We provide a matching lower bound under SETH.

</details>


### [13] [Adaptive Matrix Sparsification and Applications to Empirical Risk Minimization](https://arxiv.org/abs/2512.02003)
*Yang P. Liu,Richard Peng,Colin Tang,Albert Weng,Junzhao Yang*

Main category: cs.DS

TL;DR: 提出一种高效算法，在近线性时间内求解具有块结构约束的ERM问题，核心是通过动态数据结构维护杠杆分数上界，实现快速内点法


<details>
  <summary>Details</summary>
Motivation: 解决大规模经验风险最小化（ERM）问题的计算效率问题。传统方法在处理具有块结构凸约束和线性等式约束的ERM问题时，当数据维度n远大于特征维度d时，需要高效算法来应对大规模数据集

Method: 采用内点法（IPM）框架，通过动态数据结构维护矩阵行更新时的杠杆分数上界。提出新算法在总时间Õ(nd + Td⁶)内维护杠杆分数上界，用于采样谱稀疏化器，实现Õ(√n)次迭代的高效IPM

Result: 算法在时间Õ(nd + d⁶√n) ≤ Õ(nd + d¹¹)内高精度求解ERM问题，当A稠密且n ≥ d¹⁰时达到近线性时间

Conclusion: 通过动态杠杆分数维护和谱稀疏化技术，实现了大规模ERM问题的高效求解，为具有块结构约束的优化问题提供了新的计算工具

Abstract: Consider the empirical risk minimization (ERM) problem, which is stated as follows. Let $K_1, \dots, K_m$ be compact convex sets with $K_i \subseteq \mathbb{R}^{n_i}$ for $i \in [m]$, $n = \sum_{i=1}^m n_i$, and $n_i\le C_K$ for some absolute constant $C_K$. Also, consider a matrix $A \in \mathbb{R}^{n \times d}$ and vectors $b \in \mathbb{R}^d$ and $c \in \mathbb{R}^n$. Then the ERM problem asks to find \[ \min_{\substack{x \in K_1 \times \dots \times K_m\\ A^\top x = b}}
  c^\top x. \] We give an algorithm to solve this to high accuracy in time $\widetilde{O}(nd + d^6\sqrt{n}) \le \widetilde{O} (nd + d^{11})$, which is nearly-linear time in the input size when $A$ is dense and $n \ge d^{10}$.
  Our result is achieved by implementing an $\widetilde{O}(\sqrt{n})$-iteration interior point method (IPM) efficiently using dynamic data structures. In this direction, our key technical advance is a new algorithm for maintaining leverage score overestimates of matrices undergoing row updates. Formally, given a matrix $A \in \mathbb{R}^{n \times d}$ undergoing $T$ batches of row updates of total size $n$ we give an algorithm which can maintain leverage score overestimates of the rows of $A$ summing to $\widetilde{O}(d)$ in total time $\widetilde{O}(nd + Td^6)$. This data structure is used to sample a spectral sparsifier within a robust IPM framework to establish the main result.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [14] [Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound](https://arxiv.org/abs/2512.00883)
*Jiahua Wang,Shannan Yan,Leqi Zheng,Jialong Wu,Yaoxin Mao*

Main category: cs.MM

TL;DR: 首个音频-视觉世界模型框架，通过条件扩散Transformer实现多模态环境模拟，在连续导航任务中提升智能体性能


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要关注视觉观测，但真实世界感知涉及多种感官模态。音频提供重要的空间和时间线索（如声源定位和声学场景特性），然而将音频整合到世界模型中的研究仍很有限。目前没有工作正式定义什么是音频-视觉世界模型，也没有研究如何在精确动作控制和任务奖励预测下联合捕捉双耳空间音频和视觉动态。

Method: 提出首个音频-视觉世界模型（AVWM）正式框架，将多模态环境模拟形式化为具有同步音频-视觉观测、细粒度动作和任务奖励的部分可观测马尔可夫决策过程。构建AVW-4k数据集（30小时双耳音频-视觉轨迹，包含动作标注和奖励信号）。提出AV-CDiT（音频-视觉条件扩散Transformer），采用新颖的模态专家架构平衡视觉和听觉学习，通过三阶段训练策略实现有效的多模态整合。

Result: AV-CDiT在视觉和听觉模态上实现了高保真度的多模态预测（含奖励）。在连续音频-视觉导航任务中，AVWM显著提升了智能体的性能。

Conclusion: 该工作首次正式定义了音频-视觉世界模型框架，通过创新的模态专家架构和三阶段训练策略，成功整合了音频和视觉信息，为多模态环境模拟和智能体规划提供了新的解决方案。

Abstract: World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.

</details>


### [15] [Augmenting Intra-Modal Understanding in MLLMs for Robust Multimodal Keyphrase Generation](https://arxiv.org/abs/2512.00928)
*Jiajun Cao,Qinggang Zhang,Yunbo Tang,Zhishang Xiang,Chang Yang,Jinsong Su*

Main category: cs.MM

TL;DR: AimKP是一个新颖的多模态关键词生成框架，通过渐进式模态掩码和基于梯度的过滤来增强MLLMs的模态内语义学习，同时保持跨模态对齐，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态关键词生成需要有效桥接异构模态间的语义鸿沟。虽然MLLMs通过大规模预训练实现了优越的跨模态理解，但存在模态偏见和细粒度模态内特征提取不足的问题，导致在现实世界噪声、不完整或未对齐的多媒体数据中缺乏鲁棒性。

Method: 提出AimKP框架，包含两个核心创新：(1)渐进式模态掩码：在训练过程中逐步掩码模态信息，强制模型从损坏的输入中提取细粒度特征；(2)基于梯度的过滤：识别并丢弃噪声样本，防止它们破坏模型的核心跨模态学习。

Result: 大量实验验证了AimKP在多模态关键词生成中的有效性，以及在不同场景下的鲁棒性。

Conclusion: AimKP通过显式增强模态内语义学习同时保持跨模态对齐，成功解决了MLLMs在模态偏见和细粒度特征提取方面的不足，提高了多模态关键词生成任务的鲁棒性。

Abstract: Multimodal keyphrase generation (MKP) aims to extract a concise set of keyphrases that capture the essential meaning of paired image-text inputs, enabling structured understanding, indexing, and retrieval of multimedia data across the web and social platforms. Success in this task demands effectively bridging the semantic gap between heterogeneous modalities. While multimodal large language models (MLLMs) achieve superior cross-modal understanding by leveraging massive pretraining on image-text corpora, we observe that they often struggle with modality bias and fine-grained intra-modal feature extraction. This oversight leads to a lack of robustness in real-world scenarios where multimedia data is noisy, along with incomplete or misaligned modalities. To address this problem, we propose AimKP, a novel framework that explicitly reinforces intra-modal semantic learning in MLLMs while preserving cross-modal alignment. AimKP incorporates two core innovations: (i) Progressive Modality Masking, which forces fine-grained feature extraction from corrupted inputs by progressively masking modality information during training; (ii) Gradient-based Filtering, that identifies and discards noisy samples, preventing them from corrupting the model's core cross-modal learning. Extensive experiments validate AimKP's effectiveness in multimodal keyphrase generation and its robustness across different scenarios.

</details>


### [16] [ZO-ASR: Zeroth-Order Fine-Tuning of Speech Foundation Models without Back-Propagation](https://arxiv.org/abs/2512.01267)
*Yuezhang Peng,Yuxin Liu,Yao Li,Sheng Wang,Fei Wen,Xie Chen*

Main category: cs.MM

TL;DR: ZO-ASR是一种内存高效的零阶优化方法，通过前向传播估计梯度，避免反向传播和激活内存，使ASR模型微调仅需推理内存。


<details>
  <summary>Details</summary>
Motivation: 预训练语音基础模型的微调通常需要大量GPU内存，限制了在计算资源受限或梯度不可访问场景下的应用。

Method: 提出ZO-ASR方法，使用零阶优化通过前向传播估计梯度，避免反向传播和激活内存。结合SGD优化器，实现仅需推理内存的ASR模型微调。

Result: 在监督领域适应任务中，ZO-ASR相比零样本基线相对词错误率降低18.9%，优于现有零阶方法；在无监督测试时适应任务中，性能略低于一阶优化器Adam。

Conclusion: ZO-ASR为计算资源受限或梯度不可访问场景下的ASR模型微调提供了可行的无反向传播解决方案。

Abstract: Fine-tuning pre-trained speech foundation models for Automatic Speech Recognition (ASR) is prevalent, yet constrained by substantial GPU memory requirements. We introduce ZO-ASR, a memory-efficient Zeroth-Order (ZO) method that avoids Back-Propagation (BP) and activation memory by estimating gradients via forward passes. When combined with SGD optimizer, ZO-ASR-SGD fine-tunes ASR models using only inference memory. Our evaluation spans supervised and unsupervised tasks. For Supervised Domain Adaptation on Whisper-Large-V3, ZO-ASR's multiple query mechanism enhances robustness and achieves up to an 18.9\% relative Word Error Rate reduction over zero-shot baselines, outperforming existing ZO methods. For unsupervised Test-Time Adaptation on Wav2Vec2-Base, ZO-ASR exhibits moderately lower performance compared to first-order optimizer Adam. Our BP-free approach provides a viable solution for fine-tuning ASR models in computationally resource-constrained or gradient-inaccessible scenarios.

</details>


### [17] [PSA-MF: Personality-Sentiment Aligned Multi-Level Fusion for Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.01442)
*Heng Xie,Kang Zhu,Zhengqi Wen,Jianhua Tao,Xuefei Liu,Ruibo Fu,Changsheng Li*

Main category: cs.MM

TL;DR: 提出了一种基于人格-情感对齐的多层次融合框架，用于多模态情感分析，通过引入人格特征和多层次融合策略提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法存在两个主要问题：1）在单模态特征提取阶段只提取浅层信息，忽略了不同人格之间的情感差异；2）在融合阶段直接合并各模态特征，没有考虑特征层面的差异。这些问题影响了模型的识别性能。

Method: 提出了人格-情感对齐的多层次融合框架：1）在特征提取阶段引入人格特征，首次提出人格-情感对齐方法从文本模态获取个性化情感嵌入；2）在融合阶段提出新颖的多层次融合方法，通过多模态预融合和多层次增强融合策略逐步整合文本、视觉和音频模态的情感信息。

Result: 在两个常用数据集上进行了多次实验评估，取得了最先进的结果。

Conclusion: 提出的框架通过考虑人格差异和多层次融合策略，有效提升了多模态情感分析的性能，为解决现有方法的局限性提供了新的思路。

Abstract: Multimodal sentiment analysis (MSA) is a research field that recognizes human sentiments by combining textual, visual, and audio modalities. The main challenge lies in integrating sentiment-related information from different modalities, which typically arises during the unimodal feature extraction phase and the multimodal feature fusion phase. Existing methods extract only shallow information from unimodal features during the extraction phase, neglecting sentimental differences across different personalities. During the fusion phase, they directly merge the feature information from each modality without considering differences at the feature level. This ultimately affects the model's recognition performance. To address this problem, we propose a personality-sentiment aligned multi-level fusion framework. We introduce personality traits during the feature extraction phase and propose a novel personality-sentiment alignment method to obtain personalized sentiment embeddings from the textual modality for the first time. In the fusion phase, we introduce a novel multi-level fusion method. This method gradually integrates sentimental information from textual, visual, and audio modalities through multimodal pre-fusion and a multi-level enhanced fusion strategy. Our method has been evaluated through multiple experiments on two commonly used datasets, achieving state-of-the-art results.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [18] [Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions](https://arxiv.org/abs/2512.00004)
*Jihang Li,Bing Xu,Zulong Chen,Chuanfei Xu,Minping Chen,Suyu Liu,Ying Zhou,Zeyi Wen*

Main category: cs.IR

TL;DR: 提出基于LLM和角色感知MoE网络的人才搜索框架，通过多任务学习优化招聘效果，实现显著业务价值


<details>
  <summary>Details</summary>
Motivation: 现有人才搜索方法难以捕捉细粒度的职位偏好和招聘者行为差异，且容易受到主观判断的噪声影响

Method: 1) 利用LLM从职位描述和历史招聘数据中提取细粒度招聘信号；2) 使用角色感知多门MoE网络捕捉不同招聘者角色的行为差异；3) 引入多任务学习模块联合优化CTR、CVR和简历匹配相关性

Result: 在真实招聘数据和在线A/B测试中，CTR相对AUC提升1.70%，CVR提升5.97%，点击转化率提升17.29%，预计每年节省数百万人民币成本

Conclusion: 该框架显著提升了人才搜索效果，减少了对外部招聘渠道的依赖，具有重要的商业应用价值

Abstract: Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY.

</details>


### [19] [Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking](https://arxiv.org/abs/2512.00007)
*Jingyi Huang,Yuyi Yang,Mengmeng Ji,Charles Alba,Sheng Zhang,Ruopeng An*

Main category: cs.IR

TL;DR: SAFE是一个结合大语言模型和检索增强生成的代理系统，用于长篇幅COVID-19虚假信息的自动化事实核查，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: COVID-19信息疫情需要可扩展的事实核查解决方案，以准确可靠地处理长篇虚假信息，解决大语言模型在一致性和可解释性方面的局限性。

Method: SAFE系统包含两个代理：一个用于声明提取，另一个使用LOTR-RAG（基于13万份COVID-19研究文档）进行声明验证。增强变体SAFE (LOTR-RAG + SRAG)加入了Self-RAG通过查询重写来优化检索。

Result: 在50篇假新闻文章（包含246个标注声明）上评估，SAFE系统在所有指标上显著优于基线LLMs（p < 0.001）。SAFE (LOTR-RAG)在一致性（0.629）、有用性（3.640）、清晰度（3.800）和真实性（3.526）方面表现最佳。添加SRAG略微降低了整体性能。

Conclusion: SAFE通过解决LLM在一致性和可解释性方面的限制，在长篇幅COVID-19事实核查方面实现了显著改进。核心LOTR-RAG设计比SRAG增强变体更有效，为可扩展的虚假信息缓解提供了坚实基础。

Abstract: The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.

</details>


### [20] [Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System](https://arxiv.org/abs/2512.00313)
*Zhitong Guan,Yi Wang*

Main category: cs.IR

TL;DR: 该研究比较了传统搜索引擎与LLM驱动的搜索系统在搜索行为和认知结果上的差异，探讨生成式AI如何影响信息检索和学习过程。


<details>
  <summary>Details</summary>
Motivation: LLM正在重塑信息检索，但传统关键词搜索在多步推理和探索性学习任务中存在局限。虽然ChatGPT等LLM搜索界面引入了新能力，但对其如何影响用户查询制定、信息导航和知识构建的实证理解仍然有限。

Method: 比较研究设计：在两个环境中对比搜索行为和认知结果——标准搜索引擎与LLM驱动的搜索系统。研究考察搜索策略、查询制定、评估行为的差异，以及LLM使用对理解、知识整合和批判性思维的影响。

Result: 研究结果为生成式AI如何塑造信息寻求过程提供了见解，但具体结果未在摘要中详细说明。

Conclusion: 该研究为信息检索、人机交互和技术支持学习领域的持续讨论做出贡献，帮助理解LLM如何影响搜索行为和认知结果。

Abstract: Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.

</details>


### [21] [Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation](https://arxiv.org/abs/2512.00367)
*Aparajitha Allamraju,Maitreya Prafulla Chitale,Hiranmai Sri Adibhatla,Rahul Mishra,Manish Shrivastava*

Main category: cs.IR

TL;DR: 论文提出两种高效的语义分块方法PSC和MFC，在PubMed数据上训练，显著提升RAG系统的检索和生成质量


<details>
  <summary>Details</summary>
Motivation: 传统固定长度和递归分块方法会产生任意、不连贯的片段，无法保持语义结构，而语义分块对生成质量的影响尚未充分探索

Method: 提出两种语义分块方法：投影相似度分块(PSC)和度量融合分块(MFC)，在PubMed数据上使用三种嵌入模型训练，并建立评估框架通过PubMedQA与PubMed Central全文文章评估分块效果

Result: PSC在MRR指标上实现24倍检索改进，在PubMedQA上获得更高的Hits@k，尽管在单一领域训练，PSC和MFC在多个数据集上表现出良好的跨领域泛化能力

Conclusion: 语义分块方法特别是PSC能持续提供优越性能，显著提升RAG系统的检索和生成质量

Abstract: Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.

</details>


### [22] [PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing](https://arxiv.org/abs/2512.00439)
*Xiaoshan Yu,Ziwei Huang,Shangshang Yang,Ziwen Wang,Haiping Ma,Xingyi Zhang*

Main category: cs.IR

TL;DR: 提出PEOAT框架，用于一次性自适应测试(OAT)，通过个性化引导的进化算法优化试题组合选择，解决传统CAT在实时性和资源受限场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统计算机化自适应测试(CAT)在实时性、大规模评估和敏感领域(如心理评估)中存在局限性，交互成本高且易受噪声干扰，需要一种能在一次性选择中确定最优试题组合的方法。

Method: 提出PEOAT框架：1) 个性化感知初始化策略，结合考生能力与试题难度差异，使用多策略采样构建多样化初始种群；2) 认知增强进化框架，包含模式保持交叉和认知引导变异；3) 多样性感知环境选择机制，平衡多样性与适应性。

Result: 在两个数据集上的实验验证了PEOAT的有效性，并通过案例研究揭示了有价值的洞察，表明该方法能够高效地为每位考生选择最优的固定试题集。

Conclusion: PEOAT框架成功解决了传统CAT在时间敏感或资源受限环境中的适用性问题，通过一次性自适应测试(OAT)任务，为智能教育评估提供了更高效、更实用的解决方案。

Abstract: With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.

</details>


### [23] [DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems](https://arxiv.org/abs/2512.00596)
*Jiahao Tian,Zhenkai Wang*

Main category: cs.IR

TL;DR: 提出一个融合多模态与协同知识的去噪框架，通过端到端协同训练和对比学习目标，有效利用LLM生成的高维噪声特征进行推荐


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统难以有效利用LLM生成的高维、噪声多的多模态特征，传统方法将这些特征作为静态输入，与核心推荐任务脱节

Method: 1) 将降维直接集成到推荐模型中，实现端到端协同训练；2) 引入包含协同过滤信号的对比学习目标，在潜在空间中融合多模态和协同知识

Result: 实验证明该方法具有优越的判别能力，这种集成融合和去噪策略对于实现最先进性能至关重要

Conclusion: 为在推荐系统中有效利用LLM提供了基础范式，通过深度融合多模态和协同知识进行表示去噪

Abstract: Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.

</details>


### [24] [ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation](https://arxiv.org/abs/2512.00679)
*Yi Zhang,Yiwen Zhang,Yu Wang,Tong Chen,Hongzhi Yin*

Main category: cs.IR

TL;DR: ProEx提出多面画像外推框架，通过生成多个用户/物品画像来捕捉复杂意图，利用思维链推理构建不同画像，通过环境概念揭示用户偏好不变性，显著提升推荐模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的推荐方法使用单一用户/物品画像可能无法全面捕捉复杂意图，且LLM的不稳定性可能导致偏见或误解，反而损害推荐性能。需要更全面的多面表征方法。

Method: 提出ProEx框架：1) 利用思维链推理为每个用户和物品构建多个不同画像；2) 将新画像映射为语义向量，从原始画像位置外推探索更广语言空间；3) 引入环境概念，每个环境代表所有画像的线性组合，最小化环境间差异以揭示用户偏好不变性。

Result: 在三个数据集上对三种判别式方法和三种生成式方法进行实验，结果表明ProEx显著提升了这些基础推荐模型的性能。

Conclusion: 多面画像外推能更全面捕捉用户和物品特征，通过环境概念揭示偏好不变性，为基于LLM的推荐系统提供了有效框架，显著提升推荐性能。

Abstract: The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.

</details>


### [25] [SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG](https://arxiv.org/abs/2512.00772)
*Hyunseok Ryu,Wonjune Shin,Hyun Park*

Main category: cs.IR

TL;DR: SHRAG是一个新颖的检索增强生成框架，通过LLM作为查询策略器将自然语言查询转换为结构化布尔查询，结合多语言查询扩展和嵌入模型，在ScienceON挑战中实现高效跨语言问答。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统面临两个主要挑战：1）构建高质量检索系统需要专业知识；2）RAG处理速度相对较慢，因为它同时包含检索和生成阶段。需要一种能够无缝集成信息检索和RAG，同时确保精确检索性能的解决方案。

Method: SHRAG框架使用大型语言模型作为查询策略器，自动将非结构化自然语言查询转换为逻辑结构化的搜索查询，然后执行布尔检索以模拟专家人类搜索过程。同时结合多语言查询扩展和多语言嵌入模型，在多语言数据集环境中实现高效跨语言问答。

Result: 实验结果表明，结合逻辑检索能力和生成推理的SHRAG方法能够显著提高RAG系统的准确性和可靠性。此外，SHRAG超越了传统的以文档为中心的检索方法，展示了能够为查询提供直接可靠响应的新搜索范式潜力。

Conclusion: SHRAG通过将LLM作为查询策略器，实现了自然语言查询到结构化布尔查询的自动转换，结合多语言能力，不仅提高了RAG系统的准确性和可靠性，还为信息检索领域提供了新的搜索范式，能够直接提供可靠响应。

Abstract: Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.

</details>


### [26] [Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search](https://arxiv.org/abs/2512.00968)
*Ziyang Zeng,Heming Jing,Jindong Chen,Xiangli Li,Hongyu Liu,Yixuan He,Zhengyu Li,Yige Sun,Zheyong Xie,Yuqing Yang,Shaosheng Cao,Jun Fan,Yi Wu,Yao Hu*

Main category: cs.IR

TL;DR: 提出基于强化学习的推理增强相关性模型，通过分步优势掩码策略提升开放域搜索中的相关性建模性能


<details>
  <summary>Details</summary>
Motivation: 传统相关性模型输出标量分数或直接预测标签，缺乏可解释性且难以建模复杂相关性信号。现有基于推理的生成式相关性模型依赖大量人工标注数据，泛化能力有限，且领域无关的自由推理过于通用，难以处理开放域搜索中的多样性和模糊性

Method: 将小红书搜索中的相关性建模重新定义为推理任务，提出基于强化学习的训练框架：1) 将实际业务相关性标准融入多步推理提示设计；2) 提出分步优势掩码(SAM)轻量级过程监督策略，通过改进信用分配有效学习这些标准；3) 将大规模RL调优模型蒸馏为适合实际搜索系统的轻量版本

Result: 在工业数据集上进行广泛实验，并通过在线A/B测试验证了方法的有效性

Conclusion: 通过强化学习增强的推理框架能够提升相关性模型的性能和可解释性，同时通过模型蒸馏实现了工业部署

Abstract: Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.

</details>


### [27] [Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions](https://arxiv.org/abs/2512.01171)
*Tao Xue,Yanwu Yang,Panyu Zhai*

Main category: cs.IR

TL;DR: 该论文对在线广告中的转化率预测进行了全面的文献综述，将现有模型分为六类，分析了技术演进关系，总结了各模型性能，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 转化率预测在广告决策中至关重要，但现有研究缺乏对方法演进和技术关系的系统性梳理，需要全面的文献综述来指导未来研究。

Method: 对CVR预测文献进行系统性综述，将现有模型按技术分为六类，分析各类技术框架、优缺点及其在CVR预测中的应用，并总结在公开和专有数据集上的性能表现。

Result: 发现先前研究的性能评估结果不一致，识别出语义增强、归因增强、去偏的CVR预测以及CTR与CVR联合建模是未来有前景的研究方向。

Conclusion: 该综述为CVR预测领域的研究者和从业者提供了有价值的参考和见解，有助于推动该领域的进一步发展。

Abstract: Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.

</details>


### [28] [Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives](https://arxiv.org/abs/2512.01179)
*Shan Gao,Yanwu Yang*

Main category: cs.IR

TL;DR: Bench-CTR是一个统一的CTR预测基准平台，提供灵活的接口和评估协议，通过实验发现高阶模型优于低阶模型，LLM模型具有显著的数据效率，CTR模型性能在2015-2016年有显著提升后进入缓慢进展阶段。


<details>
  <summary>Details</summary>
Motivation: 当前CTR预测领域缺乏统一的基准平台和评估标准，导致模型开发和评估存在困难。需要设计一个统一的架构来提供灵活的数据集和模型组件接口，并建立全面的评估协议来校准CTR预测模型的性能。

Method: 设计Bench-CTR平台架构，提供灵活的数据集和模型组件接口；构建包含真实和合成数据集、指标分类、标准化程序和实验指南的评估协议系统；在三个公共数据集和两个合成数据集上对传统多元统计到现代LLM模型进行对比研究。

Result: 实验结果显示：(1)高阶模型普遍优于低阶模型，但优势因指标和数据集而异；(2)LLM模型具有显著的数据效率，仅使用2%训练数据即可达到其他模型的性能；(3)CTR模型性能在2015-2016年显著提升，随后进入缓慢进展阶段，这一趋势在不同数据集中一致。

Conclusion: Bench-CTR基准平台有助于促进CTR预测模型的开发和评估，增强从业者对模型底层机制的理解。该平台代码已开源，预计将推动该领域的研究进展。

Abstract: This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.

</details>


### [29] [Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation](https://arxiv.org/abs/2512.01372)
*Wei Yang,Rui Zhong,Yiqun Chen,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出SSR框架，通过结构化谱推理解决多模态推荐中的模态噪声、语义不一致和图传播不稳定问题，在稀疏和冷启动场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐面临模态特定噪声、语义不一致和图传播不稳定等挑战，现有方法依赖静态滤波或重加权，缺乏对谱结构的推理能力和模态特定可靠性的自适应能力。

Method: 四阶段框架：1) 通过图引导变换将多模态信号分解为谱带；2) 使用谱带掩码调制带级可靠性；3) 通过低秩跨带交互进行超谱推理融合频率线索；4) 通过对比正则化对齐模态特定谱特征。

Result: 在三个真实基准测试中均优于强基线，在稀疏和冷启动设置下表现尤其突出，结构化谱建模提高了鲁棒性并提供了更清晰的频带贡献诊断。

Conclusion: SSR框架通过结构化谱推理有效解决了多模态推荐的关键挑战，为频率感知的多模态推荐提供了新的解决方案，在稀疏和冷启动场景下具有实际应用价值。

Abstract: Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [30] [Evaluating LLMs in Open-Source Games](https://arxiv.org/abs/2512.00371)
*Swadesh Sistla,Max Kleiman-Weiner*

Main category: cs.GT

TL;DR: LLMs在开源游戏（程序提交代替动作的游戏论设定）中展现编程能力，研究评估了LLMs预测分类程序策略的能力，以及在二元和进化设置中达到近似程序均衡的特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在开源游戏中的编程能力，利用程序提交的优势（可解释性、代理间透明度、形式可验证性），探索程序均衡这一在标准形式设置中无法实现的解决方案。

Method: 评估领先的开源和闭源权重LLMs预测和分类程序策略的能力，分析LLM代理在二元和进化设置中达到的近似程序均衡特征，识别策略类型并分析进化适应性。

Result: 发现了收益最大化、合作性和欺骗性策略的出现，描述了这些程序在重复开源游戏中机制的适应过程，分析了它们的比较进化适应性，发现开源游戏是研究和引导多代理困境中合作策略出现的可行环境。

Conclusion: 开源游戏为研究和引导多代理困境中合作策略的出现提供了可行环境，LLMs的编程能力使其能够参与这种游戏论设置，程序均衡提供了传统形式无法实现的解决方案。

Abstract: Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.

</details>


### [31] [Truthful Double Auctions under Approximate VCG: Immediate-Penalty Enforcement in P2P Energy Trading](https://arxiv.org/abs/2512.00513)
*Xun Shao,Ryuuto Shimizu*

Main category: cs.GT

TL;DR: 论文研究了当精确VCG分配计算不可行且重复博弈惩罚不切实际时的真实双边拍卖，提出基于即时惩罚的α近似VCG机制，证明当惩罚超过近似产生的激励缺口时，真实报告成为子博弈完美均衡。


<details>
  <summary>Details</summary>
Motivation: 在分布式市场环境中，精确的VCG机制计算复杂度高，而重复博弈惩罚在实际中难以实施，需要一种既能处理计算限制又能维持真实报告的实用机制。

Method: 提出α近似VCG机制，通过即时惩罚来弥补近似计算产生的激励缺口，使用PPO多智能体强化学习构建P2P智能电网交易环境进行验证，系统性地改变近似精度、容忍度、惩罚幅度和折现因子等参数。

Result: 理论分析表明当即时惩罚超过近似产生的激励缺口（按监测精度缩放）时，真实报告成为子博弈完美均衡；实验结果显示学习到的行为与理论预测高度一致，验证了机制的有效性。

Conclusion: 基于即时惩罚的近似VCG机制为分布式市场环境中的真实行为维持提供了一种实用且透明的解决方案，能够在计算限制下有效激励参与者真实报告估值。

Abstract: This paper examines truthful double auctions when exact VCG allocation is computationally infeasible and repeated-game punishments are impractical. We analyze an $α$-approximate VCG mechanism and show that truthful reporting becomes a subgame-perfect equilibrium when the immediate penalty exceeds the incentive gap created by approximation, scaled by monitoring accuracy. To validate this result, we construct a PPO-based multi-agent reinforcement learning environment for P2P smart-grid trading, where prosumers incur penalties for bidding far from their true valuations. Across systematic experiments varying approximation accuracy, tolerance, penalty magnitude, and discounting, the learned behavior closely matches theoretical predictions. The findings demonstrate that immediate-penalty approximate VCG mechanisms provide a practical and transparent approach to sustaining truthful behavior in distributed market settings.

</details>


### [32] [Stable Voting and the Splitting of Cycles](https://arxiv.org/abs/2512.00616)
*Wesley H. Holliday,Milan Mossé,Chase Norman,Eric Pacuit,Cynthia Wang*

Main category: cs.GT

TL;DR: 本文证明了一个关于Simple Stable Voting (SSV) 与Split Cycle (SC) 关系的猜想：当多数胜利规模都不相同时，SSV是SC的细化。证明在最多6个候选者时成立，但在超过6个候选者时被证伪。


<details>
  <summary>Details</summary>
Motivation: 研究投票理论中循环解决方法的数学性质，特别是验证Holliday和Pacuit提出的关于Simple Stable Voting (SSV) 与Split Cycle (SC) 关系的猜想。

Method: 结合传统数学推理和SAT求解技术：1) 对于最多5个候选者使用传统数学推理；2) 对于6个候选者的证明和7个候选者的反例使用SAT求解；3) 开发了适用于任何仅依赖胜利边际大小排序的投票方法的SAT编码。

Result: 1) 证明了当多数胜利规模都不相同时，SSV是SC的细化这一猜想在最多6个候选者时成立；2) 构造了7个候选者的反例，证明该猜想在超过6个候选者时不成立；3) 开发了通用的SAT编码框架。

Conclusion: 该猜想在有限范围内（最多6个候选者）成立，但在更大规模（7个及以上候选者）下不成立。开发的SAT编码方法具有广泛适用性，可用于测试任何仅依赖胜利边际排序的投票方法的性质。

Abstract: Algorithms for resolving majority cycles in preference aggregation have been studied extensively in computational social choice. Several sophisticated cycle-resolving methods, including Tideman's Ranked Pairs, Schulze's Beat Path, and Heitzig's River, are refinements of the Split Cycle (SC) method that resolves majority cycles by discarding the weakest majority victories in each cycle. Recently, Holliday and Pacuit proposed a new refinement of Split Cycle, dubbed Stable Voting, and a simplification thereof, called Simple Stable Voting (SSV). They conjectured that SSV is a refinement of SC whenever no two majority victories are of the same size. In this paper, we prove the conjecture up to 6 alternatives and refute it for more than 6 alternatives. While our proof of the conjecture for up to 5 alternatives uses traditional mathematical reasoning, our 6-alternative proof and 7-alternative counterexample were obtained with the use of SAT solving. The SAT encoding underlying this proof and counterexample is applicable far beyond SC and SSV: it can be used to test properties of any voting method whose choice of winners depends only on the ordering of margins of victory by size.

</details>


### [33] [Price of Anarchy of Multi-Stage Machine Scheduling Games](https://arxiv.org/abs/2512.00733)
*Ho-Lin Chen,Pin-Ju Huang*

Main category: cs.GT

TL;DR: 本文分析了多阶段机器调度博弈的无政府价格，在贪婪选择策略下，单阶段调度无政府价格为2-1/m，多阶段调度无政府价格在[2-1/m, 3-1/m]之间。


<details>
  <summary>Details</summary>
Motivation: 制造业流水线和分布式计算工作流中，作业需要经过多个处理阶段，形成多阶段调度环境。虽然经典单阶段调度中无政府价格已有研究，但多阶段调度中的效率损失尚未被分析。

Method: 假设每个任务采用贪婪策略，在到达每个阶段时分配到负载最小的机器。分析贪婪选择下的均衡状态，而非子博弈完美纳什均衡，因为这对计算能力有限的自私代理或使用最小负载启发式调度的中央调度器更合理。

Result: 1. 单阶段调度中，贪婪选择的无政府价格精确为2-1/m；2. 多阶段调度中，阶段间的完成时间最多增加两倍的最大作业执行时间；3. 多阶段调度在贪婪选择下的无政府价格在[2-1/m, 3-1/m]之间，其中m为单阶段最大机器数。

Conclusion: 多阶段调度在贪婪策略下的效率损失有界，无政府价格上界为3-1/m，为多阶段调度系统的性能分析提供了理论保证。

Abstract: In this paper, we extend the discussion of the price of anarchy of machine scheduling games to a multi-stage machine setting. The multi-stage setting arises naturally in manufacturing pipelines and distributed computing workflows, when each job must traverse a fixed sequence of processing stages. While the classical makespan price of anarchy of $2 - \frac{1}{m}$ has been established for sequential scheduling on identical machines, the efficiency loss in multi-stage scheduling has, to the best of our knowledge, not been previously analyzed. We assume that each task follows a greedy strategy and gets assigned to the least-loaded machine upon arrival at each stage. Notably, we observe that in multi-stage environments, greedy behavior generally does not coincide with a subgame perfect Nash equilibrium. We continue with analyzing the equilibrium under greedy choices, since it is logical for modeling selfish agents with limited computational power, and may also model a central scheduler performing the common least-load scheduling heuristics. Under this model, we first show that in single-stage scheduling, greedy choice again yields an exact price of anarchy of $2 - \frac{1}{m}$. In multi-stage scheduling, we show that the completion time from one stage to the next increases by at most two times the maximum job execution time. Using this relationship, we derived the price of anarchy of multistage scheduling under greedy choices to lie within $[2 - \frac{1}{m}, 3 - \frac{1}{m}]$, where $m$ denote the maximum number of machines in one stage.

</details>


### [34] [Mechanism Design with Spiteful Agents](https://arxiv.org/abs/2512.01021)
*Aditya Aradhye,David Lagziel,Eilon Solan*

Main category: cs.GT

TL;DR: 研究具有恶意偏好的机制设计问题，代理不仅想最大化自身收益，还想降低对手收益。完全刻画了所有IR和IC且抗恶意行为的机制，发现它们都是具有代理排序的阈值机制。在匿名性或效率性要求下，任何此类机制都会退化为空机制（从不分配物品）。


<details>
  <summary>Details</summary>
Motivation: 研究在存在其他关注偏好（特别是恶意偏好）情况下的机制设计问题。现实中的代理可能不仅关心自身收益，还会根据自身收益水平试图降低对手的收益，这种恶意行为对传统机制设计理论提出了挑战。

Method: 采用机制设计理论框架，分析具有恶意偏好的代理行为。首先刻画所有满足个体理性(IR)和激励相容(IC)且能抵抗恶意行为的机制形式，证明它们都是具有代理排序的阈值机制。然后证明在匿名性或效率性要求下的不可能性结果。

Result: 1. 所有IR、IC且抗恶意行为的机制都是具有代理排序的阈值机制；2. 在匿名性要求下，任何此类机制都会退化为空机制（从不分配物品）；3. 在效率性要求下，同样会退化为空机制；4. 部分结果扩展到多物品设置。

Conclusion: 在存在恶意偏好的情况下，机制设计面临严重挑战。即使要求最基本的IR和IC性质，加上匿名性或效率性条件，可行的机制都会退化为完全不分配物品的空机制，这揭示了在其他关注偏好自然存在的情况下拍卖物品的困难。

Abstract: We study a mechanism-design problem in which spiteful agents strive to not only maximize their rewards but also, contingent upon their own payoff levels, seek to lower the opponents' rewards. We characterize all individually rational (IR) and incentive-compatible (IC) mechanisms that are immune to such spiteful behavior, showing that they take the form of threshold mechanisms with an ordering of the agents. Building on this characterization, we prove two impossibility results: under either anonymity or efficiency, any such IR and IC mechanism collapses to the null mechanism, which never allocates the item to any agent. Leveraging these findings, we partially extend our analysis to a multi-item setup. These results illuminate the challenges of auctioning items in the natural presence of other-regarding preferences.

</details>


### [35] [Autodeleveraging: Impossibilities and Optimization](https://arxiv.org/abs/2512.01112)
*Tarun Chitra*

Main category: cs.GT

TL;DR: ADL机制面临三难困境：无法同时满足交易所偿付能力、收入和交易者公平性。本文提出了三种优化ADL机制，实证显示Hyperliquid过度使用ADL约8倍，造成约6.3亿美元不必要损失。


<details>
  <summary>Details</summary>
Motivation: 尽管永续期货在加密衍生品市场占据主导地位（2024年交易量超60万亿美元），但自动去杠杆化（ADL）作为最后损失社会化机制，尚未有正式研究。本文旨在首次对ADL进行严格建模，分析其机制设计问题。

Method: 建立了ADL的严格数学模型，证明了ADL机制面临偿付能力、收入和公平性的三难困境。提出了三类优化ADL机制，并在Hyperliquid数据集（2025年10月10日，12分钟内处理21亿美元头寸）上进行实证分析。

Result: 证明了ADL机制不可能同时满足交易所偿付能力、收入和交易者公平性。实证显示Hyperliquid的ADL使用过度约8倍，造成约6.3亿美元不必要损失。Binance的ADL过度使用情况可能更严重。

Conclusion: 优化的ADL机制能显著减少交易者利润损失，同时保持交易所偿付能力。研究为ADL机制设计提供了理论和实证基础，揭示了当前实践中存在的效率损失问题。

Abstract: Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \emph{trilemma}: no policy can simultaneously satisfy exchange \emph{solvency}, \emph{revenue}, and \emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by approximately $8\times$ relative to our optimal policy, imposing roughly \$630 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [Efficiently Sampling Interval Patterns from Numerical Databases](https://arxiv.org/abs/2512.00105)
*Djawad Bekkoucha,Lamine Diop,Abdelkader Ouali,Bruno Crémilleux,Patrice Boizumault*

Main category: cs.DB

TL;DR: Fips和HFips是首个针对数值数据库中区间模式的采样方法，分别按频率和频率×超体积进行比例采样，有效解决了模式采样中的长尾现象。


<details>
  <summary>Details</summary>
Motivation: 模式采样已成为大型数据库中信息发现的有前景方法，但目前缺乏针对数值数据库中区间模式的采样方法。数值数据中准确确定覆盖每个对象的区间模式数量是一个关键挑战。

Method: Fips使用多步采样程序，按频率比例采样区间模式。HFips扩展了Fips，按频率和超体积的乘积比例采样区间模式。两种方法都解决了数值数据中准确计算覆盖每个对象的区间模式数量的挑战。

Result: 实验证明Fips和HFips能获得高质量的模式，并对长尾现象具有鲁棒性。形式化证明表明Fips按频率比例采样，HFips按频率和超体积的乘积比例采样。

Conclusion: Fips和HFips是首个针对数值数据库区间模式的采样方法，能有效处理长尾现象，为数值数据分析提供了实用的模式采样工具。

Abstract: Pattern sampling has emerged as a promising approach for information discovery in large databases, allowing analysts to focus on a manageable subset of patterns. In this approach, patterns are randomly drawn based on an interestingness measure, such as frequency or hyper-volume. This paper presents the first sampling approach designed to handle interval patterns in numerical databases. This approach, named Fips, samples interval patterns proportionally to their frequency. It uses a multi-step sampling procedure and addresses a key challenge in numerical data: accurately determining the number of interval patterns that cover each object. We extend this work with HFips, which samples interval patterns proportionally to both their frequency and hyper-volume. These methods efficiently tackle the well-known long-tail phenomenon in pattern sampling. We formally prove that Fips and HFips sample interval patterns in proportion to their frequency and the product of hyper-volume and frequency, respectively. Through experiments on several databases, we demonstrate the quality of the obtained patterns and their robustness against the long-tail phenomenon.

</details>


### [37] [MatBase algorithm for translating (E)MDM schemes into E-R data models](https://arxiv.org/abs/2512.00662)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出将数学数据模型转换为实体关系模型的线性算法，证明其正确性、完备性和半最优性，并在MatBase系统中实现


<details>
  <summary>Details</summary>
Motivation: 需要将数学数据模型（Elementary Mathematical Data Model）转换为实体关系模型，以便在基于多种数据模型的智能知识库系统MatBase中实现互操作性

Method: 设计伪代码算法，将数学数据模型模式转换为实体关系模型，证明算法具有线性复杂度、正确性、完备性和半最优性

Result: 算法成功应用于家谱树的数学数据模型转换，并在MatBase原型系统中实现，增加了主要功能特性

Conclusion: 提出的算法有效实现了数学数据模型到实体关系模型的转换，为多模型数据库系统提供了实用的转换工具

Abstract: This paper presents a pseudocode algorithm for translating (Elementary) Mathematical Data Model schemes into Entity-Relationship data models. We prove that this algorithm is linear, sound, complete, and semi-optimal. As an example, we apply this algorithm to an (Elementary) Mathematical Data Model scheme for a genealogical tree sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models.

</details>


### [38] [PG-HIVE: Hybrid Incremental Schema Discovery for Property Graphs](https://arxiv.org/abs/2512.01092)
*Sofia Sideri,Georgia Troullinou,Elisjana Ymeralli,Vasilis Efthymiou,Dimitris Plexousakis,Haridimos Kondylakis*

Main category: cs.DB

TL;DR: PG-HIVE是一个用于属性图自动模式发现的框架，通过LSH和聚类技术发现隐式节点/边类型、属性数据类型、约束和基数，支持增量更新，在准确性和效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 属性图已成为表示复杂互联数据的实际标准，但其无模式特性给数据集成、探索、可视化和高效查询带来了重大挑战，需要自动模式发现来弥补这一差距。

Method: 采用局部敏感哈希(LSH)结合基于属性和标签的聚类技术，识别大规模结构相似性；支持增量模式发现，避免新数据到达时的重复计算。

Result: PG-HIVE在准确性上比现有方法提升高达65%（节点）和40%（边），效率上快1.95倍，显著优于最先进解决方案。

Conclusion: PG-HIVE框架能够有效发现属性图中的隐式模式结构，支持增量更新，释放了模式感知属性图管理的全部潜力。

Abstract: Property graphs have rapidly become the de facto standard for representing and managing complex, interconnected data, powering applications across domains from knowledge graphs to social networks. Despite the advantages, their schema-free nature poses major challenges for integration, exploration, visualization, and efficient querying. To bridge this gap, we present PG-HIVE, a novel framework for automatic schema discovery in property graphs. PG-HIVE goes beyond existing approaches by uncovering latent node and edge types, inferring property datatypes, constraints, and cardinalities, and doing so even in the absence of explicit labeling information. Leveraging a unique combination of Locality-Sensitive Hashing with property- and label-based clustering, PG-HIVE identifies structural similarities at scale. Moreover, it introduces incremental schema discovery, eliminating costly recomputation as new data arrives. Through extensive experimentation, we demonstrate that PG-HIVE consistently outperforms state-of-the-art solutions, in both accuracy (by up to 65% for nodes and 40% for edges), and efficiency (up to 1.95x faster execution), unlocking the full potential of schema-aware property graph management.

</details>


### [39] [DuckDB on xNVMe](https://arxiv.org/abs/2512.01490)
*Marius Ottosen,Magnus Keinicke Parlo,Philippe Bonnet*

Main category: cs.DB

TL;DR: DuckDB通过xNVMe库直接访问NVMe SSD，绕过传统POSIX文件系统，实现异步I/O操作，显著提升扫描查询性能


<details>
  <summary>Details</summary>
Motivation: 研究DuckDB与本地存储的交互方式，探索SSD与DuckDB的协同设计可能性，特别是如何直接访问NVMe SSD以提升性能

Method: 使用xNVMe库绕过POSIX文件接口、文件系统和块管理器，直接对SSD逻辑块地址空间发起异步I/O操作，利用DuckDB缓冲管理器的块特性

Result: 相比DuckDB基线，即使是简单的TPC-H表扫描查询也能获得显著加速，加速效果随规模因子增加而提升，Linux NVMe直通进一步改善性能

Conclusion: 直接访问NVMe SSD能显著提升DuckDB性能，未来工作包括更全面的实验研究、结合原始NVMe访问与传统POSIX接口的灵活方案，以及DuckDB与SSD的协同设计

Abstract: DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs.

</details>


### [40] [LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database](https://arxiv.org/abs/2512.01693)
*Honghui Kim,Dohoon Kim,Jihan Kim*

Main category: cs.DB

TL;DR: LitMOF是一个基于大语言模型的多智能体框架，通过从原始文献验证晶体学信息并交叉验证数据库条目来修复MOF结构错误，构建了包含118,464个计算就绪结构的LitMOF-DB数据库。


<details>
  <summary>Details</summary>
Motivation: MOF数据库中近一半条目存在显著结构错误，这些错误会通过高通量筛选和机器学习工作流传播，限制了数据驱动MOF发现的可靠性。修复这些错误非常困难，因为需要整合分散在文献中的晶体学文件、合成描述和上下文证据。

Method: 开发LitMOF框架，这是一个大语言模型驱动的多智能体系统，能够直接从原始文献验证晶体学信息，并与数据库条目进行交叉验证来修复结构错误。应用于实验MOF数据库（CSD MOF子集）。

Result: 构建了LitMOF-DB，包含118,464个计算就绪结构，修复了最新CoRE MOF数据库中69%（6,161个MOF）的无效结构。系统还发现了12,646个实验报道但现有资源中缺失的MOF，显著扩展了已知实验设计空间。

Conclusion: 这项工作为自校正科学数据库建立了可扩展的途径，并为材料科学中的LLM驱动数据管理提供了一个通用范式。

Abstract: Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science.

</details>


### [41] [Answering Constraint Path Queries over Graphs](https://arxiv.org/abs/2512.01733)
*Heyang Li,Anthony Widjaja Lin,Domagoj Vrgoč*

Main category: cs.DB

TL;DR: 提出了一种在属性图上支持SMT约束的路径查询语言，扩展了正则路径查询，并实现了高效的查询评估算法


<details>
  <summary>Details</summary>
Motivation: 约束是强大的声明式构造，可以方便地限制可能无限域上的变量值。需要扩展属性图上的路径查询语言，使其能够支持数据属性上的SMT约束

Method: 提出了一种约束路径查询语言，扩展了正则路径查询（RPQs），支持等式约束和线性实数算术（LRA）约束。开发了高效的查询评估算法，利用宏状态优化和理论特定技术，并集成高度优化的SMT求解器来解决路径上的约束

Result: 在MillenniumDB开源图引擎中实现了该算法，支持属性图查询和GQL。在真实世界环境中的广泛实证评估证明了该方法的可行性

Conclusion: 成功开发了一种支持SMT约束的路径查询语言和高效评估算法，能够有效处理属性图上的约束路径查询问题

Abstract: Constraints are powerful declarative constructs that allow users to
  conveniently restrict variable values that potentially range over an
  infinite domain. In this paper, we propose a constraint path query language
  over property graphs,
  which extends Regular Path Queries (RPQs) with SMT constraints on data
  attributes in the form of equality constraints and Linear
  Real Arithmetic (LRA) constraints. We provide efficient algorithms
  for evaluating such path queries over property graphs, which exploits
  optimization of macro-states (among others, using theory-specific
  techniques).
  In particular, we demonstrate how such an algorithm may effectively utilize
  highly optimized SMT solvers for resolving such constraints over paths.
  We implement our algorithm in MillenniumDB, an open-source graph engine
  supporting property graph queries and GQL. Our extensive empirical
  evaluation in a real-world setting demonstrates the viability of our
  approach.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [42] [An Information Geometric Approach to Fairness With Equalized Odds Constraint](https://arxiv.org/abs/2512.00135)
*Amirreza Zamani,Ayfer Özgür,Mikael Skoglund*

Main category: cs.IT

TL;DR: 研究在无法直接访问敏感属性S和任务T的情况下，设计满足均衡赔率（equalized odds）的公平机制，通过信息几何方法将问题转化为二次规划


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，代理通常无法直接访问敏感属性S和任务T，只能通过相关数据X来设计公平表示Y。需要解决在S和T不可直接观测的情况下，如何设计满足均衡赔率约束的机制

Method: 假设马尔可夫链S-X-Y和T-X-Y成立，对条件分布P_{S|Y}施加几何结构约束，当阈值较小时使用信息几何方法近似互信息，将公平机制设计问题转化为二次规划。对于其他情况，基于矩阵最大奇异值和向量推导简单低复杂度的下界

Result: 在特定约束下获得了闭式解，推导了基于矩阵最大奇异值的低复杂度下界，并通过数值示例与最优解进行了比较

Conclusion: 提出的方法能够在无法直接访问敏感属性和任务的情况下，有效设计满足均衡赔率约束的公平机制，为实际应用中的公平机器学习提供了可行的解决方案

Abstract: We study the statistical design of a fair mechanism that attains equalized odds, where an agent uses some useful data (database) $X$ to solve a task $T$. Since both $X$ and $T$ are correlated with some latent sensitive attribute $S$, the agent designs a representation $Y$ that satisfies an equalized odds, that is, such that $I(Y;S|T) =0$. In contrast to our previous work, we assume here that the agent has no direct access to $S$ and $T$; hence, the Markov chains $S - X - Y$ and $T - X - Y$ hold. Furthermore, we impose a geometric structure on the conditional distribution $P_{S|Y}$, allowing $Y$ and $S$ to have a small correlation, bounded by a threshold. When the threshold is small, concepts from information geometry allow us to approximate mutual information and reformulate the fair mechanism design problem as a quadratic program with closed-form solutions under certain constraints. For other cases, we derive simple, low-complexity lower bounds based on the maximum singular value and vector of a matrix. Finally, we compare our designs with the optimal solution in a numerical example.

</details>


### [43] [Fast list recovery of univariate multiplicity and folded Reed-Solomon codes](https://arxiv.org/abs/2512.00248)
*Rohan Goyal,Prahladh Harsha,Mrinal Kumar,Ashutosh Shankar*

Main category: cs.IT

TL;DR: 本文为折叠Reed-Solomon码和单变量重数码设计了近线性时间列表恢复算法，填补了之前列表解码算法无法直接扩展到列表恢复的空白。


<details>
  <summary>Details</summary>
Motivation: Goyal等人的工作为折叠Reed-Solomon码和单变量重数码提供了近线性时间的列表解码算法，但这些算法似乎与列表解码问题紧密绑定，无法自然地扩展到列表恢复问题。本文旨在解决这一局限，为这些码设计高效的列表恢复算法。

Method: 基于Goyal等人使用的格基思想，增加了一个关键技术成分：构造了单变量多项式环上适当结构的格，这些格能够"捕获"这些码的列表恢复问题。通过这种构造实现了近线性时间算法。

Result: 成功设计了时间复杂度为$\tilde{O}(n)$的列表恢复算法，其中$n$是码的块长度。这些算法能够达到折叠Reed-Solomon码和单变量重数码的容量极限。

Conclusion: 本文填补了列表解码算法向列表恢复扩展的空白，为折叠Reed-Solomon码和单变量重数码提供了高效的列表恢复算法，达到了容量极限且具有近线性时间复杂度。

Abstract: A recent work of Goyal, Harsha, Kumar and Shankar gave nearly linear time algorithms for the list decoding of Folded Reed-Solomon codes (FRS) and univariate multiplicity codes up to list decoding capacity in their natural setting of parameters. A curious aspect of this work was that unlike most list decoding algorithms for codes that also naturally extend to the problem of list recovery, the algorithm in the work of Goyal et al. seemed to be crucially tied to the problem of list decoding. In particular, it wasn't clear if their algorithm could be generalized to solve the problem of list recovery FRS and univariate multiplicity codes in near linear time.
  In this work, we address this question and design $\tilde{O}(n)$-time algorithms for list recovery of Folded Reed-Solomon codes and univariate Multiplicity codes up to capacity, where $n$ is the blocklength of the code. For our proof, we build upon the lattice based ideas crucially used by Goyal et al. with one additional technical ingredient - we show the construction of appropriately structured lattices over the univariate polynomial ring that \emph{capture} the list recovery problem for these codes.

</details>


### [44] [ORBGRAND Is Exactly Capacity-achieving via Rank Companding](https://arxiv.org/abs/2512.00347)
*Zhuang Li,Wenyi Zhang*

Main category: cs.IT

TL;DR: CDF-ORBGRAND算法通过根据信道可靠性的逆CDF压缩排序，能够精确达到二进制输入无记忆信道的对称容量，并应用于BICM系统实现BICM容量。


<details>
  <summary>Details</summary>
Motivation: ORBGRAND算法虽然能有效利用软信息且适合硬件实现，但在一般二进制输入无记忆信道下是否能达到对称容量尚不明确。本文旨在通过改进ORBGRAND算法，使其能够精确达到对称容量，并扩展到高阶调制的BICM系统。

Method: 提出CDF-ORBGRAND算法，通过根据信道可靠性的逆累积分布函数（CDF）压缩ORBGRAND中的排序，使算法能够精确达到二进制输入无记忆信道的对称容量。然后将该方法应用于BICM系统，考虑BICM和ORBGRAND带来的失配解码效应。

Result: CDF-ORBGRAND算法能够精确达到一般二进制输入无记忆信道的对称容量（互信息）。在BICM系统中，该算法能够实现BICM容量，该容量最初是通过将BICM视为一组独立并行信道推导出来的。

Conclusion: 通过根据信道可靠性的逆CDF压缩排序，CDF-ORBGRAND算法能够精确达到二进制输入无记忆信道的对称容量，并能成功应用于BICM系统实现BICM容量，为高效解码算法设计提供了理论保证。

Abstract: Among guessing random additive noise decoding (GRAND) algorithms, ordered reliability bits GRAND (ORBGRAND) has attracted considerable attention due to its efficient use of soft information and suitability for hardware implementation. It has also been shown that ORBGRAND achieves a rate very close to the capacity over additive white Gaussian noise channels with antipodal inputs. In this work, it is further established that, via suitably companding the ranks in ORBGRAND according to the inverse cumulative distribution function (CDF) of channel reliability, the resulting CDF-ORBGRAND algorithm exactly achieves the mutual information of general binary-input memoryless channels under symmetric input distribution, i.e., the symmetric capacity. This result is then applied to bit-interleaved coded modulation (BICM) systems to handle high-order input constellations. Via considering the effects of mismatched decoding due to both BICM and ORBGRAND, it is shown that CDF-ORBGRAND is capable of achieving the BICM capacity, which was initially derived by treating BICM as a set of independent parallel channels.

</details>


### [45] [The Information Theory of Similarity](https://arxiv.org/abs/2512.00378)
*Nikit Phadke*

Main category: cs.IT

TL;DR: 论文建立了见证相似性系统(REWA)与香农信息论之间的精确数学等价关系，证明了见证重叠就是互信息，REWA比特复杂度源于信道容量限制，排序保持编码服从率失真约束。


<details>
  <summary>Details</summary>
Motivation: 统一五十年来相似性搜索研究（从布隆过滤器到局部敏感哈希再到神经检索），揭示这些方法本质上是在为关系数据开发信息论，为语义相似性建立物理单位（互信息比特），将搜索视为通信过程。

Method: 建立REWA与香农信息论的数学等价性证明：证明见证重叠等于互信息，REWA比特复杂度源于信道容量限制，排序保持编码服从率失真约束，推导出REWA复杂度O(Δ^{-2} log N)的基本下界。

Result: 证明了REWA复杂度O(Δ^{-2} log N)是最优的：没有任何编码方案能用更少的比特保持相似性排序。建立了语义相似性具有物理单位（互信息比特），搜索是通信（通过噪声信道传输查询），检索系统面临与香农信道编码定理类似的基本容量限制。

Conclusion: 该框架统一了相似性搜索与信息论，揭示了五十年来相似性搜索研究本质上是在为关系数据开发信息论，为理解语义相似性、搜索过程和检索系统容量限制提供了理论基础。

Abstract: We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(Δ^{-2} \log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem.

</details>


### [46] [Robust Precoding for Resilient Cell-Free Networks](https://arxiv.org/abs/2512.00531)
*Saeed Mashdour,André R. Flores,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出一种用于弹性无蜂窝大规模MIMO系统的鲁棒预编码器设计，在总发射功率约束下最小化期望信号均方误差和残余干扰泄漏功率的加权和，利用CSI误差统计增强对信道状态信息不完美的弹性。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO系统在实际部署中面临信道状态信息不完美的挑战，传统预编码器对CSI误差敏感，需要设计鲁棒预编码器来增强系统弹性，平衡性能和计算效率。

Method: 采用交替优化算法，以最小均方误差型解初始化，迭代优化预编码器，同时保持低计算复杂度并确保快速收敛，预编码器设计考虑了CSI误差统计。

Result: 数值结果表明，所提方法显著优于传统线性预编码器，在性能和计算效率之间提供了有效平衡。

Conclusion: 提出的鲁棒预编码器设计能够有效增强无蜂窝大规模MIMO系统对CSI不完美的弹性，在保证计算效率的同时提升系统性能。

Abstract: This paper presents a robust precoder design for resilient cell-free massive MIMO (CF-mMIMO) systems that minimizes the weighted sum of desired signal mean square error (MSE) and residual interference leakage power under a total transmit power constraint. The proposed robust precoder incorporates channel state information (CSI) error statistics to enhance resilience against CSI imperfections. We employ an alternating optimization algorithm initialized with a minimum MSE-type solution, which iteratively refines the precoder while maintaining low computational complexity and ensuring fast convergence. Numerical results show that the proposed method significantly outperforms conventional linear precoders, providing an effective balance between performance and computational efficiency.

</details>


### [47] [Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation](https://arxiv.org/abs/2512.00711)
*Loc X. Nguyen,Ji Su Yoon,Huy Q. Le,Yu Qiao,Avi Deb Raha,Eui-Nam Huh,Walid Saad,Dusit Niyato,Zhu Han,Choong Seon Hong*

Main category: cs.IT

TL;DR: 提出一个新颖的联邦学习框架来解决语义通信系统中训练数据来自不同领域时的领域偏移问题，通过构建全局表示来对齐本地特征，并采用领域感知聚合解决样本数量不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 语义通信依赖深度学习模型进行联合源信道编码，需要大量训练数据。联邦学习可以分布式训练模型，但传统联邦学习方法在客户端数据来自不同领域时会出现灾难性性能下降。需要解决领域偏移和样本数量不平衡问题。

Method: 提出新颖的联邦学习框架：1）构建全局表示来对齐客户端本地特征，以保留不同数据领域的语义；2）识别并解决样本数量多的客户端领域主导问题，采用领域感知聚合方法。这是首个考虑图像重建任务中语义通信系统领域偏移的工作。

Result: 模拟结果显示，在1dB信噪比下，提出的方法在三个领域上的PSNR值比模型对比联邦学习（MOON）框架高出0.5，且随着信道质量改善，这一差距继续扩大。

Conclusion: 提出的联邦学习框架有效解决了语义通信系统中领域偏移问题，通过全局表示对齐和领域感知聚合，显著提升了图像重建性能，特别是在信道质量改善时优势更加明显。

Abstract: Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves.

</details>


### [48] [Movable Antenna Empowered Near-Field Sensing via Antenna Position Optimization](https://arxiv.org/abs/2512.00758)
*Yushen Wang,Weidong Mei,Xin Wei,Zhi Chen,Boyu Ning*

Main category: cs.IT

TL;DR: 可移动天线阵列通过优化天线位置显著提升6G近场感知性能，相比固定天线在角度和距离联合估计方面有巨大优势


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向超高频率发展，近场传播越来越普遍。传统固定位置天线在近场感知中存在性能限制，而可移动天线技术能够通过改变天线阵列几何结构来增强感知能力，特别是在联合估计目标角度和距离信息方面具有巨大潜力。

Method: 研究一维和二维可移动天线阵列在近场感知中的应用。首先针对一维阵列，分别研究仅角度估计和仅距离估计的简化情况，推导最坏情况克拉美罗下界。然后联合优化天线位置以最小化CRB，对于角度和距离联合估计的复杂非凸问题，提出基于离散采样的方法进行顺序更新。最后将方法扩展到二维阵列。

Result: 数值结果表明，提出的可移动天线增强近场感知方案显著优于传统固定位置天线。角度和距离联合估计产生的阵列几何结构与单独角度/距离估计或远场感知不同，验证了可移动天线在近场感知中的独特优势。

Conclusion: 可移动天线技术通过优化天线阵列几何结构，能够显著提升6G网络在近场环境下的感知性能，特别是在角度和距离联合估计方面展现出巨大潜力，为未来6G网络的感知能力提供了新的技术方向。

Abstract: Movable antenna (MA) technology exhibits great promise for enhancing the sensing capabilities of future sixth-generation (6G) networks due to its capability to alter antenna array geometry. With the growing prevalence of near-field propagation at ultra-high frequencies, this paper focuses on the application of one-dimensional (1D) and two-dimensional (2D) MA arrays for near-field sensing to jointly estimate the angle and distance information about a target. First, for the 1D MA array scenario, to gain insights into MA-enhanced near-field sensing, we investigate two simplified cases with only angle-of-arrival (AoA) or distance estimation, respectively, assuming that the other information is already known. The worst-case Cramer-Rao bounds (CRBs) on the mean square errors (MSEs) of the AoA estimation and the distance estimation are derived in these two cases. Then, we jointly optimize the positions of the MAs within the 1D array to minimize these CRBs and derive their closed-form solutions, which yield an identical array geometry to MA-enhanced far-field sensing. For the more challenging joint AoA and distance estimation, since the associated worst-case CRB is a highly complex and non-convex function with respect to the MA positions, a discrete sampling-based approach is proposed to sequentially update the MA positions and obtain an efficient suboptimal solution. Furthermore, we investigate the worst-case CRB minimization problems for a 2D MA array under various conditions and extend our proposed algorithms to solve them efficiently. Numerical results demonstrate that the proposed MA-enhanced near-field sensing scheme dramatically outperforms conventional fixed-position antennas (FPAs). Moreover, the joint angle and distance estimation results in a different array geometry from that in the individual estimation of angle/distance or far-field sensing.

</details>


### [49] [Rate-Splitting Multiple Access for Secure Near-Field Integrated Sensing and Communication](https://arxiv.org/abs/2512.00770)
*Jiasi Zhou,Chintha Tellambura,Geoffrey Ye Li*

Main category: cs.IT

TL;DR: 提出RSMA增强的近场ISAC安全传输方案，利用RSMA公共流同时管理干扰、抑制窃听和作为感知序列，在满足CRB约束下最大化最小保密率，显著降低硬件成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 近场ISAC利用距离相关信道变化进行联合距离和角度估计，但全数字架构硬件成本过高，而混合模拟数字架构会降低波束聚焦精度，导致能量泄漏，加剧用户间干扰和窃听风险。

Method: 提出RSMA增强的安全传输方案，首次利用RSMA公共流同时实现干扰管理、窃听抑制和感知序列功能。采用块坐标下降法分解为三个子问题：全数字波束聚焦器优化（使用WMMSE、二次变换和泰勒展开）、模拟波束聚焦器优化、数字波束聚焦器优化。

Result: 仿真结果表明：(1)在RF链减少16倍的情况下实现接近全数字波束聚焦性能；(2)相比传统波束聚焦和远场安全方案提供更优的保密性能；(3)在保密性能损失可忽略的情况下实现高精度感知。

Conclusion: RSMA增强的近场ISAC安全传输方案能有效解决混合架构的波束聚焦精度损失和窃听风险问题，在显著降低硬件成本的同时保持优异的保密性能和感知精度。

Abstract: Near-field integrated sensing and communication (ISAC) leverages distance-dependent channel variations for joint distance and angle estimation. However, full-digital architectures have prohibitive hardware costs, making hybrid analog-digital (HAD) designs the primary alternative. Nevertheless, such architectures compromise beamfocusing precision and lead to energy leakage, which exacerbates inter-user interference and increases eavesdropping risks. To address these challenges, this paper proposes a rate-splitting multiple access (RSMA)-enhanced secure transmit scheme for near-field ISAC. For the first time, it exploits the common stream in RSMA to concurrently (i) flexibly manage interference, (ii) act as artificial noise to suppress eavesdropping, and (iii) serve as sensing sequences. The objective is to maximize the minimum secrecy rate while satisfying the angle and distance Cramer-Rao Bound (CRB) constraints. This results in a hard, non-convex optimization problem, and we employ block coordinate descent to decompose it into three sub-problems with lower computational complexity. In the first stage of optimizing fully digital beamfocusers, we develop an iterative solution using weighted minimum mean-squared error (WMMSE), quadratic transform, and Taylor expansion methods, thus avoiding conventional semidefinite relaxation. In the second and third stages, the analog and digital beamfocusers are optimized in closed form. Simulation results show that the proposed scheme (1) achieves near full-digital beamfocusing performance with a 16-fold reduction in RF chains, (2) provides superior secrecy performance compared to conventional beamfocusing-only and far-field security schemes, and (3) enables high-accuracy sensing with negligible loss in secrecy performance.

</details>


### [50] [Hybrid Beamfocusing Design for RSMA-Enhanced Near-Field Secure Communications](https://arxiv.org/abs/2512.00774)
*Jiasi Zhou,Huiyun Xia,Chuan Wu,Chintha Tellambura*

Main category: cs.IT

TL;DR: 提出一种基于速率分割多址（RSMA）的近场通信安全传输方案，通过混合模拟-数字天线架构实现波束聚焦与人工噪声的联合优化，提升物理层安全性。


<details>
  <summary>Details</summary>
Motivation: 近场球面波前虽然能实现类似聚光灯的波束聚焦，减少能量泄漏，但在混合模拟-数字天线架构下，仅靠波束聚焦无法提供可靠的安全保护。需要新的方法来增强近场通信的物理层安全性。

Method: 提出RSMA增强的安全传输方案，将公共流设计为双重用途：既向合法用户传输信息，又作为人工噪声干扰窃听。通过联合优化模拟波束聚焦器、数字波束聚焦器和公共保密速率分配，最大化最小保密速率。采用基于惩罚的交替优化算法，将变量分为三个模块分别求解。

Result: 仿真结果表明：1）方案能以远少于射频链路的数量接近全数字波束聚焦性能；2）优于传统的仅波束聚焦和远场安全方案；3）在不显著降低通信速率的情况下保持安全性。

Conclusion: 提出的RSMA增强方案能有效提升近场通信的物理层安全性，在混合模拟-数字架构下实现了安全性与通信性能的良好平衡，为未来近场通信系统提供了可行的安全解决方案。

Abstract: Near-field spherical wavefronts enable spotlight-like beam focusing to mitigate unintended energy leakage, creating new opportunities for physical-layer security (PLS). However, under hybrid analog-digital (HAD) antenna architectures, beamfocusing alone may not provide foolproof privacy protection due to reduced focusing precision. To address this issue, this paper proposes a rate-splitting multiple access (RSMA)-enhanced secure transmit scheme for near-field communications with fully-connected or sub-connected HAD architectures. In the proposed scheme, the common stream is designed for dual purposes, delivering the desired message for legitimate users while acting as artificial noise to disrupt eavesdropping. The primary objective is to maximize the minimum secrecy rate by jointly optimizing the analog beamfocuser, digital beamfocuser, and common secrecy rate allocation. To solve the formulated non-convex problem, we develop a penalty-based alternating optimization algorithm. Specifically, the variables are partitioned into three blocks, where one block is solved via a surrogate optimization method, while the others are updated in closed form. Simulation results reveal that our transmit scheme: (1) approaches fully digital beamfocusing with substantially fewer radio frequency chains, (2) outperforms conventional beamfocusing-only and far-field security schemes, and (3) preserves secrecy without significantly compromising communication rates.

</details>


### [51] [Age Optimal Sampling and Routing under Intermittent Links and Energy Constraints](https://arxiv.org/abs/2512.00985)
*Adem Utku Atasayar,Aimin Li,Çağrı Arı,Elif Uysal*

Main category: cs.IT

TL;DR: 该论文针对卫星-地面融合网络中链路延迟分布不同、间歇可用和能耗异构的特点，提出联合优化采样和路由决策的方法，以最小化单调非线性信息年龄(AoI)。


<details>
  <summary>Details</summary>
Motivation: 实际系统（如卫星-地面融合网络）中的链路具有不同的延迟分布、间歇可用性和异构能耗特性，这对保持及时且节能的状态更新提出了挑战。链路可用性限制了可行的传输路径，而路由决策决定了实际的延迟和能耗。

Method: 将问题建模为无限时域约束半马尔可夫决策过程(CSMDP)，提出高效的嵌套算法Bisec-ReaVI。揭示了最优策略结构：最优路由策略是适应路径可用性和平均延迟的单调切换策略；最优采样策略是分段线性等待策略。

Result: 在卫星-地面融合路由场景的数值实验中，所提方案能有效平衡能耗和信息新鲜度。揭示了反直觉的洞察：即使具有更高平均延迟、更高延迟方差或更低可用性的路径，在最小化AoI单调函数方面仍能发挥关键作用。

Conclusion: 该研究为具有异构链路特性的网络提供了联合优化采样和路由决策的理论框架，揭示了最优策略的结构特性，并通过卫星-地面融合网络验证了方案的有效性。

Abstract: Links in practical systems, such as satellite-terrestrial integrated networks, exhibit distinct delay distributions, intermittent availability, and heterogeneous energy costs. These characteristics pose significant challenges to maintaining timely and energy-efficient status updates. While link availability restricts feasible transmission routes, routing decisions determine the actual delay and energy expenditure. This paper tackles these challenges by jointly optimizing sampling and routing decisions to minimize monotonic, nonlinear Age of Information (AoI). The proposed formulation incorporates key system features, including multiple routes with correlated random delays, stochastic link availability, and route-dependent energy consumption. We model the problem as an infinite-horizon constrained semi-Markov decision process (CSMDP) with a hybrid state-action space and develop an efficient nested algorithm, termed Bisec-ReaVI, to solve this problem. We reveal a well-defined jointly optimal policy structure: (i) the optimal routing policy is a monotonic handover policy that adapts to the availability of routes and their mean delays; and (ii) the optimal sampling policy is a piecewise linear waiting policy, with at most "N choose 2 + N" breakpoints given N routes. Numerical experiments in a satellite-terrestrial integrated routing scenario demonstrate that the proposed scheme efficiently balances energy usage and information freshness, and reveal a counter-intuitive insight: even routes with higher average delay, higher delay variance, or lower availability can still play a critical role in minimizing monotonic functions of AoI.

</details>


### [52] [Value of Communication in Goal-Oriented Semantic Communications: A Pareto Analysis](https://arxiv.org/abs/2512.01454)
*Jiping Luo,Bowen Li,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 该论文研究通信受限系统中信息的价值，通过马尔可夫源的远程估计场景，分析通信预算与估计性能之间的最优权衡，提出SPLIT算法计算帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 新兴信息物理系统在严格通信约束下运行，无法可靠传输大量机器类型数据流。原始测量常包含相关或冗余成分，有效运行依赖于选择对系统目标有价值的信息，而非传输所有数据。需要超越准确性，从目标导向的语义通信角度评估信息价值。

Method: 采用帕累托分析方法，研究马尔可夫源远程估计的典型场景，通过信息价值度量量化信息相关性。分析通信预算与估计性能之间的最优权衡，定义通信价值为帕累托前沿的绝对斜率。利用该场景下帕累托前沿的特殊结构（严格递减、凸、分段线性），提出SPLIT算法高效构建完整帕累托前沿。

Result: 证明了在该设置下帕累托前沿具有严格递减、凸、分段线性的结构，其斜率由有限个常数控制。每个帕累托最优操作点可实现为两个平稳确定性策略的凸组合。SPLIT算法能够高效且可证明最优地构建完整帕累托前沿。

Conclusion: 该工作为通信受限系统中的信息价值评估提供了理论框架和实用算法，通过帕累托分析揭示了通信预算与估计性能之间的最优权衡关系，SPLIT算法使得实际系统能够根据通信成本选择最优操作策略。

Abstract: Emerging cyber-physical systems increasingly operate under stringent communication constraints that preclude the reliable transmission of their extensive machine-type data streams. Since raw measurements often contain correlated or redundant components, effective operation depends not on transmitting all available data but on selecting the information that contributes to achieving the objectives of the system. Beyond accuracy, goal-oriented semantic communication assesses the \emph{value of information} and aims to generate and transmit only what is relevant and at the right time. Motivated by this perspective, this work studies the \emph{value of communication} through the canonical setting of remote estimation of Markov sources, where a value-of-information measure quantifies the relevance of information. We investigate how optimal estimation performance varies with the available communication budget and determine the marginal performance gain attributable to additional communication. Our approach is based on a \emph{Pareto analysis} that characterizes the complete set of policies that achieve optimal trade-offs between estimation performance and communication cost. The value of communication is defined as the absolute slope of the resulting Pareto frontier. Although computing this frontier is non-trivial, we demonstrate that in our setting it admits a notably tractable structure: it is strictly decreasing, convex, and piecewise linear, and its slope is governed by a finite collection of constants. Moreover, each Pareto-optimal operating point is realizable as a convex combination of two stationary deterministic policies, enabling practical implementation. Leveraging these structural insights, we introduce SPLIT, an efficient and provably optimal algorithm for constructing the complete Pareto frontier.

</details>


### [53] [Generating Random Hyperfractal Cities](https://arxiv.org/abs/2512.01505)
*Geoffrey Deperle,Philippe Jacquet*

Main category: cs.IT

TL;DR: 提出基于分形和超分形的随机城市生成模型，通过瓦片结构连接多个超分形区域，提供城市网络交互建模工具


<details>
  <summary>Details</summary>
Motivation: 解决街道网络交互建模的挑战，扩展简单分形模型以更好地描述城市结构，特别是连接多个区域形成更全面的城市表示

Method: 在瓦片结构上分布超分形构建随机城市，连接多个超分形区域，利用分形分析将城市分割为不同区域

Result: 开发了基于该模型的随机城市数值生成工具，能够创建更全面的城市表示并实现区域分割

Conclusion: 提出的超分形模型扩展了传统分形方法，为街道网络交互建模提供了更强大的框架和实用工具

Abstract: This paper focuses on the challenge of interactively modeling street networks. In this work, we extend the simple fractal model, which is particularly useful for describing small cities or individual districts, by constructing random cities based on a tiling structure over which hyperfractals are distributed. This approach enables the connection of multiple hyperfractal districts, providing a more comprehensive urban representation. Furthermore, we demonstrate how this decomposition can be used to segment a city into distinct districts through fractal analysis. Finally, we present tools for the numerical generation of random cities following this model.

</details>


### [54] [Storage capacity of perceptron with variable selection](https://arxiv.org/abs/2512.01861)
*Yingying Xu,Masayuki Ohzeki,Yoshiyuki Kabashima*

Main category: cs.IT

TL;DR: 本文通过统计力学方法研究了感知机的最优变量选择问题，发现最优变量选择可以超越Cover-Gardner理论的随机子集界限，为区分数据中的真实结构与虚假相关性提供了定量标准。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的一个核心挑战是在高维数据中区分真实结构与偶然相关性。本文针对感知机这一神经网络计算的基础模型，研究在何种条件下可以通过最优变量选择实现完美分类，从而为区分数据中的真实结构提供理论依据。

Method: 基于统计力学中的副本方法，开发了一种枚举能够实现完美模式分类的变量组合的方法。该方法可以计算感知机在最优选择M=ρN个变量时能够完美分类P=αN个随机模式的条件。

Result: 研究发现，最优变量选择可以超越Cover-Gardner理论中随机子集的界限（α<2ρ）。通过副本方法，作者能够精确计算感知机在最优变量选择下的存储容量，这为区分数据中的真实结构与虚假相关性提供了定量标准。

Conclusion: 最优变量选择能够显著提升感知机的分类能力，超越传统随机子集的界限。这一发现不仅为机器学习中区分真实结构与虚假相关性提供了理论框架，还为具有稀疏非对称连接的联想记忆模型的存储容量计算提供了新方法。

Abstract: A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $α$ and the variable selection ratio $ρ$ for which a simple perceptron can perfectly classify $P = αN$ random patterns by optimally selecting $M = ρN$ variables out of $N$ variables. While the Cover--Gardner theory establishes that a random subset of $ρN$ dimensions can separate $αN$ random patterns if and only if $α< 2ρ$, we demonstrate that optimal variable selection can surpass this bound by developing a method, based on the replica method from statistical mechanics, for enumerating the combinations of variables that enable perfect pattern classification. This not only provides a quantitative criterion for distinguishing true structure in the data from spurious regularities, but also yields the storage capacity of associative memory models with sparse asymmetric couplings.

</details>


### [55] [A Dual Approach for Hierarchical Information-Theoretic Tree Abstractions](https://arxiv.org/abs/2512.01985)
*Daniel T. Larsson,Dipankar Maity,Panagiotis Tsiotras*

Main category: cs.IT

TL;DR: 本文建立了基于信息瓶颈方法的两种树抽象问题（硬约束和软约束）之间的形式化联系，分析了它们的等价条件，并提出了利用树相变和整数规划松弛的算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立信息瓶颈方法中两种树抽象问题（硬约束和软约束）之间的形式化联系，确定它们何时等价，从而为树搜索问题提供统一的理论框架。

Method: 1. 使用拉格朗日松弛和对偶理论将硬约束问题的对偶函数与Q树搜索中的Q函数联系起来；2. 分析树相变与对偶问题解的关系；3. 提出利用树相变知识求解对偶变量的算法；4. 提出基于整数规划松弛的替代方法，证明约束矩阵完全单模性以获得线性规划。

Result: 1. 建立了硬约束和软约束树抽象问题之间的理论联系；2. 证明了硬约束问题的整数规划松弛具有完全单模性，可转化为线性规划；3. 提出了两种求解对偶变量的算法；4. 通过实证结果验证了理论发展。

Conclusion: 本文成功建立了信息瓶颈方法中两种树抽象问题的形式化联系，提出了有效的算法框架，并通过理论分析和实证验证了方法的有效性，为树搜索问题提供了新的理论视角和实用工具。

Abstract: In this paper, we consider establishing a formal connection between two distinct tree-abstraction problems inspired by the information-bottleneck (IB) method. Specifically, we consider the hard- and soft-constrained formulations that have recently appeared in the literature to determine the conditions for which the two approaches are equivalent. Our analysis leverages concepts from Lagrangian relaxation and duality theory to relate the dual function of the hard-constrained problem to the Q-function employed in Q-tree search and shows the connection between tree phase transitions and solutions to the dual problem obtained by exploiting the problem structure. An algorithm is proposed that employs knowledge of the tree phase transitions to find a setting of the dual variable that solves the dual problem. Furthermore, we present an alternative approach to select the dual variable that leverages the integer programming formulation of the hard-constrained problem and the strong duality of linear programming. To obtain a linear program, we establish that a relaxation of the integer programming formulation of the hard-constrained tree-search problem has the integrality property by showing that the program constraint matrix is totally unimodular. Empirical results that corroborate the theoretical developments are presented and discussed throughout.

</details>
