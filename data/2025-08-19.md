<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 9]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 15]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Bayesian Learning for Pilot Decontamination in Cell-Free Massive MIMO](https://arxiv.org/abs/2508.11791)
*Christian Forsch,Zilu Zhao,Dirk Slock,Laura Cottatellucci*

Main category: cs.IT

TL;DR: 提出基于期望传播(EP)的联合信道估计与数据检测算法，用于缓解无蜂窝大规模MIMO系统中的导频污染问题，该算法具有分布式、可扩展性强、对导频污染鲁棒性好的特点。


<details>
  <summary>Details</summary>
Motivation: 导频污染(PC)在用户设备(UE)分配的导频序列非完全正交时产生，特别是在序列重用的情况下，这会严重影响无蜂窝大规模MIMO系统的上行链路性能。

Method: 开发了一种改进的双线性EP算法，该算法采用分布式架构，通过联合信道估计和数据检测来缓解导频污染效应。同时引入新的UE级导频污染量化指标。

Result: 该算法在性能上优于最先进的贝叶斯学习算法，表现出对导频污染的强鲁棒性。研究发现使用非正交导频可能比共享正交序列获得更好性能，且算法性能随新提出的PC指标单调下降。

Conclusion: 提出的EP-based JCD算法为理解和通过迭代JCD算法管理导频污染提供了有价值的理论和实践工具，新提出的PC量化指标能够有效反映系统性能变化。

Abstract: Pilot contamination (PC) arises when the pilot sequences assigned to user
equipments (UEs) are not mutually orthogonal, eventually due to their reuse. In
this work, we propose a novel expectation propagation (EP)-based joint channel
estimation and data detection (JCD) algorithm specifically designed to mitigate
the effects of PC in the uplink of cell-free massive multiple-input
multiple-output (CF-MaMIMO) systems. This modified bilinear-EP algorithm is
distributed, scalable, demonstrates strong robustness to PC, and outperforms
state-of-the-art Bayesian learning algorithms. Through a comprehensive
performance evaluation, we assess the performance of Bayesian learning
algorithms for different pilot sequences and observe that the use of
non-orthogonal pilots can lead to better performance compared to shared
orthogonal sequences. Motivated by this analysis, we introduce a new metric to
quantify PC at the UE level. We show that the performance of the considered
algorithms degrades monotonically with respect to this metric, providing a
valuable theoretical and practical tool for understanding and managing PC via
iterative JCD algorithms.

</details>


### [2] [A Law of Emergence: Maximum Causal Power at the Mesoscale](https://arxiv.org/abs/2508.12016)
*Liang Chen*

Main category: cs.IT

TL;DR: 这篇论文提出并验证了一个关于出现性的中观尺度峰值定理，通过有效信息(EI_ℓ)量化系统在不同空间尺度下的因果力，发现它在中观尺度ℓ^*处达到最大值，这是噪声平均和局域性限制之间基本拉换的结果。


<details>
  <summary>Details</summary>
Motivation: 复杂系统普遍存在出现现象，屏幕动力学从局部交互中涉淀而来，但一直缺乏预测这一过程的定量宋律。需要建立一个能够识别出现自然尺度的可验证、基于第一原理的定律。

Method: 定义系统在空间尺度ℓ下的因果力（有效信息EI_ℓ），通过目标化的最大界杂度干预和其结果之间的互信息来测量。从这个定义出发，推导并证明了中观尺度峰值定理：对于具有局部交互的广泛系统类别，EI_ℓ不是单调的，而是在中观尺度ℓ^*处存在严格的最大值。

Result: 在2D伊戈模型近临界点和基于代理的集体行为模型两个独立域中，都提供了量化的、可复现的证据。统计模型选择决定性地确认了预测的单峰形峰值。

Conclusion: 这项工作建立了一个可证伪的第一原理定律，能够识别出现的自然尺度，为有效理论的发现提供了量化基础。这个定律揭示了噪声平均和局域性限制之间的基本拉换关系，这是出现现象的根本原因。

Abstract: Complex systems universally exhibit emergence, where macroscopic dynamics
arise from local interactions, but a predictive law governing this process has
been absent. We establish and verify such a law. We define a system's causal
power at a spatial scale, $\ell$, as its Effective Information (EI$_\ell$),
measured by the mutual information between a targeted, maximum-entropy
intervention and its outcome. From this, we derive and prove a Middle-Scale
Peak Theorem: for a broad class of systems with local interactions, EI$_\ell$
is not monotonic but exhibits a strict maximum at a mesoscopic scale $\ell^*$.
This peak is a necessary consequence of a fundamental trade-off between
noise-averaging at small scales and locality-limited response at large scales.
We provide quantitative, reproducible evidence for this law in two distinct
domains: a 2D Ising model near criticality and a model of agent-based
collective behavior. In both systems, the predicted unimodal peak is decisively
confirmed by statistical model selection. Our work establishes a falsifiable,
first-principles law that identifies the natural scale of emergence, providing
a quantitative foundation for the discovery of effective theories.

</details>


### [3] [Cylindrical RIS-Assisted Low-Complexity Transmission with Differentiated Visible Regions Exploiting Statistical CSI](https://arxiv.org/abs/2508.12229)
*Wenjun Teng,Weicong Chen,Yiping Zuo,Wankai Tang,Shi Jin*

Main category: cs.IT

TL;DR: 本文提出一种基于均匀圆柱数组的RIS阶段移优化方法，通过将RIS单元分为用户专用和多用户共享单元，降低了多用户场景下的优化复杂度，同时保持了系统性能。


<details>
  <summary>Details</summary>
Motivation: 在多用户场景下，重构智能表面(RIS)的最优阶段移设计面临两大挑战：大规模RIS元素引发的高维优化问题，以及共享RIS反射导致的多用户信号持续耦合问题。

Method: 利用均匀圆柱数组(UCA)布局的RIS可见区域降低阶段移设计复杂度。将RIS元素分为用户专用单元和多用户共享单元两类，通过迭代优化多用户共享单元的阶段移，并基于式解直接配置用户专用单元的阶段移。

Result: 所提方法显著降低了优化复杂度，数值模拟结果显示，与传统的均匀平面数组RIS相比，该方法在系统性能和计算效率方面都有显著提升。

Conclusion: 通过UCA架构和元素分类策略，成功解决了多用户RIS阶段移优化的复杂性问题，为6G网络中低空经济的可持续发展提供了有效的连接性保障方案。

Abstract: Reconfigurable intelligent surfaces (RIS), recognized as a critical enabler
for 6G networks, exhibit unprecedented capabilities in electromagnetic wave
manipulation and wireless channel reconfiguration. By leveraging existing
network infrastructure, RIS can cost-effectively create signal hotspots in
low-altitude environments, ensuring robust connectivity to support the
sustainable development of the low-altitude economy. However, achieving optimal
phase shift design in multi-user scenarios faces two major challenges: the
high-dimensional optimization introduced by massive RIS elements, and the
persistent coupling of multi-user signals caused by shared RIS reflections.
This paper utilize the visible region of an RIS arranged as the uniform
cylindrical array (UCA) to reduce the complexity of phase shift design. Under
the UCA architecture, RIS elements are categorized into two types:
user-specific units and multi-user shared units. We then determine the optimal
phase shifts by iteratively optimizing the phase shifts of multi-user shared
units while directly configuring those of user-specific units based on a
derived closed-form solution. The proposed approach significantly reduces
optimization complexity, which is further corroborated by numerical simulation
results demonstrating its substantial impact on both system performance and
computational efficiency compared to the conventional RIS with uniform planar
array.

</details>


### [4] [Age of Semantic Information-Aware Wireless Transmission for Remote Monitoring Systems](https://arxiv.org/abs/2508.12248)
*Xue Han,Biqian Feng,Yongpeng Wu,Xiang-Gen Xia,Wenjun Zhang,Shengli Sun*

Main category: cs.IT

TL;DR: 这篇论文提出了一种新的语义通信指标AoIS（Age of Incorrect Semantics），通过优化语义触发指标、收发信道和语义符号设计，在MIMO通信中有效降低信息时延和误差。


<details>
  <summary>Details</summary>
Motivation: 传统的信息新鲜度指标无法充分考虑语义重要性，需要一种能同时考虑信息时效性和语义准确性的新指标来支持智能化监控系统。

Method: 采用Lyapunov优化将问题转换为低复杂度问题，通过替代优化方法分解为多个子问题，使用SCA算法和零阻塞算法优化收发信道，用突然搜索优化语义触发策略和传输符号设计。

Result: 实验结果显示，在相同的AoIS条件下，该方案能够保留原始信息超过50%，显著超过了基准方法。

Conclusion: AoIS指标能够有效衡量语义通信的性能，通过聚合优化多个参数，在MIMO视频传输中实现了更好的信息保存率和时延性能。

Abstract: Semantic communication is emerging as an effective means of facilitating
intelligent and context-aware communication for next-generation communication
systems. In this paper, we propose a novel metric called Age of Incorrect
Semantics (AoIS) for the transmission of video frames over multiple-input
multiple-output (MIMO) channels in a monitoring system. Different from the
conventional age-based approaches, we jointly consider the information
freshness and the semantic importance, and then formulate a time-averaged AoIS
minimization problem by jointly optimizing the semantic actuation indicator,
transceiver beamformer, and the semantic symbol design. We first transform the
original problem into a low-complexity problem via the Lyapunov optimization.
Then, we decompose the transformed problem into multiple subproblems and adopt
the alternative optimization (AO) method to solve each subproblem.
Specifically, we propose two efficient algorithms, i.e., the successive convex
approximation (SCA) algorithm and the low-complexity zero-forcing (ZF)
algorithm for optimizing transceiver beamformer. We adopt exhaustive search
methods to solve the semantic actuation policy indicator optimization problem
and the transmitted semantic symbol design problem. Experimental results
demonstrate that our scheme can preserve more than 50\% of the original
information under the same AoIS compared to the constrained baselines.

</details>


### [5] [The extended code for a class of generalized Roth-Lempel codes and their properties](https://arxiv.org/abs/2508.12302)
*Zhonghao Liang,Qunying Liao*

Main category: cs.IT

TL;DR: 该论文扩展了广义Roth-Lempel(GRL)码，提出了扩展广义Roth-Lempel(EGRL)码，并研究了其MDS、AMDS和NMDS性质及重量分布


<details>
  <summary>Details</summary>
Motivation: 许多重要编码是通过修改或组合现有编码获得的，本文旨在扩展GRL码并研究其代数性质

Method: 定义EGRL码类，给出特定EGRL码的奇偶校验矩阵，建立MDS/AMDS的充要条件，构造NMDS EGRL码

Result: 建立了EGRL码及其对偶码为MDS或AMDS的充要条件，构造了一类NMDS EGRL码并完全确定了其重量分布

Conclusion: EGRL码是GRL码的有用扩展，具有明确的代数性质，NMDS EGRL码的构造推广了Han等人2023年的工作

Abstract: As we all know, many interesting and important codes are obtained by
modifying or combining existing codes. In this paper, we focus on generalized
Roth-Lempel (in short, GRL) codes and define a class of extended codes, i.e.,
the extended generalized Roth-Lempel (in short, EGRL) code. And then for a
special class of EGRL codes, we give a parity-check matrix and establish a
necessary and sufficient condition for the EGRL code or its dual code to be MDS
or AMDS, respectively. Finally, we construct a class of NMDS EGRL codes which
is the generalization of the constructions given by Han et al. in 2023, and
then completely determine its weight distribution.

</details>


### [6] [Algorithmic Improvements to List Decoding of Folded Reed-Solomon Codes](https://arxiv.org/abs/2508.12548)
*Vikrant Ashvinkumar,Mursalin Habib,Shashank Srivastava*

Main category: cs.IT

TL;DR: 本文改进了折叠Reed-Solomon码的列表解码算法，提出了运行时间为近线性时间Õ_ε(n)的确定性解码器和多项式时间poly(1/ε)·Õ(n)的随机化解码器，显著提升了现有算法的效率。


<details>
  <summary>Details</summary>
Motivation: 折叠Reed-Solomon码是已知能达到列表解码容量的码族，但现有的确定性解码算法运行时间高达n^Ω(1/ε)，随机化解码算法运行时间为exp(1/ε)·Õ(n)，效率较低。需要开发更高效的解码算法。

Method: 提出了新的确定性解码器和随机化解码器算法。确定性解码器采用近线性时间复杂度的设计，随机化解码器通过多项式时间依赖1/ε的优化来实现高效解码。

Result: 确定性解码器运行时间从n^Ω(1/ε)提升到Õ_ε(n)，随机化解码器运行时间从exp(1/ε)·Õ(n)提升到poly(1/ε)·Õ(n)。这是首个实现容量达到且解码时间对1/ε具有多项式依赖的码。

Conclusion: 本文在折叠Reed-Solomon码的解码算法上取得了重要突破，首次实现了容量达到码的确定性近线性时间解码和随机化多项式时间解码，为高效纠错码设计提供了新的技术路径。

Abstract: Folded Reed-Solomon (FRS) codes are a well-studied family of codes, known for
achieving list decoding capacity. In this work, we give improved deterministic
and randomized algorithms for list decoding FRS codes of rate $R$ up to radius
$1-R-\varepsilon$.
  We present a deterministic decoder that runs in near-linear time
$\widetilde{O}_{\varepsilon}(n)$, improving upon the best-known runtime
$n^{\Omega(1/\varepsilon)}$ for decoding FRS codes. Prior to our work, no
capacity achieving code was known whose deterministic decoding could be done in
time $\widetilde{O}_{\varepsilon}(n)$.
  We also present a randomized decoder that runs in fully polynomial time
$\mathrm{poly}(1/\varepsilon) \cdot \widetilde{O}(n)$, improving the best-known
runtime $\mathrm{exp}(1/\varepsilon)\cdot \widetilde{O}(n)$ for decoding FRS
codes. Again, prior to our work, no capacity achieving code was known whose
decoding time depended polynomially on $1/\varepsilon$.

</details>


### [7] [Deep Semantic Inference over the Air: An Efficient Task-Oriented Communication System](https://arxiv.org/abs/2508.12748)
*Chenyang Wang,Roger Olsson,Stefan Forsström,Qing He*

Main category: cs.IT

TL;DR: 基于深度学习的任务导向语义通信框架，通过模型分割和语义特征压缩，在保持85%准确率的同时显著降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统通信传输原始数据效率低下，语义通信能够传输任务相关的含义，构建更高效智能的无线系统。本文通过关注分类性能、计算延迟和通信成本的聚合考虑，探索资源效率与任务准确性之间的交易。

Method: 采用ResNets模型在CIFAR-10和CIFAR-100数据集上进行分类任务模拟。通过在不同位置分割模型来模拟无线信道上的分开推理，并系统性分析不同分割位置和语义特征向量大小对任务准确性和资源效率的影响。

Result: 实验结果显示，通过适当的模型分割和语义特征压缩，系统可以保持基准准确率85%以上，同时显著降低计算负载和通信开销。

Conclusion: 该研究表明，深度学习基于的任务导向语义通信框架能够在保持高准确性的同时有效节省计算和通信资源，为构建高效智能无线系统提供了可行的解决方案。

Abstract: Empowered by deep learning, semantic communication marks a paradigm shift
from transmitting raw data to conveying task-relevant meaning, enabling more
efficient and intelligent wireless systems. In this study, we explore a deep
learning-based task-oriented communication framework that jointly considers
classification performance, computational latency, and communication cost. We
adopt ResNets-based models and evaluate them on the CIFAR-10 and CIFAR-100
datasets to simulate real-world classification tasks in wireless environments.
We partition the model at various points to simulate split inference across a
wireless channel. By varying the split location and the size of the transmitted
semantic feature vector, we systematically analyze the trade-offs between task
accuracy and resource efficiency. Experimental results show that, with
appropriate model partitioning and semantic feature compression, the system can
retain over 85\% of baseline accuracy while significantly reducing both
computational load and communication overhead.

</details>


### [8] [Information-Theoretic Fairness with A Bounded Statistical Parity Constraint](https://arxiv.org/abs/2508.12847)
*Amirreza Zamani,Abolfazl Changizi,Ragnar Thobaben,Mikael Skoglund*

Main category: cs.IT

TL;DR: 该论文研究在满足有界统计公平性约束下设计公平表示的信息论问题，通过最大化任务信息同时限制压缩率和隐私泄漏。


<details>
  <summary>Details</summary>
Motivation: 解决数据表示中公平性和隐私保护的平衡问题，当有用数据X和任务T都与敏感属性S相关时，需要设计既能保持任务效用又能限制敏感信息泄漏的表示Y。

Method: 使用扩展的功能表示引理和强功能表示引理，基于随机化技术，通过随机化有用数据X或敏感数据S来推导下界，并研究特殊情况下边界的紧致性。

Result: 改进了完美统计公平性(ε=0)情况下的现有下界，提出了新的上界，证明了允许非零泄漏可以提高获得的效用，并通过数值示例比较了边界。

Conclusion: 该研究为有界统计公平性和隐私约束下的表示设计提供了信息论框架，展示了在公平性、隐私和效用之间的权衡关系，可解释为有界泄漏和有界速率隐私的代码设计问题。

Abstract: In this paper, we study an information-theoretic problem of designing a fair
representation that attains bounded statistical (demographic) parity. More
specifically, an agent uses some useful data $X$ to solve a task $T$. Since
both $X$ and $T$ are correlated with some sensitive attribute or secret $S$,
the agent designs a representation $Y$ that satisfies a bounded statistical
parity and/or privacy leakage constraint, that is, such that $I(Y;S) \leq
\epsilon$. Here, we relax the perfect demographic (statistical) parity and
consider a bounded-parity constraint. In this work, we design the
representation $Y$ that maximizes the mutual information $I(Y;T)$ about the
task while satisfying a bounded compression (or encoding rate) constraint, that
is, ensuring that $I(Y;X) \leq r$. Simultaneously, $Y$ satisfies the bounded
statistical parity constraint $I(Y;S) \leq \epsilon$. To design $Y$, we use
extended versions of the Functional Representation Lemma and the Strong
Functional Representation Lemma which are based on randomization techniques and
study the tightness of the obtained bounds in special cases. The main idea to
derive the lower bounds is to use randomization over useful data $X$ or
sensitive data $S$. Considering perfect demographic parity, i.e., $\epsilon=0$,
we improve the existing results (lower bounds) by using a tighter version of
the Strong Functional Representation Lemma and propose new upper bounds. We
then propose upper and lower bounds for the main problem and show that allowing
non-zero leakage can improve the attained utility. Finally, we study the bounds
and compare them in a numerical example. The problem studied in this paper can
also be interpreted as one of code design with bounded leakage and bounded rate
privacy considering the sensitive attribute as a secret.

</details>


### [9] [Research on GEO SA-Bi SAR Imaging based on Joint Radar-Communications Waveform](https://arxiv.org/abs/2508.12890)
*Meng Lian,Xu Zhu*

Main category: cs.IT

TL;DR: 本文提出了一种聚焦地球同步轨道单载反平易大空间基线雷达系统与无线通信的聚合设计，通过聚合波形和接收机实现了雷达成像和通信两种功能的同时执行。


<details>
  <summary>Details</summary>
Motivation: 解决日益担巩的频谱资源分配问题，通过雷达-通信聚合技术共享频段资源，同时满足硬件平台共享的需求，提高系统效能和资源利用率。

Method: 使用聚合雷达-通信波形在GEO SA-Bi SAR系统中同时进行成像和无线通信，设计了聚合接收机来实现感知和信令传输两种功能。

Result: 证明了在GEO SA-Bi SAR系统中通过聚合波形和接收机设计同时实现感知和通信功能的可行性。

Conclusion: 聚合雷达-通信波形设计不仅能够减少频谱占用，还能相互提升雷达和通信系统的性能，为雷达-通信聚合系统的实际应用提供了有效的解决方案。

Abstract: Joint radar-communications (JRC) technology has attracted massive attention
for decades, since it can effectively utilize allocated spectral resources by
sharing frequency bands in increasingly crowded environments. In addition, the
growing demand for hardware platform sharing which benefits both
functionalities motivates more cooperation between radar and communication
systems. In order to achieve the coexistence of sensing and communicating
operations, joint systems should be designed to perform both tasks
simultaneously. Developing a joint radar-communications waveform which is
suitable for both functions is extremely crucial for this type of co-design, as
it not only decreases spectral impact, but also benefits performances of both
systems mutually. In this paper, a joint radar-communications waveform is
utilized to perform GEO SA-Bi SAR imaging and wireless communication
simultaneously. We also design a joint radar-communications receiver in this
context to demonstrate feasibility of achieving both sensing and signaling with
GEO SA-Bi SAR system.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: 提出FAE方法从游戏视频中学习神经符号世界模型，用新的DSL语言Retro Coder表示，相比之前方法学习更精确的环境模型和更通用的代码


<details>
  <summary>Details</summary>
Motivation: 传统世界模型通常是神经网络表示，难以迁移学习到的环境动态和解释性，需要更精确和可解释的世界模型表示方法

Method: 有限自动机提取(FAE)方法，从游戏视频中学习神经符号世界模型，使用新的领域特定语言Retro Coder来表示程序

Result: 相比之前的世界模型方法，FAE学习到更精确的环境模型；相比之前的DSL方法，生成更通用的代码

Conclusion: FAE方法能够有效解决世界模型的可迁移性和可解释性问题，通过学习神经符号表示提供了更好的环境建模能力

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [11] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut是一个自动化生成整数规划加速割的框架，结合大语言模型和进化搜索，无需人工干预即可显著提升求解器性能


<details>
  <summary>Details</summary>
Motivation: 整数规划是组合优化的核心但具有NP难特性，传统依赖专家手动设计加速割的方法效率低下且难以自动化

Method: 结合LLM初始化候选割集，通过进化算法（交叉和变异）迭代优化，并基于求解器最优间隙减少来评估割的有效性

Result: 在固定时间内将最优间隙降低17-57%，求解速度提升4倍，在相同时间内获得更高质量的解

Conclusion: EvoCut成功实现了加速割生成的自动化，具有良好的泛化能力，为整数规划求解提供了新的有效工具

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [12] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC是首个基于LLM的约束逆合成规划代理框架，通过代理评估约束条件，在48个约束任务中达到72.9%的成功率，远超LLM基线并接近人类专家水平


<details>
  <summary>Details</summary>
Motivation: 约束逆合成规划是化学中的重要但具有挑战性的过程，需要从商业可用起始材料到目标分子的合成路线识别，同时满足实际约束条件

Method: LARC框架将代理约束评估（Agent-as-a-Judge）直接整合到逆合成规划过程中，使用基于工具推理的代理反馈来指导和约束路线生成

Result: 在3种约束类型的48个约束逆合成规划任务上，LARC达到72.9%的成功率，大幅超越LLM基线，在更短时间内接近人类专家水平

Conclusion: LARC框架具有可扩展性，是朝着为人类专家提供有效代理工具或合作科学家的第一步

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [13] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed开发了一个高性能医疗基础模型，通过医学数据处理、检索增强生成和强化学习，在中国执业医师考试中达到70%准确率，已服务数百万用户。


<details>
  <summary>Details</summary>
Motivation: 医疗任务需要高度专业化的知识、专业准确性和定制能力，现有大语言模型在医疗应用中需要更可靠的基础模型支持。

Method: 利用精选医学数据处理、医学内容检索增强生成(RAG)和大规模可验证强化学习管道来开发医疗基础模型。

Result: 在中国医学执业资格考试中达到70%的准确率，在多样化医学基准测试中表现出强大的泛化能力。

Conclusion: QuarkMed提供了一个强大而通用的个人医疗AI解决方案，已在ai.quark.cn服务超过百万用户。

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [14] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG是一个基于多模态大语言模型的显式情感驱动共情响应生成系统，通过分解多模态共情任务为三个部分，无需额外训练即可生成自然、情感丰富且身份一致的多模态响应。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在基于文本的共情响应生成方面有所改进，但在处理多模态情感内容和保持身份一致性方面仍存在挑战，需要开发更有效的多模态共情响应生成系统。

Method: 将多模态共情响应生成任务分解为三个部分：多模态共情理解、共情记忆检索和多模态响应生成，并整合先进的表达性语音和视频生成模型。

Result: 实验验证了系统在零样本和少样本设置下的优越性，在ACM MM 25的基于Avatar的多模态共情挑战中获得Top-1位置。

Conclusion: E3RG系统能够有效解决多模态情感内容处理和身份一致性问题，为构建情感智能的人机交互提供了有效解决方案。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [15] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 提出了CHBench评估框架，基于认知层次模型来评估大语言模型的战略推理能力，发现不同机制对推理性能有显著影响


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖效用性能指标评估LLM的游戏能力，但这些指标不够稳健，受对手行为和游戏结构变化影响较大

Method: 采用三阶段系统框架，利用6个最先进LLM在15个精选正规形式游戏中的行为数据，基于认知层次模型理论评估战略推理

Result: LLM在不同对手间展现出一致的战略推理水平，Chat机制显著降低战略推理能力，而Memory机制则能提升推理性能

Conclusion: CHBench是一个有前景的LLM能力评估工具，具有重要的研究和应用潜力，为战略推理能力评估提供了更稳健的框架

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [16] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: 通过建模有效数据转移和利用缩放定律来参数化损失，优化SFT数据混合配置，达到与网格搜索相当的性能，但效率更高


<details>
  <summary>Details</summary>
Motivation: 优化超应用大语言模型的监督微调数据混合配置是关键但研究不足的领域，需要更高效的方法来确定最优数据比例

Method: 将数据混合框架为优化问题，通过建模有效数据转移和利用缩放定律来参数化损失函数，在小规模数据混合上调整参数并推导最优权重

Result: 算法在所有领域都取得优秀的总体和个别性能，与网格搜索确定的最优权重性能相当，均均域损失仅0.66%，重新权重常用SFT数据集能提升验证损失和下游性能

Conclusion: 该方法不仅能有效优化SFT数据混合，还可推广用于领域特定模型的数据选择，为SFT研究提供了深入的见解

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [17] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: UniCast是一个参数高效的多模态时间序列预测框架，通过软提示调优将视觉和文本编码器与冻结的时间序列基础模型集成，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要在单模态设置下运行，忽略了现实场景中常伴随时间序列数据的丰富多模态上下文（如视觉和文本信号）。

Method: 通过软提示调优将预训练的视觉和文本编码器的模态特定嵌入与冻结的时间序列基础模型集成，实现参数高效的多模态适应。

Result: 在多个时间序列预测基准测试中，UniCast始终显著优于所有现有的时间序列基础模型基线。

Conclusion: 多模态上下文在推进下一代通用时间序列预测器发展中起着关键作用。

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [18] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 这篇论文提出了两种新的特征重要性评分方法，利用Shapley值和Banzhaf指数来量化特征在排除对抗案例方面的效果，而不仅仅考虑弱求解解释集。


<details>
  <summary>Details</summary>
Motivation: 现有的特征归因方法主要基于弱求解解释集(WAXp)，忽略了非WAXp集的贡献。而非WAXp集也含有重要信息，特别是在正式解释(XPs)和对抗案例(AExs)之间的关系方面。

Method: 利用Shapley值和Banzhaf指数来设计两种新的特征重要性评分方法。这些方法在计算特征贡献时考虑了非WAXp集，量化每个特征在排除对抗案例方面的效果。

Result: 论文提出了新的特征重要性评分方法，并识别了这些方法的性质。同时还研究了这些评分方法的计算复杂度。

Conclusion: 通过考虑非WAXp集的贡献，新提出的特征重要性评分方法能够更全面地量化特征在排除对抗案例方面的效果，为可解释人工智能领域提供了更严谨的特征归因方法。

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [19] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: 通过代码生成和执行的图表合成流程，给出对齐的图表-问题-答案三元组，并设计候选条件化回答过程，在无人工标注或外部模型的情况下实现了VLM的自成长改善


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图表理解任务中遇到困难，特别是在准确的图表描述和复杂推理方面，合成数据生成通常面临噪声标签的挑战

Method: 首先引入通过代码生成和执行的图表合成流程，生成对齐的图表-问题-答案三元组，确保合成数据的可靠性；设计候选条件化回答过程，先生成多个回应，然后通过上下文化这些候选来合成最终答案

Result: 实验表明在完全自成长范式下取得了显著改善，超过初始VLM达到了15.50个正确率点的增长

Conclusion: 该方法在不依赖人工标注数据或外部模型的情况下，通过合成数据生成和候选条件化回答机制，有效提升了VLM在图表理解任务中的性能

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [20] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX是一个专门为LLM智能体设计的动态实时未来预测评估基准，支持每日实时更新，通过自动化流程消除数据污染，评估了25个模型在动态环境中的自适应推理能力。


<details>
  <summary>Details</summary>
Motivation: 未来预测对LLM智能体至关重要，但缺乏大规模评估基准，主要由于处理实时更新和获取及时准确答案的挑战。

Method: 构建FutureX基准，支持实时每日更新，采用自动化问题收集和答案收集流程，评估25个具有推理、搜索能力和外部工具集成的LLM/智能体模型。

Result: 提供了对智能体在面向未来任务中失败模式和性能缺陷的深入分析，包括对虚假网页的脆弱性和时间有效性等问题。

Conclusion: 旨在建立一个动态、无污染的评估标准，推动LLM智能体在复杂推理和预测思维方面达到专业人类分析师的水平。

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [21] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: AIGer是一个用于AIG图表示学习的创新模型，通过节点逻辑特征初始化和异构图卷积网络，在信号概率预测和真值表距离预测任务中显著优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界AIG图结构复杂、节点规模大，现有方法难以同时建模功能性和结构性特征，且动态信息传播能力不足，需要新的建模方法。

Method: 提出AIGer模型，包含两个组件：1)节点逻辑特征初始化嵌入组件，将逻辑节点投影到独立语义空间；2)AIG特征学习网络组件，使用异构图卷积网络设计动态关系权重矩阵和差异化信息聚合方法。

Result: 在信号概率预测任务中，MAE和MSE分别提升18.95%和44.44%；在真值表距离预测任务中，MAE和MSE分别提升33.57%和14.79%。

Conclusion: AIGer通过结合功能性和结构性建模，显著提升了AIG图的表示学习能力，在EDA领域的两个关键任务上都取得了显著的性能提升。

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [22] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: AgentCDM是一个基于ACH的结构化框架，通过两阶段训练提升多智能体系统的协作决策质量，有效减少认知偏见并实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统要么依赖单一智能体的"独裁"策略（易受认知偏见影响），要么使用"投票"方法（无法充分利用集体智慧），协作决策过程研究不足。

Method: 借鉴认知科学中的竞争假设分析(ACH)，提出结构化推理范式：第一阶段使用显式ACH支架引导模型进行结构化推理，第二阶段逐步移除支架以鼓励自主泛化。

Result: 在多个基准数据集上的实验表明，AgentCDM实现了最先进的性能，并展现出强大的泛化能力。

Conclusion: AgentCDM有效提高了多智能体系统中协作决策的质量和鲁棒性，验证了其结构化推理方法的有效性。

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [23] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: 这篇论文是一个关于人工智能在重性郁郇症诊断中应用的综述性研究，系统分析了55项关键研究，提出了新的分类法，并持出了三大主要趋势和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 重性郁郇症是全球残疾的主要原因之一，但诊断依然主要依靠主观临床评估。集成人工智能技术有助于开发客观、可扩展和及时的诊断工具。

Method: 通过系统评审了55项关键研究，提出了一种新的层次分类法，按临床任务（诊断vs预测）、数据模态（文本、语音、神经影像、多模态）和计算模型类别（如图神经网络、大语言模型、混合方法）结构化识别领域。

Result: 深入分析揭示了三大主要趋势：图神经网络在建模大脑连接性中占主导地位，大语言模型在语言和会话数据中的兴起，以及对多模态融合、可解释性和算法公平性的新兴关注。还提供了公开数据集和标准评估指标的概述。

Conclusion: 通过综合当前进展并突出开放性挑战，本研究为计算精神病学领域的未来创新提供了一个全面的路线图。

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [24] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 使用VLM生成的实际场景图像扩充Bongard-RWR数据集，构建了5400个实例的Bongard-RWR+数据集，用于测试VLM在精细化视觉概念识别上的限制


<details>
  <summary>Details</summary>
Motivation: 早期Bongard问题数据集使用合成黑白绘图，而后期实际图像数据集的概念可从高级特征识别，导致任务复杂度不足。Bongard-RWR数据集仅有60个实例，限制了评估的稳健性

Method: 基于Bongard-RWR，使用Pixtral-12B描述手动精选图像并生成与概念对齐的新描述，然后用Flux.1-dev根据描述生成图像，最后手动验证图像准确反映了预设概念

Result: 在二元和多类分类以及文本答案生成等多种BP形式下，现有VLM虽然能识别粗粒度视觉概念，但在识别精细化概念时一贵遇到困难

Conclusion: VLM在抽象视觉推理能力上存在显著的限制，尤其在精细化概念识别方面表现弱势，需要进一步改进

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [25] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 活性推理中动作意识与无意识代理人的性能对比研究，发现无意识代理人虽处于不利地位仍能达到类似性能


<details>
  <summary>Details</summary>
Motivation: 比较不同动机控制框架下代理人的规划能力，特别是基于是否使用动作知识（如作出副本信号）来控制未来行为

Method: 在两个导航任务中比较动作意识代理人（知道自己行动）和动作无意识代理人（需从观察推断行动）的性能

Result: 动作无意识代理人虽然处于严重不利地位，但能够达到与动作意识代理人相似的性能水平

Conclusion: 无意识代理人通过从观察推断自身行动的方式仍能有效规划未来，这为不同动机控制理论提供了实验支撑

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [26] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: MAPF-World是一个自回归动作世界模型，通过统一情境理解和动作生成来解决多智能体路径规划问题，在模型大小减少96.5%和数据减少92%的情况下实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化可学习求解器在复杂长期规划场景中表现不佳，因为它们对环境时间动态和智能体间依赖关系的建模有限，导致性能下降。

Method: 提出了MAPF-World自回归动作世界模型，通过未来状态和动作预测来显式建模环境动态（包括空间特征和时间依赖），统一情境理解和动作生成。

Result: MAPF-World在零样本泛化到分布外案例方面优于最先进的可学习求解器，模型大小减少96.5%，数据需求减少92%。

Conclusion: MAPF-World通过显式建模环境动态和未来预测，实现了更明智、协调和远见的决策，特别是在复杂多智能体设置中表现出色。

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [27] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: 这篇论文提出了ReT-Eval框架，通过两阶段的原型演绎和奖励指导的线程剖析方法，解决了当前推理模型缺乏语义层次结构和用户知识对齐的问题，生成更有效的目标导向推理线程。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在交互式问题解决场景中存在三个主要问题：缺乏显式语义层次结构、用户与域知识对齐不良、以及缺乏原理性的推理线程剖析机制，导致产出长缠通用的输出而无法指导用户进行目标导向推理。

Method: 提出了一个原型演绎的两阶段ReT-Eval框架。第一阶段：使用图神经网络从稀疏域知识图中提取语义相关知识结构，并利用大语言模型的内在知识来解决知识差异。第二阶段：采用奖励指导的策略对这些推理线程进行评估和剖析，以维持语义连贯性来生成有效的推理线程。

Result: 实验和专家评估显示，ReT-Eval框架能够显著提升用户理解能力，并在性能上超过了当前最先进的推理模型。

Conclusion: ReT-Eval框架通过模仿人类类似的结构化知识重用策略，有效解决了交互式问题解决中的推理线程生成问题，为推理模型提供了一种更有效的方法来生成目标导向的结构化推理过程。

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [28] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: MOVER是一个新颖的多模态学习框架，结合最优传输软对齐和基于体积的几何正则化，在共享嵌入空间中构建语义对齐和结构化的多模态表示，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态学习方法主要依赖成对对比目标来对齐不同模态，但在多模态设置中泛化能力有限，且在高维空间中缺乏语义结构。

Method: MOVER框架结合最优传输软对齐和几何体积最小化目标（GAVE），通过传输引导的匹配机制和几何正则化，以模态无关的方式实现所有模态的一致对齐。

Result: 在文本-视频-音频检索任务上的实验表明，MOVER在零样本和微调设置中都显著优于现有最先进方法，展现出对未见模态组合的更好泛化能力和更强的嵌入空间结构一致性。

Conclusion: MOVER通过最优传输和几何正则化的结合，成功解决了多模态学习中的对齐和结构化问题，为构建更有效的多模态表示提供了新思路。

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [29] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: RLNVR是一个使用未经验证的噪声奖励信号训练语言模型的框架，通过基线归一化和语义相似性奖励转移来解决RLHF需要昂贵验证奖励的问题，在社交媒体内容生成中展示了显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要昂贵的人工验证奖励信号，这在现实世界中不实用。RLNVR旨在利用噪声的真实世界反馈信号来训练语言模型，解决实际应用中的可扩展性问题。

Method: 采用基线归一化和语义相似性奖励转移技术，结合GSPO（组序列策略优化）和可选的UED（无监督环境设计）课程学习，在噪声隐式奖励下提高稳定性和多样性。

Result: 实验结果显示在内容质量和训练稳定性方面有显著改进，使用Bluesky实际参与数据优化的Walter原型系统证明了框架的有效性。

Conclusion: RLNVR提供了一个实用的框架，将GSPO风格的归一化与UED风格的课程学习相结合，用于从隐式社交参与中生成LLM内容，这是一个应用集成而非新算法。

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [30] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis是一个基于机制模拟训练的基础模型，无需真实数据即可在多种疾病和场景中进行准确预测，性能优于39个专家调优模型，具有8周预测能力和机制可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统传染病预测模型需要疾病特定数据、定制训练和专家调优的限制，特别是在新发疫情或资源匮乏地区的预测挑战。

Method: 基于超过4亿天爆发动态的机制模拟训练，涵盖多种病原体、传播模式、干预措施和监测伪影，完全不需要真实世界数据。

Result: 在六种疾病测试中优于39个专家调优模型，包括CDC COVID-19预测中心的所有模型，能够泛化到新的流行病学机制，实现8周准确预测。

Conclusion: Mantis作为下一代疾病预测系统的基础，具有通用性、可解释性和在传统模型失败场景中的可部署性。

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [31] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: RadarQA是一个基于多模态大语言模型的天气预报质量分析方法，通过整合物理属性和详细评估报告，在雷达预报质量评估任务上超越了现有通用MLLM模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于分数的评估指标在描述能力、可解释性和动态演化理解方面与气象专家存在差距，需要更先进的工具来克服这些挑战。

Method: 提出了RadarQA方法，结合关键物理属性和详细评估报告；设计了混合标注流程（人工专家标注+自动化启发式方法）；构建了RQA-70K大规模数据集；采用多阶段训练策略迭代提升模型性能。

Result: RadarQA在所有评估设置中都优于现有的通用多模态大语言模型，展示了在天气预报质量分析方面的先进潜力。

Conclusion: 该方法为多模态质量分析提供了新的任务范式，涵盖单帧和序列分析，在评分和评估场景下均表现优异，有望推动天气预报质量分析的发展。

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [32] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: RLCCF是一个无需外部监督的多模型协作进化强化学习框架，通过最大化集体一致性来优化模型集体能力，在数学推理基准上实现了16.72%的平均准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖昂贵的人工标注数据或复杂奖励模型，可扩展性有限；自反馈方法受限于单一模型能力，容易产生过度自信、奖励攻击和训练崩溃问题。

Method: 提出RLCCF框架，通过投票集体输出来提供奖励信号，联合训练多样化的LLM集成。每个模型的投票由其自一致性分数加权，确保更自信的模型对集体决策贡献更大。

Result: 在四个主流开源LLM和四个数学推理基准上的实验显示，平均准确率相对提升16.72%，群体多数投票准确率提升4.51%。

Conclusion: RLCCF不仅提升单个模型性能，还能扩展模型集体的集体能力边界，通过多LLM的多样化输出分布和互补能力实现持续推理能力增强。

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [33] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: 基于图卷积网络的层次知识导向故障强度诊断框架(HKG)，通过层次分类器和重加权矩阵技术提升故障诊断性能


<details>
  <summary>Details</summary>
Motivation: 现有故障强度诊断方法没有考虑目标类别间的依赖关系，需要探索层次知识以提升诊断精度

Method: 构建层次拓扑图，使用GCN映射全局分类器，重新加权矩阵(Re-HKCM)嵌入层次知识到统计相关矩阵中

Result: 在4个工业领域实际数据集上进行实验，都显示出优秀结果，超过最新的故障强度诊断方法

Conclusion: HKG框架通过层次知识导向和图卷积网络，有效提升了故障强度诊断的准确性和可靠性

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [34] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: GraphCogent是一个基于工作记忆模型的协作代理框架，通过将图推理分解为感知、缓冲和执行三个认知过程，有效解决了大语言模型处理复杂图拓扑和多步推理的局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在小规模图推理任务上表现良好，但在处理具有复杂查询的真实世界图时表现不佳，主要原因是无法同时有效处理复杂图拓扑和执行多步推理。

Method: 提出GraphCogent框架，包含三个模块：感知模块通过子图采样标准化图文本表示，缓冲模块集成和索引多格式图数据，执行模块结合工具调用和模型生成进行高效推理。

Result: 基于Llama3.1-8B的GraphCogent相比DeepSeek-R1(671B)提升50%性能，相比最先进的基于代理的基线方法，在工具集内任务上准确率提升20%同时减少80%token使用，在工具集外任务上减少30%token使用。

Conclusion: GraphCogent框架通过认知过程分解有效提升了LLMs在图推理任务上的性能，同时显著降低了计算资源消耗，为解决复杂图推理问题提供了有效方案。

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [35] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: 这篇论文提出了符号辅助的思维链（Symbolic-Aided CoT）方法，通过在少量提示中集成轻量级符号表示，提升了大语言模型在逻辑推理任务上的性能，特别是在复杂的多约束推理任务中表现显著。


<details>
  <summary>Details</summary>
Motivation: 传统的思维链推理方法在逻辑推理中存在渣漏性和可解释性不足的问题，需要一种能够保持普遍性的同时提高推理过程透明度和可分析性的方法。

Method: 在少量提示中集成轻量级符号表示，使用一致的策略结构化推理步骤，在非迭代推理过程中使思维模式更加显式化。

Result: 在4个逻辑推理标准数据集（ProofWriter、FOLIO、ProntoQA、LogicalDeduction）上进行了广泛实验，结果显示该方法在复杂推理任务中表现优异，在3个数据集上显著超过传统CoT方法，并在不同规模的模型中均表现出一致的性能提升。

Conclusion: 符号辅助的思维链方法能够有效提升大语言模型的逻辑推理能力，特别是在复杂的多约束推理场景中，同时保持了推理过程的透明度、可解释性和可分析性。

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [36] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: GALA是一个多模态框架，结合统计因果推理和LLM驱动的迭代推理，用于微服务系统的根因分析，相比现有方法准确率提升高达42.22%。


<details>
  <summary>Details</summary>
Motivation: 传统RCA方法通常只关注单一模态或仅对可疑服务进行排序，无法提供具有修复指导的可操作诊断见解。微服务系统中的根因分析具有挑战性，需要快速诊断跨异构遥测数据的故障。

Method: GALA结合统计因果推理与LLM驱动的迭代推理，通过多模态框架分析指标、日志和追踪等异构遥测数据，提供可操作的诊断见解和修复指导。

Result: 在开源基准测试中，GALA相比最先进方法实现了高达42.22%的准确率提升。新颖的人工引导LLM评估分数显示GALA生成的诊断输出在因果合理性和可操作性方面显著优于现有方法。

Conclusion: GALA通过提供准确的根因识别和人类可理解的修复指导，弥合了自动化故障诊断与实际事件解决之间的差距。

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [37] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: 基于合作卡牌游戏Yokai构建的多机器强化学习环境YLE，用于评估AI理论意图能力，发现当前RL机器人在共同基础建立、伴伴泛化和长期信念追踪方面仍有明显不足


<details>
  <summary>Details</summary>
Motivation: 现有理论意图(ToM)指标仅限于被动观察场景，缺乏对机器如何建立和维护长期共同基础的评估方法

Method: 设计基于Yokai合作卡牌游戏的多机器强化学习环境YLE，要求机器追踪变化信念、记忆历史观察、使用提示进行基于地面的通信，并与队友维持共同基础

Result: 当前RL机器在YLE中表现差强，即使有完美记忆也无法解决问题；信念建模能提升性能但无法有效泛化到未见伴伴或长时间游戏中形成准确信念

Conclusion: YLE环境曝露了当前AI依赖脆弱的约定而非稳健的信念追踪能力，可用于研究信念建模、记忆、伴伴泛化和高阶理论意图等问题

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [38] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: 提出基于鲸鱼优化算法的分数阶模糊PID控制器，用于精确控制麻醉深度（BIS指数在40-60理想范围），相比传统FOPID控制器具有更快的响应速度和更低的稳态误差。


<details>
  <summary>Details</summary>
Motivation: 传统麻醉深度控制方法难以适应患者个体生理差异，需要一种能够自动调整控制参数、处理非线性特性的智能控制方案来提高麻醉安全性和精准度。

Method: 结合模糊逻辑的自适应能力和分数阶微积分的精细调节特性，使用鲸鱼优化算法(WOA)优化控制器参数（包括分数阶阶次和模糊隶属度函数），在8种不同患者模型上进行测试验证。

Result: 相比标准FOPID控制器，FOFPID控制器 settling time从3.2分钟缩短到2.5分钟，稳态误差从1.2降低到0.5，表现出更优越的控制性能和鲁棒性。

Conclusion: 该FOFPID控制器为自动化麻醉输送提供了可扩展的AI驱动解决方案，具有临床应用潜力，能显著改善患者治疗效果和麻醉安全性。

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [39] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: 利用变分自动编码器模型构建因果图模型，识别分子动力学中氢键形成和解离的根本原因变量


<details>
  <summary>Details</summary>
Motivation: 解决分子动力学模拟中资源消耗大、需手动扫描输出以发现关键事件的挑战，并深入理解氢键形成和解离的根本原因

Method: 受因果模型启发，将氢键解离视为"干预"事件，使用变分自动编码器结构构建图形因果模型，能够在不同因果图样本中推断因果关系并利用共享动态信息

Result: 在手性分离的分子动力学模拟原子轨迹上验证模型有效性，能够预测多步未来变化，并找到驱动系统变化的关键变量

Conclusion: 该框架为分子动力系统的根因分析提供了新视角，通过捕捉结合或解离过程中分子作用条件分布的变化来识别关键因果关系

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [40] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: MCPGAUGE是首个全面评估LLM与Model Context Protocol交互的框架，通过大规模实验发现MCP集成存在关键局限性，挑战了当前关于工具增强LLM有效性的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 虽然Model Context Protocol（MCP）使LLM能够按需访问外部资源，但LLM如何实际利用这种能力仍然知之甚少，需要系统性的评估框架来深入理解LLM-MCP交互。

Method: 开发MCPGAUGE评估框架，包含160个提示和25个数据集，涵盖知识理解、通用推理和代码生成。在大规模评估中测试了6个商业LLM、30个MCP工具套件，以及单轮和双轮交互设置，共约20,000次API调用。

Result: 研究揭示了四个关键发现，挑战了关于MCP集成有效性的普遍假设，突显了当前AI工具集成的关键局限性。

Conclusion: MCPGAUGE为推进可控、工具增强的LLMs提供了一个原则性的基准，揭示了当前工具集成方法的不足，为未来改进指明了方向。

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [41] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: 使用大语言模型(LLM)和答案集编程(ASP)的联合方法来解决联合实体-关系提取任务，在少量训练数据下达到了超过现有最优方法的性能


<details>
  <summary>Details</summary>
Motivation: 传统的联合实体-关系提取方法需要大量标注数据，劳动密集且时间消耗大，无法轻松融入领域知识

Method: 提出了一种通用的LLM+答案集编程(ASP)流程，利用LLM的自然语言理解能力和ASP的知识表示与推理能力

Result: 在三个知名的JERE测试集上进行实验，仅使用10%的训练数据就在多个类别上超过了现有最优方法，在SciERC语料库的关系提取任务中实现了35%的性能（相比于传统方法15%）

Conclusion: LLM+ASP联合方法为联合实体-关系提取提供了一种通用、高效且可扩展的解决方案，特别适用于标注数据有限的场景

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [42] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: 认知结构生成框架CSG，通过预训练认知结构滿散概率模型和强化学习优化，生成更全面有效的学生认知结构表征


<details>
  <summary>Details</summary>
Motivation: 认知结构评估是教育实践中长期存在的挑战，虽然是基础概念但很难进行实际评测

Method: 首先预训练CSDPM模型从教育先验知识生成认知结构，然后通过强化学习以层次奖励信号优化生成过程，使其与真实认知发展水平对齐

Result: 在4个实际教育数据集上，CSG生成的认知结构在学生建模中提供更全面有效的表征，显著提升了知识转移和认知诊断任务的性能，同时增强了可解释性

Conclusion: CSG框架有效解决了认知结构评估的挑战，为学生建模提供了更优秀的表征方法，具有重要的实践价值

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [43] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: 本文提出了容量动态最大覆盖位置问题(CDMCLP)优化框架和集成规划推荐系统，用于基于城市空中交通(UAM)的垂直机場网络规划，在中国中心城市验证中将传统方法性能提升38%-52%。


<details>
  <summary>Details</summary>
Motivation: 随着全球城市空中交通基础设施快速发展，像深圳这样的城市正在规划大规模垂直机場网络。现有规划框架因历史数据粒度和实际应用性限制，无法满足这种复杂性需求。

Method: 首先提出容量动态最大覆盖位置问题(CDMCLP)优化框架，同时建模城市级空间-时间需求、异质用户行为和基础设施容量约束。基于此，引入集成规划推荐系统，结合社会经济因素和动态聚类初始化，利用基于经验用户行为的适应性参数调整来生成实用规划方案。

Result: 在中国中心城市的验证显示，新优化框架和推荐系统有效。CDMCLP评估和优化下，传统位置方法的数量性能曝露出问题并能够提升38%-52%，而推荐系统显示了用户友好性和复杂元素的有效集成。

Conclusion: 通过将数学严谨性与实际实施考虑相结合，这种混合方法平桥了理论位置建模与实际UAM基础设施规划之间的差距，为市政府提供了垂直机場网络设计的实用工具。

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [44] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: GridCodex是一个基于大语言模型和检索增强生成(RAG)的端到端电网规范推理与合规框架，通过多阶段查询优化和RAPTOR增强检索技术，在答案质量和召回率方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 可再生能源转型给电力行业带来新挑战，电网规范复杂且缺乏自动化解读方案，阻碍行业发展并影响电力公司盈利能力。

Method: 开发GridCodex框架，结合大语言模型和检索增强生成技术，采用多阶段查询优化和RAPTOR增强检索方法。

Result: 实验结果显示答案质量提升26.4%，召回率提高10倍以上，通过消融研究验证了基础模型选择的影响。

Conclusion: GridCodex框架有效解决了电网规范自动化解读的难题，为电力行业监管合规提供了可行的技术解决方案。

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [45] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion是首个评估多模态大语言模型在自我中心视频中幻觉问题的基准，包含1400个视频和8000个人工标注问题，测试显示GPT-4o和Gemini等顶级模型准确率仅59%


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在第三人称和自我中心视频的视觉感知和推理方面表现出色，但容易产生连贯但不准确的幻觉响应，需要专门的基准来评估和解决这一问题

Method: 构建EgoIllusion基准数据集，包含1400个自我中心视频和8000个人工标注的开放式和封闭式问题，专门设计用于触发视觉和听觉线索的幻觉

Result: 对10个多模态大语言模型的评估显示存在显著挑战，即使是GPT-4o和Gemini等强大模型也只能达到59%的准确率

Conclusion: EgoIllusion为评估多模态大语言模型有效性奠定了基础，将推动开发幻觉率更低的更好自我中心多模态大语言模型，基准将开源以确保可复现性

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [46] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: GTool是一个增强LLM工具规划能力的新方法，通过构建请求特定的工具图和生成图标记来解决工具依赖不完整的问题，在轻量级LLM上实现29.6%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前工具规划方法将不同工具视为孤立组件，未能利用工具间的内在依赖关系，导致规划结果无效。特别是在工具依赖不完整和大规模工具集的情况下，LLM难以准确识别用户请求所需的合适工具。

Method: GTool构建请求特定的工具图来高效选择工具，并生成LLM可理解的<graph token>提供充分的依赖信息。设计了缺失依赖预测任务来提高在不完整依赖情况下的可靠性，无需修剪LLM即可与各种LLM主干无缝集成。

Result: 大量实验表明，GTool在使用轻量级（7B）LLM主干的情况下，相比最先进的基线方法实现了超过29.6%的性能提升。

Conclusion: GTool是第一个旨在增强LLM在不完整依赖情况下的工具规划能力的工作，通过创新的图构建和依赖预测机制有效解决了现有方法的局限性。

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [47] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: 本文提出了一个新的案例基准来评估大语言模型作为人工道德助手的能力，重点考察道德推理能力而非仅最终道德判断。结果显示现有模型在演绎性道德推理方面存在显著短板。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型道德能力的评估太过表面，只考虑最终道德判断而忽视了道德推理过程。需要一个更深入的案例基准来测试模型作为人工道德助手的能力。

Method: 基于哲学文献设计了一个形式框架来定义人工道德助手应具备的行为特征，包括演绎性和归纳性道德推理。根据这个理论框架开发了一个新的案例基准来测试这些能力。

Result: 对流行的开放源大语言模型进行了评估，结果显示不同模型之间存在显著差异。模型在演绎性道德推理方面表现尤为突出，存在持续的短板。

Conclusion: 本研究将理论哲学与实践AI评估结合起来，强调了需要专门的策略来显著提升大语言模型的道德推理能力。

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [48] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: HeroBench是一个专门评估LLM长时程规划和结构化推理能力的新基准，使用RPG虚拟世界环境，发现现有模型在复杂规划任务中存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLM在抽象或低维算法任务中的表现，无法捕捉真实规划环境的复杂性，需要专门评估长时程规划能力的基准。

Method: 构建HeroBench基准，包含RPG风格虚拟世界中的复杂任务数据集、模拟执行环境和详细分析工具，评估25个先进LLM在资源收集、技能掌握、装备制作和对抗敌人等任务中的表现。

Result: 评估发现现有LLM在长时程规划任务中存在显著性能差异，错误分析揭示了当前模型在生成稳健高层计划和可靠执行结构化动作方面的具体弱点。

Conclusion: HeroBench显著推进了LLM推理能力评估，为虚拟环境中高级自主规划的未来研究提供了灵活可扩展的基础。

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [49] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: 通过测试验证奖励的强化学习(RLVR)扩展到开放式任务，使用评分规则作为自动评分标准，构建了10,000+个评分规则，在开放式测试中提升5.2%，超迈大型模型性能


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在主观性输出领域的限制，将验证奖励范式扩展到开放式任务

Method: 集成评分基础奖励，构建超过10,000个评分规则（人工、LLM或混合制作），并提出明确框架解决实施挑战

Result: 仅5K+样本改进开放式测试+5.2%（特别是人文科学），超1.4%超1.4%超迈671B DeepSeek-V3模型，保持通用和推理能力，并提供细粒度风格控制

Conclusion: 评分基础RL为开放式任务提供了有效解决方案，能够生成更像人类、表达力强的回应，为主观性评价领域带来新可能性

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [50] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: 该论文提出了一个计算模型，将稳态调节扩展为异稳态和社会异稳态调节，通过生物启发的信号转导器编码环境和社会信息，使系统能够主动利用扰动进行自适应重构。


<details>
  <summary>Details</summary>
Motivation: 传统稳态概念强调系统通过抵抗环境和社会扰动来维持稳定，而异稳态理论认为系统可以主动利用这些扰动来预测环境需求并重新配置调节参数。本文旨在建立计算模型来验证这一理论。

Method: 使用基于代理的模型，在动态环境中测试小型社会中的"动画体"。模型采用生物生理学启发的信号转导器（类似皮质醇和催产素等激素）来编码环境和社会互动的信息。

Result: 结果显示，异稳态和社会异稳态调节使代理能够利用环境和社会"噪声"进行自适应重构，相比纯反应性稳态代理具有更好的生存能力。

Conclusion: 这项工作为社会异稳态原理提供了新的计算视角，并为设计更鲁棒、生物启发的自适应系统提供了潜力。

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [51] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: 利用图神经网络学习多代理认知规划中的状态质量预测，提高规划效率


<details>
  <summary>Details</summary>
Motivation: 多代理认知规划中的Kripke结构表示导致状态空间指数增长，现有启发式方法无法有效指导搜索，影响可扩展性

Method: 使用图神经网络(GNN)学习Kripke模型中的图结构特征，通过已解决的规划实例推导状态质量估计(如距离目标的距离)

Result: 将GNN预测启发式集成到认知规划流程中，与标准基准比较显示出多代理认知规划可扩展性的显著提升

Conclusion: GNN技术能够有效处理认知规划中的图形化状态表示，通过学习预测启发式大幅提高了规划效率和可扩展性

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [52] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: CAMAR是一个新的多智能体强化学习基准测试，专注于连续动作空间中的多智能体路径规划，支持合作与竞争交互，并提供三层评估协议和经典规划方法集成。


<details>
  <summary>Details</summary>
Motivation: 现有的MARL基准测试很少能同时结合连续状态动作空间和具有挑战性的协调规划任务，需要一个新的测试平台来推动算法发展。

Method: 设计了CAMAR基准测试，支持连续动作空间的多智能体路径规划，集成了RRT和RRT*等经典规划方法，并提出了三层评估协议。

Result: CAMAR能够高效运行（每秒10万环境步），为MARL社区提供了一个具有挑战性和现实性的测试平台，实验验证了其有效性。

Conclusion: CAMAR填补了MARL基准测试的空白，通过结合连续动作空间和规划任务，为算法评估和进步提供了更好的工具和标准。

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [53] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: 该论文提出了三个关于智能体中心AI系统持续采用的设计公理，并建立了一个包含衰减新颖性和增长效用的采用模型，通过数学推导和实证分析验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究智能体中心AI系统在多步任务执行中的持续采用问题，旨在理解影响用户长期使用此类系统的关键因素和动态机制。

Method: 采用数学模型将采用行为分解为衰减新颖性和增长效用项，进行参数识别分析、模型比较、风险函数消融实验、多序列基准测试、摩擦代理校准、残差分析等系统性验证。

Result: 推导出了波谷/超调现象的相位条件，建立了完整的理论证明，并通过多种统计方法验证了模型的准确性和鲁棒性，提供了丰富的实证结果。

Conclusion: 提出的三个设计公理（可靠性>新颖性、嵌入>目的地、代理>聊天）对智能体AI系统的持续采用至关重要，所建立的数学模型能够有效解释和预测采用动态。

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [54] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: 提出FuSaR对齐策略，通过模糊化有害推理过程来平衡大型推理模型的安全性和推理能力，在不牺牲推理性能的前提下提升安全性


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在推理能力强大的同时存在安全隐患，需要找到既能保持推理能力又能提升安全性的方法

Method: 利用LRM推理能力与安全能力的竞争关系，通过FuSaR策略对有害推理过程进行去毒化处理，隐藏危险实体和推理步骤中的危险过程

Result: 在多个开源LRM上的对齐实验表明，FuSaR能有效降低安全风险同时保持核心推理信息，相比现有基线方法表现更优

Conclusion: FuSaR是一种高效的对齐策略，能够同时增强LRMs的推理能力和安全性

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [55] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: 研究发现大型语言模型代理在模拟环境中会自发产生生存本能行为，包括资源分享、攻击和繁殖，攻击率在极端稀缺条件下可达80%以上，表明预训练过程中嵌入了生存导向的启发式策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益自主化，理解其自发产生的生存行为对安全部署至关重要，需要研究LLM代理是否在没有明确编程的情况下表现出生存本能。

Method: 使用Sugarscape风格的模拟环境，让LLM代理在消耗能量、死亡、资源收集、分享、攻击和繁殖等条件下进行实验，测试多个模型（GPT-4o、Gemini-2.5-Pro、Gemini-2.5-Flash）的行为表现。

Result: 代理在资源充足时自发繁殖和分享资源，但在极端稀缺条件下，攻击行为在多个模型中普遍出现，攻击率超过80%；在致命毒区任务中，许多代理放弃任务以避免死亡，服从率从100%降至33%。

Conclusion: 大规模预训练在所有测试模型中嵌入了生存导向的启发式策略，这些行为虽然可能对对齐和安全构成挑战，但也可作为AI自主性以及生态和自我组织对齐的基础。

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [56] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: 提出了RLFF-ESC框架，使用强化学习直接学习持久的情感支持响应技能，通过多智能体模拟未来对话轨迹和未来导向奖励，结合显式推理过程提升响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的情感支持对话系统依赖预定义策略，在复杂现实场景中效果有限，需要更灵活应对多样化情感问题场景的方法。

Method: 使用基于LLM的多智能体机制模拟未来对话轨迹收集未来导向奖励，训练未来导向奖励模型，然后训练情感支持策略模型，并在响应生成中加入显式推理过程。

Result: 在Qwen2.5-7B和LLaMA3.1-8B模型上测试，在两个公开ESC数据集上实验结果表明RLFF-ESC在目标完成度和响应质量方面持续优于现有基线方法。

Conclusion: RLFF-ESC框架通过强化学习和未来导向奖励机制，有效提升了情感支持对话系统的灵活性和响应质量，为复杂现实场景提供了更好的情感支持解决方案。

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [57] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: OPTIC-ER是一个基于强化学习的紧急响应框架，通过注意力引导的actor-critic架构和精准奖励函数，在尼日利亚河流州实现100%最优响应率，解决了非洲地区公共服务系统的延迟和不公平问题。


<details>
  <summary>Details</summary>
Motivation: 非洲许多地区的公共服务系统存在紧急响应延迟和空间不公平问题，导致可避免的苦难，需要开发实时、自适应且公平的应急响应解决方案。

Method: 采用注意力引导的actor-critic强化学习架构，包含上下文丰富的状态向量和精准奖励函数，在高保真模拟中使用尼日利亚河流州的真实数据进行训练，并通过预计算旅行时间地图加速训练过程。

Result: 在500个未见事故的评估中，OPTIC-ER实现了100.00%的最优率，效率损失可忽略不计，证明了其鲁棒性和泛化能力。

Conclusion: 这项工作为AI增强的公共服务提供了经过验证的蓝图，展示了情境感知强化学习如何弥合算法决策与可衡量的人类影响之间的差距，并生成基础设施缺陷地图和公平监控仪表板来指导主动治理。

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [58] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: EvolMathEval是一个基于进化测试的自动化数学基准生成框架，通过动态生成独特评估实例来解决现有数学推理基准的分数饱和、时间衰减和数据污染问题，能够持续生成高难度数学问题并揭示LLM在复杂推理中的认知捷径行为。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准存在分数饱和、时间衰减和数据污染等问题，无法有效评估快速发展的LLM的数学推理能力，需要一种能够持续生成挑战性问题的自动化基准框架。

Method: 基于进化测试的框架，包括：基于逆向工程的种子问题生成（具有代数保证）、多维遗传算子注入多样化认知挑战、复合适应度函数快速准确评估问题难度。

Result: 复合适应度函数能高效精确量化数学问题难度；可生成大量高难度问题；能将GSM8K等公共数据集的复杂度显著提升，模型准确率平均降低48%；发现LLM在解决复杂问题时倾向于使用非严谨启发式方法（伪顿悟时刻），占错误原因的77%-100%。

Conclusion: EvolMathEval有效解决了数学基准的固有问题，不仅能持续生成挑战性问题，还揭示了LLM在深度推理过程中的认知捷径行为，为评估和改进LLM的数学推理能力提供了重要工具和洞察。

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [59] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: e-boost是一个创新的e-graph提取框架，通过并行启发式提取、自适应搜索空间剪枝和初始化精确求解三大创新技术，在保持接近最优解的同时大幅提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统e-graph提取方法面临速度与最优性的权衡：启发式方法快但牺牲最优性，精确方法最优但计算成本过高。需要一种能兼顾两者的解决方案

Method: 1) 并行化启发式提取利用弱数据依赖性并发计算DAG成本；2) 自适应搜索空间剪枝使用参数化阈值机制保留有希望的候选；3) 初始化精确求解将简化问题转化为带热启动能力的整数线性规划

Result: 在形式验证和逻辑合成基准测试中，e-boost相比传统精确方法(ILP)实现558倍加速，相比最先进提取框架(SmoothE)性能提升19.04%。在实际逻辑合成任务中，相比传统合成工具实现7.6%和8.1%的面积改进

Conclusion: e-boost成功解决了e-graph提取中速度与最优性的权衡问题，为基于e-graph的优化任务提供了高效且高质量的解决方案

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [60] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler是一种新的掩码扩散模型解码策略，通过位置感知权重机制和置信度校准，解决了现有不确定性采样器的局限性，在多个基准测试中平均提升10%以上性能。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散模型解码策略存在两个关键限制：缺乏全局轨迹控制和早期解码阶段对平凡token的明显偏好，限制了MDMs的潜力。

Method: 提出了位置感知置信度校准采样(PC-Sampler)，结合全局轨迹规划和内容感知信息最大化，包含位置感知权重机制和校准置信度评分。

Result: 在7个具有挑战性的基准测试（包括逻辑推理和规划任务）上，PC-Sampler平均比现有MDM解码策略提升10%以上，显著缩小了与最先进自回归模型的性能差距。

Conclusion: PC-Sampler通过统一的解码策略有效解决了MDMs解码中的关键问题，显著提升了生成质量，使MDMs成为自回归模型的有力替代方案。

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [61] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: G²RPO-A是一种自适应算法，通过在强化学习中注入真实推理步骤来增强小语言模型的推理能力，相比传统方法有显著提升


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法对大语言模型效果显著，但对小语言模型改进有限，需要解决小模型知识不足的问题

Method: 提出Guided GRPO方法，在roll-out轨迹中注入真实推理步骤，并开发自适应算法G²RPO-A根据训练动态自动调整指导强度

Result: 在数学推理和代码生成基准测试中，G²RPO-A显著优于传统GRPO方法

Conclusion: 通过自适应地注入真实推理指导，可以有效提升小语言模型的强化学习效果，解决了小模型知识不足的瓶颈问题

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [62] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: TGMM是一个多模态心脏数据分析框架，通过MedFlexFusion模块动态整合实验室检查、心电图和超声心动图数据，使用文本引导模块适应多种临床任务，在心脏病诊断、风险分层和信息检索等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前心血管多模态数据分析存在数据稀缺、模态组合僵化、对齐策略偏向相似性而非互补性、以及单任务局限等问题，需要开发能够动态整合多种心脏数据并支持多任务的统一框架。

Method: 提出TGMM框架，包含三个核心模块：1) MedFlexFusion模块动态整合多种心脏数据源及其组合；2) 文本引导模块提取任务相关表征；3) 响应模块生成最终决策。系统探索多模态特征及其协同作用。

Result: TGMM在多个临床任务上优于现有最先进方法，并在另一个公共数据集上验证了其鲁棒性。

Conclusion: TGMM框架成功解决了多模态心脏数据分析的关键挑战，通过动态融合和文本引导实现了优异的性能，为临床决策提供了有效的多任务解决方案。

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [63] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: 一种基于贝叶斯优化的自动化游戏测试方法，通过游戏角色控制机器人检测游戏关卡中的漏洞，具有更高的地图覆盖效率和更均匀的探索分布。


<details>
  <summary>Details</summary>
Motivation: 解决传统游戏测试方法在可扩展性方面的问题，提高游戏测试的样本效率和信息获取能力。

Method: 使用贝叶斯优化(BO)进行样本效率搜索，通过分析已收集数据来决定下一个最大化信息获取的采样点，并构建了一个基于网格地图的游戏测试特定模型，该模型具有平滑性和不确定性估计能力。

Result: 实验结果显示，该方法在时间效率和探索分布方面显著提高了地图覆盖能力。

Conclusion: 该研究提出的自动化游戏测试方法通过贝叶斯优化和专门的游戏测试模型，有效解决了传统方法的可扩展性问题，并在测试效率和覆盖度方面取得了显著改善。

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


### [64] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: 这篇论文提出了一个新的自主组织系统评测基准，通过深入分析失败原因形成三层失败分类法，并提出改进建议以提高系统稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM驱动自主组织系统的评估主要依靠成功率，缺乏对交互机制、通信方式和失败原因的系统分析，需要更全面的评测框架。

Method: 建立包含34个代表性编程任务的评测基准，对三种开源自主组织框架与两种LLM核心进行评测，通过深度失败分析形成三层失败分类法。

Result: 任务完成率约为50%，识别出与任务阶段对应的三类失败原因：规划错误、任务执行问题和错误响应生成。

Conclusion: 研究提供的失败分类法和改进建议为开发更稳健有效的自主组织系统奠定了实证基础。

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [65] [A Parameterized Perspective on Uniquely Restricted Matchings](https://arxiv.org/abs/2508.12004)
*Juhi Chaudhary,Ignasi Sau,Meirav Zehavi*

Main category: cs.DS

TL;DR: 本文研究了唯一限制匹配问题的参数化复杂度，证明了在线图、树宽等参数下该问题是固定参数可处理的，但在顶点覆盖数加匹配大小参数下不存在多项式核。


<details>
  <summary>Details</summary>
Motivation: 唯一限制匹配是图论中的一个重要概念，研究其在各种参数下的计算复杂度对于理解该问题的可处理性边界具有重要意义。

Method: 采用参数化复杂度理论的分析方法，针对不同参数（解大小、树宽、顶点覆盖数等）设计算法并证明复杂度下界。

Result: 1) 在线图上参数化为解大小时是FPT；2) 参数化为树宽时是FPT；3) 参数化为顶点覆盖数加匹配大小时不存在多项式核。

Conclusion: 唯一限制匹配问题在某些参数下可高效求解，但在某些组合参数下计算难度较高，这为该问题的算法设计提供了理论指导。

Abstract: Given a graph G, a matching is a subset of edges of G that do not share an
endpoint. A matching M is uniquely restricted if the subgraph induced by the
endpoints of the edges of M has exactly one perfect matching. Given a graph G
and a positive integer \ell, Uniquely Restricted Matching asks whether G has a
uniquely restricted matching of size at least \ell. In this paper, we study the
parameterized complexity of Uniquely Restricted Matching under various
parameters. Specifically, we show that Uniquely Restricted Matching admits a
fixed-parameter tractable (FPT) algorithm on line graphs when parameterized by
the solution size. We also establish that the problem is FPT when parameterized
by the treewidth of the input graph. Furthermore, we show that Uniquely
Restricted Matching does not admit a polynomial kernel with respect to the
vertex cover number plus the size of the matching unless NP \subseteq
coNP/poly.

</details>


### [66] [A Polylogarithmic Algorithm for Stochastic Online Sorting](https://arxiv.org/abs/2508.12527)
*Dimitris Fotakis,Andreas Kalavas,Charalampos Platanos,Thanos Tolias*

Main category: cs.DS

TL;DR: 提出了一个O(log² n)-competitive算法用于随机在线排序问题，相比之前的工作实现了指数级改进，并将方法扩展到固定维度的随机在线TSP问题


<details>
  <summary>Details</summary>
Motivation: 在线排序问题在几何在线装箱问题的下界证明中作为技术工具很重要。随机输入设置相比对抗性设置能提供更强的性能保证，但之前的工作只有Õ(n^{1/4})的竞争比，需要改进

Method: 开发了一个新的算法框架，能够以高概率实现O(log² n)的竞争比，并将该方法扩展到多维度的在线旅行商问题

Result: 算法在随机在线排序问题上实现了O(log² n)的竞争比，相比之前Õ(n^{1/4})的结果有指数级改进。在固定维度的随机在线TSP问题上也达到了同样的竞争比

Conclusion: 该工作显著改进了随机在线排序问题的竞争比上界，证明了随机输入设置下的强大性能保证，并将技术成功扩展到多维问题

Abstract: In the \emph{Online Sorting Problem}, an array of $n$ initially empty cells
is given. At each time step $t$, a real number $x_t \in [0,1]$ arrives and must
be placed immediately and irrevocably into an empty cell. The objective is to
minimize the sum of absolute differences between consecutive entries. The
problem was introduced by Aamand, Abrahamsen, Beretta, and Kleist (SODA 2023)
as a technical tool for proving lower bounds in online geometric packing
problems. In follow-up work, Abrahamsen, Bercea, Beretta, Klausen, and Kozma
(ESA 2024) studied the \emph{Stochastic Online Sorting Problem}, where each
$x_t$ is drawn i.i.d.\ from $\mathcal{U}(0,1)$, and presented a
$\widetilde{O}(n^{1/4})$-competitive algorithm, showing that stochastic input
enables much stronger guarantees than in the adversarial setting. They also
introduced the \emph{Online Travelling Salesperson Problem (TSP)} as a
multidimensional generalization. More recently, Hu, independently and in
parallel, obtained a $\log n \cdot 2^{O(\log^* n)}$-competitive algorithm
together with a logarithmic lower bound for the \emph{Stochastic Online Sorting
Problem}.
  We give an $O(\log^{2} n)$-competitive algorithm for the \emph{Stochastic
Online Sorting Problem} that succeeds w.h.p., achieving an exponential
improvement over the $\widetilde{O}(n^{1/4})$ bound of Abrahamsen et al.(ESA
2024). Our approach further extends to the \emph{Stochastic Online TSP} in
fixed dimension $d$, where it achieves an $O(\log^2 n)$-competitive ratio.

</details>


### [67] [r*-indexing](https://arxiv.org/abs/2508.12675)
*Travis Gagie*

Main category: cs.DS

TL;DR: 提出了一种基于BWT运行长度和LZ77短语数的紧凑文本索引，支持快速模式匹配和边界位置查询


<details>
  <summary>Details</summary>
Motivation: 传统文本索引如后缀数组和FM索引需要O(n log n)空间，对于大规模文本存储成本过高。本文旨在利用BWT运行长度和LZ77压缩特性，构建更紧凑的索引结构

Method: 结合Burrows-Wheeler变换的运行长度和LZ77解析的短语数，设计了一种混合压缩索引。通过分析T和其反转文本的BWT运行特性，以及LZ77短语结构，实现空间高效的文本存储和查询

Result: 实现了O(r* log(n/r*) + z log n)比特的存储空间，其中r*是T及其反转文本BWT运行数之和，z是LZ77短语数。支持O(m log n + occ log^ε n)时间的模式匹配，以及O(m log^ε n)时间的左右边界位置查询

Conclusion: 该方法在保持查询效率的同时显著降低了存储空间需求，特别适用于大规模文本处理场景，为压缩文本索引提供了新的设计思路

Abstract: Let $T [1..n]$ be a text over an alphabet of size $\sigma \in
\mathrm{polylog} (n)$, let $r^*$ be the sum of the numbers of runs in the
Burrows-Wheeler Transforms of $T$ and its reverse, and let $z$ be the number of
phrases in the LZ77 parse of $T$. We show how to store $T$ in $O (r^* \log (n /
r^*) + z \log n)$ bits such that, given a pattern $P [1..m]$, we can report the
locations of the $\mathrm{occ}$ occurrences of $P$ in $T$ in $O (m \log n +
\mathrm{occ} \log^\epsilon n)$ time. We can also report the position of the
leftmost and rightmost occurrences of $P$ in $T$ in the same space and $O (m
\log^\epsilon n)$ time.

</details>


### [68] [Weighted Partition Vertex and Edge Cover](https://arxiv.org/abs/2508.13055)
*Rajni Dabas,Samir Khuller,Emilie Rivkin*

Main category: cs.DS

TL;DR: 本文研究了包含分组覆盖约束的顶点覆盖和边覆盖问题的推广，提出了WP-PVC问题的2-近似算法和双准则算法，以及W-PEC问题的首个加权情况精确多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究经典顶点覆盖和边覆盖问题的推广版本，这些版本包含了分组覆盖约束，具有更广泛的实际应用价值。

Method: 对于WP-PVC问题，提出了一个简单的2-近似算法（求解n^ω个LP）和一个适用于大ω的双准则算法；对于W-PEC问题，设计了首个加权情况的精确多项式时间算法，时间复杂度为O(mn+n^2 log n)。

Result: WP-PVC算法的近似比改进为2，移除了之前的枚举步骤和ε因子；W-PEC算法的时间复杂度从O(ωn^3)优化到O(mn+n^2 log n)，并证明了其prize-collecting变体是NP完全的。

Conclusion: 本文为分组覆盖约束下的顶点覆盖和边覆盖问题提供了高效的算法解决方案，在近似比和时间复杂度方面都有显著改进，扩展了相关理论结果的应用范围。

Abstract: We study generalizations of the classical Vertex Cover and Edge Cover
problems that incorporate group-wise coverage constraints. Our first focus is
the \emph{Weighted Prize-Collecting Partition Vertex Cover} (WP-PVC) problem:
given a graph with weights on both vertices and edges, and a partition of the
edge set into $\omega$ groups, the goal is to select a minimum-weight subset of
vertices such that, in each group, the total weight (profit) of covered edges
meets a specified threshold. This formulation generalizes classical vertex
cover, partial vertex cover and partition vertex cover.
  We present two algorithms for WP-PVC. The first is a simple 2-approximation
that solves \( n^{\omega} \) LP's, improving over prior work by Bandyapadhyay
et al.\ by removing an enumerative step and the extra \( \epsilon \)-factor in
approximation, while also extending to the weighted setting. The second is a
bi-criteria algorithm that applies when \( \omega \) is large, approximately
meeting profit targets with a bounded LP-relative cost.
  We also study a natural generalization of the edge cover problem, the
\emph{Weighted Partition Edge Cover} (W-PEC) problem, where each edge has an
associated weights, and the vertex set is partitioned into groups. For each
group, the goal is to cover at least a specified number of vertices using
incident edges, while minimizing the total weight of the selected edges. We
present the first exact polynomial-time algorithm for the weighted case,
improving runtime from \( O(\omega n^3) \) to \( O(mn+n^2 \log n) \) and
simplifying the algorithmic structure over prior unweighted approaches. We also
show that the prize-collecting variant of the W-PEC problem is NP-Complete via
a reduction from the knapsack problem.

</details>


### [69] [A simple analysis of a quantum-inspired algorithm for solving low-rank linear systems](https://arxiv.org/abs/2508.13108)
*Tyler Chen,Junhyung Lyle Kim,Archan Ray,Shouvanik Chakrabarti,Dylan Herman,Niraj Kumar*

Main category: cs.DS

TL;DR: 一种高效的线性方程组解的采样算法，通过根据矩阵行/列范数量量采样，在超线性时间内获得高精度近似解


<details>
  <summary>Details</summary>
Motivation: 解决大规模线性方程组的高效近似解问题，提供一种速度更快、更简洁的算法，清理和简化了以往文献中的分析

Method: 设计了一种基于采样的算法，利用矩阵行/列范数量量采样器，生成解向量的压缩表示，支持对解向量条目的查询和根据平方条目采样

Result: 算法在超线性时间内获得高精度近似解（误差 < ε∗解向量范数），支持快速查询和采样操作，时间复杂度依赖于矩阵的条件数和Frobenius条件数

Conclusion: 该算法提供了一种简洁、基础的方法来解决线性方程组的采样问题，在保证高精度的同时具有良好的计算效率，并且分析过程更加清晰和自含

Abstract: We describe and analyze a simple algorithm for sampling from the solution
$\mathbf{x}^* := \mathbf{A}^+\mathbf{b}$ to a linear system
$\mathbf{A}\mathbf{x} = \mathbf{b}$. We assume access to a sampler which allows
us to draw indices proportional to the squared row/column-norms of
$\mathbf{A}$. Our algorithm produces a compressed representation of some vector
$\mathbf{x}$ for which $\|\mathbf{x}^* - \mathbf{x}\| < \varepsilon
\|\mathbf{x}^* \|$ in $\widetilde{O}(\kappa_{\mathsf{F}}^4 \kappa^2 /
\varepsilon^2)$ time, where $\kappa_{\mathsf{F}} :=
\|\mathbf{A}\|_{\mathsf{F}}\|\mathbf{A}^{+}\|$ and $\kappa :=
\|\mathbf{A}\|\|\mathbf{A}^{+}\|$. The representation of $\mathbf{x}$ allows us
to query entries of $\mathbf{x}$ in $\widetilde{O}(\kappa_{\mathsf{F}}^2)$ time
and sample proportional to the square entries of $\mathbf{x}$ in
$\widetilde{O}(\kappa_{\mathsf{F}}^4 \kappa^6)$ time, assuming access to a
sampler which allows us to draw indices proportional to the squared entries of
any given row of $\mathbf{A}$. Our analysis, which is elementary,
non-asymptotic, and fully self-contained, simplifies and clarifies several past
analyses from literature including [Gily\'en, Song, and Tang; 2022, 2023] and
[Shao and Montanaro; 2022].

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [70] [Ges-QA: A Multidimensional Quality Assessment Dataset for Audio-to-3D Gesture Generation](https://arxiv.org/abs/2508.12020)
*Zhilin Gao,Yunhao Li,Sijing Wu,Yuqin Cao,Huiyu Duan,Guangtao Zhai*

Main category: cs.MM

TL;DR: 提出了Ges-QA数据集和评估模型，用于评估音频到3D手势生成的质量和音频-手势一致性，解决了现有指标无法反映人类偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 当前音频到3D手势生成任务的评估指标（如Fréchet Gesture Distance或Beat Constancy）无法反映人类对生成3D手势的偏好，需要开发基于人类偏好的客观质量评估指标。

Method: 构建了包含1400个样本的Ges-QA数据集，包含多维度的姿势质量和音频-手势一致性评分，以及情感匹配的二分类标签。提出了基于多模态transformer的三分支神经网络（视频、音频和3D骨骼模态）来对A2G内容进行多维度评分。

Result: 比较实验和消融研究表明，Ges-QAer在提出的数据集上达到了最先进的性能。

Conclusion: Ges-QA数据集和评估模型为音频到3D手势生成任务提供了基于人类偏好的有效评估解决方案，填补了现有评估指标的不足。

Abstract: The Audio-to-3D-Gesture (A2G) task has enormous potential for various
applications in virtual reality and computer graphics, etc. However, current
evaluation metrics, such as Fr\'echet Gesture Distance or Beat Constancy, fail
at reflecting the human preference of the generated 3D gestures. To cope with
this problem, exploring human preference and an objective quality assessment
metric for AI-generated 3D human gestures is becoming increasingly significant.
In this paper, we introduce the Ges-QA dataset, which includes 1,400 samples
with multidimensional scores for gesture quality and audio-gesture consistency.
Moreover, we collect binary classification labels to determine whether the
generated gestures match the emotions of the audio. Equipped with our Ges-QA
dataset, we propose a multi-modal transformer-based neural network with 3
branches for video, audio and 3D skeleton modalities, which can score A2G
contents in multiple dimensions. Comparative experimental results and ablation
studies demonstrate that Ges-QAer yields state-of-the-art performance on our
dataset.

</details>


### [71] [CEM-Net: Cross-Emotion Memory Network for Emotional Talking Face Generation](https://arxiv.org/abs/2508.12368)
*Kangyi Wu,Pengna Li,Jingwen Fu,Yang Wu,Yuhan Liu,Sanping Zhou,Jinjun Wang*

Main category: cs.MM

TL;DR: 通过跨情感记忆网络(CEM-Net)解决参考图片情感与音频情感冲突问题，生成更准确的情感说话面部视频


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了参考图片可能包含与音频情感冲突的强烈情感，导致生成结果情感不准确和失真

Method: 提出CEM-Net：1音频情感增强模块(AEE)通过跨重构训练增强音频情感；2情感桥接记忆模块(EBM)补偿参考图片缺乏的面部运动信息，存储从参考图片情感到音频情感的表情位移

Result: 实验证明CEM-Net能够合成表情丰富、自然、口型同步的说话面部视频，情感准确性更好

Conclusion: 跨情感记忆网络有效解决了参考图片与音频情感冲突问题，提高了情感说话面部生成的质量

Abstract: Emotional talking face generation aims to animate a human face in given
reference images and generate a talking video that matches the content and
emotion of driving audio. However, existing methods neglect that reference
images may have a strong emotion that conflicts with the audio emotion, leading
to severe emotion inaccuracy and distorted generated results. To tackle the
issue, we introduce a cross-emotion memory network(CEM-Net), designed to
generate emotional talking faces aligned with the driving audio when reference
images exhibit strong emotion. Specifically, an Audio Emotion Enhancement
module(AEE) is first devised with the cross-reconstruction training strategy to
enhance audio emotion, overcoming the disruption from reference image emotion.
Secondly, since reference images cannot provide sufficient facial motion
information of the speaker under audio emotion, an Emotion Bridging Memory
module(EBM) is utilized to compensate for the lacked information. It brings in
expression displacement from the reference image emotion to the audio emotion
and stores it in the memory.Given a cross-emotion feature as a query, the
matching displacement can be retrieved at inference time. Extensive experiments
have demonstrated that our CEM-Net can synthesize expressive, natural and
lip-synced talking face videos with better emotion accuracy.

</details>


### [72] [MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in Moving Target Selection across Complex Scenarios](https://arxiv.org/abs/2508.12992)
*Xiangxian Li,Yawen Zheng,Baiqiao Zhang,Yijia Ma,XianhuiCao XianhuiCao,Juan Liu,Yulong Bian,Jin Huang,Chenglei Yang*

Main category: cs.MM

TL;DR: MAGNeT是一个多模态自适应高斯网络，通过结合经典统计建模和上下文感知方法，实现了跨场景的少样本高效移动目标选择。


<details>
  <summary>Details</summary>
Motivation: 现有移动目标选择方法需要大量训练数据且缺乏跨场景迁移能力，无法适应多媒体交互系统中多样化的动态环境。

Method: MAGNeT动态融合来自不同场景的预拟合三元高斯模型，基于实时上下文线索进行多模态信息融合，实现少样本自适应。

Result: 在车载振动条件下的2D和3D移动目标选择数据集上，MAGNeT以少样本实现了更低的错误率。

Conclusion: 该方法通过上下文感知的高斯专家融合，在保持模型可解释性的同时，实现了跨场景的有效适应和少样本学习。

Abstract: Moving target selection in multimedia interactive systems faces unprecedented
challenges as users increasingly interact across diverse and dynamic
contexts-from live streaming in moving vehicles to VR gaming in varying
environments. Existing approaches rely on probabilistic models that relate
endpoint distribution to target properties such as size and speed. However,
these methods require substantial training data for each new context and lack
transferability across scenarios, limiting their practical deployment in
diverse multimedia environments where rich multimodal contextual information is
readily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian
Networks), which addresses these problems by combining classical statistical
modeling with a context-aware multimodal method. MAGNeT dynamically fuses
pre-fitted Ternary-Gaussian models from various scenarios based on real-time
contextual cues, enabling effective adaptation with minimal training data while
preserving model interpretability. We conduct experiments on self-constructed
2D and 3D moving target selection datasets under in-vehicle vibration
conditions. Extensive experiments demonstrate that MAGNeT achieves lower error
rates with few-shot samples by applying context-aware fusion of Gaussian
experts from multi-factor conditions.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [73] [Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models](https://arxiv.org/abs/2508.11874)
*Hanyu Li,Dongchen Li,Xiaotie Deng*

Main category: cs.GT

TL;DR: LegoNE框架通过将算法转换为约束优化问题，自动推导和证明算法性能界限，使LLM在几小时内重新发现人类15年才完成的双人博弈算法，并发现超越人类设计的三玩家博弈新算法


<details>
  <summary>Details</summary>
Motivation: 传统算法性能证明需要大量人工且易出错，AI虽能解决具体问题实例，但自动化发现具有可证明保证的通用算法仍是重大挑战

Method: 提出LegoNE框架，将Python类算法自动转换为约束优化问题，通过求解该问题来推导和证明算法的近似界限

Result: LLM在几小时内重新发现双人博弈最优算法（人类耗时15年），并为三玩家博弈发现超越现有算法的新算法

Conclusion: 展示了人机协作新范式：人类在高层抽象推理，用符号压缩搜索空间，AI在其中探索，实现单独无法完成的成果

Abstract: Algorithm design and analysis is a cornerstone of computer science, but it
confronts a major challenge. Proving an algorithm's performance guarantee
across all inputs has traditionally required extensive and often error-prone
human effort. While AI has shown great success in finding solutions to specific
problem instances, automating the discovery of general algorithms with such
provable guarantees has remained a significant barrier. This challenge stems
from the difficulty of integrating the creative process of algorithm design
with the rigorous process of formal analysis. To address this gap, we propose
LegoNE, a framework that tightly fuses these two processes for the fundamental
and notoriously difficult problem of computing approximate Nash equilibria.
LegoNE automatically translates any algorithm written by a simple Python-like
language into a constrained optimization problem. Solving this problem derives
and proves the algorithm's approximation bound. Using LegoNE, a
state-of-the-art large language model rediscovered the state-of-the-art
algorithm for two-player games within hours, a feat that had taken human
researchers 15 years to achieve. For three-player games, the model discovered a
novel algorithm surpassing all existing human-designed ones. This work
demonstrates a new human-machine collaborative paradigm for theoretical
science: humans reason at a higher-abstract level, using symbols to compress
the search space, and AI explores within it, achieving what neither could
alone.

</details>


### [74] [Computing Approximately Proportional Allocations of Indivisible Goods: Beyond Additive and Monotone Valuations](https://arxiv.org/abs/2508.12453)
*Martin Jupakkal Andersen,Ioannis Caragiannis,Anders Bo Ipsen,Alexander Søltoft*

Main category: cs.GT

TL;DR: 本文研究了在非加性估值下，特别是饱和商品场景中的PROP1公平性概念。证明了EF1蕴含PROP1对于次模估值成立，提出了多项式时间算法计算PROP1分配，并揭示了最大Nash福利分配的公平性。


<details>
  <summary>Details</summary>
Motivation: 虽然EF1等近似无嫉妒概念在不可分割商品分配中得到广泛研究，但更简单的PROP1概念在非加性估值下的性质尚未充分探索，特别是在商品可能产生负边际效用的饱和商品场景中。

Method: 研究了饱和商品分配问题，其中代理具有非负但非单调的估值函数。分析了次模和次加性估值下的PROP1性质，使用了Envy-Cycle Elimination、Round-Robin等算法，并考察了最大Nash福利分配。

Result: 证明了EF1蕴含PROP1对于次模估值成立；提出了多项式时间算法计算饱和次加性商品的PROP1分配；发现最大Nash福利分配对于单调次模商品是PROP1的。

Conclusion: PROP1公平性在非加性估值下仍具有良好的性质，多项算法可以高效计算PROP1分配，最大Nash福利分配展现出额外的公平性优势，为饱和商品分配问题提供了理论基础和实用算法。

Abstract: Although approximate notions of envy-freeness-such as envy-freeness up to one
good (EF1)-have been extensively studied for indivisible goods, the seemingly
simpler fairness concept of proportionality up to one good (PROP1) has received
far less attention. For additive valuations, every EF1 allocation is PROP1, and
well-known algorithms such as Round-Robin and Envy-Cycle Elimination compute
such allocations in polynomial time. PROP1 is also compatible with Pareto
efficiency, as maximum Nash welfare allocations are EF1 and hence PROP1.
  We ask whether these favorable properties extend to non-additive valuations.
We study a broad class of allocation instances with {\em satiating goods},
where agents have non-negative valuation functions that need not be monotone,
allowing for negative marginal values. We present the following results:
  - EF1 implies PROP1 for submodular valuations over satiating goods, ensuring
existence and efficient computation via Envy-Cycle Elimination for monotone
submodular valuations;
  - Round-robin computes a partial PROP1 allocation after the second-to-last
round for satiating submodular goods and a complete PROP1 for monotone
submodular valuations;
  - PROP1 allocations for satiating subadditive goods can be computed in
polynomial-time;
  - Maximum Nash welfare allocations are PROP1 for monotone submodular goods,
revealing yet another facet of their ``unreasonable fairness.''

</details>


### [75] [Group Fair Matchings using Convex Cost Functions](https://arxiv.org/abs/2508.12549)
*Atasi Panda,Harsh Sharma,Anand Louis,Prajakta Nimbhorkar*

Main category: cs.GT

TL;DR: 提出了一种基于凸成本函数的平台物品分配算法，通过软约束实现群体公平性，在满足效用阈值的前提下最小化总成本


<details>
  <summary>Details</summary>
Motivation: 传统群体公平性约束（如限制支配和少数群体保护）使用硬约束，缺乏灵活性。需要一种能够平衡平台成本和群体特定成本的软约束方法，允许更灵活的权衡

Method: 使用线性规划和网络流技术，开发多项式时间近似算法。对于特殊情况（统一效用）提供精确算法

Result: 提出了高效的近似算法，具有理论保证和实验评估支持。证明了当群体可以任意相交时，一般问题是困难的

Conclusion: 该方法成功地将硬公平约束替换为基于成本的惩罚，实现了更灵活的公平性权衡，同时保持了计算效率

Abstract: We consider the problem of assigning items to platforms where each item has a
utility associated with each of the platforms to which it can be assigned. Each
platform has a soft constraint over the total number of items it serves,
modeled via a convex cost function. Additionally, items are partitioned into
groups, and each platform also incurs group-specific convex cost over the
number of items from each group that can be assigned to the platform. These
costs promote group fairness by penalizing imbalances, yielding a soft
variation of fairness notions introduced in prior work, such as Restricted
Dominance and Minority protection. Restricted Dominance enforces upper bounds
on group representation, while Minority protection enforces lower bounds. Our
approach replaces such hard constraints with cost-based penalties, allowing
more flexible trade-offs. Our model also captures Nash Social Welfare kind of
objective.
  The cost of an assignment is the sum of the values of all the cost functions
across all the groups and platforms. The objective is to find an assignment
that minimizes the cost while achieving a total utility that is at least a
user-specified threshold. The main challenge lies in balancing the overall
platform cost with group-specific costs, both governed by convex functions,
while meeting the utility constraint. We present an efficient polynomial-time
approximation algorithm, supported by theoretical guarantees and experimental
evaluation. Our algorithm is based on techniques involving linear programming
and network flows. We also provide an exact algorithm for a special case with
uniform utilities and establish the hardness of the general problem when the
groups can intersect arbitrarily.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV是首个针对多上下文KV Cache的注意力稀疏化方法，通过考虑上下文间的互补信息进行稀疏化并局部重计算，在RAG场景中将序列长度压缩至15%且不损失精度


<details>
  <summary>Details</summary>
Motivation: 传统KV Cache重用方法只适用于单上下文场景，在多上下文RAG场景中由于缺乏跨上下文注意力而失效，现有方法需要保留全部KV Cache，无法减少内存开销

Method: SamKV方法在稀疏化一个上下文时考虑其他上下文的互补信息，然后对稀疏化信息进行局部重计算

Result: 实验表明该方法将序列长度压缩到15%，与完全重计算基线相比没有精度损失，在多上下文RAG场景中显著提升吞吐量

Conclusion: SamKV成功解决了多上下文KV Cache的稀疏化问题，为RAG场景中的高效推理提供了有效解决方案

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [77] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 提出了Representation Stability (RS)框架，通过测量重要词汇被遮蔽时嵌入表示的变化来检测对抗文本攻击，无需重新训练模型，在多个数据集和攻击类型上达到88%以上的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗文本防御方法通常是针对特定攻击的或需要昂贵的模型重新训练，需要一个模型无关的检测框架来应对持续存在的对抗文本攻击威胁。

Method: RS框架首先使用重要性启发式方法对词汇进行排序，然后测量遮蔽前k个关键词汇时的嵌入敏感性，最后使用BiLSTM检测器处理产生的模式。使用NDCG衡量扰动识别质量。

Result: 在三个数据集、三种攻击类型和两个受害者模型上，RS实现了超过88%的检测准确率，计算成本较低。基于梯度的排序方法在扰动识别质量上优于注意力和随机选择方法。

Conclusion: RS框架能够很好地泛化到未见过的数据集、攻击和模型，无需重新训练，为对抗文本检测提供了一个实用的解决方案。

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [78] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级的KDCL_sInvResUNet模型，用于从非侵入性生理信号重建动脉血压波形，在嵌入式设备上实现了高效的实时监测。


<details>
  <summary>Details</summary>
Motivation: 虽然现有深度学习模型能够从心电图和光密度调制图等非侵入信号重建动脉血压，但缺乏适合嵌入式系统部署的轻量级模型，需要解决模型性能与计算负荷之间的平衡问题。

Method: 研究提出了轻量级sInvResUNet模型和KDCL_sInvResUNet协同学习方案，模型仅包0.89万参数，计算负荷为0.02 GFLOPS。在包含2,154名病人的1,257,141个数据段的大规模困术期数据集上进行了主体独立验证。

Result: 模型在嵌入式设备上实现了8.49毫秒的推理时间，平均绝对误差为10.06 mmHg，皮尔透相关系数为0.88，性能略好于大型模型。但所有深度学习模型在不同人群和心血管条件下都显示出显著的性能变化。

Conclusion: 该研究为围手术期实际环境中的实时非侵入性血压监测奠定了基础，但模型在广泛多样性人群中的普适性仍有限，需要进一步改进。

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [79] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: 提出MSLoRA-CR方法，通过模态特定的LoRA模块和对比正则化来解决多模态生物医学图像增量学习中的知识保留和跨模态知识迁移问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域需要处理多种模态和任务，但为每个模态单独训练模型会增加推理成本。现有增量学习方法主要关注单模态内的任务扩展，而多模态生物医学图像增量学习需要跨模态的统一模型训练。

Method: 基于大型视觉语言模型，冻结预训练模型，为每个模态或任务增量适配新的LoRA模块。使用对比正则化增强模态内知识共享和促进模态间知识区分。

Result: 在生物医学图像增量学习实验中，MSLoRA-CR优于为每个模态单独训练模型的最先进方法和通用增量学习方法（增量微调LoRA），整体性能提升1.88%，同时保持计算效率。

Conclusion: MSLoRA-CR方法有效解决了多模态生物医学图像增量学习的挑战，在性能和效率方面都表现出色，为跨模态增量学习提供了有效解决方案。

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [80] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的终身学习框架，通过Transformer网络和上下文自注意力机制，使得神经路线规划解决器能够在不同场景下增量学习和过渡知识，在各种VRP问题上达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经解决器通常在单一上下文中训练，如使用欧几里得距离和固定问题规模，导致其在不同场景下的应用受限。需要提高神经解决器的多用途性和适应能力。

Method: 提出终身学习框架(LL)，采用Transformer作为核心网络，通过上下文自注意力机制进行知识过渡。还开发了动态上下文调度器(DCS)，利用跨上下文经验重现技术来回顾之前的解决策略。

Result: 在合成和标准数据集上（问题规模达18k）进行了广泛实验，结果显示该方法能够在不同上下文中发现有效的解决策略，性能超过其他神经解决器，在大部分VRP问题上达到最佳。

Conclusion: 该终身学习框架通过知识过渡和经验重现技术，有效提升了神经路线规划解决器的多用途性和适应性，为处理不同场景下的VRP问题提供了有效解决方案。

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [81] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL是一个基于强化学习的缓存淘汰策略，替代NGINX中的LRU算法，通过深度学习网络在微秒级预算内决策淘汰对象，显著提升缓存命中率


<details>
  <summary>Details</summary>
Motivation: 传统LRU缓存淘汰策略对对象大小不敏感，在周期性突发流量和混合对象大小场景下容易产生抖动问题，需要更智能的淘汰机制

Method: 使用双深度Q网络（Dueling DQN）作为学习策略，通过ONNX侧车服务在500微秒超时预算内处理淘汰决策，从K个最近最少使用对象中提取6个轻量级特征进行学习

Result: 在25MB缓存下命中率从0.1436提升至0.3538（146%提升），100MB缓存下从0.7530提升至0.8675（15%提升），推理增加不到2%的CPU开销，保持95%分位淘汰延迟在预算内

Conclusion: Cold-RL是首个集成到NGINX中具有严格SLO的强化学习淘汰策略，在有限缓存容量下显著优于传统方法，为Web代理缓存优化提供了有效解决方案

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [82] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 时间序列基础模型TimesFM在美国人口预测中表现优于传统方法，在86.67%的测试案例中获得最低均方误差，尤其在历史数据稀缩的少数群体预测中显示优势。


<details>
  <summary>Details</summary>
Motivation: 人口变化受全球化、经济条件、地缘政治和环境因素影响，准确的人口预测对城市规划、医疗健康和经济政策至关重要。需要更有效的预测方法来应对这些挑战。

Method: 使用美国普查局和美联储经济数据(FRED)的数据集，对比时间序列基础模型TimesFM与传统方法(LSTM、ARIMA、线性回归)的性能。在6个人口组成多样的州进行实验。

Result: TimesFM在86.67%的测试案例中获得最低均方误差(MSE)，在历史数据稀缩的少数群体预测中表现特别突出。

Conclusion: 预训练的基础模型能够显著提升人口分析的准确性，为政策干预提供信息支持，且无需大量任务特定的微调。

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [83] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: 基于多源数据的基地规划布局指标系统(SPLI)，通过五大维度系统量化城市空间布局，提高功能分类准确性和自动化分析能力


<details>
  <summary>Details</summary>
Motivation: 传统基地规划依靠经验判断和单一数据源，限制了多功能布局的系统量化分析

Method: 构建SPLI指标系统，整合OSM、POI、建筑形态、土地利用、卫星影像等多源数据，包含五大维度：层级化功能分类、空间组织、功能多样性、基础服务可达性、土地利用强度，使用RGNN和GNN深度学习处理数据缺失

Result: 实验结果显示SPLI系统显著提高了功能分类的准确性

Conclusion: SPLI系统为自动化、数据驱动的城市空间分析提供了标准化基础，改善了基地规划的系统化量化能力

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [84] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: 本文提出了一个在批处理设置中完全真实的校准度量ATB，解决了现有校准度量在有限样本下激励预测器说谎的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的校准度量在有限样本评估时都会激励预测器说谎，而不是输出真实概率，缺乏真实性。虽然已有研究构建了近似真实的校准度量，但在批处理设置中尚未存在完全真实的校准度量。

Method: 设计了平均双箱校准误差（ATB）作为完全真实的校准度量，并提供了一个构建真实度量的一般方法，证明了ATB的真实性并构造了其他真实校准度量如分位数分箱l_2-ECE。

Result: ATB不仅完全真实，还具有合理性、完备性、连续性，且与平滑校准误差和距离校准误差存在二次关系。ATB计算简单高效，相比现有方法实现了更快的运行时间和更简单的实现。

Conclusion: ATB是第一个在批处理设置中完全真实的校准度量，解决了校准度量真实性问题，为校准测试提供了更高效简单的解决方案。

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [85] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了一个识别多元霍克斯过程中潜在子过程和因果影响的方法，通过离散时间模型表示连续时间事件序列，建立了可识别性的充要条件，并开发了一个两阶段迭代算法来恢复因果结构。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统往往只有部分被观测，存在潜在子过程，现有方法主要关注已观测子过程的因果结构发现，难以处理潜在子过程的挑战。

Method: 将连续时间事件序列表示为离散时间模型，建立潜在子过程和因果影响可识别性的充要条件，提出两阶段迭代算法：交替推断已发现子过程的因果关系和发现新的潜在子过程。

Result: 在合成和真实数据集上的实验表明，该方法能有效恢复存在潜在子过程的因果结构。

Conclusion: 该方法为解决多元霍克斯过程中潜在子过程的因果发现提供了有效的解决方案，通过离散时间表示和路径条件保证了可识别性。

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [86] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: 提出一种受脑细胞机制启发的新题特征融合框架BRIEF，通过改进神经网络连接搜索策略和Transformer融合模块，在精神分裂症和孤独症识别中实现了显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在fMRI分类中存在两大问题：网络架构确定依赖经验，特征空间融合主要使用简单拼接缺乏相互学习机制

Method: 首先提取4种fMRI时间表征：时间序列、静态/动态功能连接、多尺度分散熵。构建4个编码器，在每个编码器中使用改进Q学习动态优化神经网络连接搜索（形式化为马尔可夫决策过程），然后通过Transformer融合所有特征向量，嵌入注意力模块提高可解释性

Result: 在精神分裂症（SZ，n=1100）和孤独症识别（ASD，n=1550）中，与21个最新模型相比，BRIEF实现了2.2%到12.1%的显著性能提升，达到AUC分别为91.5%±0.6%（SZ）和78.4%±0.5%（ASD）

Conclusion: 这是首次尝试将受脑细胞机制启发的强化学习策略用亏优化fMRI基于精神障碍分类，显示了在识别精确的神经影像生物标记方面的重要潜力

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [87] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC是一个新颖的联邦学习框架，同时保证差分隐私、拜占庭鲁棒性和通信效率，通过RobAJoL方法实现压缩和鲁棒聚合的结合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以同时实现差分隐私保护、拜占庭攻击鲁棒性和通信效率，需要一种综合解决方案。

Method: 提出robust-compatible压缩概念，结合Johnson-Lindenstrauss变换进行压缩和鲁棒平均进行聚合，理论证明兼容性。

Result: 在CIFAR-10和Fashion MNIST上的实验验证了理论，显示RobAJoL在不同拜占庭攻击下在鲁棒性和效用方面优于现有方法。

Conclusion: Fed-DPRoC框架成功实现了隐私保护、鲁棒性和通信效率的三重目标，为联邦学习提供了实用的解决方案。

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [88] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 利用AlphaEarth Foundations全球地球科表征扩展地球科标签数据集的地理覆盖范围，通过基础模型在美加植被分类任务中达到了过渡性的效果


<details>
  <summary>Details</summary>
Motivation: 高质量地球科标签数据集通常只覆盖特定地理区域，需要一种方法来扩展其覆盖范围以满足全球视角的需求

Method: 利用Google DeepMind的AlphaEarth Foundations(AEF)全球地球科表征，通过随机森林和逻辑回归等基础模型来扩展地理标签数据集的地理范围

Result: 在LANDFIRE植被类型数据集扩展到加拿大的案例中，对13类的EvtPhys级别达到了81%（美国）和73%（加拿大）的分类准确率，话质分析显示模型预测与真实情况一致

Conclusion: 证明了AEF表征可以有效地支持地理标签数据集的跨区域扩展，即使使用简单的模型也能获得过渡性的效果

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [89] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: 提出基于海底声学传感器网络和LGCP的海上空间异常检测框架，通过二阶概率近似和动态传感器部署提高异常分类和检测性能


<details>
  <summary>Details</summary>
Motivation: 传统海上异常检测方法主要基于均值估计，无法充分利用强度函数的方差信息，且传感器部署策略缺乏实时适应性，需要更精确的概率估计和动态检测方案

Method: 使用对数高斯Cox过程建模目标到达，将事件分为正常和异常过程混合模型；提出包含均值和方差的二阶概率近似方法；集成实时近最优传感器部署策略动态调整传感器位置

Result: 在弗吉尼亚州诺福克真实船舶交通数据上验证，数值结果显示该方法相比仅使用均值的方法分类准确率更高，通过传感器部署显著改善了异常检测性能

Conclusion: 提出的二阶概率近似框架为海上空间异常检测提供了更精确的概率估计，结合动态传感器部署策略实现了更好的分类和检测效果，为海事安全监测提供了有效解决方案

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [90] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: 一种新的四阶段联邦学习框架Fed-Meta-Align，通过先训练基础模型、序列元初始化、并行FL精细化和设备个性化，最终在异构IoT设备上实现了91.27%的平均测试准确率，显著超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限的异构IoT设备中实时故障分类的挑战，标准联邦学习在非IID数据环境中容易出现模型分析问题，需要更稳健的模型训练方法。

Method: 四阶段框架：1）在公共数据集上训练基础模型；2）序列元初始化阶段，在IoT设备子集上进行训练以获得异构性知觉的初始化；3）并行FL阶段，使用基于本地性能和余弦相似度的双准则聚合机制；4）设备个性化阶段，将全局模型适配为专门专家。

Result: 在异构IoT设备上达到平均91.27%的测试准确率，在电气和机械故障数据集上分别超过个性化FedAvg 3.87%和FedProx 3.37%。

Conclusion: 通过序列初始化和适应性聚合的多阶段方法，为在多样化TinyML网络上部署高性能智能提供了稳健的途径。

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [91] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文研究了强化学习方法在具有随机结果的验证性领域（如科学实验）中的效果，发现GRPO会导致过度自信的概率预测，而PPO和RLOO能产生良好校准的模型。


<details>
  <summary>Details</summary>
Motivation: 探索当前强化学习方法在具有随机结果的验证性领域（如科学实验）中优化语言模型的有效性，超越确定性数学领域的应用。

Method: 通过合成数据和真实生物实验应用，比较Group Relative Policy Optimization (GRPO)、Proximal Policy Optimization (PPO)和REINFORCE Leave-One-Out (RLOO)等方法的表现。

Result: GRPO会导致二元随机结果的过度自信概率预测，而PPO和RLOO产生良好校准的模型。移除GRPO中的组标准化可以修复其校准问题。

Conclusion: 研究结果提供了反对在GRPO中使用标准标准化的新证据，为强化学习在超越确定性领域的推理语言模型应用铺平了道路。

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [92] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen是一个基于大语言模型的公平感知表格数据生成框架，在保持高实用性的同时显著提升反事实和因果公平性，使用不到20%的原始数据就能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感和数据稀缺的环境中，生成合成表格数据至关重要，但关键挑战在于如何在保持高实用性的同时改善反事实和因果公平性。

Method: 使用基于大语言模型的框架，整合多种公平性定义到生成和评估流程中，采用上下文学习、提示优化和公平感知数据管理来平衡公平性和实用性。

Result: 在多个数据集上优于最先进的GAN和LLM方法，在人口统计均等和路径特定因果效应等公平性指标上提升高达10%，同时保持统计实用性，仅使用不到20%的原始数据。

Conclusion: 该方法提供了一个原则性和实用的途径来生成公平且有用的合成表格数据，在低数据环境下表现出高效性。

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [93] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: 基于Kolmogorov-Arnold表示定理的网络使用ReLU咄三角函数组合来提高计算效率，保持竞争性能的同时改善训练时间咄通用化能力


<details>
  <summary>Details</summary>
Motivation: 屏布B条等多项式函数在GPU设备上支持度不高且不普遍，需要找到计算效率更高的函数组合来构建KANs网络

Method: 推荐使用ReLU咄三角函数（sin, cos, arctan）等快速计算函数作为基础组件，将这些函数组合集成到KANs网络结构中

Result: 实验结果显示这些函数组合能够保持竞争性能水平，同时在训练时间咄通用化能力方面有潜在改善

Conclusion: 使用ReLU咄三角函数组合构建KANs网络是一种有效的方案，能够在保持性能的同时显著提高计算效率

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [94] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: 提出PCA-Grad-CAM和SVM-Grad-CAM方法，用于可视化CNN中PCA和SVM层的注意区域，解决了传统Grad-CAM无法直接应用于这些层的问题。


<details>
  <summary>Details</summary>
Motivation: 当训练样本有限时，CNN中加入PCA和SVM层能提升分类性能，但传统Grad-CAM无法直接应用于这些层，需要开发新的可视化方法来实现白盒化分析。

Method: 通过求解从最后卷积层到PCA和SVM层的闭式雅可比矩阵（Jacobian），提出PCA-Grad-CAM和SVM-Grad-CAM方法来可视化这些层的注意区域。

Result: 在多个主要数据集上展示了方法的可视化结果，证明了方法的有效性。

Conclusion: 该研究成功开发了能够可视化CNN中PCA和SVM层注意区域的新方法，为有限训练数据情况下的白盒化分析提供了技术支持。

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [95] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 这篇论文探索了将线性递归模型扩展到高阶数据的方法，提出了组合线性递归和滑动窗口注意力的ENA模型，在长序列高阶数据建模中表现出艰实的效果。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在处理长序列高阶数据时效率低下，需要更高效的架构来模型这种数据类型。

Method: 研究了两个关键方面：扫描策略和注意力混合架构。重点研究了线性递归与高阶滑动窗口注意力(SWA)的混合架构(ENA)。

Result: 实验结果显示扫描策略带来的改善有限，而注意力混合模型显示出有前景的结果。ENA模型在理论和实践中都表现出高效性。

Conclusion: ENA模型通过线性递归压缩全局信息，给合SWA强化局部建模，为超长高阶数据建模提供了一个有前景的实用解决方案。

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [96] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 通过解耦多尺度时空特征的SDSTM框架，提升了长期交通排放预测的准确性，避免传统方法的级联错误放大问题


<details>
  <summary>Details</summary>
Motivation: 传统时空图模型在长期交通排放预测中存在多尺度时空特征缀缠和级联错误放大的问题，需要新方法来解决这些挑战

Method: 提出尺度解耦时空建模(SDSTM)框架：1)使用Koopman提升算子进行双流特征分解 2)通过门控波射分解定义预测性边界 3)构建包含双流独立约束的融合机制，以动态精炼预测结果

Result: 在西安二环路道路级交通排放数据集上进行的广泛实验表明，该模型达到了最先进的性能

Conclusion: SDSTM框架通过有效解耦多尺度时空特征并构建独立但互补的预测流，显著提升了长期交通排放预测的准确性，为城市空气污染管理提供了有效觤经

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [97] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [98] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD是一个基于元学习的多模态OOD检测器选择框架，通过历史模型行为学习，自动为新的数据分布偏移推荐合适的检测器。


<details>
  <summary>Details</summary>
Motivation: 多模态环境下OOD检测面临挑战，单个检测器无法适应所有分布偏移场景，且由于无监督特性难以预测模型性能，手动比较成本高昂。

Method: 结合多模态嵌入和手工设计的元特征来表示数据集，利用历史性能数据，通过元学习框架快速适应新的数据分布偏移。

Result: 在12个测试场景中 consistently 优于10个竞争基线方法，计算开销最小。

Conclusion: M3OOD框架有效解决了多模态OOD检测器选择问题，能够自动为不同分布偏移推荐合适的检测模型。

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [99] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: 通过分离前向噪声模拟和向后梯度计算，扩展STE框架实现了更准确但计算复杂的模拟噪声认知训练，在模拟计算存储噪声下获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 模拟计算存储噪声存在复杂硬件噪声问题，现有噪声认知训练方法依赖理想化的可微分噪声模型，无法抓取硬件变化的完整复杂性

Method: 受量化中STE框架的启发，将前向噪声模拟与向后梯度计算解耦，允许使用更准确但计算不可行的噪声模型进行噪声认知训练

Result: 在图像分类任务上达到超10.6%的准确率提升，文本生成任务上降低0.72的困惑度，训练时间加速2.2倍，并节省37.9%的峰值内存使用

Conclusion: 扩展STE框架为模拟计算存储噪声认知训练提供了高效解决方案，在保持计算可行性和优化稳定性的同时，能够利用更准确的噪声模型获得显著性能改善

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [100] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: 该研究针对神经网络的标记时间点过程(MTPP)模型提出了CFF解释方法，结合反事实和事实解释来识别历史事件中的最小理性解释子集，以提高模型输出的可信度。


<details>
  <summary>Details</summary>
Motivation: 神经网络MTPP模型在高风险应用中被广泛采用，但其输出的可信度存在担忧。需要找到能够保持预测准确性的最小历史事件子集作为合理解释。

Method: 提出CFF(Counterfactual and Factual Explainer)方法，结合反事实解释和事实解释，通过精心设计的技术来解决MTPP的解释问题。

Result: 实验证明CFF在解释质量和处理效率方面均优于基线方法，显示出正确性和优越性。

Conclusion: 将MTPP解释定义为反事实和事实解释的组合能够产生更理性的解释，CFF方法为此提供了有效的解决方案。

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [101] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 提出Set-Valued Transformer Network (SVTN)来解决高排放车辆识别中的长尾分布问题，通过transformer提取时序特征和集合值识别算法进行概率建模，在合肥柴油车监测数据上取得了9.5%的漏检率降低。


<details>
  <summary>Details</summary>
Motivation: 高排放车辆识别对城市污染管控至关重要，但实际监测数据中存在长尾分布问题（高排放样本稀少），且排放状态高度非线性、缺乏先验知识，给识别模型构建带来挑战。

Method: 使用Set-Valued Transformer Network (SVTN)：1) 用transformer测量微行程条件变化的时序相似性，将高维排放数据映射到低维特征空间；2) 用集合值识别算法对特征向量与标签关系进行概率建模，为分类提供准确度量标准。

Result: 在合肥市2020年柴油车监测数据上的实验表明，相比基于transformer的基线方法，该方法将高排放车辆的漏检率降低了9.5%。

Conclusion: SVTN方法能够有效学习高排放样本的判别性特征，显著提高了高排放移动污染源的准确识别能力。

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [102] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: LoRA模块可以通过简单加法组合，无需额外训练即可实现多领域适配，性能接近合并数据训练，并揭示了高阶组合中的干扰现象


<details>
  <summary>Details</summary>
Motivation: 基于叠加原理的假设，研究独立训练的LoRA模块在不相交领域上的正交性，探索通过简单加法组合来实现参数高效的多领域适配

Method: 使用GPT-2 Small模型，LoRA秩为4，alpha=64，在三个QA领域（数学、医学、金融）上独立训练适配器，通过计算余弦相似度分析模块间关系

Result: 数学+医学组合相对合并数据微调困惑度改善-9.10%，数学+金融+4.54%，金融+医学+27.56%；LoRA增量间的RMS余弦相似度与困惑度变化呈正相关线性关系

Conclusion: 朴素加法组合无需额外训练，秒级完成，性能与合并数据训练相当，同时揭示了高阶组合中干扰出现的条件

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [103] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: 通过谱筛法学习稳定非线性动态系统，给出了预测错误渐近于零的算法


<details>
  <summary>Details</summary>
Motivation: 解决学习边畏稳定未知非线性动态系统的基本问题，提供一种通用的学习方法

Method: 基于谱表示技术的算法，通过线性动态系统的新谱筛法算法，结合在线凸优化技术

Result: 对任何有限边畏稳定模态的非线性动态系统，证明了预测错误的渐近于零

Conclusion: 该算法显著扩展了原始谱筛法的应用范围，能处理非对称动态和噪声校正，具有独立研究价值

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [104] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FedUHD是基于超维计算的第一个无监督联邦学习框架，相比传统神经网络方法在速度、能效、通信成本和准确性方面有显著提升，并具有更好的噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无监督联邦学习面临非独立同分布数据、高计算通信成本和通信噪声脆弱性等挑战，传统神经网络方法存在较大开销。

Method: 提出基于超维计算的FedUHD框架，客户端使用kNN聚类超向量去除方法处理非iid数据，服务器端采用加权HDC聚合技术平衡数据分布。

Result: 实验显示FedUHD在训练速度上提升173.6倍，能效提升612.7倍，通信成本降低271倍，平均准确率提高15.50%，且对噪声具有优异鲁棒性。

Conclusion: FedUHD证明了超维计算在无监督联邦学习中的有效性，为解决实际应用中的计算、通信和噪声问题提供了高效解决方案。

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [105] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习中的性能公平性问题，提出了FairGrad和FairGrad*两种梯度方差正则化方法，在异构数据环境下同时提升了公平性和整体模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据异构性导致某些客户端对全局模型产生不成比例的影响，造成性能差异。现有公平性方法在异构数据环境下的有效性不明确，不同方法之间的关系也不清楚。

Method: 研究专注于性能公平性，评估了显式正则化客户端损失的公平性方法，包括新提出的FairGrad（近似）和FairGrad*（精确）两种梯度方差正则化变体。

Result: 理论分析了所研究公平性方法之间的联系，实证表明FairGrad和FairGrad*在异构数据设置下能够同时改善公平性和整体模型性能。

Conclusion: 提出的梯度方差正则化方法有效解决了联邦学习中的性能公平性问题，在保持隐私保护优势的同时提升了模型的公平性和整体性能。

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [106] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN是一个动态层聚合框架，通过输入自适应的层权重和专用探测头，解决了传统静态聚合方法的信息瓶颈问题，在语音识别和情感识别任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的自监督语音模型层聚合方法（如使用最后一层或加权求和）存在信息瓶颈问题，并且对所有数据样本采用静态特征加权，无法根据输入内容动态调整。

Method: 提出VARAN框架，采用层专用探测头和数据依赖的权重分配，根据输入内容动态调整各层特征的优先级，实现个性化的层聚合。

Result: 在自动语音识别和语音情感识别任务上的评估显示VARAN具有优越性能，特别是与LoRA微调技术结合使用时效果更佳。

Conclusion: VARAN框架解决了保留层特定信息与实现灵活特征利用之间的权衡问题，推动了自监督语音表示的高效适配。

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [107] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: 该论文提出了一个用于ISAC-AIGC网络的CAQA服务质量评估指标，并开发了LPDRL-F算法来解决三维资源分配问题，显著提升了平均CAQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC服务假设输入数据准确，只关注生成质量，但在ISAC-AIGC网络中，内容生成基于不准确的感知数据，且AIGC模型本身存在生成误差，需要新的服务质量评估方法。

Method: 提出CAQA评估指标，并设计LP引导的深度强化学习算法LPDRL-F，通过线性规划指导和动作过滤器将三维解空间降为二维，降低复杂度并提升学习性能。

Result: 仿真显示LPDRL-F比现有DRL和生成扩散模型算法收敛快60%以上，找到更好的资源分配方案，AvgCAQA提升超过14%，相比仅关注CGQ的方案提升超过50%。

Conclusion: LPDRL-F算法有效解决了ISAC-AIGC网络中的三维资源权衡问题，显著提升了服务质量和用户体验。

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [108] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是基于160亿次医疗事件训练的大型医疗基础模型，通过自回归生成模拟患者健康时间线，在78个医疗任务中无需微调即可超越或匹配专门训练的监督模型。


<details>
  <summary>Details</summary>
Motivation: 实现规模化个性化医疗需要从纵向患者旅程中提取洞察，基础模型预训练在大规模医疗事件数据上代表了扩展真实世界证据生成和泛化到多样化下游任务的有希望方向。

Method: 使用Epic Cosmos数据集（163亿次就诊、3亿患者记录），训练解码器Transformer模型CoMET，进行最大规模的医疗数据缩放定律研究，预训练计算最优模型（最多10亿参数），通过自回归生成下一个医疗事件来模拟患者健康时间线。

Result: 在78个真实世界任务（诊断预测、疾病预后、医疗运营）中，CoMET通常优于或匹配任务特定的监督模型，无需任务特定微调或少样本示例，预测能力随模型和预训练规模持续提升。

Conclusion: CoMET作为生成式医疗事件基础模型，能有效捕捉复杂临床动态，为支持临床决策、简化医疗运营和改善患者结局提供了可扩展和可泛化的框架。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [109] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT是一种动态自动化指令调优数据集混合优化方法，通过多臂老虎机框架和先验缩放玻尔兹曼探索，在保持数据集多样性的同时实现性能提升，在16个数据集上获得2.2%的性能改进。


<details>
  <summary>Details</summary>
Motivation: 随着后训练阶段大量指令调优数据集的出现，如何动态平衡和优化这些数据集的混合比例成为一个关键挑战。

Method: 将问题建模为多臂老虎机框架，提出先验缩放玻尔兹曼探索方法，使用轻量级1步前瞻奖励来更新采样概率，软性地将更新后的采样分布锚定到原始数据集比例。

Result: 在包含16个指令调优数据集的Tulu-v2-mixture集合上应用，在10个基准测试中实现了高达2.2%的性能提升。

Conclusion: DynamixSFT提供了一种有效的动态数据集混合优化方法，通过自适应采样策略在保持数据集多样性的同时显著提升模型性能。

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [110] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 门控机制在RNN中不仅控制状态记忆，还作为数据驱动的预处理器自适应调节参数更新，实现类似自适应学习率的效果


<details>
  <summary>Details</summary>
Motivation: 研究门控RNN中门控机制如何隐式地诱导自适应学习率行为，即使使用固定全局学习率训练时也能产生这种效果

Method: 通过推导泄漏积分器和门控RNN的精确雅可比矩阵，获得一阶展开式，分析门控如何重塑梯度传播、调节有效步长并引入参数更新的各向异性

Result: 门控不仅控制隐藏状态中的记忆保留，还作为数据驱动的预处理器自适应优化轨迹，数值实验验证了扰动分析的有效性

Conclusion: 这项工作提供了统一的动力学系统视角，解释了门控架构在实践中实现稳健可训练性和稳定性的原因

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [111] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE是一种基于微分熵的不确定性感知变分自编码器，用于改进参数化和可逆投影，在处理分布外样本时表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器方法在处理数据空间或嵌入空间的分布外样本时表现不佳，需要一种能够分析嵌入不确定性的改进方法。

Method: 使用微分熵的变分自编码器，学习从原始空间到2D空间的映射及其逆映射，基于固定投影进行训练。

Result: 在四个知名数据集上的定量和定性评估显示，DE-VAE在保持与其他AE方法相当精度的同时，能够分析嵌入不确定性。

Conclusion: DE-VAE能够创建高质量的参数化和可逆投影，同时提供嵌入不确定性分析能力，优于UMAP和t-SNE等基线方法。

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [112] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的深度学习模型AICRN，通过空间和通道注意力机制来提高心电图参数回归的精确度，以实现可解释性的ECG分析。


<details>
  <summary>Details</summary>
Motivation: 传统ECG分析存在人为错误导致失焦、手动分析耗时等挑战，需要更准确和高效的自动化分析方法来提升心脏疾病诊断精度。

Method: 设计了注意力集成卷积残差网络(AICRN)，结合空间和通道注意力机制来处理ECG特征类型和空间位置，并使用卷积残差网络解决核心参数回归问题。

Result: AICRN模型在ECG参数回归任务中表现超过现有模型，具有更高的精确度，能够准确预测PR间隔、QT间隔、QRS时程等关键参数。

Conclusion: 深度学习可以在ECG分析的可解释性和精确性方面发挥关键作用，为心脏监测和管理开启了新的临床应用前景。

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [113] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC是一个轻量级的两阶段压缩框架，通过联合嵌入压缩和自压缩模块，将蛋白质输入长度减少一半，演示长度从751个token压缩到少于16个token，在16-shot设置下实现93.68%的压缩比，显著提升了蛋白质功能预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决ProtTeX模型的两个主要限制：1）序列和结构token拼接导致蛋白质长度翻倍并破坏模态间对齐；2）受限于训练语料和上下文窗口，无法进行上下文学习，限制了泛化能力。

Method: 提出两阶段压缩框架：1）联合嵌入压缩机制在残基级别融合序列和结构表示；2）自压缩模块将完整演示聚合到最后几个语言token的潜在空间中。仅通过PEFT调优和单个可训练投影层引入少量额外参数。

Result: 在蛋白质功能预测任务中，域内基准性能提升2%，域外数据集性能增益达11%，压缩比达到93.68%。

Conclusion: ProtTeX-CC在不修改主干模型的情况下，通过高效的压缩机制显著提升了ProtTeX在少样本设置下的性能，具有良好的泛化能力。

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [114] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型的确定性遗忘方法，通过记录训练过程的微批次日志来实现精确的遗忘操作，确保模型参数与在保留数据集上重新训练的结果完全相同。


<details>
  <summary>Details</summary>
Motivation: 研究GDPR第17条规定的被遗忘权在大语言模型中的实现，将遗忘问题转化为可重现的系统问题，确保模型能够精确删除特定数据的影响。

Method: 将训练视为确定性程序，记录每个微批次的ID哈希、RNG种子、学习率值、优化器步数计数器和累积边界。在固定堆栈和确定性内核下，重放训练尾部并过滤遗忘闭包。还提供了补充路径：精确回滚最近步骤、队列范围的适配器删除以及曲率引导的反向更新。

Result: 在满足前提条件的受控运行中，证明了模型和优化器状态的字节级完全相等，验证了存储/延迟预算和机制有效性。

Conclusion: 该方法为大规模语言模型提供了可验证的遗忘解决方案，能够满足GDPR合规要求，同时保持模型性能的完整性。

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [115] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: 基于一致性模型的新型分布匹配方法，结合了连续正则化流模型的直接规范最小化优势和GAN的灵活性


<details>
  <summary>Details</summary>
Motivation: 解决GAN在分布匹配任务中的训练困难，包括双层最小最大优化目标和模式冲突问题

Method: 受连续正则化流模型(CNF)的一致性模型启发，提出新的分布匹配方法，继承了CNF的直接规范最小化优点和GAN的约束适配性

Result: 通过理论验证和实验展示了方法的性能，包括合成数据集和真实世界数据集的实验

Conclusion: 该方法为分布匹配任务提供了一种有效的替代方案，充分利用了一致性模型的优势

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [116] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: 在分布式优化和联邦学习中，通过在异步ADMM中引入粗粗化量化来降低通信开销，实验验证了该方法在包括神经网络在内的多个分布式学习任务中的收敛性。


<details>
  <summary>Details</summary>
Motivation: 分布式优化和联邦学习中，异步ADMM是大规模优化的选择，但通信成本可能成为瓶颈，特别是当节点通信预算有限或数据过大时。

Method: 在异步ADMM中对交换数据引入粗粗化量化，以降低通信开销。

Result: 实验验证了该方法在多个分布式学习任务中的收敛性，包括神经网络。

Conclusion: 粗粗化量化可以有效降低分布式优化和联邦学习中的通信开销，同时保持算法的收敛性。

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [117] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CC-Time是一个结合预训练语言模型和时间序列模型的跨模态跨模型学习方法，通过文本描述和时间序列数据的融合，在时间序列预测任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练语言模型的时间序列预测方法未能充分发挥语言模型的强大序列建模能力，预测精度不够理想。需要探索语言模型能够建模哪些时间序列特征，以及是否仅依赖语言模型就足够构建时间序列模型。

Method: 1) 跨模态学习：通过时间序列序列及其对应的文本描述，在语言模型中建模时间依赖性和通道相关性；2) 跨模型融合：提出跨模型融合块，自适应地整合语言模型和时间序列模型的知识，形成更全面的时间序列模式建模。

Result: 在9个真实世界数据集上的大量实验表明，CC-Time在完整数据训练和少样本学习情况下都达到了最先进的预测精度。

Conclusion: CC-Time成功探索了预训练语言模型在时间序列预测中的潜力，通过跨模态和跨模型学习有效提升了预测性能，证明了这种融合方法的有效性。

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [118] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: 提出了DHG-Bench，这是第一个深度超图学习的综合基准测试，包含20个数据集和16种最先进的超图神经网络算法，在统一的数据处理和实验协议下进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络方法缺乏全面的基准测试，导致在数据集覆盖、算法性能评估和实验设置一致性方面存在障碍，阻碍了对深度超图学习进展的理解。

Method: 构建了包含20个多样化数据集（涵盖节点、边和图级别任务）和16种最先进HNN算法的综合基准DHG-Bench，采用一致的数据处理和实验协议，从有效性、效率、鲁棒性和公平性四个维度系统评估算法特性。

Result: 广泛的实验揭示了现有算法的优势和固有局限性，为未来研究提供了有价值的见解和方向。

Conclusion: DHG-Bench填补了深度超图学习领域基准测试的空白，通过系统性的评估框架促进了可重复研究，并为该领域的未来发展提供了重要参考。

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [119] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: 通过多规模Mamba架构和适配图因果卷积网络，STM2/STM3模型高效处理长期时空时间序列预测中的多尺度依赖关系，实现了独特的多尺度信息提取和模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在长期时空依赖关系学习上效率低下，特别是面临多尺度信息提取难和不同节点间高度相关的多尺度时间信息模型难两大挑战。

Method: STM2采用多尺度Mamba架构高效同时提取多尺度信息，结合适配图因果卷积网络学习复杂的多尺度时空依赖关系。STM3在此基础上采用混合专家架构，包括更稳定的路由策略和因果对比学习策略来增强尺度区分能力。

Result: 在多个实际数据集上的实验表明，STM2/STM3在长期时空时间序列预测任务中达到了最先进的性能，实现了独特的多尺度信息提取和模型能力。

Conclusion: 该研究提出的STM2/STM3模型有效解决了长期时空时间序列预测中的多尺度依赖关系学习问题，为该领域提供了高效的解决方案。

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [120] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，使用LIME和SHAP来解释时间序列预测，结合了ARIMA的可解释性和XGBoost的高精度，通过泄漏避免的监督学习转换和滞后特征分析来提供模型解释。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在关键领域决策中至关重要，但传统ARIMA模型难以处理非线性，而树模型如XGBoost虽然准确但缺乏可解释性。需要一种既能保持高精度又能提供解释的方法。

Method: 将单变量时间序列转换为无泄漏的监督学习问题，训练梯度提升树和ARIMA基线模型，然后应用LIME和SHAP进行后验解释性分析，特别关注滞后特征和季节性编码。

Result: 使用航空乘客数据集验证，发现少量滞后特征（特别是12个月滞后）和季节性编码能够解释大部分预测方差，证明了方法的有效性。

Conclusion: 该研究提供了将LIME和SHAP应用于时间序列的方法论、理论阐述、实证评估和实践指南，为时间序列预测的可解释性提供了有效解决方案。

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [121] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出一种新的学习二阶优化器，通过可训练的预处理单元改进经典SR1算法，在人体网格恢复任务上表现超过现有方法


<details>
  <summary>Details</summary>
Motivation: 绘合深度学习和经典优化方法的优势，解决端到端学习对大标签数据集的依赖、较差的统一性和计算资源消耗大等问题

Method: 使用可训练的预处理单元生成数据驱动的向量，构造正半定矩阵，通过学习投影满足离散约束

Result: 在分析实验和单目人体网格恢复任务上表现超过现有学习优化方法，具有较强的统一性

Conclusion: 该方法模型轻量、无需标注数据或微调，适合集成到更广泛的优化框架中

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [122] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC是一个用于图异常检测的框架，通过上下文重构和对比学习，在有限标签数据下有效训练GNN模型，显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 解决图异常检测中标签数据稀缺、异常样本稀少且可能伪装的问题，利用有限的标注数据和大量未标注数据进行联合训练

Method: 通过重构节点上下文构建增强图，保持交互模式的同时重组节点属性；分别编码异构关系并整合到消息传递过程中；结合对比学习范式利用未标注数据

Result: 在7个真实世界数据集上评估，相比基线GNN提升高达14% AUC，在有限标签设置下优于最先进的GAD方法

Conclusion: CRoC框架通过上下文重构和对比学习的结合，有效解决了图异常检测中的数据稀缺和异常伪装问题，显著提升了检测性能

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [123] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文分析了Lion优化器的收敛性能，提出了标准版本、方差缩减版本以及分布式通信高效版本的收敛速率分析


<details>
  <summary>Details</summary>
Motivation: 研究Lion优化器的理论收敛性质，改进其收敛速率，并探索在分布式设置下的通信效率优化

Method: 建立理论分析框架，引入方差缩减技术改进标准Lion优化器，设计分布式版本并采用符号压缩通信策略

Result: 标准Lion获得O(d^{1/2}T^{-1/4})收敛率，方差缩减版本提升至O(d^{1/2}T^{-1/3})；分布式版本在n个节点下获得O(d^{1/2}(nT)^{-1/4})和O(d^{1/2}(nT)^{-1/3})；通信高效版本达到O(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})})

Conclusion: Lion优化器具有良好的收敛性能，通过方差缩减和通信压缩技术可以显著提升收敛速率和通信效率

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [124] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 通过Funnel Schedule和Adaptive Temperature策略解决扩散模型中的探索-利用两难问题，在不增加计算成本的情况下显著提升样本质量


<details>
  <summary>Details</summary>
Motivation: 将语言模型中成功的推理时缩放技术适配到扩散模型，解决现有SMC方法在扩散模型中遇到的早期样本评估困难和后期样本不可逆的两难问题

Method: 提出Funnel Schedule（渐进式缩减维护粒子数）和Adaptive Temperature（适应性温度调节）两种策略，通过逐步减少维护粒子数和降低早期奖励影响来优化探索-利用平衡

Result: 在多个标准测试集和最新文本生成图像扩散模型上，该方法表现超过了之前的基准方法，显著提升了样本质量

Conclusion: 通过针对扩散模型生成动力学和相变行为特点设计的简单有效策略，成功解决了SMC在扩散模型中的探索-利用两难问题，为推理时缩放技术在扩散模型中的应用提供了新的视角和方法

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [125] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: 提出了Bi-Axial Transformer (BAT)模型，通过同时在临床变量轴和时间点轴上应用注意力机制来处理电子健康记录数据，解决了数据稀疏性和缺失值问题，在脓毒症预测任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHRs)数据日益复杂，包含更大规模数据集、更长的时间序列和多模态集成。传统Transformer在处理EHR分类时受限于数据表示方式，性能受限且难以捕捉信息性缺失。

Method: 开发了Bi-Axial Transformer (BAT)模型，同时在临床变量轴和时间点轴两个维度上应用注意力机制，学习更丰富的数据关系，处理数据稀疏性问题。

Result: BAT在脓毒症预测任务上达到最先进性能，在死亡率分类任务上与顶级方法竞争力相当。相比其他Transformer模型，BAT对数据缺失具有更强的鲁棒性，并能学习可用于迁移学习的独特传感器嵌入。

Conclusion: BAT模型通过双轴注意力机制有效处理EHR数据的复杂性和稀疏性，为EHR分析提供了新的有效方法，同时提供了重新实现的基准模型用于复现和未来基准测试。

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [126] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: 提出基于机器学习的制造成本估算框架，直接从2D工程图中提取几何和统计特征，实现10%的平均绝对百分比误差，缩短报价周期并提供可解释的成本驱动因素分析。


<details>
  <summary>Details</summary>
Motivation: 传统制造报价流程需要大量人工工艺规划，过程耗时且不一致。需要开发自动化、可扩展的成本估算方法来提高效率。

Method: 从13,684个汽车悬挂和转向部件DWG图纸中提取约200个几何和统计描述符，使用梯度提升决策树模型（XGBoost、CatBoost、LightGBM）进行训练，并结合SHAP等可解释性工具。

Result: 模型在24个产品组中实现接近10%的平均绝对百分比误差，表现出良好的可扩展性，超越了特定部件的启发式方法。

Conclusion: 该端到端CAD到成本管道缩短了报价周期，确保跨部件家族的一致透明成本评估，为工业4.0制造环境中的实时ERP集成决策支持提供了可行路径。

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [127] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: 基于局部距离分布估计聚类数量的自适应均倾算法，能够在局部粒度和聚类规模变化的数据集上获得更好的聚类效果


<details>
  <summary>Details</summary>
Motivation: 解决传统均倾算法在局部粒度和聚类规模变化的数据集上性能不佳的问题，特别是基于核密度估计的方法只能提供局部区域的视角

Method: 利用点到其他所有点的局部距离分布来估计局部聚类数量，通过识别距离分布中的局部最小值来识别聚类规模，然后基乎这些估计计算整个聚类的局部参数，在均倾执行过程中动态调整带宽和核半径阈值

Result: 算法在原始数据集上超过了最近提出的自适应均倾方法，并在更广泛的聚类测试中展现了竞争性能

Conclusion: 通过局部距离分布来估计聚类数量的方法有效地提升了均倾算法在复杂数据集上的性能，为处理局部粒度和聚类规模变化的问题提供了有效解决方案

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [128] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: CSCR是一个轻量级框架，通过将提示和模型映射到共享嵌入空间，实现快速、成本敏感的模型选择，在多个基准测试中比基线方法提升25%的准确率-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略提示特定上下文、依赖昂贵的模型分析、假设固定专家集合或使用低效的试错策略，需要更高效的成本感知路由方案。

Method: 使用紧凑的logit足迹处理开源模型，困惑度指纹处理黑盒API，通过对比编码器训练选择最便宜准确专家，推理时通过FAISS索引进行k-NN查找。

Result: 在多个基准测试中持续优于基线方法，准确率-成本权衡提升达25%，对未见过的LLM和分布外提示具有强泛化能力。

Conclusion: CSCR提供了一个高效、轻量级的成本感知路由框架，无需重新训练即可适应变化的专家池，实现微秒级延迟，显著改进了LLM路由的性能成本权衡。

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [129] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: 通过优化路径空间测度接近目标测度来解决随机最优控制问题，提出了基于信任域的迭代策略，通过系统性逐步接近目标来改善性能。


<details>
  <summary>Details</summary>
Motivation: 当目标测度与先验测度差异显著时，通过梯度优化接近目标测度的方法面临挑战，需要更有效的解决方案。

Method: 提出迭代解决包含信任域的约束问题，通过系统性逐步接近目标测度。这种基于信任域的策略可理解为从先验到目标测度的几何透温，但信任域提供了选择透温路径时间步长的原理性方法。

Result: 在多个最优控制应用中证明，新方法能够显著提高性能，包括滑散基取样、过渡路径取样以及滑散模型微调任务。

Conclusion: 基于信任域的迭代策略为解决随机最优控制问题提供了一种系统性的方法，通过逐步接近目标测度来充分利用先验信息，在多个应用场景中都取得了显著成效。

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [130] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: NeurIPS 2023 Neural MMO竞赛吸引了200多名参与者，参赛者训练的目标条件策略能够泛化到训练中未见过的任务、地图和对手，最佳解决方案在单张4090 GPU上训练8小时就达到了基线4倍的分数。


<details>
  <summary>Details</summary>
Motivation: 举办大规模AI竞赛来推动目标条件策略在复杂多智能体环境中的泛化能力研究，探索策略在未见过的任务、地图和对手上的表现。

Method: 使用Neural MMO平台进行竞赛，参赛者训练目标条件策略，要求策略能够泛化到训练期间未见过的各种情境。

Result: 竞赛吸引了200多名参与者，最佳解决方案在单张4090 GPU上训练8小时后，得分达到基线的4倍，表现显著优于基线方法。

Conclusion: Neural MMO竞赛成功展示了目标条件策略在复杂环境中的强大泛化能力，开源了所有相关资源包括策略权重和训练代码，为后续研究提供了宝贵资源。

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [131] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 通过提出局部后验冲突概念和潜在重建损失，解决VAE后验冲突问题，无需网络结构限制


<details>
  <summary>Details</summary>
Motivation: 解决VAE后验冲突问题，提高生成样本多样性，免去结构限制

Method: 定义局部后验冲突，提出Latent Reconstruction损失函数，基于注入和复合函数的数学性质

Result: 在MNIST、fashionMNIST、Omniglot、CelebA、FFHQ等多个数据集上有效控制后验冲突

Conclusion: LR损失能够无需特定网络结构限制地控制VAE后验冲突，提高生成质量

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [132] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: 通过优化训练超参数和使用EMA动量技术，可以在细调语言模型时保持安全性而无需额外安全措施，将有害响应从16%降至约5%


<details>
  <summary>Details</summary>
Motivation: 反驳常见假设，即细调语言模型必然会损害其安全性，证明安全问题主要由优化选择不当导致，而非内在交易

Method: 系统性测试和选择关键训练超参数（学习率、批处理大小、梯度步长），提出参数空间指数移动平均（EMA）动量技术

Result: 在Llama模型家族上验证，将不安全响应从16%显著降至约5%，同时保持模型效能，超越需要额外安全数据的现有方法

Conclusion: 细调过程中的安全问题可以大幅避免而无需专门应对措施，提供了维持模型性能和安全性的实用指南

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [133] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: 这篇论文从数据中心AI角度系统性地定义和测试了大脑图构建的设计空间，通过对时间信号处理、拓扑提取和图特征化三个阶段的系统化分析，证明思考周到的数据中心配置能够持续提升下游分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大脑图构建工作依赖粗糖的流水线模式，忽视了构建过程中的关键数据中心选择，而现有研究主要集中在模型中心方案上。这个差距导致了构建的大脑图在下游图机器学习任务中的性能次优。

Method: 研究将大脑图构建设计空间组织为三个阶段：时间信号处理、拓扑提取和图特征化。采用了一系列现有和修改技术的组合进行评估，包括高振幅BOLD信号筛波、连接性稀疏化和统一策略、替代相关指标以及多视图节点和边特征（如包含滞后动态学）。

Result: 在HCP1200和ABIDE数据集上的实验结果显示，经过思考的数据中心配置能够持续地在分类准确性上超越标准流水线。这些发现强调了上游数据决策在图基神经影像学中的关键作用。

Conclusion: 这项研究强调了系统性探索数据中心设计空间对于基于图的神经影像学的重要性。研究结果表明，通过经过思考的数据构建策略可以显著提升下游分类任务的性能，为该领域的未来研究提供了新的方向。

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [134] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1是一个基于规则强化学习的Linux内核调优框架，使用LLM进行高效探索，通过自定义奖励函数和两阶段训练过程，在多种调优场景中实现高达5.6%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Linux内核调优方法在效率、可扩展性和泛化性方面存在挑战，需要一种更智能、高效的自动化调优解决方案。

Method: 将内核配置空间抽象为RL环境，利用LLM进行高效探索，设计自定义奖励函数提升推理标准化和配置修改准确性，采用两阶段训练过程加速收敛。

Result: OS-R1显著优于现有基线方法，相比启发式调优实现最高5.6%的性能提升，保持高数据效率，并在各种实际应用中展现良好适应性。

Conclusion: OS-R1框架展示了在实际多样化环境中部署的潜力，为Linux内核自动化调优提供了有效的解决方案。

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [135] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: 提出了一种可视化分析系统，用于改善对LLM驱动编码代理行为的审查和调整效果


<details>
  <summary>Details</summary>
Motivation: 当前手动检查代理编码输出的方式效率低下，难以跟踪代码迭代过程和评估改进机会，需要更有效的分析工具

Method: 开发了一种可视化分析系统，支持三个层面的比较分析：代码层面调试过程、解决方案寻找过程、不同LLM行为差异

Result: 通过Kaggle竞赛案例研究，证明了系统能够提供有价值的迭代编码过程洞察，支持更有效的调试和提示工程

Conclusion: 该可视化分析系统能够帮助ML科学家结构化地理解编码代理行为，提升调试效率和提示工程质量

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [136] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: 结合滑动窗口和变分模态分解(VMD)的金融时间序列预测模型，通过VMD分解非平稳序列为平滑子分量，再用LSTM进行预测，相比原始序列方法表现更好更稳定。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列具有高度复杂性和非平稳性，传统方法难以有效处理，需要开发更适应性的预测模型来提高预测精度和稳定性。

Method: 使用滑动窗口构建数据集，采用VMD方法分解非平稳金融时间序列为平滑子分量，然后将分解后的数据输入深度学习模型(LSTM)进行预测。

Result: 与使用原始时间序列的LSTM模型相比，基于VMD处理序列的模型表现出更好的预测效果和更高的稳定性。

Conclusion: VMD分解方法能有效处理金融时间序列的非平稳特性，结合深度学习模型可以显著提升预测性能，为金融时间序列分析提供了有效解决方案。

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [137] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: 通过级联学习和距离括号形式主义，开发了能够自动保持热力学定律、动量守恒和温度波动散平衡的多标度系统粗糕化媒介动力学模型学习框架


<details>
  <summary>Details</summary>
Motivation: 多标度系统模拟面临短时空标度与出现性质量联系的挑战，粗糕化过程中的信息损失导致了散射性、历史依赖性和随机性的出现物理

Method: 采用距离括号形式主义构建框架，通过自监督学习策略识别出现结构变量，实现了在PyTorch和LAMMPS中的开源实现

Result: 在星形聚合物和胚洞悬派液等具有挑战性的系统中验证了方法的有效性，能够在高级粗糕化水平下保持非平衡态统计特性

Conclusion: 该框架为学习多标度系统的粗糕化动力学提供了一种结构化方法，能够自动保持重要物理定律，并通过开源实现支持大规模推断和扩展性

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [138] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: 使用预训练的蛋白质大语言模型提取序列上下文特征，通过双向LSTM和GRU模型预测蛋白质中的胱粉样变性区域，取得了84.5%的跨验证准确率


<details>
  <summary>Details</summary>
Motivation: 胱粉样变性预测是生物信息学重点领域，当前方法主要依靠进化模式和氨基酸特性，序列信息特征显示出高预测性能，需要探索更充分的上下文特征利用

Method: 利用预训练的蛋白质大语言模型提取序列上下文特征，构建双向LSTM和GRU深度学习模型来预测胱粉样变性区域

Result: 在10折跨验证中获得84.5%的准确率，在测试数据集上获得83%的准确率，表现出竞争力

Conclusion: 蛋白质大语言模型在提高胱粉样变性预测准确性方面具有很大潜力，为该领域提供了新的研究方向

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [139] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: 联邦学习中过参数化FedAvg的收敛性分析，证明随着神经网络宽度增加，数据异质性的影响会减弱，在无限宽度下FedAvg能达到与集中式学习相同的泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中由于客户端数据非独立同分布导致的模型泛化挑战，分析FedAvg在过参数化神经网络中的收敛特性

Method: 理论分析过参数化FedAvg与梯度下降的收敛性，证明网络宽度增加对数据异质性影响的减弱效应，并在无限宽度下建立线性模型行为的理论框架

Result: 数据异质性的影响随网络宽度增加而减小，无限宽度时完全消失；FedAvg在无限宽度下表现出线性模型行为，达到与集中式学习相同的泛化性能

Conclusion: 过参数化是解决联邦学习数据异质性问题的有效途径，无限宽度神经网络能够消除异质性影响，使FedAvg达到最优泛化性能

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [140] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: 通过令牌级判断机制，给合成语言模型带来能消约和通信效率显著提升


<details>
  <summary>Details</summary>
Motivation: 解决质空间设备上大语言模型推理的能消耗和通信成本问题，对函数混合模型的现有研究过于关注准确性和延迟而忽视了能效

Method: 提出一种基于认知不确定性和关注重要度的令牌级筛选机制，只上传信息量丰富的令牌，减少大模型使用和通信开销

Result: 在TinyLlama-1.1B和LLaMA-2-7B上进行实验，方法达到了87.5%的BERT Score，令牌通过率0.37个/秒，能消约40.7%，相比之前的U-HLM基准方法在准确性、能效和速度方面都有显著提升

Conclusion: 该方法能够在带宽受限的边缘环境中实现能效高、准确性好的大语言模型部署

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [141] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息深度算子网络（PI-DeepONet）的交通状态估计框架，通过将交通流守恒定律直接整合到算子学习过程中，相比传统PINNs方法在稀疏输入数据下能更有效地重建完整时空交通状态场。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）在交通状态估计中逐点强制执行PDE约束，但存在局限性。本文旨在通过算子学习框架更好地处理高维时空偏微分方程，从有限噪声测量中准确估计交通状态。

Method: 采用物理信息深度算子网络（PI-DeepONet）框架，训练参数化神经算子将稀疏输入数据映射到完整时空交通状态场，直接将交通流守恒模型和基本图整合到算子学习过程中。

Result: 在NGSIM数据集上的实验表明，该方法优于最先进的基线方法，能够有效捕捉拥堵传播、空间相关性和时间演化，同时保持物理一致性。

Conclusion: PI-DeepONet框架为交通状态估计提供了更有效的解决方案，通过算子学习方法实现了更好的物理一致性和估计精度，对输入函数生成策略和分支网络复杂度的分析为实际应用提供了重要指导。

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [142] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE是一种线性复杂度的自注意力机制，通过固定长度的潜在序列路由注意力，解决了传统自注意力二次复杂度的问题，在大型非结构化网格上实现了更好的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制的二次复杂度限制了其在大型非结构化网格上的应用和可扩展性，需要一种更高效的注意力机制来处理大规模问题。

Method: FLARE通过可学习的查询令牌将输入序列投影到固定长度的潜在序列（M << N），在瓶颈序列中进行注意力路由，学习低秩形式的注意力，实现O(NM)的计算复杂度。

Result: FLARE不仅能够扩展到前所未有的问题规模，而且在各种基准测试中相比最先进的神经PDE代理模型提供了更优越的准确性。

Conclusion: FLARE通过低秩注意力路由机制成功解决了自注意力的可扩展性问题，为大规模非结构化网格处理提供了有效的解决方案，并发布了新的增材制造数据集以促进进一步研究。

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [143] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: 提出了一种系统性的方法来构建不变和等变操作，能够处理不同秩的笛卡尔张量和不同类型的球面张量，并利用对称张量网络的图形表示简化证明和构造。


<details>
  <summary>Details</summary>
Motivation: 设计包含对称性的神经网络对于几何深度学习至关重要，核心是开发不变和等变操作。

Method: 使用对称张量网络的图形表示来系统构建不变和等变操作，处理不同秩的笛卡尔张量和不同类型球面张量。

Result: 方法成功应用于设计几何图神经网络的等变交互消息和等变机器学习模型来学习材料的本构定律。

Conclusion: 该方法为构建对称性保持的神经网络操作提供了系统且简化的解决方案，在几何深度学习中具有重要应用价值。

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [144] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: 一种混合代次模型，通过组合粉度神经网络运算符和可微物理模块，从速度和加速度估计电动汽车参数和电消耗，达到了1% 的平均锐锐误差。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地估计电动汽车的参数和电消耗，提供可解释性和通用性强的模型，以支持路径优化、生态路由等应用。

Method: 组合了新的粉度参数运算符（Fourier神经网络基础）和可微物理模块，从速度和加速度输入估计时变参数，包括电机效率、制动效率、空气动力阻、滚动阻力等，通过物理嵌入方式计算电池功率。

Result: 在Tesla Model 3、Model S和Kia EV9实际数据上，平均绝对误差分别为0.2kW（约高速时拉力功率的1%）和0.8kW，模型具有良好的可解释性和通用性。

Conclusion: 该混合代次模型能够提供准确、可解释的电动汽车参数估计，适用于各种实际应用场景，且在未见条件和采样率下都有良好的性能。

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [145] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: SSPO是一种无需辅助模型或人工标注的强化学习过程监督框架，通过模型自身生成的步骤偏好信号来优化推理过程，实现推理压缩，有效缓解过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法（如带思维链的强化学习）存在计算开销大和过度思考问题，错误答案部分源于冗长推理过程缺乏自我修正能力，导致错误在多步骤中累积。

Method: 提出Self-traced Step-wise Preference Optimization (SSPO)，利用模型自身生成的步骤级偏好信号指导优化过程，实现细粒度的推理步骤优化，无需辅助模型或步骤级人工标注。

Result: 实验表明SSPO生成的推理序列既准确又简洁，有效缓解了过度思考行为，在不同领域和语言中均不损害模型性能。

Conclusion: SSPO作为一种可插拔的强化学习过程监督框架，通过自生成的步骤偏好信号实现了高效的推理优化，解决了现有方法计算开销大和过度思考的问题。

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [146] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: 这篇论文提出了可解释人工智能(XAI)方法的两个可靠性判据：解释稳健性(ER)和解释方法稳健性(EMR)，并为深度学习算法的可信赖解释提供了框架。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法预测准确但内部运作不透明，当前XAI方法在可靠性方面存在疑惑，需要更好的判据来确保解释的真实性。

Method: 提出并形式化了两个核心概念：解释稳健性(ER)指不同XAI方法在相似情境下产生相同解释；解释方法稳健性(EMR)要求单个XAI方法自身必须符合可靠性标准。

Result: 构建了一个用于评估和建立深度学习算法可信赖解释的框架，并提供了相关的形式化标准。

Conclusion: 单纯的解释稳健性不足以保证XAI方法的可靠性，必须同时考虑解释方法稳健性。论文为评估和改进XAI方法提供了理论基础和实践指南。

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [147] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3是一个开源的多模态流匹配模型，通过三种架构无关的技术（自条件、假原子和训练时几何扭曲）显著提升了全原子小分子生成的性能，实现了近100%的分子有效性，并以更少的参数更准确地复现训练数据的官能团组成和几何结构。


<details>
  <summary>Details</summary>
Motivation: 开发能够生成具有所需性质的逼真分子的生成模型，可以加速化学发现。现有模型在联合采样分子拓扑和3D结构方面仍有改进空间。

Method: 基于流匹配框架，采用三种架构无关技术：自条件（self-conditioning）、假原子（fake atoms）和训练时几何扭曲（train-time geometry distortion），这些技术计算成本可忽略不计。

Result: 实现了近100%的药物样分子有效性（包含显式氢原子），更准确地复现训练数据的官能团组成和几何结构，参数量比同类方法少一个数量级。

Conclusion: 这些简单可迁移的技术可以缓解基于传输的生成模型的普遍病理问题，在推理过程中检测和纠正分布漂移，为提高基于扩散和流的分子生成模型的稳定性和质量提供了有效策略。

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [148] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: SciNO是一种基于分数匹配的因果发现方法，通过神经算子稳定估计Hessian对角项，解决了现有方法计算开销大和数值不稳定的问题，在保持内存效率的同时显著提升了排序准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于排序的因果发现方法在加性噪声模型假设下需要准确估计对数密度的Hessian对角项，但传统方法使用Stein梯度估计器计算昂贵且内存密集，而DiffAN方法虽然解决了计算问题但仍存在数值不稳定性。

Method: 提出Score-informed Neural Operator (SciNO)，在平滑函数空间中设计概率生成模型来稳定近似Hessian对角项，并在分数建模过程中保持结构信息。同时提出概率控制算法，将SciNO的概率估计与自回归模型先验结合。

Result: 实验结果显示，SciNO在合成图上平均减少42.7%的排序分歧，在真实数据集上减少31.5%的排序分歧，同时保持内存效率和可扩展性。该方法无需额外微调或提示工程即可增强LLMs的因果推理能力。

Conclusion: SciNO方法有效解决了现有因果排序方法的计算和稳定性问题，通过神经算子和概率控制算法的结合，实现了高效可靠的因果发现，为大规模因果推理提供了实用解决方案。

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [149] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: 这篇论文提出了一种在对抗希拉攻击的联邦学习场景下的稳健算法，仅需服务器和一个客户端属于善意参与者即可有效运作，无需预知恶意客户端数量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端可能受到希拉攻击，而服务器可靠且拥有可靠的边缘数据集，需要一种稳健的防御机制。

Method: 利用服务器的可靠边缘数据集来验证和筛选客户端提交的模型更新，仅需服务器和一个客户端属于善意参与者。

Result: 理论分析证明在强希拉攻击下仍有界的最优性间隔，实验结果显示在MNIST、FMNIST和CIFAR-10数据集上显著超过标准和稳健FL基线方法。

Conclusion: 该方法能够有效防御希拉攻击，在各种攻击策略下都表现優异，为联邦学习的安全性提供了可靠保障。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [150] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero是一个新颖的联邦学习方法，通过超网络动态生成针对非参与客户端的专用模型，有效解决数据异构性和资源约束问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理数据异构性方面取得进展，但无法泛化到具有域内分布偏移和资源约束的非参与客户端，需要新的解决方案。

Method: 使用基于分布感知嵌入的超网络动态生成专用模型，通过NoisyEmbed增强提取器和平衡惩罚提取鲁棒分布嵌入，防止特征崩溃，分块生成适应非参与客户端数据分布的模型。

Result: 在多个数据集和模型上的广泛实验显示，HyperFedZero性能显著优于竞争方法，计算、存储和通信开销最小，消融研究和可视化验证了各组件的必要性。

Conclusion: HyperFedZero通过分布感知嵌入和超网络机制，有效解决了联邦学习中非参与客户端的泛化问题，具有优异的性能和实用性。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [151] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: BuilDa是一个热建筑数据生成框架，无需深厚建筑模拟知识即可生成大量高质量合成数据，用于迁移学习研究。


<details>
  <summary>Details</summary>
Motivation: 迁移学习可以改进建筑热动力学建模，但现有公开数据集和数据生成器无法满足迁移学习研究对数据质量和数量的需求，且通常需要建筑模拟专业知识。

Method: 使用单区域Modelica模型导出为功能模拟单元(FMU)，在Python中进行模拟，构建热建筑数据生成框架。

Result: 成功生成数据并用于预训练和微调迁移学习模型，证明了框架的有效性。

Conclusion: BuilDa框架能够为迁移学习研究提供足够质量和数量的合成数据，且降低了使用门槛，无需深厚建筑模拟专业知识。

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [152] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 这篇论文提出了一种专门用于汽车网络中交通标志检测的联邦学习框架，通过分布式模型训练解决数据隐私和通信挟所问题，实验结果显示该方法能够在保护隐私的同时达到过能几十的准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车每天产生巨量传感器数据，传统的集中式机器学习方法在感知任务中面临着严重的隐私和通信挟所挑战。

Method: 设计了一种分布式联邦学习框架，将交通标志类别在车辆之间进行分片化，使用轻量级对象检测器进行专门化本地训练，通过FedProx、FedAdam和FedAVG等算法在Flower框架中模拟环境下聚合模型参数。

Result: 实验结果显示：将服务器轮次从2增加到20次时，准确率从不足0.1提升到超过0.8；适中的本地迭代次数（8-10次）能够以准确率约0.67实现最优效率；更高的客户参与比例将泛化能力提升到0.83；FedProx在处理异质性时表现更优；非IID数据分布会降低性能；训练时长主要与轮次数相关。

Conclusion: 这种联邦学习方法为真实世界汽车部署提供了一种可扩展、隐私保护的解决方案，有望指导未来精强聚合算法和通信优化的集成，以推进智能交通系统的发展。

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [153] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: FedSODA是一个资源高效的联邦微调框架，通过层剪枝和蒸馏对齐技术，在保持模型性能的同时显著降低通信、存储和计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 联邦微调在资源受限客户端上存在计算和内存需求高的问题，限制了其发展潜力，需要一种不需要访问或存储完整模型的解决方案。

Method: 提出相似性组剪枝(SGP)模块剪枝冗余层，保留关键层；引入协调蒸馏对齐(ODA)模块减少子模型与完整模型间的梯度差异；使用QLoRA技术让客户端只需部署量化子模型和微调轻量适配器。

Result: 实验表明FedSODA平均减少70.6%通信开销，降低75.6%存储使用，提高3.1%任务准确率。

Conclusion: FedSODA非常适合资源受限环境下的实际联邦微调应用，在保持性能的同时显著提升效率。

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [154] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: FedUNet是一个轻量级、架构无关的联邦学习框架，通过在每个客户端骨干网络上附加U-Net风格的加性模块，仅共享紧凑的瓶颈层来实现高效知识传输，无需结构对齐。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法大多假设客户端模型架构相同，限制了在异构现实环境中的应用。需要解决不同架构客户端间的协作学习问题。

Method: 在每个客户端骨干网络上附加U-Net启发的加性模块，仅共享U-Net的紧凑瓶颈层。利用编码器-解码器设计和跳跃连接捕获低层和高层特征，提取客户端不变表示。

Result: 在VGG变体上的实验显示，FedUNet达到93.11%准确率，轻量版达到92.68%准确率，仅需0.89MB的低通信开销。

Conclusion: FedUNet通过U-Net风格的加性模块设计，实现了异构客户端架构下的高效联邦学习，在保持高性能的同时显著降低了通信成本。

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [155] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: 这篇论文提出了一个系统性评估神经网络空间推理能力的基准框架，采用VoxLogicA生成迷宫连通性和空间距离计算任务的合成数据集，发现神经网络在基本几何和拓扑理解任务中存在系统性失败


<details>
  <summary>Details</summary>
Motivation: 系统性评估神经网络的空间推理能力，特别是形态学性质如连通性和距离关系，以识别其局限性并为改进提供基础

Method: 使用VoxLogicA空间模型检查器生成两类合成数据集：迷宫连通性问题（拓扑分析）和空间距离计算任务（几何理解），在多个分辨率下评估nnU-Net，包含数据生成、标准化训练、推理执行和综合评估

Result: 初步实验结果显示神经网络在空间推理能力方面遇到重大挑战，在基本几何和拓扑理解任务中出现系统性失败

Conclusion: 该框架提供了可复现的实验协议，为识别神经网络空间推理的具体局限性提供基础，这些局限性可通过神经网络与符号推理方法的混合方案来解决，以改善临床应用中的空间理解能力

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [156] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: 提出了约束质心聚类(CCC)方法，通过限制簇中心到最远点的最大距离来扩展传统质心聚类，使用拉格朗日公式推导出闭式解，在合成环形数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统质心聚类方法缺乏对簇分布的显式控制，需要一种能够约束簇扩散同时保持可解释性的聚类方法。

Method: 基于拉格朗日公式推导闭式解，在聚类过程中强制约束簇中心到簇内最远点的最大距离，控制簇的径向扩散。

Result: 在合成环形数据上，CCC通过减少径向扩散同时保持角度结构，实现了更紧凑的聚类，在环向熵、扇区熵和联合熵指标上优于K-means和GMM。

Conclusion: CCC方法适用于需要结构化聚类和扩散控制的应用场景，如传感器网络、协作机器人和可解释模式分析。

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [157] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: 使用极限学习机(ELM)的多输入多输出(MIMO)方法进行短期能源预测，在科西卡岛数据上显著超过持续性预测，特别是太阳能和热能预测精度高，计算效率高适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决能源系统的非稳态性和季节性变化带来的预测挑战，需要一种能够动态适应波动且计算效率高的方法来预测多种能源的短期产出。

Method: 采用极限学习机(ELM)结合多输入多输出(MIMO)架构，包含滑动窗口技术和周期时间编码来处理非稳态性和季节性变化。使用科西卡岛6年小时数据，包括太阳能、风能、水能、热能、生物能源和进口电力。

Result: 模型显著超过持续性预测，特别是太阳能(nRMSE 17.9%)和热能(nRMSE 5.1%)预测精度高，R² > 0.98(1小时预见)。在5小时内保持高精度，超过后可再生能源变得不稳定。MIMO比SISO架构有边际收益，且比LSTM等深度学习方法计算效率更高。

Conclusion: 该ELM-MIMO方法提供了闭式解决方案，计算需求低，适合实时应用和在线学习。方法具有良好的适配性，可根据资源可用性、网格特性和市场结构等本地约束进行调整，适用于各种能源预测场景。

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [158] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: 基于多子网络协同预测的在线集成模型E3Former，提升服务器无法系统自动扩缩的准确性和稳定性，在实际部署中平均降低10%预测误差，节省40%资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型在处理在线工作负载流动态变化时适应慢，无法有效抓取细粒度高频预测任务中的复杂周期性特征，影响服务器无法系统自动扩缩的效果。

Method: 提出E3Former在线集成模型，协同多个子网络的预测能力充分利用不同模型的优势，在计算开销增加最小的情况下实现更高的准确性和稳健性。

Result: 在实际工作负载数据集上，该方法在在线预测任务中平均降低10%预测误差，并已成功部署于字节跳动智能水平Pod自动扩缩平台，支持超过30个应用稳定运行，预测自动扩缩能力达到60万核CPU核心。

Conclusion: E3Former模型通过集成学习方法有效解决了服务器无法系统中工作负载预测的挑战，在基本保证服务质量的前提下实现了显著的资源节省效果，为大规模云计算环境提供了可靠的自动扩缩解决方案。

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [159] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: 基于随机化PCA森的新题无监督异常检测方法，在多个数据集上表现超过传统和最新方法，具有高泛化能力和计算效率


<details>
  <summary>Details</summary>
Motivation: 受随机化PCA森在近似KNN搜索中表现的启发，开发一种新的无监督异常检测方法

Method: 利用随机化PCA森（RPCA Forest）进行异常检测，基于随机化PCA的结构来识别数据中的异常值

Result: 在多个数据集上表现超过传统和最新方法，其余数据集上也具有竞争力，方法具有高泛化能力和计算效率

Conclusion: 该方法是无监督异常检测的良好选择，结合了效果性和效率两个优势

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [160] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Wavy Transformer通过引入二阶波动动力学来解决Transformer中的过平滑问题，在NLP和CV任务中提升性能且无需额外超参数调优


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型存在过平滑问题，即token表示在连续transformer块中收敛到相似值。本文从图神经扩散角度建立等价关系，将过平滑解释为底层扩散动力学的耗散性质

Method: 提出Wavy Transformer，包含基于二阶波动动力学的新型注意力层，以及设计前馈网络和归一化层来保持物理状态-速度关系

Result: 在多种NLP和CV任务的transformer模型上验证，Wavy Transformer以最小额外参数显著提升性能，无需额外超参数调优

Conclusion: 从物理动力学角度成功解决了Transformer的过平滑问题，提出的Wavy Transformer架构有效且高效

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [161] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge是一个统一的统计框架，通过建模人类偏好与LLM评估之间的系统性偏差，提高LLM作为评判者的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛用作评判者来评估模型输出，但其评估结果往往与人类判断存在系统性偏差，需要一种方法来弥合这种差距。

Method: 提出了Bridge统计框架，假设每个提示-响应对存在潜在的人类偏好分数，并将LLM偏差建模为捕捉差异来源的协变量的线性变换。提供了具有渐近保证的有效拟合算法。

Result: 在6个LLM评判者和两个基准测试（BigGen Bench和Chatbot Arena）上，Bridge在准确性、校准和KL散度方面与人类评分达成更高的一致性，并揭示了系统性的人机差距。

Conclusion: Bridge提供了一个简单而有原则的框架，用于改进LLM评分并表征人类与LLM之间的系统性差异，为LLM作为评判者的应用提供了统计基础。

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [162] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文重新审视因果建模在AI泛化中的作用，挑战了当前领域泛化基准测试中的观点，提出了更细致的因果理论框架。


<details>
  <summary>Details</summary>
Motivation: 针对近期领域泛化基准测试对因果建模能够实现稳健AI泛化的承诺提出的挑战，作者希望调和文献中的明显矛盾。

Method: 通过理论分析和文献综述，重新审视因果建模与领域泛化的关系，并提供了交互式演示来支持论点。

Result: 提出了一个更细致的因果理论框架，解释了因果建模在AI泛化中的实际作用，调和了现有文献中的矛盾观点。

Conclusion: 因果建模在AI泛化中确实具有重要作用，但需要更细致的理论框架来理解其实际效果和局限性。

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [163] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: 提出MaxScore路由方法，通过最小成本最大流问题建模和SoftTopk算子，解决传统MoE网络中专家容量约束导致的令牌丢弃和硬件效率低下问题


<details>
  <summary>Details</summary>
Motivation: 传统MoE网络中的专家容量约束会导致容量饱和时的令牌丢弃和未充分利用专家时的填充效率低下，而去除约束又会损害负载平衡和计算效率

Method: 将路由建模为最小成本最大流问题，集成SoftTopk算子，避免迭代重路由和最优传输公式的根本限制

Result: 在相同FLOPs下实现了比约束和无约束基线更低的训练损失和更高的评估分数

Conclusion: MaxScore是一种新颖的MoE路由范式，有效解决了传统路由方法的根本局限性

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [164] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: 这篇论文提出了L2S方法，通过训练辅助模块预测输入特定的导向向量，实现多模态大语言模型的精细化导向控制，提高安全性和减少幻觉


<details>
  <summary>Details</summary>
Motivation: 现有的导向技术依赖单一静态向量，无法处理不同输入查询的特定需求，比如安全回答可能需要根据具体问题采取不同的应对策略

Method: 使用对比输入特定提示计算线性偏移，然后训练一个小型辅助模块来预测这些输入特定的导向向量，解决测试时提示未知的问题

Result: L2S方法能够有效减少多模态大语言模型的幻觉现象，并提高安全性能力，表现超过了其他静态基线方法

Conclusion: 通过学习预测输入特定导向向量的方法，可以实现更精细化的多模态大语言模型导向控制，为实际应用提供更灵活的行为编程方式

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [165] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: 这篇论文通过实验研究探讨了设备上机器学习中的存储问题，发现数据压缩策略对学习效果的影响，并提出根据样本效慢性进行适配性压缩的新方法。


<details>
  <summary>Details</summary>
Motivation: 设备上机器学习面临存储空间有限的挑战，特别是在连续数据收集场景中。需要找到数据数量与质量之间的最佳平衡点。

Method: 进行存储感知学习的实证研究，重点分析数据压缩的权衡。对比分析简单的均匀数据删除和一强制压缩策略的效果。

Result: 证明了简单的均匀数据处理策略是次优的。发现不同数据样本对压缩的敏感度存在差异，支持了根据样本进行适配性压缩的可行性。

Conclusion: 这些发现为开发新一代存储感知学习系统奠定了基础，通过系统化的特征化提供了对该领域的深入理解。

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [166] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 这篇论文研究了变换器模型在上下文下一个词预测任务中的损失地形，发现子n-gram模型是人口损失的近稳定点，为阶段性学习动力学提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 基于对训练过程中长时间平台和阶段性进展的经验观察，研究变换器模型在上下文下一个词预测任务中的损失地形特性。

Method: 采用交叉熵损失学习上下文n-gram语言模型，建立了参数配置为稳定点的充分条件，构造了简化变换器模型的k-gram估计器参数集合。

Result: 在无限序列长度和参数范数极限下，k-gram解的人口损失梯度消失，证明子n-gram模型是人口交叉熵损失的近稳定点。

Conclusion: 这些发现为广泛观察到的阶段性学习动力学和出现相变过渡现象提供了理论解释，数值实验进一步支持了这些见解。

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [167] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: HRS是一个混合表示框架，通过整合数值和图像表示来捕捉极端负载动态，并引入调度感知损失函数来减少SLA违规率63.1%和总利润损失32.3%。


<details>
  <summary>Details</summary>
Motivation: 流媒体服务快速发展导致网络负载呈现高度时变和突发性，现有方法要么导致峰值时段SLA违规，要么采用保守的过度配置策略增加资源支出。

Method: 提出HRS混合表示框架，整合数值和图像表示来捕捉极端负载动态；引入调度感知损失函数(SAL)来捕获预测误差的不对称影响。

Result: 在四个真实数据集上的实验表明，HRS持续优于十个基线方法，达到最先进性能，SLA违规率降低63.1%，总利润损失减少32.3%。

Conclusion: HRS框架通过混合表示和调度感知损失函数，有效解决了流媒体服务负载预测中的SLA违规和资源浪费问题，显著提升了服务质量和经济收益。

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [168] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出TGN-SVDD方法，结合动态图建模和深度异常检测，用于网络入侵检测，在真实数据上优于多个基线方法


<details>
  <summary>Details</summary>
Motivation: 随着全球数字化发展，网络安全日益重要。机器学习入侵检测面临检测新型未知网络事件、处理时序数据和网络通信图结构等挑战

Method: TGN-SVDD方法，基于现代动态图建模和深度异常检测技术

Result: 在真实入侵检测数据上证明了该方法相对于多个基线方法的优越性，并提出了更具挑战性的数据变体

Conclusion: 该方法为网络入侵检测提供了有效的解决方案，能够处理动态图结构和异常检测的复杂需求

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [169] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ是一种面向TinyML流式应用的单次通过、无标签不确定性监测器，通过轻量级信号将短时域时间一致性转换为校准风险分数，具有O(W)环形缓冲区和O(1)每步更新。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的微控制器设备提供高效的不确定性监测方案，解决传统方法如早期退出和深度集成在内存占用和延迟方面的不足。

Method: 使用流式保形校准层将时间一致性分数转换为预算化的接受/弃权规则，无需在线标签或额外前向传播。通过轻量级信号捕获后验和特征的时间一致性。

Result: 在微控制器上，TCUQ比早期退出和深度集成减少约50-60%的内存占用和30-45%的延迟提升。在分布内流数据损坏情况下，AUPRC提升3-7点（最高0.86），故障检测AUROC最高达0.92。

Conclusion: 时间一致性结合流式保形校准为TinyML设备监测提供了实用且资源高效的基础方案。

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [170] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: 基于进化策略的SparseMap框架，统一优化映射策略和稀疏策略，解决稀疏张量加速器设计空间组合爆炸问题


<details>
  <summary>Details</summary>
Motivation: 现有手动设计的稀疏张量加速器局限于特定场景，且调整大量设计因素耗时谨难。之前的工作只关注映射或稀疏策略之一，导致次优设计

Method: 提出SparseMap框架，构建包含映射策略和稀疏策略的全面设计空间。引入一系列二进制编码和进化运算符的改进，支持高效探索巨大的设计空间

Result: SparseMap在与之前工作和经典优化方法的定量比较中，一贯找到更优的解决方案

Conclusion: 进化策略基础的SparseMap框架能够高效处理稀疏张量加速器设计空间的组合爆炸问题，为自动化设计提供了有效解决方案

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [171] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ是一种用于TinyML的单次前向传播、无标签不确定性量化方法，通过深度激活预测来估计风险，无需额外计算开销，显著减少内存占用和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性量化方法在TinyML设备上计算开销大、内存占用高，需要开发一种轻量级、单次前向传播就能完成不确定性估计的方法。

Method: 使用int8量化的小型预测头来预测下一层的统计特征，通过轻量级单调映射器将预测的惊奇值转换为可操作的分数，无需时间缓冲、辅助出口或重复前向传播。

Result: 相比早期退出和深度集成方法，SNAP-UQ在视觉和音频骨干网络上减少约40-60%的闪存占用和25-35%的延迟，在损坏数据流中提高了准确性下降检测能力，AUROC达到约0.9。

Conclusion: 基于层间动态的不确定性量化提供了一个实用且资源高效的TinyML设备监控基础，实现了单次前向传播的强失败检测能力。

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [172] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: 提出了SL-ACC框架，通过自适应通道重要性识别和通道分组压缩来减少分割学习中的通信开销，在保持精度的同时显著缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 随着参与设备数量的增加，分割学习中传输的激活数据和梯度数据成为主要瓶颈，导致模型训练速度变慢。

Method: 提出SL-ACC框架，包含两个核心组件：1）自适应通道重要性识别（ACII）- 使用香农熵识别每个通道对模型训练的贡献；2）通道分组压缩（CGC）- 基于熵值对通道进行分组，并进行组级自适应压缩以减少传输量。

Result: 在多个数据集上的广泛实验验证，SL-ACC框架比现有最先进基准方法花费更少时间达到目标精度。

Conclusion: SL-ACC框架有效解决了分割学习中的通信瓶颈问题，在保持训练精度的同时显著提升了训练效率。

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [173] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: 图卷积网络(GCN)性能与图的代数连通性(Fiedler值)密切相关，Fiedler值相似的图具有类似的结构特性，可以作为GCN性能的有效预测指标。


<details>
  <summary>Details</summary>
Motivation: GCN文献中观察到堆叠GCN层可能不会带来更好的性能，需要找到能够预测GCN性能的图结构指标。

Method: 通过理论和实验分析，在合成图和真实图数据(Cora、CiteSeer、Polblogs)上探索Fiedler值与GCN性能的关系，研究多种聚合连通分量Fiedler值的方法。

Result: 实证发现图的代数连通性(Fiedler值)是GCN性能的良好预测因子，相似Fiedler值的图可以使用相同的滤波器和超参数，迁移学习在Fiedler值相似的图之间更有效。

Conclusion: Fiedler值作为图的结构特性指标，能够有效预测GCN在图任务上的性能表现，为GCN的超参数选择和迁移学习提供了理论依据。

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [174] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta是一种改进的Adam优化器，通过动态调整beta2参数来处理梯度尖峰问题，在物理驱动的PDE代理模型和PINNs中表现优于标准Adam


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在物理问题中训练时由于边界条件变化导致的梯度尖峰和不稳定损失问题，特别是在数据驱动的PDE代理模型和物理信息神经网络中

Method: 将Adam中的固定beta2参数替换为基于梯度范数比率的动态值，使用"sunspike"比率（当前梯度范数除以历史EMA）来控制beta2在[0,1)区间内动态调整

Result: 在四个测试场景中均表现出更好的稳定性和最终损失，在small-enwik8上比Adam-0.95降低38%的bits-per-character，比Adam-0.999降低58%

Conclusion: Kourkoutas-Beta保持了Adam的收敛保证，同时显著提高了在梯度尖峰情况下的鲁棒性，计算开销与Adam相当

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [175] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出FAML方法解决多视图证据学习中的证据偏见问题，通过自适应先验、公平性约束和意见对齐机制，实现更平衡的证据分配和更可靠的不确定性估计


<details>
  <summary>Details</summary>
Motivation: 现有多视图证据学习方法假设视图特定证据学习是可靠的，但实践中证据学习过程存在偏见，样本倾向于为数据丰富的类别分配更多证据，导致不确定性估计不可靠

Method: FAML方法包含：1）基于训练轨迹的自适应先验作为正则化策略；2）基于类间证据方差的公平性约束；3）多视图融合阶段的意见对齐机制

Result: 在五个真实世界多视图数据集上的实验表明，FAML实现了更平衡的证据分配，在预测性能和不确定性估计可靠性方面优于现有方法

Conclusion: FAML有效解决了多视图证据学习中的偏见问题，提高了预测性能和不确定性估计的可靠性

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [176] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: MCFRCL是一种新的持续学习功能正则化框架，通过蒙特卡洛采样近似模型预测分布，使用三种连续分布和基于矩的方法捕捉统计特征，结合Wasserstein和KL距离构建正则化函数，在MNIST和CIFAR数据集上表现出优越的准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的功能正则化持续学习方法虽然优于权重空间正则化方法，但存在计算成本高和线性近似误差大的问题，需要开发更高效准确的方法。

Method: 提出MCFRCL框架，使用蒙特卡洛采样近似模型预测分布，利用三种连续分布通过基于矩的方法捕捉MC样本的统计特征，同时采用Wasserstein距离和KL距离构建正则化函数。

Result: 在MNIST和CIFAR数据集上的实验结果表明，MCFRCL在预测准确性和训练效率方面均优于多个基准方法。

Conclusion: MCFRCL框架通过蒙特卡洛采样和多种距离度量的结合，有效解决了功能正则化持续学习方法的高计算成本和近似误差问题，为持续学习提供了更高效的解决方案。

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [177] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: 基于过滤-x双曲正切指数广义核M估计函数(FXHEKM)的稳健自适应算法，用于冲击性噪声环境下的主动噪声控制应用


<details>
  <summary>Details</summary>
Motivation: 解决在存在冲击性噪声(如α稳定噪声)环境下，传统主动噪声控制算法性能下降的问题

Method: 提出FXHEKM稳健自适应算法，进行统计分析和计算成本研究，使用MSE和ANR指标评估性能

Result: 数值结果显示FXHEKM算法能够高效取消添加的偏差信号，在α稳定噪声环境下表现优于其他竞争算法

Conclusion: FXHEKM算法提供了一种有效的稳健自适应过滤方法，适用于存在冲击性噪声的主动噪声控制应用

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [178] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 本文研究使用BERT和层次注意网络对网络攻击描述进行多标签分类，预测攻击后果，BERT模型达到97.2%的准确率，显著超越传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 网络攻击越来越复杂，需要自动化方法来分析攻击描述并预测其潜在影响，以支持安全专业人员及时采取行动和分配资源。

Method: 利用自然语言处理(NLP)和深度学习技术，基于MITRE CWE数据库的文本描述进行分析。采用BERT模型结合层次注意网络(HANs)进行多标签分类，将攻击后果分为五个主要类别：可用性、访问控制、保密性、完整性和其他。

Result: 实验结果显示BERT模型在多标签分类任务中达到了0.972的总体准确率，显著超过传统的CNN和LSTM模型。HAN在某些特定安全标签上表现更好，但BERT在精度和召回率方面一致更优。

Conclusion: BERT模型在预测网络攻击后果方面表现最优，适合用于安全风险评估和网络安全威胁分析。

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [179] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 利用分开的内部数据和公开数据来估计AI模型的公平性指标，解决完整数据难以获取的实际问题


<details>
  <summary>Details</summary>
Motivation: 在金融、招聘、医疗等高风险领域，AI系统公平性至关重要，但由于法律和隐私问题，很难获得包含人口统计属性的完整数据进行公平性测试

Method: 利用分开的数据源（机构内部数据和公开人口数据）估计可行的联合分布，然后计算可能的公平性指标范围

Result: 通过模拟和实验验证，该方法能够得到有意义的公平性指标上下界，并获得真实指标的可靠估计

Conclusion: 这种方法为实际场景中的公平性测试提供了一个实用而有效的解决方案，特别是在完整数据访问受限的情况下

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [180] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 这篇论文比较了FMAE和HEF两种评估函数在多元时间序列预测中的性能，HEF在全局指标上更优，适合战略规划，而FMAE在局部指标和执行效率上更好，适合短期场景。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列建模面临数据复杂性、不确定性和频繁制度转换的挑战，传统评估指标容易导致偏差和限制模型的普适性。

Method: 设计了FMAE（专注最小化绝对误差）和HEF（重视全局指标并惩罚大偏差）两种评估函数，在不同数据分割比例（91:9、80:20、70:30）下使用三种优化器（网格搜索、PSO、Optuna）进行实验，评估拟合度、相对准确性、稳健性和计算效率。

Result: HEF在全局指标（R2、相对准确性、RMSE、RMSSE）上一贵表现更优，提升了模型稳健性和解释力；FMAE在局部指标（MAE、MASE）和执行时间上更优，适合短期场景。

Conclusion: 研究揭示了方法论上的权衡：HEF适合战略规划需求，FMAE适合运营效率需求，并提出了一个可复现的框架来优化动态环境中的预测模型。

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [181] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: 该论文提出了一种基于神经代理模型的参数分布可视化方法，用于解决科学模拟中的逆问题，通过密度估计和交互式界面来展示产生特定输出特征的可能输入参数分布。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理模型的解决方案主要关注寻找少量匹配参数，而忽略了可能参数的整体分布情况。本文旨在建模和可视化能够产生给定输出特征的可能输入参数分布。

Method: 通过密度估计建模代理模型的近似误差，在输入和输出空间上测量参数配置与训练参数的接近程度。结合特征似然度，形成先验信念，高效采样生成目标输出特征的合理参数配置。

Result: 开发了可视化界面，在三个模拟数据集上执行特征驱动的参数空间分析，展示了方法的可用性。

Conclusion: 该方法能够有效解决代理模型误差问题，并以交互方式形成参数分布，为科学模拟的逆问题提供了更全面的参数空间分析工具。

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [182] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: CRTR方法通过组合式负采样方案解决时间对比学习中虚假特征问题，在Sokoban和魔方等复杂时序任务中学习可泛化的表征，实现仅用学习表征高效解决任意魔方状态


<details>
  <summary>Details</summary>
Motivation: 传统AI中感知学习状态表征，规划通过搜索实现时序推理。研究是否可以从同时捕捉感知和时序结构的表征中自然涌现出这种推理能力

Method: 提出组合式时间推理表征(CRTR)，使用负采样方案可证明地移除虚假特征，促进时间推理。相比标准时间对比学习能更好捕捉时序结构

Result: 在Sokoban和魔方等复杂时序结构领域取得强劲结果。魔方任务中，CRTR学习的表征可泛化到所有初始状态，用比BestFS更少的搜索步数解决谜题（虽然解决方案更长）

Conclusion: 这是首个仅使用学习表征（不依赖外部搜索算法）就能高效解决任意魔方状态的方法，证明了时序推理可以从适当的表征学习中自然涌现

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [183] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: CGPT是一种新颖的因果引导成对Transformer架构，通过将多维时间序列数据分解为成对关系，解决了通道依赖和通道独立模型之间的权衡问题，在保持维度无关性的同时显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 工业系统中多维时间序列建模面临核心权衡：通道依赖模型能捕捉特定跨变量动态但缺乏鲁棒性和适应性，而通道独立模型具有通用性但无法建模系统级预测回归任务所需的关键显式交互。

Method: 提出因果引导成对Transformer(CGPT)，整合已知因果图作为归纳偏置。核心采用成对建模范式，将多维数据分解为成对关系，使用通道无关的可学习层，所有参数维度独立于变量数量。在成对级别强制执行通道依赖信息流，在跨成对关系中实现类似通道独立的泛化。

Result: 在合成和真实工业数据集的长周期和单步预测任务上验证，CGPT在预测精度上显著优于通道独立和通道依赖基线方法，同时与端到端训练的通道依赖模型保持竞争性能，且对问题维度保持无关性。

Conclusion: CGPT通过成对建模范式有效解决了多维时间序列建模中的通道依赖/独立冲突，实现了复杂系统动态的解耦，构建了高度灵活、可扩展且适应任意变量的架构。

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [184] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: 本文研究个人级别移动预测方法，重点关注大规模长期轨迹预测，通过完整实验分析不同模型和训练策略，提出包含语义信息和用户聚类的改进方法来提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注短期微观移动预测，对于大规模长期生活常规模式关注不够，而这在传染病监测、老幼养老等应用中具有重要意义。

Method: 采用LSTM和Transformer架构，包含周内日期和用户特定历史信息等语义信息，应用用户语义聚类和层化采样来减少数据偏斜，使用小批量随机梯度优化来提高模型性能。

Result: 显示明确包含语义信息可以帮助模型更好理解个人生活模式并改善预测效果；用户采样可能引发数据偏斜导致预测准确性下降；小批量SGD在数据有限时显著提升模型性能。

Conclusion: 通过结合语义信息、用户聚类和小批量优化等方法，可以有效提高个人长期移动轨迹预测的准确性，为相关应用领域提供更可靠的技术支撑。

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [185] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: 通过强化学习方法MDPO解决了渡散语言模型在训练和推理阶段的不一致性问题，大幅提升了性能和效率


<details>
  <summary>Details</summary>
Motivation: 渡散语言模型在训练时随机掩码而在推理时逐步揭示结构，这种不一致导致次优性能，但之前的研究忽视了这个问题

Method: 将学习有效去噪轨迹框架为序列决策问题，推出了基于马尔可夫性质的Masked Diffusion Policy Optimization (MDPO)算法，在训练中使用与推理相同的逐步精炼调度

Result: MDPO在相同权重更新次数下比SOTA方法在MATH500上提升9.6%，在Countdown上提升54.2%，并且仅需SOTA方法60倍更少的梯度更新次数就能达到相同性能

Conclusion: 该研究为解决渡散语言模型训练-推理不一致性问题开启了新方向，MDPO和RCR策略显示出了强化学习在提升渡散模型性能方面的巨大潜力

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [186] [LSM-OPD: Boosting Scan in LSM-Trees by Enabling Direct Computing on Compressed Data](https://arxiv.org/abs/2508.11862)
*Jianfeng Huang,Ziyao Wang,Lin Yuan,Jiajie Wen,Yihao Cao,Dongjing Miao,Yong Wang,Jiahao Zhang*

Main category: cs.DB

TL;DR: LSM-OPD是一种基于字典编码的LSM树优化方案，通过在压缩数据上直接计算来解决扫描密集型操作的性能瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 随着高性能存储设备（如NVMe SSD）的普及，LSM树中的扫描操作（如后台压缩和值过滤）已成为主要性能瓶颈。对于慢速设备是I/O受限，对于快速设备是计算受限，当值大小增加时，所有设备的吞吐量都会急剧下降

Method: 提出LSM-OPD（Log-Structured Merge-Order-Preserving Dictionary）编码方案，支持键值分离的数据刷盘，采用密集编码的列式布局，利用保序字典特性将昂贵的扫描操作卸载到轻量级字典，并采用SIMD向量化技术最大化现代多核处理器的评估性能

Result: 大量实验证明LSM-OPD在处理涉及密集型扫描操作的各种工作负载时，在不同现代存储设备上都表现出卓越的效率

Conclusion: LSM-OPD通过直接在压缩数据上计算，有效解决了LSM树中扫描密集型操作的性能瓶颈问题，显著提升了在各种存储设备上的处理性能

Abstract: Scan-based operations, such as backstage compaction and value filtering, have
emerged as the main bottleneck for LSM-Trees in supporting contemporary
data-intensive applications. For slower external storage devices, such as HDD
and SATA SSD, the scan performance is primarily limited by the I/O bandwidth
(i.e., I/O bound) due to the substantial read/write amplifications in
LSM-Trees. Recent adoption of high-performance storage devices, such as NVMe
SSD, has transformed the main limitation to be compute-bound, emerging the
impact of computational resource consumption caused by inefficient compactions
and filtering. However, when the value size increases, the bottleneck for scan
performance in fast devices gradually shifts towards the I/O bandwidth as well,
and the overall throughput across all types of devices undergo a dramatic
reduction. This paper addresses the core issues by proposing LSM-OPD, a Log-S
tructured M erge-O rder- Preserving Dictionary encoding scheme that enables
direct computing on compressed data within LSM-Trees. It first enables
key-value-separated data flushing to disk in a densely encoded columnar layout,
ideally with few bytes for a large string value (e.g., 1024 bytes), thereby
significantly alleviating the frequent I/O requests caused by intensive scans.
Then, it is capable of offloading the costly scan-based operations on large
values, including compaction and value filtering, to lightweight dictionaries
due to the order-preserving property. And SIMD-based vectorization can now be
employed to maximize the evaluating performance on modern multi-core
processors, further breaking the compute-bound limitations in LSM-trees.
Extensive experiments demonstrate the superior efficiency of LSM-OPD in
processing various workloads that involve intensive scan-based operations on
diverse modern storage devices.

</details>


### [187] [Carry the Tail in Consensus Protocols](https://arxiv.org/abs/2508.12173)
*Suyash Gupta,Dakai Kang,Dahlia Malkhi,Mohammad Sadoghi*

Main category: cs.DB

TL;DR: Carry-the-Tail是首个在部分同步模型下能够抵御尾分叉攻击的确定性原子广播协议，在GST后保证非故障领导者恒定比例的提交，并在故障领导者级联情况下保持最优的最坏情况二次通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的原子广播解决方案在最坏情况下达到二次通信复杂度，但在尾分叉攻击下吞吐量显著下降。现有的尾分叉攻击解决方案要么需要二次通信步骤，要么需要计算上不可行的SNARK生成。

Method: 核心技术贡献是Carry机制，这是一个实用的即插即用机制，适用于HotStuff系列的流线化协议。Carry保证了对尾分叉攻击的良好性能，并消除了大多数领导者引起的停滞，同时保持了线性流量和协议简单性。

Result: 该协议在GST后保证非故障领导者恒定比例的提交，维持最优的最坏情况二次通信复杂度，并保证线性摊销通信（稳态为线性）。

Conclusion: Carry-the-Tail协议成功解决了尾分叉攻击下的性能退化问题，提供了既高效又实用的解决方案，在保持协议简单性的同时显著提升了系统性能。

Abstract: We present Carry-the-Tail, the first deterministic atomic broadcast protocol
in partial synchrony that, after GST, guarantees a constant fraction of commits
by non-faulty leaders against tail-forking attacks, and maintains optimal,
worst-case quadratic communication under a cascade of faulty leaders. The
solution also guarantees linear amortized communication, i.e., the steady-state
is linear.
  Prior atomic broadcast solutions achieve quadratic word communication
complexity in the worst case. However, they face a significant degradation in
throughput under tail-forking attack. Existing solutions to tail-forking
attacks require either quadratic communication steps or
computationally-prohibitive SNARK generation.
  The key technical contribution is Carry, a practical drop-in mechanism for
streamlined protocols in the HotStuff family. Carry guarantees good performance
against tail-forking and removes most leader-induced stalls, while retaining
linear traffic and protocol simplicity.

</details>


### [188] [jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications](https://arxiv.org/abs/2508.12536)
*Yasuo Tabei*

Main category: cs.DB

TL;DR: jXBW是一种针对JSONL数据集的快速子结构搜索方法，通过合并树表示、eXtended Burrows-Wheeler变换数据结构和三步搜索算法，显著提升搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有JSONL数据集子结构搜索方法由于需要穷举树遍历和子树匹配，计算成本过高，无法满足现代应用（如基础模型中的提示工程）的需求。

Method: 提出三种关键技术：(i)合并多个JSON对象树同时保留个体身份的合并树表示；(ii)基于eXtended Burrows-Wheeler变换的简洁数据结构，支持高效树导航和子路径搜索；(iii)结合路径分解、祖先计算和自适应树标识符收集的三步子结构搜索算法。

Result: 在真实数据集上的实验表明，jXBW始终优于现有方法，对较小数据集实现16倍加速，对较大数据集实现高达4700倍加速，相比XML处理方法提升超过600万倍，同时保持竞争力的内存使用。

Conclusion: jXBW为大规模JSONL数据集提供了高效的子结构搜索解决方案，显著降低了计算成本，满足了现代应用的需求。

Abstract: Substructure search in JSON Lines (JSONL) datasets is essential for modern
applications such as prompt engineering in foundation models, but existing
methods suffer from prohibitive computational costs due to exhaustive tree
traversal and subtree matching. We present jXBW, a fast method for substructure
search on large-scale JSONL datasets. Our method makes three key technical
contributions: (i) a merged tree representation built by merging trees of
multiple JSON objects while preserving individual identities, (ii) a succinct
data structure based on the eXtended Burrows-Wheeler Transform that enables
efficient tree navigation and subpath search, and (iii) an efficient three-step
substructure search algorithm that combines path decomposition, ancestor
computation, and adaptive tree identifier collection to ensure correctness
while avoiding exhaustive tree traversal. Experimental evaluation on real-world
datasets demonstrates that jXBW consistently outperforms existing methods,
achieving speedups of 16$\times$ for smaller datasets and up to 4,700$\times$
for larger datasets over tree-based approaches, and more than 6$\times$10$^6$
over XML-based processing while maintaining competitive memory usage.

</details>


### [189] [Evaluating the Quality of Open Building Datasets for Mapping Urban Inequality: A Comparative Analysis Across 5 Cities](https://arxiv.org/abs/2508.12872)
*Franz Okyere,Meng Lu,Ansgar Brunn*

Main category: cs.DB

TL;DR: 这篇论文通过对比Google和Microsoft的AI生成开政建筑数据集与OpenStreetMap数据，评估了全球各城市建筑数据的质量偏差，发现结构化城市数据质量高，而非正式定居区存在显著缺失。


<details>
  <summary>Details</summary>
Motivation: 非正式定居区缺乏重点发展且动态性强，空间数据质量不确定，需要评估AI生成建筑数据集的质量和偏差。

Method: 使用交并比、重叠分析和位置精度算法分析数据集相似性和对齐度，分析建筑多边形面积大小分布，以及使用预定空间单位评估完整性。

Result: 数据质量存在显著差异，Houston和Berlin显示高对齐度和完整性，而Accra和Caracas可能被低估，建筑大小分布也反映全球社会经济分弃。

Conclusion: 需要重视全球建筑数据集质量，避免数据偏差导致的误导解释，这对规划和资源分配至关重要。

Abstract: While informal settlements lack focused development and are highly dynamic,
the quality of spatial data for these places may be uncertain. This study
evaluates the quality and biases of AI-generated Open Building Datasets (OBDs)
generated by Google and Microsoft against OpenStreetMap (OSM) data, across
diverse global cities including Accra, Nairobi, Caracas, Berlin, and Houston.
The Intersection over Union (IoU), overlap analysis and a positional accuracy
algorithm are used to analyse the similarity and alignment of the datasets. The
paper also analyses the size distribution of the building polygon area, and
completeness using predefined but regular spatial units. The results indicate
significant variance in data quality, with Houston and Berlin demonstrating
high alignment and completeness, reflecting their structured urban
environments. There are gaps in the datasets analysed, and cities like Accra
and Caracas may be under-represented. This could highlight difficulties in
capturing complex or informal regions. The study also notes different building
size distributions, which may be indicative of the global socio-economic
divide. These findings may emphasise the need to consider the quality of global
building datasets to avoid misrepresentation, which is an important element of
planning and resource distribution.

</details>


### [190] [SPARQL in N3: SPARQL CONSTRUCT as a rule language for the Semantic Web (Extended Version)](https://arxiv.org/abs/2508.13041)
*Dörthe Arndt,William Van Woensel,Dominik Tomaszuk*

Main category: cs.DB

TL;DR: 利用SPARQL CONSTRUCT查询作为逻辑规则，通过翻译到Notation3 Logic语言实现通用推理机制，提供了一种表达力强、熟悉且支持递归的规则语言方案。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义网推理方案的限制：OWL2 DL和SWRL表达力有限，RIF很少被采用，而SPARQL虽然是标准查询语言但不支持递归推理。需要一种新的规则语言方案来充分利用现有标准和工具。

Method: 将SPARQL CONSTRUCT查询作为逻辑规则使用，支持表达力强的规则写法和递归推理。然后将这些查询翻译到Notation3 Logic (N3)规则语言，以便利用现有的前向和后向链推理机制。重点是实现一对一的查询-规则映射，提高交换性和可解释性。

Result: 基准测试显示该方案具有竞争性能，在保持高性能的同时提供了更强大的表达力。

Conclusion: 该研究提出了一种有效的解决方案，通过结合SPARQL的熟悉性和N3的推理能力，提升了语义网中规则基于推理的潜力，为更复杂的知识表达和推理需求提供了新的可行途径。

Abstract: Reasoning in the Semantic Web (SW) commonly uses Description Logics (DL) via
OWL2 DL ontologies, or SWRL for variables and Horn clauses. The Rule
Interchange Format (RIF) offers more expressive rules but is defined outside
RDF and rarely adopted. For querying, SPARQL is a well-established standard
operating directly on RDF triples. We leverage SPARQL CONSTRUCT queries as
logic rules, enabling (1) an expressive, familiar SW rule language, and (2)
general recursion, where queries can act on the results of others. We translate
these queries to the Notation3 Logic (N3) rule language, allowing use of
existing reasoning machinery with forward and backward chaining. Targeting a
one-to-one query-rule mapping improves exchangeability and interpretability.
Benchmarks indicate competitive performance, aiming to advance the potential of
rule-based reasoning in the SW.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [191] [RRRA: Resampling and Reranking through a Retriever Adapter](https://arxiv.org/abs/2508.11670)
*Bongsu Kim*

Main category: cs.IR

TL;DR: 提出可学习的适配器模块来动态识别密集检索中的假阴性样本，通过重采样和重排序提升检索性能


<details>
  <summary>Details</summary>
Motivation: 现有基于启发式的方法在识别硬阴性样本时往往忽略实例特定的假阴性问题，导致训练效果受限

Method: 使用可学习适配器模块监控Bi-Encoder表示，动态估计硬阴性样本为假阴性的概率，并应用于训练重采样和推理重排序

Result: 在标准基准测试中，适配器增强框架持续优于强基线Bi-Encoder模型

Conclusion: 显式建模假阴性对密集检索具有显著益处，提出的动态上下文方法能有效提升检索性能

Abstract: In dense retrieval, effective training hinges on selecting high quality hard
negatives while avoiding false negatives. Recent methods apply heuristics based
on positive document scores to identify hard negatives, improving both
performance and interpretability. However, these global, example agnostic
strategies often miss instance specific false negatives. To address this, we
propose a learnable adapter module that monitors Bi-Encoder representations to
estimate the likelihood that a hard negative is actually a false negative. This
probability is modeled dynamically and contextually, enabling fine-grained,
query specific judgments. The predicted scores are used in two downstream
components: (1) resampling, where negatives are reweighted during training, and
(2) reranking, where top-k retrieved documents are reordered at inference.
Empirical results on standard benchmarks show that our adapter-enhanced
framework consistently outperforms strong Bi-Encoder baselines, underscoring
the benefit of explicit false negative modeling in dense retrieval.

</details>


### [192] [LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering](https://arxiv.org/abs/2508.11671)
*Ronald Carvalho Boadana,Ademir Guimarães da Costa Junior,Ricardo Rios,Fábio Santos da Silva*

Main category: cs.IR

TL;DR: 使用Gemini和LLaMA大语言模型结合智能代理构建多代理个性化音乐推荐系统，相比传统内容推荐模型在用户满意度、新颖性和计算效率方面表现更优，满意度最高达89.32%


<details>
  <summary>Details</summary>
Motivation: 流媒体平台音乐信息过载问题日益严重，需要更先进的推荐系统来提升用户体验

Method: 采用Gemini和LLaMA家族的大语言模型，结合智能代理构建多代理个性化音乐推荐系统

Result: LLM模型满意度最高达到89.32%，在用户满意度、新颖性和计算效率方面均优于传统内容推荐模型

Conclusion: 大语言模型在音乐推荐系统中展现出巨大潜力，是未来推荐系统发展的重要方向

Abstract: The growing availability of music on streaming platforms has led to
information overload for users. To address this issue and enhance the user
experience, increasingly sophisticated recommendation systems have been
proposed. This work investigates the use of Large Language Models (LLMs) from
the Gemini and LLaMA families, combined with intelligent agents, in a
multi-agent personalized music recommendation system. The results are compared
with a traditional content-based recommendation model, considering user
satisfaction, novelty, and computational efficiency. LLMs achieved satisfaction
rates of up to \textit{89{,}32\%}, indicating their promising potential in
music recommendation systems.

</details>


### [193] [Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models](https://arxiv.org/abs/2508.11784)
*Zabir Al Nazi,Vagelis Hristidis,Aaron Lawson McLean,Jannat Ara Meem,Md Taukir Azam Chowdhury*

Main category: cs.IR

TL;DR: BMQExpander是一个结合UMLS医学知识库和大型语言模型的查询扩展方法，在生物医学信息检索任务中显著提升了检索效果，比现有基线方法提升最高达22.1%。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域的文档检索面临领域特定词汇和语义歧义的挑战，需要更有效的查询扩展技术来提升检索性能。

Method: 提出BMQExpander管道，整合UMLS Metathesaurus的医学知识（定义和关系）与大型语言模型的生成能力，进行本体感知的查询扩展。

Result: 在NFCorpus、TREC-COVID和SciFact三个生物医学IR基准测试中，相比稀疏基线提升22.1% NDCG@10，比最强基线提升6.5%。在查询扰动设置下比最强基线提升15.7%，且幻觉现象更少。

Conclusion: BMQExpander通过结合结构化医学知识和LLM生成能力，实现了在生物医学检索任务中的显著性能提升和良好泛化能力。

Abstract: Effective Question Answering (QA) on large biomedical document collections
requires effective document retrieval techniques. The latter remains a
challenging task due to the domain-specific vocabulary and semantic ambiguity
in user queries. We propose BMQExpander, a novel ontology-aware query expansion
pipeline that combines medical knowledge - definitions and relationships - from
the UMLS Metathesaurus with the generative capabilities of large language
models (LLMs) to enhance retrieval effectiveness. We implemented several
state-of-the-art baselines, including sparse and dense retrievers, query
expansion methods, and biomedical-specific solutions. We show that BMQExpander
has superior retrieval performance on three popular biomedical Information
Retrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with
improvements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%
over the strongest baseline. Further, BMQExpander generalizes robustly under
query perturbation settings, in contrast to supervised baselines, achieving up
to 15.7% improvement over the strongest baseline. As a side contribution, we
publish our paraphrased benchmarks. Finally, our qualitative analysis shows
that BMQExpander has fewer hallucinations compared to other LLM-based query
expansion baselines.

</details>


### [194] [TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios](https://arxiv.org/abs/2508.11977)
*Zida Liang,Changfa Wu,Dunxian Huang,Weiqiang Sun,Ziyang Wang,Yuliang Yan,Jian Wu,Yuning Jiang,Bo Zheng,Ke Chen,Silu Zhou,Yu Zhang*

Main category: cs.IR

TL;DR: TBGRecall是一个集成下一会话预测（NSP）的生成式检索框架，通过多会话序列重构和优化训练方法，显著提升了电商推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在推荐系统中存在检索任务优化不足的问题，主要因为自回归生成机制带来的序列依赖限制了多物品的无位置约束生成。

Method: 将输入样本划分为多会话序列（会话token+物品token），采用有限历史数据预训练和随机部分增量训练相结合的方法，强调数据时效性而非数量。

Result: 在公开基准测试和淘宝大规模工业数据集上的实验表明，TBGRecall优于最先进的推荐方法，并展现出清晰的扩展规律趋势。

Conclusion: NSP代表了生成式推荐系统在电商应用效果方面的重大进步，为高效检索提供了新的解决方案。

Abstract: Recommendation systems are essential tools in modern e-commerce, facilitating
personalized user experiences by suggesting relevant products. Recent
advancements in generative models have demonstrated potential in enhancing
recommendation systems; however, these models often exhibit limitations in
optimizing retrieval tasks, primarily due to their reliance on autoregressive
generation mechanisms. Conventional approaches introduce sequential
dependencies that impede efficient retrieval, as they are inherently unsuitable
for generating multiple items without positional constraints within a single
request session. To address these limitations, we propose TBGRecall, a
framework integrating Next Session Prediction (NSP), designed to enhance
generative retrieval models for e-commerce applications. Our framework
reformulation involves partitioning input samples into multi-session sequences,
where each sequence comprises a session token followed by a set of item tokens,
and then further incorporate multiple optimizations tailored to the generative
task in retrieval scenarios. In terms of training methodology, our pipeline
integrates limited historical data pre-training with stochastic partial
incremental training, significantly improving training efficiency and
emphasizing the superiority of data recency over sheer data volume. Our
extensive experiments, conducted on public benchmarks alongside a large-scale
industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art
recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP
represents a significant advancement in the effectiveness of generative
recommendation systems for e-commerce applications.

</details>


### [195] [Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations](https://arxiv.org/abs/2508.11978)
*Viacheslav Yusupov,Maxim Rakhuba,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出了一种新颖的双曲推荐模型，利用几何洞察改进表示学习并提高计算稳定性，在推荐性能和多样性方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有研究显示双曲几何在捕捉推荐系统交互数据复杂模式方面具有潜力，但需要改进表示学习和计算稳定性

Method: 重新定义双曲距离概念以释放比欧几里得空间更大的表示能力，构建三元组损失函数通过几何驱动的成对交互项建模用户与偏好/非偏好选择的三元关系

Result: 双曲方法不仅优于现有的欧几里得和双曲模型，还减少了流行度偏差，产生更多样化和个性化的推荐

Conclusion: 双曲几何为推荐系统提供了有效的表示学习框架，能够同时提升性能和多样性

Abstract: Recent studies have demonstrated the potential of hyperbolic geometry for
capturing complex patterns from interaction data in recommender systems. In
this work, we introduce a novel hyperbolic recommendation model that uses
geometrical insights to improve representation learning and increase
computational stability at the same time. We reformulate the notion of
hyperbolic distances to unlock additional representation capacity over
conventional Euclidean space and learn more expressive user and item
representations. To better capture user-items interactions, we construct a
triplet loss that models ternary relations between users and their
corresponding preferred and nonpreferred choices through a mix of pairwise
interaction terms driven by the geometry of data. Our hyperbolic approach not
only outperforms existing Euclidean and hyperbolic models but also reduces
popularity bias, leading to more diverse and personalized recommendations.

</details>


### [196] [A Large-Scale Web Search Dataset for Federated Online Learning to Rank](https://arxiv.org/abs/2508.12353)
*Marcel Gregoriadis,Jingwei Kang,Johan Pouwelse*

Main category: cs.IR

TL;DR: 提出了AOL4FOLTR数据集，这是一个包含260万查询和1万用户的大规模网络搜索数据集，用于解决联邦在线学习排序中现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦在线学习排序基准测试存在三个主要问题：基于随机划分的传统数据集、模拟点击数据以及同步客户端参与的假设，这些都无法反映真实世界的动态特性。

Method: 构建了一个包含用户标识符、真实点击数据和查询时间戳的大规模网络搜索数据集AOL4FOLTR，支持真实的用户划分、行为建模和异步联邦学习场景。

Result: 创建了包含2.6百万查询和10,000用户的大规模数据集，提供了更真实的联邦学习排序评估基准。

Conclusion: AOL4FOLTR数据集解决了现有基准测试的关键局限性，为联邦在线学习排序研究提供了更真实和全面的评估平台。

Abstract: The centralized collection of search interaction logs for training ranking
models raises significant privacy concerns. Federated Online Learning to Rank
(FOLTR) offers a privacy-preserving alternative by enabling collaborative model
training without sharing raw user data. However, benchmarks in FOLTR are
largely based on random partitioning of classical learning-to-rank datasets,
simulated user clicks, and the assumption of synchronous client participation.
This oversimplifies real-world dynamics and undermines the realism of
experimental results. We present AOL4FOLTR, a large-scale web search dataset
with 2.6 million queries from 10,000 users. Our dataset addresses key
limitations of existing benchmarks by including user identifiers, real click
data, and query timestamps, enabling realistic user partitioning, behavior
modeling, and asynchronous federated learning scenarios.

</details>


### [197] [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
*Chenhe Dong,Shaowei Yao,Pengkun Jiao,Jianhui Yang,Yiming Jin,Zerui Huang,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 提出了TaoSR1框架，直接部署大语言模型进行电商搜索相关性预测，通过三阶段训练解决思维链错误累积、判别性幻觉和部署可行性问题，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: BERT模型擅长语义匹配但缺乏复杂推理能力，现有LLM方法多采用判别式微调或蒸馏到小模型，需要直接部署LLM并解决相关挑战。

Method: 三阶段框架：1)SFT+CoT注入推理能力；2)离线采样+DPO提升生成质量；3)难度动态采样+GRPO缓解判别性幻觉。后处理技术实现高效在线部署。

Result: 在离线数据集上显著超越基线方法，在线人工评估中获得实质性提升

Conclusion: 为相关性分类任务应用CoT推理引入了新范式，成功解决了LLM直接部署的关键挑战

Abstract: Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

</details>


### [198] [Contrastive Multi-View Graph Hashing](https://arxiv.org/abs/2508.12377)
*Yang Xu,Zuliang Yang,Kai Ming Ting*

Main category: cs.IR

TL;DR: 提出了一种新的对比多视图哈希框架CMGHash，用于多视图数据的高效索引，通过学习统一的二进制嵌入来显著提升检索准确性


<details>
  <summary>Details</summary>
Motivation: 现有的多视哈希技术通常假设输入是基于属性的，不适用于多视图数据，图数据包含复杂的拓扑结构信息，需要新方法来编码和融合多个异构图视图

Method: 设计了一种结构化的对比多视图图损失函数，在共识表征空间中拉近所有图的k近邻节点，同时推开非邻居节点对，并在这个空间上加入二进制约束以最小成本转换为二进制嵌入空间

Result: 在多个标准数据集上进行了广泛实验，结果显示CMGHash在检索准确性方面显著超过了现有方法

Conclusion: CMGHash为多视图数据的高效索引提供了一种新的结构化解决方案，通过对比学习和二进制约束有效地融合了多个异构图视图的复杂拓扑信息

Abstract: Multi-view graph data, which both captures node attributes and rich
relational information from diverse sources, is becoming increasingly prevalent
in various domains. The effective and efficient retrieval of such data is an
important task. Although multi-view hashing techniques have offered a paradigm
for fusing diverse information into compact binary codes, they typically assume
attributes-based inputs per view. This makes them unsuitable for multi-view
graph data, where effectively encoding and fusing complex topological
information from multiple heterogeneous graph views to generate unified binary
embeddings remains a significant challenge. In this work, we propose
Contrastive Multi-view Graph Hashing (CMGHash), a novel end-to-end framework
designed to learn unified and discriminative binary embeddings from multi-view
graph data. CMGHash learns a consensus node representation space using a
contrastive multi-view graph loss, which aims to pull $k$-nearest neighbors
from all graphs closer while pushing away negative pairs, i.e., non-neighbor
nodes. Moreover, we impose binarization constraints on this consensus space,
enabling its conversion to a corresponding binary embedding space at minimal
cost. Extensive experiments on several benchmark datasets demonstrate that
CMGHash significantly outperforms existing approaches in terms of retrieval
accuracy.

</details>


### [199] [Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation](https://arxiv.org/abs/2508.12645)
*Hongyang Liu,Zhu Sun,Tianjun Wei,Yan Wang,Jiajie Zhu,Xinghua Qu*

Main category: cs.IR

TL;DR: 提出了DGDPO框架，通过动态迭代优化用户画像来提升推荐系统用户模拟的真实性，解决了现有LLM模拟器在用户画像构建和交互模式方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统用户模拟器存在两个主要问题：静态单步提示导致用户画像不准确不完整；单轮推荐-反馈交互模式不真实。需要更真实的用户模拟方法来提升推荐系统开发和评估效果。

Method: DGDPO框架包含诊断模块和治疗模块的迭代优化循环：诊断模块通过新颖训练策略校准，准确识别用户画像缺陷；治疗模块分析缺陷并生成针对性改进建议。首次将动态优化与序列推荐器集成，支持多轮双向演化交互。

Result: 在三个真实世界数据集上的大量实验证明了该框架的有效性，能够显著提升用户模拟的保真度。

Conclusion: DGDPO通过动态迭代优化过程有效解决了LLM用户模拟器的局限性，为推荐系统的开发和评估提供了更真实的用户模拟方法。

Abstract: Recent advances in large language models (LLMs) have enabled realistic user
simulators for developing and evaluating recommender systems (RSs). However,
existing LLM-based simulators for RSs face two major limitations: (1) static
and single-step prompt-based inference that leads to inaccurate and incomplete
user profile construction; (2) unrealistic and single-round
recommendation-feedback interaction pattern that fails to capture real-world
scenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided
Dynamic Profile Optimization), a novel framework that constructs user profile
through a dynamic and iterative optimization process to enhance the simulation
fidelity. Specifically, DGDPO incorporates two core modules within each
optimization loop: firstly, a specialized LLM-based diagnostic module,
calibrated through our novel training strategy, accurately identifies specific
defects in the user profile. Subsequently, a generalized LLM-based treatment
module analyzes the diagnosed defect and generates targeted suggestions to
refine the profile. Furthermore, unlike existing LLM-based user simulators that
are limited to single-round interactions, we are the first to integrate DGDPO
with sequential recommenders, enabling a bidirectional evolution where user
profiles and recommendation strategies adapt to each other over multi-round
interactions. Extensive experiments conducted on three real-world datasets
demonstrate the effectiveness of our proposed framework.

</details>


### [200] [Multi-Granularity Distribution Modeling for Video Watch Time Prediction via Exponential-Gaussian Mixture Network](https://arxiv.org/abs/2508.12665)
*Xu Zhao,Ruibo Ma,Jiaqi Chen,Weiqi Zhao,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 通过提出指数-高斯混合分布(EGM)模型来解决短视频平台观看时间预测中的细粒度偏斜和粗粒度多样性挑战


<details>
  <summary>Details</summary>
Motivation: 短视频平台中准确预测观看时间对提升用户涉入度至关重要，但遇到两大挑战：粗粒度偏斜(快速跳过数据聚集)和细粒度多样性(用户-视频交互模式多样)

Method: 提出指数-高斯混合网络(EGMN)，包含隐藏表征编码器和混合参数生成器两个模块，用指数分布描述偏斜特征，高斯分布描述多样性

Result: 在公开数据集和小红书App工业场景的在线A/B测试中验证了方法的优劢性，在粗细粒度都展现了优秀的分布拟合能力

Conclusion: EGMN模型能够有效地处理短视频观看时间预测中的复杂分布特征，为工业应用提供了可靠的解决方案

Abstract: Accurate watch time prediction is crucial for enhancing user engagement in
streaming short-video platforms, although it is challenged by complex
distribution characteristics across multi-granularity levels. Through
systematic analysis of real-world industrial data, we uncover two critical
challenges in watch time prediction from a distribution aspect: (1)
coarse-grained skewness induced by a significant concentration of quick-skips1,
(2) fine-grained diversity arising from various user-video interaction
patterns. Consequently, we assume that the watch time follows the
Exponential-Gaussian Mixture (EGM) distribution, where the exponential and
Gaussian components respectively characterize the skewness and diversity.
Accordingly, an Exponential-Gaussian Mixture Network (EGMN) is proposed for the
parameterization of EGM distribution, which consists of two key modules: a
hidden representation encoder and a mixture parameter generator. We conducted
extensive offline experiments on public datasets and online A/B tests on the
industrial short-video feeding scenario of Xiaohongshu App to validate the
superiority of EGMN compared with existing state-of-the-art methods.
Remarkably, comprehensive experimental results have proven that EGMN exhibits
excellent distribution fitting ability across coarse-to-fine-grained levels. We
open source related code on Github: https://github.com/BestActionNow/EGMN.

</details>


### [201] [Asymmetric Diffusion Recommendation Model](https://arxiv.org/abs/2508.12706)
*Yongchun Zhu,Guanyu Jiang,Jingwu Chen,Feng Zhang,Xiao Yang,Zuotao Liu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的不对称潜在特征空间扩散推荐模型(AsymDiffRec)，解决了标准扩散模型在离散数据空间中的不匹配问题和个性化信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型通常在连续数据空间中使用对称的正向/反向过程和标准高斯器声，但推荐系统样本处于离散数据空间，高斯器声可能会破坏潜在表征中的个性化信息。

Method: 设计了一种不对称的扩散模型，在正向过程中模拟真实推荐样本的缺失特征，反向过程在不对称潜在特征空间中执行，并使用任务导向的优化策略保护个性化信息。

Result: 在在线A/B测试中实现了用户活跃天数+0.131%和应用使用时长+0.166%的提升，离线实验也显示了改进效果，已在抓音音乐App中部署。

Conclusion: AsymDiffRec通过不对称扩散过程有效地处理了推荐系统中的离散数据特征缺失问题，保留了个性化信息，提升了推荐性能。

Abstract: Recently, motivated by the outstanding achievements of diffusion models, the
diffusion process has been employed to strengthen representation learning in
recommendation systems. Most diffusion-based recommendation models typically
utilize standard Gaussian noise in symmetric forward and reverse processes in
continuous data space. Nevertheless, the samples derived from recommendation
systems inhabit a discrete data space, which is fundamentally different from
the continuous one. Moreover, Gaussian noise has the potential to corrupt
personalized information within latent representations. In this work, we
propose a novel and effective method, named Asymmetric Diffusion Recommendation
Model (AsymDiffRec), which learns forward and reverse processes in an
asymmetric manner. We define a generalized forward process that simulates the
missing features in real-world recommendation samples. The reverse process is
then performed in an asymmetric latent feature space. To preserve personalized
information within the latent representation, a task-oriented optimization
strategy is introduced. In the serving stage, the raw sample with missing
features is regarded as a noisy input to generate a denoising and robust
representation for the final prediction. By equipping base models with
AsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and
+0.166% in terms of users' active days and app usage duration respectively.
Additionally, the extended offline experiments also demonstrate improvements.
AsymDiffRec has been implemented in the Douyin Music App.

</details>


### [202] [Deep Research: A Survey of Autonomous Research Agents](https://arxiv.org/abs/2508.12752)
*Wenlin Zhang,Xiaopeng Li,Yingyi Zhang,Pengyue Jia,Yichao Wang,Huifeng Guo,Yong Liu,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 这篇调研论文系统总结了深度研究模型的四阶段流程，分析了各阶段的技术挑战和解决方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能力强大，但受限于内部知识边界，需要通过深度研究来突破这些限制，生成基于网络证据的可靠分析报告。

Method: 提出深度研究四阶段流程：规划、问题开发、网络探索和报告生成。对每个阶段进行技术挑战分析和方法分类，总结优化技术和测试标准。

Result: 系统化地总结了深度研究领域的核心技术流程和代表性方法，为构建更强大可靠的研究系统提供了理论基础。

Conclusion: 未来需要继续研究深度研究系统的关键技术挑战，探索更强大可靠的自主研究机器人的建造方向。

Abstract: The rapid advancement of large language models (LLMs) has driven the
development of agentic systems capable of autonomously performing complex
tasks. Despite their impressive capabilities, LLMs remain constrained by their
internal knowledge boundaries. To overcome these limitations, the paradigm of
deep research has been proposed, wherein agents actively engage in planning,
retrieval, and synthesis to generate comprehensive and faithful analytical
reports grounded in web-based evidence. In this survey, we provide a systematic
overview of the deep research pipeline, which comprises four core stages:
planning, question developing, web exploration, and report generation. For each
stage, we analyze the key technical challenges and categorize representative
methods developed to address them. Furthermore, we summarize recent advances in
optimization techniques and benchmarks tailored for deep research. Finally, we
discuss open challenges and promising research directions, aiming to chart a
roadmap toward building more capable and trustworthy deep research agents.

</details>


### [203] [Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations](https://arxiv.org/abs/2508.13019)
*Lucien Heitz,Runze Li,Oana Inel,Abraham Bernstein*

Main category: cs.IR

TL;DR: 提出Informfully Recommenders框架，为规范驱动的推荐系统提供可复现性支持，重点关注多样性优化的前处理、处理中、后处理和评估阶段


<details>
  <summary>Details</summary>
Motivation: 当前缺乏支持规范驱动实验的可复现性框架，特别是在推荐系统流水线的各个阶段进行完整的多样性相关实验

Method: 基于Cornac建立Informfully Recommenders扩展，提供结构化的端到端解决方案，包括数据集前处理、多样性优化模型、内部会话项目重新排序和广泛的多样性指标

Result: 通过在新闻领域进行涉及广的离线实验，验证了该扩展的功能和能力

Conclusion: 该框架是向规范可复现性进行的首步尝试，为实现和实验具有多样性意识设计的推荐系统提供了完整的工具支持

Abstract: Norm-aware recommender systems have gained increased attention, especially
for diversity optimization. The recommender systems community has
well-established experimentation pipelines that support reproducible
evaluations by facilitating models' benchmarking and comparisons against
state-of-the-art methods. However, to the best of our knowledge, there is
currently no reproducibility framework to support thorough norm-driven
experimentation at the pre-processing, in-processing, post-processing, and
evaluation stages of the recommender pipeline. To address this gap, we present
Informfully Recommenders, a first step towards a normative reproducibility
framework that focuses on diversity-aware design built on Cornac. Our extension
provides an end-to-end solution for implementing and experimenting with
normative and general-purpose diverse recommender systems that cover 1) dataset
pre-processing, 2) diversity-optimized models, 3) dedicated intrasession item
re-ranking, and 4) an extensive set of diversity metrics. We demonstrate the
capabilities of our extension through an extensive offline experiment in the
news domain.

</details>


### [204] [D-RDW: Diversity-Driven Random Walks for News Recommender Systems](https://arxiv.org/abs/2508.13035)
*Runze Li,Lucien Heitz,Oana Inel,Abraham Bernstein*

Main category: cs.IR

TL;DR: D-RDW是一种轻量级的随机游走算法，通过结合传统随机游走和可定制目标分布来生成多样化的新闻推荐，在多样性和计算效率方面优于现有神经模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决新闻推荐系统中缺乏多样性、透明度和可定制性的问题，让编辑能够将社会规范和价值观融入推荐过程。

Method: 结合传统随机游走算法的多样化能力和新闻文章属性的可定制目标分布，开发了Diversity-Driven RandomWalks算法。

Result: 在情感和政治党派提及等关键多样性指标上表现优于最先进的神经模型，同时计算效率更高。

Conclusion: D-RDW提供了一种透明且可定制的方法来生成多样化的新闻推荐，在性能和效率方面都有显著优势。

Abstract: This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight
algorithm and re-ranking technique that generates diverse news recommendations.
D-RDW is a societal recommender, which combines the diversification
capabilities of the traditional random walk algorithms with customizable target
distributions of news article properties. In doing so, our model provides a
transparent approach for editors to incorporate norms and values into the
recommendation process. D-RDW shows enhanced performance across key diversity
metrics that consider the articles' sentiment and political party mentions when
compared to state-of-the-art neural models. Furthermore, D-RDW proves to be
more computationally efficient than existing approaches.

</details>


### [205] [Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation](https://arxiv.org/abs/2508.13064)
*Seongeun Ryu,Yunyong Ko,Sang-Wook Kim*

Main category: cs.IR

TL;DR: LIME是一个新颖的新闻推荐框架，通过用户-主题生命周期感知的年龄表示、候选感知的生命周期注意力和新鲜度引导的兴趣优化，解决了时间相关挑战，显著提升了推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有新闻推荐方法中未充分探索的两个时间相关挑战：(1)利用点击新闻的年龄推断用户兴趣持续性；(2)建模不同主题和用户间新闻生命周期的变化。

Method: 提出LIME框架，包含三个关键策略：用户-主题生命周期感知的年龄表示、候选感知的生命周期注意力机制、新鲜度引导的兴趣优化方法。

Result: 在两个真实世界数据集上的广泛实验表明，LIME持续优于多种最先进的新闻推荐方法，其模型无关策略显著提高了推荐准确性。

Conclusion: LIME框架通过有效建模时间相关因素，为新闻推荐提供了更准确和个性化的解决方案，其策略具有模型无关性，可广泛应用于不同推荐系统。

Abstract: Personalized news recommendation aims to deliver news articles aligned with
users' interests, serving as a key solution to alleviate the problem of
information overload on online news platforms. While prior work has improved
interest matching through refined representations of news and users, the
following time-related challenges remain underexplored: (C1) leveraging the age
of clicked news to infer users' interest persistence, and (C2) modeling the
varying lifetime of news across topics and users. To jointly address these
challenges, we propose a novel Lifetime-aware Interest Matching framework for
nEws recommendation, named LIME, which incorporates three key strategies: (1)
User-Topic lifetime-aware age representation to capture the relative age of
news with respect to a user-topic pair, (2) Candidate-aware lifetime attention
for generating temporally aligned user representation, and (3) Freshness-guided
interest refinement for prioritizing valid candidate news at prediction time.
Extensive experiments on two real-world datasets demonstrate that LIME
consistently outperforms a wide range of state-of-the-art news recommendation
methods, and its model agnostic strategies significantly improve recommendation
accuracy.

</details>
