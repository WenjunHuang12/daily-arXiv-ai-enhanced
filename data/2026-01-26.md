<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 3]
- [cs.IT](#cs.IT) [Total: 16]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.GT](#cs.GT) [Total: 7]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Online Computation of Palindromes and Suffix Trees on Tries](https://arxiv.org/abs/2601.16485)
*Hiroki Shibata,Mitsuru Funakoshi,Takuya Mieno,Masakazu Ishihata,Yuto Nakashima,Shunsuke Inenaga,Hideo Bannai,Masayuki Takeda*

Main category: cs.DS

TL;DR: 本文提出了在支持叶子节点插入和删除的动态字典树中枚举回文串的首个亚二次算法，包括最大回文和不同回文的在线算法。


<details>
  <summary>Details</summary>
Motivation: 现有算法仅适用于静态字典树，而实际应用中字典树经常需要动态更新。需要开发支持叶子节点增删的动态字典树回文枚举算法。

Method: 针对最大回文，提出O(N min(log h, σ))时间复杂度的算法；针对不同回文，基于EERTREE和字典树后缀树等不同框架开发多种在线算法，支持叶子插入和删除。

Result: 实现了首个动态字典树回文枚举的亚二次算法，最大回文算法时间复杂度为O(N min(log h, σ))，不同回文算法提供多种时空权衡方案。

Conclusion: 本文填补了动态字典树回文枚举算法的空白，提出的算法具有理论意义和实用价值，同时构建的字典树后缀树和EERTREE算法也具有独立研究价值。

Abstract: We consider the problems of computing maximal palindromes and distinct palindromes in a trie. A trie is a natural generalization of a string, which can be seen as a single-path tree. There is a linear-time offline algorithm to compute maximal palindromes and distinct palindromes in a given (static) trie whose edge-labels are drawn from a linearly-sortable alphabet [Mieno et al., ISAAC 2022]. In this paper, we tackle problems of palindrome enumeration on dynamic tries which support leaf additions and leaf deletions. We propose the first sub-quadratic algorithms to enumerate palindromes in a dynamic trie. For maximal palindromes, we propose an algorithm that runs in $O(N \min(\log h, σ))$ time and uses $O(N)$ space, where $N$ is the maximum number of edges in the trie, $σ$ is the size of the alphabet, and $h$ is the height of the trie. For distinct palindromes, we develop several online algorithms based on different algorithmic frameworks, including approaches using the EERTREE (a.k.a. palindromic tree) and the suffix tree of a trie. These algorithms support leaf insertions and deletions in the trie and achieve different time and space trade-offs. Furthermore, as a by-product, we present online algorithms to construct the suffix tree and the EERTREE of the input trie, which is of independent interest.

</details>


### [2] [Recovering Communities in Structured Random Graphs](https://arxiv.org/abs/2601.16910)
*Michael Kapralov,Luca Trevisan,Weronika Wrzos-Kaminska*

Main category: cs.DS

TL;DR: 在随机超立方体图中，即使存在大量重叠的最稀疏割，仍然可以从随机样本中恢复所有坐标割，这扩展了传统社区恢复理论。


<details>
  <summary>Details</summary>
Motivation: 传统随机块模型研究社区恢复时假设社区形成不相交的稀疏割。本文探讨当期望图中存在大量重叠的最稀疏割时（如超立方体图有d个坐标割），是否仍能从随机样本中恢复这些割。

Method: 结合Friedgut-Kalai-Naor定理（关于傅里叶谱集中在第一层的布尔函数）和Karger的割计数论证，建立了超立方体图的强割稀疏化边界。

Result: 当采样率p=C log d/d（C足够大）时，超立方体样本中最稀疏平衡割以高概率接近某个坐标割，误差为1/poly(d)。这是渐近最优的，且能同时近似恢复所有d个割。对超立方体类图的适当样本可实现精确恢复。

Conclusion: 即使在期望图中存在大量重叠的最稀疏割，仍能从随机图中恢复这些割，扩展了传统社区恢复理论的范围，为超立方体等结构提供了恢复保证。

Abstract: The problem of recovering planted community structure in random graphs has received a lot of attention in the literature on the stochastic block model, where the input is a random graph in which edges crossing between different communities appear with smaller probability than edges induced by communities. The communities themselves form a collection of vertex-disjoint sparse cuts in the expected graph, and can be recovered, often exactly, from a sample as long as a separation condition on the intra- and inter-community edge probabilities is satisfied.
  In this paper, we ask whether the presence of a large number of overlapping sparsest cuts in the expected graph still allows recovery. For example, the $d$-dimensional hypercube graph admits $d$ distinct (balanced) sparsest cuts, one for every coordinate. Can these cuts be identified given a random sample of the edges of the hypercube where each edge is present independently with some probability $p\in (0, 1)$? We show that this is the case, in a very strong sense: the sparsest balanced cut in a sample of the hypercube at rate $p=C\log d/d$ for a sufficiently large constant $C$ is $1/\text{poly}(d)$-close to a coordinate cut with high probability. This is asymptotically optimal and allows approximate recovery of all $d$ cuts simultaneously. Furthermore, for an appropriate sample of hypercube-like graphs recovery can be made exact. The proof is essentially a strong hypercube cut sparsification bound that combines a theorem of Friedgut, Kalai and Naor on boolean functions whose Fourier transform concentrates on the first level of the Fourier spectrum with Karger's cut counting argument.

</details>


### [3] [Conditionally Tight Algorithms for Maximum k-Coverage and Partial k-Dominating Set via Arity-Reducing Hypercuts](https://arxiv.org/abs/2601.16923)
*Nick Fischer,Marvin Künnemann,Mirza Redzic*

Main category: cs.DS

TL;DR: 本文重新审视经典的最大k覆盖问题，针对小t值改进了Partial k-Dominating Set的算法时间复杂度，并针对最大k覆盖问题给出了基于参数u、s、f的条件最优时间复杂度的算法。


<details>
  <summary>Details</summary>
Motivation: 最大k覆盖问题已有广泛研究，但现有算法在参数化复杂性方面仍有改进空间。本文旨在：(1) 针对Partial k-Dominating Set，在小t值时改进时间复杂度；(2) 针对最大k覆盖问题，基于最自然的参数（宇宙大小u、最大集合大小s、最大频率f）确定最优运行时间。

Method: 对于问题(1)，设计了基于矩阵乘法指数的算法，时间复杂度为O(nt + t^{2ω/3 k+O(1)})或O(nt+ t^{3/2 k+O(1)})。对于问题(2)，设计了复杂的时间复杂度表达式，包含(f·min{∛u, √s})^k + min{n,f·min{√u, s}}^{kω/3}等项，并基于细粒度复杂性假设证明其最优性。

Result: 成功解决了两个问题：获得了Partial k-Dominating Set的条件最优时间界限，并针对最大k覆盖问题给出了基于参数u、s、f的条件最优算法，其复杂时间界限被证明是最优的。

Conclusion: 本文在参数化复杂性框架下显著改进了最大k覆盖问题的算法效率，为小覆盖数情形和基于自然参数的情形提供了条件最优的时间复杂度界限，深化了对该经典问题的理解。

Abstract: We revisit the classic Maximum $k$-Coverage problem: Determine the largest number $t$ of elements that can be covered by choosing $k$ sets from a given family $\mathcal{F} = \{S_1,\dots, S_n\}$ of a size-$u$ universe. A notable special case is Partial $k$-Dominating Set, where one chooses $k$ vertices in a graph to maximize the number of dominated vertices.
  Extensive research has established strong hardness results for various aspects of Maximum $k$-Coverage, such as tight inapproximability results, $W[2]$-hardness, and a conditionally tight worst-case running time of $n^{k\pm o(1)}$. In this paper we ask: (1) Can this time bound be improved for small $t$, at least for Partial $k$-Dominating Set, ideally to time~$t^{k\pm O(1)}$? (2) More ambitiously, can we even determine the best-possible running time of Maximum $k$-Coverage with respect to the perhaps most natural parameters: the universe size $u$, the maximum set size $s$, and the maximum frequency $f$?
  We successfully resolve both questions. (1) We give an algorithm that solves Partial $k$-Dominating Set in time $O(nt + t^{\frac{2ω}{3} k+O(1)})$ if $ω\ge 2.25$ and time $O(nt+ t^{\frac{3}{2} k+O(1)})$ if $ω\le 2.25$, where $ω\le 2.372$ is the matrix multiplication exponent. From this we derive a time bound that is conditionally optimal, regardless of $ω$, based on the well-established $k$-clique and 3-uniform hyperclique hypotheses from fine-grained complexity. We also obtain matching upper and lower bounds for sparse graphs. To address (2) we design an algorithm for Maximum $k$-Coverage running in time
  $$
  \min \left\{ (f\cdot \min\{\sqrt[3]{u}, \sqrt{s}\})^k + \min\{n,f\cdot \min\{\sqrt{u}, s\}\}^{kω/3}, n^k\right\}
  \cdot g(k)n^{\pm O(1)}, $$ and, surprisingly, further show that this complicated time bound is also conditionally optimal.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [4] [Study of Switched Step-size Based Filtered-x NLMS Algorithm for Active Noise Cancellation](https://arxiv.org/abs/2601.16382)
*Zhiyuan Li,Yi Yu,Hongsen He,Yuyu Zhu,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出两种改进的FxNLMS算法：SSS-FxNLMS通过动态切换步长解决收敛速度与稳态误差的权衡问题，以及其鲁棒变体用于脉冲噪声环境。


<details>
  <summary>Details</summary>
Motivation: 传统FxNLMS算法存在两个关键限制：固定步长导致收敛速度与稳态误差之间的权衡，以及在脉冲噪声环境下性能显著下降。

Method: 1. 推导FxNLMS算法的均方偏差趋势；2. 通过比较不同步长对应的MSD趋势，为每次迭代选择最优步长，提出SSS-FxNLMS算法；3. 将鲁棒策略集成到SSS-FxNLMS中，形成鲁棒变体。

Result: 通过计算机仿真在不同噪声场景下验证了所提算法的有效性和优越性。

Conclusion: 提出的SSS-FxNLMS算法及其鲁棒变体成功解决了传统FxNLMS算法的步长约束问题和脉冲噪声环境下的性能下降问题。

Abstract: While the filtered-x normalized least mean square (FxNLMS) algorithm is widely applied due to its simple structure and easy implementation for active noise control system, it faces two critical limitations: the fixed step-size causes a trade-off between convergence rate and steady-state residual error, and its performance deteriorates significantly in impulsive noise environments. To address the step-size constraint issue, we propose the switched \mbox{step-size} FxNLMS (SSS-FxNLMS) algorithm. Specifically, we derive the \mbox{mean-square} deviation (MSD) trend of the FxNLMS algorithm, and then by comparing the MSD trends corresponding to different \mbox{step-sizes}, the optimal step-size for each iteration is selected. Furthermore, to enhance the algorithm's robustness in impulsive noise scenarios, we integrate a robust strategy into the SSS-FxNLMS algorithm, resulting in a robust variant of it. The effectiveness and superiority of the proposed algorithms has been confirmed through computer simulations in different noise scenarios.

</details>


### [5] [Two classes of LCD codes derived from $(\mathcal{L},\mathcal{P})$-TGRS codes](https://arxiv.org/abs/2601.16438)
*Ziwei Zhao,Xiaoni DU,Xingbin Qiao*

Main category: cs.IT

TL;DR: 本文从扭曲广义Reed-Solomon码构造了两类LCD码，并进一步得到LCD MDS码


<details>
  <summary>Details</summary>
Motivation: 扭曲广义Reed-Solomon码作为经典GRS码的灵活扩展，近年来受到广泛关注。本文旨在从特定的TGRS码构造线性互补对偶码，特别是LCD MDS码，以增强编码理论中的码类多样性。

Method: 首先推导了特定参数下TGRS码的校验矩阵，给出了其成为AMDS码的充要条件。然后通过适当选择评估点并对扭曲项多项式中x^{h-1}系数施加限制，从该TGRS码构造了两类LCD码。

Result: 成功构造了两类LCD码，并进一步获得了LCD MDS码。通过多个示例验证了构造方法的有效性。

Conclusion: 本文提供了一种从扭曲广义Reed-Solomon码构造LCD码和LCD MDS码的系统方法，丰富了编码理论中的码类构造技术。

Abstract: Twisted generalized Reed-Solomon (TGRS) codes, as a flexible extension of classical generalized Reed-Solomon (GRS) codes, have attracted significant attention in recent years. In this paper, we construct two classes of LCD codes from the $(\mathcal{L},\mathcal{P})$-TGRS code $\mathcal{C}_h$ of length $n$ and dimension $k$, where $\mathcal{L}=\{0,1,\ldots,l\}$ for $l\leq n-k-1$ and $\mathcal{P}=\{h\}$ for $1\leq h\leq k-1$. First, we derive the parity check matrix of $\mathcal{C}_h$ and provide a necessary and sufficient condition for $\mathcal{C}_h$ to be an AMDS code. Then, we construct two classes of LCD codes from $\mathcal{C}_h$ by suitably choosing the evaluation points together with certain restrictions on the coefficient of $x^{h-1}$ in the polynomial associated with the twisting term. From the constructed LCD codes we further obtain two classes of LCD MDS codes. Finally, several examples are presented.

</details>


### [6] [Cramér-Rao Bound Minimization for Flexible Intelligent Metasurface-Enabled ISAC Systems](https://arxiv.org/abs/2601.16455)
*Qian Zhang,Yufei Zhao,Jiancheng An,Zheng Dong,Yong Liang Guan,Ju Liu,Chau Yuen*

Main category: cs.IT

TL;DR: 本文首次研究了柔性智能超表面（FIM）使能的ISAC系统中的CRB最小化问题，通过优化FIM表面形状和波束成形，显著降低感知CRB同时保持通信质量。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信（ISAC）是未来无线网络的关键使能技术，Cramér-Rao界（CRB）在量化感知精度中起核心作用。目前缺乏对柔性智能超表面（FIM）使能的ISAC系统中CRB最小化的研究。

Method: 1. 推导了明确依赖于FIM表面形状的平均CRB表达式；2. 采用平均Fisher信息最大化作为替代目标，使用Gauss-Hermite求积法获得目标函数的显式近似；3. 将问题分解为三个子问题：波束成形优化、发射/接收FIM表面形状优化；4. 波束成形优化使用Schur补和基于惩罚的半定松弛技术；5. 接收FIM表面形状优化使用定点方程法，发射FIM使用投影梯度算法。

Result: 仿真结果表明，与刚性阵列相比，发射和接收FIM的表面形状优化能显著降低平均感知CRB，同时保持通信质量，并且在多目标场景中仍然有效。

Conclusion: 本研究首次探索了FIM使能的ISAC系统中的CRB最小化问题，证明了FIM表面形状的可重构性能够显著提升感知性能，为未来ISAC系统设计提供了新思路。

Abstract: Integrated sensing and communication (ISAC) have been widely recognized as a key enabler for future wireless networks, where the Cramér-Rao bound (CRB) plays a central role in quantifying sensing accuracy.In this paper, we present the first study on CRB minimization in flexible intelligent metasurface (FIM)-enabled ISAC systems.Specifically, we first derive an average CRB expression that explicitly depends on FIM surface shape and demonstrate that array reconfigurability can substantially reduce the CRB, thereby significantly enhancing sensing performance.Moreover, to tackle the challenging CRB minimization problem, we adopt average Fisher information maximization as a surrogate objective and use the Gauss-Hermite quadrature method to obtain an explicit approximation of the objective function.The resulting problem is then decoupled into three subproblem, i.e., beamforming optimization and transmit/receive FIM surface shape optimization.For beamforming optimization, we employ the Schur complement and penalty-based semi-definite relaxation (SDR) technique to solve it.Furthermore, we propose a fixed-point equation method and a projected gradient algorithm to optimize the surface shapes of the receive and transmit FIMs, respectively.Simulation results demonstrate that, compared to rigid arrays, surface shaping of both transmit and receive FIMs can significantly reduce the average sensing CRB while maintaining communication quality, and remains effective even in multi-target scenarios.

</details>


### [7] [Log-Likelihood Loss for Semantic Compression](https://arxiv.org/abs/2601.16461)
*Anuj Kumar Yadav,Dan Song,Yanina Shkel,Ayfer Özgür*

Main category: cs.IT

TL;DR: 本文研究基于指定条件分布P_{X|U}的负对数似然定义的失真度量下的有损信源编码，这种对数似然失真建模了压缩场景，其中重建是语义表示而非逐点近似。


<details>
  <summary>Details</summary>
Motivation: 传统有损压缩通常关注逐点近似失真，但许多实际应用需要重建能够概率生成源的语义表示。对数似然失真提供了一种框架，将压缩与概率生成模型联系起来，适用于语义压缩场景。

Method: 提出基于条件分布P_{X|U}的负对数似然失真度量，构建相应的率失真问题框架。分析该率失真函数的基本性质，包括与对数损失下的有损压缩、任意失真度量的经典率失真问题以及完美感知率失真的联系。

Result: 建立了对数似然失真率失真函数的基本理论框架，揭示了其与多种压缩问题的内在联系，为语义压缩提供了理论基础。

Conclusion: 对数似然失真为建模语义压缩提供了一种自然框架，其中重建是能够概率生成源的语义表示。该框架统一了多种压缩问题，为理解语义压缩的率失真权衡提供了理论基础。

Abstract: We study lossy source coding under a distortion measure defined by the negative log-likelihood induced by a prescribed conditional distribution $P_{X|U}$. This \emph{log-likelihood distortion} models compression settings in which the reconstruction is a semantic representation from which the source can be probabilistically generated, rather than a pointwise approximation. We formulate the corresponding rate-distortion problem and characterize fundamental properties of the resulting rate-distortion function, including its connections to lossy compression under log-loss, classical rate-distortion problems with arbitrary distortion measures, and rate-distortion with perfect perception.

</details>


### [8] [Load Balanced ISAC Systems for URLLC Users](https://arxiv.org/abs/2601.16495)
*Shivani Singh,Amudheesan Nakkeeran,Prem Singh,Ekant Sharma,Jyotsna Bapat*

Main category: cs.IT

TL;DR: 该论文提出了一种用于集成感知与通信(ISAC)网络的能量高效负载均衡算法，在服务URLLC用户的同时检测目标，相比无负载均衡基线可减少约33%的功耗。


<details>
  <summary>Details</summary>
Motivation: 在CF-mMIMO ISAC网络中，需要同时满足URLLC用户的通信QoS要求和目标检测的感知QoS要求，同时最小化网络功耗。现有方法缺乏有效的负载均衡机制，导致功耗较高。

Method: 提出联合功率分配和AP负载均衡(JPALB)算法，通过混合整数非凸优化问题建模，迭代优化功率分配和AP负载，考虑了发射功率、固定静态功率和流量相关的前传功率。

Result: 仿真结果显示，与无负载均衡的基线相比，JPALB算法可减少约33%的总功耗，同时满足URLLC通信和目标检测的QoS要求。算法在MRT和RZF预编码器下均表现良好。

Conclusion: 提出的JPALB算法能有效平衡CF-mMIMO ISAC网络中的负载，显著降低功耗而不影响通信和感知性能，为能量高效的ISAC网络设计提供了有效解决方案。

Abstract: This paper presents an energy-efficient downlink cell-free massive multiple-input multiple-output (CF-mMIMO) integrated sensing and communication (ISAC) network that serves ultra-reliable low-latency communication (URLLC) users while simultaneously detecting a target. We propose a load-balancing algorithm that minimizes the total network power consumption; including transmit power, fixed static power, and traffic-dependent fronthaul power at the access points (APs) without degrading system performance. To this end, we formulate a mixed-integer non-convex optimization problem and introduce an iterative joint power allocation and AP load balancing (JPALB) algorithm. The algorithm aims to reduce total power usage while meeting both the communication quality-of-service (QoS) requirements of URLLC users and the sensing QoS needed for target detection. Proposed JPALB algorithm for ISAC systems was simulated with maximum-ratio transmission (MRT) and regularized zero-forcing (RZF) precoders. Simulation results show approximately 33% reduction in power consumption, using JPALB algorithm compared to a baseline with no load balancing, without compromising communication and sensing QoS requirements.

</details>


### [9] [Noise-immune and AI-enhanced DNA storage via adaptive partition mapping of digital data](https://arxiv.org/abs/2601.16518)
*Zimu Li,Bingyi Liu,Lei Zhao,Qian Zhang,Yang Liu,Jun Liu,Ke Ke,Huating Kong,Xiaolei Zuo,Chunhai Fan,Fei Wang*

Main category: cs.IT

TL;DR: PJ编码方案通过分区映射和跳转旋转策略，为DNA存储提供卓越的抗噪能力，可在任意链丢失比例下解码原始文件，并利用AI推理实现可控恢复。


<details>
  <summary>Details</summary>
Motivation: DNA存储作为应对信息时代和人工智能数据增长的有前景方案，但实际应用中受到合成、保存和测序过程中引入错误的限制，传统纠错码在噪声超过预设阈值时仍然脆弱。

Method: 开发了分区映射与跳转旋转(PJ)编码方案：1) 分区映射消除跨链信息依赖，使链丢失表现为局部间隙而非灾难性文件失效；2) 跳转旋转策略放宽传统旋转码的序列约束，通过可调跳转长度提供可调信息密度；3) 利用AI推理实现可控信息恢复。

Result: PJ编码可在任意链丢失比例下解码原始文件，保真度随损坏增加而平滑下降。即使10%链丢失也能有效恢复原始文件，存储的机器学习数据集保持分类性能。实验证实PJ在加速老化和高强度X射线辐照等极端环境扰动后仍能成功解码图像文件。

Conclusion: PJ通过消除对先验错误概率的依赖，建立了一个通用的稳健DNA存储框架，能够承受现实世界保存的严苛条件，为档案级DNA存储提供了解决方案。

Abstract: Encoding digital information into DNA sequences offers an attractive potential solution for storing rapidly growing data under the information age and the rise of artificial intelligence. However, practical implementations of DNA storage are constrained by errors introduced during synthesis, preservation, and sequencing processes, and traditional error-correcting codes remain vulnerable to noise levels that exceed predefined thresholds. Here, we developed a Partitioning-mapping with Jump-rotating (PJ) encoding scheme, which exhibits exceptional noise resilience. PJ removes cross-strand information dependencies so that strand loss manifests as localized gaps rather than catastrophic file failure. It prioritizes file decodability under arbitrary noise conditions and leverages AI-based inference to enable controllable recovery of digital information. For the intra-strand encoding, we develop a jump-rotating strategy that relaxes sequence constraints relative to conventional rotating codes and provides tunable information density via an adjustable jump length. Based on this encoding architecture, the original file information can always be decoded and recovered under any strand loss ratio, with fidelity degrading smoothly as damage increases. We demonstrate that original files can be effectively recovered even with 10% strand loss, and machine learning datasets stored under these conditions retain their classification performance. Experiments further confirmed that PJ successfully decodes image files after extreme environmental disturbance using accelerated aging and high-intensity X-ray irradiation. By eliminating reliance on prior error probabilities, PJ establishes a general framework for robust, archival DNA storage capable of withstanding the rigorous conditions of real-world preservation.

</details>


### [10] [Generalized Forms of the Kraft Inequality for Finite-State Encoders](https://arxiv.org/abs/2601.16594)
*Neri Merhav*

Main category: cs.IT

TL;DR: 该论文推导了有限状态无损编码器的Kraft不等式扩展版本，定义了Kraft矩阵概念，并证明信息无损的必要条件是Kraft矩阵的谱半径不超过1。


<details>
  <summary>Details</summary>
Motivation: 研究有限状态编码器的信息无损条件，扩展经典Kraft不等式到更一般的有限状态编码场景，为编码理论提供更严格的理论基础。

Method: 定义Kraft矩阵概念，利用谱半径理论分析有限状态编码器的性质，推导Kraft不等式的扩展版本，特别针对不可约情况建立等价形式。

Result: 建立了有限状态无损编码器的Kraft不等式扩展：Kraft矩阵谱半径≤1是信息无损的必要条件；不可约情况下Kraft和有界且不随块长增长；提出了带边信息和有损压缩的扩展。

Conclusion: 该工作将经典Kraft不等式推广到有限状态编码器，建立了基于谱半径的无损编码必要条件，为编码理论提供了新的分析工具和理论框架。

Abstract: We derive a few extended versions of the Kraft inequality for information lossless finite-state encoders. The main basic contribution is in defining a notion of a Kraft matrix and in establishing the fact that a necessary condition for information losslessness of a finite-state encoder is that none of the eigenvalues of this matrix have modulus larger than unity, or equivalently, the generalized Kraft inequality asserts that the spectral radius of the Kraft matrix cannot exceed one. For the important special case where the FS encoder is irreducible, we derive several equivalent forms of this inequality, which are based on well known formulas for spectral radius. It also turns out that in the irreducible case, Kraft sums are bounded by a constant, independent of the block length, and thus cannot grow even in any subexponential rate. Finally, two extensions are outlined - one concerns the case of side information available to both encoder and decoder, and the other is for lossy compression.

</details>


### [11] [An Explicit Upper Bound of Generalized Quadratic Gauss Sums and Its Applications for Asymptotically Optimal Aperiodic Polyphase Sequence Design](https://arxiv.org/abs/2601.16599)
*Huaning Liu,Zilong Liu*

Main category: cs.IT

TL;DR: 该论文解决了设计渐进最优非周期多相序列集的长期开放问题，通过推导广义二次高斯和的上界，并基于此构建了四种系统性的最优序列集构造方法。


<details>
  <summary>Details</summary>
Motivation: 解决设计渐进最优非周期多相序列集的长期开放问题，该问题自Mow 30多年前提出以来一直未得到全面理解，特别是关于Welch界的序列设计。

Method: 1. 通过递归应用Paris渐近展开并利用Fibonacci zeta函数的快速收敛性，推导出广义二次高斯和的显式上界；2. 基于此上界，通过精心选择Chu序列和Alltop序列，提出了四种系统性的最优序列集构造方法。

Result: 1. 首次揭示了完整Alltop序列集在非周期相关旁瓣方面是渐进最优的；2. 提出了具有在整个时移窗口内同时具备最优非周期相关和模糊特性的新型Alltop序列子集；3. 成功构建了四种渐进最优序列集。

Conclusion: 该研究解决了30多年的开放问题，通过理论推导和系统构造，为设计渐进最优非周期多相序列集提供了完整的解决方案，并发现了Alltop序列集的优化特性。

Abstract: This work is motivated by the long-standing open problem of designing asymptotically order-optimal aperiodic polyphase sequence sets with respect to the celebrated Welch bound. Attempts were made by Mow over 30 years ago, but a comprehensive understanding to this problem is lacking. Our first key contribution is an explicit upper bound of generalized quadratic Gauss sums which is obtained by recursively applying Paris' asymptotic expansion and then bounding it by leveraging the fast convergence property of the Fibonacci zeta function. Building upon this major finding, our second key contribution includes four systematic constructions of order-optimal sequence sets with low aperiodic correlation and/or ambiguity properties via carefully selected Chu sequences and Alltop sequences. For the first time in the literature, we reveal that the full Alltop sequence set is asymptotically optimal for its low aperiodic correlation sidelobes. Besides, we introduce a novel subset of Alltop sequences possessing both order-optimal aperiodic correlation and ambiguity properties for the entire time-shift window.

</details>


### [12] [Term Coding: An Entropic Framework for Extremal Combinatorics and the Guessing--Number Sandwich Theorem](https://arxiv.org/abs/2601.16614)
*Søren Riis*

Main category: cs.IT

TL;DR: 本文提出了一种将代数恒等式求解问题转化为图猜测数问题的框架，建立了项编码与图熵之间的联系，证明了最大编码大小与规范化依赖结构的猜测数之间的渐近等价关系。


<details>
  <summary>Details</summary>
Motivation: 研究有限项恒等式系统在n元字母表上最大解集的大小，将拟群、设计等组合结构的经典存在性问题转化为极值量化问题。

Method: 提出猜测数三明治定理，将项编码问题转化为图猜测数问题。通过显式规范化和多样化约简，将每个实例转化为具有猜测数α的规范有向依赖结构，然后使用熵和多拟阵方法计算α。

Result: 证明了最大编码大小满足log_n S_n(Γ) = α + o(1)，即S_n(Γ) = n^{α+o(1)}，其中α可通过熵和多拟阵方法计算或界定。

Conclusion: 该框架统一了极值组合学（Steiner型恒等式、自正交拉丁方）和信息流/网络编码约束中的问题，展示了分数指数和小存储/中继映射的五环实例。

Abstract: Term Coding asks: given a finite system of term identities $Γ$ in $v$ variables, how large can its solution set be on an $n$--element alphabet, when we are free to choose the interpretations of the function symbols? This turns familiar existence problems for quasigroups, designs, and related objects into quantitative extremal questions.
  We prove a guessing-number sandwich theorem that connects term coding to graph guessing numbers (graph entropy). After explicit normalisation and diversification reductions, every instance yields a canonical directed dependency structure with guessing number $α$ such that the maximum code size satisfies $\log_n \Sn(Γ)=α+o(1)$ (equivalently, $\Sn(Γ)=n^{α+o(1)}$), and $α$ can be bounded or computed using entropy and polymatroid methods.
  We illustrate the framework with examples from extremal combinatorics (Steiner-type identities, self-orthogonal Latin squares) and from information-flow / network-coding style constraints (including a five-cycle instance with fractional exponent and small storage/relay maps).

</details>


### [13] [Taming the Heavy Tail: Age-Optimal Preemption](https://arxiv.org/abs/2601.16624)
*Aimin Li,Yiğit İnce,Elif Uysal*

Main category: cs.IT

TL;DR: 本文研究连续时间联合采样与抢占问题，在一般服务时间分布下考虑采样和抢占惩罚，通过脉冲控制PDMP建模，提出高效策略迭代算法，在重尾服务时间下相比非抢占采样和零等待基线实现高达30倍的平均成本降低。


<details>
  <summary>Details</summary>
Motivation: 研究在一般服务时间分布下，结合采样和抢占惩罚的联合优化问题，旨在提升信息新鲜度（AoI）性能。传统方法需要平滑性假设，本文希望避免这些限制，同时探索抢占控制下延迟方差可能带来的意外优势。

Method: 将系统建模为脉冲控制的分段确定性马尔可夫过程（PDMP），通过动态规划原理推导耦合积分平均成本最优性方程，避免传统HJB-QVI所需的平滑性假设。利用繁忙阶段的关键不变性将动态压缩到一维繁忙起始边界，将抢占控制简化为最优停止问题。开发了带有重尾加速的高效策略迭代算法，采用混合（均匀/对数间隔）动作网格和远场线性闭合技术。

Result: 在Pareto和对数正态服务时间分布下的仿真显示，相比AoI最优的非抢占采样和零等待基线，实现了显著改进，在重尾机制下平均成本降低高达30倍。仿真还发现了一个反直觉的洞察：在抢占控制下，延迟方差（通常被视为不利因素）可能成为信息新鲜度的战略优势。

Conclusion: 本文提出的PDMP建模框架和高效算法有效解决了连续时间联合采样与抢占问题，避免了传统方法的平滑性限制。研究不仅展示了在重尾服务时间下的显著性能提升，还揭示了延迟方差在抢占控制下可能带来的意外战略价值，为信息新鲜度优化提供了新的理论视角和实用工具。

Abstract: This paper studies a continuous-time joint sampling-and-preemption problem, incorporating sampling and preemption penalties under general service-time distributions. We formulate the system as an impulse-controlled piecewise-deterministic Markov process (PDMP) and derive coupled integral average-cost optimality equations via the dynamic programming principle, thereby avoiding the smoothness assumptions typically required for an average-cost Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI) characterization. A key invariance in the busy phase collapses the dynamics onto a one-dimensional busy-start boundary, reducing preemption control to an optimal stopping problem. Building on this structure, we develop an efficient policy iteration algorithm with heavy-tail acceleration, employing a hybrid (uniform/log-spaced) action grid and a far-field linear closure. Simulations under Pareto and log-normal service times demonstrate substantial improvements over AoI-optimal non-preemptive sampling and zero-wait baselines, achieving up to a 30x reduction in average cost in heavy-tailed regimes. Finally, simulations uncover a counterintuitive insight: under preemption, delay variance, despite typically being a liability, can become a strategic advantage for information freshness.

</details>


### [14] [The Oval Strikes Back](https://arxiv.org/abs/2601.16628)
*Andrea Di Giusto,Alberto Ravagnani,Emina Soljanin*

Main category: cs.IT

TL;DR: 论文研究椭圆在射影平面中应用于分布式存储，特别是服务率区域问题，构建了具有大量小且不相交恢复集的非系统MDS矩阵，在某些参数下优于系统生成矩阵。


<details>
  <summary>Details</summary>
Motivation: 探索有限几何中的经典对象（椭圆）在现代编码理论中的应用，特别是解决分布式存储中的服务率区域问题，寻找比传统系统矩阵性能更好的MDS矩阵构造。

Method: 利用射影平面中线与椭圆的关联关系，构建一类非系统MDS矩阵，这些矩阵具有大量小且不相交的恢复集，并分析了其PIR特性和一步多数逻辑译码算法。

Result: 在某些参数选择下，所构造矩阵的服务率区域包含相同码的系统生成矩阵区域，表现出更好的服务性能，同时具有强纠错能力。

Conclusion: 椭圆作为有限几何中的经典对象，在现代编码理论中重新成为有用工具，特别是在分布式存储和服务率区域优化方面展现出优越性能。

Abstract: We investigate the applications of ovals in projective planes to distributed storage, with a focus on the Service Rate Region problem. Leveraging the incidence relations between lines and ovals, we describe a class of non-systematic MDS matrices with a large number of small and disjoint recovery sets. For certain parameter choices, the service-rate region of these matrices contains the region of a systematic generator matrix for the same code, yielding better service performance. We further apply our construction to analyze the PIR properties of the considered MDS matrices and present a one-step majority-logic decoding algorithm with strong error-correcting capability. These results highlight how ovals, a classical object in finite geometry, re-emerge as a useful tool in modern coding theory.

</details>


### [15] [Stable Source Coding](https://arxiv.org/abs/2601.16680)
*Zhenduo Wen,Amin Gohari*

Main category: cs.IT

TL;DR: 研究稳定无损信源编码的压缩率，推导稳定性参数下的可达速率信息论极限


<details>
  <summary>Details</summary>
Motivation: 传统随机分箱编码方法不稳定，因为随机映射导致相似信源序列可能被分配到完全不相关的箱索引中，需要研究稳定编码的性能极限

Method: 使用组合论证方法，推导稳定无损信源编码的可达速率作为稳定性参数的函数

Result: 得到了稳定编码的压缩率信息论极限，量化了稳定性要求对压缩性能的影响

Conclusion: 稳定信源编码在保证映射连续性的同时，其压缩性能存在理论极限，为实际编码系统设计提供了理论指导

Abstract: A source encoder is stable if a small change in the source sequence (e.g., changing a few symbols) results in a small (or bounded) change in the output codeword. By this definition, the common technique of random binning is unstable; because the mapping is random, two nearly identical source sequences can be assigned to completely unrelated bin indices. We study compression rates of stable lossless source codes. Using combinatorial arguments, we derive information-theoretic limits on the achievable rate as a function of the stability parameters.

</details>


### [16] [Adaptive Beam Alignment using Noisy Twenty Questions Estimation with Trained Questioner](https://arxiv.org/abs/2601.16799)
*Chunsong Sun,Lin Zhou*

Main category: cs.IT

TL;DR: 提出基于噪声二十问题估计框架的自适应波束对准算法，通过训练提问者解决传统方法延迟高、现有方法假设理想或缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 6G通信系统使用毫米波和MIMO技术，需要波束对准克服信号衰减。传统扇区搜索算法延迟高，现有自适应算法要么依赖理想假设不可行，要么使用黑盒神经网络缺乏可解释性。

Method: 提出基于噪声二十问题估计框架的自适应波束对准算法，训练提问者。使用两种方法：1) 通过导向矢量加权和将查询映射到波束赋形矢量；2) 使用多层全连接神经网络训练提问者，保持可解释性。

Result: 数值仿真表明提出的自适应波束对准算法有效，性能优于所有基准算法。

Conclusion: 提出的算法避免了理想假设，保持了可解释性，解决了现有方法的可行性问题和可解释性问题，在6G波束对准中表现出优越性能。

Abstract: The 6G communication systems use mmWave and MIMO technologies to achieve wide bandwidth and high throughout, leading to indispensable need for beam alignment to overcome severe signal attenuation. Traditional sector-search-based beam alignment algorithms rely on sequential sampling to identify the best sector, resulting in a significant latency burden on 6G communication systems. Recently proposed adaptive beam alignment algorithms based on the active learning framework address the problem, aiming to identify the optimal sector with the fewest possible samples under an identical sector partition. Nevertheless, these algorithms either lack feasibility (Chiu, Ronquillo and Javidi, JSAC 2019) due to ideal assumptions or lack interpretability (Sohrabi, Chen and Yu, JSAC 2021) due to the use of end-to-end black-box neural networks. To avoid ideal assumptions and maintain interpretability, we address all above problems by proposing an adaptive beam alignment algorithm using the framework of noisy twenty questions estimation with a trained questioner. Specifically, we use two methods for training the questioner to eliminate reliance on ideal assumptions. The first method maps queries of twenty questions estimation to beamforming vectors via weighted summation of steering vectors, as an initial attempt to address the feasibility problem encountered in prior pioneering study by Chiu, Ronquillo and Javidi (JSAC 2019). The second method uses multi-layer fully connected neural networks to achieve improved performance while only employing them to train the questioner, which can effectively mitigate the interpretability issues in prior study by Sohrabi, Chen and Yu (JSAC 2021). Furthermore, we provide numerical simulations to illustrate the effectiveness of our proposed adaptive beam alignment algorithms and demonstrate that our algorithms outperform all benchmark algorithms.

</details>


### [17] [Privacy-Resolution Tradeoff for Adaptive Noisy Twenty Questions Estimation](https://arxiv.org/abs/2601.16825)
*Chunsong Sun,Lin Zhou*

Main category: cs.IT

TL;DR: 本文研究带噪声的二十问估计中的隐私-分辨率权衡，提出两阶段隐私查询方案，分析其非渐近和二阶渐近性能，并在无噪声情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自适应查询在二十问估计中性能更好，但会引发隐私问题。先前研究主要关注无噪声情况，本文旨在将隐私分析扩展到更实际的带噪声场景。

Method: 提出两阶段隐私查询方案：第一阶段使用非自适应查询获取初步信息，第二阶段基于第一阶段结果设计自适应查询，同时考虑隐私保护。

Result: 分析了该方案的非渐近和二阶渐近性能，量化了隐私约束对分辨率的影响。在无噪声特例下，本方案性能优于COLT 2018和AISTATS 2021的现有方法。

Conclusion: 本文成功将隐私-分辨率权衡分析扩展到带噪声的二十问估计，提出的两阶段方案在保护隐私的同时实现了良好的估计性能，为实际应用提供了理论指导。

Abstract: We revisit noisy twenty questions estimation and study the privacy-resolution tradeoff for adaptive query procedures. Specifically, in twenty questions estimation, there are two players: an oracle and a questioner. The questioner aims to estimate target variables by posing queries to the oracle that knows the variables and using noisy responses to form reliable estimates. Typically, there are adaptive and non-adaptive query procedures. In adaptive querying, one designs the current query using previous queries and their noisy responses while in non-adaptive querying, all queries are posed simultaneously. Generally speaking, adaptive query procedures yield better performance. However, adaptive querying leads to privacy concerns, which were first studied by Tsitsiklis, Xu and Xu (COLT 2018) and by Xu, Xu and Yang (AISTATS 2021) for the noiseless case, where the oracle always provides correct answers to queries. In this paper, we generalize the above results to the more practical noisy case, by proposing a two-stage private query procedure, analyzing its non-asymptotic and second-order asymptotic achievable performance and discussing the impact of privacy concerns. Furthermore, when specialized to the noiseless case, our private query procedure achieves better performance than above-mentioned query procedures (COLT 2018, AISTATS 2021).

</details>


### [18] [Information Contraction under $(\varepsilon,δ)$-Differentially Private Mechanisms](https://arxiv.org/abs/2601.16845)
*Theshani Nuradha,Ian George,Christoph Hirche*

Main category: cs.IT

TL;DR: 论文为(ε,δ)-局部差分隐私机制推导了曲棍球散度和f-散度的线性和非线性强数据处理不等式，改进了先前仅适用于(ε,0)-LDP的界限。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数信息度量收缩特征（包括全变差距离、曲棍球散度和f-散度）仅适用于(ε,0)-局部差分隐私机制，缺乏对更一般的(ε,δ)-LDP机制（δ≠0）的分析。

Method: 为曲棍球散度和f-散度推导了线性和非线性强数据处理不等式，这些不等式对所有(ε,δ)-局部差分隐私机制都有效，即使δ≠0。

Result: 得到的SDPI结果要么推广了先前已知的界限，要么改进了这些可区分性度量的收缩界限，为(ε,δ)-LDP机制提供了更全面的分析工具。

Conclusion: 该工作填补了(ε,δ)-局部差分隐私机制下信息度量收缩分析的空白，为更广泛的隐私保护场景提供了理论工具。

Abstract: The distinguishability quantified by information measures after being processed by a private mechanism has been a useful tool in studying various statistical and operational tasks while ensuring privacy. To this end, standard data-processing inequalities and strong data-processing inequalities (SDPI) are employed. Most of the previously known and even tight characterizations of contraction of information measures, including total variation distance, hockey-stick divergences, and $f$-divergences, are applicable for $(\varepsilon,0)$-local differential private (LDP) mechanisms. In this work, we derive both linear and non-linear strong data-processing inequalities for hockey-stick divergence and $f$-divergences that are valid for all $(\varepsilon,δ)$-LDP mechanisms even when $δ\neq 0$. Our results either generalize or improve the previously known bounds on the contraction of these distinguishability measures.

</details>


### [19] [Perfect Privacy and Strong Stationary Times for Markovian Sources](https://arxiv.org/abs/2601.16857)
*Fangwei Ye,Zonghong Liu,Parimal Parag,Salim El Rouayheb*

Main category: cs.IT

TL;DR: 研究在完美信息论隐私约束下共享相关数据的问题，提出基于擦除的机制，在保护初始状态的同时最大化共享数据量，证明了两种机制都能达到最优失真且仅需擦除常数个数据点。


<details>
  <summary>Details</summary>
Motivation: 研究如何在完美信息论隐私约束下共享相关数据，特别关注保护数据的初始状态，同时最大化共享的数据量。这个问题在隐私保护的数据共享场景中具有重要意义。

Method: 采用擦除（删除）机制，数据要么被保留要么被完全删除。研究基于窗口的擦除方案，证明擦除数据直到强平稳时间可以保护隐私。进一步研究最优顺序擦除机制，并证明其具有等效的窗口解释。

Result: 建立了完美隐私与基于窗口的擦除方案之间的联系，证明了两种机制都能达到最优失真，并且只需要擦除常数个数据点（与数据长度N无关）。

Conclusion: 在完美信息论隐私约束下，通过适当的擦除机制可以有效保护相关数据的初始状态，同时实现高效的数据共享，仅需擦除常数个数据点即可达到最优性能。

Abstract: We consider the problem of sharing correlated data under a perfect information-theoretic privacy constraint. We focus on redaction (erasure) mechanisms, in which data are either withheld or released unchanged, and measure utility by the average cardinality of the released set, equivalently, the expected Hamming distortion. Assuming the data are generated by a finite time-homogeneous Markov chain, we study the protection of the initial state while maximizing the amount of shared data. We establish a connection between perfect privacy and window-based redaction schemes, showing that erasing data up to a strong stationary time preserves privacy under suitable conditions. We further study an optimal sequential redaction mechanism and prove that it admits an equivalent window interpretation. Interestingly, we show that both mechanisms achieve the optimal distortion while redacting only a constant average number of data points, independent of the data length~$N$.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [20] [Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)](https://arxiv.org/abs/2601.16409)
*Yeasir Rayhan,Walid G. Aref*

Main category: cs.DB

TL;DR: 该论文探讨了AI4DB（数据库系统人工智能）研究的现状，提出了构建生成式数据库代理（Gen-DBA）作为实现数据库系统"Move 37时刻"的路径，并阐述了构建Gen-DBA的关键要素。


<details>
  <summary>Details</summary>
Motivation: 受围棋AI的"Move 37"突破启发，作者认为自然语言处理、计算机视觉和机器人领域已通过大型基础模型取得类似突破，但数据库系统AI研究尚未达到同等里程碑。需要探索如何让数据库系统也实现类似的创造性突破。

Method: 提出构建生成式数据库代理（Gen-DBA）的框架，包括：Transformer骨干架构、硬件基础的分词机制、两阶段目标导向的下一个令牌预测训练范式，以及生成式推理过程。

Result: 论文提出了Gen-DBA的概念框架和构建方法，为数据库系统实现类似"Move 37"的创造性突破提供了具体的技术路径和实现方案。

Conclusion: 通过构建生成式数据库代理，数据库系统有望实现自己的"Move 37时刻"，将生成式推理和创造性引入数据库学习任务，推动AI4DB研究进入新阶段。

Abstract: Move\,37 marks one of the major breakthroughs in AI in terms of its ability to surpass human expertise and discover novel strategies beyond the traditional game play in the strategic two-player board game of Go. The domains of Natural Language Processing, Computer Vision, and Robotics have also undergone a similar phenomenon through the advent of large foundational models in the form of Large Language Models (LLMs), Vision Language Models (VLMs) and Vision Language Action models (VLAs), respectively. In this paper, we investigate the current state of Artificial Intelligence for Database Systems research (AI4DB), and assess how far AI4DB systems are from achieving their own Move\,37 moment. We envision a Generative Database Agent (Gen-DBA, for short) as the pathway to achieving Move\,37 for database systems that will bring generative reasoning and creativity into the realm of database learning tasks. This vision paper explores this direction by presenting the recipe for building Gen-DBA that encompasses but is not limited to a Transformer backbone, a hardware-grounded tokenization mechanism, a two-stage Goal-Directed Next Token Prediction training paradigm, and a generative inference process.

</details>


### [21] [iPDB -- Optimizing SQL Queries with ML and LLM Predicates](https://arxiv.org/abs/2601.16432)
*Udesh Kumarasinghe,Tyler Liu,Chunwei Liu,Walid G. Aref*

Main category: cs.DB

TL;DR: iPDB是一个支持在关系数据库中直接进行机器学习和LLM推理的系统，通过扩展SQL语法实现语义查询，避免了数据迁移的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统SQL和关系数据库系统在处理需要利用学习模型的工作负载时存在不兼容或效率低下的问题，导致复杂的工程实现和多次数据迁移操作。

Method: iPDB通过扩展SQL语法，支持LLM和ML调用作为语义投影、谓词执行语义选择和连接，以及语义分组操作。系统包含新颖的关系预测算子和语义查询优化技术。

Result: iPDB能够高效执行语义SQL查询，性能优于现有最先进技术。

Conclusion: iPDB为关系数据库系统提供了在数据库内直接进行机器学习和LLM推理的能力，简化了应用开发流程并提高了效率。

Abstract: Structured Query Language (SQL) has remained the standard query language for databases. SQL is highly optimized for processing structured data laid out in relations. Meanwhile, in the present application development landscape, it is highly desirable to utilize the power of learned models to perform complex tasks. Large language models (LLMs) have been shown to understand and extract information from unstructured textual data. However, SQL as a query language and accompanying relational database systems are either incompatible or inefficient for workloads that require leveraging learned models. This results in complex engineering and multiple data migration operations that move data between the data sources and the model inference platform. In this paper, we present iPDB, a relational system that supports in-database machine learning (ML) and large language model (LLM) inferencing using extended SQL syntax. In iPDB, LLMs and ML calls can function as semantic projects, as predicates to perform semantic selects and semantic joins, or for semantic grouping in group-by clauses. iPDB has a novel relational predict operator and semantic query optimizations that enable users to write and efficiently execute semantic SQL queries, outperforming the state-of-the-art.

</details>


### [22] [A Scalable Transaction Management Framework for Consistent Document-Oriented NoSQL Databases](https://arxiv.org/abs/2601.16490)
*Adam A. E. Alflahi,Mohammed A. Y. Mohammed,Abdallah Alsammani*

Main category: cs.DB

TL;DR: 提出一个四阶段事务管理框架，用于文档型NoSQL数据库，结合事务生命周期管理、操作分类、冲突检测和自适应锁策略，在保证冲突可串行化的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: NoSQL数据库虽然具有可扩展性和模式灵活性，但通常依赖最终一致性模型，限制了可靠的事务处理能力。需要一种既能保证数据完整性又不损害可扩展性的事务管理机制。

Method: 提出四阶段事务管理框架：1) 事务生命周期管理；2) 操作分类；3) 预执行冲突检测；4) 自适应锁策略（含基于超时的死锁预防）。以MongoDB为参考平台，使用形式化正确性分析验证冲突可串行化。

Result: 实验评估显示：事务中止率从8.3%降至4.7%；死锁完全消除；延迟方差降低34.2%；高并发下吞吐量提升6.3%-18.4%（特别是读写修改工作负载）；9节点集群吞吐量提升15.2%，中止率降低53%。

Conclusion: 精心设计的一致性机制可以显著改善NoSQL系统的数据完整性，同时不损害可扩展性。该框架在一致性保证和性能开销之间取得了良好平衡，优于MongoDB原生事务、CockroachDB和TiDB。

Abstract: NoSQL databases are widely used in modern applications due to their scalability and schema flexibility, yet they often rely on eventual consistency models that limit reliable transaction processing. This study proposes a four-stage transaction management framework for document-oriented NoSQL databases, with MongoDB as the reference platform. The framework combines transaction lifecycle management, operation classification, pre-execution conflict detection, and an adaptive locking strategy with timeout-based deadlock prevention. Formal correctness analysis shows that the proposed approach guarantees conflict serializability under defined conditions. An experimental evaluation using the Yahoo Cloud Serving Benchmark (YCSB) workloads A, B, and F, with concurrency levels ranging from 1 to 100 clients, demonstrates a reduction in transaction abort rates from 8.3% to 4.7%, the elimination of observed deadlocks, and a 34.2% decrease in latency variance. Throughput improvements ranging from 6.3% to 18.4% are observed under high concurrency, particularly for read-modify-write workloads. Distributed experiments on clusters of up to 9 nodes confirm scalability, achieving 15.2% higher throughput and 53% lower abort rates than baseline systems. Comparisons with MongoDB's native transactions, CockroachDB, and TiDB indicate that the proposed framework strikes a good balance between consistency guarantees and performance overhead. Sensitivity analysis identifies optimal parameter settings, including a lock timeout of 100 ms, an initial backoff of 10 ms, and a maximum backoff of 500 ms. These results show that carefully designed consistency mechanisms can significantly improve data integrity in NoSQL systems without undermining scalability.

</details>


### [23] [A Categorical Approach to Semantic Interoperability across Building Lifecycle](https://arxiv.org/abs/2601.16663)
*Zoltan Nagy,Ryan Wisnesky,Kevin Carlson,Eswaran Subrahmanian,Gioele Zardini*

Main category: cs.DB

TL;DR: 论文提出使用范畴论作为建筑数据集成的基础数学框架，解决现有40多种元数据模式碎片化问题，实现O(n)复杂度的系统集成，而非传统点对点映射的O(n²)复杂度。


<details>
  <summary>Details</summary>
Motivation: 建筑生命周期产生异构数据，但集成这些数据仍是未解决的挑战。尽管有30年标准化努力，但出现了40多种元数据模式，碎片化反而加剧。现有方法要么采用点对点映射（复杂度O(n²)），要么采用笨重的通用本体，缺乏跨异构建筑数据的结构保持转换的数学基础。

Method: 使用范畴论作为数学基础，将建筑本体形式化为一级理论。在Categorical Query Language (CQL)中实现两个概念验证：1) 从IFC设计数据生成BRICK模型；2) IFC、BRICK和RealEstateCore的三向集成，仅需两个显式映射即可通过范畴组合自动获得第三个。

Result: 证明范畴方法在建筑数据集成中的可行性：实现O(n)规范复杂度（而非O(n²)），将属性集作为一级模式实体，提供自动双向迁移，支持跨本体查询。通过正确性构造方法确保集成可靠性。

Conclusion: 范畴论为建筑数据集成提供了数学基础，使系统集成复杂度从O(n²)降至O(n)。这为建筑应用生态系统开辟了道路，类似于智能手机平台的可靠组件集成，数学基础确保了集成的可靠性。

Abstract: Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or universal ontologies that become unwieldy monoliths. The fundamental gap is the absence of mathematical foundations for structure-preserving transformations across heterogeneous building data. Here we show that category theory provides these foundations, enabling systematic data integration with $O(n)$ specification complexity for $n$ ontologies. We formalize building ontologies as first-order theories and demonstrate two proof-of-concept implementations in Categorical Query Language (CQL): 1) generating BRICK models from IFC design data at commissioning, and 2) three-way integration of IFC, BRICK, and RealEstateCore where only two explicit mappings yield the third automatically through categorical composition. Our correct-by-construction approach treats property sets as first-class schema entities and provides automated bidirectional migrations, and enables cross-ontology queries. These results establish feasibility of categorical methods for building data integration and suggest a path toward an app ecosystem for buildings, where mathematical foundations enable reliable component integration analogous to smartphone platforms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [24] [LLM-based Semantic Search for Conversational Queries in E-commerce](https://arxiv.org/abs/2601.16492)
*Emad Siddiqui,Venkatesh Terikuti,Xuan Lu*

Main category: cs.IR

TL;DR: 提出基于LLM的语义搜索框架，通过领域特定嵌入和结构化过滤器结合，有效捕捉对话式查询的用户意图，在真实数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统电商平台的搜索系统主要针对关键词查询优化，难以有效处理日益增长的对话式用户查询，需要能够更好理解用户意图的语义搜索方案。

Method: 1. 使用LLM生成合成数据解决标注数据不足问题；2. 微调两个模型：嵌入模型（将语义相似产品在表示空间中靠近）和生成模型（将自然语言查询转换为结构化约束）；3. 结合基于相似性的检索和基于约束的过滤。

Result: 在真实世界数据集上，相比基线方法，该框架在各种设置下都实现了较强的精确率和召回率。

Conclusion: 提出的LLM语义搜索框架能够有效处理对话式查询，通过结合语义相似性和结构化约束，显著提升了电商搜索系统的性能。

Abstract: Conversational user queries are increasingly challenging traditional e-commerce platforms, whose search systems are typically optimized for keyword-based queries. We present an LLM-based semantic search framework that effectively captures user intent from conversational queries by combining domain-specific embeddings with structured filters. To address the challenge of limited labeled data, we generate synthetic data using LLMs to guide the fine-tuning of two models: an embedding model that positions semantically similar products close together in the representation space, and a generative model for converting natural language queries into structured constraints. By combining similarity-based retrieval with constraint-based filtering, our framework achieves strong precision and recall across various settings compared to baseline approaches on a real-world dataset.

</details>


### [25] [PRISM: Purified Representation and Integrated Semantic Modeling for Generative Sequential Recommendation](https://arxiv.org/abs/2601.16556)
*Dengzhao Fang,Jingtong Gao,Yu Li,Xiangyu Zhao,Yi Chang*

Main category: cs.IR

TL;DR: PRISM是一个新的生成式序列推荐框架，通过纯化语义量化器和集成语义建模解决现有方法中的语义标记不纯和生成信息损失问题，在稀疏场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有生成式序列推荐框架面临两个关键限制：1）不纯且不稳定的语义标记化，量化方法难以处理交互噪声和码本崩溃，导致语义ID具有模糊的区分性；2）有损且弱结构化的生成，仅依赖粗粒度离散标记会引入信息损失并忽略项目的层次逻辑。

Method: 提出PRISM框架，包含两个核心组件：1）纯化语义量化器，通过自适应协同去噪和层次语义锚定机制构建鲁棒码本；2）集成语义推荐器，通过动态语义集成机制整合细粒度语义，并通过语义结构对齐目标强制逻辑有效性。

Result: PRISM在四个真实世界数据集上持续超越最先进的基线方法，表现出显著的性能提升，特别是在高稀疏性场景下。

Conclusion: PRISM通过解决语义标记化和生成过程中的关键限制，为生成式序列推荐提供了一个更有效和鲁棒的框架，特别是在数据稀疏的情况下。

Abstract: Generative Sequential Recommendation (GSR) has emerged as a promising paradigm, reframing recommendation as an autoregressive sequence generation task over discrete Semantic IDs (SIDs), typically derived via codebook-based quantization. Despite its great potential in unifying retrieval and ranking, existing GSR frameworks still face two critical limitations: (1) impure and unstable semantic tokenization, where quantization methods struggle with interaction noise and codebook collapse, resulting in SIDs with ambiguous discrimination; and (2) lossy and weakly structured generation, where reliance solely on coarse-grained discrete tokens inevitably introduces information loss and neglects items' hierarchical logic. To address these issues, we propose a novel generative recommendation framework, PRISM, with Purified Representation and Integrated Semantic Modeling. Specifically, to ensure high-quality tokenization, we design a Purified Semantic Quantizer that constructs a robust codebook via adaptive collaborative denoising and hierarchical semantic anchoring mechanisms. To compensate for information loss during quantization, we further propose an Integrated Semantic Recommender, which incorporates a dynamic semantic integration mechanism to integrate fine-grained semantics and enforces logical validity through a semantic structure alignment objective. PRISM consistently outperforms state-of-the-art baselines across four real-world datasets, demonstrating substantial performance gains, particularly in high-sparsity scenarios.

</details>


### [26] [LLM-powered Real-time Patent Citation Recommendation for Financial Technologies](https://arxiv.org/abs/2601.16775)
*Tianang Deng,Yu Deng,Tianchen Gao,Yonghong Hu,Rui Pan*

Main category: cs.IR

TL;DR: 提出针对金融专利的实时引文推荐框架，使用LLM嵌入、近似最近邻搜索和增量索引，在动态专利系统中实现高效准确的推荐。


<details>
  <summary>Details</summary>
Motivation: 金融创新快速发展导致专利申请激增，传统静态索引或定期重训练方法难以及时发现现有技术，无法适应金融专利的动态变化特性。

Method: 三阶段推荐流程：1) 使用LLM嵌入表示专利摘要语义；2) 高效近似最近邻搜索构建候选集；3) 基于语义相似度排序生成top-k推荐。采用基于HNSW图的增量索引策略，支持新专利实时添加。

Result: 在428,843个中国金融专利数据集上验证，增量更新在提高召回率的同时大幅降低计算成本，优于传统文本基线和替代最近邻检索方法。

Conclusion: 提出的实时引文推荐框架能有效应对金融专利的动态变化，为快速发展的金融技术领域提供及时、全面的现有技术发现解决方案。

Abstract: Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches.

</details>


### [27] [PI2I: A Personalized Item-Based Collaborative Filtering Retrieval Framework](https://arxiv.org/abs/2601.16815)
*Shaoqing Wang,Yingcai Ma,Kairui Fu,Ziyang Wang,Dunxian Huang,Yuliang Yan,Jian Wu*

Main category: cs.IR

TL;DR: 提出PI2I两阶段检索框架，通过放宽截断阈值和引入交互式评分模型，提升推荐系统的个性化能力，在淘宝"猜你喜欢"中实现交易率提升1.05%


<details>
  <summary>Details</summary>
Motivation: 传统方法（如item-to-item协同过滤和双塔模型）在捕捉复杂用户-物品交互方面存在不足，主要受限于统一的截断策略和用户-物品交叉建模不够充分

Method: 提出PI2I两阶段框架：1) 索引构建阶段：放宽截断阈值最大化命中率，保留更多潜在相关物品；2) 个性化检索阶段：引入交互式评分模型替代内积计算，基于触发-目标关系构建负样本

Result: 离线实验在大规模真实数据集上优于传统协同过滤方法，与双塔模型相当；在淘宝"猜你喜欢"部署后在线交易率提升1.05%；发布了包含1.3亿用户交互的公开数据集

Conclusion: PI2I通过两阶段个性化检索框架有效解决了传统方法的局限性，显著提升了推荐性能，同时发布的公开数据集可为研究社区提供有价值的基准

Abstract: Efficiently selecting relevant content from vast candidate pools is a critical challenge in modern recommender systems. Traditional methods, such as item-to-item collaborative filtering (CF) and two-tower models, often fall short in capturing the complex user-item interactions due to uniform truncation strategies and overdue user-item crossing. To address these limitations, we propose Personalized Item-to-Item (PI2I), a novel two-stage retrieval framework that enhances the personalization capabilities of CF. In the first Indexer Building Stage (IBS), we optimize the retrieval pool by relaxing truncation thresholds to maximize Hit Rate, thereby temporarily retaining more items users might be interested in. In the second Personalized Retrieval Stage (PRS), we introduce an interactive scoring model to overcome the limitations of inner product calculations, allowing for richer modeling of intricate user-item interactions. Additionally, we construct negative samples based on the trigger-target (item-to-item) relationship, ensuring consistency between offline training and online inference. Offline experiments on large-scale real-world datasets demonstrate that PI2I outperforms traditional CF methods and rivals Two-Tower models. Deployed in the "Guess You Like" section on Taobao, PI2I achieved a 1.05% increase in online transaction rates. In addition, we have released a large-scale recommendation dataset collected from Taobao, containing 130 million real-world user interactions used in the experiments of this paper. The dataset is publicly available at https://huggingface.co/datasets/PI2I/PI2I, which could serve as a valuable benchmark for the research community.

</details>


### [28] [Navigating the Shift: A Comparative Analysis of Web Search and Generative AI Response Generation](https://arxiv.org/abs/2601.16858)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 该研究通过大规模实证分析，揭示了生成式AI与传统搜索引擎在信息源、内容类型、查询意图和信息新鲜度等方面的显著差异，并探讨了LLM预训练知识库对实时网络搜索的影响，为新兴的答案引擎优化(AEO)提供了关键见解。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI成为主要信息来源，其与传统网络搜索的范式转变需要系统性的量化研究。研究者旨在揭示这两种信息生态系统在多个维度上的根本差异，以理解生成式AI作为信息源的特性和影响。

Method: 采用大规模实证研究方法，对比分析Google搜索与领先生成式AI服务的结果。从多个维度进行量化：咨询的源域、域类型（如付费媒体vs自有媒体、社交媒体）、查询意图和信息新鲜度。同时分析LLM预训练作为关键因素如何塑造这些差异，以及预训练知识库与实时网络搜索的交互影响。

Result: 研究发现AI生成的答案与网络搜索结果在多个维度上存在显著差异：1）咨询的源域不同；2）域类型分布不同（如付费媒体、自有媒体、社交媒体的比例差异）；3）查询意图处理方式不同；4）信息新鲜度存在差异。LLM预训练知识库是塑造这些差异的关键因素，当启用实时网络搜索时，预训练知识会与实时信息交互并产生影响。

Conclusion: 研究揭示了两种信息生态系统的不同机制，为新兴的答案引擎优化(AEO)领域提供了关键观察，并对比了传统搜索引擎优化(SEO)。这些发现对于理解生成式AI作为信息源的特性、局限性和优化策略具有重要意义。

Abstract: The rise of generative AI as a primary information source presents a paradigm shift from traditional web search. This paper presents a large-scale empirical study quantifying the fundamental differences between the results returned by Google Search and leading generative AI services. We analyze multiple dimensions, demonstrating that AI-generated answers and web search results diverge significantly in their consulted source domains, the typology of these domains (e.g., earned media vs. owned, social), query intent and the freshness of the information provided. We then investigate the role of LLM pre-training as a key factor shaping these differences, analyzing how this intrinsic knowledge base interacts with and influences real-time web search when enabled. Our findings reveal the distinct mechanics of these two information ecosystems, leading to critical observations on the emergent field of Answer Engine Optimization (AEO) and its contrast with traditional Search Engine Optimization (SEO).

</details>


### [29] [From Atom to Community: Structured and Evolving Agent Memory for User Behavior Modeling](https://arxiv.org/abs/2601.16872)
*Yuxin Liao,Le Wu,Min Hou,Yu Wang,Han Wu,Meng Wang*

Main category: cs.IR

TL;DR: STEAM是一个结构化、演化的智能体记忆框架，通过将用户偏好分解为原子记忆单元，组织跨用户记忆社区，并采用自适应演化机制，显著提升了推荐准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体记忆机制主要针对文本对话设计，难以有效建模非文本行为（如点击）。传统方法使用单一非结构化摘要，存在三个主要问题：1）用户多面兴趣被混淆；2）简单覆盖更新导致遗忘；3）稀疏个体交互缺乏协同信号。

Method: STEAM框架包含三个核心组件：1）将用户偏好分解为原子记忆单元，每个单元捕获一个独立兴趣维度并与观察到的行为显式链接；2）将跨用户的相似记忆组织成社区，并生成原型记忆进行信号传播；3）自适应演化机制，包括用于精炼记忆的巩固机制和用于捕捉新兴兴趣的形成机制。

Result: 在三个真实世界数据集上的实验表明，STEAM在推荐准确性、模拟保真度和多样性方面显著优于最先进的基线方法。

Conclusion: STEAM通过结构化组织和自适应演化重新构想智能体记忆机制，有效解决了用户多面兴趣混淆、偏好演化遗忘和稀疏交互协同信号不足的问题，为非文本行为建模提供了更优的解决方案。

Abstract: User behavior modeling lies at the heart of personalized applications like recommender systems. With LLM-based agents, user preference representation has evolved from latent embeddings to semantic memory. While existing memory mechanisms show promise in textual dialogues, modeling non-textual behaviors remains challenging, as preferences must be inferred from implicit signals like clicks without ground truth supervision. Current approaches rely on a single unstructured summary, updated through simple overwriting. However, this is suboptimal: users exhibit multi-faceted interests that get conflated, preferences evolve yet naive overwriting causes forgetting, and sparse individual interactions necessitate collaborative signals. We present STEAM (\textit{\textbf{ST}ructured and \textbf{E}volving \textbf{A}gent \textbf{M}emory}), a novel framework that reimagines how agent memory is organized and updated. STEAM decomposes preferences into atomic memory units, each capturing a distinct interest dimension with explicit links to observed behaviors. To exploit collaborative patterns, STEAM organizes similar memories across users into communities and generates prototype memories for signal propagation. The framework further incorporates adaptive evolution mechanisms, including consolidation for refining memories and formation for capturing emerging interests. Experiments on three real-world datasets demonstrate that STEAM substantially outperforms state-of-the-art baselines in recommendation accuracy, simulation fidelity, and diversity.

</details>


### [30] [Explaining Group Recommendations via Counterfactuals](https://arxiv.org/abs/2601.16882)
*Maria Stratigi,Nikos Bikakis*

Main category: cs.IR

TL;DR: 提出群体反事实解释框架，通过移除特定历史交互来揭示推荐变化，平衡效用与公平性


<details>
  <summary>Details</summary>
Motivation: 群体推荐系统缺乏透明度，现有解释方法主要针对个体，难以处理群体中多个偏好的交互问题

Method: 提出群体反事实解释框架，形式化概念，引入群体效用和公平性度量，设计启发式算法（帕累托过滤、生长剪枝策略）

Result: 实验显示权衡关系：低成本方法产生更大但更不公平的解释，其他方法以更高成本获得简洁平衡的结果；帕累托过滤在稀疏设置中效率显著提升

Conclusion: 群体反事实解释框架能有效揭示推荐变化，不同算法在成本、解释大小和公平性间提供权衡选择

Abstract: Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [31] [Game-to-Real Gap: Quantifying the Effect of Model Misspecification in Network Games](https://arxiv.org/abs/2601.16367)
*Bryce L. Ferguson,Chinmay Maheshwari,Manxi Wu,Shankar Sastry*

Main category: cs.GT

TL;DR: 论文提出"game-to-real gap"新指标，量化多智能体系统中模型误设的影响，聚焦二次网络博弈，证明误设可导致任意大差距，并开发新的网络中心性度量方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，不同智能体可能依赖不同的博弈论模型设计策略，当这些异构模型交互时，实际结果可能与每个智能体基于自身局部模型预期的结果存在显著偏差，需要量化这种模型误设的影响。

Method: 提出"game-to-real gap"新指标，定义为智能体在多智能体环境中实际获得的效用与其基于自身博弈模型预期的效用之差。聚焦二次网络博弈，分析外部冲击或玩家交互网络误设的影响，开发新的网络中心性度量方法进行精确评估。

Result: 证明在二次网络博弈中，外部冲击或交互网络的误设可导致任意大的game-to-real gap。开发的新网络中心性度量能精确评估此差距，而标准网络中心性度量无法捕捉模型误设效应。数值实验显示现有中心性度量对模型误设影响的理解可能反直觉。

Conclusion: 模型误设在多智能体系统中影响显著，需要新的结构度量来量化其影响。提出的game-to-real gap和新网络中心性度量为此提供了有效工具，揭示了标准网络中心性度量的局限性。

Abstract: Game-theoretic models and solution concepts provide rigorous tools for predicting collective behavior in multi-agent systems. In practice, however, different agents may rely on different game-theoretic models to design their strategies. As a result, when these heterogeneous models interact, the realized outcome can deviate substantially from the outcome each agent expects based on its own local model. In this work, we introduce the game-to-real gap, a new metric that quantifies the impact of such model misspecification in multi-agent environments. The game-to-real gap is defined as the difference between the utility an agent actually obtains in the multi-agent environment (where other agents may have misspecified models) and the utility it expects under its own game model. Focusing on quadratic network games, we show that misspecifications in either (i) the external shock or (ii) the player interaction network can lead to arbitrarily large game-to-real gaps. We further develop novel network centrality measures that allow exact evaluation of this gap in quadratic network games. Our analysis reveals that standard network centrality measures fail to capture the effects of model misspecification, underscoring the need for new structural metrics that account for this limitation. Finally, through illustrative numerical experiments, we show that existing centrality measures in network games may provide a counterintuitive understanding of the impact of model misspecification.

</details>


### [32] [Tight Regret Bounds for Bilateral Trade under Semi Feedback](https://arxiv.org/abs/2601.16412)
*Yaonan Jin*

Main category: cs.GT

TL;DR: 解决了固定价格双边交易中GBB半反馈机制在对抗性价值下的紧致遗憾界问题，提出了与下界匹配的$\widetilde{O}(T^{2/3})$遗憾机制


<details>
  <summary>Details</summary>
Motivation: 虽然固定价格双边交易的遗憾最小化研究已取得显著进展，但GBB半反馈机制在对抗性价值下的紧致遗憾界仍是一个未解决的问题

Method: 设计了一个新的$\widetilde{O}(T^{2/3})$遗憾机制，匹配了先前工作中证明的$Ω(T^{2/3})$下界

Result: 成功解决了该开放问题，达到了与下界匹配的遗憾界（忽略对数因子）

Conclusion: 该工作填补了固定价格双边交易遗憾最小化研究的最后空白，为GBB半反馈机制在对抗性价值下提供了最优的遗憾保证

Abstract: The study of \textit{regret minimization in fixed-price bilateral trade} has received considerable attention in recent research. Previous works [CCC+24a, CCC+24b, AFF24, BCCF24, CJLZ25, LCM25a, GDFS25] have acquired a thorough understanding of the problem, except for determining the tight regret bound for GBB semi-feedback fixed-price mechanisms under adversarial values.
  In this paper, we resolve this open question by devising an $\widetilde{O}(T^{2 / 3})$-regret mechanism, matching the $Ω(T^{2 / 3})$ lower bound from [CJLZ25] up to polylogarithmic factors.

</details>


### [33] [Anonymous Pricing in Large Markets](https://arxiv.org/abs/2601.16488)
*Yaonan Jin,Yingkai Li*

Main category: cs.GT

TL;DR: 在大型市场中，匿名定价（统一定价）对多单位商品销售可以达到接近最优机制的近似比，表明价格歧视的收益有限


<details>
  <summary>Details</summary>
Motivation: 研究在多单位商品销售中，当卖家向异质买家提供k个相同商品时，匿名定价（统一定价）相对于最优机制的效率损失。虽然理论上匿名定价在最坏情况下可能比最优机制差Θ(log k)倍，但作者想探究在大型市场中这种悲观情况是否会消失。

Method: 研究卖家向异质单位需求买家提供k个相同单位商品的情况。在(准)正则性假设下，分析匿名定价与最优机制的性能比较。特别关注大型市场，即没有单个买家占据最优收入的显著份额的情况。

Result: 在大型市场中，匿名定价可以达到2+O(1/√k)的近似比。最坏情况比率在k=1时约为2.47，随着k增长收敛到2。这表明在大型市场中，三级价格歧视带来的收益是有限的。

Conclusion: 在大型多单位商品市场中，匿名定价（统一定价）相对于最优机制的效率损失有限，价格歧视的收益相对温和。这为实践中使用简单定价策略提供了理论支持。

Abstract: We study revenue maximization when a seller offers $k$ identical units to ex ante heterogeneous, unit-demand buyers. While anonymous pricing can be $Θ(\log k)$ worse than optimal in general multi-unit environments, we show that this pessimism disappears in large markets, where no single buyer accounts for a non-negligible share of optimal revenue. Under (quasi-)regularity, anonymous pricing achieves a $2+O(1/\sqrt{k})$ approximation to the optimal mechanism; the worst-case ratio is maximized at about $2.47$ when $k=1$ and converges to $2$ as $k$ grows. This indicates that the gains from third-degree price discrimination are mild in large markets.

</details>


### [34] [Participatory Budgeting Project Strength via Candidate Control](https://arxiv.org/abs/2601.16511)
*Piotr Faliszewski,Łukasz Janeczko,Dušan Knop,Jan Pokorný,Šimon Schierreich,Mateusz Słuszniak,Krzysztof Sornat*

Main category: cs.GT

TL;DR: 研究参与式预算选举中候选人控制的复杂性，发现多数投票规则下的控制问题是NP难的，但存在多项式时间可解的自然情况


<details>
  <summary>Details</summary>
Motivation: 研究参与式预算选举中的候选人控制问题，包括建设性控制（确保特定候选人获胜）和破坏性控制（阻止特定候选人获胜），旨在理解这些控制问题的计算复杂性

Method: 分析多种参与式预算投票规则（包括Phragmén、Method of Equal Shares、GreedyAV等）下候选人控制问题的计算复杂性，区分添加候选人和删除候选人两种控制方式

Result: 对于多数投票规则（如Phragmén和Method of Equal Shares），候选人控制问题是NP难的；但对于GreedyAV规则和项目成本以一元编码的情况，存在多项式时间算法

Conclusion: 参与式预算选举中的候选人控制问题通常计算困难，但存在可高效解决的自然情况；删除候选人的控制方法可作为评估初始失败项目性能的有用工具

Abstract: We study the complexity of candidate control in participatory budgeting elections. The goal of constructive candidate control is to ensure that a given candidate wins by either adding or deleting candidates from the election (in the destructive setting, the goal is to prevent a given candidate from winning). We show that such control problems are NP-hard to solve for many participatory budgeting voting rules, including Phragmén and Method of Equal Shares, but there are natural cases with polynomial-time algorithms (e.g., for the GreedyAV rule and projects with costs encoded in unary). We also argue that control by deleting candidates is a useful tool for assessing the performance (or, strength) of initially losing projects, and we support this view with experiments.

</details>


### [35] [On Best-of-Both-Worlds Fairness via Sum-of-Variances Minimization](https://arxiv.org/abs/2601.16579)
*Moshe Babaioff,Yuval Grofman*

Main category: cs.GT

TL;DR: 研究公平分配不可分割物品问题，探索通过最小化方差来同时实现事前比例公平和事后公平的可行性


<details>
  <summary>Details</summary>
Motivation: 传统随机分配方法虽然能实现事前比例公平，但事后分配可能非常不公平。研究者希望找到既能保证事前比例公平，又能确保事后分配具有公平性（如EF1或MMS保证）的随机分配方法。

Method: 采用最小化方差方法：在满足事前比例公平约束下，最小化代理人价值的总方差。这种方法在存在确定性比例分配时会自动输出该分配。

Result: 当估值相同时，该方法能保证事后公平性：所有分配都是EFX，且保证每个代理人至少获得4/7的最大最小份额。但当估值不同时，即使在最简单的2代理人2物品情况下，该方法可能将所有物品分配给同一代理人，无法保证EF1或任何常数比例的MMS。

Conclusion: 最小化方差方法在估值相同情况下有效，但在估值不同时完全失败。其他基于方差的优化目标也存在类似负面结果，表明这种方法在异质估值环境下不可行。

Abstract: We consider the problem of fairly allocating a set of indivisible goods among agents with additive valuations. Ex-ante fairness (proportionality) can trivially be obtained by giving all goods to a random agent. Yet, such an allocation is very unfair ex-post. This has motivated the Best-of-Both-Worlds (BoBW) approach, seeking a randomized allocation that is ex-ante proportional and is supported only on ex-post fair allocations (e.g., on allocations that are envy-free-up-to-one-good (EF1), or give some constant fraction of the maximin share (MMS)). It is commonly pointed out that the distribution that allocates all goods to one agent at random fails to be ex-post fair as it ignores the variances of the values of the agents. We examine the approach of trying to mitigate this problem by minimizing the sum-of-variances of the values of the agents, subject to ex-ante proportionality. We study the ex-post fairness properties of the resulting distributions. In support of this approach, observe that such an optimization will indeed deterministically output a proportional allocation if such exists. We show that when valuations are identical, this approach indeed guarantees fairness ex-post: all allocations in the support are envy-free-up-to-any-good (EFX), and thus guarantee every agent at least 4/7 of her maximin share (but not her full MMS). On the negative side, we show that this approach completely fails when valuations are not identical: even in the simplest setting of only two agents and two goods, when the additive valuations are not identical, there is positive probability of allocating both goods to the same agent. Thus, the supporting ex-post allocation might not even be EF1, and might not give an agent any constant fraction of her MMS. Finally, we present similar negative results for other natural minimization objectives that are based on variances.

</details>


### [36] [The Geometry of Coalition Power: Majorization, Lattices, and Displacement in Multiwinner Elections](https://arxiv.org/abs/2601.16723)
*Qian Guo,Yidan Hu,Rui Zhang*

Main category: cs.GT

TL;DR: 研究联盟在多赢家Top-k选举中能驱逐多少当前获胜者，提出最大位移问题，证明联盟能力分解为两个独立的前缀多数化约束，对常见计分规则给出精确可行性检验算法。


<details>
  <summary>Details</summary>
Motivation: 量化协调联盟在位置计分规则下的多赢家选举中能施加多大影响力，特别是能迫使多少当前前k名获胜者出局，理解联盟操纵选举的极限能力。

Method: 将联盟能力分解为提升外部候选人和压制弱势获胜者两个独立的前缀多数化约束。对常见算术级数计分规则（如Borda、k-approval等），证明Majorization-Lattice定理，给出精确可行性检验算法和最大位移计算算法。

Result: 提出O(k'logk')精确可行性检验算法和O(k(logk)²log(mx))最大位移计算算法。实验验证了精确截止点、收益递减现象，以及前缀检验在g>1时会产生误报的模同余效应。

Conclusion: 联盟影响力受限于两个前缀多数化约束，对常见计分规则有精确的数学刻画和高效算法，为选举操纵分析提供了理论基础和实用工具。

Abstract: How much influence can a coordinated coalition exert in a multiwinner Top-$k$ election under a positional scoring rule? We study the maximum displacement problem: with coalition size $m$, how many of the current top-$k$ winners can be forced out? We show coalition power decomposes into two independent prefix-majorization constraints, capturing how much the coalition can (i) boost outsiders and (ii) suppress weak winners. For arbitrary scoring rules these prefix inequalities are tight, efficiently checkable necessary conditions (exact in the continuous relaxation).
  For common-step arithmetic-progression (AP) score ladders, including Borda, truncated Borda, $k$-approval/$k$-veto, plurality, and multi-level rules such as $3$--$2$--$1$, we prove a Majorization--Lattice Theorem: feasible aggregate score vectors are exactly the integer points satisfying the Block--HLP prefix-sum capacity constraints plus a single global congruence condition modulo the step size $g$. For Borda ($g=1$) the congruence vanishes, yielding a pure prefix-majorization test.
  This characterization yields an $O(k'\log k')$ exact feasibility oracle for displacing $k'$ winners, and an $O(k(\log k)^2\log(mx))$ algorithm (via dual-envelope binary search) for computing the maximum achievable displacement $k^\ast$. Experiments on Mallows profiles and PrefLib elections confirm exact cutoffs, diminishing returns, and substantial gains over baseline heuristics; for $g>1$ they also demonstrate the predicted congruence effect, where prefix-only tests produce false positives. The oracle scales to extreme instances, processing $10^9$ candidates in under 28 seconds (memory permitting).

</details>


### [37] [Multi-Agent Non-Discriminatory Contracts](https://arxiv.org/abs/2601.16835)
*Ke Ding,Bo Li,Ankang Sun*

Main category: cs.GT

TL;DR: 该论文研究多智能体合同中的公平性权衡，提出了"非歧视价格"概念来量化委托人效用最大化与代理人报酬平等化之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体合同研究主要关注委托人效用最大化，导致代理人之间报酬差异巨大。但在标准化公共合同或工人合作社等实际场景中，公平性至关重要。本文旨在量化委托人效用最大化与代理人报酬平等化之间的权衡。

Method: 通过定义"非歧视价格"来衡量公平性成本，分析不同非歧视水平下的最优效用损失，提供理论界限和权衡关系的全面特征化。

Result: 发现非歧视价格与代理人数量呈对数关系，通过放宽非歧视要求可将其改进为常数界限。全面刻画了非歧视水平与最优效用损失之间的权衡关系。

Conclusion: 在多智能体合同中存在显著的公平性权衡，非歧视价格随代理人数量对数增长，但通过适当放宽公平性要求可实现常数界限的改进，为实际合同设计提供了理论指导。

Abstract: We study multi-agent contracts, in which a principal delegates a task to multiple agents and incentivizes them to exert effort. Prior research has mostly focused on maximizing the principal's utility, often resulting in highly disparate payments among agents. Such disparities among agents may be undesirable in practice, for example, in standardized public contracting or worker cooperatives where fairness concerns are essential. Motivated by these considerations, our objective is to quantify the tradeoff between maximizing the principal's utility and equalizing payments among agents, which we call the price of non-discrimination. Our first result is an almost tight bound on the price of non-discrimination, which scales logarithmically with the number of agents. This bound can be improved to a constant by allowing some relaxation of the non-discrimination requirement. We then provide a comprehensive characterization of the tradeoff between the level of non-discrimination and the loss in the optimal utility.

</details>
