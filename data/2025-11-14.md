<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management](https://arxiv.org/abs/2511.10063)
*Yiwen Wang,Vivek Shah,Marcos Antonio Vaz Salles,Claudia Bauzer Medeiros,Julio Cesar Dos Reis,Yongluan Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种名为Moving Actor-Oriented Databases (M-AODBs)的分布式反应式移动对象数据管理平台，通过引入移动参与者抽象来增强分布式参与者框架，支持反应式感知、移动和空间查询功能。


<details>
  <summary>Details</summary>
Motivation: 现有移动对象应用需要支持对象反应行为来查询和更新动态数据，但反应行为通常需要复杂的终端用户实现，且需要满足低延迟计算要求和可扩展性。

Method: 提出移动参与者抽象概念，增强参与者模型，构建M-AODBs平台Dolphin，基于Microsoft Orleans分布式虚拟参与者框架实现非侵入式设计。

Result: 在真实反应式移动对象场景的实验评估中，Dolphin在多机器上表现出可扩展性，并提供近实时反应延迟。

Conclusion: 移动参与者抽象和M-AODBs平台能够帮助开发者避免在性能与一致性之间平衡的应用级方案实现负担，有效支持反应式移动对象应用。

Abstract: Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.

</details>


### [2] [CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models](https://arxiv.org/abs/2511.10418)
*Yaqiao Zhu,Hongkai Wen,Mark Birkin,Man Luo*

Main category: cs.DB

TL;DR: CityVerse是一个统一平台，整合多源城市数据、基于能力的任务分类和动态模拟，用于在城市计算领域系统评估大语言模型。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在城市计算任务中面临两个关键挑战：缺乏统一平台进行一致的多源数据访问，以及碎片化的任务定义阻碍公平比较。

Method: 提供坐标基础的Data API统一十类城市数据，Task API将43个城市计算任务组织为四级认知层次，以及支持实时数据检索和多层显示的交互式可视化前端。

Result: 通过主流大语言模型在代表性任务上的评估验证了平台有效性，展示了其支持可重复和系统评估的能力。

Conclusion: CityVerse为推进大语言模型和多任务方法在城市计算领域提供了可重用的基础。

Abstract: Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [3] [Truth, Justice, and Secrecy: Cake Cutting Under Privacy Constraints](https://arxiv.org/abs/2511.09882)
*Yaron Salman,Tamir Tassa,Omer Lev,Roie Zivan*

Main category: cs.GT

TL;DR: 提出首个隐私保护的蛋糕切割协议，在保持公平性和策略证明性的同时，通过密码学技术保护代理人的偏好隐私。


<details>
  <summary>Details</summary>
Motivation: 现有蛋糕切割算法虽能保证公平性和策略证明性，但代理人仍可能因隐私顾虑不愿透露真实偏好（如商业竞争中的营销策略泄露风险）。

Method: 在Chen等人的策略证明算法基础上，用密码学技术替代集中式计算，实现隐私保护而不牺牲公平性或策略证明性。

Result: 开发出首个同时具备隐私保护、无嫉妒性和策略证明性的蛋糕切割协议。

Conclusion: 该协议不仅消除了代理人说谎的动机，还保护了其偏好隐私，从而更有效地鼓励真实偏好报告。

Abstract: Cake-cutting algorithms, which aim to fairly allocate a continuous resource based on individual agent preferences, have seen significant progress over the past two decades. Much of the research has concentrated on fairness, with comparatively less attention given to other important aspects. Chen et al. (2010) introduced an algorithm that, in addition to ensuring fairness, was strategyproof -- meaning agents had no incentive to misreport their valuations. However, even in the absence of strategic incentives to misreport, agents may still hesitate to reveal their true preferences due to privacy concerns (e.g., when allocating advertising time between firms, revealing preferences could inadvertently expose planned marketing strategies or product launch timelines). In this work, we extend the strategyproof algorithm of Chen et al. by introducing a privacy-preserving dimension. To the best of our knowledge, we present the first private cake-cutting protocol, and, in addition, this protocol is also envy-free and strategyproof. Our approach replaces the algorithm's centralized computation with a novel adaptation of cryptographic techniques, enabling privacy without compromising fairness or strategyproofness. Thus, our protocol encourages agents to report their true preferences not only because they are not incentivized to lie, but also because they are protected from having their preferences exposed.

</details>


### [4] [Robust Resource Allocation via Competitive Subsidies](https://arxiv.org/abs/2511.09934)
*David X. Lin,Giannis Fikioris,Siddhartha Banerjee,Éva Tardos*

Main category: cs.GT

TL;DR: 提出了一种新的0.625鲁棒机制，打破了之前0.6的第一价格拍卖限制，通过简单的竞拍机制和竞争补贴概念，几乎达到了非策略性鲁棒界限0.63。


<details>
  <summary>Details</summary>
Motivation: 在非货币在线资源分配中，现有机制最多只能达到0.59的鲁棒性，而第一价格拍卖无法突破0.6的限制，但非策略性最佳可达0.63，存在改进空间。

Method: 使用简单拍卖机制：每轮竞标者决定是否请求物品，然后在请求者中随机分配。引入竞争补贴概念，根据竞标者数量调整获胜者的信用支付（当k个竞标者时，获胜者支付与k/(k+1)成比例）。

Result: 新机制实现了0.625的鲁棒性，几乎接近非策略性界限0.63。修改后可在均衡策略下达到约0.61的鲁棒性，并在均衡时达到最优的1-1/e因子。

Conclusion: 该机制突破了第一价格拍卖的0.6限制，在广泛的基于拍卖的机制类别中提供了最佳可能的界限。

Abstract: A canonical setting for non-monetary online resource allocation is one where agents compete over multiple rounds for a single item per round, with i.i.d. valuations and additive utilities across rounds. With $n$ symmetric agents, a natural benchmark for each agent is the utility realized by her favorite $1/n$-fraction of rounds; a line of work has demonstrated one can robustly guarantee each agent a constant fraction of this ideal utility, irrespective of how other agents behave. In particular, several mechanisms have been shown to be $1/2$-robust, and recent work established that repeated first-price auctions based on artificial credits have a robustness factor of $0.59$, which cannot be improved beyond $0.6$ using first-price and simple strategies. In contrast, even without strategic considerations, the best achievable factor is $1-1/e\approx 0.63$.
  In this work, we break the $0.6$ first-price barrier to get a new $0.625$-robust mechanism, which almost closes the gap to the non-strategic robustness bound. Surprisingly, we do so via a simple auction, where in each round, bidders decide if they ask for the item, and we allocate uniformly at random among those who ask. The main new ingredient is the idea of competitive subsidies, wherein we charge the winning agent an amount in artificial credits that decreases when fewer agents are bidding (specifically, when $k$ agents bid, then the winner pays proportional to $k/(k+1)$, varying the payment by a factor of 2 depending on the competition). Moreover, we show how it can be modified to get an equilibrium strategy with a slightly weaker robust guarantee of $5/(3e) \approx 0.61$ (and the optimal $1-1/e$ factor at equilibrium). Finally, we show that our mechanism gives the best possible bound under a wide class of auction-based mechanisms.

</details>


### [5] [Facility Location for Congesting Commuters and Generalizing the Cost-Distance Problem](https://arxiv.org/abs/2511.10228)
*Thanasis Lianeas,Marios Mertzanidis,Aikaterini Nikolidaki*

Main category: cs.GT

TL;DR: 本文提出了一种新的设施选址问题——拥堵通勤者设施选址问题，其中代理连接到设施的成本取决于拥堵情况。通过近似Caratheodory定理和借鉴成本-距离问题的算法，为不同成本函数类型提供了近似解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统无容量限制设施选址问题未考虑拥堵依赖性连接成本。本文通过假设代理到设施的连接成本受拥堵影响，定义了一个新问题，更贴近实际通勤场景。

Method: 对于非递减成本函数，采用近似Caratheodory定理的新方法；对于非递增成本函数，将问题推广为成本-距离问题，并设计了具有相同近似保证的算法。

Result: 从相关文献可知该问题存在不可近似性结果，因此只能寻求近似解。本文为两种不同类型的成本函数提供了有效的近似解决方案。

Conclusion: 通过结合近似Caratheodory定理和成本-距离问题的推广，本文为拥堵依赖性连接成本的设施选址问题提供了可行的近似算法框架。

Abstract: In Facility Location problems there are agents that should be connected to facilities and locations where facilities may be opened so that agents can connect to them. We depart from Uncapacitated Facility Location and by assuming that the connection costs of agents to facilities are congestion dependent, we define a novel problem, namely, Facility Location for Congesting (Selfish) Commuters. The connection costs of agents to facilities come as a result of how the agents commute to reach the facilities in an underlying network with cost functions on the edges. Inapproximability results follow from the related literature and thus approximate solutions is all we can hope for. For when the cost functions are nondecreasing we employ in a novel way an approximate version of Caratheodory's Theorem [5] to show how approximate solutions for different versions of the problem can be derived. For when the cost functions are nonincreasing we show how this problem generalizes the Cost-Distance problem [38] and provide an algorithm that for this more general case achieves the same approximation guarantees.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [6] [Robustness and Imperceptibility Analysis of Hybrid Spatial-Frequency Domain Image Watermarking](https://arxiv.org/abs/2511.10245)
*Rizal Khoirul Anam*

Main category: cs.MM

TL;DR: 比较三种数字图像水印技术：空间域LSB、频域DFT和混合LSB+DFT方法，评估它们在不可感知性和鲁棒性之间的权衡。混合方法在保持高视觉质量的同时，对所有测试攻击表现出最佳鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 数字媒体的普及需要强大的版权保护和内容认证方法，需要在水印的不可感知性和鲁棒性之间找到最佳平衡。

Method: 在统一MATLAB实验框架中实现三种水印技术：空间域LSB、频域DFT和混合LSB+DFT方法。对水印图像进行多种图像处理攻击测试，包括JPEG压缩、高斯噪声和椒盐噪声。

Result: LSB方法具有最佳不可感知性但非常脆弱；DFT方法鲁棒性强但视觉质量较差；混合LSB+DFT方法通过冗余嵌入和备用提取机制，在所有测试攻击中保持高视觉保真度和最佳鲁棒性平衡。

Conclusion: 提出的混合LSB+DFT水印技术是数字图像版权保护的最优解决方案，在不可感知性和鲁棒性之间实现了最佳权衡。

Abstract: The proliferation of digital media necessitates robust methods for copyright protection and content authentication. This paper presents a comprehensive comparative study of digital image watermarking techniques implemented using the spatial domain (Least Significant Bit - LSB), the frequency domain (Discrete Fourier Transform - DFT), and a novel hybrid (LSB+DFT) approach. The core objective is to evaluate the trade-offs between imperceptibility (measured by Peak Signal-to-Noise Ratio - PSNR) and robustness (measured by Normalized Correlation - NC and Bit Error Rate - BER). We implemented these three techniques within a unified MATLAB-based experimental framework. The watermarked images were subjected to a battery of common image processing attacks, including JPEG compression, Gaussian noise, and salt-and-pepper noise, at varying intensities. Experimental results generated from standard image datasets (USC-SIPI) demonstrate that while LSB provides superior imperceptibility, it is extremely fragile. The DFT method offers significant robustness at the cost of visual quality. The proposed hybrid LSB+DFT technique, which leverages redundant embedding and a fallback extraction mechanism, is shown to provide the optimal balance, maintaining high visual fidelity while exhibiting superior resilience to all tested attacks.

</details>


### [7] [TMDC: A Two-Stage Modality Denoising and Complementation Framework for Multimodal Sentiment Analysis with Missing and Noisy Modalities](https://arxiv.org/abs/2511.10325)
*Yan Zhuang,Minhao Liu,Yanru Zhang,Jiawen Deng,Fuji Ren*

Main category: cs.MM

TL;DR: 提出了TMDC框架，通过两阶段训练联合解决多模态情感分析中的模态缺失和噪声问题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态情感分析面临模态缺失和噪声信号的挑战，现有方法通常单独处理这些问题，限制了实际应用效果。

Method: TMDC框架包含两个阶段：1) 模态内去噪阶段，从完整数据中提取去噪后的模态特定和模态共享表示；2) 模态间互补阶段，利用这些表示补偿缺失模态，丰富可用信息。

Result: 在MOSI、MOSEI和IEMOCAP数据集上的广泛评估表明，TMDC相比现有方法持续获得更优性能，建立了新的SOTA结果。

Conclusion: TMDC框架能有效联合缓解多模态情感分析中的模态缺失和噪声问题，提升模型的鲁棒性和准确性。

Abstract: Multimodal Sentiment Analysis (MSA) aims to infer human sentiment by integrating information from multiple modalities such as text, audio, and video. In real-world scenarios, however, the presence of missing modalities and noisy signals significantly hinders the robustness and accuracy of existing models. While prior works have made progress on these issues, they are typically addressed in isolation, limiting overall effectiveness in practical settings. To jointly mitigate the challenges posed by missing and noisy modalities, we propose a framework called Two-stage Modality Denoising and Complementation (TMDC). TMDC comprises two sequential training stages. In the Intra-Modality Denoising Stage, denoised modality-specific and modality-shared representations are extracted from complete data using dedicated denoising modules, reducing the impact of noise and enhancing representational robustness. In the Inter-Modality Complementation Stage, these representations are leveraged to compensate for missing modalities, thereby enriching the available information and further improving robustness. Extensive evaluations on MOSI, MOSEI, and IEMOCAP demonstrate that TMDC consistently achieves superior performance compared to existing methods, establishing new state-of-the-art results.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [A Quasi-Polynomial Time Algorithm for 3-Coloring Circle Graphs](https://arxiv.org/abs/2511.09707)
*Ajaykrishnan E S,Robert Ganian,Daniel Lokshtanov,Vaishali Surianarayanan*

Main category: cs.DS

TL;DR: 该论文提出了一个算法，用于在圆图中进行3-着色，运行时间为n^O(log n)，并应用于确定有序图是否具有3页书籍嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决Dujmović和Wood等人提出的开放问题：圆图上的3-着色问题是否存在多项式时间算法。

Method: 设计了一个算法，输入n个顶点的圆图G，在最多n^O(log n)时间内找到G的适当3-着色（如果存在）。

Result: 获得了在相同运行时间内确定给定有序图是否具有3页书籍嵌入的算法。

Conclusion: 该工作部分解决了圆图3-着色问题的多项式时间算法开放问题。

Abstract: A graph $G$ is a circle graph if it is an intersection graph of chords of a unit circle. We give an algorithm that takes as input an $n$ vertex circle graph $G$, runs in time at most $n^{O(\log n)}$ and finds a proper $3$-coloring of $G$, if one exists. As a consequence we obtain an algorithm with the same running time to determine whether a given ordered graph $(G, \prec)$ has a $3$-page book embedding. This gives a partial resolution to the well known open problem of Dujmović and Wood [Discret. Math. Theor. Comput. Sci. 2004], Eppstein [2014], and Bachmann, Rutter and Stumpf [J. Graph Algorithms Appl. 2024] of whether 3-Coloring on circle graphs admits a polynomial time algorithm.

</details>


### [9] [Hardness of Dynamic Tree Edit Distance and Friends](https://arxiv.org/abs/2511.09842)
*Bingbing Hu,Jakob Nogler,Barna Saha*

Main category: cs.DS

TL;DR: 本文证明了树编辑距离、Dyck编辑距离和RNA折叠等问题的动态算法无法比静态算法更快，必须每次更新后重新计算。


<details>
  <summary>Details</summary>
Motivation: 虽然字符串编辑距离在动态设置下已有高效算法，但对于树编辑距离、Dyck编辑距离和RNA折叠等密切相关的问题，是否能在动态设置中获得优势仍是未知的。

Method: 基于精细复杂度假设，通过理论证明这些问题的动态算法无法获得加速，必须重新计算。

Result: 证明了树编辑距离、Dyck编辑距离和RNA折叠在动态设置下没有加速可能，并给出了无权重树编辑距离的二次更新下界。

Conclusion: 这是首次在动态设置下分离字符串编辑距离和树编辑距离，揭示了这些相关问题的根本差异。

Abstract: String Edit Distance is a more-than-classical problem whose behavior in the dynamic setting, where the strings are updated over time, is well understood. A single character substitution, insertion, or deletion can be handled in $\tilde{\mathcal{O}}(n \cdot \min(\sqrt{n},w))$ time [Charalampopoulos, Kociumaka, Mozes, CPM 2020], where $w$ is the maximum operation weight. This bound is optimal [Cassis, Kociumaka, Wellnitz, FOCS 2023] and provides a substantial improvement over the static $\mathcal{O}(n^2)$ algorithm when few characters of the input string are updated.
  In contrast, for inherently related problems such as Tree Edit Distance, Dyck Edit Distance, and RNA Folding, it has remained unknown whether it is possible to devise dynamic algorithms with an advantage over the static algorithm. In this paper, we resolve this question by showing that (weighted) Tree Edit Distance, Dyck Edit Distance, and RNA Folding admit no dynamic speedup: under well-known fine-grained assumptions we show that the best possible algorithm recomputes the solution from scratch after each update. Furthermore, we prove a quadratic per-update lower bound for unweighted Tree Edit Distance under the $k$-Clique Conjecture. This provides the first separation between dynamic unweighted String Edit Distance and unweighted Tree Edit Distance, problems whose relative difficulty in the static setting is still open.

</details>


### [10] [Faster All-Pairs Minimum Cut: Bypassing Exact Max-Flow](https://arxiv.org/abs/2511.10036)
*Yotam Kenneth-Mordoch,Robert Krauthgamer*

Main category: cs.DS

TL;DR: 提出了一个用于无权重图中所有最小割对的稀疏化方法，并在多种计算模型中改进了全对最小割问题的算法效率


<details>
  <summary>Details</summary>
Motivation: 现有的全对最小割算法依赖于精确最大流计算，但在某些计算模型（如割查询模型和全动态模型）中缺乏快速的最大流算法

Method: 构建一个能保持所有最小s,t割的稀疏化器，仅使用近似最大流计算，并将其应用于割查询、全动态和流式计算模型

Result: 在三种模型中取得改进：随机割查询算法复杂度为Õ(n^{3/2})，确定性全动态算法更新时间为n^{3/2+o(1)}，随机两遍流算法空间需求为Õ(n^{3/2})

Conclusion: 通过稀疏化技术避免了精确最大流计算的需求，为全对最小割问题在多种计算模型中提供了更高效的解决方案

Abstract: All-Pairs Minimum Cut (APMC) is a fundamental graph problem that asks to find a minimum $s,t$-cut for every pair of vertices $s,t$. A recent line of work on fast algorithms for APMC has culminated with a reduction of APMC to $\mathrm{polylog}(n)$-many max-flow computations. But unfortunately, no fast algorithms are currently known for exact max-flow in several standard models of computation, such as the cut-query model and the fully-dynamic model.
  Our main technical contribution is a sparsifier that preserves all minimum $s,t$-cuts in an unweighted graph, and can be constructed using only approximate max-flow computations. We then use this sparsifier to devise new algorithms for APMC in unweighted graphs in several computational models: (i) a randomized algorithm that makes $\tilde{O}(n^{3/2})$ cut queries to the input graph; (ii) a deterministic fully-dynamic algorithm with $n^{3/2+o(1)}$ worst-case update time; and (iii) a randomized two-pass streaming algorithm with space requirement $\tilde{O}(n^{3/2})$. These results improve over the known bounds, even for (single pair) minimum $s,t$-cut in the respective models.

</details>


### [11] [Algorithms and Complexity of Hedge Cluster Deletion Problems](https://arxiv.org/abs/2511.10202)
*Athanasios L. Konstantinidis,Charis Papadopoulos,Georgios Velissaris*

Main category: cs.DS

TL;DR: 本文研究了Hedge Cluster Deletion问题，这是一个图聚类问题的推广，证明了在某些条件下可在多项式时间求解，并给出了近似算法和不可近似性结果。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge Cluster Deletion问题的动机是推广经典的Cluster Deletion问题，考虑边集被划分为称为hedges的分组的情况，探索在更复杂结构下的聚类问题求解方法。

Method: 通过分析图的结构特性（如顶点不相交的3-顶点路径、三角形覆盖等），建立与Min Horn Deletion问题的联系，并设计基于hedge交集图的算法框架。

Result: 证明了对于包含有界大小顶点不相交3-顶点路径的子图的图，问题可在多项式时间求解；给出了常数近似比的算法；建立了问题的不可近似性下界。

Conclusion: Hedge Cluster Deletion问题在特定图结构下可高效求解，但在一般情况下具有强的不可近似性，通过hedge交集图等结构分析可提供有效的求解途径。

Abstract: A hedge graph is a graph whose edge set has been partitioned into groups called hedges. Here we consider a generalization of the well-known \textsc{Cluster Deletion} problem, named \textsc{Hedge Cluster Deletion}. The task is to compute the minimum number of hedges of a hedge graph so that their removal results in a graph that is isomorphic to a disjoint union of cliques. We show that for graphs that contain bounded size of vertex-disjoint 3-vertex-paths as subgraphs, \textsc{Hedge Cluster Deletion} can be solved in polynomial time. Regarding its approximability, we prove that the problem is tightly connected to the related complexity of the \textsc{Min Horn Deletion} problem, a well-known boolean CSP problem. Our connection shows that it is NP-hard to approximate \textsc{Hedge Cluster Deletion} within factor $2^{O(\log^{1-ε} r)}$ for any $ε>0$, where $r$ is the number of hedges in a given hedge graph.
  Based on its classified (in)approximability and the difficulty imposed by the structure of almost all non-trivial graphs, we consider the hedge underlying structure. We give a polynomial-time algorithm with constant approximation ratio for \textsc{Hedge Cluster Deletion} whenever each triangle of the input graph is covered by at most two hedges. On the way to this result, an interesting ingredient that we solved efficiently is a variant of the \textsc{Vertex Cover} problem in which apart from the desired vertex set that covers the edge set, a given set of vertex-constraints should also be included in the solution. Moreover, as a possible workaround for the existence of efficient exact algorithms, we propose the hedge intersection graph which is the intersection graph spanned by the hedges. Towards this direction, we give a polynomial-time algorithm for \textsc{Hedge Cluster Deletion} whenever the hedge intersection graph is acyclic.

</details>


### [12] [Testing H-freeness on sparse graphs, the case of bounded expansion](https://arxiv.org/abs/2511.10230)
*Samuel Humeau,Mamadou Moustapha Kanté,Daniel Mock,Timothé Picavet,Alexandre Vigny*

Main category: cs.DS

TL;DR: 本文提出了一个更简单的证明方法，用于在具有有界扩展性的图类中测试H-无性，将查询复杂度从依赖于图大小降低到常数级别。


<details>
  <summary>Details</summary>
Motivation: 在稀疏图模型中，虽然稠密图的属性测试已有较好特征化，但稀疏图的测试器特征化仍不完善。Czumaj和Sohler证明了在平面图和排除子图的图类中H-无性可以用常数查询测试，本文旨在用稀疏性工具包提供更简单的证明。

Method: 利用稀疏性工具包的结果，为具有有界扩展性的图类提供H-无性测试的简化证明方法。该方法将查询复杂度从依赖于图大小降低到常数级别，常数仅依赖于H和图类C。

Result: 证明了对于任何具有有界扩展性的图类C和任何图H，在C中的任何图G上测试H-无性都可以用常数查询复杂度完成，该常数仅依赖于H和C，与G无关。

Conclusion: 本文提供了一个更简洁的证明框架，将H-无性测试的常数查询复杂度结果扩展到更广泛的有界扩展性图类，包括排除子图的图类、立方图、有界最大度图类等。

Abstract: In property testing, a tester makes queries to (an oracle for) a graph and, on a graph having or being far from having a property P, it decides with high probability whether the graph satisfies P or not. Often, testers are restricted to a constant number of queries. While the graph properties for which there exists such a tester are somewhat well characterized in the dense graph model, it is not the case for sparse graphs. In this area, Czumaj and Sohler (FOCS'19) proved that H-freeness (i.e. the property of excluding the graph H as a subgraph) can be tested with constant queries on planar graphs as well as on graph classes excluding a minor.
  Using results from the sparsity toolkit, we propose a simpler alternative to the proof of Czumaj and Sohler, for a statement generalized to the broader notion of bounded expansion. That is, we prove that for any class C with bounded expansion and any graph H, testing H-freeness can be done with constant query complexity on any graph G in C, where the constant depends on H and C, but is independent of G.
  While classes excluding a minor are prime examples of classes with bounded expansion, so are, for example, cubic graphs, graph classes with bounded maximum degree, graphs of bounded book thickness, or random graphs of bounded average degree.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [13] [A Universal Block Error Rate Bound for Fluid Antenna Systems](https://arxiv.org/abs/2511.09929)
*Zhentian Zhang,David Morales-Jimenez,Hao Jiang,Christos Masouros*

Main category: cs.IT

TL;DR: 本文研究了有限块长流体天线系统(FBL-FAS)，提出了一个通用的块错误率(BLER)边界，可作为各种FAS架构的性能基准。该边界可在有/无统计模型的情况下计算，具有简单、准确和普适性的特点。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的研究很少探讨有限块长约束对FAS设计的影响，在分析建模和建立通用性能指标方面存在空白。

Method: 推导了一个块错误率(BLER)边界，该边界可在有/无明确统计模型的情况下计算，适用于模型感知或模型无关的系统场景。

Result: 当统计模型已知时，基于所提BLER边界的分析结果与实证发现高度一致，验证了该边界的准确性。

Conclusion: 所提出的BLER边界具有显著的简单性、准确性和普适性，为FBL-FAS提供了一个通用且实用的性能基准。

Abstract: Fluid antenna systems (FASs) offer genuine simplicity for communication network design by eliminating expensive hardware overhead and reducing the complexity of access protocol architectures. Through the discovery of significant spatial diversity within a compact antenna space, FASs enable the implementation of reconfigurable-antenna-based architectures. However, current state-of-the-art studies rarely investigate the impact of finite blocklength constraints on FAS-based designs, leaving a gap in both analytical modeling and the establishment of a solid, universally applicable performance metric for finite blocklength fluid antenna systems (FBL-FAS). In this work, we focus on the study of FBL-FAS and, more importantly, derive a block error rate (BLER) bound that serves as a general and practical performance benchmark across various FAS architectures. The proposed BLER bound is computable both with and without an explicit statistical model, meaning that the BLER performance can be characterized analytically or empirically under model-aware or model-free system scenarios. Moreover, when the statistical model is known, the analytical results derived from the proposed BLER bound exhibit strong alignment with the empirical findings, demonstrating the remarkable simplicity, accuracy, and universality of the proposed BLER bound.

</details>


### [14] [Implicit Semantic Communication Based on Bayesian Reconstruction Framework](https://arxiv.org/abs/2511.10052)
*Yiwei Liao,Shurui Tu,Yujie Zhou,Dongzi Jin,Yong Xiao,Yingyu Li*

Main category: cs.IT

TL;DR: 提出了一种基于贝叶斯超图推理的语义通信框架，能够从基于成对关系的显式语义中恢复高阶超边隐含语义信息


<details>
  <summary>Details</summary>
Motivation: 现有语义通信大多基于成对关系图，无法捕捉语义源中必需的高阶交互信息

Method: 贝叶斯超图推理框架，在接收端基于发送端传输的成对关系显式语义直接恢复涉及高阶超边的隐含语义信息

Result: 在真实数据集上的实验表明，该框架能够基于成对关系显式语义实现高达90%的高阶超边恢复准确率

Conclusion: 所提出的SBRF框架成功解决了语义通信中高阶语义信息恢复的问题

Abstract: Semantic communication is a novel communication paradigm that focuses on the transportation and delivery of the \emph{meaning} of messages. Recent results have verified that a graphical structure provides the most expressive and structurally faithful formalism for representing the relational semantics in most information sources. However, most existing works represent the semantics based on pairwise relation-based graphs, which cannot capture the higher-order interactions that are essential for some semantic sources. This paper proposes a novel Bayesian hypergraph inference-based semantic communication framework that can directly recover implicit semantic information involving high-order hyperedges at the receiver based on the pairwise relation-based explicit semantics sent by the transmitter. Experimental results based on real-world datasets demonstrated that the proposed SBRF achieves up to 90\% recovery accuracy of the high-order hyperedges based on the pairwise relation-based explicit semantics.

</details>


### [15] [Generalized Spectral Bound for Quasi-Twisted Codes](https://arxiv.org/abs/2511.10066)
*Buket Özkaya*

Main category: cs.IT

TL;DR: 本文提出了一种改进的准扭曲码谱界方法，该方法在最小距离估计上优于Jensen界和Ezerman等人的界。


<details>
  <summary>Details</summary>
Motivation: 现有的准扭曲码谱界方法性能不如Jensen界，而最近提出的改进准循环码谱界方法在许多情况下优于Jensen界，因此希望将其推广到准扭曲码情况。

Method: 采用最近提出的改进准循环码谱界方法，并将其推广到准扭曲码情况，得到新的广义谱界。

Result: 新提出的广义谱界在最小距离估计上比Jensen界和Ezerman等人的界更紧。

Conclusion: 将改进的准循环码谱界方法推广到准扭曲码是有效的，可以获得更优的最小距离下界估计。

Abstract: Semenov and Trifonov [22] developed a spectral theory for quasi-cyclic codes and formulated a BCH-like minimum distance bound. Their approach was generalized by Zeh and Ling [24], by using the HT bound. The first spectral bound for quasi-twisted codes appeared in [7], which generalizes Semenov-Trifonov and Zeh-Ling bounds, but its overall performance was observed to be worse than the Jensen bound. More recently, an improved spectral bound for quasi-cyclic codes was proposed in [15], which outperforms the Jensen bound in many cases. In this paper, we adopt this approach to quasi-twisted case and we show that this new generalized spectral bound provides tighter lower bounds on the minimum distance compared to the Jensen and Ezerman et. al. bounds.

</details>


### [16] [Sequential Adversarial Hypothesis Testing](https://arxiv.org/abs/2511.10181)
*Eeshan Modak,Mayank Bakshi,Bikash Kumar Dey,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 本文研究了序列设置中的对抗性二元假设检验问题，其中每个假设对应一个封闭的凸分布集，对手根据过去观测选择分布生成观测。序列设置通过可变观测次数提高了测试的渐近性能。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性环境下的二元假设检验问题，其中对手能够根据历史观测自适应地选择分布，这在实际应用中更贴近现实场景。序列设置提供了额外的自由度来改善测试性能。

Method: 在序列设置中，检测器使用可变数量的观测来做出决策。每个假设对应一个封闭的凸分布集，对手根据过去观测从相应集合中选择分布生成新观测。

Result: 刻画了可实现的错误指数对的闭包，并研究了在观测次数和错误概率约束下的问题表现。

Conclusion: 序列设置在对抗性二元假设检验中通过可变观测次数显著改善了渐近性能，为实际应用提供了更优的检测策略。

Abstract: We study the adversarial binary hypothesis testing problem in the sequential setting. Associated with each hypothesis is a closed, convex set of distributions. Given the hypothesis, each observation is generated according to a distribution chosen (from the set associated with the hypothesis) by an adversary who has access to past observations. In the sequential setting, the number of observations the detector uses to arrive at a decision is variable; this extra freedom improves the asymptotic performance of the test. We characterize the closure of the set of achievable pairs of error exponents. We also study the problem under constraints on the number of observations used and the probability of error incurred.

</details>


### [17] [Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access](https://arxiv.org/abs/2511.10291)
*Aswin Arun,Christo Kurisummoottil Thomas,Rimalpudi Sarvendranath,Walid Saad*

Main category: cs.IT

TL;DR: 提出了一种基于因果模型的多智能体强化学习框架，通过结构因果模型和注意力网络显式表示网络变量间的因果关系，使用数据增强技术生成合成数据用于策略优化，相比传统方法显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在无线物联网应用中面临样本效率低的问题，而传统的基于模型的方法依赖黑盒模型，缺乏可解释性和推理能力。

Method: 使用结构因果模型和基于注意力的推理网络显式表示网络变量间的因果关系，开发可解释的因果模型捕获MAC控制消息、传输动作和信道观测之间的影响关系，利用数据增强技术生成合成数据，通过近端策略优化进行策略优化。

Result: 分析结果表明因果MBRL相比黑盒方法具有指数级的样本复杂度优势，平均减少58%的环境交互，收敛速度更快，并能通过基于注意力的因果归因提供可解释的调度决策。

Conclusion: 因果MBRL结合了样本效率和可解释性，为资源受限的无线系统提供了一种实用的方法。

Abstract: Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.

</details>


### [18] [Reconfigurable Airspace: Synergizing Movable Antenna and Intelligent Surface for Low-Altitude ISAC Networks](https://arxiv.org/abs/2511.10310)
*Honghao Wang,Qingqing Wu,Yifan Jiang,Ziyuan Zheng,Ziheng Zhang,Yanze Zhu,Ying Gao,Wen Chen,Guanghai Liu,Abbas Jamalipour*

Main category: cs.IT

TL;DR: 提出了一种利用可移动天线和智能反射表面协同工作的新型框架，以解决低空无人机网络中感知与通信功能之间的权衡问题，提升6G ISAC系统性能。


<details>
  <summary>Details</summary>
Motivation: 低空无人机网络是未来6G ISAC系统的关键组成部分，但其部署面临无人机高机动性、复杂传播环境以及感知与通信功能之间固有权衡的挑战。

Method: 利用可移动天线通过主动收发器重构和智能反射表面通过被动信道重构的协同工作机制，显著增强系统性能。研究了两种核心无人机部署场景：作为ISAC用户和作为空中网络节点。

Result: 仿真结果验证了MA-IRS支持的ISAC架构的巨大潜力，能够实现高精度跟踪、空中安全、鲁棒设计和复杂耦合资源优化。

Conclusion: 为先进低空ISAC网络的未来设计指明了清晰的技术挑战和研究机会路线图。

Abstract: Low-altitude unmanned aerial vehicle (UAV) networks are integral to future 6G integrated sensing and communication (ISAC) systems. However, their deployment is hindered by challenges stemming from high mobility of UAVs, complex propagation environments, and the inherent trade-offs between coexisting sensing and communication functions. This article proposes a novel framework that leverages movable antennas (MAs) and intelligent reflecting surfaces (IRSs) as dual enablers to overcome these limitations. MAs, through active transceiver reconfiguration, and IRSs, via passive channel reconstruction, can work in synergy to significantly enhance system performance. Our analysis first elaborates on the fundamental gains offered by MAs and IRSs, and provides simulation results that validate the immense potential of the MA-IRS-enabled ISAC architecture. Two core UAV deployment scenarios are then investigated: (i) UAVs as ISAC users, where we focus on achieving high-precision tracking and aerial safety, and (ii) UAVs as aerial network nodes, where we address robust design and complex coupled resource optimization. Finally, key technical challenges and research opportunities are identified and analyzed for each scenario, charting a clear course for the future design of advanced low-altitude ISAC networks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [19] [GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation](https://arxiv.org/abs/2511.10138)
*Jun Zhang,Yi Li,Yue Liu,Changping Wang,Yuan Wang,Yuling Xiong,Xun Liu,Haiyang Wu,Qian Li,Enming Zhang,Jiawei Sun,Xin Xu,Zishuai Zhang,Ruoran Liu,Suyuan Huang,Zhaoxin Zhang,Zhengkai Guo,Shuojin Yang,Meng-Hao Guo,Huan Yu,Jie Jiang,Shi-Min Hu*

Main category: cs.IR

TL;DR: GPR是首个将广告推荐重新定义为端到端生成任务的统一模型框架，通过统一表示、异构分层解码器和多阶段联合训练策略，解决了传统多阶段系统的目标错位和误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有广告推荐系统存在目标错位和误差传播问题，难以实现全局最优，而统一的生成推荐模型仍难以满足工业应用需求。

Method: 提出统一输入模式和tokenization方法，将广告和自然内容映射到共享多级语义ID空间；开发异构分层解码器，解耦用户意图建模和广告生成；采用多阶段联合训练策略，整合多token预测、价值感知微调和层次增强策略优化。

Result: GPR已在腾讯微信视频号广告系统全面部署，在GMV和CTCVR等关键业务指标上取得显著提升。

Conclusion: GPR框架成功实现了端到端的生成式广告推荐，解决了传统系统的局限性，在实际工业应用中表现出色。

Abstract: As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.

</details>


### [20] [Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding](https://arxiv.org/abs/2511.10492)
*Yunkai Zhang,Qiang Zhang,Feng,Lin,Ruizhong Qiu,Hanchao Yu,Jason Liu,Yinglong Xia,Zhuoran Yu,Zeyu Zheng,Diji Yang*

Main category: cs.IR

TL;DR: 提出了一个与骨干网络无关的框架，将人类先验知识直接集成到生成式推荐系统的端到端训练中，通过轻量级的先验条件适配器头来引导模型沿着人类可理解的维度解耦用户意图。


<details>
  <summary>Details</summary>
Motivation: 工业实践中积累了丰富的结构化领域知识（人类先验），但现有方法要么在排序后进行调整（与核心模型学习脱节），要么丢弃这些宝贵先验知识以无监督方式学习用户意图。

Method: 使用轻量级的先验条件适配器头，引导模型沿着人类可理解的维度（如交互类型、长短期兴趣）解耦用户意图，并引入分层组合策略来建模不同先验类型间的复杂交互。

Result: 在三个大规模数据集上的广泛实验表明，该方法显著提升了准确性和超越准确性的目标，同时人类先验使骨干模型能更有效地利用更长的上下文长度和更大的模型规模。

Conclusion: 人类先验知识可以有效地集成到生成式推荐系统的端到端训练中，不仅提升推荐性能，还能增强模型对上下文和规模的利用能力。

Abstract: Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.
  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.

</details>
