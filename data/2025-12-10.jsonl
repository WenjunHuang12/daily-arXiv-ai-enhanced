{"id": "2512.08483", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.08483", "abs": "https://arxiv.org/abs/2512.08483", "authors": ["Lingze Zeng", "Naili Xing", "Shaofeng Cai", "Peng Lu", "Gang Chen", "Jian Pei", "Beng Chin Ooi"], "title": "NeurIDA: Dynamic Modeling for Effective In-Database Analytics", "comment": "13 pages", "summary": "Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.\n  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically \"tweaks\" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA"}
{"id": "2512.08526", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.08526", "abs": "https://arxiv.org/abs/2512.08526", "authors": ["Shunit Agmon", "Jonathan Gal", "Amir Gilad", "Ester Livshits", "Or Mutay", "Brit Youngmann", "Benny Kimelfeld"], "title": "Analyzing Deviations from Monotonic Trends through Database Repair", "comment": null, "summary": "Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).\n  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework."}
{"id": "2512.08679", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.08679", "abs": "https://arxiv.org/abs/2512.08679", "authors": ["Tal Blau", "Brit Youngmann", "Anna Fariha", "Yuval Moskovitch"], "title": "Causal Explanations for Disparate Trends: Where and Why?", "comment": null, "summary": "During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets."}
{"id": "2512.07846", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07846", "abs": "https://arxiv.org/abs/2512.07846", "authors": ["Guoyao Li", "Ran He", "Shusen Jing", "Kayhan Behdin", "Yubo Wang", "Sundara Raman Ramachandran", "Chanh Nguyen", "Jian Sheng", "Xiaojing Ma", "Chuanrui Zhu", "Sriram Vasudevan", "Muchen Wu", "Sayan Ghosh", "Lin Su", "Qingquan Song", "Xiaoqing Wang", "Zhipeng Wang", "Qing Lan", "Yanning Chen", "Jingwei Wu", "Luke Simon", "Wenjing Zhang", "Qi Guo", "Fedor Borisyuk"], "title": "MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction", "comment": null, "summary": "Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests."}
{"id": "2512.08017", "categories": ["cs.IT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.08017", "abs": "https://arxiv.org/abs/2512.08017", "authors": ["Rohan Goyal", "Venkatesan Guruswami"], "title": "Structure Theorems (and Fast Algorithms) for List Recovery of Subspace-Design Codes", "comment": null, "summary": "List recovery of error-correcting codes has emerged as a fundamental notion with broad applications across coding theory and theoretical computer science. Folded Reed-Solomon (FRS) and univariate multiplicity codes are explicit constructions which can be efficiently list-recovered up to capacity, namely a fraction of errors approaching $1-R$ where $R$ is the code rate.\n  Chen and Zhang and related works showed that folded Reed-Solomon codes and linear codes must have list sizes exponential in $1/ε$ for list-recovering from an error-fraction $1-R-ε$. These results suggest that one cannot list-recover FRS codes in time that is also polynomial in $1/ε$. In contrast to such limitations, we show, extending algorithmic advances of Ashvinkumar, Habib, and Srivastava for list decoding, that even if the lists in the case of list-recovery are large, they are highly structured. In particular, we can output a compact description of a set of size only $\\ell^{O((\\log \\ell)/ε)}$ which contains the relevant list, while running in time only polynomial in $1/ε$ (the previously known compact description due to Guruswami and Wang had size $\\approx n^{\\ell/ε}$). We also improve on the state-of-the-art algorithmic results for the task of list-recovery."}
{"id": "2512.08111", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.08111", "abs": "https://arxiv.org/abs/2512.08111", "authors": ["Qi Sun", "Jingru Zhang"], "title": "The Bichromatic Two-Center Problem on Graphs", "comment": null, "summary": "In this paper, we study the (weighted) bichromatic two-center problem on graphs. The input consists of a graph $G$ of $n$ (weighted) vertices and $m$ edges, and a set $\\mathcal{P}$ of pairs of distinct vertices, where no vertex appears in more than one pair. The problem aims to find two points (i.e., centers) on $G$ by assigning vertices of each pair to different centers so as to minimize the maximum (weighted) distance of vertices to their assigned centers (so that the graph can be bi-colored with this goal). To the best of our knowledge, this problem has not been studied on graphs, including tree graphs. In this paper, we propose an $O(m^2n\\log n\\log mn)$ algorithm for solving the problem on an undirected graph provided with the distance matrix, an $O(n\\log n)$-time algorithm for the problem on trees, and a linear-time approach for the unweighted tree version."}
{"id": "2512.07901", "categories": ["cs.GT", "cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.07901", "abs": "https://arxiv.org/abs/2512.07901", "authors": ["Kevin Vallier"], "title": "The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators", "comment": "Draft manuscript, 30k words. Companion to Agentic Capital. Submitted to establish priority", "summary": "This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation."}
{"id": "2512.08073", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08073", "abs": "https://arxiv.org/abs/2512.08073", "authors": ["Jianping Zhang", "Han Qin", "Nathaniel Huber-Fliflet"], "title": "Detecting Privileged Documents by Ranking Connected Network Entities", "comment": null, "summary": "This paper presents a link analysis approach for identifying privileged documents by constructing a network of human entities derived from email header metadata. Entities are classified as either counsel or non-counsel based on a predefined list of known legal professionals. The core assumption is that individuals with frequent interactions with lawyers are more likely to participate in privileged communications. To quantify this likelihood, an algorithm assigns a score to each entity within the network. By utilizing both entity scores and the strength of their connections, the method enhances the identification of privileged documents. Experimental results demonstrate the algorithm's effectiveness in ranking legal entities for privileged document detection."}
{"id": "2512.08034", "categories": ["cs.IT", "eess.SP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.08034", "abs": "https://arxiv.org/abs/2512.08034", "authors": ["Zilu Zhao", "Fangqing Xiao", "Dirk Slock"], "title": "Expectations in Expectation Propagation", "comment": "9 pages, 2 figures, will be submitted to asilomar25", "summary": "Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have \"negative\" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.\n  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.\n  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values."}
{"id": "2512.08350", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.08350", "abs": "https://arxiv.org/abs/2512.08350", "authors": ["Zeev Nutov"], "title": "A tight example for approximation ratio 5 for covering small cuts by the primal-dual method", "comment": null, "summary": "In the Small Cuts Cover problem we seek to cover by a min-cost edge-set the set family of cuts of size/capacity $<k$ of a graph. Recently, Simmons showed that the primal-dual algorithm of Williamson, Goemans, Mihail, and Vazirani achieves approximation ratio $5$ for this problem, and asked whether this bound is tight. We will answer this question positively, by providing an example in which the ratio between the solution produced by the primal-dual algorithm and the optimum is arbitrarily close to $5$."}
{"id": "2512.08096", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.08096", "abs": "https://arxiv.org/abs/2512.08096", "authors": ["Georgios Chionas", "Olga Gorelkina", "Piotr Krysta", "Rida Laraki"], "title": "Selling Privacy in Blockchain Transactions", "comment": "20 pages, The 21st Conference on Web and Internet Economics (WINE 2025)", "summary": "We study methods to enhance privacy in blockchain transactions from an economic angle. We consider mechanisms for privacy-aware users whose utility depends not only on the outcome of the mechanism but also negatively on the exposure of their economic preferences. Specifically, we study two auction-theoretic settings with privacy-aware users. First, we analyze an order flow auction, where a user auctions off to specialized agents, called searchers, the right to execute her transaction while maintaining a degree of privacy. We examine how the degree of privacy affects the revenue of the auction and, broadly, the net utility of the privacy-aware user. In this new setting, we describe the optimal auction, which is a sealed-bid auction. Subsequently, we analyze a variant of a Dutch auction in which the user gradually decreases the price and the degree of privacy until the transaction is sold. We compare the revenue of this auction to that of the optimal one as a function of the number of communication rounds. Then, we introduce a two-sided market - a privacy marketplace - with multiple users selling their transactions under their privacy preferences to multiple searchers. We propose a posted-price mechanism for the two-sided market that guarantees constant approximation of the optimal social welfare while maintaining incentive compatibility (from both sides of the market) and budget balance. This work builds on the emerging line of research that attempts to improve the performance of economic mechanisms by appending cryptographic primitives to them."}
{"id": "2512.08078", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08078", "abs": "https://arxiv.org/abs/2512.08078", "authors": ["Qiang Mao", "Han Qin", "Robert Neary", "Charles Wang", "Fusheng Wei", "Jianping Zhang", "Nathaniel Huber-Fliflet"], "title": "A Comparative Study of Retrieval Methods in Azure AI Search", "comment": null, "summary": "Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future."}
{"id": "2512.08157", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.08157", "abs": "https://arxiv.org/abs/2512.08157", "authors": ["Lei Xie", "Hengtao He", "Yifeng Xiong", "Fan Liu", "Shi Jin"], "title": "Adaptive Matched Filtering for Sensing With Communication Signals in Cluttered Environments", "comment": null, "summary": "This paper investigates the performance of the adaptive matched filtering (AMF) in cluttered environments, particularly when operating with superimposed signals. Since the instantaneous signal-to-clutter-plus-noise ratio (SCNR) is a random variable dependent on the data payload, using it directly as a design objective poses severe practical challenges, such as prohibitive computational burdens and signaling overhead. To address this, we propose shifting the optimization objective from an instantaneous to a statistical metric, which focuses on maximizing the average SCNR over all possible payloads. Due to its analytical intractability, we leverage tools from random matrix theory (RMT) to derive an asymptotic approximation for the average SCNR, which remains accurate even in moderate-dimensional regimes. A key finding from our theoretical analysis is that, for a fixed modulation basis, the PSK achieves a superior average SCNR compared to QAM and the pure Gaussian constellation. Furthermore, for any given constellation, the OFDM achieves a higher average SCNR than SC and AFDM. Then, we propose two pilot design schemes to enhance system performance: a Data-Payload-Dependent (DPD) scheme and a Data-Payload-Independent (DPI) scheme. The DPD approach maximizes the instantaneous SCNR for each transmission. Conversely, the DPI scheme optimizes the average SCNR, offering a flexible trade-off between sensing performance and implementation complexity. Then, we develop two dedicated optimization algorithms for DPD and DPI schemes. In particular, for the DPD problem, we employ fractional optimization and the KKT conditions to derive a closed-form solution. For the DPI problem, we adopt a manifold optimization approach to handle the inherent rank-one constraint efficiently. Simulation results validate the accuracy of our theoretical analysis and demonstrate the effectiveness of the proposed methods."}
{"id": "2512.08376", "categories": ["cs.DS", "cs.IT", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08376", "abs": "https://arxiv.org/abs/2512.08376", "authors": ["Gunjan Kumar", "Yash Pote", "Jonathan Scarlett"], "title": "A Distribution Testing Approach to Clustering Distributions", "comment": null, "summary": "We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\\varepsilon)$ (up to an $O(\\log k)$ factor) for all regimes."}
{"id": "2512.08106", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.08106", "abs": "https://arxiv.org/abs/2512.08106", "authors": ["Sara Jalili Shani", "Kris Joseph", "Michael B. McNally", "James R. Wright"], "title": "Beyond Revenue and Welfare: Counterfactual Analysis of Spectrum Auctions with Application to Canada's 3800MHz Allocation", "comment": null, "summary": "Spectrum auctions are the primary mechanism through which governments allocate scarce radio frequencies, with outcomes that shape competition, coverage, and innovation in telecommunications markets. While traditional models of spectrum auctions often rely on strong equilibrium assumptions, we take a more parsimonious approach by modeling bidders as myopic and straightforward: in each round, firms simply demand the bundle that maximizes their utility given current prices. Despite its simplicity, this model proves effective in predicting the outcomes of Canada's 2023 auction of 3800 MHz spectrum licenses. Using detailed round-by-round bidding data, we estimate bidders' valuations through a linear programming framework and validate that our model reproduces key features of the observed allocation and price evolution. We then use these estimated valuations to simulate a counterfactual auction under an alternative mechanism that incentivizes deployment in rural and remote regions, aligning with one of the key objectives set out in the Canadian Telecommunications Act. The results show that the proposed mechanism substantially improves population coverage in underserved areas. These findings demonstrate that a behavioral model with minimal assumptions is sufficient to generate reliable counterfactual predictions, making it a practical tool for policymakers to evaluate how alternative auction designs may influence future outcomes. In particular, our study demonstrates a method for counterfactual mechanism design, providing a framework to evaluate how alternative auction rules could advance policy goals such as equitable deployment across Canada."}
{"id": "2512.08079", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08079", "abs": "https://arxiv.org/abs/2512.08079", "authors": ["Qiang Mao", "Fusheng Wei", "Robert Neary", "Charles Wang", "Han Qin", "Jianping Zhang", "Nathaniel Huber-Fliflet"], "title": "Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery", "comment": null, "summary": "The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections."}
{"id": "2512.08332", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.08332", "abs": "https://arxiv.org/abs/2512.08332", "authors": ["Sung Hoon Lim", "Daewon Seo"], "title": "On the Fundamental Tradeoff of Joint Communication and QCD: The Monostatic Case", "comment": null, "summary": "This paper investigates the fundamental tradeoff between communication and quickest change detection (QCD) in integrated sensing and communication (ISAC) systems under a monostatic setup. We introduce a novel Joint Communication and quickest Change subblock coding Strategy (JCCS) that leverages feedback to adapt coding dynamically based on real-time state estimation. The achievable rate-delay region is characterized using state-dependent mutual information and KL divergence, providing a comprehensive framework for analyzing the interplay between communication performance and detection delay. Moreover, we provide a partial converse demonstrating the asymptotic optimality of the proposed detection algorithm within the JCCS framework. To illustrate the practical implications, we analyze binary and MIMO Gaussian channels, revealing insights into achieving optimal tradeoffs in ISAC system design."}
{"id": "2512.08392", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.08392", "abs": "https://arxiv.org/abs/2512.08392", "authors": ["Frank Bauernöppel", "Jörg-Rüdiger Sack"], "title": "Finding All Bounded-Length Simple Cycles in a Directed Graphs -- Revisited", "comment": "11 pages, 9 figures", "summary": "In 2021, Gupta and Suzumura proposed a novel algorithm for enumerating all bounded-length simple cycles in directed graphs. In this work, we present concrete examples demonstrating that the proposed algorithm fails to enumerate certain valid cycles. Via these examples, we perform a detailed analysis pinpointing the specific points at which the proofs exhibit logical gaps. Furthermore, we propose a corrected formulation that resolves these issues while preserving the desirable property that the algorithm's computational complexity remains $O((c + 1) \\cdot k \\cdot (n + e))$ where $c$ is the number of simple cycles of a specified maximum length $k$, and $n$ and $e$ the number of graph nodes and edges respectively."}
{"id": "2512.08132", "categories": ["cs.GT", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.08132", "abs": "https://arxiv.org/abs/2512.08132", "authors": ["Kyriakos Lotidis", "Panayotis Mertikopoulos", "Nicholas Bambos", "Jose Blanchet"], "title": "Multi-agent learning under uncertainty: Recurrence vs. concentration", "comment": "44 pages, 17 figures", "summary": "In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games -- one in continuous and one in discrete time with the aim of characterizing the long-run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone -- underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty."}
{"id": "2512.08083", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08083", "abs": "https://arxiv.org/abs/2512.08083", "authors": ["Keith Huffman", "Jianping Zhang", "Nathaniel Huber-Fliflet", "Fusheng Wei", "Peter Gronvall"], "title": "Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters", "comment": null, "summary": "In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions."}
{"id": "2512.08352", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.08352", "abs": "https://arxiv.org/abs/2512.08352", "authors": ["Ying Zhang", "Fan Liu", "Yifeng Xiong", "Weijie Yuan", "Shuangyang Li", "Le Zheng", "Tony Xiao Han", "Christos Masouros", "Shi Jin"], "title": "On Discrete Ambiguity Functions of Random Communication Waveforms", "comment": "18 pages, 2 figures", "summary": "This paper provides a fundamental characterization of the discrete ambiguity functions (AFs) of random communication waveforms under arbitrary orthonormal modulation with random constellation symbols, which serve as a key metric for evaluating the delay-Doppler sensing performance in future ISAC applications. A unified analytical framework is developed for two types of AFs, namely the discrete periodic AF (DP-AF) and the fast-slow time AF (FST-AF), where the latter may be seen as a small-Doppler approximation of the DP-AF. By analyzing the expectation of squared AFs, we derive exact closed-form expressions for both the expected sidelobe level (ESL) and the expected integrated sidelobe level (EISL) under the DP-AF and FST-AF formulations. For the DP-AF, we prove that the normalized EISL is identical for all orthogonal waveforms. To gain structural insights, we introduce a matrix representation based on the finite Weyl-Heisenberg (WH) group, where each delay-Doppler shift corresponds to a WH operator acting on the ISAC signal. This WH-group viewpoint yields sharp geometric constraints on the lowest sidelobes: The minimum ESL can only occur along a one-dimensional cut or over a set of widely dispersed delay-Doppler bins. Consequently, no waveform can attain the minimum ESL over any compact two-dimensional region, leading to a no-optimality (no-go) result under the DP-AF framework. For the FST-AF, the closed-form ESL and EISL expressions reveal a constellation-dependent regime governed by its kurtosis: The OFDM modulation achieves the minimum ESL for sub-Gaussian constellations, whereas the OTFS waveform becomes optimal for super-Gaussian constellations. Finally, four representative waveforms, namely, SC, OFDM, OTFS, and AFDM, are examined under both frameworks, and all theoretical results are verified through numerical examples."}
{"id": "2512.08583", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.08583", "abs": "https://arxiv.org/abs/2512.08583", "authors": ["Jesper Nederlof"], "title": "Weighted $k$-Path and Other Problems in Almost $O^*(2^k)$ Deterministic Time via Dynamic Representative Sets", "comment": "22 pages, to appear at FOCS 2025 (online video available at FOCS youtube channel)", "summary": "We present a data structure that we call a Dynamic Representative Set. In its most basic form, it is given two parameters $0< k < n$ and allows us to maintain a representation of a family $\\mathcal{F}$ of subsets of $\\{1,\\ldots,n\\}$. It supports basic update operations (unioning of two families, element convolution) and a query operation that determines for a set $B \\subseteq \\{1,\\ldots,n\\}$ whether there is a set $A \\in \\mathcal{F}$ of size at most $k-|B|$ such that $A$ and $B$ are disjoint. After $2^{k+O(\\sqrt{k}\\log^2k)}n \\log n$ preprocessing time, all operations use $2^{k+O(\\sqrt{k}\\log^2k)}\\log n$ time.\n  Our data structure has many algorithmic consequences that improve over previous works. One application is a deterministic algorithm for the Weighted Directed $k$-Path problem, one of the central problems in parameterized complexity. Our algorithm takes as input an $n$-vertex directed graph $G=(V,E)$ with edge lengths and an integer $k$, and it outputs the minimum edge length of a path on $k$ vertices in $2^{k+O(\\sqrt{k}\\log^2k)}(n+m)\\log n$ time (in the word RAM model where weights fit into a single word). Modulo the lower order term $2^{O(\\sqrt{k}\\log^2k)}$, this answers a question that has been repeatedly posed as a major open problem in the field."}
{"id": "2512.08138", "categories": ["cs.GT", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.08138", "abs": "https://arxiv.org/abs/2512.08138", "authors": ["Kyriakos Lotidis", "Panayotis Mertikopoulos", "Nicholas Bambos", "Jose Blanchet"], "title": "Robust equilibria in continuous games: From strategic to dynamic robustness", "comment": "33 pages, 5 figures", "summary": "In this paper, we examine the robustness of Nash equilibria in continuous games, under both strategic and dynamic uncertainty. Starting with the former, we introduce the notion of a robust equilibrium as those equilibria that remain invariant to small -- but otherwise arbitrary -- perturbations to the game's payoff structure, and we provide a crisp geometric characterization thereof. Subsequently, we turn to the question of dynamic robustness, and we examine which equilibria may arise as stable limit points of the dynamics of \"follow the regularized leader\" (FTRL) in the presence of randomness and uncertainty. Despite their very distinct origins, we establish a structural correspondence between these two notions of robustness: strategic robustness implies dynamic robustness, and, conversely, the requirement of strategic robustness cannot be relaxed if dynamic robustness is to be maintained. Finally, we examine the rate of convergence to robust equilibria as a function of the underlying regularizer, and we show that entropically regularized learning converges at a geometric rate in games with affinely constrained action spaces."}
{"id": "2512.08398", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08398", "abs": "https://arxiv.org/abs/2512.08398", "authors": ["Jiin Park", "Hyuna Jeon", "Yoonseo Lee", "Jisu Hong", "Misuk Kim"], "title": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring", "comment": null, "summary": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management."}
{"id": "2512.08602", "categories": ["cs.IT", "math.RA"], "pdf": "https://arxiv.org/pdf/2512.08602", "abs": "https://arxiv.org/abs/2512.08602", "authors": ["Alessandro Neri", "Paolo Santonastaso"], "title": "Skew polynomial representations of matrix algebras and applications to coding theory", "comment": null, "summary": "We extend the existing skew polynomial representations of matrix algebras which are direct sum of matrix spaces over division rings. In this representation, the sum-rank distance between two tuples of matrices is captured by a weight function on their associated skew polynomials, defined through degrees and greatest common right divisors with the polynomial that defines the representation. We exploit this representation to construct new families of maximum sum-rank distance (MSRD) codes over finite and infinite fields, and over division rings. These constructions generalize many of the known existing constructions of MSRD codes as well as of optimal codes in the rank and in the Hamming metric. As a byproduct, in the case of finite fields we obtain new families of MDS codes which are linear over a subfield and whose length is close to the field size."}
{"id": "2512.08600", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.08600", "abs": "https://arxiv.org/abs/2512.08600", "authors": ["V. Arvind", "Srijan Chakraborty", "Samir Datta", "Asif Khan"], "title": "Fast exact algorithms via the Matrix Tree Theorem", "comment": null, "summary": "Fast exact algorithms are known for Hamiltonian paths in undirected and directed bipartite graphs through elegant though involved algorithms that are quite different from each other. We devise algorithms that are simple and similar to each other while having the same upper bounds. The common features of these algorithms is the use of the Matrix-Tree theorem and sieving using roots of unity.\n  Next, we use the framework to provide alternative algorithms to count perfect matchings in bipartite graphs on $n$ vertices, i.e., computing the $\\{0,1\\}$-permanent of a square $n/2 \\times n/2$ matrix which runs in a time similar to Ryser.\n  We demonstrate the flexibility of our method by counting the number of ways to vertex partition the graph into $k$-stars (a $k$-star consist of a tree with a root having $k-1$ children that are all leaves). Interestingly, our running time improves to $O^*((1+ε_k)^n)$ with $ε_k \\rightarrow 0$ as $k \\rightarrow \\infty$.\n  As an aside, making use of Björklund's algorithm for exact counting perfect matchings in general graphs, we show that the count of maximum matchings can be computed in time $O^*(2^ν)$ where $ν$ is the size of a maximum matching. The crucial ingredient here is the famous Gallai-Edmonds decomposition theorem.\n  All our algorithms run in polynomial space."}
{"id": "2512.08702", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08702", "abs": "https://arxiv.org/abs/2512.08702", "authors": ["Jinfeng Xu", "Zheyu Chen", "Shuo Yang", "Jinze Li", "Zitong Wan", "Hewei Wang", "Weijie Liu", "Yijie Li", "Edith C. H. Ngai"], "title": "VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation", "comment": "Accepted by KDD 2026", "summary": "Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec."}
{"id": "2512.08376", "categories": ["cs.DS", "cs.IT", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08376", "abs": "https://arxiv.org/abs/2512.08376", "authors": ["Gunjan Kumar", "Yash Pote", "Jonathan Scarlett"], "title": "A Distribution Testing Approach to Clustering Distributions", "comment": null, "summary": "We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\\varepsilon)$ (up to an $O(\\log k)$ factor) for all regimes."}
{"id": "2512.08742", "categories": ["cs.DS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08742", "abs": "https://arxiv.org/abs/2512.08742", "authors": ["Chase Hutton", "Adam Melrod"], "title": "Parallel Batch Dynamic Vertex Coloring in $O(\\log Δ)$ Amortized Update Time", "comment": null, "summary": "We present the first parallel batch-dynamic algorithm for maintaining a proper $(Δ+ 1)$-vertex coloring. Our approach builds on a new sequential dynamic algorithm inspired by the work of Bhattacharya et al. (SODA'18). The resulting randomized algorithm achieves $O(\\log Δ)$ expected amortized update time and, for any batch of $b$ updates, has parallel span $O(\\operatorname{polylog} b + \\operatorname{polylog} n)$ with high probability."}
