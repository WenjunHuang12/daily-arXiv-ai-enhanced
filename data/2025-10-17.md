<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 11]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.DS](#cs.DS) [Total: 3]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Information flow in multilayer perceptrons: an in-depth analysis](https://arxiv.org/abs/2510.13846)
*Giuliano Armano*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Analysing how information flows along the layers of a multilayer perceptron
is a topic of paramount importance in the field of artificial neural networks.
After framing the problem from the point of view of information theory, in this
position article a specific investigation is conducted on the way information
is processed, with particular reference to the requirements imposed by
supervised learning. To this end, the concept of information matrix is devised
and then used as formal framework for understanding the aetiology of
optimisation strategies and for studying the information flow. The underlying
research for this article has also produced several key outcomes: i) the
definition of a parametric optimisation strategy, ii) the finding that the
optimisation strategy proposed in the information bottleneck framework shares
strong similarities with the one derived from the information matrix, and iii)
the insight that a multilayer perceptron serves as a kind of "adaptor", meant
to process the input according to the given objective.

</details>


### [2] [Structure-Preserving Error-Correcting Codes for Polynomial Frames](https://arxiv.org/abs/2510.13882)
*Baigang Chen,Dongfang Zhao*

Main category: cs.IT

TL;DR: 提出了一种结构保持的可靠性层，在多项式环中操作，通过系统冗余纠正符号错误和标记擦除，无需往返或格式转换，适用于现代FFT/NTT分析和隐私保护ML


<details>
  <summary>Details</summary>
Motivation: 传统防御方法不适用于当前低延迟流水线：检测重传增加RTT，字节流ECC忽略代数结构并强制格式转换。需要一种在编码数据原始多项式环中操作的结构保持可靠性方案

Method: 构建两种互补方案：针对奇数长度使用Hensel提升BCH理想和幂等编码器；针对2的幂长度使用重复根负循环码和导数式解码。通过环自同构提供原地交织以分散突发错误

Result: 在四个帧大小N=1024,2048,4096,8192上，在符号错误率10^-6-10^-5时达到每帧故障目标10^-9，t=8-9，仅产生0.20%-1.56%开销，容忍约32-72字节未知错误突发

Conclusion: 通过将纠错与环语义对齐，从代数编码角度为多项式帧计算提供了可部署的鲁棒性

Abstract: Modern FFT/NTT analytics, coded computation, and privacy-preserving ML
interface routinely move polynomial frames across NICs, storage, and
accelerators. However, even rare silent data corruption (SDC) can flip a few
ring coefficients and cascade through downstream arithmetic. Conventional
defenses are ill-matched to current low-latency pipelines:
detect-and-retransmit adds RTTs, while byte-stream ECC ignores the algebraic
structure and forces format conversions. To that end, we propose a
structure-preserving reliability layer that operates in the encoded data's
original polynomial ring, adds a small amount of systematic redundancy, and
corrects symbol errors/flagged erasures without round-trip or format changes.
We construct two complementary schemes: one for odd length $N_{odd}$ via a
Hensel-lifted BCH ideal with an idempotent encoder, and one for power-of-two
length $N_{2^m}$ via a repeated-root negacyclic code with derivative-style
decoding. In particular, to stay robust against clustered errors, a ring
automorphism provides in-place interleaving to disperse bursts. Implementation
wise, on four frame sizes $N\!=\!1024, 2048, 4096, 8192$, we meet a per-frame
failure target of $10^{-9}$ at symbol error rates $10^{-6}\text{--}10^{-5}$
with $t\!=\!8\text{--}9$, incurring only $0.20\%\text{--}1.56\%$ overhead and
tolerating $\sim\!32\text{--}72$\,B unknown-error bursts (roughly doubled when
flagged as erasures) after interleaving. By aligning error correction with ring
semantics, we take a practical step toward deployable robustness for
polynomial-frame computations from an algebraic coding perspective.

</details>


### [3] [Location-Aided Distributed Beamforming for Near-Field Communications with Element-Wise RIS](https://arxiv.org/abs/2510.14226)
*Xiao Zheng,Wenchi Cheng,Jingqing Wang,Zhuohui Yao,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 该论文提出了一种新的元素级RIS架构和分布式位置辅助传输方案，用于解决零功率有源RIS在近场通信中的信道估计困难和离散相位约束问题。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视了RIS辅助系统中信道估计的固有困难以及实际部署中的离散相位约束，特别是在零功率有源RIS场景下，需要解决这些挑战以提升系统性能。

Method: 设计了新的元素级RIS架构，提供动态元素选择能力；基于菲涅尔衍射理论构建空间域位置到相位域波分布的映射；提出分布式波束成形设计，采用确定后对齐的相位策略，免除了基站对RIS关联信道估计的需求。

Result: 渐近分析表明，当RIS较大时，所提方案能以固定比例的反射元素实现最优增益，仿真验证了其相对于其他协议的优越性。

Conclusion: 提出的元素级RIS架构和分布式位置辅助传输方案有效解决了零功率有源RIS在近场通信中的信道估计和相位约束问题，实现了低复杂度和高性能的通信。

Abstract: Active reconfigurable intelligent surface (RIS) emerges as an effective
technique to resist the double-fading attenuation of passive RIS. By embedding
with power harvesting function, it further evolves to zero-power active RIS,
which can effectively enhance the flexibility of RIS deployment without
external power demand. Nevertheless, existing works neglected the inherent
difficulty of channel estimation (CE) for RIS-assisted systems, and the
discrete phase shift constraint in practical deployment. In this paper we
design a new element-wise RIS architecture and propose a distributed
location-aided transmission scheme with low complexity to enhance the reflected
gain for channel state information (CSI)-limited RIS-assisted near-field
communications. Specifically, the new element-wise RIS provides dynamic element
selection capability with low hardware resources. Based on Fresnel diffraction
theory, we construct the mapping from locations in space-domain to phase
distributions of waves in phase-domain and reveal the priority of elements for
harvesting and reflecting. {Then, the distributed beamforming design with the
phase of determine-then-align is proposed, where the estimation overhead
reduction stems from exempted requirements of RIS-associated CE at base station
(BS).} The asymptotic analysis indicates that the proposed scheme can achieve
the optimal gain with a fixed proportion of reflective elements when RIS is
large, followed by simulations to verify its superiority to other protocols.

</details>


### [4] [Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network](https://arxiv.org/abs/2510.14243)
*Caolu Xu,Zhiyong Chen,Meixia Tao,Li Song,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出空间计算通信(SCC)框架，通过多目标组合优化解决多用户VR在分布式移动边缘计算网络中的延迟和能耗问题，使用MO-CMPO算法生成Pareto最优解。


<details>
  <summary>Details</summary>
Motivation: 沉浸式VR应用在多用户交互场景中对延迟、能效和计算资源有严格要求，需要解决分布式移动边缘计算网络中的资源部署挑战。

Method: 提出SCC框架联合表示物理空间和虚拟空间，将资源部署建模为多目标组合优化问题，使用MO-CMPO算法（结合监督学习和强化学习微调）和稀疏图神经网络生成Pareto最优解。

Result: 在真实世界基站数据集上的仿真显示，MO-CMPO在超体积性能上优于基线方法，推理延迟显著降低，并揭示了延迟导向和能耗导向的不同部署模式。

Conclusion: SCC框架和MO-CMPO算法能有效解决多用户VR在MEC网络中的资源优化问题，为实际部署提供了实用的解决方案。

Abstract: Immersive virtual reality (VR) applications impose stringent requirements on
latency, energy efficiency, and computational resources, particularly in
multi-user interactive scenarios. To address these challenges, we introduce the
concept of spatial computing communications (SCC), a framework designed to meet
the latency and energy demands of multi-user VR over distributed mobile edge
computing (MEC) networks. SCC jointly represents the physical space, defined by
users and base stations, and the virtual space, representing shared immersive
environments, using a probabilistic model of user dynamics and resource
requirements. The resource deployment task is then formulated as a
multi-objective combinatorial optimization (MOCO) problem that simultaneously
minimizes system latency and energy consumption across distributed MEC
resources. To solve this problem, we propose MO-CMPO, a multi-objective
consistency model with policy optimization that integrates supervised learning
and reinforcement learning (RL) fine-tuning guided by preference weights.
Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates
Pareto-optimal solutions. Simulations with real-world New Radio base station
datasets demonstrate that MO-CMPO achieves superior hypervolume performance and
significantly lower inference latency than baseline methods. Furthermore, the
analysis reveals practical deployment patterns: latency-oriented solutions
favor local MEC execution to reduce transmission delay, while energy-oriented
solutions minimize redundant placements to save energy.

</details>


### [5] [Reconfigurable Intelligent Surface-Enabled Channel Signature Modulation](https://arxiv.org/abs/2510.14290)
*M. A. Teeti*

Main category: cs.IT

TL;DR: 提出RIS-CSM方案，通过将RIS划分为不相交组并使用预定义二进制反射模式生成不同信道签名，在接收端实现索引调制，无需RIS波束成形。


<details>
  <summary>Details</summary>
Motivation: 为可重构智能表面设计轻量级索引调制方案，实现简单的信道估计和可扩展的频谱效率。

Method: 将N单元RIS划分为不相交组，每组使用预定义二进制反射模式生成不同信道签名，信息嵌入在签名索引中。

Result: 推导了误码概率上界和容量分析，显示分集阶数为n_R，编码增益与N成正比。瑞利衰落下的仿真验证了理论分析，且RIS元素空间相关性在低频谱效率下可提升性能。

Conclusion: RIS-CSM是一种有效的轻量级索引调制方案，具有简单信道估计和可扩展频谱效率的优势。

Abstract: This work proposes RIS-enabled channel signature modulation (RIS-CSM), a
lightweight index modulation scheme for reconfigurable intelligent surfaces
(RIS). An N-element RIS is partitioned into disjoint groups, each employing
predetermined binary reflection patterns to generate distinct channel
signatures at an $n_R$-antenna receiver, without RIS-side beamforming.
Information is embedded in the indices of these signatures, enabling simple
channel estimation and scalable spectral efficiency. A closed-form upper bound
on error probability and capacity analysis are derived, revealing diversity
order $n_R$ and coding gain proportional to N. Simulation results under
Rayleigh fading validate the theoretical analysis. Moreover, simulations
indicate that spatial correlation among RIS elements can improve system
performance at low spectral efficiency.

</details>


### [6] [The asymptotic number of equivalence classes of linear codes with given dimension](https://arxiv.org/abs/2510.14424)
*Andrea Di Giusto,Alberto Ravagnani*

Main category: cs.IT

TL;DR: 本文研究了具有规定长度和维度的线性码等价类的渐近数量，推导了三种标准等价概念下的显式渐近公式，并建立了与布朗运动离散高斯分布的自然联系。


<details>
  <summary>Details</summary>
Motivation: 虽然给定长度的不等价码总数已被研究过，但维度随长度变化的情况尚未被考虑。本文旨在填补这一空白，研究线性码等价类在固定字母表大小和增加长度下的渐近数量。

Method: 采用渐近分析方法，推导了三种标准等价概念下的显式渐近公式，同时得到了q-二项式系数和的精确渐近表达式。

Result: 获得了线性码等价类数量的渐近公式，解决了该领域的开放性问题，并建立了与离散高斯分布的概率联系。

Conclusion: 研究为线性码等价类的渐近计数提供了完整理论框架，揭示了与概率论中布朗运动的深刻联系，为编码理论提供了新的视角。

Abstract: We investigate the asymptotic number of equivalence classes of linear codes
with prescribed length and dimension. While the total number of inequivalent
codes of a given length has been studied previously, the case where the
dimension varies as a function of the length has not yet been considered. We
derive explicit asymptotic formulas for the number of equivalence classes under
three standard notions of equivalence, for a fixed alphabet size and increasing
length. Our approach also yields an exact asymptotic expression for the sum of
all q-binomial coefficients, which is of independent interest and answers an
open question in this context. Finally, we establish a natural connection
between these asymptotic quantities and certain discrete Gaussian distributions
arising from Brownian motion, providing a probabilistic interpretation of our
results.

</details>


### [7] [Rotatable Antenna-Enhanced Beamforming: Signal Enhancement and Interference Suppression](https://arxiv.org/abs/2510.14574)
*Jie Feng,Zhenbing Liu,Junjie Dai,Hongbin Chen,Fangjiong Chen*

Main category: cs.IT

TL;DR: 本文研究了可旋转天线增强的单/多波束形成技术，通过优化天线旋转来利用新的空间自由度，显著提升阵列增益性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定方向天线阵列在不同转向角度下天线定向增益变化显著，难以有效增强信号和抑制干扰。为了突破这一限制，需要开发新的天线配置方案。

Method: 联合优化天线旋转向量和天线权重向量，在干扰方向最大阵列增益约束下最大化信号方向的最小阵列增益。针对无干扰单波束情况推导闭式解，对多波束情况提出交替优化算法。

Result: 仿真结果表明，所提出的可旋转天线方案在阵列增益方面显著优于传统的固定方向天线和各向同性天线方案。

Conclusion: 通过优化天线旋转引入新的空间自由度，可旋转天线技术能够有效提升波束形成性能，特别是在信号增强和干扰抑制方面具有明显优势。

Abstract: Conventional beamforming with fixed-orientation antenna (FOA) arrays may
struggle to effectively enhance signal and/or suppress interference due to
significant variations in antenna directive gains over different steering
angles. To break this limitation, we investigate in this paper the rotatable
antenna (RA)-enhanced single/multi-beam forming by exploiting the new spatial
degrees of freedom (DoFs) via antennas' rotation optimization. Specifically,
the antenna rotation vector (ARV) and antenna weight vector (AWV) are jointly
optimized to maximize the minimum array gain over signal directions, subject to
a given constraint on the maximum array gain over interference directions. For
the special case of single-beam forming without interference, the optimal ARV
is derived in closed-form with the maximum ratio combining (MRC) beamformer
applied to the AWV. For the general case of multi-beam forming, we propose an
efficient alternating optimization (AO) algorithm to find a high-quality
suboptimal solution by iteratively optimizing one of the ARV and AWV with the
other being fixed. Simulation results demonstrate that the proposed RA-based
scheme can significantly outperform the traditional FOA-based and isotropic
antenna (IA)-based schemes in terms of array gain.

</details>


### [8] [Task-Based Quantization for Channel Estimation in RIS Empowered MmWave Systems](https://arxiv.org/abs/2510.14649)
*Gyoseung Lee,In-soo Kim,Yonina C. Eldar,A. Lee Swindlehurst,Hyeongtaek Lee,Minje Kim,Junil Choi*

Main category: cs.IT

TL;DR: 本文研究了RIS赋能的毫米波多用户单输入多输出通信系统中的信道估计问题，针对低分辨率量化提出基于任务的量化方法，开发了两种信道估计器以在有限比特分辨率约束下提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模天线阵列和宽信号带宽中ADC的高成本和功耗，设计具有低分辨率ADC的毫米波系统是有益的。需要解决在有限比特分辨率约束下的信道估计问题。

Method: 提出基于任务的量化信道估计设计，考虑混合模拟和数字架构。开发了两种信道估计器：针对纯无源RIS元件的级联信道估计器，以及利用RIS处少量半无源元件附加信息的分离信道估计器。

Result: 数值结果表明，所提出的基于任务量化的信道估计设计优于纯数字方法，并能有效接近无限制分辨率ADC系统的性能。所提出的信道估计器在训练开销较小的情况下优于基线方法。

Conclusion: 基于任务的量化方法能够有效解决低分辨率ADC下的RIS信道估计问题，在有限比特分辨率约束下实现接近理想性能的系统表现。

Abstract: In this paper, we investigate channel estimation for reconfigurable
intelligent surface (RIS) empowered millimeter-wave (mmWave) multi-user
single-input multiple-output communication systems using low-resolution
quantization. Due to the high cost and power consumption of analog-to-digital
converters (ADCs) in large antenna arrays and for wide signal bandwidths,
designing mmWave systems with low-resolution ADCs is beneficial. To tackle this
issue, we propose a channel estimation design using task-based quantization
that considers the underlying hybrid analog and digital architecture in order
to improve the system performance under finite bit-resolution constraints. Our
goal is to accomplish a channel estimation task that minimizes the mean squared
error distortion between the true and estimated channel. We develop two types
of channel estimators: a cascaded channel estimator for an RIS with purely
passive elements, and an estimator for the separate RIS-related channels that
leverages additional information from a few semi-passive elements at the RIS
capable of processing the received signals with radio frequency chains.
Numerical results demonstrate that the proposed channel estimation designs
exploiting task-based quantization outperform purely digital methods and can
effectively approach the performance of a system with unlimited resolution
ADCs. Furthermore, the proposed channel estimators are shown to be superior to
baselines with small training overhead.

</details>


### [9] [Rate-Adaptive Spatially Coupled MacKay-Neal Codes with Thresholds Close to Capacity](https://arxiv.org/abs/2510.14843)
*Ayman Zahr,Gianluigi Liva*

Main category: cs.IT

TL;DR: 分析速率自适应的MacKay-Neal码集合的渐近性能，其中内码是原图空间耦合LDPC码，通过密度演化计算BP解码阈值，显示SC MN码集合在[0,1]速率范围内距离BIAWGN容量仅0.15 dB。


<details>
  <summary>Details</summary>
Motivation: 研究速率自适应码在完整速率范围内的性能，探索如何接近信道容量极限。

Method: 使用密度演化分析，建立并行信道模型，计算信念传播解码阈值。

Result: SC MN码集合在[0,1]速率范围内距离BIAWGN信道容量仅0.15 dB。

Conclusion: 空间耦合的MN码集合在完整速率范围内能够接近信道容量极限。

Abstract: We analyze by density evolution the asymptotic performance of rate-adaptive
MacKay-Neal (MN) code ensembles, where the inner code is a protograph spatially
coupled (SC) low-density parity-check code. By resorting to a suitably-defined
parallel channel model, we compute belief propagation decoding thresholds,
showing that SC MN code ensembles can perform within 0.15 dB from the
binary-input additive white Gaussian noise capacity over the full [0,1] rate
range.

</details>


### [10] [Rate-Adaptive Protograph-Based MacKay-Neal Codes](https://arxiv.org/abs/2510.14856)
*Ayman Zahr,Emna Ben Yacoub,Balázs Matuz,Gianluigi Liva*

Main category: cs.IT

TL;DR: 本文分析了基于原图的速率自适应MacKay-Neal码，通过外部分布匹配器和内层LDPC码实现速率调整，能在固定码长下实现宽范围速率自适应，性能接近香农极限。


<details>
  <summary>Details</summary>
Motivation: 为高速无线（光）通信链路提供一种能在固定块长度下实现速率自适应的编码方案，使用单一LDPC码集合覆盖多种码率需求。

Method: 采用外部分布匹配器与内层基于原图的LDPC码相结合的非线性编码结构，通过密度演进和错误平层分析进行性能评估。

Result: 设计的码能在1dB内接近香农极限，在宽码率范围内实现高效传输，且码率可通过调整分布匹配器参数灵活选择。

Conclusion: 该构造为采用二进制输入调制的高速无线（光）链路提供了具有恒定块长度和固定内层LDPC码的速率灵活解决方案。

Abstract: Rate-adaptive MacKay-Neal (MN) codes based on protographs are analyzed. The
code construction employs an outer distribution matcher (DM) to adapt the rate
of the scheme. The DM is coupled with an inner protograph-based low-density
parity-check (LDPC) code. The performance achievable by the resulting code
structure, that is nonlinear, is studied by means of an equivalent
communication model that reduces the problem to the analysis of the inner
(linear) LDPC code with transmission that takes place in parallel over the
communication channel, and over a suitably defined binary symmetric channel. A
density evolution analysis of protograph MN code ensembles is outlined, and it
is complemented by an error floor analysis that relies on the derivation of the
average input-output weight distribution of the inner LDPC code ensemble.
Conditions on the shape of the normalized logarithmic asymptotic input-output
weight distribution are defined, which allow discarding code ensembles with bad
error floor properties during the code design phase. Examples of code designs
are provided, showing how the use of a single LDPC code ensemble allows
operating within 1 dB from the Shannon limit over a wide range of code rates,
where the code rate is selected by tuning the DM parameters. By enabling rate
flexibility with a constant blocklength, and with a fixed LDPC code as inner
code, the construction provides an appealing solution for very high-throughput
wireless (optical) links that employ binary-input modulations.

</details>


### [11] [The Whole Is Less than the Sum of Parts: Subsystem Inconsistency in Partial Information Decomposition](https://arxiv.org/abs/2510.14864)
*Aobo Lyu,Andrew Clark,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文指出了部分信息分解(PID)框架违反整体等于部分之和原则(WESP)的根本问题，提出了针对三变量系统的系统信息分解(SID)新框架来解决该问题，但证明在四变量及以上系统中，现有格结构无法完全消除WESP不一致性。


<details>
  <summary>Details</summary>
Motivation: 部分信息分解(PID)自2010年提出以来在神经科学和隐私等领域有广泛应用，但缺乏统一理论框架，存在关键概念和技术挑战。作者发现PID违反了整体等于部分之和这一基本原则。

Method: 通过三变量系统的反例展示PID如何违反WESP原则，引入新的公理化框架SID，基于协同关系重新定义分解信息原子的求和规则，并分析四变量及以上系统中格结构的内在局限性。

Result: SID框架成功解决了三变量系统中的WESP违反问题，但证明对于四变量及以上系统，现有格结构无法完全消除WESP不一致性。

Conclusion: 基于反链格的分解方法对于一般多变量系统存在固有的不充分性，需要新的理论框架来克服这一根本限制。

Abstract: Partial Information Decomposition (PID) was proposed by Williams and Beer in
2010 as a tool for analyzing fine-grained interactions between multiple random
variables, and has since found numerous applications ranging from neuroscience
to privacy. However, a unified theoretical framework remains elusive due to key
conceptual and technical challenges. We identify and illustrate a crucial
problem: PID violates the set-theoretic principle that the whole equals the sum
of its parts (WESP). Through a counterexample in a three-variable system, we
demonstrate how such violations naturally arise, revealing a fundamental
limitation of current lattice-based PID frameworks. To address this issue, we
introduce a new axiomatic framework, termed System Information Decomposition
(SID), specifically tailored for three-variable systems. SID resolves the WESP
violation by redefining the summation rules of decomposed information atoms
based on synergistic relationships. However, we further show that for systems
with four or more variables, no partial summation approach within the existing
lattice-based structures can fully eliminate WESP inconsistencies. Our results
thus highlight the inherent inadequacy of (antichain) lattice-based
decompositions for general multivariate systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [12] [360CityGML: Realistic and Interactive Urban Visualization System Integrating CityGML Model and 360° Videos](https://arxiv.org/abs/2510.14189)
*Tatsuro Banno,Mizuki Takenawa,Leslie Wöhler,Satoshi Ikehata,Kiyoharu Aizawa*

Main category: cs.MM

TL;DR: 开发了一个结合3D城市模型和360度视频的城市场景可视化系统，通过视频帧投影实现逼真的行人视角可视化


<details>
  <summary>Details</summary>
Motivation: 为了创建更直观的行人视角城市场景可视化，让用户能够更好地理解地理空间数据

Method: 将360度步行视频与CityGML 3D城市模型对齐，并动态地将相关视频帧投影到几何体上

Result: 系统能够生成逼真的城市可视化效果，用户可以直观地从行人视角解读地理空间数据

Conclusion: 该系统成功整合了3D模型和视频数据，为城市场景可视化提供了一种新的有效方法

Abstract: We introduce a novel urban visualization system that integrates 3D urban
model (CityGML) and 360{\deg} walkthrough videos. By aligning the videos with
the model and dynamically projecting relevant video frames onto the geometries,
our system creates photorealistic urban visualizations, allowing users to
intuitively interpret geospatial data from a pedestrian view.

</details>


### [13] [Deep Compositional Phase Diffusion for Long Motion Sequence Generation](https://arxiv.org/abs/2510.14427)
*Ho Yin Au,Jie Chen,Junkun Jiang,Jingyu Xiang*

Main category: cs.MM

TL;DR: 提出Compositional Phase Diffusion框架，通过SPDM和TPDM模块在潜在运动频率域中处理语义引导和相位细节，解决多语义运动序列生成中的过渡不连续问题。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成模型在创建包含多个语义运动片段的复合序列时，难以保持运动动态在片段过渡边界处的连续性，导致尴尬的过渡和突然的伪影。

Method: 使用语义相位扩散模块(SPDM)和过渡相位扩散模块(TPDM)，在预训练的ACT-PAE建立的潜在运动频率域中，逐步将语义引导和相邻运动片段的相位细节融入扩散过程。

Result: 实验结果显示该框架在生成与输入条件语义对齐的复合运动序列方面具有竞争力，同时保持了前后运动片段之间的相位过渡连续性。

Conclusion: 该框架通过固定输入运动序列的相位参数，还可实现运动插值任务，展示了扩展到各种应用场景的潜力。

Abstract: Recent research on motion generation has shown significant progress in
generating semantically aligned motion with singular semantics. However, when
employing these models to create composite sequences containing multiple
semantically generated motion clips, they often struggle to preserve the
continuity of motion dynamics at the transition boundaries between clips,
resulting in awkward transitions and abrupt artifacts. To address these
challenges, we present Compositional Phase Diffusion, which leverages the
Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module
(TPDM) to progressively incorporate semantic guidance and phase details from
adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM
operate within the latent motion frequency domain established by the
pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them
to learn semantically important and transition-aware phase information from
variable-length motion clips during training. Experimental results demonstrate
the competitive performance of our proposed framework in generating
compositional motion sequences that align semantically with the input
conditions, while preserving phase transitional continuity between preceding
and succeeding motion clips. Additionally, motion inbetweening task is made
possible by keeping the phase parameter of the input motion sequences fixed
throughout the diffusion process, showcasing the potential for extending the
proposed framework to accommodate various application scenarios. Codes are
available at https://github.com/asdryau/TransPhase.

</details>


### [14] [Block-Partitioning Strategies for Accelerated Multi-rate Encoding in Adaptive VVC Streaming](https://arxiv.org/abs/2510.14645)
*Vignesh V Menon,Adam Wieckowski,Yiquin Liu,Benjamin Bross,Detlev Marpe*

Main category: cs.MM

TL;DR: 本文提出基于CU划分策略的单边界和双边界方法，利用参考编码的CU深度约束来指导多QP下的依赖编码，在VVC中减少冗余计算，实现编码效率提升。


<details>
  <summary>Details</summary>
Motivation: 随着超高清视频内容在自适应流媒体中的普及，多速率编码需求激增。VVC虽然提升了压缩效率，但计算复杂度高，使得多速率编码成为资源密集型任务。

Method: 提出单边界和双边界方法，利用参考编码的CU深度约束来指导依赖编码，通过VVenC工具在不同预设下进行评估。

Result: 方法实现了最高11.69%的编码时间减少，比特率开销小于0.6%。比较Pareto前沿分析显示多速率方法优于现有配置。

Conclusion: CU引导策略在自适应流媒体中具有可扩展多速率编码的潜力。

Abstract: The demand for efficient multi-rate encoding techniques has surged with the
increasing prevalence of ultra-high-definition (UHD) video content,
particularly in adaptive streaming scenarios where a single video must be
encoded at multiple bitrates to accommodate diverse network conditions. While
Versatile Video Coding (VVC) significantly improves compression efficiency, it
introduces considerable computational complexity, making multi-rate encoding a
resource-intensive task. This paper examines coding unit (CU) partitioning
strategies to minimize redundant computations in VVC while preserving high
video quality. We propose single- and double-bound approaches, leveraging CU
depth constraints from reference encodes to guide dependent encodes across
multiple QPs. These methods are evaluated using VVenC with various presets,
demonstrating consistent improvements in encoding efficiency. Our methods
achieve up to 11.69 % reduction in encoding time with minimal bitrate overhead
(<0.6 %). Comparative Pareto-front (PF) analysis highlights the superior
performance of multi-rate approaches over existing configurations. These
findings validate the potential of CU-guided strategies for scalable multi-rate
encoding in adaptive streaming.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [15] [Towards a Multimodal Stream Processing System](https://arxiv.org/abs/2510.14631)
*Uélison Jean Lopes dos Santos,Alessandro Ferri,Szilard Nistor,Riccardo Tommasini,Carsten Binnig,Manisha Luthra*

Main category: cs.DB

TL;DR: 提出将多模态大语言模型作为流处理系统的一等公民，实现跨模态实时查询处理的新一代多模态流处理系统


<details>
  <summary>Details</summary>
Motivation: 现有工作将MLLMs集成到数据库中处理多模态查询，但流处理系统由于严格的延迟和吞吐量要求需要完全不同的方法

Method: 在逻辑、物理和语义查询转换等各个层面提出新颖优化，减少模型负载以提高吞吐量同时保持准确性

Result: 原型系统\system{}利用这些优化将性能提升了一个数量级以上

Conclusion: 提出了构建可扩展高效多模态流处理系统的研究路线图，概述了开放研究挑战

Abstract: In this paper, we present a vision for a new generation of multimodal
streaming systems that embed MLLMs as first-class operators, enabling real-time
query processing across multiple modalities. Achieving this is non-trivial:
while recent work has integrated MLLMs into databases for multimodal queries,
streaming systems require fundamentally different approaches due to their
strict latency and throughput requirements. Our approach proposes novel
optimizations at all levels, including logical, physical, and semantic query
transformations that reduce model load to improve throughput while preserving
accuracy. We demonstrate this with \system{}, a prototype leveraging such
optimizations to improve performance by more than an order of magnitude.
Moreover, we discuss a research roadmap that outlines open research challenges
for building a scalable and efficient multimodal stream processing systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [16] [FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API](https://arxiv.org/abs/2510.14162)
*Juhyeong Kim,Yejin Kim,Youngbin Lee,Hyunwoo Byun*

Main category: cs.IR

TL;DR: FinAI Data Assistant使用LLM结合OpenAI Function Calling API，通过参数化查询而非完整SQL生成来实现金融数据库的自然语言查询，在可靠性、延迟和成本方面优于text-to-SQL方法。


<details>
  <summary>Details</summary>
Motivation: 解决金融数据库自然语言查询的可靠性问题，避免text-to-SQL方法的不稳定性，提供更可靠、低延迟和成本效益的解决方案。

Method: 结合大语言模型和OpenAI Function Calling API，将用户请求路由到经过验证的参数化查询库，而非生成完整SQL语句。

Result: LLM单独预测存在不可忽视的错误和前瞻偏差；股票代码映射准确率接近完美；FinAI Data Assistant在延迟、成本和可靠性方面优于text-to-SQL基准。

Conclusion: 参数化查询方法在金融数据库查询中比text-to-SQL更具优势，提供了更可靠、高效的解决方案，适合实际部署。

Abstract: We present FinAI Data Assistant, a practical approach for natural-language
querying over financial databases that combines large language models (LLMs)
with the OpenAI Function Calling API. Rather than synthesizing complete SQL via
text-to-SQL, our system routes user requests to a small library of vetted,
parameterized queries, trading generative flexibility for reliability, low
latency, and cost efficiency. We empirically study three questions: (RQ1)
whether LLMs alone can reliably recall or extrapolate time-dependent financial
data without external retrieval; (RQ2) how well LLMs map company names to stock
ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for
end-to-end database query processing. Across controlled experiments on prices
and fundamentals, LLM-only predictions exhibit non-negligible error and show
look-ahead bias primarily for stock prices relative to model knowledge cutoffs.
Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high
for S\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and
cost and higher reliability than a text-to-SQL baseline on our task suite. We
discuss design trade-offs, limitations, and avenues for deployment.

</details>


### [17] [Large Scale Retrieval for the LinkedIn Feed using Causal Language Models](https://arxiv.org/abs/2510.14223)
*Sudarshan Srinivasa Ramanujam,Antonio Alonso,Saurabh Kataria,Siddharth Dangi,Akhilesh Gupta,Birjodh Singh Tiwana,Manas Somaiya,Luke Simon,David Byrne,Sojeong Ha,Sen Zhou,Andrei Akterskii,Zhanglong Liu,Samira Sriram,Crescent Xiong,Zhoutao Pei,Angela Shao,Alex Li,Annie Xiao,Caitlin Kolb,Thomas Kistler,Zach Moore,Hamed Firooz*

Main category: cs.IR

TL;DR: LinkedIn提出了一种基于LLaMA 3大语言模型的双编码器检索方法，用于从数亿候选内容中快速检索相关推荐内容，显著提升了用户参与度。


<details>
  <summary>Details</summary>
Motivation: 在LinkedIn Feed等大规模推荐系统中，检索阶段需要从数亿候选中快速筛选出少量相关候选，传统方法难以在毫秒级延迟和数千QPS下保证质量。

Method: 使用Meta的LLaMA 3作为双编码器，仅基于文本输入生成用户和内容的高质量嵌入，包括提示设计、大规模微调技术和低延迟在线服务基础设施。

Result: 离线指标和在线A/B测试显示成员参与度显著提升，特别是对新用户效果更明显，表明高质量推荐内容有助于用户留存。

Conclusion: 这项工作证明了生成式语言模型可以有效地应用于工业级实时高吞吐量检索系统中。

Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval
stage is critical for narrowing hundreds of millions of potential candidates to
a manageable subset for ranking. LinkedIn's Feed serves suggested content from
outside of the member's network (based on the member's topical interests),
where 2000 candidates are retrieved from a pool of hundreds of millions
candidate with a latency budget of a few milliseconds and inbound QPS of
several thousand per second. This paper presents a novel retrieval approach
that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual
encoder to generate high quality embeddings for both users (members) and
content (items), using only textual input. We describe the end to end pipeline,
including prompt design for embedding generation, techniques for fine-tuning at
LinkedIn's scale, and infrastructure for low latency, cost effective online
serving. We share our findings on how quantizing numerical features in the
prompt enables the information to get properly encoded in the embedding,
facilitating greater alignment between the retrieval and ranking layer. The
system was evaluated using offline metrics and an online A/B test, which showed
substantial improvements in member engagement. We observed significant gains
among newer members, who often lack strong network connections, indicating that
high-quality suggested content aids retention. This work demonstrates how
generative language models can be effectively adapted for real time, high
throughput retrieval in industrial applications.

</details>


### [18] [Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation](https://arxiv.org/abs/2510.14257)
*Lingyu Mu,Hao Deng,Haibo Xing,Kaican Lin,Zhitong Zhu,Yu Zhang,Xiaoyi Zeng,Zhengxiao Liu,Zheng Lin,Jinxin Hu*

Main category: cs.IR

TL;DR: CoCo框架通过动态构建用户特定的上下文知识嵌入，解决了传统推荐系统中静态模式提示机制的局限性，实现了语义和行为潜在维度的深度融合。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用静态模式提示机制存在两个主要问题：忽视用户偏好的多面性，以及语义知识表示与行为特征空间的浅层对齐，未能实现全面的潜在空间整合。

Method: CoCo是一个端到端框架，通过双机制方法动态构建用户特定的上下文知识嵌入，包含自适应知识融合和矛盾解决模块，实现语义和行为潜在维度的深度整合。

Result: 在多个基准数据集和企业级电商平台上的实验评估显示，CoCo在推荐准确率上比七种前沿方法最高提升8.58%。在生产广告系统中的部署带来了1.91%的销售增长。

Conclusion: CoCo凭借其模块化设计和模型无关架构，为需要知识增强推理和个性化适应的下一代推荐系统提供了通用解决方案。

Abstract: The integration of large language models (LLMs) into recommendation systems
has revealed promising potential through their capacity to extract world
knowledge for enhanced reasoning capabilities. However, current methodologies
that adopt static schema-based prompting mechanisms encounter significant
limitations: (1) they employ universal template structures that neglect the
multi-faceted nature of user preference diversity; (2) they implement
superficial alignment between semantic knowledge representations and behavioral
feature spaces without achieving comprehensive latent space integration. To
address these challenges, we introduce CoCo, an end-to-end framework that
dynamically constructs user-specific contextual knowledge embeddings through a
dual-mechanism approach. Our method realizes profound integration of semantic
and behavioral latent dimensions via adaptive knowledge fusion and
contradiction resolution modules. Experimental evaluations across diverse
benchmark datasets and an enterprise-level e-commerce platform demonstrate
CoCo's superiority, achieving a maximum 8.58% improvement over seven
cutting-edge methods in recommendation accuracy. The framework's deployment on
a production advertising system resulted in a 1.91% sales growth, validating
its practical effectiveness. With its modular design and model-agnostic
architecture, CoCo provides a versatile solution for next-generation
recommendation systems requiring both knowledge-enhanced reasoning and
personalized adaptation.

</details>


### [19] [Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm](https://arxiv.org/abs/2510.14321)
*Jianting Tang,Dongshuai Li,Tao Wen,Fuyu Lv,Dan Ou,Linli Xu*

Main category: cs.IR

TL;DR: 提出LREM模型，通过将推理过程整合到表示学习中，解决密集检索中语义准确性问题。对于困难查询，先进行推理理解，再生成推理增强的查询嵌入，显著提升检索准确性。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索模型采用直接嵌入方法，语义准确性不足，且容易捕获训练数据中的统计共现模式，偏向浅层词汇和语义匹配。对于词汇差异大的困难查询，性能显著下降。

Method: 采用两阶段训练：第一阶段通过SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习进一步优化推理轨迹。

Result: 广泛的离线和在线实验验证了LREM的有效性，自2025年8月起已在中国最大电商平台部署。

Conclusion: LREM通过集成推理过程到表示学习中，有效弥合原始查询与目标商品之间的语义鸿沟，显著提高检索准确性。

Abstract: In modern e-commerce search systems, dense retrieval has become an
indispensable component. By computing similarities between query and item
(product) embeddings, it efficiently selects candidate products from
large-scale repositories. With the breakthroughs in large language models
(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs
for more accurate text modeling. However, these models still adopt
direct-embedding methods, and the semantic accuracy of embeddings remains
inadequate. Therefore, contrastive learning is heavily employed to achieve
tight semantic alignment between positive pairs. Consequently, such models tend
to capture statistical co-occurrence patterns in the training data, biasing
them toward shallow lexical and semantic matches. For difficult queries
exhibiting notable lexical disparity from target items, the performance
degrades significantly. In this work, we propose the Large Reasoning Embedding
Model (LREM), which novelly integrates reasoning processes into representation
learning. For difficult queries, LREM first conducts reasoning to achieve a
deep understanding of the original query, and then produces a
reasoning-augmented query embedding for retrieval. This reasoning process
effectively bridges the semantic gap between original queries and target items,
significantly improving retrieval accuracy. Specifically, we adopt a two-stage
training process: the first stage optimizes the LLM on carefully curated
Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary
reasoning and embedding capabilities, and the second stage further refines the
reasoning trajectories via reinforcement learning (RL). Extensive offline and
online experiments validate the effectiveness of LREM, leading to its
deployment on China's largest e-commerce platform since August 2025.

</details>


### [20] [Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations](https://arxiv.org/abs/2510.14330)
*Yuto Nakamizo,Ryuhei Miyazato,Hikaru Tanabe,Ryuta Yamakura,Kiori Hatanaka*

Main category: cs.IR

TL;DR: 本文提出了在KDD Cup 2025 Meta CRAG-MM挑战赛中获得第5名的解决方案，通过训练基于逻辑回归的幻觉检测模型来减少VLM中的幻觉，虽然牺牲了部分正确答案但显著降低了幻觉率。


<details>
  <summary>Details</summary>
Motivation: 由于错误答案会导致负分，该方案专注于减少视觉语言模型内部表示中的幻觉问题，以提高在视觉问答任务中的准确性。

Method: 使用隐藏状态和特定注意力头的输出训练逻辑回归幻觉检测模型，并采用这些模型的集成方法。

Result: 该方法虽然牺牲了一些正确答案，但显著减少了幻觉，最终在最终排行榜上名列前茅。

Conclusion: 通过幻觉检测模型的集成策略，有效降低了VLM中的幻觉问题，在CRAG-MM基准测试中取得了优异成绩。

Abstract: This paper presents the 5th place solution by our team, y3h2, for the Meta
CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question
answering (VQA) dataset focused on factual questions about images, including
egocentric images. The competition was contested based on VQA accuracy, as
judged by an LLM-based automatic evaluator. Since incorrect answers result in
negative scores, our strategy focused on reducing hallucinations from the
internal representations of the VLM. Specifically, we trained logistic
regression-based hallucination detection models using both the hidden_state and
the outputs of specific attention heads. We then employed an ensemble of these
models. As a result, while our method sacrificed some correct answers, it
significantly reduced hallucinations and allowed us to place among the top
entries on the final leaderboard. For implementation details and code, please
refer to
https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.

</details>


### [21] [GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation](https://arxiv.org/abs/2510.14626)
*Zhibo Wu,Yunfan Wu,Quan Liu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: GemiRec是一个多兴趣推荐框架，通过兴趣量化和兴趣生成解决兴趣坍塌和兴趣演化建模不足的问题，已在工业环境中部署。


<details>
  <summary>Details</summary>
Motivation: 解决多兴趣推荐中的两个核心问题：兴趣坍塌（多个用户表示同质化）和兴趣演化建模不足（难以捕捉用户历史行为中未出现的潜在兴趣）。

Method: 提出GemiRec框架，包含三个模块：兴趣字典维护模块（IDMM）维护共享量化兴趣字典；多兴趣后验分布模块（MIPDM）使用生成模型捕捉用户未来兴趣分布；多兴趣检索模块（MIRM）使用多个用户兴趣表示进行物品检索。

Result: 理论和实证分析以及广泛实验证明了该方法的优势和有效性，已于2025年3月在工业环境中部署。

Conclusion: GemiRec通过框架级改进有效解决了多兴趣推荐中的关键挑战，具有实际工业应用价值。

Abstract: Multi-interest recommendation has gained attention, especially in industrial
retrieval stage. Unlike classical dual-tower methods, it generates multiple
user representations instead of a single one to model comprehensive user
interests. However, prior studies have identified two underlying limitations:
the first is interest collapse, where multiple representations homogenize. The
second is insufficient modeling of interest evolution, as they struggle to
capture latent interests absent from a user's historical behavior. We begin
with a thorough review of existing works in tackling these limitations. Then,
we attempt to tackle these limitations from a new perspective. Specifically, we
propose a framework-level refinement for multi-interest recommendation, named
GemiRec. The proposed framework leverages interest quantization to enforce a
structural interest separation and interest generation to learn the evolving
dynamics of user interests explicitly. It comprises three modules: (a) Interest
Dictionary Maintenance Module (IDMM) maintains a shared quantized interest
dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a
generative model to capture the distribution of user future interests. (c)
Multi-Interest Retrieval Module (MIRM) retrieves items using multiple
user-interest representations. Both theoretical and empirical analyses, as well
as extensive experiments, demonstrate its advantages and effectiveness.
Moreover, it has been deployed in production since March 2025, showing its
practical value in industrial applications.

</details>


### [22] [MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs](https://arxiv.org/abs/2510.14629)
*Jiani Huang,Xingchen Zou,Lianghao Xia,Qing Li*

Main category: cs.IR

TL;DR: MR.Rec是一个结合记忆与推理的LLM推荐框架，通过检索增强生成系统实现个性化，集成推理增强记忆检索，并使用强化学习训练LLM自主优化记忆利用和推理策略。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在推荐系统中面临的深度个性化和智能推理挑战，特别是交互场景下动态用户偏好捕捉和主动推理能力不足的问题。

Method: 开发全面的RAG系统进行高效索引和检索外部记忆；集成推理增强记忆检索；设计强化学习框架训练LLM自主学习记忆利用和推理优化策略。

Result: 在多个指标上显著优于最先进的基线方法，验证了其在提供智能和个性化推荐方面的有效性。

Conclusion: 通过动态记忆检索与自适应推理的结合，MR.Rec能够提供更准确、上下文感知和高度个性化的推荐。

Abstract: The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.

</details>


### [23] [Causality Enhancement for Cross-Domain Recommendation](https://arxiv.org/abs/2510.14641)
*Zhibo Wu,Yunfan Wu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出了CE-CDR框架，通过因果图建模和部分标签因果损失来增强跨域推荐，解决传统方法中负迁移和因果关系忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐方法存在两个问题：不一致的源域任务可能导致负迁移，而忽略因果关系的源域特征可能限制预测效果。直接在有因果标签的数据集上训练跨域表示是理想方案，但现实中获取无偏因果标签极具挑战。

Method: 1) 将跨域推荐重新表述为因果图；2) 启发式构建因果感知数据集；3) 推导理论上无偏的部分标签因果损失，生成增强的跨域表示；4) 将表示输入目标模型提升推荐效果。

Result: 理论和实证分析证明了CE-CDR的合理性和有效性，该框架可作为模型无关的插件通用应用。自2025年4月起已在生产环境中部署，显示出实际应用价值。

Conclusion: CE-CDR是首个探索因果增强跨域推荐的研究，通过因果图建模和无偏损失函数，有效解决了传统方法的局限性，具有理论支撑和实际应用价值。

Abstract: Cross-domain recommendation forms a crucial component in recommendation
systems. It leverages auxiliary information through source domain tasks or
features to enhance target domain recommendations. However, incorporating
inconsistent source domain tasks may result in insufficient cross-domain
modeling or negative transfer. While incorporating source domain features
without considering the underlying causal relationships may limit their
contribution to final predictions. Thus, a natural idea is to directly train a
cross-domain representation on a causality-labeled dataset from the source to
target domain. Yet this direction has been rarely explored, as identifying
unbiased real causal labels is highly challenging in real-world scenarios. In
this work, we attempt to take a first step in this direction by proposing a
causality-enhanced framework, named CE-CDR. Specifically, we first reformulate
the cross-domain recommendation as a causal graph for principled guidance. We
then construct a causality-aware dataset heuristically. Subsequently, we derive
a theoretically unbiased Partial Label Causal Loss to generalize beyond the
biased causality-aware dataset to unseen cross-domain patterns, yielding an
enriched cross-domain representation, which is then fed into the target model
to enhance target-domain recommendations. Theoretical and empirical analyses,
as well as extensive experiments, demonstrate the rationality and effectiveness
of CE-CDR and its general applicability as a model-agnostic plugin. Moreover,
it has been deployed in production since April 2025, showing its practical
value in real-world applications.

</details>


### [24] [Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?](https://arxiv.org/abs/2510.14704)
*Leonie Winter*

Main category: cs.IR

TL;DR: 该论文研究了推荐系统数据修剪对数据集特征和算法性能的影响，发现常用的核心修剪可能只保留原始用户的2%，在修剪数据上训练测试会虚高算法性能，但在未修剪测试集上这种优势消失。


<details>
  <summary>Details</summary>
Motivation: 推荐系统离线评估严重依赖数据集，许多数据集都经过修剪处理，但数据修剪对数据集特征和算法性能的影响尚未得到充分研究。

Method: 分析五个基准数据集在未修剪和五个修剪级别（5、10、20、50、100）下的情况，检查结构和分布特征，训练测试11种代表性算法，并评估在修剪训练集上训练但在未修剪数据上测试的模型。

Result: 传统算法在修剪数据上训练测试时获得更高的nDCG@10分数，但在未修剪测试集上这种优势基本消失；所有算法在未修剪数据上测试时，随着修剪级别增加性能下降。

Conclusion: 数据修剪对推荐算法性能有显著影响，在修剪数据上评估会虚高算法性能，建议在未修剪数据上进行更真实的评估。

Abstract: Offline evaluations in recommender system research depend heavily on
datasets, many of which are pruned, such as the widely used MovieLens
collections. This thesis examines the impact of data pruning - specifically,
removing users with fewer than a specified number of interactions - on both
dataset characteristics and algorithm performance. Five benchmark datasets were
analysed in both their unpruned form and at five successive pruning levels (5,
10, 20, 50, 100). For each coreset, we examined structural and distributional
characteristics and trained and tested eleven representative algorithms. To
further assess if pruned datasets lead to artificially inflated performance
results, we also evaluated models trained on the pruned train sets but tested
on unpruned data. Results show that commonly applied core pruning can be highly
selective, leaving as little as 2% of the original users in some datasets.
Traditional algorithms achieved higher nDCG@10 scores when both training and
testing on pruned data; however, this advantage largely disappeared when
evaluated on unpruned test sets. Across all algorithms, performance declined
with increasing pruning levels when tested on unpruned data, highlighting the
impact of dataset reduction on the performance of recommender algorithms.

</details>


### [25] [Cross-Scenario Unified Modeling of User Interests at Billion Scale](https://arxiv.org/abs/2510.14788)
*Manjie Xu,Cheng Chen,Xin Jia,Jingyi Zhou,Yongji Wu,Zejian Wang,Chi Zhang,Kai Zuo,Yibo Chen,Xu Tang,Yao Hu,Yixin Zhu*

Main category: cs.IR

TL;DR: 提出RED-Rec，一个面向工业级内容推荐系统的LLM增强分层推荐引擎，通过统一多场景用户兴趣表示和两塔LLM框架，在亿级用户规模上实现跨场景推荐性能提升


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统在孤立场景中优化业务指标，忽略了跨场景行为信号，难以在十亿级部署中整合LLM等先进技术，限制了捕捉平台全触点用户兴趣的能力

Method: RED-Rec采用两塔LLM框架，通过聚合和综合不同场景的行为，统一用户兴趣表示，使用场景感知的密集混合和查询策略有效融合多样化行为信号

Result: 在RedNote平台上对亿级用户进行在线A/B测试，显示在内容推荐和广告定向任务中均取得显著性能提升，并发布了百万级序列推荐数据集RED-MMU

Conclusion: 该工作推进了统一用户建模，在大规模UGC平台中实现更深入的个性化，促进更有意义的用户参与

Abstract: User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.

</details>


### [26] [A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2510.14857)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Luca Pappalardo*

Main category: cs.IR

TL;DR: 该论文提出了一个模拟框架来分析推荐系统中的反馈循环效应，发现在线零售环境中推荐算法会带来个体多样性增加但集体多样性减少的权衡，以及用户同质化问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统与用户的持续交互形成了反馈循环，这些循环既影响个体行为也塑造集体市场动态，需要系统性地理解这些效应。

Method: 使用亚马逊电子商务数据集，构建模拟框架来建模推荐系统在周期性重训练下的反馈循环，分析不同推荐算法对多样性、购买集中度和用户同质化的影响。

Result: 反馈循环增加了个体多样性，但减少了集体多样性，并将需求集中在少数热门商品上；某些推荐系统还会随时间增加用户同质化，使用户购买档案越来越相似。

Conclusion: 推荐系统设计需要在个性化与长期多样性之间取得平衡，以应对反馈循环带来的系统性权衡问题。

Abstract: Recommender systems continuously interact with users, creating feedback loops
that shape both individual behavior and collective market dynamics. This paper
introduces a simulation framework to model these loops in online retail
environments, where recommenders are periodically retrained on evolving
user-item interactions. Using the Amazon e-Commerce dataset, we analyze how
different recommendation algorithms influence diversity, purchase
concentration, and user homogenization over time. Results reveal a systematic
trade-off: while the feedback loop increases individual diversity, it
simultaneously reduces collective diversity and concentrates demand on a few
popular items. Moreover, for some recommender systems, the feedback loop
increases user homogenization over time, making user purchase profiles
increasingly similar. These findings underscore the need for recommender
designs that balance personalization with long-term diversity.

</details>


### [27] [Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report](https://arxiv.org/abs/2510.14880)
*Rikiya Takehi,Benjamin Clavié,Sean Lee,Aamir Shakir*

Main category: cs.IR

TL;DR: 提出了mxbai-edge-colbert-v0模型（17M和32M参数），旨在改进检索和延迟交互模型，支持从云端到本地设备的全尺度检索。


<details>
  <summary>Details</summary>
Motivation: 开发能够在各种设备上本地运行的检索模型，支持从大规模云端检索到本地设备检索的全尺度应用。

Method: 通过多次消融实验改进检索和延迟交互模型，并将研究成果蒸馏到小型模型中作为概念验证。

Result: mxbai-edge-colbert-v0在常见短文本基准（BEIR）上优于ColBERTv2，在长上下文任务中表现突出，具有前所未有的效率。

Conclusion: 该模型为未来实验提供了坚实的基础骨干，是小型概念验证系列的第一个版本，在性能和效率方面都有显著提升。

Abstract: In this work, we introduce mxbai-edge-colbert-v0 models, at two different
parameter counts: 17M and 32M. As part of our research, we conduct numerous
experiments to improve retrieval and late-interaction models, which we intend
to distill into smaller models as proof-of-concepts. Our ultimate aim is to
support retrieval at all scales, from large-scale retrieval which lives in the
cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a
model that we hope will serve as a solid foundation backbone for all future
experiments, representing the first version of a long series of small
proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we
conducted multiple ablation studies, of which we report the results. In terms
of downstream performance, mxbai-edge-colbert-v0 is a particularly capable
small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and
representing a large step forward in long-context tasks, with unprecedented
efficiency.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [28] [Online Proportional Apportionment](https://arxiv.org/abs/2510.14752)
*Javier Cembrano,Jose Correa,Svenja M. Griesbach,Victor Verdugo*

Main category: cs.GT

TL;DR: 该论文首次研究在线环境下的席位分配问题，提出了一个无未来信息的在线席位分配框架，分析了确定性和随机性在线分配方法的表现界限。


<details>
  <summary>Details</summary>
Motivation: 传统的席位分配被视为一次性过程，忽略了动态因素。但在许多实际场景中，动态特性起着重要作用，因此需要研究在线环境下的席位分配问题。

Method: 引入离散时间框架，n个政党在每个时间步获得一定投票份额。在线算法需在每一步不可撤销地分配规定数量的席位，确保每个政党获得其分数份额的向上或向下取整，且累计席位分配接近其累计份额。

Result: 对于确定性方法，存在线性于n的最坏情况偏差下界，且贪心方法可达此界。当n≤3时存在满足全局配额的方法。对于随机分配，当n≤3时可随机化满足全局配额的方法，确保期望比例份额。

Conclusion: 论文建立了在线席位分配的理论框架，确定了确定性方法的性能界限，并展示了随机方法在n≤3时的可行性，为在线依赖舍入程序提供了近似解决方案。

Abstract: Traditionally, the problem of apportioning the seats of a legislative body
has been viewed as a one-shot process with no dynamic considerations. While
this approach is reasonable for some settings, dynamic aspects play an
important role in many others. We initiate the study of apportionment problems
in an online setting. Specifically, we introduce a framework for proportional
apportionment with no information about the future. In this model, time is
discrete and there are $n$ parties that receive a certain share of the votes at
each time step. An online algorithm needs to irrevocably assign a prescribed
number of seats at each time, ensuring that each party receives its fractional
share rounded up or down, and that the cumulative number of seats allocated to
each party remains close to its cumulative share up to that time.
  We study deterministic and randomized online apportionment methods. For
deterministic methods, we construct a family of adversarial instances that
yield a lower bound, linear in $n$, on the worst-case deviation between the
seats allocated to a party and its cumulative share. We show that this bound is
best possible and is matched by a natural greedy method. As a consequence, a
method guaranteeing that the cumulative number of seats assigned to each party
up to any step equals its cumulative share rounded up or down (global quota)
exists if and only if $n\leq 3$. Then, we turn to randomized allocations and
show that, for $n\leq 3$, we can randomize over methods satisfying global quota
with the additional guarantee that each party receives, in expectation, its
proportional share in every step. Our proof is constructive: Any method
satisfying these properties can be obtained from a flow on a recursively
constructed network. We showcase the applicability of our results to obtain
approximate solutions in the context of online dependent rounding procedures.

</details>


### [29] [Why Instant-Runoff Voting Is So Resilient to Coalitional Manipulation: Phase Transitions in the Perturbed Culture](https://arxiv.org/abs/2510.14450)
*François Durand*

Main category: cs.GT

TL;DR: 本文分析了三种主要投票规则（多数制、两轮制和即时复选制）在联盟操纵下的脆弱性，发现在偏好集中度临界值θ_c处存在相变，并证明即时复选制由于超级孔多塞赢家的存在而对联盟操纵具有高度抵抗力。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明即时复选制对联盟操纵具有高度抵抗力，但其理论原因尚不清楚。本文旨在填补这一理论空白，分析不同投票规则对联盟操纵的脆弱性。

Method: 使用扰动文化模型分析三种主要投票规则（多数制、两轮制和即时复选制）对联盟操纵的敏感性，引入超级孔多塞赢家概念，并研究偏好集中度临界值θ_c处的相变行为。

Result: 发现每种投票规则在偏好集中度临界值θ_c处都经历相变：当偏好集中度低于θ_c时，大规模选举中联盟操纵的概率以指数速度收敛到1；高于θ_c时收敛到0。证明即时复选制的θ_c=0，即使在最小偏好集中度下也对联盟操纵具有抵抗力。

Conclusion: 超级孔多塞赢家的存在是即时复选制抵抗联盟操纵的关键因素，理论上和实证上都证明了即时复选制在最小偏好集中度下对联盟操纵具有高度抵抗力。

Abstract: Previous studies have shown that Instant-Runoff Voting (IRV) is highly
resistant to coalitional manipulation (CM), though the theoretical reasons for
this remain unclear. To address this gap, we analyze the susceptibility to CM
of three major voting rules-Plurality, Two-Round System, and IRV-within the
Perturbed Culture model. Our findings reveal that each rule undergoes a phase
transition at a critical value theta\_c of the concentration of preferences:
the probability of CM for large electorates converges exponentially fast to 1
below theta\_c and to 0 above theta\_c. We introduce the Super Condorcet Winner
(SCW), showing that its presence is a key factor of IRV's resistance to
coalitional manipulation, both theoretically and empirically. Notably, we use
this notion to prove that for IRV, theta\_c = 0, making it resistant to CM with
even minimal preference concentration.

</details>


### [30] [Co-Investment under Revenue Uncertainty Based on Stochastic Coalitional Game Theory](https://arxiv.org/abs/2510.14555)
*Amal Sakr,Andrea Araldo,Tijani Chahed,Daniel Kofman*

Main category: cs.GT

TL;DR: 提出了一种基于随机联盟博弈的移动边缘计算(MEC)共同投资方案，基础设施提供商和服务提供商组成大联盟来分担成本和分享收益，解决未来收益不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算等新服务需要大规模投资，单个利益相关者(如基础设施提供商)难以承担，而服务提供商也有部署这些服务的需求。

Method: 构建了基于鲁棒博弈理论的随机联盟博弈模型，推导了大联盟稳定性的概率下界，并使用盈利性作为共同投资的必要条件。

Result: 当服务提供商的收益规模相似且投资周期足够长时，即使存在高度不确定性，稳定性概率的下界仍然很高；但当收益高度波动时，稳定性下界可能很低但共同投资仍然盈利。

Conclusion: 该框架为MEC部署提供了一种可行的共同投资方案，在收益相似且投资周期长的情况下能保证联盟稳定性，在收益波动大时仍能通过盈利性条件确保投资可行性。

Abstract: The introduction of new services, such as Mobile Edge Computing (MEC),
requires a massive investment that cannot be assumed by a single stakeholder,
for instance the Infrastructure Provider (InP). Service Providers (SPs) however
also have an interest in the deployment of such services. We hence propose a
co-investment scheme in which all stakeholders, i.e., the InP and the SPs, form
the so-called grand coalition composed of all the stakeholders with the aim of
sharing costs and revenues and maximizing their payoffs. The challenge comes
from the fact that future revenues are uncertain. We devise in this case a
novel stochastic coalitional game formulation which builds upon robust game
theory and derive a lower bound on the probability of the stability of the
grand coalition, wherein no player can be better off outside of it. In the
presence of some correlated fluctuations of revenues however, stability can be
too conservative. In this case, we make use also of profitability, in which
payoffs of players are non-negative, as a necessary condition for
co-investment. The proposed framework is showcased for MEC deployment, where
computational resources need to be deployed in nodes at the edge of a
telecommunication network. Numerical results show high lower bound on the
probability of stability when the SPs' revenues are of similar magnitude and
the investment period is sufficiently long, even with high levels of
uncertainty. In the case where revenues are highly variable however, the lower
bound on stability can be trivially low whereas co-investment is still
profitable.

</details>


### [31] [The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain](https://arxiv.org/abs/2510.14642)
*Andrei Seoev,Leonid Gremyachikh,Anastasiia Smirnova,Yash Madhwal,Alisa Kalacheva,Dmitry Belousov,Ilia Zubov,Aleksei Smirnov,Denis Fedyanin,Vladimir Gorgadze,Yury Yanovich*

Main category: cs.GT

TL;DR: 提出了一个强化学习框架来解决Polygon Atlas区块链中MEV提取的复杂策略挑战，在部分可观察的高频环境中显著优于传统博弈论方法。


<details>
  <summary>Details</summary>
Motivation: 区块链网络中的MEV提取已从公开竞价转向密封投标竞争，传统博弈论方法因依赖完全信息和静态均衡假设而无法应对这种高频、部分可观察的环境。

Method: 开发了一个强化学习框架，包括：1）模拟Polygon Atlas拍卖随机套利机会和概率竞争的仿真环境；2）基于PPO的实时投标智能体，能在连续动作空间中制定自适应策略；3）历史条件化智能体设计。

Result: 实证验证显示，该智能体与现有搜索者一起部署时捕获49%的可用利润，替换市场领导者时捕获81%的利润，显著优于静态投标策略。

Conclusion: 强化学习在高频MEV环境中提供了关键优势，为行业参与者和协议设计者提供即时价值，而传统优化方法在此类环境中失效。

Abstract: In blockchain networks, the strategic ordering of transactions within blocks
has emerged as a significant source of profit extraction, known as Maximal
Extractable Value (MEV). The transition from spam-based Priority Gas Auctions
to structured auction mechanisms like Polygon Atlas has transformed MEV
extraction from public bidding wars into sealed-bid competitions under extreme
time constraints. While this shift reduces network congestion, it introduces
complex strategic challenges where searchers must make optimal bidding
decisions within a sub-second window without knowledge of competitor behavior
or presence. Traditional game-theoretic approaches struggle in this
high-frequency, partially observable environment due to their reliance on
complete information and static equilibrium assumptions. We present a
reinforcement learning framework for MEV extraction on Polygon Atlas and make
three contributions: (1) A novel simulation environment that accurately models
the stochastic arrival of arbitrage opportunities and probabilistic competition
in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time
constraints, capable of adaptive strategy formulation in continuous action
spaces while maintaining production-ready inference speeds; (3) Empirical
validation demonstrating our history-conditioned agent captures 49\% of
available profits when deployed alongside existing searchers and 81\% when
replacing the market leader, significantly outperforming static bidding
strategies. Our work establishes that reinforcement learning provides a
critical advantage in high-frequency MEV environments where traditional
optimization methods fail, offering immediate value for industrial participants
and protocol designers alike.

</details>


### [32] [Learnable Mixed Nash Equilibria are Collectively Rational](https://arxiv.org/abs/2510.14907)
*Geelon So,Yi-An Ma*

Main category: cs.GT

TL;DR: 该论文研究了博弈学习中非渐近稳定性的动态特性，通过均匀稳定性概念连接个体效用寻求行为与集体理性。研究发现，在非退化条件下，混合均衡的均匀稳定性与弱帕累托最优性密切相关，且这种稳定性决定了增量平滑最佳响应动态的最终迭代收敛行为。


<details>
  <summary>Details</summary>
Motivation: 扩展博弈学习研究到非渐近稳定性动态，探索个体效用寻求行为如何与集体理性相关联，解决严格均衡可能导致社会效率低下解决方案的问题。

Method: 使用均匀稳定性概念分析混合纳什均衡，在非退化条件下研究战略等价性，并分析增量平滑最佳响应动态的收敛行为。

Result: 发现混合均衡的均匀稳定性与弱帕累托最优性紧密相关：非均匀稳定均衡不是弱帕累托最优的，而局部均匀稳定均衡必须是弱帕累托最优的。均匀稳定性决定了增量平滑最佳响应动态的最终迭代收敛。

Conclusion: 个体效用寻求行为在混合纳什均衡附近会导致集体理性，这与严格均衡周围可能导致社会效率低下解决方案的动态形成对比，为博弈学习中的非渐近稳定性提供了新的理论洞见。

Abstract: We extend the study of learning in games to dynamics that exhibit
non-asymptotic stability. We do so through the notion of uniform stability,
which is concerned with equilibria of individually utility-seeking dynamics.
Perhaps surprisingly, it turns out to be closely connected to economic
properties of collective rationality. Under mild non-degeneracy conditions and
up to strategic equivalence, if a mixed equilibrium is not uniformly stable,
then it is not weakly Pareto optimal: there is a way for all players to improve
by jointly deviating from the equilibrium. On the other hand, if it is locally
uniformly stable, then the equilibrium must be weakly Pareto optimal. Moreover,
we show that uniform stability determines the last-iterate convergence behavior
for the family of incremental smoothed best-response dynamics, used to model
individual and corporate behaviors in the markets. Unlike dynamics around
strict equilibria, which can stabilize to socially-inefficient solutions,
individually utility-seeking behaviors near mixed Nash equilibria lead to
collective rationality.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [33] [A Levelset Algorithm for 3D-Tarksi](https://arxiv.org/abs/2510.14777)
*Sebastian Haslebacher,Jonas Lill*

Main category: cs.DS

TL;DR: 提出了一种新的简单算法，用于在O(log² N)时间和查询复杂度内找到单调函数F: [N]³ → [N]³的Tarski不动点，匹配现有最佳算法和查询下界。


<details>
  <summary>Details</summary>
Motivation: 寻找更简单高效的Tarski不动点算法，以匹配已知的查询复杂度下界并改进现有算法的实现复杂度。

Method: 设计了一个新的简单算法，通过O(log² N)次查询和计算来定位单调函数在三维格上的Tarski不动点。

Result: 算法在O(log² N)时间内完成，仅需O(log² N)次函数查询，与Etessami等人证明的下界和Fearnley等人的现有最优算法相匹配。

Conclusion: 成功开发了一个简单且最优的Tarski不动点算法，在三维格上达到了理论最优的查询和时间复杂度。

Abstract: We present a simple new algorithm for finding a Tarski fixed point of a
monotone function $F : [N]^3 \rightarrow [N]^3$. Our algorithm runs in
$O(\log^2 N)$ time and makes $O(\log^2 N)$ queries to $F$, matching the
$\Omega(\log^2 N)$ query lower bound due to Etessami et al.\ as well as the
existing state-of-the-art algorithm due to Fearnley et al.

</details>


### [34] [Prediction-Specific Design of Learning-Augmented Algorithms](https://arxiv.org/abs/2510.14887)
*Sizhe Li,Nicolas Christianson,Tongxin Li*

Main category: cs.DS

TL;DR: 本文提出了一种"强最优"算法框架，将预测与在线算法结合，在保持鲁棒性的同时通过预测特定设计显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有预测算法过于保守，未能充分利用问题结构来针对特定预测优化性能，需要更精细的性能标准来超越传统一致性和鲁棒性概念。

Method: 提出了强最优算法概念，开发了双层优化框架，为确定性/随机滑雪租赁和单峰搜索等经典在线问题设计了显式强最优算法。

Result: 分析揭示了通过预测特定设计最优整合预测的新结构洞见，实证验证在动态电源管理和波动性指数交易等场景中显著提升性能。

Conclusion: 预测特定的强最优算法能够显著改善各种在线决策场景的性能，为预测与在线算法的结合提供了更优化的框架。

Abstract: Algorithms with predictions} has emerged as a powerful framework to combine
the robustness of traditional online algorithms with the data-driven
performance benefits of machine-learned (ML) predictions. However, most
existing approaches in this paradigm are overly conservative, {as they do not
leverage problem structure to optimize performance in a prediction-specific
manner}. In this paper, we show that such prediction-specific performance
criteria can enable significant performance improvements over the coarser
notions of consistency and robustness considered in prior work. Specifically,
we propose a notion of \emph{strongly-optimal} algorithms with predictions,
which obtain Pareto optimality not just in the worst-case tradeoff between
robustness and consistency, but also in the prediction-specific tradeoff
between these metrics. We develop a general bi-level optimization framework
that enables systematically designing strongly-optimal algorithms in a wide
variety of problem settings, and we propose explicit strongly-optimal
algorithms for several classic online problems: deterministic and randomized
ski rental, and one-max search. Our analysis reveals new structural insights
into how predictions can be optimally integrated into online algorithms by
leveraging a prediction-specific design. To validate the benefits of our
proposed framework, we empirically evaluate our algorithms in case studies on
problems including dynamic power management and volatility-based index trading.
Our results demonstrate that prediction-specific, strongly-optimal algorithms
can significantly improve performance across a variety of online
decision-making settings.

</details>


### [35] [Tree-Like Shortcuttings of Trees](https://arxiv.org/abs/2510.14918)
*Hung Le,Lazar Milenković,Shay Solomon,Cuong Than*

Main category: cs.DS

TL;DR: 该论文研究了树状稀疏捷径（1-扩张子图）的构造，关注树状性质（树宽和树性度），提出了树状捷径的上下界，解决了关于跳径与树宽乘积的开放问题。


<details>
  <summary>Details</summary>
Motivation: 现有常数跳径树捷径虽然稀疏度高，但包含稠密子图（稀疏度Ω(log n)），这在许多应用中是一个显著缺点。论文旨在研究具有树状性质的常数跳捷径。

Method: 系统研究树状捷径，聚焦于衡量图与树距离的两个参数：树性度和树宽。提供树状捷径的新上下界，包括跳径与树宽之间的最优权衡。

Result: 为所有跳径达O(log log n)的情况提供了跳径与树宽的最优权衡，并为更大的k值提供了下界，得出跳径×树宽=Ω((log log n)²)的结果。

Conclusion: 解决了[FL22, Le23]提出的开放问题，建立了树状捷径的完整理论框架，为树捷径的树状性质提供了系统分析。

Abstract: Sparse shortcuttings of trees -- equivalently, sparse 1-spanners for tree
metrics with bounded hop-diameter -- have been studied extensively (under
different names and settings), since the pioneering works of [Yao82, Cha87,
AS87, BTS94], initially motivated by applications to range queries, online tree
product, and MST verification, to name a few. These constructions were also
lifted from trees to other graph families using known low-distortion embedding
results. The works of [Yao82, Cha87, AS87, BTS94] establish a tight tradeoff
between hop-diameter and sparsity (or average degree) for tree shortcuttings
and imply constant-hop shortcuttings for $n$-node trees with sparsity $O(\log^*
n)$. Despite their small sparsity, all known constant-hop shortcuttings contain
dense subgraphs (of sparsity $\Omega(\log n)$), which is a significant drawback
for many applications.
  We initiate a systematic study of constant-hop tree shortcuttings that are
``tree-like''. We focus on two well-studied graph parameters that measure how
far a graph is from a tree: arboricity and treewidth. Our contribution is
twofold.
  * New upper and lower bounds for tree-like shortcuttings of trees, including
an optimal tradeoff between hop-diameter and treewidth for all hop-diameter up
to $O(\log\log n)$. We also provide a lower bound for larger values of $k$,
which together yield $\text{hop-diameter}\times \text{treewidth} =
\Omega((\log\log n)^2)$ for all values of hop-diameter, resolving an open
question of [FL22, Le23]. [...]

</details>
