<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IT](#cs.IT) [Total: 9]
- [cs.DS](#cs.DS) [Total: 11]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Unending Sequential Auctions](https://arxiv.org/abs/2510.08742)
*Amir Ban*

Main category: cs.GT

TL;DR: 研究带有不确定性的无限期序贯拍卖，发现存在唯一稳态。无不确定性时类似固定价格机制，有明确阈值；引入不确定性后阈值变模糊，低价值和高价值竞标者都能获益，但拍卖者效用通常降低。


<details>
  <summary>Details</summary>
Motivation: 现实中存在大量无限期进行的序贯拍卖，新竞标者不断替换离开者，需要研究这种动态环境下的均衡状态和不确定性影响。

Method: 将序贯拍卖建模为马尔可夫过程，引入竞标者退出概率来模拟不确定性，分析稳态均衡特性。

Result: 存在唯一稳态；无不确定性时形成明确阈值，类似固定价格机制；不确定性使阈值模糊，低价值竞标者获胜概率增加，高价值竞标者预期效用提升（在一定价值范围内），但拍卖者效用通常下降。

Conclusion: 竞标者不确定性改变了序贯拍卖的均衡特性，使阈值模糊化，对竞标者有利但对拍卖者不利，揭示了动态拍卖中不确定性分配效应的重要作用。

Abstract: Sequential auctions for identical items with unit-demand, private-value
buyers are common and often occur periodically without end, as new bidders
replace departing ones. We model bidder uncertainty by introducing a
probability that a bidder must exit the auction in each period. Treating the
sequential auction as a Markov process, we demonstrate the existence of a
unique steady state.
  In the absence of uncertainty, the steady state resembles a posted-price
mechanism: bidders with values above a threshold almost surely win items by
repeatedly bidding the threshold price, while those below the threshold almost
surely do not. The equilibrium price corresponds to the threshold value that
balances supply (bidders with values above the threshold) and demand (auction
winners).
  When uncertainty is introduced, the threshold value persists but becomes less
precise, growing "fuzzier" as uncertainty increases. This uncertainty benefits
low-value bidders, those below the threshold, by giving them a significant
chance of winning. Surprisingly, high-value bidders also benefit from
uncertainty, up to a certain value limit, as it lowers equilibrium bids and
increases their expected utility. On the other hand, this bidder uncertainty
often reduces the auctioneer's utility.

</details>


### [2] [Robust autobidding for noisy conversion prediction models](https://arxiv.org/abs/2510.08788)
*Andrey Pudovikov,Alexandra Khirianova,Ekaterina Solodneva,Gleb Molodtsov,Aleksandr Katrutsa,Yuriy Dorn,Egor Samosvat*

Main category: cs.GT

TL;DR: 提出RobustBid方法，通过鲁棒优化技术处理CTR和CVR预测不确定性，在预测扰动较大时能提供更高的转化量和更低的平均点击成本。


<details>
  <summary>Details</summary>
Motivation: 数字广告拍卖系统中，CTR和CVR的预测不确定性直接影响广告主收入和竞价策略，需要解决这一问题。

Method: 利用先进的鲁棒优化技术，防止CTR/CVR估计扰动导致的竞价大误差，并推导出鲁棒优化问题的解析解以实现运行效率。

Result: 在合成数据、iPinYou和BAT基准测试中，与基线方法和RiskBid算法相比，RobustBid在CTR/CVR预测大扰动情况下能产生更高的总转化量和更低的平均点击成本。

Conclusion: RobustBid方法能有效处理预测不确定性，在预测扰动较大时优于现有方法，为广告拍卖系统提供更可靠的竞价策略。

Abstract: Managing millions of digital auctions is an essential task for modern
advertising auction systems. The main approach to managing digital auctions is
an autobidding approach, which depends on the Click-Through Rate and Conversion
Rate values. While these quantities are estimated with ML models, their
prediction uncertainty directly impacts advertisers' revenue and bidding
strategies. To address this issue, we propose RobustBid, an efficient method
for robust autobidding taking into account uncertainty in CTR and CVR
predictions. Our approach leverages advanced, robust optimization techniques to
prevent large errors in bids if the estimates of CTR/CVR are perturbed. We
derive the analytical solution of the stated robust optimization problem, which
leads to the runtime efficiency of the RobustBid method. The synthetic,
iPinYou, and BAT benchmarks are used in our experimental evaluation of
RobustBid. We compare our method with the non-robust baseline and the RiskBid
algorithm in terms of total conversion volume (TCV) and average cost-per-click
($CPC_{avg}$) performance metrics. The experiments demonstrate that RobustBid
provides bids that yield larger TCV and smaller $CPC_{avg}$ than competitors in
the case of large perturbations in CTR/CVR predictions.

</details>


### [3] [Measuring the Hidden Cost of Data Valuation through Collective Disclosure](https://arxiv.org/abs/2510.08869)
*Patrick Mesana,Gilles Caporossi,Sebastien Gambs*

Main category: cs.GT

TL;DR: 本文提出了信息披露博弈模型，分析数据估值方法在直接作为支付机制时的隐藏成本问题，并通过差分隐私机制和博弈论方法展示了数据获取成本的存在及其在成员间的分布变化。


<details>
  <summary>Details</summary>
Motivation: 数据估值方法直接作为支付机制时，边际价值接近零的贡献者可能一无所获，但其数据仍需收集和评估，这产生了隐藏成本。

Method: 引入信息披露博弈模型，数据联盟采用差分隐私机制逐步释放信息，通过数据Shapley值和多臂老虎机策略进行模拟。

Result: 在Yelp评论有用性预测任务中，证明了数据估值必然产生明确的获取成本，且数据联盟的集体披露策略改变了这一成本在成员间的分布。

Conclusion: 数据估值存在固有的获取成本，需要通过适当的披露策略来合理分配这些成本，避免对边际价值低的贡献者不公平。

Abstract: Data valuation methods assign marginal utility to each data point that has
contributed to the training of a machine learning model. If used directly as a
payout mechanism, this creates a hidden cost of valuation, in which
contributors with near-zero marginal value would receive nothing, even though
their data had to be collected and assessed. To better formalize this cost, we
introduce a conceptual and game-theoretic model, the Information Disclosure
Game, between a Data Union (sometimes also called a data trust), a member-run
agent representing contributors, and a Data Consumer (e.g., a platform). After
first aggregating members' data, the DU releases information progressively by
adding Laplacian noise under a differentially-private mechanism. Through
simulations with strategies guided by data Shapley values and multi-armed
bandit exploration, we demonstrate on a Yelp review helpfulness prediction task
that data valuation inherently incurs an explicit acquisition cost and that the
DU's collective disclosure policy changes how this cost is distributed across
members.

</details>


### [4] [Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising](https://arxiv.org/abs/2510.09084)
*Dildar Ali,Suman Benerjee,Yamuna Prasad*

Main category: cs.GT

TL;DR: 该论文研究数字广告牌分配问题，旨在最小化广告商需求与实际展示量之间的遗憾值。提出了两种解决方案：预算有效贪婪方法和随机化版本，通过实验验证了随机化方法在计算时间和遗憾最小化方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 数字广告牌提供商需要合理分配广告位给多个广告商，如果提供的展示量过多或过少都会造成损失（遗憾）。目标是设计分配策略来最小化总遗憾值。

Method: 提出了两种方法：1）预算有效贪婪方法，逐步选择广告位；2）随机化预算有效贪婪方法，在样本上计算边际收益以减少计算量。

Result: 通过真实数据集实验，发现随机化预算有效贪婪方法在合理计算时间内能有效最小化遗憾值。

Conclusion: 随机化预算有效贪婪方法在计算效率和遗憾最小化之间取得了良好平衡，适合解决数字广告牌分配问题。

Abstract: In a typical \emph{billboard advertisement} technique, a number of digital
billboards are owned by an \emph{influence provider}, and several commercial
houses approach the influence provider for a specific number of views of their
advertisement content on a payment basis. If the influence provider provides
the demanded or more influence, then he will receive the full payment else a
partial payment. In the context of an influence provider, if he provides more
or less than the advertisers demanded influence, it is a loss for him. This is
formalized as 'Regret', and naturally, in the context of the influence
provider, the goal will be to allocate the billboard slots among the
advertisers such that the total regret is minimized. In this paper, we study
this problem as a discrete optimization problem and propose two solution
approaches. The first one selects the billboard slots from the available ones
in an incremental greedy manner, and we call this method the Budget Effective
Greedy approach. In the second one, we introduce randomness in the first one,
where we do it for a sample of slots instead of calculating the marginal gains
of all the billboard slots. We analyze both algorithms to understand their time
and space complexity. We implement them with real-life datasets and conduct a
number of experiments. We observe that the randomized budget effective greedy
approach takes reasonable computational time while minimizing the regret.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [5] [Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric Retrieval](https://arxiv.org/abs/2510.08935)
*Yingyi Zhang,Pengyue Jia,Derong Xu,Yi Wen,Xianneng Li,Yichao Wang,Wenlin Zhang,Xiaopeng Li,Weinan Gan,Huifeng Guo,Yong Liu,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出了PBR框架，在检索前进行个性化查询扩展，通过P-PRF模拟用户表达风格，P-Anchor对齐用户语料结构，显著提升个性化RAG系统的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有查询扩展方法采用统一策略，忽略了用户特定的表达风格、偏好和历史上下文，导致相同文本查询在不同用户间表达意图差异巨大，限制了RAG系统在个性化场景的泛化能力。

Method: PBR框架包含两个组件：P-PRF使用用户历史生成风格对齐的伪反馈来模拟用户表达风格；P-Anchor通过图结构对齐用户语料来捕捉其语义结构，共同生成个性化查询表示。

Result: 在两个个性化基准测试中，PBR始终优于强基线方法，在PersonaBench上检索器性能提升高达10%。

Conclusion: 在检索前建模个性化对于缩小用户自适应RAG系统中的语义差距具有重要价值，证明了个性化查询扩展的有效性。

Abstract: Retrieval-Augmented Generation (RAG) critically depends on effective query
expansion to retrieve relevant information. However, existing expansion methods
adopt uniform strategies that overlook user-specific semantics, ignoring
individual expression styles, preferences, and historical context. In practice,
identical queries in text can express vastly different intentions across users.
This representational rigidity limits the ability of current RAG systems to
generalize effectively in personalized settings. Specifically, we identify two
core challenges for personalization: 1) user expression styles are inherently
diverse, making it difficult for standard expansions to preserve personalized
intent. 2) user corpora induce heterogeneous semantic structures-varying in
topical focus and lexical organization-which hinders the effective anchoring of
expanded queries within the user's corpora space. To address these challenges,
we propose Personalize Before Retrieve (PBR), a framework that incorporates
user-specific signals into query expansion prior to retrieval. PBR consists of
two components: P-PRF, which generates stylistically aligned pseudo feedback
using user history for simulating user expression style, and P-Anchor, which
performs graph-based structure alignment over user corpora to capture its
structure. Together, they produce personalized query representations tailored
for retrieval. Experiments on two personalized benchmarks show that PBR
consistently outperforms strong baselines, with up to 10% gains on PersonaBench
across retrievers. Our findings demonstrate the value of modeling
personalization before retrieval to close the semantic gap in user-adaptive RAG
systems. Our code is available at https://github.com/Zhang-Yingyi/PBR-code.

</details>


### [6] [SHERLOCK: Towards Dynamic Knowledge Adaptation in LLM-enhanced E-commerce Risk Management](https://arxiv.org/abs/2510.08948)
*Nan Lu,Yurong Hu,Jiaquan Fang,Yan Liu,Rui Dong,Yiming Wang,Rui Lin,Shaoyi Xu*

Main category: cs.IR

TL;DR: 提出了SHERLOCK框架，利用大语言模型的推理能力辅助风险调查，通过构建领域知识库、智能平台和反思优化模块，显著提升了风险管理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电子商务行业的增长加剧了影子经济参与者与风险管理团队之间的对抗动态。大量案例分析给风险分析师带来沉重负担，且分析师个体差异阻碍了统一高标准工作流程的建立。

Method: SHERLOCK框架包含三个主要组件：(1)从多模态数据提取风险管理知识并构建领域知识库；(2)基于数据飞轮范式构建智能平台，整合日常操作、专家标注和模型评估；(3)引入反思优化模块与领域知识库协作，建立快速响应机制。

Result: 在京东真实交易数据集上的实验表明，该方法显著提高了LLM分析结果的事实对齐精度和风险定位精度。基于SHERLOCK的LLM系统在京东的部署大幅提升了案件调查工作流程的效率。

Conclusion: SHERLOCK框架有效解决了风险管理中的工作负担和标准不统一问题，通过LLM推理能力和领域知识的结合，为风险调查提供了高效可靠的辅助工具。

Abstract: The growth of the e-commerce industry has intensified the adversarial
dynamics between shadow economy actors and risk management teams. Companies
often conduct risk investigations into suspicious cases to identify emerging
fraud patterns, thereby enhancing both preemptive risk prevention and post-hoc
governance. However, the sheer volume of case analyses imposes a substantial
workload on risk management analysts, as each case requires the integration of
long-term expert experience and meticulous scrutiny across multiple risk
dimensions. Additionally, individual disparities among analysts hinder the
establishment of uniform and high-standard workflows. To address these
challenges, we propose the SHERLOCK framework, which leverages the reasoning
capabilities of large language models (LLMs) to assist analysts in risk
investigations. Our approach consists of three primary components: (1)
extracting risk management knowledge from multi-modal data and constructing a
domain knowledge base (KB), (2) building an intelligent platform guided by the
data flywheel paradigm that integrates daily operations, expert annotations,
and model evaluations, with iteratively fine-tuning for preference alignment,
and (3) introducing a Reflect & Refine (R&R) module that collaborates with the
domain KB to establish a rapid response mechanism for evolving risk patterns.
Experiments conducted on the real-world transaction dataset from JD.com
demonstrate that our method significantly improves the precision of both
factual alignment and risk localization within the LLM analysis results.
Deployment of the SHERLOCK-based LLM system on JD.com has substantially
enhanced the efficiency of case investigation workflows for risk managers.

</details>


### [7] [Rethinking Reasoning in Document Ranking: Why Chain-of-Thought Falls Short](https://arxiv.org/abs/2510.08985)
*Xuan Lu,Haohang Huang,Rui Meng,Yaohui Jin,Wenjun Zeng,Xiaoyu Shen*

Main category: cs.IR

TL;DR: 本文首次系统研究了在文档重排序任务中使用推理链（CoT）的效果，发现在点式和列表式重排序中，带有推理的模型表现均不如直接预测排名的模型，尽管推理成本更高。


<details>
  <summary>Details</summary>
Motivation: 受大型推理模型启发，研究者开始将显式推理链引入基于LLM的重排序器，但推理在排序任务中的有效性尚未得到充分探索。

Method: 在点式和列表式设置下，通过监督微调和强化学习，使用多样化基准（包括推理密集型数据集BRIGHT和标准IR基准BEIR）系统评估推理增强重排序器的性能。

Result: 推理增强重排序器始终表现不如直接预测排名的模型，存在三个核心局限：点式重排序中推理破坏校准并偏向正类；列表式重排序中推理提高域内拟合但增加方差且域外泛化差；直接微调的重排序器更稳定有效。

Conclusion: 显式推理对重排序并非普遍有益，未来方向包括点式重排序的校准感知评分和设计简洁、有针对性的推理策略以减少列表式重排序中的过拟合和过度思考。

Abstract: Document reranking is a key component in information retrieval (IR), aimed at
refining initial retrieval results to improve ranking quality for downstream
tasks. Recent studies--motivated by large reasoning models (LRMs)--have begun
incorporating explicit chain-of-thought (CoT) reasoning into LLM-based
rerankers. However, the effectiveness of such reasoning for ranking tasks
remains underexplored. In this work, we present the first systematic study of
reasoning in reranking across both pointwise and listwise settings, under both
supervised fine-tuning and reinforcement learning. Using diverse benchmarks,
including reasoning-intensive datasets (BRIGHT) and standard IR benchmarks
(BEIR), we find that reasoning-augmented rerankers consistently underperform
their direct counterparts that predict rankings without CoT, despite
substantially higher inference costs. Our analysis reveals three core
limitations: (i) in pointwise rerankers, reasoning breaks calibration and
biases models toward the positive class, raising TPR but lowering TNR, which
inflates false positives and degrades ranking in negative-dominant pools; (ii)
in listwise rerankers, reasoning improves in-domain fit but increases variance
and fails to generalize out-of-domain, even when reinforcement learning
shortens rationales; and (iii) overall, directly fine-tuned rerankers remain
more stable, effective, and robust. These findings challenge the assumption
that explicit reasoning is universally beneficial for reranking. We conclude by
highlighting future directions, including calibration-aware scoring for
pointwise rerankers and the design of concise, targeted reasoning strategies to
mitigate overfitting and overthinking in listwise rerankers.

</details>


### [8] [Generative Data Augmentation in Graph Contrastive Learning for Recommendation](https://arxiv.org/abs/2510.09129)
*Yansong Wang,Qihui Lin,Junjie Huang,Tao Jia*

Main category: cs.IR

TL;DR: 提出了GDA4Rec框架，使用生成式数据增强和图对比学习来改进推荐系统中的嵌入学习，通过深度生成模型生成高质量增强视图，并提取物品互补矩阵提供额外自监督信号。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中从稀疏用户-物品交互学习有效嵌入的挑战，现有随机数据增强方法会改变原始语义信息，需要生成更高质量的增强视图。

Method: 使用噪声生成模块通过深度生成模型近似原始数据分布进行数据增强；提取物品互补矩阵表征物品间潜在相关性；采用集成推荐、数据增强和对比学习的联合目标函数。

Result: 在三个公共数据集上的广泛实验证明了模型的优越性。

Conclusion: GDA4Rec框架能够生成高质量增强视图，提供鲁棒的自监督信号，学习更有效和信息丰富的嵌入。

Abstract: Recommendation systems have become indispensable in various online platforms,
from e-commerce to streaming services. A fundamental challenge in this domain
is learning effective embeddings from sparse user-item interactions. While
contrastive learning has recently emerged as a promising solution to this
issue, generating augmented views for contrastive learning through most
existing random data augmentation methods often leads to the alteration of
original semantic information. In this paper, we propose a novel framework,
GDA4Rec (Generative Data Augmentation in graph contrastive learning for
Recommendation) to generate high-quality augmented views and provide robust
self-supervised signals. Specifically, we employ a noise generation module that
leverages deep generative models to approximate the distribution of original
data for data augmentation. Additionally, GDA4Rec further extracts an item
complement matrix to characterize the latent correlations between items and
provide additional self-supervised signals. Lastly, a joint objective that
integrates recommendation, data augmentation and contrastive learning is used
to enforce the model to learn more effective and informative embeddings.
Extensive experiments are conducted on three public datasets to demonstrate the
superiority of the model. The code is available at:
https://github.com/MrYansong/GDA4Rec.

</details>


### [9] [Controlled Personalization in Legacy Media Online Services: A Case Study in News Recommendation](https://arxiv.org/abs/2510.09136)
*Marlene Holzleitner,Stephan Leitner,Hanna Lind Jorgensen,Christoph Schmitz,Jacob Welander,Dietmar Jannach*

Main category: cs.IR

TL;DR: 传统新闻媒体采用受控个性化策略，将编辑策划与算法推荐相结合，A/B测试表明即使适度个性化也能显著提升用户点击率、内容多样性并减少流行度偏见。


<details>
  <summary>Details</summary>
Motivation: 传统新闻媒体在采用个性化推荐技术时面临平衡技术创新与编辑价值观的挑战，需要找到既能满足用户需求又能维护新闻专业标准的方法。

Method: 在挪威主要传统新闻机构的网站上进行了A/B测试，比较受控个性化策略（编辑策划内容与算法推荐相结合）与传统方式的差异。

Result: 受控个性化显著提高了点击率，降低了用户导航成本，提升了内容多样性和目录覆盖率，同时减少了流行度偏见。

Conclusion: 受控个性化能够成功协调用户需求与编辑目标，为传统媒体采用个性化技术同时维护新闻价值观提供了可行路径。

Abstract: Personalized news recommendations have become a standard feature of large
news aggregation services, optimizing user engagement through automated content
selection. In contrast, legacy news media often approach personalization
cautiously, striving to balance technological innovation with core editorial
values. As a result, online platforms of traditional news outlets typically
combine editorially curated content with algorithmically selected articles - a
strategy we term controlled personalization. In this industry paper, we
evaluate the effectiveness of controlled personalization through an A/B test
conducted on the website of a major Norwegian legacy news organization. Our
findings indicate that even a modest level of personalization yields
substantial benefits. Specifically, we observe that users exposed to
personalized content demonstrate higher click-through rates and reduced
navigation effort, suggesting improved discovery of relevant content. Moreover,
our analysis reveals that controlled personalization contributes to greater
content diversity and catalog coverage and in addition reduces popularity bias.
Overall, our results suggest that controlled personalization can successfully
align user needs with editorial goals, offering a viable path for legacy media
to adopt personalization technologies while upholding journalistic values.

</details>


### [10] [Hierarchical Semantic RL: Tackling the Problem of Dynamic Action Space for RL-based Recommendations](https://arxiv.org/abs/2510.09167)
*Minmao Wang,Xingchen Liu,Shijie Yi,Likang Wu,Hongke Zhao,Fei Pan,Qingpeng Cai,Peng Jiang*

Main category: cs.IR

TL;DR: HSRL通过引入固定语义动作空间和分层策略网络，解决了推荐系统中强化学习面临的大规模动态动作空间问题，显著提升了长期用户价值建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统大多优化短期参与度，而强化学习方法在处理推荐系统的大规模动态动作空间时面临训练不稳定的挑战。

Method: 提出分层语义强化学习（HSRL），使用固定的语义动作空间（SAS）和语义ID（SID）进行策略学习，通过分层策略网络（HPN）和多级评论家（MLC）实现从粗到细的决策和细粒度信用分配。

Result: 在公开基准测试和大规模生产数据集上，HSRL持续超越最先进基线方法。在线A/B测试中，CVR提升18.421%，成本仅增加1.251%。

Conclusion: HSRL为基于强化学习的推荐系统提供了一个可扩展的范式，能够有效建模长期用户价值。

Abstract: Recommender Systems (RS) are fundamental to modern online services. While
most existing approaches optimize for short-term engagement, recent work has
begun to explore reinforcement learning (RL) to model long-term user value.
However, these efforts face significant challenges due to the vast, dynamic
action spaces inherent in recommendation, which hinder stable policy learning.
To resolve this bottleneck, we introduce Hierarchical Semantic RL (HSRL), which
reframes RL-based recommendation over a fixed Semantic Action Space (SAS). HSRL
encodes items as Semantic IDs (SIDs) for policy learning, and maps SIDs back to
their original items via a fixed, invertible lookup during execution. To align
decision-making with SID generation, the Hierarchical Policy Network (HPN)
operates in a coarse-to-fine manner, employing hierarchical residual state
modeling to refine each level's context from the previous level's residual,
thereby stabilizing training and reducing representation-decision mismatch. In
parallel, a Multi-level Critic (MLC) provides token-level value estimates,
enabling fine-grained credit assignment. Across public benchmarks and a
large-scale production dataset from a leading Chinese short-video advertising
platform, HSRL consistently surpasses state-of-the-art baselines. In online
deployment over a seven-day A/B testing, it delivers an 18.421% CVR lift with
only a 1.251% increase in cost, supporting HSRL as a scalable paradigm for
RL-based recommendation. Our code is released at
https://github.com/MinmaoWang/HSRL.

</details>


### [11] [ChoirRec: Semantic User Grouping via LLMs for Conversion Rate Prediction of Low-Activity Users](https://arxiv.org/abs/2510.09393)
*Dakai Zhai,Jiong Gao,Boya Du,Junwei Xu,Qijie Shen,Jialin Zhu,Yuning Jiang*

Main category: cs.IR

TL;DR: ChoirRec是一个利用大语言模型构建语义用户组来提升低活跃用户转化率预测的新框架，通过双通道架构实现跨用户知识迁移，在淘宝平台上显著提升了预测效果和订单量。


<details>
  <summary>Details</summary>
Motivation: 解决电商推荐系统中低活跃用户转化率预测的三个关键挑战：依赖噪声行为信号、用户级信息不足、训练偏向高活跃用户。

Method: 提出ChoirRec框架，包含三个组件：语义组生成模块（使用LLM构建跨活跃度用户集群）、组感知分层表示模块（用组级先验丰富稀疏用户嵌入）、组感知多粒度模块（双通道架构和自适应融合机制）。

Result: 在淘宝平台离线评估中GAUC提升1.16%，在线A/B测试显示订单量增加7.24%。

Conclusion: ChoirRec通过LLM构建语义用户组和双通道架构，有效解决了低活跃用户转化率预测问题，具有显著的实用价值。

Abstract: Accurately predicting conversion rates (CVR) for low-activity users remains a
fundamental challenge in large-scale e-commerce recommender systems.Existing
approaches face three critical limitations: (i) reliance on noisy and
unreliable behavioral signals; (ii) insufficient user-level information due to
the lack of diverse interaction data; and (iii) a systemic training bias toward
high-activity users that overshadows the needs of low-activity users.To address
these challenges, we propose ChoirRec, a novel framework that leverages the
semantic capabilities of Large Language Models (LLMs) to construct semantic
user groups and enhance CVR prediction for low-activity users.With a
dual-channel architecture designed for robust cross-user knowledge transfer,
ChoirRec comprises three components: (i) a Semantic Group Generation module
that utilizes LLMs to form reliable, cross-activity user clusters, thereby
filtering out noisy signals; (ii) a Group-aware Hierarchical Representation
module that enriches sparse user embeddings with informative group-level priors
to mitigate data insufficiency; and (iii) a Group-aware Multi-granularity
Modual that employs a dual-channel architecture and adaptive fusion mechanism
to ensure effective learning and utilization of group knowledge. We conduct
extensive offline and online experiments on Taobao, a leading industrial-scale
e-commerce platform.ChoirRec improves GAUC by 1.16\% in offline evaluations,
while online A/B testing reveals a 7.24\% increase in order volume,
highlighting its substantial practical value in real-world applications.

</details>


### [12] [MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval](https://arxiv.org/abs/2510.09510)
*Siyue Zhang,Yuan Gao,Xiao Zhou,Yilun Zhao,Tingyu Song,Arman Cohan,Anh Tuan Luu,Chen Zhao*

Main category: cs.IR

TL;DR: MRMR是首个需要深度推理的专家级多学科多模态检索基准，包含1,502个查询，涵盖23个领域，通过引入多领域挑战、推理密集型任务和图像-文本交错序列，为多模态检索系统提供了更现实的评估环境。


<details>
  <summary>Details</summary>
Motivation: 现有基准在领域多样性、推理深度和模态组合方面存在局限，无法充分评估多模态检索系统在真实复杂场景下的性能。

Method: 构建包含1,502个专家验证查询的基准，涵盖23个专业领域，引入矛盾检索新任务，采用图像-文本交错序列的查询和文档格式。

Result: 评估了4类多模态检索系统和14个前沿模型，Qwen3-Embedding结合LLM生成图像描述表现最佳，但多模态模型在推理密集型任务上仍有不足。

Conclusion: MRMR为推进多模态检索在更现实和挑战性场景中的发展铺平了道路，揭示了当前模型在复杂推理任务上的改进空间。

Abstract: We introduce MRMR, the first expert-level multidisciplinary multimodal
retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries
spanning 23 domains, with positive documents carefully verified by human
experts. Compared to prior benchmarks, MRMR introduces three key advancements.
First, it challenges retrieval systems across diverse areas of expertise,
enabling fine-grained model comparison across domains. Second, queries are
reasoning-intensive, with images requiring deeper interpretation such as
diagnosing microscopic slides. We further introduce Contradiction Retrieval, a
novel task requiring models to identify conflicting concepts. Finally, queries
and documents are constructed as image-text interleaved sequences. Unlike
earlier benchmarks restricted to single images or unimodal documents, MRMR
offers a realistic setting with multi-image queries and mixed-modality corpus
documents. We conduct an extensive evaluation of 4 categories of multimodal
retrieval systems and 14 frontier models on MRMR. The text embedding model
Qwen3-Embedding with LLM-generated image captions achieves the highest
performance, highlighting substantial room for improving multimodal retrieval
models. Although latest multimodal models such as Ops-MM-Embedding perform
competitively on expert-domain queries, they fall short on reasoning-intensive
tasks. We believe that MRMR paves the way for advancing multimodal retrieval in
more realistic and challenging scenarios.

</details>


### [13] [Doc2Query++: Topic-Coverage based Document Expansion and its Application to Dense Retrieval via Dual-Index Fusion](https://arxiv.org/abs/2510.09557)
*Tzu-Lin Kuo,Wei-Ning Chiu,Wei-Yun Ma,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: Doc2Query++是一个文档扩展框架，通过无监督主题建模和混合关键词选择来指导LLM生成多样化且相关的查询，解决了传统文档扩展方法中的幻觉、冗余和跨域泛化问题，并采用双索引融合策略提升稠密检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决文档扩展方法中的三个主要问题：不受控生成导致幻觉或冗余查询、从领域内训练到领域外数据的泛化能力差、以及查询拼接对稠密检索产生的噪声影响。

Method: 1. 使用无监督主题建模推断文档的潜在主题，确保跨域适用性；2. 通过混合关键词选择为每个文档创建多样化且相关的关键词集；3. 指导LLM利用关键词生成查询，确保全面主题覆盖并减少冗余；4. 提出双索引融合策略，隔离文本和查询信号以提升稠密检索性能。

Result: 在多个数据集上的广泛实验表明，Doc2Query++在稀疏和稠密检索中都显著优于现有最先进基线方法，在MAP、nDCG@10和Recall@100等指标上取得了显著提升。

Conclusion: Doc2Query++通过结构化查询生成和双索引融合策略，有效解决了文档扩展中的关键挑战，在跨域场景下实现了优异的检索性能。

Abstract: Document expansion (DE) via query generation tackles vocabulary mismatch in
sparse retrieval, yet faces limitations: uncontrolled generation producing
hallucinated or redundant queries with low diversity; poor generalization from
in-domain training (e.g., MS MARCO) to out-of-domain data like BEIR; and noise
from concatenation harming dense retrieval. While Large Language Models (LLMs)
enable cross-domain query generation, basic prompting lacks control, and
taxonomy-based methods rely on domain-specific structures, limiting
applicability. To address these challenges, we introduce Doc2Query++, a DE
framework that structures query generation by first inferring a document's
latent topics via unsupervised topic modeling for cross-domain applicability,
then using hybrid keyword selection to create a diverse and relevant keyword
set per document. This guides LLM not only to leverage keywords, which ensure
comprehensive topic representation, but also to reduce redundancy through
diverse, relevant terms. To prevent noise from query appending in dense
retrieval, we propose Dual-Index Fusion strategy that isolates text and query
signals, boosting performance in dense settings. Extensive experiments show
Doc2Query++ significantly outperforms state-of-the-art baselines, achieving
substantial gains in MAP, nDCG@10 and Recall@100 across diverse datasets on
both sparse and dense retrieval.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [14] [Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly](https://arxiv.org/abs/2510.08863)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DB

TL;DR: 对Redis、Aerospike和Dragonfly三种NoSQL键值存储系统进行性能评估，使用YCSB基准测试框架，在不同工作负载模式（读密集型、写密集型、平衡型）和1-32个客户端并发下测试延迟、吞吐量和内存特性。


<details>
  <summary>Details</summary>
Motivation: 分布式应用和云计算的发展对可扩展、高性能的键值存储系统提出了需求，需要评估不同NoSQL系统的性能表现。

Method: 使用Yahoo! Cloud Serving Benchmark (YCSB)框架，在三种不同工作负载模式（读密集型、写密集型、平衡型）下，通过系统性地改变客户端并发数（1-32个客户端）进行广泛实验。

Result: 评估方法捕获了在实际操作条件下的延迟、吞吐量和内存特性，揭示了每个系统的性能权衡和可扩展性行为。

Conclusion: 该研究为不同NoSQL键值存储系统的性能特征提供了重要见解，有助于在实际部署中做出合适的选择。

Abstract: The rise of distributed applications and cloud computing has created a demand
for scalable, high-performance key-value storage systems. This paper presents a
performance evaluation of three prominent NoSQL key-value stores: Redis,
Aerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)
framework. We conducted extensive experiments across three distinct workload
patterns (read-heavy, write-heavy), and balanced while systematically varying
client concurrency from 1 to 32 clients. Our evaluation methodology captures
both latency, throughput, and memory characteristics under realistic
operational conditions, providing insights into the performance trade-offs and
scalability behaviour of each system

</details>


### [15] [HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance](https://arxiv.org/abs/2510.08896)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.DB

TL;DR: HES-SQL是一个新颖的混合训练框架，通过结合思维模式融合的监督微调(SFT)和组相对策略优化(GRPO)来推进Text-to-SQL生成。该框架在准确性和计算效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL系统在语义准确性和计算效率之间存在权衡，需要开发既能生成正确SQL查询又能优化执行效率的方法。

Method: 提出了三个关键创新：(1)骨架完整性评分机制增强生成查询与最优SQL结构的偏好对齐；(2)查询延迟感知奖励系统激励生成计算效率高的SQL查询；(3)思维模式完成的自蒸馏过程防止模型推理能力退化。

Result: 在MySQL 8.0和SQLite 3.42上的实验显示，HES-SQL在BIRD和KaggleDBQA基准测试中分别达到79.14%和54.9%的执行准确率，效率提升11%-20%。

Conclusion: HES-SQL为Text-to-SQL系统建立了新范式，通过执行感知的强化学习有效平衡语义准确性和计算效率，对开发稳健的数据库自然语言接口具有重要意义。

Abstract: We present HES-SQL, a novel hybrid training framework that advances
Text-to-SQL generation through the integration of thinking-mode-fused
supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).
Our approach introduces three key innovations: (1) a skeleton-completeness
scoring mechanism that enhances preference alignment between generated queries
and optimal SQL structures; (2) a query-latency-aware reward system that
incentivizes the generation of computationally efficient SQL queries; (3) a
self-distillation process for thinking-mode completion that prevents
degradation of the model's reasoning capabilities. This framework enables
hybrid thinking models to switch between reasoning and non-reasoning modes
while improving SQL query accuracy and execution efficiency.
  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under
controlled single-user conditions, demonstrates that HES-SQL achieves
competitive performance with execution accuracies of 79.14\% and 54.9\% on the
BIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the
end-to-end execution time of generated queries on the DBMS, averaged over
multiple runs to mitigate variance. Efficiency gains range from 11\% to 20\%
relative to supervised baselines. Our results establish a new paradigm for
Text-to-SQL systems that effectively balances semantic accuracy with
computational efficiency through execution-informed reinforcement learning
(RL). The proposed methodology has significant implications for developing
robust natural language interfaces to databases and can be extended to broader
structured generation tasks requiring both correctness and efficiency
optimization.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [16] [On Estimation of Angles of Arrival in Monostatic ISAC Without Instantaneous Transmit CSI](https://arxiv.org/abs/2510.08793)
*Ataher Sams,Simone Di Bari,Besma Smida,Natasha Devroye,Daniela Tuninetti,Giorgio Taricco*

Main category: cs.IT

TL;DR: 该论文在基站仅有统计CSI而非完整CSI的更现实场景下，分析了集成感知与通信(ISAC)的基本限制。通过贝叶斯克拉美-罗界(BCRB)框架推导了感知与通信的权衡区域，提出了多种波束分配策略。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究多假设基站拥有完整的信道状态信息(CSI)，但在实际场景中基站通常只有统计CSI。本文旨在探索在这种更现实条件下的ISAC基本性能限制。

Method: 采用贝叶斯克拉美-罗界(BCRB)框架分析最小均方误差准则下的感知基本限制，推导BCRB-速率权衡区域，提出多种在相干时间内共享感知与通信波束功率的传输策略。

Result: 分析表明，利用目标特定感知矩阵主特征向量的波束分配策略能最小化单个AoA估计误差，而平衡感知与通信方向的策略虽牺牲个体精度但优化了联合估计性能。利用通信接收器较低信道不确定性更新的BCRB感知信息可显著提高通信速率。

Conclusion: 在仅有统计CSI的现实ISAC场景中，通过适当的波束分配策略可以在感知与通信之间实现有效权衡，利用通信接收器的信道优势可显著提升系统性能。

Abstract: This paper explores the fundamental limits of Integrated Sensing and
Communication (ISAC) in a more realistic setting compared to previous
literature when the Base Staion (BS) has only statistical CSI of the
communication user rather than full CSI. We analyze a monostatic setting where
the BS performs multi-target Angle of Arrival (AoA) estimation while
simultaneously communicating with one of the targets. We assume that the BS has
statistical CSI about all AoAs, with less uncertainty in the AoA of the
communication receiver. The communication receiver is assumed to have perfect
CSI. Utilizing a Bayesian Cram\'er-Rao Bound (BCRB) framework to characterize
the fundamental limits of sensing under minimum mean square error (MMSE)
criteria, we derive achievable BCRB-rate trade-off regions. Our approach
introduces a number of transmission strategies that share power across sensing
and communication beams over a coherence time. Our analysis reveals that beam
allocation strategies leveraging the principal eigenvectors of the
target-specific sensing matrices minimize individual AoA estimation errors,
while strategies balancing sensing and communication directions optimize joint
estimation performance at the cost of individual accuracy. We demonstrate that
leveraging updated BCRB-based sensing information for the communication
receiver, due to its lower channel uncertainty, enables significantly improved
communication rates.

</details>


### [17] [Observation Matrix Design for Densifying MIMO Channel Estimation via 2D Ice Filling](https://arxiv.org/abs/2510.08887)
*Zijian Zhang,Mingyao Cui*

Main category: cs.IT

TL;DR: 提出了一种基于互信息最大化的二维注水算法（2DIF）及其在混合波束成形系统中的扩展版本TS-2DIF，用于优化密集MIMO系统的信道估计性能。


<details>
  <summary>Details</summary>
Motivation: 密集MIMO系统中天线间距小于波长导致强相关性，这为利用信道状态信息的先验知识提供了机会，通过精心设计观测矩阵（预编码器和组合器）来提升信道估计性能。

Method: 通过最大化接收导频与密集MIMO信道之间的互信息来联合设计组合器和预编码器。利用MIMO信道协方差的特征空间可解耦为发射天线和接收天线相关性的两个子特征空间，将预编码器和组合器设置为这些子特征空间的特征向量。

Result: 仿真结果表明，与现有最先进方案相比，所提出的2DIF和TS-2DIF方法能够实现更优越的信道估计精度。

Conclusion: 所提出的二维注水算法及其扩展版本能够有效利用密集MIMO系统的信道先验知识，通过优化观测矩阵设计显著提升信道估计性能。

Abstract: In recent years, densifying multiple-input multiple-output (MIMO) has
attracted much attention from the communication community. Thanks to the
subwavelength antenna spacing, the strong correlations among densifying
antennas provide sufficient prior knowledge about channel state information
(CSI). This inspires the careful design of observation matrices (e.g., transmit
precoders and receive combiners), that exploits the CSI prior knowledge, to
boost channel estimation performance. Aligned with this vision, this work
proposes to jointly design the combiners and precoders by maximizing the mutual
information between the received pilots and densifying MIMO channels. A
two-dimensional ice-filling (2DIF) algorithm is proposed to efficiently
accomplish this objective. The algorithm is motivated by the fact that the
eigenspace of MIMO channel covariance can be decoupled into two
sub-eigenspaces, which are associated with the correlations of transmitter
antennas and receiver antennas, respectively. By properly setting the precoder
and the combiner as the eigenvectors from these two sub-eigenspaces, the 2DIF
promises to generate near-optimal observation matrices. Moreover, we further
extend the 2DIF method to the popular hybrid combining systems, where a
two-stage 2DIF (TS-2DIF) algorithm is developed to handle the analog combining
circuits realized by phase shifters. Simulation results demonstrate that,
compared to the state-of-the-art schemes, the proposed 2DIF and TS-2DIF methods
can achieve superior channel estimation accuracy.

</details>


### [18] [Soft Guessing Under Logarithmic Loss Allowing Errors and Variable-Length Source Coding](https://arxiv.org/abs/2510.09015)
*Shota Saito,Hamdi Joudeh*

Main category: cs.IT

TL;DR: 本文研究了在对数损失失真度量下允许错误的软猜测问题，推导了最优猜测策略、单次上下界和i.i.d.源的渐近展开，并扩展到有边信息的情况，建立了软猜测与变长有损源编码的联系。


<details>
  <summary>Details</summary>
Motivation: 研究在对数损失下允许错误的软猜测问题，探索其与信息论中其他问题的联系，特别是与变长有损源编码的关系。

Method: 推导最优猜测策略，建立单次上下界，进行i.i.d.源的渐近分析，扩展到边信息情况，并建立与变长有损源编码的联系。

Result: 得到了最小猜测矩的单次上下界和渐近展开，Rényi熵、平滑Rényi熵及其条件版本在分析中起关键作用。

Conclusion: 成功建立了软猜测允许错误与变长有损源编码之间的重要联系，Rényi熵相关概念在分析中发挥核心作用。

Abstract: This paper considers the problem of soft guessing under a logarithmic loss
distortion measure while allowing errors. We find an optimal guessing strategy,
and derive single-shot upper and lower bounds for the minimal guessing moments
as well as an asymptotic expansion for i.i.d. sources. These results are
extended to the case where side information is available to the guesser.
Furthermore, a connection between soft guessing allowing errors and
variable-length lossy source coding under logarithmic loss is demonstrated. The
R\'enyi entropy, the smooth R\'enyi entropy, and their conditional versions
play an important role.

</details>


### [19] [Low Complexity Detector for XL-MIMO Uplink: A Cross Splitting Based Information Geometry Approach](https://arxiv.org/abs/2510.09039)
*Wenjun Zhang,An-An Lu,Xiqi Gao*

Main category: cs.IT

TL;DR: 提出CS-IGA算法，一种用于超大规模MIMO系统上行链路信号恢复的低复杂度迭代检测器，通过交叉矩阵分解降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统迭代检测器如AMP和IGA的每次迭代复杂度随基站天线数量线性增长，造成计算瓶颈，需要降低复杂度。

Method: 引入自然参数的后验分布交叉矩阵分解，基于匹配滤波器进行迭代检测，并扩展到非线性检测NCS-IGA，嵌入离散星座约束。

Result: CS-IGA在线性和非线性检测中均达到或超过贝叶斯最优AMP和IGA的误码率性能，且迭代次数更少、计算成本显著降低。

Conclusion: CS-IGA是下一代XL-MIMO系统中高吞吐量信号检测的实用且强大的解决方案。

Abstract: In this paper, we propose the cross splitting based information geometry
approach (CS-IGA), a novel and low complexity iterative detector for uplink
signal recovery in extralarge-scale MIMO (XL-MIMO) systems. Conventional
iterative detectors, such as the approximate message passing (AMP) algorithm
and the traditional information geometry algorithm (IGA), suffer from a per
iteration complexity that scales with the number of base station (BS) antennas,
creating a computational bottleneck. To overcome this, CS-IGA introduces a
novel cross matrix splitting of the natural parameter in the a posteriori
distribution. This factorization allows the iterative detection based on the
matched filter, which reduces per iteration computational complexity.
Furthermore, we extend this framework to nonlinear detection and propose
nonlinear CSIGA (NCS-IGA) by seamlessly embedding discrete constellation
constraints, enabling symbol-wise processing without external interference
cancellation loops. Comprehensive simulations under realistic channel
conditions demonstrate that CS-IGA matches or surpasses the bit error rate
(BER) performance of Bayes optimal AMP and IGA for both linear and nonlinear
detection, while achieving this with fewer iterations and a substantially lower
computational cost. These results establish CS-IGA as a practical and powerful
solution for high-throughput signal detection in next generation XL-MIMO
systems.

</details>


### [20] [Optimal binary codes from $\mathcal{C}_{D}$-codes over a non-chain ring](https://arxiv.org/abs/2510.09057)
*Ankit Yadav,Ritumoni Sarma,Anuj Kumar Bhagat*

Main category: cs.IT

TL;DR: 该论文研究了环$\mathcal{R}$上$\mathcal{C}_D$-码的子域码$\mathcal{C}_{D}^{(2)}$，计算了其汉明重量分布和参数，发现了多个距离最优的无限码族，并给出了这些码为极小码和自正交码的充分条件，还构造了强正则图。


<details>
  <summary>Details</summary>
Motivation: 基于Shi和Li在2022年对环$\mathcal{R}$上$\mathcal{C}_D$-码及其二进制Gray像的研究，本文进一步研究这些码的子域码，探索其参数特性和应用价值。

Method: 使用与Shi和Li相同的环$\mathcal{R}$和由单纯复形导出的集合$D$，研究$\mathcal{C}_D$-码的子域码$\mathcal{C}_{D}^{(2)}$，分析其汉明重量分布和参数特性。

Result: 发现了多个无限码族是距离最优的，给出了这些码为极小码和自正交码的充分条件，并利用构造的两重量码得到了两个强正则图族。

Conclusion: 该研究扩展了环上码的理论，提供了新的距离最优码族，并展示了编码理论在图论中的应用价值。

Abstract: In \cite{shi2022few-weight}, Shi and Li studied $\mathcal{C}_D$-codes over
the ring $\mathcal{R}:=\mathbb{F}_2[x,y]/\langle x^2, y^2, xy-yx\rangle$ and
their binary Gray images, where $D$ is derived using certain simplicial
complexes. We study the subfield codes $\mathcal{C}_{D}^{(2)}$ of
$\mathcal{C}_{D}$-codes over $\mathcal{R},$ where $D$ is as in
\cite{shi2022few-weight} and more. We find the Hamming weight distribution and
the parameters of $\mathcal{C}_D^{(2)}$ for various $D$, and identify several
infinite families of codes that are distance-optimal. Besides, we provide
sufficient conditions under which these codes are minimal and self-orthogonal.
Two families of strongly regular graphs are obtained as an application of the
constructed two-weight codes.

</details>


### [21] [A Hybrid I/O Relation Estimation Scheme for Zak-OTFS Receivers](https://arxiv.org/abs/2510.09215)
*Sai Pradeep Muppaneni,Vineetha Yogesh,A. Chockalingam*

Main category: cs.IT

TL;DR: 提出了一种混合估计方法，结合模型依赖和模型无关的方法来改进Zak-OTFS调制中的延迟-多普勒域输入输出关系估计，解决了模型无关方法在DD平面有限区域外估计性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 模型无关方法虽然简单，但只能估计DD平面有限区域内的有效信道，对于DD脉冲形状定位不佳的情况会导致性能下降。需要一种方法能够克服这一局限性。

Method: 提出混合估计方案：首先使用模型无关方法估计有限区域内的信道，然后使用新颖的模型依赖方法粗略估计区域外的有效信道，最后结合两者获得改进的总体I/O关系估计。

Result: 在Vehicular-A、TDL-A和TDL-C信道模型上的仿真结果表明，所提出的混合估计方法相比纯模型无关方法具有更优越的性能。

Conclusion: 混合估计方法有效解决了模型无关方法在DD平面区域外估计性能受限的问题，为Zak-OTFS调制中的信道估计提供了更优的解决方案。

Abstract: In this paper, we consider the problem of estimating the delay-Doppler (DD)
domain input-output (I/O) relation in Zak-OTFS modulation, which is needed for
signal detection. Two approaches, namely, model-dependent and model-free
approaches, can be employed for this purpose. The model-dependent approach
requires explicit estimation of the physical channel parameters (path delays,
Dopplers, and gains) to obtain the I/O relation. Such an explicit estimation is
not required in the model-free approach, where the I/O relation can be
estimated by reading off the samples in the fundamental DD period of the
received pilot frame. Model-free approach has the advantage of acquiring
fractional DD channels with simplicity. However, the read-off in the model-free
approach provides an estimate of the effective channel only over a limited
region in the DD plane but it does not provide an estimate for the region
outside, and this can affect the estimation performance depending on the pulse
shaping characteristics of the DD pulse shaping filter used. A poorly localized
DD pulse shape leads to an increased degradation in performance. Motivated by
this, in this paper, we propose a novel, yet simple, I/O relation estimation
scheme that alleviates the above issue in the model-free approach. We achieve
this by obtaining a coarse estimate of the effective channel outside the
model-free estimation region using a novel model-dependent scheme and using
this estimate along with the model-free estimate to obtain an improved estimate
of the overall I/O relation. We devise the proposed estimation scheme for both
exclusive and embedded pilot frames. Our simulation results using Vehicular-A,
TDL-A and TDL-C channel models with fractional DDs show that the proposed
hybrid estimation approach achieves superior performance compared to the pure
model-free approach.

</details>


### [22] [Serial Polar Automorphism Ensemble Decoders for Physical Unclonable Functions](https://arxiv.org/abs/2510.09220)
*Marvin Rübenacke,Sebastian Cammerer,Michael Sullivan,Alexander Keller*

Main category: cs.IT

TL;DR: 提出了一种基于Polar码和低复杂度自同构集成解码的物理不可克隆函数编码方案，在保持10^-6误码率的同时，比BCH码减少1.75倍码字位数。


<details>
  <summary>Details</summary>
Motivation: 物理不可克隆函数需要极低故障率（10^-6以下）和高原始误码率（22%），这要求设计高效的超低码率编码方案。

Method: 使用Polar码和低复杂度自同构集成解码，采用串行AED方案重用单个连续消除解码器，通过级联和递归交织器扩展候选解码，并采用3位量化策略。

Result: 在K=312有效载荷位下，达到与BCH码相同的10^-6误块率，但码字位数减少1.75倍，相应减少辅助数据存储和芯片面积。

Conclusion: 该编码方案为PUF应用提供了高效的超低码率解决方案，显著减少了存储需求和芯片面积。

Abstract: Physical unclonable functions (PUFs) involve challenging practical
applications of error-correcting codes (ECCs), requiring extremely low failure
rates on the order of $10^{-6}$ and below despite raw input bit error rates as
high as 22%. These requirements call for an efficient ultra-low rate code
design. In this work, we propose a novel coding scheme tailored for PUFs based
on Polar codes and a low-complexity version of automorphism ensemble decoding
(AED). Notably, our serial AED scheme reuses a single successive cancellation
(SC) decoder across multiple decoding attempts. By introducing cascaded and
recursive interleavers, we efficiently scale the number of AED candidates
without requiring expensive large multiplexers. An aggressive quantization
strategy of only 3 bits per message further reduces the area requirements of
the underlying SC decoder. The resulting coding scheme achieves the same block
error rate of $10^{-6}$ as our baseline based on Bose-Ray-Chaudhuri-Hocquenghem
(BCH) codes while requiring 1.75x fewer codeword bits to encode the same K =
312 payload bits. This reduction translates directly into 1.75x less helper
data storage and, consequently, a smaller overall chip area.

</details>


### [23] [Site-Specific RIS Deployment in Cellular Networks via Calibrated Ray Tracing](https://arxiv.org/abs/2510.09478)
*Sina Beyraghi,Javad Shabanpour,Giovanni Geraci,Paul Almasan,Angel Lozano*

Main category: cs.IT

TL;DR: 提出了一种基于数字孪生的全自动RIS部署策略，在4G、5G和6G频段上联合优化RIS位置、方向、配置和基站波束成形，结果显示需要密集的大孔径RIS部署才能实现有意义的覆盖增强。


<details>
  <summary>Details</summary>
Motivation: 探索可重构智能表面(RIS)在实际城市环境中的部署可行性，通过数字孪生技术验证自动化部署策略的有效性。

Method: 使用Sionna射线追踪构建英国城市的数字孪生模型，基于散射射线识别候选RIS位置，通过用户聚类减少部署开销，联合优化RIS位置、方向、配置和基站波束成形。

Result: 研究发现要实现有意义的覆盖增强需要密集的大孔径RIS部署，这引发了对大规模RIS部署实用性和成本的质疑。

Conclusion: 虽然RIS技术有潜力增强无线覆盖，但实际大规模部署面临成本效益和实用性的挑战，需要进一步研究更经济的部署方案。

Abstract: This work introduces a fully-automated RIS deployment strategy validated
through a digital twin, powered by Sionna ray tracing, of a UK city. On a scene
calibrated with measured data, the method jointly optimizes RIS placement,
orientation, configuration, and BS beamforming across 4G, 5G, and hypothetical
6G frequencies. Candidate RIS sites are identified via scattering-based rays,
while user clustering reduces deployment overhead. Results show that meaningful
coverage enhancement requires dense, large-aperture RIS deployments, raising
questions about the practicality and cost of large-scale RIS adoption.

</details>


### [24] [Precoder Design in Multi-User FDD Systems with VQ-VAE and GNN](https://arxiv.org/abs/2510.09495)
*Srikar Allaparapu,Michael Baur,Benedikt Böck,Michael Joham,Wolfgang Utschick*

Main category: cs.IT

TL;DR: 提出基于VQ-VAE的端到端预编码框架，通过生成模型学习传播环境统计特性，在FDD系统中实现鲁棒预编码，显著减少导频和反馈比特需求。


<details>
  <summary>Details</summary>
Motivation: 传统GMM方法存在组件数量随反馈比特指数增长的问题，且无法实现导频优化与预编码的联合训练，限制了系统性能。

Method: 使用VQ-VAE替代GMM，结合图神经网络，构建端到端模型，联合训练VQ-VAE、GNN和导频优化。

Result: 仿真显示该方法在系统和速率上优于传统子DFT导频矩阵和迭代预编码算法，能够以更少的导频或反馈比特部署系统。

Conclusion: VQ-VAE框架有效解决了GMM的可扩展性问题，端到端训练带来显著性能提升，为FDD系统提供了更高效的预编码解决方案。

Abstract: Robust precoding is efficiently feasible in frequency division duplex (FDD)
systems by incorporating the learnt statistics of the propagation environment
through a generative model. We build on previous work that successfully
designed site-specific precoders based on a combination of Gaussian mixture
models (GMMs) and graph neural networks (GNNs). In this paper, by utilizing a
vector quantized-variational autoencoder (VQ-VAE), we circumvent one of the key
drawbacks of GMMs, i.e., the number of GMM components scales exponentially to
the feedback bits. In addition, the deep learning architecture of the VQ-VAE
allows us to jointly train the GNN together with VQ-VAE along with pilot
optimization forming an end-to-end (E2E) model, resulting in considerable
performance gains in sum rate for multi-user wireless systems. Simulations
demonstrate the superiority of the proposed frameworks over the conventional
methods involving the sub-discrete Fourier transform (DFT) pilot matrix and
iterative precoder algorithms enabling the deployment of systems characterized
by fewer pilots or feedback bits.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [25] [The Online Submodular Cover Problem](https://arxiv.org/abs/2510.08883)
*Anupam Gupta,Roie Levin*

Main category: cs.DS

TL;DR: 提出了在线次模覆盖问题，给出了多对数竞争比的在线算法，竞争比为O(ln n ln(T·f(N)/f_min))，对于在线集合覆盖特例匹配了已知最优结果。


<details>
  <summary>Details</summary>
Motivation: 受网络监控和资源分配问题启发，研究次模覆盖问题在在线设置下的解决方案，其中函数随时间累积且不能撤销先前选择。

Method: 通过在线方式近似求解次模覆盖的指数大小线性规划松弛并进行舍入，克服了离线贪婪方法难以在线实现的挑战。

Result: 获得了多对数竞争比的在线算法，对于长度为T的输入序列，竞争比为O(ln n ln(T·f(N)/f_min))，其中f_min是最小非零边际值。

Conclusion: 该工作为在线次模覆盖问题提供了有效的多对数竞争比算法，在在线集合覆盖特例中达到了已知最优结果。

Abstract: In the submodular cover problem, we are given a monotone submodular function
$f$, and we want to pick the min-cost set $S$ such that $f(S) = f(N)$.
Motivated by problems in network monitoring and resource allocation, we
consider the submodular cover problem in an online setting. As a concrete
example, suppose at each time $t$, a nonnegative monotone submodular function
$g_t$ is given to us. We define $f^{(t)} = \sum_{s \leq t} g_s$ as the sum of
all functions seen so far. We need to maintain a submodular cover of these
submodular functions $f^{(1)}, f^{(2)}, \ldots f^{(T)}$ in an online fashion;
i.e., we cannot revoke previous choices. Formally, at each time $t$ we produce
a set $S_t \subseteq N$ such that $f^{(t)}(S_t) = f^{(t)}(N)$ -- i.e., this set
$S_t$ is a cover -- such that $S_{t-1} \subseteq S_t$, so previously decisions
to pick elements cannot be revoked. (We actually allow more general sequences
$\{f^{(t)}\}$ of submodular functions, but this
sum-of-simpler-submodular-functions case is useful for concreteness.)
  We give polylogarithmic competitive algorithms for this online submodular
cover problem. The competitive ratio on an input sequence of length $T$ is
$O(\ln n \ln (T \cdot f(N) / f_{\text{min}}))$, where $f_{\text{min}}$ is the
smallest nonzero marginal for functions $f^{(t)}$, and $|N| = n$. For the
special case of online set cover, our competitive ratio matches that of Alon et
al. [SIAM J. Comp. 03], which are best possible for polynomial-time online
algorithms unless $NP \subseteq BPP$ (see Korman 04). Since existing offline
algorithms for submodular cover are based on greedy approaches which seem
difficult to implement online, the technical challenge is to (approximately)
solve the exponential-sized linear programming relaxation for submodular cover,
and to round it, both in the online setting.

</details>


### [26] [Planar Length-Constrained Minimum Spanning Trees](https://arxiv.org/abs/2510.09002)
*D Ellis Hershkowitz,Richard Z Huang*

Main category: cs.DS

TL;DR: 本文提出了平面图中长度约束最小生成树问题的多项式时间近似算法，能在任意常数ε>0下输出O(log^(1+ε)n)近似解，同时保证节点到根节点的距离不超过(1+ε)h。


<details>
  <summary>Details</summary>
Motivation: 长度约束最小生成树问题在一般图上难以获得好的近似比，但在平面图这一特殊结构上可能存在更好的算法。本文旨在探索平面图中该问题的可近似性。

Method: 基于新的长度约束平面分隔器技术，该技术可能具有独立研究价值。算法也适用于长度约束Steiner树问题。

Result: 对于平面图，算法能在多项式时间内提供O(log^(1+ε)n)近似解，且节点距离约束放宽至(1+ε)h。在一般图上，任何算法若要求节点距离不超过2h，则无法达到O(log^(2-ε)n)的近似比。

Conclusion: 本文结果在平面图和一般图之间建立了长度约束最小生成树问题的可近似性分离，表明平面图结构对此问题具有更好的算法特性。

Abstract: In length-constrained minimum spanning tree (MST) we are given an $n$-node
graph $G = (V,E)$ with edge weights $w : E \to \mathbb{Z}_{\geq 0}$ and edge
lengths $l: E \to \mathbb{Z}_{\geq 0}$ along with a root node $r \in V$ and a
length-constraint $h \in \mathbb{Z}_{\geq 0}$. Our goal is to output a spanning
tree of minimum weight according to $w$ in which every node is at distance at
most $h$ from $r$ according to $l$.
  We give a polynomial-time algorithm for planar graphs which, for any constant
$\epsilon > 0$, outputs an $O\left(\log^{1+\epsilon} n\right)$-approximate
solution with every node at distance at most $(1+\epsilon)h$ from $r$ for any
constant $\epsilon > 0$. Our algorithm is based on new length-constrained
versions of classic planar separators which may be of independent interest.
Additionally, our algorithm works for length-constrained Steiner tree.
Complementing this, we show that any algorithm on general graphs for
length-constrained MST in which nodes are at most $2h$ from $r$ cannot achieve
an approximation of $O\left(\log ^{2-\epsilon} n\right)$ for any constant
$\epsilon > 0$ under standard complexity assumptions; as such, our results
separate the approximability of length-constrained MST in planar and general
graphs.

</details>


### [27] [A Faster Randomized Algorithm for Vertex Cover: An Automated Approach](https://arxiv.org/abs/2510.09027)
*Katie Clinch,Serge Gaspers,Tao Zixu He,Simon Mackenzie,Tiankuang Zhang*

Main category: cs.DS

TL;DR: 提出了两种分支算法设计与分析技术：自动生成分支规则的系统方法和基于Measure & Conquer的随机分支算法分析技术，在顶点覆盖问题上实现了最快的随机算法。


<details>
  <summary>Details</summary>
Motivation: 针对顶点覆盖问题，需要更高效的分支算法设计方法和更灵活的分析技术来改进现有算法性能。

Method: 1. 通过局部结构系统分析自动生成分支规则；2. 使用Measure & Conquer方法分析随机分支算法；3. 结合其他技术应用于有界度图和一般图的顶点覆盖问题。

Result: 在子立方图中达到O*(1.07625^n)和O*(1.13132^k)时间；最大度4图中达到O*(1.13735^n)和O*(1.21103^k)时间；一般图中达到O*(1.25281^k)时间。

Conclusion: 提出的两种技术显著提升了顶点覆盖问题分支算法的性能，在多种参数设置下实现了当前最快的随机算法。

Abstract: This work introduces two techniques for the design and analysis of branching
algorithms, illustrated through the case study of the Vertex Cover problem.
First, we present a method for automatically generating branching rules through
a systematic case analysis of local structures. Second, we develop a new
technique for analyzing randomized branching algorithms using the Measure &
Conquer method, offering greater flexibility in formulating branching rules. By
combining these innovations with additional techniques, we obtain the fastest
known randomized algorithms in different parameters for the Vertex Cover
problem on graphs with bounded degree (up to 6) and on general graphs. For
example, our algorithm solves Vertex Cover on subcubic graphs in
$O^*(1.07625^n)$ time and $O^*(1.13132^k)$ time, respectively. For graphs with
maximum degree 4, we achieve running times of $O^*(1.13735^n)$ and
$O^*(1.21103^k)$, while for general graphs we achieve $O^*(1.25281^k)$.

</details>


### [28] [Multi-product Influence Maximization in Billboard Advertisement](https://arxiv.org/abs/2510.09050)
*Dildar Ali,Rajibul Islam,Suman Banerjee*

Main category: cs.DS

TL;DR: 该论文研究多产品广告牌投放优化问题，提出了两种变体：一是选择k个广告位满足所有产品的需求，二是为每个产品分配独立且互斥的广告位集合。


<details>
  <summary>Details</summary>
Motivation: 解决多产品广告牌投放中的影响力最大化问题，传统方法只考虑单一产品，而商业公司通常需要同时推广多个产品，且每个产品有特定的影响力需求。

Method: 将第一个问题建模为多子模覆盖问题，采用双准则近似算法；将第二个问题建模为多子模覆盖的泛化问题，提出基于采样的近似算法。

Result: 在真实世界的轨迹和广告牌数据集上进行了广泛实验，证明了所提解决方案的有效性和效率。

Conclusion: 提出的方法能够有效解决多产品广告牌投放优化问题，为实际应用提供了可行的解决方案。

Abstract: Billboard Advertisement has emerged as an effective out-of-home advertisement
technique where the goal is to select a limited number of slots and play
advertisement content over there with the hope that this will be observed by
many people, and effectively, a significant number of them will be influenced
towards the brand. Given a trajectory and a billboard database and a positive
integer $k$, how can we select $k$ highly influential slots to maximize
influence? In this paper, we study a variant of this problem where a commercial
house wants to make a promotion of multiple products, and there is an influence
demand for each product. We have studied two variants of the problem. In the
first variant, our goal is to select $k$ slots such that the respective
influence demand of each product is satisfied. In the other variant of the
problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the
goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots,
S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i
\neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the
products gets satisfied. We model the first variant of the problem as a
multi-submodular cover problem and the second variant as its generalization.
For solving the first variant, we adopt the bi-criteria approximation
algorithm, and for the other variant, we propose a sampling-based approximation
algorithm. Extensive experiments with real-world trajectory and billboard
datasets highlight the effectiveness and efficiency of the proposed solution
approach.

</details>


### [29] [Random-Shift Revisited: Tight Approximations for Tree Embeddings and L1-Oblivious Routings](https://arxiv.org/abs/2510.09124)
*Rasmus Kyng,Maximilian Probst Gutenberg,Tim Rieder*

Main category: cs.DS

TL;DR: 本文提出了一种新的随机移位分解分析方法，证明了指数增长尺度下的分解在平均距离尺度上具有紧致的常数权衡，从而获得了具有预期O(log n)拉伸的树嵌入和ℓ1无意识路由的紧致竞争比。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单尺度下需要Ω(log n)的权衡，而本文旨在通过分析指数增长尺度的随机移位分解，获得更紧致的距离保持性能，为树嵌入和无意识路由提供更优的算法保证。

Method: 采用随机移位分解方法，分析指数增长尺度D=2^0, 2^1, ..., 2^(log_2(diam(G)))下的分解特性，证明在平均距离尺度上具有紧致的常数权衡。

Result: 获得了具有预期O(log n)拉伸的树嵌入，匹配了FRT结果的紧致近似界和Bartal的下界；同时改进了ℓ1无意识路由的分析，获得了紧致的O(log n)竞争比。

Conclusion: 提出的方法在顺序、并行和分布式设置中都能以最优的工作量、深度和轮数实现，为树嵌入和无意识路由提供了首个具有紧致保证的快速算法。

Abstract: We present a new and surprisingly simple analysis of random-shift
decompositions -- originally proposed by Miller, Peng, and Xu [SPAA'13]: We
show that decompositions for exponentially growing scales $D = 2^0, 2^1,
\ldots, 2^{\log_2(\operatorname{diam}(G))}$, have a tight constant trade-off
between distance-to-center and separation probability on average across the
distance scales -- opposed to a necessary $\Omega(\log n)$ trade-off for a
single scale.
  This almost immediately yields a way to compute a tree $T$ for graph $G$ that
preserves all graph distances with expected $O(\log n)$-stretch. This gives an
alternative proof that obtains tight approximation bounds of the seminal result
by Fakcharoenphol, Rao, and Talwar [STOC'03] matching the $\Omega(\log n)$
lower bound by Bartal [FOCS'96]. Our insights can also be used to refine the
analysis of a simple $\ell_1$-oblivious routing proposed in [FOCS'22], yielding
a tight $O(\log n)$ competitive ratio.
  Our algorithms for constructing tree embeddings and $\ell_1$-oblivious
routings can be implemented in the sequential, parallel, and distributed
settings with optimal work, depth, and rounds, up to polylogarithmic factors.
Previously, fast algorithms with tight guarantees were not known for tree
embeddings in parallel and distributed settings, and for $\ell_1$-oblivious
routings, not even a fast sequential algorithm was known.

</details>


### [30] [Confluence of the Node-Domination and Edge-Domination Hypergraph Rewrite Rules](https://arxiv.org/abs/2510.09286)
*Antoine Amarilli,Mikaël Monet,Rémi De Pretto*

Main category: cs.DS

TL;DR: 该论文研究了超图中的边支配和节点支配重写规则，证明了这些规则具有汇合性，即应用任意序列的规则后，得到的超图可以通过更多规则应用变得同构。


<details>
  <summary>Details</summary>
Motivation: 边支配和节点支配规则是计算超图最小命中集时常用的预处理规则，研究这些规则的汇合性有助于理解其理论性质。

Method: 通过形式化定义边支配和节点支配规则，分析这些规则在超图上的应用序列，证明其汇合性。

Result: 证明了边支配和节点支配规则是汇合的（在同构意义下），即任意规则应用序列最终会收敛到同构的超图。

Conclusion: 这些规则的汇合性保证了存在唯一的（在同构意义下）最小超图，为超图预处理提供了理论基础。

Abstract: In this note, we study two rewrite rules on hypergraphs, called
edge-domination and node-domination, and show that they are confluent. These
rules are rather natural and commonly used before computing the minimum hitting
sets of a hypergraph. Intuitively, edge-domination allows us to remove
hyperedges that are supersets of another hyperedge, and node-domination allows
us to remove nodes whose incident hyperedges are a subset of that of another
node. We show that these rules are confluent up to isomorphism, i.e., if we
apply any sequences of edge-domination and node-domination rules, then the
resulting hypergraphs can be made isomorphic via more rule applications. This
in particular implies the existence of a unique minimal hypergraph, up to
isomorphism.

</details>


### [31] [Improved Extended Regular Expression Matching](https://arxiv.org/abs/2510.09311)
*Philip Bille,Inge Li Gørtz,Rikke Schjeldrup Jessen*

Main category: cs.DS

TL;DR: 本文提出了一个改进的扩展正则表达式匹配算法，显著提升了时间和空间复杂度。新算法将时间复杂度从O(n³k)降低到O(n^ωk)，空间复杂度从O(n²k)降低到O(n²)，其中ω≈2.3716是矩阵乘法的指数。


<details>
  <summary>Details</summary>
Motivation: 扩展正则表达式匹配是形式语言理论中的基本问题，自Hopcroft和Ullmann在1970年代提出O(n³m)时间的算法以来，很少有显著的渐进改进。当前最优算法(Yamamoto和Miyazaki, 2003)虽然有所改进，但仍有提升空间。

Method: 作者重新审视该问题，开发了一种新算法，利用矩阵乘法技术来优化计算过程。算法将时间复杂度中的主导项n³k替换为n^ωk，同时将空间复杂度中的n²k项改进为O(n²)。

Result: 新算法实现了O(n^ωk + n²m/min(w/log w, log n) + m)的时间复杂度和O(n² log k/w + n + m) = O(n² + m)的空间复杂度，显著优于现有最佳算法。

Conclusion: 该研究在扩展正则表达式匹配问题上取得了突破性进展，通过利用矩阵乘法技术显著降低了时间和空间复杂度，为相关领域提供了更高效的解决方案。

Abstract: An extended regular expression $R$ specifies a set of strings formed by
characters from an alphabet combined with concatenation, union, intersection,
complement, and star operators. Given an extended regular expression $R$ and a
string $Q$, the extended regular expression matching problem is to decide if
$Q$ matches any of the strings specified by $R$. Extended regular expressions
are a basic concept in formal language theory and a basic primitive for
searching and processing data. Extended regular expression matching was
introduced by Hopcroft and Ullmann in the 1970s [\textit{Introduction to
Automata Theory, Languages and Computation}, 1979], who gave a simple dynamic
programming solution using $O(n^3m)$ time and $O(n^2m)$ space, where $n$ is the
length of $Q$ and $m$ is the length of $R$. Since then, several solutions have
been proposed, but few significant asymptotic improvements have been obtained.
The current state-of-the art solution, by Yamamoto and Miyazaki~[COCOON, 2003],
uses $O(\frac{n^3k + n^2m}{w} + n + m)$ time and $O(\frac{n^2k + nm}{w} + n +
m)$ space, where $k$ is the number of negation and complement operators in $R$
and $w$ is the number of bits in a word. This roughly replaces the $m$ factor
with $k$ in the dominant terms of both the space and time bounds of the
Hopcroft and Ullmann algorithm.
  We revisit the problem and present a new solution that significantly improves
the previous time and space bounds. Our main result is a new algorithm that
solves extended regular expression matching in \[O\left(n^\omega k +
\frac{n^2m}{\min(w/\log w, \log n)} + m\right)\] time and $O(\frac{n^2 \log
k}{w} + n + m) = O(n^2 +m)$ space, where $\omega \approx 2.3716$ is the
exponent of matrix multiplication. Essentially, this replaces the dominant
$n^3k$ term with $n^\omega k$ in the time bound, while simultaneously improving
the $n^2k$ term in the space to $O(n^2)$.

</details>


### [32] [Optimizing Administrative Divisions: A Vertex $k$-Center Approach for Edge-Weighted Road Graphs](https://arxiv.org/abs/2510.09334)
*Peteris Daugulis*

Main category: cs.DS

TL;DR: 提出基于Voronoi图和顶点k中心问题的数据驱动方法，用于优化行政区域划分，以最小化出行时间差异并平衡行政负担。


<details>
  <summary>Details</summary>
Motivation: 市政服务的高效公平获取依赖于良好的行政区域划分，需要适应人口、基础设施和经济因素的变化。

Method: 基于边加权道路图的Voronoi分区和顶点k中心问题（最小化设施选址问题的特例），考虑道路网络结构和行政中心战略布局。

Result: 在拉脱维亚（具有复杂地理特征和多样化人口分布）实现了该方法的应用。

Conclusion: 该方法能够有效减少出行时间差异，确保行政时间负担更均衡地分布。

Abstract: Efficient and equitable access to municipal services hinges on well-designed
administrative divisions. It requires ongoing adaptation to changing
demographics, infrastructure, and economic factors. This article proposes a
novel transparent data-driven method for territorial division based on the
Voronoi partition of edge-weighted road graphs and the vertex $k$-center
problem as a special case of the minimax facility location problem. By
considering road network structure and strategic placement of administrative
centers, this method seeks to minimize travel time disparities and ensure a
more balanced distribution of administrative time burden for the population. We
show implementations of this approach in the context of Latvia, a country with
complex geographical features and diverse population distribution.

</details>


### [33] [On Stable Cutsets in General and Minimum Degree Constrained Graphs](https://arxiv.org/abs/2510.09432)
*Mats Vroon,Hans L. Bodlaender*

Main category: cs.DS

TL;DR: 本文提出了稳定割集问题的改进精确算法，时间复杂度为O*(1.2972^n)，并研究了最小度约束下的稳定割集问题，提供了多项式时间算法和NP完全性证明。


<details>
  <summary>Details</summary>
Motivation: 稳定割集问题是NP完全问题，现有算法效率有限。本文旨在通过新的分支策略和利用现有算法，改进稳定割集问题的精确算法，并研究最小度约束下的复杂性。

Method: 通过在图配置上进行分支，并利用Beigel和Eppstein提出的O*(1.3645)算法解决(3,2)-约束满足问题，实现改进的运行时间。同时研究最小度约束下的算法设计。

Result: 实现了O*(1.2972^n)的改进运行时间；证明当最小度≥2/3(n-1)时图不含稳定割集；为最小度≥1/2n的图提供多项式时间算法；证明最小度>1时问题仍为NP完全。

Conclusion: 本文显著改进了稳定割集问题的精确算法，深入分析了最小度约束下的复杂性，并为相关图着色问题提供了改进算法。

Abstract: A stable cutset is a set of vertices $S$ of a connected graph, that is
pairwise non-adjacent and when deleting $S$, the graph becomes disconnected.
Determining the existence of a stable cutset in a graph is known to be
NP-complete. In this paper, we introduce a new exact algorithm for Stable
Cutset. By branching on graph configurations and using the $O^*(1.3645)$
algorithm for the (3,2)-Constraint Satisfaction Problem presented by Beigel and
Eppstein, we achieve an improved running time of $O^*(1.2972^n)$.
  In addition, we investigate the Stable Cutset problem for graphs with a bound
on the minimum degree $\delta$. First, we show that if the minimum degree of a
graph $G$ is at least $\frac{2}{3}(n-1)$, then $G$ does not contain a stable
cutset. Furthermore, we provide a polynomial-time algorithm for graphs where
$\delta \geq \tfrac{1}{2}n$, and a similar kernelisation algorithm for graphs
where $\delta = \tfrac{1}{2}n - k$.
  Finally, we prove that Stable Cutset remains NP-complete for graphs with
minimum degree $c$, where $c > 1$. We design an exact algorithm for this
problem that runs in $O^*(\lambda^n)$ time, where $\lambda$ is the positive
root of $x^{\delta + 2} - x^{\delta + 1} + 6$. This algorithm can also be
applied to the \textsc{3-Colouring} problem with the same minimum degree
constraint, leading to an improved exact algorithm as well.

</details>


### [34] [Parameterized Algorithms for Diversity of Networks with Ecological Dependencies](https://arxiv.org/abs/2510.09512)
*Mark Jones,Jannik Schestag*

Main category: cs.DS

TL;DR: 该论文研究在考虑食物网生态约束条件下，在系统发育网络中寻找具有最大系统发育多样性的物种集合问题，并提供了参数化复杂度分析和FPT算法。


<details>
  <summary>Details</summary>
Motivation: 系统发育多样性和生态约束（如食物网）在保护规划中都很重要，但之前的研究大多独立考虑这两个因素。需要开发同时考虑系统发育多样性和生态可行性的算法。

Method: 引入决策问题，结合系统发育网络和食物网约束，使用颜色编码方法开发新的算法框架，分析参数化复杂度（包括k、D、食物网scanwidth、网络最大入度和高度等参数）。

Result: 提供了完整的复杂度二分法，确定了导致W[1]-hardness和允许FPT算法的参数组合，提出了多个固定参数可处理算法。

Conclusion: 成功开发了在考虑生态依赖关系时解决系统发育多样性问题的算法框架，为保护生物学中的多目标优化提供了理论基础和实用工具。

Abstract: For a phylogenetic tree, the phylogenetic diversity of a set A of taxa is the
total weight of edges on paths to A. Finding small sets of maximal diversity is
crucial for conservation planning, as it indicates where limited resources can
be invested most efficiently. In recent years, efficient algorithms have been
developed to find sets of taxa that maximize phylogenetic diversity either in a
phylogenetic network or in a phylogenetic tree subject to ecological
constraints, such as a food web. However, these aspects have mostly been
studied independently. Since both factors are biologically important, it seems
natural to consider them together. In this paper, we introduce decision
problems where, given a phylogenetic network, a food web, and integers k, and
D, the task is to find a set of k taxa with phylogenetic diversity of at least
D under the maximize all paths measure, while also satisfying viability
conditions within the food web. Here, we consider different definitions of
viability, which all demand that a "sufficient" number of prey species survive
to support surviving predators. We investigate the parameterized complexity of
these problems and present several fixed-parameter tractable (FPT) algorithms.
Specifically, we provide a complete complexity dichotomy characterizing which
combinations of parameters - out of the size constraint k, the acceptable
diversity loss D, the scanwidth of the food web, the maximum in-degree in the
network, and the network height h - lead to W[1]-hardness and which admit FPT
algorithms. Our primary methodological contribution is a novel algorithmic
framework for solving phylogenetic diversity problems in networks where
dependencies (such as those from a food web) impose an order, using a color
coding approach.

</details>


### [35] [Minimizing the Weighted Makespan with Restarts on a Single Machine](https://arxiv.org/abs/2510.09589)
*Aflatoun Amouzandeh,Klaus Jansen,Lis Pirotton,Rob van Stee,Corinna Wambsganz*

Main category: cs.DS

TL;DR: 研究单机重启调度问题，目标是最小化加权完工时间。建立了确定性在线算法的竞争比下界1.4656，针对等处理时间情况设计了竞争比优于1.3098的算法，并证明了该情况下的下界为1.2344。


<details>
  <summary>Details</summary>
Motivation: 重启调度是比抢占更弱的调度方式，作业中断后必须从头开始执行。研究这种调度模型下的加权完工时间最小化问题，为实际调度系统提供理论支持。

Method: 使用竞争分析框架，建立理论下界并设计在线调度算法。针对等处理时间特殊情况，设计了改进的确定性在线算法。

Result: 证明了确定性在线算法的竞争比下界为1.4656；针对等处理时间情况，设计了竞争比优于1.3098的算法，并证明了该情况下的下界为1.2344。

Conclusion: 重启调度问题的竞争比存在显著的理论界限，等处理时间情况可以获得更好的算法性能，但仍存在明显的性能差距。

Abstract: We consider the problem of minimizing the weighted makespan on a single
machine with restarts. Restarts are similar to preemptions but weaker: a job
can be interrupted, but then it has to be run again from the start instead of
resuming at the point of interruption later. The objective is to minimize the
weighted makespan, defined as the maximum weighted completion time of jobs.
  We establish a lower bound of 1.4656 on the competitive ratio achievable by
deterministic online algorithms. For the case where all jobs have identical
processing times, we design and analyze a deterministic online algorithm that
improves the competitive ratio to better than 1.3098. Finally, we prove a lower
bound of 1.2344 for this case.

</details>
