<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Gaussian Mixture Flow Matching with Domain Alignment for Multi-Domain Sequential Recommendation](https://arxiv.org/abs/2510.21021)
*Xiaoxin Ye,Chengkai Huang,Hongtao Huang,Lina Yao*

Main category: cs.IR

TL;DR: GMFlowRec是一个高效的多域序列推荐生成框架，通过高斯混合流匹配建模域感知转移轨迹，在JD和Amazon数据集上实现了最先进的性能，NDCG@5提升高达44%。


<details>
  <summary>Details</summary>
Motivation: 用户在多域交互中产生频繁复杂的域转移，现有方法难以处理域异构性、不平衡性，且随着域数量增加难以扩展。

Method: 采用统一双掩码Transformer分离域不变和域特定意图，高斯混合流场捕捉多样化行为模式，域对齐先验支持频繁和稀疏转移。

Result: 在JD和Amazon数据集上，GMFlowRec在NDCG@5指标上提升高达44%，同时通过单一统一骨干保持高效率。

Conclusion: GMFlowRec通过生成式框架有效解决了多域序列推荐中的域转移挑战，具有可扩展性和实际应用价值。

Abstract: Users increasingly interact with content across multiple domains, resulting
in sequential behaviors marked by frequent and complex transitions. While
Cross-Domain Sequential Recommendation (CDSR) models two-domain interactions,
Multi-Domain Sequential Recommendation (MDSR) introduces significantly more
domain transitions, compounded by challenges such as domain heterogeneity and
imbalance. Existing approaches often overlook the intricacies of domain
transitions, tend to overfit to dense domains while underfitting sparse ones,
and struggle to scale effectively as the number of domains increases. We
propose \textit{GMFlowRec}, an efficient generative framework for MDSR that
models domain-aware transition trajectories via Gaussian Mixture Flow Matching.
GMFlowRec integrates: (1) a unified dual-masked Transformer to disentangle
domain-invariant and domain-specific intents, (2) a Gaussian Mixture flow field
to capture diverse behavioral patterns, and (3) a domain-aligned prior to
support frequent and sparse transitions. Extensive experiments on JD and Amazon
datasets demonstrate that GMFlowRec achieves state-of-the-art performance with
up to 44\% improvement in NDCG@5, while maintaining high efficiency via a
single unified backbone, making it scalable for real-world multi-domain
sequential recommendation.

</details>


### [2] [Communication Platform for Non-verbal Autistic children in Oman using Android mobile](https://arxiv.org/abs/2510.21028)
*Amna Al-Araimi,Yue Zheng,Haiming Liu*

Main category: cs.IR

TL;DR: 该论文提出一个结合网页面板和Android移动应用的平台，帮助阿曼的非语言自闭症儿童改善沟通能力，采用增强现实框架进行创造性游戏和自我反思活动。


<details>
  <summary>Details</summary>
Motivation: 针对全球普遍存在的非语言自闭症谱系障碍，现有技术应用分散且不系统，需要整合性解决方案来帮助患者应对社交和沟通需求。

Method: 开发包含网页面板和Android移动应用的平台，整合多种干预措施，利用增强现实框架创建交互式屏幕活动，促进创造性游戏和自我反思。

Result: 提出了一个综合性的技术平台方案，旨在通过增强现实技术改善非语言自闭症儿童的沟通能力和生活质量。

Conclusion: 整合性平台比分散方法更适合自闭症儿童，增强现实技术能够有效促进患者的创造性参与和自我反思能力发展。

Abstract: This paper discusses the issue regarding Non-verbal Autism Spectrum Disorder.
It has been observed that this mental disorder is listed in major parts of the
world including the US, UK, and India. To mitigate this type of disorder, a
wide range of smartphones, computers, and artificial intelligence technologies
have been used. This technology has helped the population cope with
socialization and communication needs. Many applications have been developed to
enhance the communication capabilities of non-verbal autistic children. This
thesis project proposes the development of a platform that includes a web panel
and an Android mobile application to assist non-verbal autistic children in
communication, especially in Oman. Different interventions have been merged to
improve the quality of life for people on the autism spectrum. The main problem
identified in this case is that fragmented approaches are not suitable for
autistic children. The augmented reality framework provides the capability to
engage autistic children in creative play and self-reflection through
interactive screen-based activities.

</details>


### [3] [VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion](https://arxiv.org/abs/2510.21151)
*David Guo,Minqi Sun,Yilun Jiang,Jiazhou Liang,Scott Sanner*

Main category: cs.IR

TL;DR: VOGUE是一个新颖的多模态对话推荐数据集，包含60个真实时尚购物场景下的人类对话，配有视觉目录、用户档案和历史记录，支持对话推理的严格评估。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对话推荐数据集存在模拟对话、忽略用户历史或反馈不足等限制，无法支持全面的研究和评估。

Method: 构建VOGUE数据集，包含人类对话、共享视觉目录、物品元数据、用户时尚档案和历史，以及对话后的双向评分。

Result: 分析发现视觉对话具有独特动态，推荐者常按特征组同时推荐物品；MLLMs在总体对齐上接近人类水平，但在评分分布和偏好推理泛化方面存在系统误差。

Conclusion: VOGUE是研究多模态对话系统的独特资源，也是当前顶级多模态基础模型推荐能力的挑战数据集。

Abstract: Multimodal conversational recommendation has emerged as a promising paradigm
for delivering personalized experiences through natural dialogue enriched by
visual and contextual grounding. Yet, current multimodal conversational
recommendation datasets remain limited: existing resources either simulate
conversations, omit user history, or fail to collect sufficiently detailed
feedback, all of which constrain the types of research and evaluation they
support.
  To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman
dialogues in realistic fashion shopping scenarios. Each dialogue is paired with
a shared visual catalogue, item metadata, user fashion profiles and histories,
and post-conversation ratings from both Seekers and Assistants. This design
enables rigorous evaluation of conversational inference, including not only
alignment between predicted and ground-truth preferences, but also calibration
against full rating distributions and comparison with explicit and implicit
user satisfaction signals.
  Our initial analyses of VOGUE reveal distinctive dynamics of visually
grounded dialogue. For example, recommenders frequently suggest items
simultaneously in feature-based groups, which creates distinct conversational
phases bridged by Seeker critiques and refinements. Benchmarking multimodal
large language models against human recommenders shows that while MLLMs
approach human-level alignment in aggregate, they exhibit systematic
distribution errors in reproducing human ratings and struggle to generalize
preference inference beyond explicitly discussed items. These findings
establish VOGUE as both a unique resource for studying multimodal
conversational systems and as a challenge dataset beyond the current
recommendation capabilities of existing top-tier multimodal foundation models
such as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.

</details>


### [4] [Bi-Level Optimization for Generative Recommendation: Bridging Tokenization and Generation](https://arxiv.org/abs/2510.21242)
*Yimeng Bai,Chang Liu,Yang Zhang,Dingxian Wang,Frank Yang,Andrew Rabinovich,Wenge Rong,Fuli Feng*

Main category: cs.IR

TL;DR: BLOGER是一个用于生成式推荐的双层优化框架，通过统一优化分词器和推荐器来解决现有方法中两者分离导致的标识符与推荐目标不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统通常将分词器和推荐器分开训练（顺序或交替），忽略了它们之间的相互依赖关系，导致分词器训练缺乏推荐目标的直接指导，产生次优的标识符从而降低推荐性能。

Method: 提出BLOGER框架，采用双层优化方法：下层训练推荐器使用分词后的序列，上层基于分词损失和推荐损失优化分词器。使用元学习方法高效求解双层优化，并引入梯度手术来缓解上层更新中的梯度冲突。

Result: 在真实世界数据集上的大量实验表明，BLOGER在保持实际效率的同时，始终优于最先进的生成式推荐方法，且没有显著增加计算开销。

Conclusion: BLOGER有效弥合了项目分词和自回归生成之间的差距，确保项目标识符既信息丰富又与推荐目标对齐。

Abstract: Generative recommendation is emerging as a transformative paradigm by
directly generating recommended items, rather than relying on matching.
Building such a system typically involves two key components: (1) optimizing
the tokenizer to derive suitable item identifiers, and (2) training the
recommender based on those identifiers. Existing approaches often treat these
components separately--either sequentially or in alternation--overlooking their
interdependence. This separation can lead to misalignment: the tokenizer is
trained without direct guidance from the recommendation objective, potentially
yielding suboptimal identifiers that degrade recommendation performance.
  To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative
Recommendation framework, which explicitly models the interdependence between
the tokenizer and the recommender in a unified optimization process. The lower
level trains the recommender using tokenized sequences, while the upper level
optimizes the tokenizer based on both the tokenization loss and recommendation
loss. We adopt a meta-learning approach to solve this bi-level optimization
efficiently, and introduce gradient surgery to mitigate gradient conflicts in
the upper-level updates, thereby ensuring that item identifiers are both
informative and recommendation-aligned. Extensive experiments on real-world
datasets demonstrate that BLOGER consistently outperforms state-of-the-art
generative recommendation methods while maintaining practical efficiency with
no significant additional computational overhead, effectively bridging the gap
between item tokenization and autoregressive generation.

</details>


### [5] [Pctx: Tokenizing Personalized Context for Generative Recommendation](https://arxiv.org/abs/2510.21276)
*Qiyong Zhong,Jiajie Su,Yunshan Ma,Julian McAuley,Yupeng Hou*

Main category: cs.IR

TL;DR: 提出个性化上下文感知分词器，将用户历史交互融入语义ID生成过程，使同一物品在不同用户上下文中被分词为不同语义ID，从而在生成式推荐模型中实现更个性化的预测。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法是非个性化的，仅从物品特征派生语义ID，假设存在通用的物品相似性标准，忽略了用户特定的视角。在实践中，同一物品可能因用户意图和偏好而被不同解读。

Method: 设计个性化上下文感知分词器，在生成语义ID时融入用户的历史交互信息，使同一物品在不同用户上下文中被分词为不同的语义ID。

Result: 在三个公共数据集上的实验表明，相比非个性化动作分词基线，NDCG@10指标提升了最高达11.44%。

Conclusion: 个性化上下文感知分词器能够使生成式推荐模型捕捉多种解释标准，产生更个性化的预测，显著提升推荐性能。

Abstract: Generative recommendation (GR) models tokenize each action into a few
discrete tokens (called semantic IDs) and autoregressively generate the next
tokens as predictions, showing advantages such as memory efficiency,
scalability, and the potential to unify retrieval and ranking. Despite these
benefits, existing tokenization methods are static and non-personalized. They
typically derive semantic IDs solely from item features, assuming a universal
item similarity that overlooks user-specific perspectives. However, under the
autoregressive paradigm, semantic IDs with the same prefixes always receive
similar probabilities, so a single fixed mapping implicitly enforces a
universal item similarity standard across all users. In practice, the same item
may be interpreted differently depending on user intentions and preferences. To
address this issue, we propose a personalized context-aware tokenizer that
incorporates a user's historical interactions when generating semantic IDs.
This design allows the same item to be tokenized into different semantic IDs
under different user contexts, enabling GR models to capture multiple
interpretive standards and produce more personalized predictions. Experiments
on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over
non-personalized action tokenization baselines. Our code is available at
https://github.com/YoungZ365/Pctx.

</details>


### [6] [CausalRec: A CausalBoost Attention Model for Sequential Recommendation](https://arxiv.org/abs/2510.21333)
*Yunbo Hou,Tianle Yang,Ruijie Li,Li He,Liang Wang,Weiping Li,Bo Zheng,Guojie Song*

Main category: cs.IR

TL;DR: 提出CausalRec框架，通过因果注意力机制改进序列推荐系统，结合因果发现模块和CausalBooster，提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性的序列推荐系统仅关注项目共现，忽略了用户行为背后的因果动机，导致虚假相关性和不准确的推荐。

Method: 使用因果发现模块学习用户行为序列中的因果图，并通过CausalBooster利用发现的因果图来优化注意力机制，优先考虑具有因果重要性的行为。

Result: 在真实数据集上的实验表明，CausalRec在命中率(HR)和归一化折损累计增益(NDCG)上分别平均提升7.21%和8.65%，优于现有最先进方法。

Conclusion: 这是首个通过注意力机制在序列推荐中引入因果关系的模型，证明了因果关系在生成更准确可靠推荐中的价值。

Abstract: Recent advances in correlation-based sequential recommendation systems have
demonstrated substantial success. Specifically, the attention-based model
outperforms other RNN-based and Markov chains-based models by capturing both
short- and long-term dependencies more effectively. However, solely focusing on
item co-occurrences overlooks the underlying motivations behind user behaviors,
leading to spurious correlations and potentially inaccurate recommendations. To
address this limitation, we present a novel framework that integrates causal
attention for sequential recommendation, CausalRec. It incorporates a causal
discovery block and a CausalBooster. The causal discovery block learns the
causal graph in user behavior sequences, and we provide a theory to guarantee
the identifiability of the learned causal graph. The CausalBooster utilizes the
discovered causal graph to refine the attention mechanism, prioritizing
behaviors with causal significance. Experimental evaluations on real-world
datasets indicate that CausalRec outperforms several state-of-the-art methods,
with average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized
Discounted Cumulative Gain (NDCG). To the best of our knowledge, this is the
first model to incorporate causality through the attention mechanism in
sequential recommendation, demonstrating the value of causality in generating
more accurate and reliable recommendations.

</details>


### [7] [SciNUP: Natural Language User Interest Profiles for Scientific Literature Recommendation](https://arxiv.org/abs/2510.21352)
*Mariam Arustashvili,Krisztian Balog*

Main category: cs.IR

TL;DR: SciNUP是一个用于学术推荐的新型合成数据集，利用作者发表历史生成自然语言用户档案，填补了该领域公开测试集的空白。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言用户档案推荐系统中缺乏大规模公开测试集的问题，为评估NL档案推荐提供资源。

Method: 创建SciNUP合成数据集，基于作者发表历史生成自然语言档案和对应真实项目，比较稀疏检索、密集检索和基于LLM的重排序等基线方法。

Result: 基线方法性能相当但检索项目不同，显示互补行为；同时存在显著改进空间，表明需要更有效的NL推荐方法。

Conclusion: SciNUP数据集为推动该领域未来研究和发展提供了宝贵资源。

Abstract: The use of natural language (NL) user profiles in recommender systems offers
greater transparency and user control compared to traditional representations.
However, there is scarcity of large-scale, publicly available test collections
for evaluating NL profile-based recommendation. To address this gap, we
introduce SciNUP, a novel synthetic dataset for scholarly recommendation that
leverages authors' publication histories to generate NL profiles and
corresponding ground truth items. We use this dataset to conduct a comparison
of baseline methods, ranging from sparse and dense retrieval approaches to
state-of-the-art LLM-based rerankers. Our results show that while baseline
methods achieve comparable performance, they often retrieve different items,
indicating complementary behaviors. At the same time, considerable headroom for
improvement remains, highlighting the need for effective NL-based
recommendation approaches. The SciNUP dataset thus serves as a valuable
resource for fostering future research and development in this area.

</details>


### [8] [Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research](https://arxiv.org/abs/2510.21603)
*Kuicai Dong,Shurui Huang,Fangda Ye,Wei Han,Zhi Zhang,Dexun Li,Wenjun Li,Qu Yang,Gang Wang,Yichao Wang,Chen Zhang,Yong Liu*

Main category: cs.IR

TL;DR: Doc-Researcher是一个统一的多模态文档深度研究系统，通过多模态解析、系统化检索架构和迭代多智能体工作流，解决了现有系统局限于文本数据的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统仅局限于文本网络数据，忽略了多模态文档中嵌入的丰富知识（图表、公式等），需要能够保持视觉语义的解析和跨模态检索能力。

Method: 系统包含三个核心组件：深度多模态解析（保持布局结构和视觉语义）、系统化检索架构（支持文本、视觉和混合检索）、迭代多智能体工作流（分解查询、积累证据、综合答案）。

Result: 在M4DocBench基准测试中，Doc-Researcher达到50.6%准确率，比最先进基线方法提升3.4倍，验证了多模态完整性和迭代研究的重要性。

Conclusion: 这项工作为多模态文档集合的深度研究建立了新范式，表明有效的文档研究不仅需要更好的检索，更需要能够保持多模态完整性的深度解析。

Abstract: Deep Research systems have revolutionized how LLMs solve complex questions
through iterative reasoning and evidence gathering. However, current systems
remain fundamentally constrained to textual web data, overlooking the vast
knowledge embedded in multimodal documents Processing such documents demands
sophisticated parsing to preserve visual semantics (figures, tables, charts,
and equations), intelligent chunking to maintain structural coherence, and
adaptive retrieval across modalities, which are capabilities absent in existing
systems. In response, we present Doc-Researcher, a unified system that bridges
this gap through three integrated components: (i) deep multimodal parsing that
preserves layout structure and visual semantics while creating multi-granular
representations from chunk to document level, (ii) systematic retrieval
architecture supporting text-only, vision-only, and hybrid paradigms with
dynamic granularity selection, and (iii) iterative multi-agent workflows that
decompose complex queries, progressively accumulate evidence, and synthesize
comprehensive answers across documents and modalities. To enable rigorous
evaluation, we introduce M4DocBench, the first benchmark for Multi-modal,
Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158
expert-annotated questions with complete evidence chains across 304 documents,
M4DocBench tests capabilities that existing benchmarks cannot assess.
Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter
than state-of-the-art baselines, validating that effective document research
requires not just better retrieval, but fundamentally deep parsing that
preserve multimodal integrity and support iterative research. Our work
establishes a new paradigm for conducting deep research on multimodal document
collections.

</details>


### [9] [A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study on Query-Category and Query-Item Relevance](https://arxiv.org/abs/2510.21671)
*Yabo Yin,Yang Xi,Jialong Wang,Shanqi Wang,Jiateng Hu*

Main category: cs.IR

TL;DR: 针对多语言电商搜索中的数据不平衡、标签噪声和低资源语言监督不足问题，提出了一个数据中心的框架，通过翻译增强、语义负采样和自验证过滤三种策略改进训练数据，在CIKM AnalytiCup 2025数据集上显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 多语言电商搜索面临跨语言数据不平衡、标签噪声和低资源语言监督不足的挑战，阻碍了相关性模型的跨语言泛化能力，尽管大型语言模型具有强大能力。

Method: 采用架构无关的数据中心框架，包含三种策略：(1)基于翻译的增强，为训练中缺失的语言合成示例；(2)语义负采样，生成困难负样本并缓解类别不平衡；(3)自验证过滤，检测并移除可能错误标记的实例。

Result: 在CIKM AnalytiCup 2025数据集上评估，该方法相比强大的LLM基线持续获得显著的F1分数提升，在官方竞赛中取得了有竞争力的结果。

Conclusion: 系统化的数据工程可以与复杂模型修改一样有效，且通常更易于部署，为在真实世界电商环境中构建稳健的多语言搜索系统提供了可行的指导。

Abstract: Multilingual e-commerce search suffers from severe data imbalance across
languages, label noise, and limited supervision for low-resource
languages--challenges that impede the cross-lingual generalization of relevance
models despite the strong capabilities of large language models (LLMs). In this
work, we present a practical, architecture-agnostic, data-centric framework to
enhance performance on two core tasks: Query-Category (QC) relevance (matching
queries to product categories) and Query-Item (QI) relevance (matching queries
to product titles). Rather than altering the model, we redesign the training
data through three complementary strategies: (1) translation-based augmentation
to synthesize examples for languages absent in training, (2) semantic negative
sampling to generate hard negatives and mitigate class imbalance, and (3)
self-validation filtering to detect and remove likely mislabeled instances.
Evaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields
substantial F1 score improvements over strong LLM baselines, achieving
competitive results in the official competition. Our findings demonstrate that
systematic data engineering can be as impactful as--and often more deployable
than--complex model modifications, offering actionable guidance for building
robust multilingual search systems in the real-world e-commerce settings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [World-POI: Global Point-of-Interest Data Enriched from Foursquare and OpenStreetMap as Tabular and Graph Data](https://arxiv.org/abs/2510.21342)
*Hossein Amiri,Mohammad Hashemi,Andreas Züfle*

Main category: cs.DB

TL;DR: 提出了一个整合Foursquare和OpenStreetMap POI数据的方法，通过名称相似度和空间距离计算进行记录链接，构建了约1TB的增强数据集，并提供过滤版本供实际使用。


<details>
  <summary>Details</summary>
Motivation: Foursquare的全球POI数据集虽然规模庞大但缺乏完整元数据，而OpenStreetMap提供丰富的元数据但未验证POI真实性。需要结合两者的优势来创建更完整可靠的POI数据集。

Method: 计算Foursquare和OSM POI之间的名称相似度得分和空间距离，识别高置信度匹配，构建图结构的POI表示并整合两个数据源的属性。

Result: 生成了约1TB的完整数据集，通过过滤阈值提供实用版本，构建了包含631GB数据的可复现构建版本，创建了增强属性的图结构POI表示。

Conclusion: 该方法成功整合了两个POI数据源的优势，为空间分析和下游应用提供了更丰富可靠的POI数据集，并通过可调节的过滤机制解决了大规模数据的实用性问题。

Abstract: Recently, Foursquare released a global dataset with more than 100 million
points of interest (POIs), each representing a real-world business on its
platform. However, many entries lack complete metadata such as addresses or
categories, and some correspond to non-existent or fictional locations. In
contrast, OpenStreetMap (OSM) offers a rich, user-contributed POI dataset with
detailed and frequently updated metadata, though it does not formally verify
whether a POI represents an actual business. In this data paper, we present a
methodology that integrates the strengths of both datasets: Foursquare as a
comprehensive baseline of commercial POIs and OSM as a source of enriched
metadata. The combined dataset totals approximately 1 TB. While this full
version is not publicly released, we provide filtered releases with adjustable
thresholds that reduce storage needs and make the data practical to download
and use across domains. We also provide step-by-step instructions to reproduce
the full 631 GB build. Record linkage is achieved by computing name similarity
scores and spatial distances between Foursquare and OSM POIs. These measures
identify and retain high-confidence matches that correspond to real businesses
in Foursquare, have representations in OSM, and show strong name similarity.
Finally, we use this filtered dataset to construct a graph-based representation
of POIs enriched with attributes from both sources, enabling advanced spatial
analyses and a range of downstream applications.

</details>


### [11] [SurVigilance: An Application for Accessing Global Pharmacovigilance Data](https://arxiv.org/abs/2510.21572)
*Raktim Mukhopadhyay,Marianthi Markatou*

Main category: cs.DB

TL;DR: SurVigilance是一个开源工具，简化了从七个主要药物警戒数据库中检索安全数据的过程，提供图形界面和编程接口。


<details>
  <summary>Details</summary>
Motivation: 现有工具通常只关注单个数据库，而从公开的药物警戒数据库中提取数据在技术上具有挑战性。

Method: 采用模块化架构访问异构数据源，提供图形用户界面和程序化访问功能。

Result: 开发了一个能够访问七个主要药物警戒数据库的工具，降低了技术门槛。

Conclusion: 通过降低访问安全数据的技术障碍，SurVigilance旨在促进药物警戒研究。

Abstract: Even though several publicly accessible pharmacovigilance databases are
available, extracting data from them is a technically challenging process.
Existing tools typically focus on a single database. We present SurVigilance,
an open-source tool that streamlines the process of retrieving safety data from
seven major pharmacovigilance databases. SurVigilance provides a graphical user
interface as well as functions for programmatic access, thus enabling
integration into existing research workflows. SurVigilance utilizes a modular
architecture to provide access to the heterogeneous sources. By reducing the
technical barriers to accessing safety data, SurVigilance aims to facilitate
pharmacovigilance research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [12] [Low Cost, Fair, and Representative Committees in a Metric Space](https://arxiv.org/abs/2510.21039)
*Christopher Jerrett,Elliot Anshelevich*

Main category: cs.GT

TL;DR: 该论文研究在度量空间中从n个代理中选择k个代表委员会的问题，提出了一种既能最小化总距离成本，又能满足多种公平性和比例性概念，同时防止任何代理子集过度代表的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统设施选址问题中每个代理只关心最近的设施，但本文考虑每个代理希望所有委员会成员都接近自己的场景，需要找到既能最小化总距离成本，又能保证公平性和代表性的委员会选择方案。

Method: 引入新的公理NORP来防止代理子集的过度代表，并开发新的算法来同时实现低成本、公平性和NORP特性。

Result: 证明总是可以找到既满足低成本目标，又符合多种现有公平性概念，同时满足新提出的NORP公理的委员会选择方案。

Conclusion: 在度量空间中选择代表性委员会时，可以同时实现低成本、公平性和防止过度代表的目标，为委员会选择问题提供了更全面的解决方案。

Abstract: We study the problem of selecting a representative committee of $k$ agents
from a collection of $n$ agents in a common metric space. This problem is
related to choosing $k$ facilities in facility location and $k$-median
problems. However, unlike in more traditional facility location where each
agent only cares about the closest selected facility, in the settings we
consider each agent desires that all selected committee members are close to
them. More precisely, we look at the sum objective, in which the goal is to
minimize the total distance from all agents to all members of the chosen
committee. We show that it is always possible to find a committee which is both
low-cost according to this objective, and also fair according to many existing
notions of fairness and proportionality defined for clustering settings.
Moreover, we introduce a new desirable axiom for representative committees we
call NORP, which prevents over-representation of any subset of agents. While
all existing algorithms for fair committee selection do not satisfy this
intuitive property, we provide new algorithms which form simultaneously
low-cost, fair, and NORP solutions, thus showing that it is always possible to
form low-cost, fair, and representative committees for our settings.

</details>


### [13] [Instance-Adaptive Hypothesis Tests with Heterogeneous Agents](https://arxiv.org/abs/2510.21178)
*Flora C. Shi,Martin J. Wainwright,Stephen Bates*

Main category: cs.GT

TL;DR: 该论文研究了在具有私有信息的战略异质群体中进行假设检验的问题，提出通过设计统计合同菜单来诱导代理人根据私有信息自选择，从而匹配预言机性能。


<details>
  <summary>Details</summary>
Motivation: 传统统一测试在异质战略群体中会产生次优的统计误差，无法利用代理人的私有信息来优化检验性能。

Method: 设计统计合同菜单，将类型最优测试与支付结构配对，诱导代理人根据私有信息自选择，实现类型分离。

Result: 完全描述了所有分离菜单的集合，这些菜单能够匹配预言机性能，且在某些设计中信息获取几乎无成本。

Conclusion: 该研究将统计决策理论与机制设计相结合，展示了如何利用异质性和战略参与来提高假设检验的效率。

Abstract: We study hypothesis testing over a heterogeneous population of strategic
agents with private information. Any single test applied uniformly across the
population yields statistical error that is sub-optimal relative to the
performance of an oracle given access to the private information. We show how
it is possible to design menus of statistical contracts that pair type-optimal
tests with payoff structures, inducing agents to self-select according to their
private information. This separating menu elicits agent types and enables the
principal to match the oracle performance even without a priori knowledge of
the agent type. Our main result fully characterizes the collection of all
separating menus that are instance-adaptive, matching oracle performance for an
arbitrary population of heterogeneous agents. We identify designs where
information elicitation is essentially costless, requiring negligible
additional expense relative to a single-test benchmark, while improving
statistical performance. Our work establishes a connection between proper
scoring rules and menu design, showing how the structure of the hypothesis test
constrains the elicitable information. Numerical examples illustrate the
geometry of separating menus and the improvements they deliver in error
trade-offs. Overall, our results connect statistical decision theory with
mechanism design, demonstrating how heterogeneity and strategic participation
can be harnessed to improve efficiency in hypothesis testing.

</details>


### [14] [Shift Bribery over Social Networks](https://arxiv.org/abs/2510.21200)
*Ashlesha Hota,Susobhan Bandopadhyay,Palash Dey,Shruti Thiagu*

Main category: cs.GT

TL;DR: 该论文研究了社交网络中的移位贿赂问题，将传统独立选民模型扩展到网络环境中，考虑贿赂效果通过社交网络传播的影响，并分析了该问题的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统移位贿赂模型假设选民独立行动，忽略了社交网络中的影响力传播。现实中，被贿赂的选民会影响其邻居，从而放大贿赂效果。

Method: 将选民建模为有向加权图中的节点，边表示社交影响力。考虑贿赂策略在预算约束下，通过直接贿赂和网络传播影响确保偏好候选人获胜。

Result: 证明该问题在双候选人和单位成本下是NP完全的，在预算或最大度参数化下是W[2]-难的。设计了多种特殊图结构下的多项式时间算法和伪多项式算法。

Conclusion: 为社交网络中的移位贿赂问题提供了详细的复杂性分析，揭示了网络结构对问题可解性的重要影响，并给出了多种图类的高效算法。

Abstract: In shift bribery, a briber seeks to promote his preferred candidate by paying
voters to raise their ranking. Classical models of shift bribery assume voters
act independently, overlooking the role of social influence. However, in
reality, individuals are social beings and are often represented as part of a
social network, where bribed voters may influence their neighbors, thereby
amplifying the effect of persuasion. We study Shift bribery over Networks,
where voters are modeled as nodes in a directed weighted graph, and arcs
represent social influence between them. In this setting, bribery is not
confined to directly targeted voters its effects can propagate through the
network, influencing neighbors and amplifying persuasion. Given a budget and
individual cost functions for shifting each voter's preference toward a
designated candidate, the goal is to determine whether a shift strategy exists
within budget that ensures the preferred candidate wins after both direct and
network-propagated influence takes effect. We show that the problem is
NP-Complete even with two candidates and unit costs, and W[2]-hard when
parameterized by budget or maximum degree. On the positive side, we design
polynomial-time algorithms for complete graphs under plurality and majority
rules and path graphs for uniform edge weights, linear-time algorithms for
transitive tournaments for two candidates, linear cost functions and uniform
arc weights, and pseudo-polynomial algorithms for cluster graphs. We further
prove the existence of fixed-parameter tractable algorithms with treewidth as
parameter for two candidates, linear cost functions and uniform arc weights and
pseudo-FPT with cluster vertex deletion number for two candidates and uniform
arc weights. Together, these results give a detailed complexity landscape for
shift bribery in social networks.

</details>


### [15] [Scale-robust Auctions](https://arxiv.org/abs/2510.21231)
*Jason Hartline,Aleck Johnsen,Yingkai Li*

Main category: cs.GT

TL;DR: 该论文研究了在不同价格尺度下都能保持鲁棒性的拍卖机制，证明了最优机制是尺度不变的，通过随机化在第二价格和2.45倍第二价格之间进行销售。


<details>
  <summary>Details</summary>
Motivation: 研究能够在昂贵和廉价物品销售中都适用的鲁棒拍卖机制，实现在最坏情况下对最优收入的最佳乘性逼近。

Method: 证明最优机制是尺度不变的，采用随机化策略在第二价格和2.45倍第二价格之间进行销售。

Result: 发现最优拍卖机制是尺度不变的，能够通过随机化在第二价格和2.45倍第二价格之间销售来实现最佳乘性收入逼近。

Conclusion: 尺度不变的最优拍卖机制通过随机化策略在第二价格和2.45倍第二价格之间销售，能够在不同价格尺度下实现鲁棒的收入最大化。

Abstract: We study auctions that are robust at any scale, i.e., they can be applied to
sell both expensive and cheap items and achieve the best multiplicative
approximations of the optimal revenue in the worst case. We show that the
optimal mechanism is scale invariant, which randomizes between selling at the
second-price and a 2.45 multiple of the second-price.

</details>


### [16] [Scalable Neural Incentive Design with Parameterized Mean-Field Approximation](https://arxiv.org/abs/2510.21442)
*Nathan Corecco,Batuhan Yardim,Vinzenz Thoma,Zebang Shen,Niao He*

Main category: cs.GT

TL;DR: 该论文提出了一种用于多智能体系统激励设计的参数化平均场博弈方法，通过无限人口极限降低复杂度，并开发了AMID算法来高效计算梯度。


<details>
  <summary>Details</summary>
Motivation: 为多智能体系统设计激励以诱导期望的纳什均衡是一个关键但具有挑战性的问题，特别是在智能体数量N很大时。

Method: 将激励设计问题形式化为参数化平均场博弈，在可交换性假设下通过无限人口极限降低复杂度，并提出了AMID算法，使用均衡算子的显式微分来高效计算梯度。

Result: 证明了在Lipschitz条件下有限N目标以O(1/√N)速率被PMFG逼近，在序列拍卖等非连续情况下也保持相同收敛速率。AMID算法在多种拍卖设置中显著提高了收入，优于现有基准方法。

Conclusion: AMID算法结合了逼近边界和优化保证，为大规模多智能体激励设计提供了一个强大且可扩展的算法工具。

Abstract: Designing incentives for a multi-agent system to induce a desirable Nash
equilibrium is both a crucial and challenging problem appearing in many
decision-making domains, especially for a large number of agents $N$. Under the
exchangeability assumption, we formalize this incentive design (ID) problem as
a parameterized mean-field game (PMFG), aiming to reduce complexity via an
infinite-population limit. We first show that when dynamics and rewards are
Lipschitz, the finite-$N$ ID objective is approximated by the PMFG at rate
$\mathscr{O}(\frac{1}{\sqrt{N}})$. Moreover, beyond the Lipschitz-continuous
setting, we prove the same $\mathscr{O}(\frac{1}{\sqrt{N}})$ decay for the
important special case of sequential auctions, despite discontinuities in
dynamics, through a tailored auction-specific analysis. Built on our novel
approximation results, we further introduce our Adjoint Mean-Field Incentive
Design (AMID) algorithm, which uses explicit differentiation of iterated
equilibrium operators to compute gradients efficiently. By uniting
approximation bounds with optimization guarantees, AMID delivers a powerful,
scalable algorithmic tool for many-agent (large $N$) ID. Across diverse auction
settings, the proposed AMID method substantially increases revenue over
first-price formats and outperforms existing benchmark methods.

</details>


### [17] [Privacy Guarantee for Nash Equilibrium Computation of Aggregative Games Based on Pointwise Maximal Leakage](https://arxiv.org/abs/2510.21668)
*Zhaoyang Cheng,Guanpu Chen,Tobias J. Oechtering,Mikael Skoglund*

Main category: cs.GT

TL;DR: 本文提出了点式最大泄漏（PML）框架来改进纳什均衡计算中的隐私保护，相比差分隐私（DP）能更精确地评估相关数据集的信息泄漏。


<details>
  <summary>Details</summary>
Motivation: 差分隐私在纳什均衡计算中无法充分利用数据集先验分布知识，且难以有效评估相关数据集的信息泄漏问题。

Method: 建立点式最大泄漏（PML）框架，结合玩家成本函数数据集的先验知识，获得具有PML保证的隐私泄漏精确可计算上界。

Result: PML通过提供更严格的隐私保证来改进DP，在相关数据集构造下，PML下界可能超过DP上界，表明PML是更合适的隐私度量。

Conclusion: PML是比DP更合适的隐私度量标准，因为后者无法充分捕捉相关数据集中的隐私泄漏，实验验证了该框架的有效性。

Abstract: Privacy preservation has served as a key metric in designing Nash equilibrium
(NE) computation algorithms. Although differential privacy (DP) has been widely
employed for privacy guarantees, it does not exploit prior distributional
knowledge of datasets and is ineffective in assessing information leakage for
correlated datasets. To address these concerns, we establish a pointwise
maximal leakage (PML) framework when computing NE in aggregative games. By
incorporating prior knowledge of players' cost function datasets, we obtain a
precise and computable upper bound of privacy leakage with PML guarantees. In
the entire view, we show PML refines DP by offering a tighter privacy
guarantee, enabling flexibility in designing NE computation. Also, in the
individual view, we reveal that the lower bound of PML can exceed the upper
bound of DP by constructing specific correlated datasets. The results emphasize
that PML is a more proper privacy measure than DP since the latter fails to
adequately capture privacy leakage in correlated datasets. Moreover, we conduct
experiments with adversaries who attempt to infer players' private information
to illustrate the effectiveness of our framework.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [18] [Hardness of Approximation for Shortest Path with Vector Costs](https://arxiv.org/abs/2510.21058)
*Charlie Carlson,Yury Makarychev,Ron Mosenzon*

Main category: cs.DS

TL;DR: 该论文获得了ℓ_p-最短路径问题的近似硬度结果，对于p∈[2,∞)证明了Ω(p(log n/log²log n)^{1-1/p})的硬度，对于p=∞证明了Ω̃(log² n)的硬度，这些结果几乎匹配了已知的近似算法性能。


<details>
  <summary>Details</summary>
Motivation: 研究ℓ_p-最短路径问题的计算复杂性，填补了p<∞情况下近似硬度结果的空白，并改进p=∞情况下的硬度界限。

Method: 通过理论分析和数学证明，建立了ℓ_p-最短路径问题的近似硬度下界，并推导了高阶Bell数的渐近界。

Result: 对于p∈[2,∞)，证明了多项式时间和拟多项式时间算法的近似硬度为Ω(p(log n/log²log n)^{1-1/p})；对于p=∞，证明了硬度为Ω̃(log² n)。

Conclusion: 该工作几乎完全匹配了已知近似算法的性能，为ℓ_p-最短路径问题的计算复杂性提供了重要的理论界限。

Abstract: We obtain hardness of approximation results for the $\ell_p$-Shortest Path
problem, a variant of the classic Shortest Path problem with vector costs. For
every integer $p \in [2,\infty)$, we show a hardness of $\Omega(p(\log n /
\log^2\log n)^{1-1/p})$ for both polynomial- and quasi-polynomial-time
approximation algorithms. This nearly matches the approximation factor of
$O(p(\log n / \log\log n)^{1-1/p})$ achieved by a quasi-polynomial-time
algorithm of Makarychev, Ovsiankin, and Tani (ICALP 2025). No hardness of
approximation results were previously known for any $p < \infty$. We also
present results for the case where $p$ is a function of $n$. For $p = \infty$,
we establish a hardness of $\tilde\Omega(\log^2 n)$, improving upon the
previous $\tilde\Omega(\log n)$ hardness result. Our result nearly matches the
$O(\log^2 n)$ approximation guarantee of the quasi-polynomial-time algorithm by
Li, Xu, and Zhang (ICALP 2025). Finally, we present asymptotic bounds on
higher-order Bell numbers, which might be of independent interest.

</details>


### [19] [A Unified Approach to Submodular Maximization Under Noise](https://arxiv.org/abs/2510.21128)
*Kshipra Bhawalkar,Yang Cai,Zhe Feng,Christopher Liaw,Tao Lin*

Main category: cs.DS

TL;DR: 提出一种元算法，可将精确子模最大化的鲁棒算法转换为噪声环境下的算法，保持近似保证。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声值预言机下进行子模最大化的问题，现有方法只能处理特定约束，需要更通用的解决方案。

Method: 设计元算法框架，将精确子模最大化的鲁棒算法作为黑盒，通过噪声处理机制转换为噪声环境下的算法。

Result: 使用元算法结合measured continuous greedy获得单调子模的(1-1/e)近似和非单调子模的1/e近似；结合double greedy获得无约束非单调子模的1/2近似。

Conclusion: 元算法提供了一种通用框架，可将精确子模最大化算法扩展到噪声环境，显著提升了噪声子模最大化的理论保证。

Abstract: We consider the problem of maximizing a submodular function with access to a
noisy value oracle for the function instead of an exact value oracle. Similar
to prior work, we assume that the noisy oracle is persistent in that multiple
calls to the oracle for a specific set always return the same value. In this
model, Hassidim and Singer (2017) design a $(1-1/e)$-approximation algorithm
for monotone submodular maximization subject to a cardinality constraint, and
Huang et al (2022) design a $(1-1/e)/2$-approximation algorithm for monotone
submodular maximization subject to any arbitrary matroid constraint. In this
paper, we design a meta-algorithm that allows us to take any "robust" algorithm
for exact submodular maximization as a black box and transform it into an
algorithm for the noisy setting while retaining the approximation guarantee. By
using the meta-algorithm with the measured continuous greedy algorithm, we
obtain a $(1-1/e)$-approximation (resp. $1/e$-approximation) for monotone
(resp. non-monotone) submodular maximization subject to a matroid constraint
under noise. Furthermore, by using the meta-algorithm with the double greedy
algorithm, we obtain a $1/2$-approximation for unconstrained (non-monotone)
submodular maximization under noise.

</details>


### [20] [Unsplittable Cost Flows from Unweighted Error-Bounded Variants](https://arxiv.org/abs/2510.21287)
*Chaitanya Swamy,Vera Traub,Laura Vargas Koch,Rico Zenklusen*

Main category: cs.DS

TL;DR: 本文证明了Morell和Skutella的猜想蕴含Goemans的猜想（需求违反度为最大需求的两倍），并推广了Linhares和Swamy的技术，为加权环加载问题提供了成本保证的近似算法。


<details>
  <summary>Details</summary>
Motivation: 解决Goemans关于单源不可分流量的著名猜想，该猜想认为可以将任何分数流量转换为不可分流量而不增加成本，同时任何弧的负载增加不超过最大需求。

Method: 推广Linhares和Swamy的技术，使用简单的初等证明方法，将无成本保证的算法转换为具有成本保证的近似算法。

Result: 证明了Morell和Skutella猜想蕴含Goemans猜想（违反度为最大需求的两倍），并将该技术应用于加权环加载问题。

Conclusion: 提供了一种将成本无感知近似算法转换为具有额外成本保证的近似算法的通用技术，对相关领域具有独立意义。

Abstract: A famous conjecture of Goemans on single-source unsplittable flows states
that one can turn any fractional flow into an unsplittable one of no higher
cost, while increasing the load on any arc by at most the maximum demand.
Despite extensive work on the topic, only limited progress has been made.
Recently, Morell and Skutella suggested an alternative conjecture, stating that
one can turn any fractional flow into an unsplittable one without changing the
load on any arc by more than the maximum demand.
  We show that their conjecture implies Goemans' conjecture (with a violation
of twice the maximum demand). To this end, we generalize a technique of
Linhares and Swamy, used to obtain a low-cost chain-constrained spanning tree
from an algorithm without cost guarantees. Whereas Linhares and Swamy's proof
relies on Langrangian duality, we provide a very simple elementary proof of a
generalized version, which we hope to be of independent interest. Moreover, we
show how this technique can also be used in the context of the weighted ring
loading problem, showing that cost-unaware approximation algorithms can be
transformed into approximation algorithms with additional cost guarantees.

</details>


### [21] [On the Complexity of Distributed Edge Coloring and Orientation Problems](https://arxiv.org/abs/2510.21327)
*Sebastian Brandt,Fabian Kuhn,Zahra Parsaeian*

Main category: cs.DS

TL;DR: 该论文研究了分布式图算法中局部可检查标记问题的随机化复杂性，特别关注度分裂问题。对于Δ-正则图，作者给出了边着色问题的随机化复杂性精确刻画，以及边定向问题的部分刻画。


<details>
  <summary>Details</summary>
Motivation: 理解在LOCAL模型中解决局部可检查标记问题时随机性的作用是分布式图算法研究的重点。目前尚不清楚哪些确定性复杂度为Ω(log n)的问题可以通过随机化指数级加速。

Method: 研究一类简单自然的LCL问题：度分裂问题，包括边着色问题和边定向问题。针对Δ-正则图，使用分布式算法分析方法。

Result: 对于边着色问题，给出了确定性复杂度为O(log n)的所有问题的随机化复杂性的精确刻画；对于边定向问题，部分刻画了随机化复杂度为Ω(log n)和poly(log log n)的问题。

Conclusion: 该研究在理解LCL问题随机化复杂性方面迈出了重要一步，特别是对度分裂问题类提供了系统分析，算法可推广到任意最大度Δ的图中。

Abstract: Understanding the role of randomness when solving locally checkable labeling
(LCL) problems in the LOCAL model has been one of the top priorities in the
research on distributed graph algorithms in recent years. For LCL problems in
bounded-degree graphs, it is known that randomness cannot help more than
polynomially, except in one case: if the deterministic complexity of an LCL
problem is in $\Omega(\log n)$ and its randomized complexity is in $o(\log n)$,
then the randomized complexity is guaranteed to be $poly(\log \log n)$.
However, the fundamental question of \emph{which} problems with a deterministic
complexity of $\Omega(\log n)$ can be solved exponentially faster using
randomization still remains wide open.
  We make a step towards answering this question by studying a simple, but
natural class of LCL problems: so-called degree splitting problems. These
problems come in two varieties: coloring problems where the edges of a graph
have to be colored with $2$ colors and orientation problems where each edge
needs to be oriented. For $\Delta$-regular graphs (where $\Delta=O(1)$), we
obtain the following results.
  - We gave an exact characterization of the randomized complexity of all
problems where the edges need to be colored with two colors, say red and blue,
and which have a deterministic complexity of $O(\log n)$. - For edge
orientation problems, we give a partial characterization of the problems that
have a randomized complexity of $\Omega(\log n)$ and the problems that have a
randomized complexity of $poly\log\log n$.
  While our results are cleanest to state for the $\Delta$-regular case, all
our algorithms naturally generalize to nodes of any degree $d<\Delta$ in
general graphs of maximum degree $\Delta$.

</details>


### [22] [Approximate minimization of interpretations in fuzzy description logics under the Gödel semantics](https://arxiv.org/abs/2510.21423)
*Linh Anh Nguyen*

Main category: cs.DS

TL;DR: 提出了首个在Gödel语义下最小化有限模糊解释的算法，该算法能在不包含Baaz投影算子和通用角色的模糊描述逻辑中保持模糊概念断言，支持近似保持，时间复杂度为O((m log l + n) log n)。


<details>
  <summary>Details</summary>
Motivation: 最小化模糊解释在理论和实践上都很重要，例如模糊社交网络的建模和压缩表示可以显著提高知识系统的推理和分析效率。

Method: 开发了一种扩展算法，能够最小化有限模糊解释，同时保持模糊概念断言到指定程度γ∈(0,1]，适用于从fALC子逻辑到扩展fALCreg的模糊描述逻辑范围。

Result: 算法成功实现了模糊解释的最小化，在保持概念断言的同时显著压缩表示规模，具有高效的时间复杂度。

Conclusion: 该研究为模糊描述逻辑中的解释最小化问题提供了首个有效解决方案，在保持语义完整性的同时实现了表示压缩，对知识系统的效率提升具有重要意义。

Abstract: The problem of minimizing fuzzy interpretations in fuzzy description logics
(FDLs) is important both theoretically and practically. For instance, fuzzy or
weighted social networks can be modeled as fuzzy interpretations, where
individuals represent actors and roles capture interactions. Minimizing such
interpretations yields more compact representations, which can significantly
improve the efficiency of reasoning and analysis tasks in knowledge-based
systems. We present the first algorithm that minimizes a finite fuzzy
interpretation while preserving fuzzy concept assertions in FDLs without the
Baaz projection operator and the universal role, under the G\"odel semantics.
The considered class of FDLs ranges from the sublogic of $f\!\mathcal{ALC}$
without the union operator and universal restriction to the FDL that extends
$f\!\mathcal{ALC}_{reg}$ with inverse roles and nominals. Our algorithm is
given in an extended form that supports approximate preservation: it minimizes
a finite fuzzy interpretation $\mathcal{I}$ while preserving fuzzy concept
assertions up to a degree $\gamma \in (0,1]$. Its time complexity is
$O((m\log{l} + n)\log{n})$, where $n$ is the size of the domain of
$\mathcal{I}$, $m$ is the number of nonzero instances of atomic roles in
$\mathcal{I}$, and $l$ is the number of distinct fuzzy values used in such
instances plus 2.

</details>


### [23] [Placing Green Bridges Optimally, with Close-Range Habitats in Sparse Graphs](https://arxiv.org/abs/2510.21540)
*Christian Wallisch,Till Fluschnik,Leon Kellerhals*

Main category: cs.DS

TL;DR: 研究野生动物廊道网络设计问题，目标是选择最便宜的边集使得每个物种栖息地具有直径2的连通性。分析了不同最大度Δ和栖息地大小在一般图和平面图上的算法效率和NP难度。


<details>
  <summary>Details</summary>
Motivation: 解决联合国可持续发展目标中的野生动物栖息地碎片化问题，通过建设绿色桥梁重新连接破碎的栖息地，保护生物多样性。

Method: 将栖息地表示为图的顶点，可能的绿色桥梁位置表示为带权边。研究栖息地直径2连通条件下的网络设计问题，分析不同Δ值和栖息地大小在一般图和平面图上的计算复杂度。

Result: 针对不同Δ值和最大栖息地规模，在一般图和平面图上给出了高效算法和NP难性结果。

Conclusion: 该网络设计问题在特定参数条件下存在高效算法，但在一般情况下是NP难的，为野生动物廊道规划提供了理论基础和计算工具。

Abstract: We study a network design problem motivated by the challenge of placing
wildlife crossings to reconnect fragmented habitats of animal species, which is
among the 17 goals towards sustainable development by the UN: Given a graph,
whose vertices represent the fragmented habitat areas and whose edges represent
possible green bridge locations (with costs), and the habitable vertex set for
each species' habitat, the goal is to find the cheapest set of edges such that
each species' habitat is sufficiently connected. We focus on the established
variant where a habitat is considered sufficiently connected if it has diameter
two in the solution and study its complexity in cases justified by our setting
namely small habitat sizes on planar graphs and graphs of small maximum degree
$\Delta$. We provide efficient algorithms and NP-hardness results for different
values of $\Delta$ and maximum habitat sizes on general and planar graphs.

</details>


### [24] [Beyond Smoothed Analysis: Analyzing the Simplex Method by the Book](https://arxiv.org/abs/2510.21613)
*Eleon Bach,Alexander E. Black,Sophie Huiberts,Sean Kafer*

Main category: cs.DS

TL;DR: 提出了一种名为"按书分析"的新算法分析框架，该框架不仅建模算法的输入数据，还建模算法本身，旨在更好地反映算法在实际中的表现。


<details>
  <summary>Details</summary>
Motivation: 缩小算法分析理论与实际应用之间的差距，理解算法在实践中的工作方式。

Method: 按书分析框架，结合实现观察、输入建模最佳实践和实际基准实例的测量，特别应用于单纯形法，考虑输入缩放假设、可行性容差等实现设计原则。

Result: 证明在输入缩放假设、可行性容差等实现设计原则下，单纯形法确实达到了多项式运行时间。

Conclusion: 按书分析框架克服了平滑分析的几个弱点，能够更好地解释单纯形法在实际中的优异性能。

Abstract: Narrowing the gap between theory and practice is a longstanding goal of the
algorithm analysis community. To further progress our understanding of how
algorithms work in practice, we propose a new algorithm analysis framework that
we call by the book analysis. In contrast to earlier frameworks, by the book
analysis not only models an algorithm's input data, but also the algorithm
itself. Results from by the book analysis are meant to correspond well with
established knowledge of an algorithm's practical behavior, as they are meant
to be grounded in observations from implementations, input modeling best
practices, and measurements on practical benchmark instances. We apply our
framework to the simplex method, an algorithm which is beloved for its
excellent performance in practice and notorious for its high running time under
worst-case analysis. The simplex method similarly showcased the state of the
art framework smoothed analysis (Spielman and Teng, STOC'01). We explain how
our framework overcomes several weaknesses of smoothed analysis and we prove
that under input scaling assumptions, feasibility tolerances and other design
principles used by simplex method implementations, the simplex method indeed
attains a polynomial running time.

</details>


### [25] [O(1)-Distortion Planar Emulators for String Graphs](https://arxiv.org/abs/2510.21700)
*Hsien-Chih Chang,Jonathan Conroy,Zihan Tan,Da Wei Zheng*

Main category: cs.DS

TL;DR: 该论文证明每个无权字符串图都有一个O(1)失真平面模拟器，即存在一个边加权的平面图H，其顶点集与G相同，且任意顶点对之间的距离满足δ_G(u,v) ≤ δ_H(u,v) ≤ O(1)·δ_G(u,v)。


<details>
  <summary>Details</summary>
Motivation: 研究字符串图的平面嵌入性质，探索如何用平面图来近似表示字符串图的结构和距离关系。

Method: 构造一个边加权的平面图H，其顶点集与原字符串图G相同，通过特定的加权方式保证距离关系的近似保持。

Result: 成功证明了每个无权字符串图都存在一个常数失真度的平面模拟器，即失真度仅为O(1)。

Conclusion: 字符串图具有良好的平面近似性质，可以用平面图以常数失真度来模拟其距离结构。

Abstract: We show that every unweighted string graph $G$ has an $O(1)$-distortion
planar emulator: that is, there exists an (edge-weighted) planar graph $H$ with
$V(H) = V(G)$, such that every pair of vertices $(u,v)$ satisfies
$\delta_G(u,v) \le \delta_H(u,v) \le O(1) \cdot \delta_G(u,v).$

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [26] [Information Theoretic Learning for Diffusion Models with Warm Start](https://arxiv.org/abs/2510.20903)
*Yirong Shen,Lu Gan,Cong Ling*

Main category: cs.IT

TL;DR: 本文提出了一种更紧致的似然边界，用于改进基于噪声的生成模型的极大似然学习，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 基于扰动的似然估计模型虽然应用广泛，但面临收敛速度慢和理论理解有限的问题。

Method: 将经典的KL散度Fisher信息关系扩展到任意噪声扰动，超越高斯假设，支持结构化噪声分布，并将扩散过程视为高斯通道来表达数据与模型之间的失配熵。

Result: 在CIFAR-10上实现了有竞争力的负对数似然，在ImageNet的多个分辨率上达到了最先进的结果，且无需数据增强。

Conclusion: 该框架能够灵活使用随机噪声分布，自然考虑传感器伪影、量化效应和数据分布平滑，同时与标准扩散训练兼容，并自然扩展到离散数据。

Abstract: Generative models that maximize model likelihood have gained traction in many
practical settings. Among them, perturbation based approaches underpin many
strong likelihood estimation models, yet they often face slow convergence and
limited theoretical understanding. In this paper, we derive a tighter
likelihood bound for noise driven models to improve both the accuracy and
efficiency of maximum likelihood learning. Our key insight extends the
classical KL divergence Fisher information relationship to arbitrary noise
perturbations, going beyond the Gaussian assumption and enabling structured
noise distributions. This formulation allows flexible use of randomized noise
distributions that naturally account for sensor artifacts, quantization
effects, and data distribution smoothing, while remaining compatible with
standard diffusion training. Treating the diffusion process as a Gaussian
channel, we further express the mismatched entropy between data and model,
showing that the proposed objective upper bounds the negative log-likelihood
(NLL). In experiments, our models achieve competitive NLL on CIFAR-10 and SOTA
results on ImageNet across multiple resolutions, all without data augmentation,
and the framework extends naturally to discrete data.

</details>


### [27] [Overlapped-repetition Shor codes achieving fourfold asymptotic rate](https://arxiv.org/abs/2510.21030)
*En-Jui Chang*

Main category: cs.IT

TL;DR: 通过重叠少量重复码，将Shor码的渐近码率提高四倍，在最小距离d=3时从[[9,1,3]]优化到[[7,1,3]]配置


<details>
  <summary>Details</summary>
Motivation: 标准Shor码使用两个重复码作为内外码，结构简单但码率相对较低，需要提高码率

Method: 通过重叠少量重复码来增强渐近码率

Result: 在最小距离d=3的情况下，将开销从[[9,1,3]]减少到更高效的[[7,1,3]]配置

Conclusion: 重叠重复码的方法能显著提高量子纠错码的码率，减少资源开销

Abstract: The standard Shor code employs two repetition codes as inner and outer codes,
yielding a simple structure but a relatively low code rate. By overlapping a
small number of repetition codes, we enhance the asymptotic code rate fourfold.
In the minimal-distance case $d = 3$, this construction reduces the overhead
from $[[9,1,3]]$ to the more efficient $[[7,1,3]]$ configuration.

</details>


### [28] [Complex DNA Synthesis Sequences](https://arxiv.org/abs/2510.21253)
*Boaz Moav,Ryan Gabrys,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了一种混合DNA合成框架，将酶合成和光刻合成方法相结合，通过受限核苷酸子集进行并行合成，建立了新的复杂合成序列概念和信息率分析。


<details>
  <summary>Details</summary>
Motivation: DNA存储具有高密度和耐久性，但并行链合成的效率限制了其可扩展性。现有方法要么允许无约束的核苷酸添加（如酶合成），要么强制所有链进行相同添加（如光刻合成），需要一种更灵活的混合方法。

Method: 引入混合合成框架：每个周期从受限核苷酸子集中选择核苷酸进行并行合成。建立了复杂合成序列概念，扩展了信息率定义，分析了删除球的类似概念。设计了动态规划算法计算最优合成序列。

Result: 推导了最大信息率及其渐近行为的紧致表达式，填补了约束模型与理想化设置之间的理论空白。为已知链设计了动态规划算法，并定义了具有行合成约束的二维阵列模型。

Conclusion: 建立了一个全面且新的DNA合成理论框架，包含了先前模型，并为该领域未来的进展奠定了基础。

Abstract: DNA-based storage offers unprecedented density and durability, but its
scalability is fundamentally limited by the efficiency of parallel strand
synthesis. Existing methods either allow unconstrained nucleotide additions to
individual strands, such as enzymatic synthesis, or enforce identical additions
across many strands, such as photolithographic synthesis. We introduce and
analyze a hybrid synthesis framework that generalizes both approaches: in each
cycle, a nucleotide is selected from a restricted subset and incorporated in
parallel. This model gives rise to a new notion of a complex synthesis
sequence. Building on this framework, we extend the information rate definition
of Lenz et al. and analyze an analog of the deletion ball, defined and studied
in this setting, deriving tight expressions for the maximal information rate
and its asymptotic behavior. These results bridge the theoretical gap between
constrained models and the idealized setting in which every nucleotide is
always available. For the case of known strands, we design a dynamic
programming algorithm that computes an optimal complex synthesis sequence,
highlighting structural similarities to the shortest common supersequence
problem. We also define a distinct two-dimensional array model with synthesis
constraints over the rows, which extends previous synthesis models in the
literature and captures new structural limitations in large-scale strand
arrays. Additionally, we develop a dynamic programming algorithm for this
problem as well. Our results establish a new and comprehensive theoretical
framework for constrained DNA, subsuming prior models and setting the stage for
future advances in the field.

</details>


### [29] [Text-Guided Diffusion Model-based Generative Communication for Wireless Image Transmission](https://arxiv.org/abs/2510.21299)
*Shengkang Chen,Tong Wu,Zhiyong Chen,Feng Yang,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出基于扩散模型的生成通信框架，结合联合源信道编码和语义引导重建，在极低传输速率下实现高质量图像传输。


<details>
  <summary>Details</summary>
Motivation: 解决无线信道中极低传输速率下传统压缩和信道编码方案无法保持足够视觉质量的问题。

Method: 使用联合源信道编码传输图像，同时发送文本提示；接收端通过Stable Diffusion模型和ControlNet融合损坏的低速率表示和提示进行重建。

Result: 在极端带宽限制下仍能产生感知上令人信服的图像，在各种信道条件下均优于传统编码方案和深度学习基线方法。

Conclusion: 该生成通信框架通过利用生成先验和语义指导，在极低传输速率下实现了卓越的感知质量和鲁棒性。

Abstract: Reliable image transmission over wireless channels is particularly
challenging at extremely low transmission rates, where conventional compression
and channel coding schemes fail to preserve adequate visual quality. To address
this issue, we propose a generative communication framework based on diffusion
models, which integrates joint source channel coding (JSCC) with
semantic-guided reconstruction leveraging a pre-trained generative model.
Unlike conventional architectures that aim to recover exact pixel values of the
original image, the proposed method focuses on preserving and reconstructing
semantically meaningful visual content under severely constrained rates,
ensuring perceptual plausibility and faithfulness to the scene intent.
Specifically, the transmitter encodes the source image via JSCC and jointly
transmits it with a textual prompt over the wireless channel. At the receiver,
the corrupted low-rate representation is fused with the prompt and
reconstructed through a Stable Diffusion model with ControlNet, enabling
high-quality visual recovery. Leveraging both generative priors and semantic
guidance, the proposed framework produces perceptually convincing images even
under extreme bandwidth limitations. Experimental results demonstrate that the
proposed method consistently outperforms conventional coding-based schemes and
deep learning baselines, achieving superior perceptual quality and robustness
across various channel conditions.

</details>


### [30] [Low-Complexity MIMO Channel Estimation with Latent Diffusion Models](https://arxiv.org/abs/2510.21386)
*Xiaotian Fan,Xingyu Zhou,Le Liang,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于潜在扩散模型(PSLD-CE)的信道估计算法，通过轻量级LDM架构捕获复杂信道分布，在保持低计算复杂度的同时显著提升性能


<details>
  <summary>Details</summary>
Motivation: 深度生成模型能够学习无线信道的复杂先验分布，为传统信道估计提供强大替代方案

Method: 设计轻量级潜在扩散模型架构作为生成先验，增强扩散后验采样过程，引入似然项有效近似和变分自编码器潜在空间的自一致性约束

Result: PSLD-CE在广泛实验中持续优于现有方法，在保持低计算复杂度和快速推理速度的同时获得显著性能提升

Conclusion: 该方法为下一代无线系统提供了一种高度有前景且实用的信道估计解决方案

Abstract: Deep generative models offer a powerful alternative to conventional channel
estimation by learning the complex prior distribution of wireless channels.
Capitalizing on this potential, this paper proposes a novel channel estimation
algorithm based on latent diffusion models (LDMs), termed posterior sampling
with latent diffusion for channel estimation (PSLD-CE). The core of our
approach is a lightweight LDM architecture specifically designed for channel
estimation, which serves as a powerful generative prior to capture the
intricate channel distribution. Furthermore, we enhance the diffusion posterior
sampling process by introducing an effective approximation for the likelihood
term and a tailored self-consistency constraint on the variational autoencoder
latent space. Extensive experimental results demonstrate that PSLD-CE
consistently outperforms a wide range of existing methods. Notably, these
significant performance gains are achieved while maintaining low computational
complexity and fast inference speed, establishing our method as a highly
promising and practical solution for next-generation wireless systems.

</details>


### [31] [Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication](https://arxiv.org/abs/2510.21414)
*Hoang Ly,Emina Soljanin*

Main category: cs.IT

TL;DR: 提出了一种通用的ML解码框架，将最坏情况复杂度从q^{k}n降低到q^{k}，通过向量-矩阵乘法实现所有码字的似然计算。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然解码复杂度高，需要q^{k}n次操作，限制了实际应用。

Method: 将码字似然表示为接收序列向量和码字向量的内积，通过预计算码本矩阵和Mailman算法优化向量-矩阵乘法。

Result: 将最坏情况复杂度降低n倍，适用于线性和非线性码、硬软判决解码、ISI信道和列表解码。

Conclusion: 该方法显著降低了ML解码复杂度，但需要较高的存储空间O(q^{k+1}n)。

Abstract: Maximum-likelihood (ML) decoding for arbitrary block codes remains
fundamentally hard, with worst-case time complexity-measured by the total
number of multiplications-being no better than straightforward exhaustive
search, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper
introduces a simple, code-agnostic framework that reduces the worst-case
complexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable
reduction in practice. The result holds for both linear and nonlinear block
codes over general memoryless channels and under both hard-decision and
soft-decision decoding. It naturally extends to intersymbol-interference (ISI)
channels and ML list decoding with only a negligible increase in complexity.
Our core insight is that, upon receipt of each sequence at the receiver, the
conditional probability of that sequence for each codeword in the codebook
(i.e., the \emph{likelihood}) can be expressed as the inner product of two
carefully constructed vectors -- the first depending on the received sequence,
and the second on that codeword itself. As a result, evaluating the likelihoods
for all codewords in the codebook reduces to a single vector-matrix
multiplication, and ML decoding (MLD) becomes the simple task of picking the
maximum entry in the resulting vector. The only non-trivial cost lies in the
vector-matrix product. However, our matrix construction allows the use of the
Mailman algorithm to reduce this cost. This time reduction is achieved at the
cost of high space complexity, requiring $\mathcal{O}(q^{k+1} n)$ space to
store the pre-computed codebook matrix.

</details>


### [32] [Resilient Radio Access Networks: AI and the Unknown Unknowns](https://arxiv.org/abs/2510.21587)
*Bho Matthiesen,Armin Dekorsy,Petar Popovski*

Main category: cs.IT

TL;DR: 本文探讨了在5G网络中设计AI以实现网络弹性的挑战，特别关注未预料和罕见中断情况，指出当前统计学习方法在弹性方面的局限性，并建议结合在线学习和因果推理。


<details>
  <summary>Details</summary>
Motivation: 5G网络虽然提供高可靠性和可用性，但在面对意外情况时仍可能失效。弹性系统能够适应现实世界的复杂性，包括系统设计时完全未预料的操作条件，这对必须在缺乏模型或模型过于复杂无法提供统计保证的场景中维持服务的通信系统至关重要。

Method: 通过理论分析探讨设计弹性无线接入网络AI的挑战，特别关注未预料和罕见中断。

Result: 理论结果表明当前统计学习方法在实现网络弹性方面存在严重局限性。

Conclusion: AI将在提供网络弹性方面发挥重要作用，但需要结合在线学习和因果推理方法来克服当前统计学习方法的局限性。

Abstract: 5G networks offer exceptional reliability and availability, ensuring
consistent performance and user satisfaction. Yet they might still fail when
confronted with the unexpected. A resilient system is able to adapt to
real-world complexity, including operating conditions completely unanticipated
during system design. This makes resilience a vital attribute for communication
systems that must sustain service in scenarios where models are absent or too
intricate to provide statistical guarantees. Such considerations indicate that
artifical intelligence (AI) will play a major role in delivering resilience. In
this paper, we examine the challenges of designing AIs for resilient radio
access networks, especially with respect to unanticipated and rare disruptions.
Our theoretical results indicate strong limitations of current statistical
learning methods for resilience and suggest connections to online learning and
causal inference.

</details>
