<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 8]
- [cs.IT](#cs.IT) [Total: 15]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [A Broader View on Clustering under Cluster-Aware Norm Objectives](https://arxiv.org/abs/2512.06211)
*Martin G. Herold,Evangelos Kipouridis,Joachim Spoerhase*

Main category: cs.DS

TL;DR: 本文改进了(f,g)-聚类问题的近似算法，将(f,L₁)-聚类的近似比从Õ(√n)提升到O(log²n)，将一般(f,g)-聚类的近似比从Õ(√kn)提升到O(k)，并设计了基于f和g参数的插值算法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决先前工作中(f,g)-聚类问题近似算法存在的较大差距，特别是对于更一般设置的情况。该问题包含了k-Center、k-Median、Min-Sum of Radii和Min-Load k-Clustering等基础聚类问题，但之前的结果与这些基础问题的最优界限存在较大差距。

Method: 1. 设计了针对(f,L₁)-聚类的O(log²n)近似算法；2. 设计了针对一般(f,g)-聚类的O(k)近似算法；3. 基于新定义的f和g参数，设计了能够插值多种聚类问题最优界限的近似算法。

Result: 1. 将(f,L₁)-聚类的近似比从Õ(√n)显著改进为O(log²n)；2. 将一般(f,g)-聚类的近似比从Õ(√kn)改进为O(k)，与Min-Load k-Clustering的最佳已知上界匹配；3. 设计了能够统一多种聚类问题近似界限的插值算法。

Conclusion: 本文显著改进了(f,g)-聚类问题的近似算法性能，填补了先前工作中的理论差距，为这一通用聚类框架提供了更清晰的近似性图景，并通过参数化方法实现了对多种基础聚类问题的统一处理。

Abstract: We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.
  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\widetilde{O}(\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\widetilde{O}(\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.
  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.

</details>


### [2] [Finding a Maximum Common (Induced) Subgraph: Structural Parameters Revisited](https://arxiv.org/abs/2512.06383)
*Tesshu Hanaka,Yuto Okada,Yota Otachi,Lena Volk*

Main category: cs.DS

TL;DR: 该论文研究了寻找两个图的最大公共（诱导）子图的参数化复杂度，在max-leaf number和neighborhood diversity参数下证明了固定参数可解性，在twin cover number参数下诱导与非诱导问题复杂度不同


<details>
  <summary>Details</summary>
Motivation: 最大公共子图问题是NP难问题，即使在强限制的结构参数下也难以处理。需要补充这些问题的硬度结果，展示固定参数可解的情况，并探索诱导与非诱导问题在复杂度上的差异

Method: 使用参数化复杂度理论分析，针对max-leaf number、neighborhood diversity和twin cover number等结构参数，分别证明最大公共子图问题的固定参数可解性

Result: 证明了在max-leaf number和neighborhood diversity参数下，诱导和非诱导的最大公共子图问题都是固定参数可解的；在twin cover number参数下，只有诱导问题是固定参数可解的，展示了诱导与非诱导问题的复杂度差异

Conclusion: 该研究几乎完全确定了最大公共子图问题相对于常用结构参数的复杂度，并提供了一个诱导与非诱导问题具有不同复杂度的罕见例子，为参数化图算法领域做出了重要贡献

Abstract: We study the parameterized complexity of the problems of finding a maximum common (induced) subgraph of two given graphs. Since these problems generalize several NP-complete problems, they are intractable even when parameterized by strongly restricted structural parameters. Our contribution in this paper is to sharply complement the hardness of the problems by showing fixed-parameter tractable cases: both induced and non-induced problems parameterized by max-leaf number and by neighborhood diversity, and the induced problem parameterized by twin cover number. These results almost completely determine the complexity of the problems with respect to well-studied structural parameters. Also, the result on the twin cover number presents a rather rare example where the induced and non-induced cases have different complexity.

</details>


### [3] [Instance Dependent Testing of Samplers using Interval Conditioning](https://arxiv.org/abs/2512.06458)
*Rishiraj Bhattacharyya,Sourav Chakraborty,Yash Pote,Uddalok Sarkar,Sayantan Sen*

Main category: cs.DS

TL;DR: 提出首个支持无限域（自然数）的采样器测试工具，采用实例依赖效率而非最坏情况效率，通过区间条件框架实现距离估计，实验显示比现有测试工具快1000倍。


<details>
  <summary>Details</summary>
Motivation: 现有采样器测试工具（如Barbarik、Teq、Flash、CubeProbe）仅关注最坏情况效率，且不支持无限域测试，而天文学、金融、网络安全等领域常需处理无限域采样器验证问题。

Method: 通过区间条件框架设计新颖的距离估计算法，连接未知分布与已知分布，核心技术创新是将问题转化为连续分布的概率质量估计。

Result: 实验结果显示，新测试工具比现有最先进测试工具快达1000倍，首次实现对自然数域采样器的有效测试。

Conclusion: 该工作首次实现了支持无限域的实例依赖效率采样器测试，通过创新的距离估计算法框架，在理论和实践上都取得了显著突破。

Abstract: Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.
  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.

</details>


### [4] [The $k$-Fold Matroid Secretary Problem](https://arxiv.org/abs/2512.06611)
*Rishi Gujjar,Kevin Hua,Robert Kleinberg,Frederick V. Qiu*

Main category: cs.DS

TL;DR: 该论文提出了一个针对k-重拟阵并的秘书问题的(1-O(√(log(n)/k)))-竞争算法，推广了Kleinberg对k-均匀拟阵的结果。


<details>
  <summary>Details</summary>
Motivation: 拟阵秘书问题中，当拟阵是k-均匀拟阵时已有(1-O(1/√k))-竞争算法，但需要将这一结果推广到更一般的k-重拟阵并结构。

Method: 作者将Kleinberg的算法从k-均匀拟阵推广到k-重拟阵并，通过分析拟阵并的结构特性来设计竞争算法。

Result: 成功设计了(1-O(√(log(n)/k)))-竞争算法，当k足够大时趋近于1-竞争，但相比均匀拟阵多了一个log(n)因子。

Conclusion: 该工作将拟阵秘书问题的算法从均匀拟阵推广到更一般的拟阵并结构，为处理更复杂的约束条件提供了理论基础。

Abstract: In the matroid secretary problem, elements $N := [n]$ of a matroid $\mathcal{M} \subseteq 2^N$ arrive in random order. When an element arrives, its weight is revealed and a choice must be made to accept or reject the element, subject to the constraint that the accepted set $S \in \mathcal{M}$. Kleinberg'05 gives a $(1-O(1/\sqrt{k}))$-competitive algorithm when $\mathcal{M}$ is a $k$-uniform matroid. We generalize their result, giving a $(1-O(\sqrt{\log(n)/k}))$-competitive algorithm when $\mathcal{M}$ is a $k$-fold matroid union.

</details>


### [5] [Near-Optimal Bayesian Online Assortment of Reusable Resources](https://arxiv.org/abs/2512.06997)
*Yiding Feng,Rad Niazadeh,Amin Saberi*

Main category: cs.DS

TL;DR: 针对电商租赁服务中的可重用资源在线组合问题，提出了一种基于期望线性规划的竞争算法，在最小初始库存较大时达到接近最优的竞争比。


<details>
  <summary>Details</summary>
Motivation: 受电商租赁服务应用的启发，研究可重用资源在线组合中的收益最大化问题，消费者类型随时间独立地从已知异质分布中抽取。

Method: 设计基于期望线性规划的竞争算法，通过独立随机舍入模拟解，引入资源丢弃策略处理可重用性下的库存可行性，并使用后处理组合程序协调并行运行的丢弃策略。

Result: 在最小初始库存较大时，对一般可重用资源情况获得接近最优的1-min(1/2,√(log(c₀)/c₀))竞争比；对非可重用资源特殊情况，获得改进的1-1/√(c₀+3)竞争比。

Conclusion: 提出的算法框架有效解决了可重用资源在线组合问题，丢弃策略和后处理程序的设计具有独立研究价值，数值模拟验证了算法性能。

Abstract: Motivated by the applications of rental services in e-commerce, we consider revenue maximization in online assortment of reusable resources for a stream of arriving consumers with different types. We design competitive online algorithms with respect to the optimum online policy in the Bayesian setting, in which types are drawn independently from known heterogeneous distributions over time. In the regime where the minimum of initial inventories $c_0$ is large, our main result is a near-optimal $1-\min\left(\frac{1}{2},\sqrt{\log(c_0)/c_0}\right)$ competitive algorithm for the general case of reusable resources. Our algorithm relies on an expected LP benchmark for the problem, solves this LP, and simulates the solution through an independent randomized rounding. The main challenge is obtaining point-wise inventory feasibility in a computationally efficient fashion from these simulation-based algorithms. To this end, we use several technical ingredients to design $\textit{discarding policies}$ -- one for each resource. These policies handle the trade-off between the inventory feasibility under reusability and the revenue loss of each of the resources. However, discarding a unit of a resource changes the future consumption of other resources. To handle this new challenge, we also introduce $\textit{post-processing}$ assortment procedures that help with designing and analyzing our discarding policies as they run in parallel, which might be of independent interest. As a side result, by leveraging techniques from the literature on prophet inequality, we further show an improved near-optimal $1-1/\sqrt{c_0+3}$ competitive algorithm for the special case of non-reusable resources. We finally evaluate the performance of our algorithms using the numerical simulations on the synthetic data.

</details>


### [6] [Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications](https://arxiv.org/abs/2512.07120)
*J. Allagan,G. Morgan,S. Langley,R. Lopez-Bonilla,V. Deriglazov*

Main category: cs.DS

TL;DR: 该论文建立了在双色三角形约束下2-树的色特征向量的闭式枚举公式，这些可高效计算的结构特征来自约束图着色，其中每个三角形恰好使用两种颜色，禁止单色和彩虹三角形。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于分布式系统中需要避免组件完全集中或完全隔离的场景，双色三角形约束反映了这种需求。传统色多项式无法区分不同结构的2-树，而双色约束能提供更具信息量的结构特征。

Method: 针对两类2-树结构（theta图和扇图），建立了约束着色数量的闭式公式。对于theta图，证明了r_k(Theta_n) = S(n-2, k-1)（第二类斯特林数）；对于扇图，建立了与斐波那契数和二项式系数相关的显式公式。

Result: 获得了高效计算的结构特征：theta图可在O(n)时间内计算，扇图可在O(n²)时间内计算。这些特征与斐波那契多项式、贝尔数和独立集枚举有联系，能捕获有意义的结构特性。

Conclusion: 双色三角形约束为2-树提供了信息丰富的结构特征，不同于传统色多项式。这些特征在拜占庭容错、云计算VM分配和分布式密码学秘密共享等应用中具有实用价值。

Abstract: We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.

</details>


### [7] [Property Testing of Computational Networks](https://arxiv.org/abs/2512.07577)
*Artur Czumaj,Christian Sohler*

Main category: cs.DS

TL;DR: 该论文提出了加权计算网络的属性测试框架，通过案例研究展示了ReLU激活的简单神经布尔网络中，任何接近常数的函数都可以用与网络规模无关的查询复杂度进行测试。


<details>
  <summary>Details</summary>
Motivation: 研究加权计算网络作为计算设备的属性测试问题，旨在设计算法来区分正确计算特定函数的网络与那些需要大量修改才能计算该函数的网络。

Method: 提出了一个属性测试框架，定义了(ε,δ)-far概念来衡量网络与目标函数的距离。通过案例研究，分析了具有ReLU激活函数的简单神经布尔网络。

Result: 对于ReLU激活的简单神经布尔网络，任何接近常数的函数都可以用与网络规模无关的查询复杂度进行测试。但在分布无关模型和普通测试模型的自然推广中，无法实现类似结果。

Conclusion: 该研究为加权计算网络的属性测试建立了理论基础，展示了在某些网络架构中可以实现高效的测试，但也指出了在更一般模型中的局限性。

Abstract: In this paper we initiate the study of \emph{property testing of weighted computational networks viewed as computational devices}. Our goal is to design property testing algorithms that for a given computational network with oracle access to the weights of the network, accept (with probability at least $\frac23$) any network that computes a certain function (or a function with a certain property) and reject (with probability at least $\frac23$) any network that is \emph{far} from computing the function (or any function with the given property). We parameterize the notion of being far and want to reject networks that are \emph{$(ε,δ)$-far}, which means that one needs to change an $ε$-fraction of the description of the network to obtain a network that computes a function that differs in at most a $δ$-fraction of inputs from the desired function (or any function with a given property).
  To exemplify our framework, we present a case study involving simple neural Boolean networks with ReLU activation function. As a highlight, we demonstrate that for such networks, any near constant function is testable in query complexity independent of the network's size. We also show that a similar result cannot be achieved in a natural generalization of the distribution-free model to our setting, and also in a related vanilla testing model.

</details>


### [8] [Approximation Algorithms for the $b$-Matching and List-Restricted Variants of MaxQAP](https://arxiv.org/abs/2512.07618)
*Jiratchaphat Nanta,Vorapong Suppakitpaisarn,Piyashat Sripratak*

Main category: cs.DS

TL;DR: 该论文研究了最大二次分配问题（MaxQAP）的两个自然变体：最大列表限制二次分配问题和最大二次b匹配分配问题，并为这两种变体设计了近似算法。


<details>
  <summary>Details</summary>
Motivation: 最大二次分配问题（MaxQAP）是一个经典的组合优化问题，但实际应用中常遇到各种约束变体。本文旨在解决两个重要的MaxQAP变体：1）列表限制版本，其中节点匹配受到预设列表的限制；2）b匹配版本，其中每个节点可以匹配多个节点。这些变体在实际应用中很常见，但缺乏有效的近似算法。

Method: 对于列表限制版本，当每个列表大小至少为n-O(√n)时，基于Makarychev等人的线性规划松弛和随机舍入框架，设计了随机O(√n)-近似算法。对于二次b匹配版本，改进了标准MaxQAP松弛，结合b次独立迭代的随机舍入和最大权重b匹配问题的多项式时间算法，获得O(√bn)-近似。

Result: 当b为常数且所有列表大小为n-O(√n)时，获得的近似保证渐近匹配MaxQAP的最佳已知近似因子，为这两种变体提供了首个近似算法。

Conclusion: 本文成功为最大二次分配问题的两个重要变体设计了有效的近似算法，填补了该领域的研究空白，为实际应用中受约束的二次分配问题提供了理论保证。

Abstract: We study approximation algorithms for two natural generalizations of the Maximum Quadratic Assignment Problem (MaxQAP). In the Maximum List-Restricted Quadratic Assignment Problem, each node in one partite set may only be matched to nodes from a prescribed list. For instances on $n$ nodes where every list has size at least $n - O(\sqrt{n})$, we design a randomized $O(\sqrt{n})$-approximation algorithm based on the linear-programming relaxation and randomized rounding framework of Makarychev, Manokaran, and Sviridenko. In the Maximum Quadratic $b$-Matching Assignment Problem, we seek a $b$-matching that maximizes the MaxQAP objective. We refine the standard MaxQAP relaxation and combine randomized rounding over $b$ independent iterations with a polynomial-time algorithm for maximum-weight $b$-matching problem to obtain an $O(\sqrt{bn})$-approximation. When $b$ is constant and all lists have size $n - O(\sqrt{n})$, our guarantees asymptotically match the best known approximation factor for MaxQAP, yielding the first approximation algorithms for these two variants.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [9] [Hybrid Beamfocusing Design for RSMA-Enabled Near-Field Wideband Communications](https://arxiv.org/abs/2512.06156)
*Jiasi Zhou,Chintha Tellambura*

Main category: cs.IT

TL;DR: 提出了一种基于速率分割多址（RSMA）的发射方案，用于宽带近场通信，采用基于真实时延（TTD）的混合波束聚焦架构来缓解空间宽带效应并降低射频链需求。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络将使用超大规模天线阵列（ELAAs）和高频段，但这会产生近场球面波前和空间宽带效应。需要开发有效方案来利用和缓解这些效应。

Method: 提出RSMA使能的发射方案，采用TTD混合波束聚焦架构。通过联合优化频率相关模拟波束聚焦、频率无关模拟波束聚焦、数字波束聚焦和公共速率分配来最大化最小速率。使用基于惩罚的迭代算法，采用块坐标下降（BCD）方法分三块交替优化变量。

Result: 仿真结果表明：1）有效补偿空间宽带效应；2）性能接近全数字波束聚焦但硬件复杂度更低；3）相比其他基准方案获得显著性能增益。

Conclusion: 提出的RSMA使能发射方案能有效解决宽带近场通信中的关键挑战，在性能和硬件复杂度之间取得良好平衡，为未来无线网络提供有前景的解决方案。

Abstract: Future wireless networks will utilize extremely large-scale antenna arrays (ELAAs) over high-frequency bands, which, however, produce near-field spherical wavefronts and spatial wideband effects. To exploit and mitigate these, this paper proposes a rate-splitting multiple access (RSMA)-enabled transmit scheme for wideband near-field communications (NFC). Our solution leverages true-time-delay (TTD)-based hybrid beamfocusing architectures to mitigate spatial wideband effect and reduce radio frequency chain requirements. The objective is to maximize the minimum rate by jointly optimizing frequency-dependent analog beamfocusing, frequency-independent analog beamfocusing, digital beamfocusing, and common rate allocation. To solve this complicated non-convex problem, we develop a penalty-based iterative algorithm that partitions the variables into three blocks and then employs block coordinate descent (BCD) to optimize each block alternately. This algorithm is further extended to support the sub-connected TTD-based analog beamfocusing architectures. Comprehensive simulation results indicate that our transmit scheme: 1) effectively compensates for spatial wideband effect, addressing a critical challenge in wideband operation; 2) achieves performance comparable to full digital beamfocusing while maintaining lower hardware complexity; 3) achieves substantial performance gains over the other two benchmarks.

</details>


### [10] [Non-Asymptotic Error Bounds for Causally Conditioned Directed Information Rates of Gaussian Sequences](https://arxiv.org/abs/2512.06238)
*Yuping Zheng,Andrew Lamperski*

Main category: cs.IT

TL;DR: 提出了一种针对高斯向量序列的因果条件定向信息率估计器，误差界为O(N^{-1/2}log(N))


<details>
  <summary>Details</summary>
Motivation: 定向信息及其因果条件变体常用于测量随机过程间的因果影响，但在实际应用中需要从数据中估计。现有非渐近误差界主要针对有限字母表序列，对实值数据（特别是高斯向量序列）的估计方法了解较少

Method: 基于最优预测推导出因果条件定向信息率的显式公式，并基于该公式构建估计器。针对高斯向量序列数据，利用其统计特性设计估计方法

Result: 提出的估计器能以高概率达到O(N^{-1/2}log(N))的误差阶，其中N为总样本量。这为高斯向量序列的因果条件定向信息率估计提供了理论保证

Conclusion: 该研究填补了实值数据（特别是高斯向量序列）中因果条件定向信息率估计的理论空白，为实际应用提供了有效的估计方法和误差界保证

Abstract: Directed information and its causally conditioned variations are often used to measure causal influences between random processes. In practice, these quantities must be measured from data. Non-asymptotic error bounds for these estimates are known for sequences over finite alphabets, but less is known for real-valued data. This paper examines the case in which the data are sequences of Gaussian vectors. We provide an explicit formula for causally conditioned directed information rate based on optimal prediction and define an estimator based on this formula. We show that our estimator gives an error of order $O\left(N^{-1/2}\log(N)\right)$ with high probability, where $N$ is the total sample size.

</details>


### [11] [Performance Bounds on Pliable Index Coding Using Absent Receivers](https://arxiv.org/abs/2512.06312)
*Lawrence Ong,Badri N. Vellambi,Parastoo Sadeghi,Jörg Kliewer*

Main category: cs.IT

TL;DR: 该论文提出了一个新颖算法来构建解码链，用于分析可塑性索引编码问题的最优广播速率下界，特别针对不满足传统特殊约束的通用实例。


<details>
  <summary>Details</summary>
Motivation: 当前大多数已解决的可塑性索引编码实例都属于特殊类别，即具有特定边信息基数的接收器要么全部存在要么全部不存在。本文旨在研究更一般的实例，不施加这种约束，以扩展理论覆盖范围。

Method: 设计了一种新颖算法，通过迭代添加可由边信息已在链中的接收器解码的消息来构建解码链。如果由于缺乏具有所需消息的接收器而无法继续解码链，则跳过消息直接将其添加到链中。证明最优广播速率的下界是跳过消息数量的函数。

Result: 推导出特定类别可塑性索引编码实例的显式下界，这些下界取决于缺失接收器的数量或其边信息集的模式。具体表征了最多包含四个缺失接收器（任意边信息模式）以及边信息集以特定方式嵌套的实例的最优广播速率。

Conclusion: 该算法和分析框架为更一般的可塑性索引编码实例提供了理论基础，扩展了现有理论覆盖范围，并为特定类别实例提供了可计算的最优广播速率下界。

Abstract: We characterise bounds on the optimal broadcast rate for a few classes of pliable-index-coding instances. Unlike the majority of currently solved instances, which belong to a special class where all receivers with a certain side-information cardinality are either present or absent, we consider more general instances without this constraint. We devise a novel algorithm that constructs a decoding chain by iteratively adding a message that can be decoded by a receiver whose side information is already in the chain. If the decoding chain cannot proceed due to the absence of a receiver with the required messages, we skip a message by adding it to the chain regardless. We prove that a lower bound on the optimal broadcast rate is a function of the number of skipped messages, across all possible decoding choices of the receivers and any realisation of the algorithm for each decoding choice. While this result is not computationally feasible in isolation, it serves as a basis for deriving explicit lower bounds on the broadcast rate for specific classes of pliable-index-coding instances. These lower bounds depend on the number of absent receivers or the pattern of their side-information sets. Specifically, we explicitly characterise the optimal broadcast rate for instances with up to and including four absent receivers with any side-information pattern, as well as instances where the side-information sets are nested in particular ways.

</details>


### [12] [Trajectory Optimization for Cellular-Connected UAV in Complex Environment with Partial CKM](https://arxiv.org/abs/2512.06452)
*Yuxuan Song,Haiquan Lu,Chiya Zhang,Beixiong Zheng,Yong Zeng*

Main category: cs.IT

TL;DR: 提出一种将无人机导航与信道知识图补全相结合的新型轨迹设计策略，通过克里金插值法补全CKM，将问题转化为图论中的最短路径和旅行商问题，实现导航与地图更新的双赢。


<details>
  <summary>Details</summary>
Motivation: 蜂窝连接无人机在无线网络中日益重要，但AG信道的高视距概率导致严重同频干扰。CKM虽能解决此问题，但通常不完整且需要定期更新以应对动态环境变化。

Method: 提出将无人机导航与CKM补全结合的框架：无人机飞行时测量无线电信息，用克里金插值法补全CKM。通过网格离散化和球面近似，建立混合整数多目标优化问题，将其转化为图论中的最短路径问题和旅行商问题，提出两种导航策略。

Result: 仿真结果表明，所提导航策略能快速扩展问题的帕累托边界，接近完全已知CKM的性能。两种策略在工程实践中各有优劣，揭示了无人机导航与CKM补全之间的权衡关系。

Conclusion: 提出的轨迹设计策略成功将无人机导航与CKM补全整合，通过图论方法有效解决了这一组合数学问题，为工程实践提供了可实施的解决方案，实现了导航精度与地图更新效率的双重优化。

Abstract: Cellular-connected unmanned aerial vehicles (UAVs) are expected to play an increasingly important role in future wireless networks. To facilitate the reliable navigation for cellular-connected UAVs, channel knowledge map (CKM) is considered a promising approach capable of tackling the non-negligible co-channel interference resulting from the high line-of-sight (LoS) probability of air-ground (AG) channels. Nevertheless, due to measurement constraints and the aging of information, CKM is usually incomplete and needs to be regularly updated to capture the dynamic nature of complex environments. In this paper, we propose a novel trajectory design strategy in which UAV navigation and CKM completion are incorporated into a common framework, enabling mutual benefits for both tasks. Specifically, a cellular-connected UAV deployed in an urban environment measures the radio information during its flight and completes the CKM with Kriging interpolation. Based on the method of grid discretization and spherical approximation, a mixed-integer multi-objective optimization problem is formulated. The problem falls into the category of combinatorial mathematics and is essentially equivalent to determining an optimum sequence of grid points to traverse. Through proper mathematical manipulation, the problem is reformulated as variants of two classic models in graph theory, namely the shortest-path problem (SPP) and the traveling salesman problem (TSP). Two navigation strategies based on the two different models are proposed and thoroughly compared based on numerical results to provide implementable methods for engineering practice and reveal the trade-offs between UAV navigation and CKM completion. Simulation results reveal that the proposed navigation strategies can quickly expand the Pareto boundary of the problem and approach the performance of fully-known CKM.

</details>


### [13] [Algebra in Algorithmic Coding Theory](https://arxiv.org/abs/2512.06478)
*Madhu Sudan*

Main category: cs.IT

TL;DR: 本文综述了纠错码的概念、历史、构造算法及其在信息传输中的应用，特别强调了代数方法在编码算法设计中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 虽然代数在纠错码构造中的作用已被广泛认可，但代数在算法设计中的作用往往较少被理解。本文旨在通过综述减少这种认知差异，展示代数方法在编码算法中的重要性。

Method: 采用综述方法，首先回顾纠错码的基本概念和历史，然后介绍使其在信息传输中有效的算法。重点展示基于应用代数的基本和现代构造方法，特别强调代数在算法设计中的角色。

Result: 提供了纠错码的全面综述，包括基本和现代构造方法，以及相关的编码算法。特别阐明了代数方法不仅在编码构造中，在算法设计中也发挥着关键作用。

Conclusion: 代数在纠错码的构造和算法设计中都至关重要。本文通过系统综述，有助于缩小人们对代数在编码构造和算法设计中作用理解的差距，促进对纠错码代数基础的更全面认识。

Abstract: We survey the notion and history of error-correcting codes and the algorithms needed to make them effective in information transmission. We then give some basic as well as more modern constructions of, and algorithms for, error-correcting codes that depend on relatively simple elements of applied algebra. While the role of algebra in the constructions of codes has been widely acknowledged in texts and other writings, the role in the design of algorithms is often less widely understood, and this survey hopes to reduce this difference to some extent.

</details>


### [14] [Improved Interactive Protocol for Synchronizing From Deletions](https://arxiv.org/abs/2512.06606)
*Haolun,Ni,Lev Tauz,Ryan Gabrys,Lara Dolecek*

Main category: cs.IT

TL;DR: 提出一种改进的文件同步协议，通过整合多删除纠错码和更通用的分区技术，降低通信成本


<details>
  <summary>Details</summary>
Motivation: 解决文件同步问题，特别是在一个文件是另一个文件的子序列且存在恒定删除率的情况下，需要降低通信成本

Method: 在现有基线协议基础上，整合先进的多删除纠错码，采用更通用的分区技术，推导适用于广泛删除纠错码类的通用上界

Result: 实验结果表明，该方法在通信成本上优于基线协议，实现了低冗余同步

Conclusion: 改进的协议在删除错误场景下能有效实现低冗余同步，证明了多删除纠错码和通用分区技术的有效性

Abstract: Data synchronization is a fundamental problem with applications in diverse fields such as cloud storage, genomics, and distributed systems. This paper addresses the challenge of synchronizing two files, one of which is a subsequence of the other and related through a constant rate of deletions, using an improved communication protocol. Building upon prior work, we integrate advanced multi-deletion correction codes into an existing baseline protocol, which previously relied on single-deletion correction. Our proposed protocol reduces communication cost by leveraging more general partitioning techniques as well as multi-deletion error correction. We derive a generalized upper bound on the expected number of transmitted bits, applicable to a broad class of deletion correction codes. Experimental results demonstrate that our approach outperforms the baseline in communication cost. These findings establish the efficacy of the improved protocol in achieving low-redundancy synchronization in scenarios where deletion errors occur.

</details>


### [15] [Cell-free ISAC for Drone Detection Considering Coverage and Age of Sensing](https://arxiv.org/abs/2512.06998)
*Zinat Behdad,Ozan Alp Topal,Cicek Cavdar*

Main category: cs.IT

TL;DR: 提出基于无小区大规模MIMO的集成感知与通信框架，用于无人机检测，引入感知年龄和感知覆盖度指标，通过热点分组、AP聚类和感知导频分配优化性能。


<details>
  <summary>Details</summary>
Motivation: 未经授权的无人机对公共安全构成威胁，需要在不影响现有通信服务的前提下实现空中监视。利用现有通信网络基础设施进行无人机检测具有重要价值。

Method: 采用无小区大规模MIMO架构，利用分布式接入点的空间多样性和协调性检测空中被动目标。引入感知年龄(AoS)和感知覆盖度指标，提出包含传输延迟、处理延迟和网络延迟的AoS模型。开发了热点分组、AP聚类和感知导频分配的网络配置策略，利用同时多点感知最小化AoS。

Result: 结果表明，当共享相同时频资源的热点数量与感知导频数量匹配时，AoS和感知覆盖度达到最佳权衡。模糊度是限制感知性能的主要因素。

Conclusion: 提出的无小区ISAC框架能够有效检测无人机，同时保持通信服务。AoS指标和网络配置策略为动态环境中的感知性能优化提供了有效方法。

Abstract: The growing presence of unauthorized drones poses significant threats to public safety, underscoring the need for aerial surveillance solutions. This work proposes a cell-free integrated sensing and communication (ISAC) framework enabling drone detection within the existing communication network infrastructure, while maintaining communication services. The system exploits the spatial diversity and coordination of distributed access points (APs) in a cell-free massive MIMO architecture to detect aerial passive targets. To evaluate sensing performance, we introduce two key metrics: age of sensing (AoS), capturing the freshness of sensing information, and sensing coverage. The proposed AoS metric includes not only the transmission delays as in the existing models, but also the processing for sensing and networking delay, which are critical in dynamic environments like drone detection. We introduce an ambiguity parameter quantifying the similarity between the target-to-receiver channels for two hotspots and develop a novel network configuration strategy, including hotspot grouping, AP clustering, and sensing pilot assignment, leveraging simultaneous multi-point sensing to minimize AoS. Our results show that the best trade-off between AoS and sensing coverage is achieved when the number of hotspots sharing the same time/frequency resource matches the number of sensing pilots, indicating ambiguity as the primary factor limiting the sensing performance.

</details>


### [16] [Function-Correcting Codes for Insertion-Deletion Channel](https://arxiv.org/abs/2512.07243)
*Anamika Singh,Abhay Kumar Singh*

Main category: cs.IT

TL;DR: 本文提出了针对插入删除信道的函数校正码新框架，建立了三种等价表述，推导了冗余度的上下界，并对特定函数类给出了具体边界。


<details>
  <summary>Details</summary>
Motivation: 插入删除信道中的冗余度优化是编码理论中长期存在的挑战，在DNA数据存储和文档交换中具有重要应用。受函数校正码框架的启发，需要针对插入删除信道开发新的编码框架来最小化冗余度同时保护消息的特定函数。

Method: 提出函数校正插入码、函数校正删除码和函数校正插入删除码三种等价表述；定义插入删除距离矩阵和不规则插入删除距离码；推导最优冗余度的上下界；建立Gilbert-Varshamov和Plotkin类边界；分析特定函数类的冗余度边界。

Result: 证明了三种函数校正码表述的等价性；获得了不规则插入删除距离码长度的边界；推导了最优冗余度的简化下界；对局部有界函数、VT综合征函数、运行次数函数和最大运行长度函数等特定函数类给出了冗余度边界。

Conclusion: 本文建立了针对插入删除信道的函数校正码理论框架，为DNA数据存储和文档交换等应用中的冗余度优化问题提供了系统的理论分析和边界结果，为进一步研究奠定了基础。

Abstract: In coding theory, handling errors that occur when symbols are inserted or deleted from a transmitted message is a long-standing challenge. Optimising redundancy for insertion and deletion channels remains a key open problem with significant importance for applications in DNA data storage and document exchange. Recently, a coding framework known as function-correcting codes has been proposed to address the challenge of minimising redundancy while preserving specific functions of the message. This framework has gained attention due to its potential applications in machine learning systems and long-term archival data storage. Motivated by the problem of redundancy optimisation for insertion and deletion channels, we propose a new framework called function-correcting codes for insdel channels. In this paper, we introduce the notions of function-correcting insertion codes, function-correcting deletion codes, and function-correcting insdel codes, and we show that these three formulations are equivalent. We then define insdel distance matrices and irregular insdel-distance codes, and derive lower and upper bounds on the optimal redundancy achievable by function-correcting codes for insdel channels. In addition, we establish Gilbert-Varshamov and Plotkin-like bounds on the length of irregular insdel-distance codes. Using the relation between optimal redundancy and the length of such codes, we obtain a simplified lower bound on optimal redundancy. Finally, we derive bounds on the optimal redundancy of function-correcting insdel codes for several classes of functions, including locally bounded functions, VT syndrome functions, the number-of-runs function, and the maximum-run-length function.

</details>


### [17] [Improved bounds and optimal constructions of pure quantum locally recoverable codes](https://arxiv.org/abs/2512.07256)
*Yang Li,Shitao Li,Gaojun Luo,San Ling*

Main category: cs.IT

TL;DR: 本文针对量子局部可恢复码(qLRCs)提出新的紧致界，基于Hermitian构造获得多种经典量子纠错码的纯qLRCs实例，并构建了多个无限族的最优qLRCs。


<details>
  <summary>Details</summary>
Motivation: 量子局部可恢复码在大型量子数据存储中具有应用潜力，且与量子LDPC码相关。然而，现有qLRCs的界不够紧致，最优构造仍待探索。

Method: 基于Hermitian构造的纯qLRCs，提出多个新的紧致界，并利用量子Hamming码、量子GRM码、量子Solomon-Stiffler码等经典量子纠错码构建纯qLRCs。

Result: 新提出的界比已知界更紧致；多种经典量子纠错码可转化为纯qLRCs并给出显式参数；构建了多个无限族的最优qLRCs，码长远超已知最优qLRCs。

Conclusion: 通过新界和经典量子码的转化，本文显著推进了qLRCs的理论研究，为大规模量子数据存储提供了更优的编码方案。

Abstract: By incorporating the concept of locality into quantum information theory, quantum locally recoverable codes (qLRCs) have been proposed, motivated by their potential applications in large-scale quantum data storage and their relevance to quantum LDPC codes. Despite the progress in optimal quantum error-correcting codes (QECCs), optimal constructions of qLRCs remain largely unexplored, partly due to the fact that the existing bounds for qLRCs are not sufficiently tight. In this paper, we focus on pure qLRCs derived from the Hermitian construction. We provide several new bounds for pure qLRCs and demonstrate that they are tighter than previously known bounds. Moreover, we show that a variety of classical QECCs, including quantum Hamming codes, quantum GRM codes, and quantum Solomon-Stiffler codes, give rise to pure qLRCs with explicit parameters. Based on these constructions, we further identify many infinite families of optimal qLRCs with respect to different bounds, achieving code lengths much larger than those of known optimal qLRCs.

</details>


### [18] [Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals](https://arxiv.org/abs/2512.07309)
*Guosheng Wang,Shen Wang,Lei Yang*

Main category: cs.IT

TL;DR: RFRP：一种自监督预训练框架，通过将定位模型与神经射频辐射场结合，利用大规模无标签RF数据进行表示学习，显著提升跨场景室内定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的室内定位模型依赖场景特定的标注数据，在跨场景泛化方面面临重大挑战，需要解决数据标注成本高和泛化能力不足的问题。

Method: 提出Radiance-Field Reinforced Pretraining (RFRP)框架，采用非对称自编码器架构，将大型定位模型(LM)与神经射频辐射场(RF-NeRF)耦合。LM编码接收的RF频谱为位置相关表示，RF-NeRF解码重建原始频谱，实现无监督表示学习。

Result: 在100个不同场景的7,327,321个位置收集RF数据，使用四种无线技术(RFID、BLE、WiFi、IIoT)。实验表明，RFRP预训练的LM相比未预训练模型减少40%以上定位误差，相比监督学习预训练模型减少21%误差。

Conclusion: RFRP框架通过自监督预训练有效利用大规模无标签RF数据，显著提升室内定位模型的跨场景泛化能力，为实际应用提供了更实用的解决方案。

Abstract: Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.

</details>


### [19] [Linear codes over $\frac{\mathbb{F}_q[u]}{\langle u^2 \rangle}$ with mixed-alphabet defining sets and their Gray images: Constructions of projective few-weight, distance-optimal and minimal codes](https://arxiv.org/abs/2512.07343)
*Leijo Jose,Lavanya G.,Anuradha Sharma*

Main category: cs.IT

TL;DR: 该论文在混合字母环上构造了四类无限族线性码，研究了它们的参数、Lee重量分布、Gray像，获得了具有新参数的少重量、近Griesmer、距离最优和极小码，并应用于秘密共享方案和局部性研究。


<details>
  <summary>Details</summary>
Motivation: 研究混合字母环上的线性码构造，特别是利用单纯复形定义集来获得具有良好性质的码，如少重量、距离最优、极小码等，并探索这些码在秘密共享方案和局部性方面的应用。

Method: 在混合字母环上构造四类无限族线性码，定义集为与三个具有单个极大元素的单纯复形相关的非空子集。通过分析Lee重量分布，研究Gray像，获得具有新参数的少重量、近Griesmer、距离最优和极小码。

Result: 获得了三类无限族的少重量、近Griesmer、距离最优和极小码，具有新参数；构造了两类无限族的射影少重量码；获得了二进制距离最优射影码和维数最优射影码；构造了四进制3重量码，其非零Hamming重量之和为码长的9/4倍，产生强正则行走图。

Conclusion: 成功构造了混合字母环上的线性码，获得了多种具有良好性质的码类，这些码在秘密共享方案中具有应用价值，其极小码可用于分析Massey秘密共享方案的最小访问结构和独裁参与者数量。

Abstract: Let $\mathcal{R}=\frac{\mathbb{F}_q[u]}{\langle u^2 \rangle}\times \mathbb{F}_q$ be the mixed alphabet ring. In this paper, we construct four infinite families of linear codes over the ring $\frac{\mathbb{F}_q[u]}{\langle u^2 \rangle}$ whose defining sets are certain nonempty subsets of $\mathcal{R}^m$ associated with three simplicial complexes of $\mathbb{F}_q^m,$ each possessing a single maximal element. We explicitly determine the parameters and Lee weight distributions of these codes. We also study their Gray images and obtain three infinite families of few weight, near Griesmer, distance optimal and minimal codes over $\mathbb{F}_q$ with new parameters. We also provide two constructions of infinite families of projective few weight codes over $\mathbb{F}_q$ with new parameters, and observe that these codes are self orthogonal for $q=2$ or $3.$ Additionally, we obtain two infinite families of binary distance optimal projective codes and an infinite family of dimension optimal projective codes over $\mathbb{F}_q$ with new parameters. Apart from this, we construct an infinite family of quaternary projective $3$-weight codes whose non zero Hamming weights sum to $\frac{9}{4}$ times the code length, which give rise to strongly walk regular graphs. As an application of our newly constructed minimal codes over $\mathbb{F}_q$, we examine the minimal access structures of Massey's secret sharing schemes based on their duals and determine the number of dictatorial participants in these schemes. Finally, we investigate the locality properties of our newly constructed projective codes.

</details>


### [20] [Dualities of dihedral and generalised quaternion codes and applications to quantum codes](https://arxiv.org/abs/2512.07354)
*Miguel Sales-Cabrera,Xaro Soler-Escrivà,Víctor Sotomayor*

Main category: cs.IT

TL;DR: 该论文研究了有限域上二面体群D_n和广义四元数群Q_n的群码，通过Wedderburn-Artin分解完全描述了这些群码的Hermitian对偶和欧几里得对偶结构，并应用于构建量子纠错码。


<details>
  <summary>Details</summary>
Motivation: 研究有限域上群码的对偶结构对于编码理论具有重要意义，特别是Hermitian对偶和欧几里得对偶的完整代数描述，这有助于构建量子纠错码等应用。

Method: 利用Wedderburn-Artin分解技术分析群代数F_q^2[D_n]和F_q[Q_n]的结构，通过群代数的分解来完全描述D_n码的Hermitian对偶和Q_n码的欧几里得对偶。

Result: 1. 完全描述了F_q^2上D_n码的Hermitian对偶结构；2. 确定了所有Hermitian自正交D_n码；3. 描述了F_q上Q_n码的欧几里得对偶结构；4. 由于F_q^2[Q_n]与F_q^2[D_{2n}]同构，也完全描述了Q_n码的Hermitian对偶。

Conclusion: 通过群代数的Wedderburn-Artin分解，成功建立了二面体群和广义四元数群码的对偶结构的完整代数描述，并将这些结果应用于系统构建量子纠错码，甚至重建了一些已知的最优量子码。

Abstract: Let $\mathbb{F}_q$ be a finite field of $q$ elements, for some prime power $q$, and let $G$ be a finite group. A (left) group code, or simply a $G$-code, is a (left) ideal of the group algebra $\mathbb{F}_q[G]$. In this paper, we provide a complete algebraic description for the hermitian dual code of any $D_n$-code over $\mathbb{F}_{q^2}$, where $D_n$ is a dihedral group of order $2n$ with $\gcd(q,n)=1$, through a suitable Wedderburn-Artin's decomposition of the group algebra $\mathbb{F}_{q^2}[D_n]$, and we determine all distinct hermitian self-orthogonal $D_n$-codes over $\mathbb{F}_{q^2}$. We also present a thorough representation of the euclidean dual code of any $Q_n$-code over $\mathbb{F}_q$, where $Q_n$ is a generalised quaternion group of order $4n$ with $\gcd(q,4n)=1$, via the Wedderburn-Artin's decomposition of the group algebra $\mathbb{F}_q[Q_n]$. In particular, since the semisimple group algebras $\mathbb{F}_{q^2}[Q_n]$ and $\mathbb{F}_{q^2}[D_{2n}]$ are isomorphic, then the hermitian dual code of any $Q_n$-code has also been fully described. As application of the hermitian dualities computed, we give a systematic construction, via the structure of the group algebra, to obtain quantum error-correcting codes, and in fact we rebuild some already known optimal quantum codes with this methodical approach.

</details>


### [21] [Orbit recovery under the rigid motions group](https://arxiv.org/abs/2512.07405)
*Amnon Balanov,Tamir Bendory,Dan Edidin*

Main category: cs.IT

TL;DR: 本文研究了SE(n)刚性运动群下的轨道恢复问题，证明了当噪声方差σ²→∞时，若d阶SO(n)矩能唯一确定信号轨道，则SE(n)轨道恢复需要N≳σ^{2d+4}个样本。算法上提出了三维刚性运动轨道恢复的可证明计算流程。


<details>
  <summary>Details</summary>
Motivation: 轨道恢复问题在信号处理、计算机视觉和结构生物学中具有基础重要性。特别是在结构生物学中，单粒子冷冻电镜(cryo-EM)和冷冻电子断层扫描(cryo-ET)等技术需要从受未知旋转和平移影响的噪声观测中重建未知信号。

Method: 主要理论贡献是：证明了d阶SO(n)矩可以从(d+2)阶SE(n)自相关中显式恢复，从而将旋转群的结果推广到刚性运动群。算法上提出了从刚性运动自相关中提取SO(3)矩，然后重建三维大分子结构的计算流程。

Result: 理论结果表明：当噪声方差σ²→∞时，轨道恢复需要N≳σ^{2d+4}个样本。该结果与多目标检测模型的样本复杂度相匹配。算法实验成功重建了三维大分子结构，且算法在任何噪声水平下都有效。

Conclusion: 本文建立了刚性运动轨道恢复的样本复杂度界限，并提出了可证明的计算方法。这一发现表明，即使是传统认为无法通过结构生物学电镜技术重建的非常小的分子，只要有足够数据，原则上也可以重建，为结构生物学开辟了新可能性。

Abstract: We study the orbit recovery problem under the rigid-motion group SE(n), where the objective is to reconstruct an unknown signal from multiple noisy observations subjected to unknown rotations and translations. This problem is fundamental in signal processing, computer vision, and structural biology.
  Our main theoretical contribution is bounding the sample complexity of this problem. We show that if the d-th order moment under the rotation group SO(n) uniquely determines the signal orbit, then orbit recovery under SE(n) is achievable with $N\gtrsim σ^{2d+4}$ samples as the noise variance $σ^2 \to \infty$. The key technical insight is that the d-th order SO(n) moments can be explicitly recovered from (d+2)-order SE(n) autocorrelations, enabling us to transfer known results from the rotation-only setting to the rigid-motion case. We further harness this result to derive a matching bound to the sample complexity of the multi-target detection model that serves as an abstract framework for electron-microscopy-based technologies in structural biology, such as single-particle cryo-electron microscopy (cryo-EM) and cryo-electron tomography (cryo-ET).
  Beyond theory, we present a provable computational pipeline for rigid-motion orbit recovery in three dimensions. Starting from rigid-motion autocorrelations, we extract the SO(3) moments and demonstrate successful reconstruction of a 3-D macromolecular structure. Importantly, this algorithmic approach is valid at any noise level, suggesting that even very small macromolecules, long believed to be inaccessible using structural biology electron-microscopy-based technologies, may, in principle, be reconstructed given sufficient data.

</details>


### [22] [Neural Compress-and-Forward for the Primitive Diamond Relay Channel](https://arxiv.org/abs/2512.07662)
*Ozan Aygün,Ezgi Ozyilkan,Elza Erkip*

Main category: cs.IT

TL;DR: 论文提出了一种基于学习的分布式压缩转发方案，用于双中继钻石信道，通过端到端训练实现接近理论界限的性能。


<details>
  <summary>Details</summary>
Motivation: 钻石中继信道是协作通信的经典模型，但将神经压缩转发扩展到双中继场景具有挑战性，因为需要完全分布式压缩且中继间无协调。现有神经CF方法主要研究单中继信道，多中继场景需要新的解决方案。

Method: 提出基于学习的量化器方案：每个中继使用一次性学习的量化器分别压缩其观测信号，目的地联合解码源消息。量化器通过端到端训练，利用输入相关性进行协作式分布式压缩，类似于Berger-Tung风格编码。

Result: 仿真结果表明，所提方案在使用有限阶调制的情况下，性能接近已知理论界限。神经CF能够扩展到多中继系统，同时保持性能和可解释性。

Conclusion: 基于学习的量化器能够实现有效的分布式压缩，神经压缩转发可以成功扩展到多中继系统，为协作通信提供了有前景的解决方案。

Abstract: The diamond relay channel, where a source communicates with a destination via two parallel relays, is one of the canonical models for cooperative communications. We focus on the primitive variant, where each relay observes a noisy version of the source signal and forwards a compressed description over an orthogonal, noiseless, finite-rate link to the destination. Compress-and-forward (CF) is particularly effective in this setting, especially under oblivious relaying where relays lack access to the source codebook. While neural CF methods have been studied in single-relay channels, extending them to the two-relay case is non-trivial, as it requires fully distributed compression without any inter-relay coordination. We demonstrate that learning-based quantizers at the relays can harness input correlations by operating remote, yet in a collaborative fashion, enabling effective distributed compression in line with Berger-Tung-style coding. Each relay separately compresses its observation using a one-shot learned quantizer, and the destination jointly decodes the source message. Simulation results show that the proposed scheme, trained end-to-end with finite-order modulation, operates close to the known theoretical bounds. These results demonstrate that neural CF can scale to multi-relay systems while maintaining both performance and interpretability.

</details>


### [23] [Enhancing Channel Estimation for OTFS systems using Sparse Bayesian Learning with Adaptive Threshold](https://arxiv.org/abs/2512.07704)
*Tengfei Qi,Yifei Yang,Xiong Deng,Zhinan Sun,Ziqiang Gao,Xihua Zou,Wei Pan,Lianshan Yan*

Main category: cs.IT

TL;DR: 本文提出了一种用于OTFS系统的自适应贝叶斯阈值去噪信道估计算法，在低信噪比下能有效解决伪峰问题并保持低复杂度


<details>
  <summary>Details</summary>
Motivation: OTFS调制在高速多普勒频移环境下优于OFDM，但需要进行DD域信道估计。现有方法在低信噪比时存在伪峰问题，需要更有效的抗噪算法

Method: 将DD域信道估计建模为稀疏信号恢复问题，在稀疏贝叶斯学习框架下提出自适应贝叶斯阈值主动去噪机制，结合无逆稀疏贝叶斯学习

Result: 仿真结果表明该算法在抗噪性能和复杂度方面优于现有信道估计算法

Conclusion: 提出的自适应贝叶斯阈值去噪算法能有效解决OTFS系统低信噪比下的伪峰问题，同时保持低复杂度，具有优越性能

Abstract: Orthogonal time frequency space (OTFS) modulation is a two-dimensional modulation scheme designed in the delay-Doppler (DD) domain, exhibiting superior performance over orthogonal frequency division multiplexing (OFDM) modulation in environments with high Doppler frequency shifts. We investigated the channel estimation in the DD domain of OTFS systems, modeling it as a sparse signal recovery problem. Subsequently, within the existing sparse Bayesian learning framework, we proposed an adaptive Bayesian threshold-based active denoising mechanism. Combined with inverse-free sparse Bayesian learning, this effectively addresses the pseudo-peak issue in low signal-to-noise ratio (SNR) scenarios while maintaining low complexity. The simulation results demonstrate that this algorithm outperforms existing channel estimation algorithms in terms of anti-noise performance and complexity.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [Distribution-Aware Exploration for Adaptive HNSW Search](https://arxiv.org/abs/2512.06636)
*Chao Zhang,Renée J. Miller*

Main category: cs.DB

TL;DR: Ada-ef：一种数据驱动的自适应探索因子方法，为HNSW向量搜索动态配置ef参数，在保证目标召回率的同时最小化计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有HNSW系统使用静态统一的ef参数配置，忽略了真实世界嵌入数据和查询工作负载的非均匀性和偏斜性，导致缺乏召回率保证以及因过度或不足搜索导致的低效ANNS性能。

Method: 提出Ada-ef方法：1）建立理论基础的统计模型，捕捉查询与数据库向量之间的相似性分布；2）设计查询评分机制，区分需要小ef和大ef的查询；3）为每个查询动态分配适当的ef值以满足目标召回率。

Result: 在OpenAI和Cohere的Transformer模型生成的真实世界嵌入上测试，相比最先进的学习型自适应方法，Ada-ef在达到目标召回率的同时，在线查询延迟降低高达4倍，离线计算时间减少50倍，离线内存使用减少100倍。

Conclusion: Ada-ef通过数据驱动、更新友好、查询自适应的方式动态配置ef参数，有效解决了HNSW系统中缺乏召回率保证和搜索效率低下的问题，显著提升了向量搜索的性能和效率。

Abstract: Hierarchical Navigable Small World (HNSW) is widely adopted for approximate nearest neighbor search (ANNS) for its ability to deliver high recall with low latency on large-scale, high-dimensional embeddings. The exploration factor, commonly referred to as ef, is a key parameter in HNSW-based vector search that balances accuracy and efficiency. However, existing systems typically rely on manually and statically configured ef values that are uniformly applied across all queries. This results in a distribution-agnostic configuration that fails to account for the non-uniform and skewed nature of real-world embedding data and query workloads. As a consequence, HNSW-based systems suffer from two key practical issues: (i) the absence of recall guarantees, and (ii) inefficient ANNS performance due to over- or under-searching. In this paper, we propose Adaptive-ef (Ada-ef), a data-driven, update-friendly, query-adaptive approach that dynamically configures ef for each query at runtime to approximately meet a declarative target recall with minimal computation. The core of our approach is a theoretically grounded statistical model that captures the similarity distribution between each query and the database vectors. Based on this foundation, we design a query scoring mechanism that distinguishes between queries requiring only small ef and those that need larger ef to meet a target recall, and accordingly assigns an appropriate ef to each query. Experimental results on real-world embeddings produced by state-of-the-art Transformer models from OpenAI and Cohere show that, compared with state-of-the-art learning-based adaptive approaches, our method achieves the target recall while avoiding both over- and under-searching, reducing online query latency by up to 4x, offline computation time by 50x, and offline memory usage by 100x.

</details>


### [25] [OSM+: Billion-Level Open Street Map Data Processing System for City-wide Experiments](https://arxiv.org/abs/2512.06743)
*Guanjie Zheng,Ziyang Su,Yiheng Wang,Yuhang Luo,Hongwei Zhang,Xuanhe Zhou,Linghe Kong,Fan Wu,Wen Ling*

Main category: cs.DB

TL;DR: 该论文处理了全球OpenStreetMap数据，发布了包含10亿顶点的结构化道路网络数据集OSM+，并提供了三个应用案例的基准测试。


<details>
  <summary>Details</summary>
Motivation: 处理全球道路网络数据需要大量计算资源，且处理结果难以统一用于下游任务测试。因此需要构建一个高可访问性和可用性的全球道路网络数据集。

Method: 使用云服务的5000核分布式计算处理OpenStreetMap数据，构建结构化全球道路网络图数据集，并开发数据转换器便于多模态时空数据集成。

Result: 发布了OSM+数据集（10亿顶点道路网络图），包含三个基准测试：31个城市的交通预测基准、6个城市的交通政策控制基准，以及便于地理空间基础模型训练的数据转换器。

Conclusion: OSM+数据集为城市研究提供了高可访问性和可用性的基础，推动了算法从百级到千级交叉口规模的扩展，并促进了地理空间基础模型的发展。

Abstract: Road network data can provide rich information about cities and thus become the base for various urban research. However, processing large volume world-wide road network data requires intensive computing resources and the processed results might be different to be unified for testing downstream tasks. Therefore, in this paper, we process the OpenStreetMap data via a distributed computing of 5,000 cores on cloud services and release a structured world-wide 1-billion-vertex road network graph dataset with high accessibility (opensource and downloadable to the whole world) and usability (open-box graph structure and easy spatial query interface). To demonstrate how this dataset can be utilized easily, we present three illustrative use cases, including traffic prediction, city boundary detection and traffic policy control, and conduct extensive experiments for these three tasks. (1) For the well-investigated traffic prediction tasks, we release a new benchmark with 31 cities (traffic data processed and combined with our released OSM+ road network dataset), to provide much larger spatial coverage and more comprehensive evaluation of compared algorithms than the previously frequently-used datasets. This new benchmark will push the algorithms on their scalability from hundreds of road network intersections to thousands of intersections. (2) While for the more advanced traffic policy control task which requires interaction with the road network, we release a new 6 city datasets with much larger scale than the previous datasets. This brings new challenge for thousand-scale multi-agent coordination. (3) Along with the OSM+ dataset, the release of data converters facilitates the integration of multimodal spatial-temporal data for geospatial foundation model training, thereby expediting the process of uncovering compelling scientific insights. PVLDB Reference Forma

</details>


### [26] [A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases](https://arxiv.org/abs/2512.06852)
*Manideep Reddy Chinthareddy*

Main category: cs.DB

TL;DR: 提出"分块对象"模式，将大型逻辑实体存储为数据库内的有序分块集合，解决键值数据库严格大小限制下的多区域访问问题。


<details>
  <summary>Details</summary>
Motivation: 许多托管键值数据库和NoSQL数据库（如DynamoDB、Cosmos DB）有严格的最大项目大小限制（如400KB），这对需要低延迟、多区域访问大型对象的应用程序造成重大架构挑战。标准解决方案是将有效负载卸载到对象存储中，但这会引入网络开销和跨系统复制延迟，导致主动-主动架构中的竞态条件。

Method: 提出"分块对象"模式，将大型逻辑实体持久化为数据库内的有序分块集合。提供使用Amazon DynamoDB Global Tables的参考实现，该设计可推广到任何具有每项大小限制和多区域复制的键值存储。

Result: 使用处理超过200,000笔交易/小时的生产系统遥测数据进行评估。结果表明，分块对象模式消除了跨系统复制延迟风险，并通过将数据和元数据保持在单一一致性域内，将1MB有效负载的p99跨区域一致性时间减少了。

Conclusion: 分块对象模式提供了一种在具有严格大小限制的键值数据库中存储大型对象的有效方法，消除了标准指针模式带来的跨系统复制延迟问题，提高了多区域架构中的一致性性能。

Abstract: Many managed key-value and NoSQL databases - such as Amazon DynamoDB, Azure Cosmos DB, and Google Cloud Firestore - enforce strict maximum item sizes (e.g., 400 KB in DynamoDB). This constraint imposes significant architectural challenges for applications requiring low-latency, multi-region access to objects that exceed these limits. The standard industry recommendation is to offload payloads to object storage (e.g., Amazon S3) while retaining a pointer in the database. While cost-efficient, this "pointer pattern" introduces network overhead and exposes applications to non-deterministic replication lag between the database and the object store, creating race conditions in active-active architectures.
  This paper presents a "chunked-object" pattern that persists large logical entities as sets of ordered chunks within the database itself. We precisely define the pattern and provide a reference implementation using Amazon DynamoDB Global Tables. The design generalizes to any key-value store with per-item size limits and multi-region replication. We evaluate the approach using telemetry from a production system processing over 200,000 transactions per hour. Results demonstrate that the chunked-object pattern eliminates cross-system replication lag hazards and reduces p99 cross-region time-to-consistency for 1 MB payloads by keeping data and metadata within a single consistency domain.

</details>


### [27] [Space efficient implementation of hypergraph dualization in the D-basis algorithm](https://arxiv.org/abs/2512.06988)
*Skylar Homan,Anoop Krishnadas,Kira Adaricheva*

Main category: cs.DB

TL;DR: 提出名为Small Space的D-basis算法新实现，显著降低内存使用，仅输出属性频率而非完整蕴含规则，适用于大数据分析场景。


<details>
  <summary>Details</summary>
Motivation: 传统D-basis算法需要存储所有蕴含规则，内存消耗大，而实际数据分析中更关注属性频率而非具体规则，需要更高效的内存优化实现。

Method: 采用Small Space实现，基于超图对偶化算法和Reverse Search对偶化技术，逐步计算属性频率而无需存储所有发现的蕴含规则，仅输出属性在D-basis蕴含前件中出现的频率。

Result: 新实现显著降低了内存使用，通过对比新旧版本的运行时间和最大内存使用，验证了Small Space实现的有效性。

Conclusion: Small Space实现为D-basis算法提供了更高效的内存优化版本，特别适合大规模数据分析应用，解决了传统实现内存消耗大的问题。

Abstract: We present a new implementation of the $D$-basis algorithm called the Small Space which considerably reduces the algorithm's memory usage for data analysis applications. The previous implementation delivers the complete set of implications that hold on the set of attributes of an input binary table. In the new version, the only output is the frequencies of attributes that appear in the antecedents of implications from the $D$-basis, with a fixed consequent attribute. Such frequencies, rather than the implications themselves, became the primary focus in analysis of datasets where the $D$-basis has been applied over the last decade. The $D$-basis employs a hypergraph dualization algorithm, and a dualization implementation known as Reverse Search allows for the gradual computation of frequencies without the need for storing all discovered implications. We demonstrate the effectiveness of the Small Space implementation by comparing the runtimes and maximum memory usage of this new version with the current implementation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [28] [Strategic Experimentation with Private Payoffs](https://arxiv.org/abs/2512.06180)
*Jérôme Renault,Eilon Solan,Nicolas Vieille*

Main category: cs.GT

TL;DR: 研究具有指数分布臂的私人观测战略实验博弈，发现均衡实验量总是高于公开观测基准，且纯策略均衡实验量至少达到社会最优水平，可能更高。


<details>
  <summary>Details</summary>
Motivation: 研究在实验结果私人观测（而非公开观测）的情况下，战略实验博弈中的均衡行为。探讨信息不对称如何影响实验决策和社会效率。

Method: 使用具有指数分布臂的战略实验博弈模型，比较私人观测和公开观测两种信息结构下的均衡。引入新的"鼓励效应"概念，分析玩家隐藏失败信息以鼓励对方未来实验的行为。

Result: 1) 私人观测下的均衡实验量总是高于公开观测基准；2) 纯策略均衡实验量至少达到社会最优水平，可能更高；3) 提供了过度实验程度的紧界。

Conclusion: 实验结果私人观测会通过鼓励效应增加实验量，这种效应可能导致过度实验。信息不对称在战略实验环境中具有重要影响，需要重新评估社会最优实验水平。

Abstract: We study a strategic experimentation game with exponential bandits, in which experiment outcomes are private. The equilibrium amount of experimentation is always higher than in the benchmark case where experiment outcomes are publicly observed. In addition, for pure equilibria, the equilibrium amount of experimentation is at least socially optimal, and possibly higher. We provide a tight bound on the degree of over-experimentation. The analysis rests on a new form of encouragement effect, according to which a player may hide the absence of a success to encourage future experimentation by the other player, which incentivizes current experimentation.

</details>


### [29] [The Communication Complexity of Combinatorial Auctions with Additional Succinct Bidders](https://arxiv.org/abs/2512.06585)
*Frederick V. Qiu,S. Matthew Weinberg,Qianfan Zhang*

Main category: cs.GT

TL;DR: 研究组合拍卖中福利最大化的通信复杂度，分析标准估值类（如次可加/XOS）与简洁估值类（如单目标）混合时的近似算法与下界，发现简洁竞拍者的加入会显著改变通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统组合拍卖研究通常关注单一估值类，但实际拍卖中可能同时存在不同类型的竞拍者。本文研究混合估值类（标准估值+简洁估值）对通信复杂度的影响，探索简洁竞拍者是否会使问题变得更容易或更难。

Method: 分析SA∪SC（次可加+简洁估值）和XOS∪SC（XOS+简洁估值）两种混合估值类的通信复杂度。设计多项式通信的近似算法，并证明匹配的近似下界。特别关注SA∪SM和XOS∪SM（次可加/XOS+单目标估值）作为简洁估值的代表。

Result: 对于SA∪SC：存在3-近似多项式通信算法；当n→∞时，存在匹配的3-近似下界（高于纯SA的2-近似下界）；对于所有n≥3，SA∪SM与SA之间存在常数分离。对于XOS∪SC：存在2-近似多项式通信算法；当n→∞时，存在匹配的2-近似下界（高于纯XOS的e/(e-1)-近似下界）；对于所有n≥2，XOS∪SM与XOS之间存在常数分离。

Conclusion: 简洁估值竞拍者的加入对组合拍卖的通信复杂度有非平凡影响：虽然简洁估值本身可以高效通信，但混合估值类会改变最优近似比，使得问题比纯标准估值类更难近似。这表明简洁竞拍者实际上增加了问题的复杂性。

Abstract: We study the communication complexity of welfare maximization in combinatorial auctions with bidders from either a standard valuation class (which require exponential communication to explicitly state, such as subadditive or XOS), or arbitrary succinct valuations (which can be fully described in polynomial communication, such as single-minded). Although succinct valuations can be efficiently communicated, we show that additional succinct bidders have a nontrivial impact on communication complexity of classical combinatorial auctions. Specifically, let $n$ be the number of subadditive/XOS bidders. We show that for SA $\cup$ SC (the union of subadditive and succinct valuations): (1) There is a polynomial communication $3$-approximation algorithm; (2) As $n \to \infty$, there is a matching $3$-hardness of approximation, which (a) is larger than the optimal approximation ratio of $2$ for SA, and (b) holds even for SA $\cup$ SM (the union of subadditive and single-minded valuations); and (3) For all $n \geq 3$, there is a constant separation between the optimal approximation ratios for SA $\cup$ SM and SA (and therefore between SA $\cup$ SC and SA as well). Similarly, we show that for XOS $\cup$ SC: (1) There is a polynomial communication $2$-approximation algorithm; (2) As $n \to \infty$, there is a matching $2$-hardness of approximation, which (a) is larger than the optimal approximation ratio of $e/(e-1)$ for XOS, and (b) holds even for XOS $\cup$ SM; and (3) For all $n \geq 2$, there is a constant separation between the optimal approximation ratios for XOS $\cup$ SM and XOS (and therefore between XOS $\cup$ SC and XOS as well).

</details>


### [30] [Learning Paths to Multi-Sector Equilibrium: Belief Dynamics Under Uncertain Returns to Scale](https://arxiv.org/abs/2512.07013)
*Stefano Nasini,Rabia Nessah,Bertrand Wigniolle*

Main category: cs.GT

TL;DR: 在多部门一般均衡模型中，企业在不完全信息下通过贝叶斯学习更新对规模报酬的认知，研究发现短期记忆学习优于长期记忆学习，且投入决策包含所有相关信息。


<details>
  <summary>Details</summary>
Motivation: 研究企业在不完全信息下如何学习自身生产规模报酬，以及这种学习动态对市场均衡的影响，特别是探究企业能否有效学习真实规模报酬的条件。

Method: 构建多部门一般均衡模型，企业使用最大后验估计（MAP）迭代更新对规模报酬的信念，基于观察到的生产结果进行学习，比较长期记忆（路径依赖）和短期记忆（路径独立）两种学习方法。

Result: 研究发现：1）异质性冲击影响学习过程；2）投入决策编码了信念更新所需的所有相关信息；3）长期记忆学习方法（跟踪所有历史估计）表现不如短期记忆方法。

Conclusion: 企业在不完全信息下能够通过短期记忆学习有效更新对规模报酬的认知，但过度依赖历史信息反而会降低学习效率，投入决策是学习的关键信息来源。

Abstract: This paper explores the dynamics of learning in a multi-sector general equilibrium model where firms operate under incomplete information about their production returns to scale. Firms iteratively update their beliefs using maximum a-posteriori estimation, derived from observed production outcomes, to refine their knowledge of their returns to scale. The implications of these learning dynamics for market equilibrium and the conditions under which firms can effectively learn their true returns to scale are the key objects of this study. Our results shed light on how idiosyncratic shocks influence the learning process and demonstrate that input decisions encode all pertinent information for belief updates. Additionally, we show that a long-memory (path-dependent) learning which keeps track of all past estimations ends up having a worse performance than a short-memory (path-independent) approach.

</details>


### [31] [Zero Carbon V2X Tariffs for Non-Domestic Customers](https://arxiv.org/abs/2512.07308)
*Elisheva S Shamash,Zhong Fan*

Main category: cs.GT

TL;DR: 基于VCG机制设计电动汽车与电网双向充电合同，利用V2X技术实现峰谷电力交易，降低成本和碳排放


<details>
  <summary>Details</summary>
Motivation: 为实现全球净零目标，电力合同交易在能源市场中日益重要。需要开发能够利用电动汽车双向电池技术，在非高峰时段储存能源并在高峰时段输出的高效能源交易合同，以调节能源市场、降低成本和碳排放。

Method: 提出基于VCG（Vickrey-Clarke-Groves）机制的合同设计，利用V2X（Vehicle to Everything）技术，让电动汽车车队能够根据各自的能源消耗、输出计划和成本，在全天高效地向电网输出电力。

Result: 开发了能够实现高效能源交易的合同机制，使电动汽车能够在非高峰时段储存能源并在高峰时段输出，从而调节能源市场运作。

Conclusion: 基于VCG机制的V2X合同能够有效促进电动汽车与电网之间的电力交易，有助于降低能源成本、减少碳排放，为实现净零目标提供可行的市场调节工具。

Abstract: With the aim of meeting the worlds net-zero objectives, electricity trading through contractual agreements is becoming increasingly relevant in global and local energy markets. We develop contracts enabling efficient energy trading using Vehicle to Everything technology which can be applied to regulate energy markets and reduce costs and carbon emissions by using electric vehicles with bidirectional batteries to store energy during offpeak hours for export during peak hours. We introduce a contract based on the VCG mechanism which enables fleets of electric vehicles to export electricity to the grid efficiently throughout the day, where each electric vehicle has its energy consumption and exporting schedules and costs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [32] [Enhanced Multimodal Video Retrieval System: Integrating Query Expansion and Cross-modal Temporal Event Retrieval](https://arxiv.org/abs/2512.06334)
*Van-Thinh Vo,Minh-Khoi Nguyen,Minh-Huy Tran,Anh-Quan Nguyen-Tran,Duy-Tan Nguyen,Khanh-Loi Nguyen,Anh-Minh Phan*

Main category: cs.IR

TL;DR: 提出跨模态时序事件检索框架，允许不同查询模态描述序列中的不同场景，通过KDE-GMM算法自适应确定场景转换阈值，提取关键帧作为高质量视觉示例，结合LLM优化查询，在胡志明AI挑战赛2025中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态检索系统通常在整个序列中使用单一查询模态，在复杂时序上下文中鲁棒性有限，无法灵活处理不同场景需要不同查询模态描述的情况。

Method: 提出跨模态时序事件检索框架，允许不同查询模态描述序列中的不同场景；开发KDE-GMM算法自适应确定场景转换和幻灯片变化的决策阈值，优化关键帧选择；提取的关键帧作为紧凑高质量视觉示例保留语义本质；结合LLM优化和扩展用户查询。

Result: 系统在胡志明AI挑战赛2025中展示了有效性和鲁棒性，取得了优异结果，提高了检索精度和效率。

Conclusion: 提出的跨模态时序事件检索框架通过自适应阈值确定和LLM查询优化，显著提升了视频多模态检索在复杂时序上下文中的性能，为多媒体信息检索提供了更灵活有效的解决方案。

Abstract: Multimedia information retrieval from videos remains a challenging problem. While recent systems have advanced multimodal search through semantic, object, and OCR queries - and can retrieve temporally consecutive scenes - they often rely on a single query modality for an entire sequence, limiting robustness in complex temporal contexts. To overcome this, we propose a cross-modal temporal event retrieval framework that enables different query modalities to describe distinct scenes within a sequence. To determine decision thresholds for scene transition and slide change adaptively, we build Kernel Density Gaussian Mixture Thresholding (KDE-GMM) algorithm, ensuring optimal keyframe selection. These extracted keyframes act as compact, high-quality visual exemplars that retain each segment's semantic essence, improving retrieval precision and efficiency. Additionally, the system incorporates a large language model (LLM) to refine and expand user queries, enhancing overall retrieval performance. The proposed system's effectiveness and robustness were demonstrated through its strong results in the Ho Chi Minh AI Challenge 2025.

</details>


### [33] [Beyond Existing Retrievals: Cross-Scenario Incremental Sample Learning Framework](https://arxiv.org/abs/2512.06381)
*Tao Wang,Xun Luo,Jinlong Guo,Yuliang Yan,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 提出IncRec框架，专注于跨场景增量样本学习，通过构建未被现有模型检索的极端增量样本，并设计一致性感知对齐模块，提升推荐系统检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行多检索架构虽然计算高效且能全面覆盖用户兴趣，但许多检索方法通过整合跨场景样本来提升性能上限时，忽略了部分跨场景样本已被系统中现有模型检索的事实，导致边际效用递减，增量性能提升有限。

Method: 1) 构建极端跨场景增量样本（未被任何现有模型检索的样本）；2) 设计增量样本学习框架，专注于捕捉增量表示以提升整体检索性能；3) 引入一致性感知对齐模块，使模型更偏好具有高曝光概率的增量样本。

Result: 通过离线和在线A/B测试验证了框架的优越性。在淘宝首页推荐中部署IncRec，实现了在线交易量1%的增长，证明了其实际应用价值。

Conclusion: IncRec框架通过专注于跨场景增量样本学习，有效解决了现有检索方法中跨场景样本边际效用递减的问题，显著提升了推荐系统的检索性能，并在实际应用中取得了显著效果。

Abstract: The parallelized multi-retrieval architecture has been widely adopted in large-scale recommender systems for its computational efficiency and comprehensive coverage of user interests. Many retrieval methods typically integrate additional cross-scenario samples to enhance the overall performance ceiling. However, those model designs neglect the fact that a part of the cross-scenario samples have already been retrieved by existing models within a system, leading to diminishing marginal utility in delivering incremental performance gains. In this paper, we propose a novel retrieval framework IncRec, specifically for cross-scenario incremental sample learning. The innovations of IncRec can be highlighted as two aspects. Firstly, we construct extreme cross-scenario incremental samples that are not retrieved by any existing model. And we design an incremental sample learning framework which focuses on capturing incremental representation to improve the overall retrieval performance. Secondly, we introduce a consistency-aware alignment module to further make the model prefer incremental samples with high exposure probability. Extensive offline and online A/B tests validate the superiority of our framework over state-of-the-art retrieval methods. In particular, we deploy IncRec in the Taobao homepage recommendation, achieving a 1% increase in online transaction count, demonstrating its practical applicability.

</details>


### [34] [Enhancing Medical Cross-Modal Hashing Retrieval using Dropout-Voting Mixture-of-Experts Fusion](https://arxiv.org/abs/2512.06449)
*Jaewon Ahn,Woosung Jang,Beakcheol Jang*

Main category: cs.IR

TL;DR: 提出MCMFH模型，结合dropout voting和MoE对比融合模块的CLIP跨模态哈希检索框架，用于医学图像-文本检索，实现高精度、快速检索和低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 医学领域多模态数据丰富，跨模态检索对图像解读、诊断支持和医学教育日益重要。随着分布式医疗数据整合需求增加，需要优化检索系统在速度、内存效率和准确性方面的性能，以应对医疗数据量激增的挑战。

Method: 提出MCMFH模型：1) 基于CLIP的跨模态哈希检索结构；2) 引入dropout voting机制；3) 集成混合专家(MoE)对比融合模块；4) 应用混合损失函数。

Result: 模型在放射学和非放射学医学数据集上验证，能够同时实现高精度和快速检索，特别适用于低内存环境。

Conclusion: MCMFH框架有效解决了医学跨模态检索中的性能平衡问题，在准确性、速度和内存效率方面表现出色，为分布式医疗数据环境下的高效检索提供了可行方案。

Abstract: In recent years, cross-modal retrieval using images and text has become an active area of research, especially in the medical domain. The abundance of data in various modalities in this field has led to a growing importance of cross-modal retrieval for efficient image interpretation, data-driven diagnostic support, and medical education. In the context of the increasing integration of distributed medical data across healthcare facilities with the objective of enhancing interoperability, it is imperative to optimize the performance of retrieval systems in terms of the speed, memory efficiency, and accuracy of the retrieved data. This necessity arises in response to the substantial surge in data volume that characterizes contemporary medical practices. In this study, we propose a novel framework that incorporates dropout voting and mixture-of-experts (MoE) based contrastive fusion modules into a CLIP-based cross-modal hashing retrieval structure. We also propose the application of hybrid loss. So we now call our model MCMFH which is a medical cross-modal fusion hashing retrieval. Our method enables the simultaneous achievement of high accuracy and fast retrieval speed in low-memory environments. The model is demonstrated through experiments on radiological and non-radiological medical datasets.

</details>


### [35] [Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems](https://arxiv.org/abs/2512.06590)
*Tendai Mukande,Esraa Ali,Annalina Caputo,Ruihai Dong,Noel OConnor*

Main category: cs.IR

TL;DR: HGLMRec：一种基于多LLM代理的推荐系统，使用超图编码器捕捉用户与物品间的复杂多行为关系，通过仅检索相关token降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统面临幻觉问题（降低推荐性能）和实际场景中的高计算成本问题，需要一种既能减少计算开销又能提升推荐准确性的解决方案。

Method: 提出HGLMRec模型，采用多LLM代理架构，结合超图编码器捕捉用户与物品间的复杂多行为关系，在推理时仅检索相关token以减少计算开销。

Result: 实验结果显示HGLMRec在较低计算成本下，性能优于现有最先进基线方法。

Conclusion: HGLMRec通过超图编码和多LLM代理架构，有效解决了生成式推荐系统的幻觉和计算成本问题，实现了性能提升与效率优化的平衡。

Abstract: Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.

</details>


### [36] [An Index-based Approach for Efficient and Effective Web Content Extraction](https://arxiv.org/abs/2512.06641)
*Yihan Chen,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.IR

TL;DR: 提出Index-based Web Content Extraction方法，将网页内容提取从缓慢的token生成任务转变为高效的索引预测任务，显著提升提取速度和准确性


<details>
  <summary>Details</summary>
Motivation: 当前网页代理需要处理大量网页内容，但现有提取方法存在不足：生成式提取模型延迟高，基于规则的方法缺乏适应性，分块重排序方法忽略网页结构。需要一种既高效又准确的网页内容提取方法

Method: 将HTML分割为结构感知、可寻址的片段，然后仅提取与查询相关的内容的位置索引。这种方法将提取延迟与内容长度解耦，实现快速、查询相关的提取

Result: 在RAG QA系统中作为后检索处理组件使用时提高了QA准确性。在两种场景下（主要内容提取和查询相关提取）直接测量与目标内容的匹配率，实验结果显示在准确性和速度上都优于现有方法

Conclusion: 该方法通过将提取过程重构为索引预测任务，有效解决了LLM上下文管理中的技术挑战，在效率和效果上取得了平衡，弥合了LLM与海量网页之间的差距

Abstract: As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.

</details>


### [37] [Foresight Prediction Enhanced Live-Streaming Recommendation](https://arxiv.org/abs/2512.06700)
*Jiangxia Cao,Ruochen Yang,Xiang Chen,Changxin Lao,Yueyang Liu,Yusheng Huang,Yuanhao Tian,Xiangyu Wu,Shuang Yang,Zhaojie Liu,Guorui Zhou*

Main category: cs.IR

TL;DR: 该论文提出了一种基于语义量化的直播内容预测方法，通过捕捉作者特征和语义演变趋势来增强推荐系统的前瞻性，从而在直播亮点时刻为用户提供更好的推荐体验。


<details>
  <summary>Details</summary>
Motivation: 直播作为一种新兴媒体，其内容的动态性和实时性对推荐算法提出了更高要求。研究发现用户在直播亮点时刻有更好的体验和更积极的行为，但由于推荐时无法获取未来内容，而用户参与度又取决于后续内容是否符合其兴趣，因此需要预测未来直播内容。

Method: 对直播片段进行语义量化得到语义ID（Sid），编码历史Sid序列以捕捉作者特征，建模Sid演变趋势以实现对未来内容的前瞻性预测，并通过精炼特征增强排序模型。

Result: 大量的离线和在线实验证明了该方法的有效性。

Conclusion: 通过语义量化和内容演变趋势建模，能够实现对未来直播内容的预测，从而在直播亮点时刻为用户提供更精准的推荐，提升用户体验和平台效果。

Abstract: Live-streaming, as an emerging media enabling real-time interaction between authors and users, has attracted significant attention. Unlike the stable playback time of traditional TV live or the fixed content of short video, live-streaming, due to the dynamics of content and time, poses higher requirements for the recommendation algorithm of the platform - understanding the ever-changing content in real time and push it to users at the appropriate moment. Through analysis, we find that users have a better experience and express more positive behaviors during highlight moments of the live-streaming. Furthermore, since the model lacks access to future content during recommendation, yet user engagement depends on how well subsequent content aligns with their interests, an intuitive solution is to predict future live-streaming content. Therefore, we perform semantic quantization on live-streaming segments to obtain Semantic ids (Sid), encode the historical Sid sequence to capture the author's characteristics, and model Sid evolution trend to enable foresight prediction of future content. This foresight enhances the ranking model through refined features. Extensive offline and online experiments demonstrate the effectiveness of our method.

</details>


### [38] [WisPaper: Your AI Scholar Search Engine](https://arxiv.org/abs/2512.06879)
*Li Ju,Jun Zhao,Mingxu Chai,Ziyu Shen,Xiangyang Wang,Yage Geng,Chunchun Ma,Hao Peng,Guangbin Li,Tao Li,Chengyong Liao,Fu Wang,Xiaolong Wang,Junshen Chen,Rui Gong,Shijia Liang,Feiyan Li,Ming Zhang,Kexin Tan,Jujie Ye,Zhiheng Xi,Shihan Dou,Tao Gui,Yuankai Ying,Yang Shi,Yue Zhang,Qi Zhang*

Main category: cs.IR

TL;DR: WisPaper是一个智能学术检索与文献管理平台，集成了学者搜索、文献库和AI推荐三大功能，为研究人员提供从文献发现到管理的闭环工作流。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物呈指数级增长，研究人员难以高效地定位和管理相关文献。现有学术工具无法提供从文献发现到持续跟踪的完整工作流。

Method: WisPaper通过三个集成功能解决文献管理挑战：1) 学者搜索（快速关键词搜索和深度智能搜索）；2) 文献库（可定制的知识库）；3) AI推荐系统（基于用户兴趣自动推送新文献）。

Result: WisPaper显著减少了不同背景研究人员在文献筛选和管理上的时间，使他们能够专注于核心研究活动。该平台支持多语言和多学科，已公开可用并服务于学术界和工业界的研究人员。

Conclusion: WisPaper提供了一个独特的闭环工作流，无缝连接文献发现、管理和研究前沿持续跟踪，解决了现有学术工具的局限性，提升了研究效率。

Abstract: Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.

</details>


### [39] [Structural and Disentangled Adaptation of Large Vision Language Models for Multimodal Recommendation](https://arxiv.org/abs/2512.06883)
*Zhongtao Rao,Peilin Zhou,Dading Chong,Zhiwei Chen,Shoujin Wang,Nan Tang*

Main category: cs.IR

TL;DR: SDA框架通过结构对齐和解耦适配，解决多模态推荐中LVLMs应用的表示对齐和梯度冲突问题，显著提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态推荐中面临两个关键挑战：表示不对齐（领域差距导致嵌入空间不一致）和梯度冲突（共享适配器导致干扰和判别力不足）

Method: 提出SDA框架，包含两个组件：跨模态结构对齐（CMSA）使用模态内结构作为软教师对齐嵌入；模态解耦适配（MoDA）通过专家化门控低秩路径解耦梯度流

Result: 在三个Amazon数据集上，SDA与现有多模态和序列推荐器无缝集成，平均提升Hit@10 6.15%，NDCG@10 8.64%，长尾物品上提升达12.83%和18.70%，推理开销最小

Conclusion: SDA框架有效解决了LVLMs在多模态推荐中的应用挑战，通过结构对齐和解耦适配显著提升了推荐性能，特别是在长尾物品上表现突出

Abstract: Multimodal recommendation enhances accuracy by leveraging visual and textual signals, and its success largely depends on learning high-quality cross-modal representations. Recent advances in Large Vision-Language Models (LVLMs) offer unified multimodal representation learning, making them a promising backbone. However, applying LVLMs to recommendation remains challenging due to (i) representation misalignment, where domain gaps between item data and general pre-training lead to unaligned embedding spaces, and (ii) gradient conflicts during fine-tuning, where shared adapters cause interference and a lack of discriminative power. To address this, we propose SDA, a lightweight framework for Structural and Disentangled Adaptation, which integrates two components: Cross-Modal Structural Alignment (CMSA) and Modality-Disentangled Adaptation. CMSA aligns embeddings using intra-modal structures as a soft teacher, while MoDA mitigates gradient conflicts via expertized, gated low-rank paths to disentangle gradient flows. Experiments on three public Amazon datasets show SDA integrates seamlessly with existing multimodal and sequential recommenders, yielding average gains of 6.15% in Hit@10 and 8.64% in NDCG@10. It also achieves up to 12.83% and 18.70% gains on long-tail items with minimal inference overhead. Our code and full experimental results are available at https://github.com/RaoZhongtao/SDA.

</details>


### [40] [Benchmarking Deep Neural Networks for Modern Recommendation Systems](https://arxiv.org/abs/2512.07000)
*Abderaouf Bahi,Ibtissem Gasmi*

Main category: cs.IR

TL;DR: 论文评估了七种神经网络架构在三个推荐系统数据集上的表现，发现GNN擅长处理电商复杂关系，RNN适合Netflix等时序动态，Siamese网络能提升推荐多样性，建议采用混合方法平衡准确性与多样性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过系统评估不同神经网络架构在推荐系统中的表现，为推荐系统的发展提供指导，解决现有模型在准确性、多样性、计算效率等方面的挑战，满足现代数字平台不断变化的需求。

Method: 在三个不同数据集（零售电商、亚马逊产品、Netflix Prize）上部署七种神经网络架构：CNN、RNN、GNN、自编码器、Transformer、NCF和Siamese网络，使用准确率、召回率、F1分数和推荐多样性等指标进行综合评估。

Result: GNN在电商环境中处理复杂商品关系表现最佳，RNN在Netflix等需要捕捉时序动态的平台中效果显著，Siamese网络在零售环境中对提升推荐多样性贡献突出。所有模型都面临计算需求大、依赖大量数据以及平衡准确性与多样性的挑战。

Conclusion: 建议采用混合方法，结合不同模型的优势来构建更强大的推荐系统，以更好地满足用户偏好并适应现代数字平台的演进需求。需要进一步研究解决计算效率、数据依赖和准确性与多样性的平衡问题。

Abstract: This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.

</details>


### [41] [MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling](https://arxiv.org/abs/2512.07216)
*Bin Wu,Feifan Yang,Zhangming Chan,Yu-Ran Gu,Jiawei Feng,Chao Yi,Xiang-Rong Sheng,Han Zhu,Jian Xu,Mang Ye,Bo Zheng*

Main category: cs.IR

TL;DR: MUSE是一个用于淘宝展示广告系统的多模态搜索框架，通过两阶段建模（GSU粗召回和ESU精排序）有效利用多模态信号，实现了10万长度用户行为序列建模，显著提升指标且在线延迟可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有终身用户兴趣建模方法主要依赖ID特征，在长尾物品上泛化能力差且语义表达能力有限。虽然最近工作探索了在GSU阶段使用多模态表示进行行为检索，但往往忽略了在ESU精细建模阶段的多模态整合。

Method: 提出MUSE框架，采用两阶段方法：在GSU阶段使用轻量级余弦相似度与高质量多模态嵌入进行检索；在ESU阶段进行丰富的多模态序列建模和有效的ID-多模态融合。框架支持10万长度用户行为序列建模。

Result: MUSE已在淘宝展示广告系统部署，显著提升了核心指标，在线延迟开销可忽略。同时开源了首个包含超长行为序列和高质量多模态嵌入的大规模数据集。

Conclusion: 简单性在GSU阶段足够，而ESU阶段需要丰富的多模态序列建模和融合。MUSE框架有效解决了多模态信号在两阶段终身建模中的整合问题，为社区研究提供了工业部署实践和开源数据集。

Abstract: Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.

</details>


### [42] [On the Impact of Graph Neural Networks in Recommender Systems: A Topological Perspective](https://arxiv.org/abs/2512.07384)
*Daniele Malitesta,Claudio Pomo,Vito Walter Anelli,Alberto Carlo Maria Mancino,Alejandro Bellogín,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 该论文提出从拓扑学角度理解基于图神经网络(GNN)的推荐系统，认为GNN性能优势源于对用户-物品图结构特性的编码能力，并建立了数据集特征与模型行为的解释框架。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN在推荐系统中表现出优于传统协同过滤方法的性能，但其系统性优势的原因尚未被充分理解。论文旨在从拓扑学角度解释GNN推荐模型的成功机制。

Method: 1) 提出拓扑中心视角分析GNN推荐；2) 建立形式化分类法，将11种代表性GNN推荐方法统一为概念管道；3) 形式化13个经典和拓扑数据集特征；4) 分析GNN架构如何编码这些特征；5) 建立数据集特征与模型行为的解释框架。

Result: 构建了连接可测量数据集特征与模型行为/性能的解释框架，重新通过拓扑学基础理解GNN推荐，并为下一代拓扑感知推荐系统指明了理论、数据中心和评估方面的挑战。

Conclusion: GNN推荐的优势源于其对用户-物品图结构特性的编码能力，拓扑学视角为理解这些模型提供了系统框架，并为未来研究指明了方向。

Abstract: In recommender systems, user-item interactions can be modeled as a bipartite graph, where user and item nodes are connected by undirected edges. This graph-based view has motivated the rapid adoption of graph neural networks (GNNs), which often outperform collaborative filtering (CF) methods such as latent factor models, deep neural networks, and generative strategies. Yet, despite their empirical success, the reasons why GNNs offer systematic advantages over other CF approaches remain only partially understood. This monograph advances a topology-centered perspective on GNN-based recommendation. We argue that a comprehensive understanding of these models' performance should consider the structural properties of user-item graphs and their interaction with GNN architectural design. To support this view, we introduce a formal taxonomy that distills common modeling patterns across eleven representative GNN-based recommendation approaches and consolidates them into a unified conceptual pipeline. We further formalize thirteen classical and topological characteristics of recommendation datasets and reinterpret them through the lens of graph machine learning. Using these definitions, we analyze the considered GNN-based recommender architectures to assess how and to what extent they encode such properties. Building on this analysis, we derive an explanatory framework that links measurable dataset characteristics to model behavior and performance. Taken together, this monograph re-frames GNN-based recommendation through its topological underpinnings and outlines open theoretical, data-centric, and evaluation challenges for the next generation of topology-aware recommender systems.

</details>


### [43] [OnePiece: The Great Route to Generative Recommendation -- A Case Study from Tencent Algorithm Competition](https://arxiv.org/abs/2512.07424)
*Jiangxia Cao,Shuo Yang,Zijun Wang,Qinghai Tan*

Main category: cs.IR

TL;DR: 本文探讨生成式推荐系统是否存在缩放定律，通过统一编码器-解码器框架验证了ANN和自回归两种范式都严格遵循幂律缩放定律


<details>
  <summary>Details</summary>
Motivation: 受OpenAI缩放定律启发，研究生成式推荐系统是否也存在类似的缩放定律。生成式推荐通常指检索阶段，但缺乏真实的下一个物品作为监督信号，这引发了哲学问题：没有真实的下一个物品，生成式推荐是否也存在潜在的缩放定律？

Method: 设计统一的编码器-解码器框架，同时验证两种生成式推荐技术范式：(1) ANN-based框架，使用压缩的用户嵌入在嵌入空间中检索最近邻物品；(2) 自回归框架，使用束搜索从整个空间解码物品。在同一架构下验证它们的缩放定律。

Result: 实证发现两种范式的损失都严格遵循幂律缩放定律（R²>0.9），表明生成式推荐系统确实存在缩放定律。

Conclusion: 生成式推荐系统存在缩放定律，ANN和自回归两种范式都遵循幂律关系，这为推荐系统的规模化发展提供了理论依据。

Abstract: In past years, the OpenAI's Scaling-Laws shows the amazing intelligence with the next-token prediction paradigm in neural language modeling, which pointing out a free-lunch way to enhance the model performance by scaling the model parameters. In RecSys, the retrieval stage is also follows a 'next-token prediction' paradigm, to recall the hunderds of items from the global item set, thus the generative recommendation usually refers specifically to the retrieval stage (without Tree-based methods). This raises a philosophical question: without a ground-truth next item, does the generative recommendation also holds a potential scaling law? In retrospect, the generative recommendation has two different technique paradigms: (1) ANN-based framework, utilizing the compressed user embedding to retrieve nearest other items in embedding space, e.g, Kuaiformer. (2) Auto-regressive-based framework, employing the beam search to decode the item from whole space, e.g, OneRec. In this paper, we devise a unified encoder-decoder framework to validate their scaling-laws at same time. Our empirical finding is that both of their losses strictly adhere to power-law Scaling Laws ($R^2$>0.9) within our unified architecture.

</details>


### [44] [From Show Programmes to Data: Designing a Workflow to Make Performing Arts Ephemera Accessible Through Language Models](https://arxiv.org/abs/2512.07452)
*Clarisse Bardiot,Pierre-Carl Langlais,Bernard Jacquemin,Jacob Hart,Antonios Lagarias,Nicolas Foucault,Aurélie Lemaître-Legargeant,Jeanne Fras*

Main category: cs.IR

TL;DR: 使用多模态大语言模型、基于本体的推理模型和Linked Art框架扩展，将剧院节目单转化为结构化数据的工作流程


<details>
  <summary>Details</summary>
Motivation: 许多文化遗产机构收藏了大量剧院节目单，但由于其复杂布局和缺乏结构化元数据，这些资源大多未被充分利用。需要一种方法将这些文档转化为可用的结构化数据。

Method: 1. 使用视觉语言模型准确解析和转录数字原生和数字化节目单（正确提取率超过98%）
2. 训练基于本体的推理模型（POntAvignon），使用强化学习结合形式和语义奖励
3. 扩展Linked Art框架，实现自动RDF三元组生成并与现有知识图谱对齐

Result: 通过阿维尼翁戏剧节语料库的案例研究，展示了大规模、本体驱动的表演艺术数据分析潜力。实现了可互操作、可解释和可持续的计算戏剧史学。

Conclusion: 该方法为剧院节目单的数字化处理提供了有效工作流程，开启了可互操作、可解释和可持续的计算戏剧史学新可能性。

Abstract: Many heritage institutions hold extensive collections of theatre programmes, which remain largely underused due to their complex layouts and lack of structured metadata. In this paper, we present a workflow for transforming such documents into structured data using a combination of multimodal large language models (LLMs), an ontology-based reasoning model, and a custom extension of the Linked Art framework. We show how vision-language models can accurately parse and transcribe born-digital and digitised programmes, achieving over 98% of correct extraction. To overcome the challenges of semantic annotation, we train a reasoning model (POntAvignon) using reinforcement learning with both formal and semantic rewards. This approach enables automated RDF triple generation and supports alignment with existing knowledge graphs. Through a case study based on the Festival d'Avignon corpus, we demonstrate the potential for large-scale, ontology-driven analysis of performing arts data. Our results open new possibilities for interoperable, explainable, and sustainable computational theatre historiography.

</details>


### [45] [Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation](https://arxiv.org/abs/2512.07650)
*Fuyuan Lyu,Zhentai Chen,Jingyan Jiang,Lingjie Li,Xing Tang,Xiuqiang He,Xue Liu*

Main category: cs.IR

TL;DR: 本文提出在推荐系统的测试阶段进行扩展（test-time scaling），通过生成多样且有意义的输出来提升性能，相比训练时参数扩展更高效。


<details>
  <summary>Details</summary>
Motivation: 受语言模型成功的启发，推荐系统也开始扩展规模，但现有方法主要在训练时扩展参数，测试时如何有效利用计算资源仍待探索。测试时扩展可以带来正交改进，且部署时能随并行服务器增加而加速。

Method: 提出两种测试时扩展方法：1）利用不同模型架构的异质性；2）在同构架构下利用模型初始化的随机性。在8个模型（包括经典和SOTA模型）和3个基准上进行了评估。

Result: 两种方法都被证明有效。在相同推理预算下，测试时扩展能超越参数扩展。测试时扩展还能随在线并行服务器增加而加速，且不影响用户端推理时间。

Conclusion: 测试时扩展是推荐系统扩展的高效方法，能带来正交性能提升，且部署灵活。代码已开源。

Abstract: Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [46] [Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits](https://arxiv.org/abs/2512.07209)
*Masato Ishii,Akio Hayakawa,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.MM

TL;DR: 提出一个联合音视频编辑的新流程，先编辑视频再同步编辑音频，通过视频到音频生成模型保持音视频一致性


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法通常只关注视觉修改，忽略了编辑后视频与原始音频的不一致问题，需要保持音视频的连贯性

Method: 采用两阶段流程：先用先进视频编辑技术生成目标视频，再用新的视频到音频生成模型编辑音频。模型基于源音频、目标视频和文本提示，引入条件音频输入和数据增强策略，并根据编辑复杂度动态调整源音频影响

Result: 实验结果表明，该方法在保持音视频对齐和内容完整性方面优于现有方法

Conclusion: 提出的联合音视频编辑流程能有效增强编辑后视频与音频的一致性，为音视频编辑提供了更完整的解决方案

Abstract: We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.

</details>
