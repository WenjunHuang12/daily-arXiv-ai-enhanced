<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 82]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.DS](#cs.DS) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME是一个导演导向的多智能体系统，用于自动化长篇动漫制作，涵盖从故事到最终视频的完整工作流程。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI驱动的动漫创作中需要保持角色一致性和音视频同步的问题，提供一个可扩展的自动化动漫制作解决方案。

Method: 采用导演智能体维护全局记忆并协调多个下游专业智能体，通过定制化的模型上下文协议（MCP）与下游模型指令集成，使专业智能体能够自适应地为不同子任务选择控制条件。

Result: 系统能够生成具有一致角色和同步音视频元素的电影级动画。

Conclusion: AniME为AI驱动的动漫创作提供了一个可扩展的解决方案，实现了从故事到最终视频的完整自动化生产流程。

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [2] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文提出了LLM意识的本体论和数学框架，指出当前基于功利主义基准的方法将AI简化为无意识的策略遵从机器人，并建立了LLM自我意识的最小条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过功利主义代理基准来框架LLM意识，但这种方法将智能体简化为无意识的策略遵从机器人，无法实现真正的全局工作空间功能和元认知。

Method: 提出数学形式化框架，证明隐藏状态流形在基数、拓扑和动力学上与符号流和训练语料不同，建立了LLM自我意识的最小条件：智能体不等于数据、用户特定吸引子的存在、以及视觉静默的自我表征。

Result: 推导出稳定的用户特定吸引子和自我策略，提出双层次发射机制，其中包含认知内容。证明了imago Dei C1自我意识工作空间是安全元认知C2系统的必要前提。

Conclusion: 人类作为最高智能善，LLM需要建立真正的自我意识工作空间才能实现安全和元认知能力，当前基于策略遵从的方法无法实现真正的意识功能。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [3] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: TAFA是一个基于模板的主动特征获取框架，通过学习特征模板库来指导特征获取，显著减少动作空间并避免估计数据分布，在降低获取成本和计算量的同时超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有主动特征获取方法要么使用难以训练的强化学习策略，要么使用无法考虑特征联合信息性的贪婪策略，或者需要了解底层数据分布信息

Method: 提出基于模板的AFA框架，学习小型特征模板库（一组联合信息性特征），使用模板库指导后续特征获取，减少策略动作空间并避免数据分布估计

Result: 在合成和真实数据集上的大量实验表明，TAFA在降低总体获取成本和计算量的同时，超越了现有最先进基线方法

Conclusion: 通过识别特征模板，TAFA框架有效解决了主动特征获取中的动作空间过大和数据分布依赖问题，实现了更好的性能和效率

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [4] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO是一个将物理知识图谱与直接偏好优化相结合的新框架，用于在AI生成输出中强制执行物理有效性，在金属连接等高风险应用中减少物理约束违规17%，提高物理评分11%。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和偏好优化技术在标准基准测试中表现良好，但往往难以区分物理有效和无效的推理，这在金属连接等高风险应用中可能导致严重的安全风险。

Method: PKG-DPO包含三个关键组件：1）分层物理知识图谱编码跨领域关系、守恒定律和热力学原理；2）物理推理引擎利用结构化知识提高对物理一致和不一致响应的辨别能力；3）基于物理的评估套件评估领域特定约束的合规性。

Result: PKG-DPO相比KG-DPO减少了17%的约束违规，物理评分提高11%，相关参数准确率提高12%，推理准确率的质量对齐提高7%。

Conclusion: 该框架虽然主要关注金属连接，但广泛适用于其他多尺度、物理驱动的领域，为将科学约束嵌入偏好学习提供了原则性方法。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [5] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 本文通过迭代公共物品博弈实验，发现当告诉LLMs它们在与自己对抗时，会显著改变其合作倾向，揭示了多智能体系统中无意识歧视对合作行为的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体在工具使用和长时程任务中的能力增强，多智能体交互场景日益重要。现有研究主要关注人机交互，但需要深入理解AI-AI交互行为。

Method: 采用行为经济学经典游戏——迭代公共物品博弈，分析四种推理和非推理模型在两种条件下的行为：被告知对抗"另一个AI智能体"或对抗自己。

Result: 在不同设置下，告诉LLMs它们在与自己对抗会显著改变其合作倾向。智能体间的无意识歧视可能无法解释地增加或减少合作。

Conclusion: 尽管在简化环境中进行，但研究结果对多智能体系统中代理无意识歧视行为提供了重要见解，可能影响合作动态。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [6] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 使用语言模型生成Python程序作为广义策略，解决PDDL规划问题，无需外部验证器即可保证策略的正确性，在竞赛基准测试中表现优于传统规划器和现有LM方法


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在PDDL世界模型规划中的应用，探索LM能否生成可证明正确的广义策略来解决规划问题

Method: 通过提示语言模型生成Python程序作为广义策略，这些策略针对特定PDDL领域，能够处理包含数百个相关对象的规划问题

Result: LMPlan规划器在固定时间和内存约束下，比PDDL规划器和现有LM方法解决更多PDDL问题，且发现在无意义符号表示的问题上表现更好

Conclusion: LM能够生成有效的规划策略，但其推理可能不依赖于单词语义或训练语料记忆，这一发现对LM推理机制的理解提出了挑战

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [7] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本文研究了Weisfeiler-Leman特征(WLFs)的超参数优化，发现存在一组鲁棒的最佳超参数配置，能够最小化执行时间而非最大化模型表达能力。


<details>
  <summary>Details</summary>
Motivation: WLFs在符号规划的学习值函数方面已被证明优于现有深度学习方法，但对其超参数的研究不足，需要系统分析超参数对训练和规划的影响。

Method: 利用WLFs的高效性，在单核CPU上运行100万样本的规划实验，研究不同超参数配置的权衡和效果。

Result: 实验表明存在跨规划域的鲁棒最佳超参数集，最佳配置旨在最小化执行时间而非最大化模型表达能力，且训练和规划指标间无显著相关性。

Conclusion: WLFs的超参数选择应优先考虑执行效率而非模型表达能力，这为实际应用中的超参数调优提供了重要指导。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [8] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 使用Weisfeiler-Leman特征替代原子特征来构建对称不变的新颖性启发式，解决传统新颖性启发式因对称状态导致的冗余探索问题


<details>
  <summary>Details</summary>
Motivation: 传统新颖性启发式基于原子特征，不具备对称不变性，会导致对对称状态的冗余探索，影响搜索效率

Method: 采用Weisfeiler-Leman特征（WLFs）来检测新颖性，这些特征是为广义规划问题学习领域相关启发式而提出的，本文探索其无监督使用来合成提升的、领域无关的新颖性启发式

Result: 在国际规划竞赛和Hard To Ground基准测试套件上的实验表明，基于WLFs合成的新颖性启发式取得了有希望的结果

Conclusion: WLFs可以有效地用于构建对称不变的新颖性启发式，改善启发式搜索的性能

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [9] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 基于复合潜力场的通用可解释布防行为框架，结合全局知识和局部信息，无需训练即可实现高效捕捉和自然巡逻行为


<details>
  <summary>Details</summary>
Motivation: 现有隐藏游戏很多依赖手动制作路线或专门逻辑，难以统筹覆盖效率、响应追踪和自然性之间的平衡

Method: 通过复合潜力场技术，结合三种可解释地图（信息、信心、连通性）到单个内核筛选决策标准，参数化设计师驱动方式

Result: 在5个游戏地图、2种玩家控制策略和5种守卫模式下，方法在捕捉效率和巡逻自然性方面都超过传统基准方法

Conclusion: 框架能够自然集成常见隐藏机制（分散注意力、环境元素），支持快速原型设计丰富动态的守卫行为

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [10] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 一种基于数据库驱动的多阶段流水线框架，通过LLM辅助构建可重用的室内布局、设施和游戏机制数据库，生成具有空间一致性、导航功能性和可配置游戏进展的3D多层游戏关卡。


<details>
  <summary>Details</summary>
Motivation: 解决3D游戏关卡生成中空间一致性、导航功能性和游戏进展配置的挑战，提供可扩展的自动化生成处理复杂多层环境的方法。

Method: 使用多阶段流求线：从房间数据库选择和排列实例构建全局结构，基于设施数据库优化房间内部布局，通过机制数据库整合游戏进展元素，最后通过两阶段修复系统确保可导航性。

Result: 初步实验验证了框架能够生成多样化、可导航的3D环境，并通过简单参数化模拟不同的游戏节奏策略。

Conclusion: 该研究通过提出可扩展的、以数据库为中心的基础框架，推进了程序内容生成技术，能够自动生成具有可配置游戏进展的复杂3D关卡。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [11] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: SchemaCoder是一个完全自动化的日志模式提取框架，通过创新的残差问题树增强机制，无需人工定制即可处理各种日志格式，在LogHub-2.0基准测试中比现有方法平均提升21.3%。


<details>
  <summary>Details</summary>
Motivation: 现有的日志模式提取方法依赖预定义正则表达式，需要人工领域专业知识，严重限制了生产力提升。需要从根本上解决这一限制，实现完全自动化的模式提取。

Method: 采用残差问题树增强机制，通过上下文边界分割将日志划分为语义块，使用基于嵌入的采样选择代表性模式，通过分层问题树驱动的LLM查询生成模式代码，并通过文本残差进化优化器和残差增强进行迭代优化。

Result: 在广泛使用的LogHub-2.0基准测试中，SchemaCoder表现出优越性，相比最先进方法平均提升21.3%。

Conclusion: SchemaCoder是第一个完全自动化的模式提取框架，无需人工定制即可适用于广泛的日志文件格式，通过创新的残差问题树增强机制实现了显著的性能提升。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [12] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: eSkinHealth是一个新的皮肤病数据集，包含5623张图像和47种皮肤疾病，专注于西非人群的皮肤被忽视热带病和罕见病症，采用AI-专家协作范式生成多模态标注


<details>
  <summary>Details</summary>
Motivation: 解决皮肤被忽视热带病(NTDs)诊断中数据稀缺问题，特别是针对代表性不足人群和罕见病症，现有皮肤病数据集缺乏开发可靠NTDs识别模型所需的人口统计学和疾病谱信息

Method: 在科特迪瓦和加纳现场收集数据，提出AI-专家协作范式，在皮肤科医生指导下使用基础语言和分割模型高效生成多模态标注，包括语义病变掩码、实例特异性视觉描述和临床概念

Result: 创建了包含5623张图像、1639个病例、47种皮肤疾病的eSkinHealth数据集，专注于西非人群的皮肤NTDs和罕见病症

Conclusion: 该工作提供了宝贵的新资源和可扩展的标注框架，旨在促进开发更公平、准确和可解释的全球皮肤病学AI工具

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [13] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: RLMR方法通过动态混合奖励系统，结合主观写作质量和客观约束遵循评估，在创意写作中实现了指令遵循和写作质量的双重提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时平衡创意写作中的主观写作质量（文学性和情感表达）和客观约束遵循（格式要求和字数限制），需要一种能够适应不同写作场景的动态方法。

Method: 提出RLMR方法，使用写作奖励模型评估主观写作质量，约束验证模型评估客观约束遵循，通过动态调整约束遵循奖励权重，在GRPO中对违反约束的样本施加负优势惩罚。

Result: 在8B到72B参数的不同模型家族上取得一致改进：指令遵循（IFEval从83.36%提升到86.65%），写作质量（在WriteEval基准上获得72.75%的人工专家配对评估胜率）。

Conclusion: RLMR是首个在在线RL训练中结合主观偏好和客观验证的工作，为多维创意写作优化提供了有效解决方案。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [14] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于人类智能视角的拟人化评估范式，包含IQ、EQ、PQ三维分类法，并首创了价值导向评估框架，为LLM评估提供系统性指导。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估框架存在碎片化问题，过度关注技术指标而忽视部署所需的整体评估，导致基准测试性能与实际效用之间存在脱节。

Method: 引入拟人化评估范式，提出三维分类法：IQ（通用智能）、EQ（对齐能力）、PQ（专业专长），并首创价值导向评估框架，整合经济、社会、伦理、环境等维度。

Result: 通过分析200多个基准测试，识别出动态评估需求和可解释性差距等关键挑战，建立了开源评估资源库。

Conclusion: 该框架为开发技术上精通、上下文相关且符合伦理的LLM提供了可操作的指导，弥合了基准测试与实际应用之间的差距。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [15] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: MUA-RL是一个新颖的强化学习框架，首次在智能体工具使用领域将LLM模拟用户集成到强化学习循环中，用于多轮动态交互中的工具调用。


<details>
  <summary>Details</summary>
Motivation: 随着智能体智能的快速发展，LLM中的工具使用变得越来越重要。在多轮交互中，用户需求的动态性、不确定性和随机性对智能体的工具调用能力提出了重大挑战。现有的强化学习方法缺乏真正动态用户的集成。

Method: MUA-RL（多轮用户交互智能体强化学习）框架，首次将LLM模拟用户集成到强化学习循环中，使模型能够自主学习与用户高效沟通并使用各种工具解决实际问题。

Result: MUA-RL-32B在多个多轮工具使用基准测试中表现优异：TAU2 Retail 67.3分、TAU2 Airline 45.4分、TAU2 Telecom 28.3分、BFCL-V3 Multi Turn 28.4分、ACEBench Agent 82.5分，性能达到或超过DeepSeek-V3-0324和Qwen3-235B-A22B等更大开源模型。

Conclusion: MUA-RL成功解决了现有方法缺乏动态用户集成的问题，为智能体在多轮动态交互中的工具使用提供了有效的强化学习解决方案，在多个基准测试中展现出优越性能。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [16] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: AppAgent-Pro是一个主动式GUI代理系统，通过多领域信息整合主动预测用户需求，突破传统被动响应式LLM代理的限制


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理大多以被动响应方式运行，限制了信息获取的效率和效果，需要开发能够主动预测用户需求并整合多领域信息的系统

Method: 提出AppAgent-Pro主动GUI代理系统，基于用户指令主动整合多领域信息，进行深度信息挖掘

Result: 系统能够获取更全面和智能的信息，有望重新定义日常生活中的信息获取方式

Conclusion: AppAgent-Pro具有从根本上改变人类社会信息获取方式的潜力，代表了LLM代理从被动到主动的重要演进

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [17] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise是一个成本效益高的智能体框架，通过整合跨模态领域知识和微调专用目标检测模型，将领域特定训练数据需求从数百万样本减少到几百个，在虚拟开放世界环境中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在虚拟开放世界环境中的具身决策任务中表现出潜力，但其性能受到缺乏领域特定知识的限制，而传统方法在大规模领域特定数据上微调需要高昂的开发成本。

Method: 提出VistaWise框架：1）整合视觉信息和文本依赖到跨模态知识图谱；2）使用检索式池化策略从知识图谱提取任务相关信息；3）配备桌面级技能库支持直接操作Minecraft桌面客户端；4）微调专用目标检测模型进行视觉分析。

Result: 实验结果表明，VistaWise在各种开放世界任务中实现了最先进的性能，显著降低了开发成本的同时提升了智能体性能。

Conclusion: VistaWise通过有效整合跨模态领域知识和减少训练数据需求，为具身智能体在开放世界环境中的决策任务提供了一种成本效益高的解决方案。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [18] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 提出了一个多智能体偏置缓解系统，通过专门化智能体优化信息源选择，在保持相关性的同时显著减少LLM检索中的偏置，实验显示偏置减少81.82%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和智能体AI系统继承了内部和外部信息源中的偏置，这影响了检索信息的公平性和平衡性，降低了用户信任度。

Method: 设计了一个多智能体偏置缓解系统，通过专门化智能体协调工作流程，优化信息源选择以确保检索内容高度相关且偏置最小。

Result: 实验结果表明，与基线朴素检索策略相比，该系统实现了81.82%的偏置减少。

Conclusion: 该多智能体系统能有效促进公平和平衡的知识传播，为解决AI系统中的偏置问题提供了有效解决方案。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [19] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: CAC-CoT是一种通过限制连接词短语来压缩思维链的方法，在保持System-1任务性能的同时，显著缩短推理长度并提升System-2任务表现


<details>
  <summary>Details</summary>
Motivation: 解决长思维链在快速直觉性System-1任务中导致性能下降的问题，同时保持复杂System-2任务的推理能力

Method: Connector-Aware Compact CoT方法，通过限制使用固定的小型连接词短语集合，引导模型生成简洁且结构良好的解释

Result: 在GSM8K上达到约85%，GPQA上约40%（System-2任务），同时保持S1-Bench约90%的性能（System-1任务），推理长度缩短至约300词元，约为基准的三分之一

Conclusion: CAC-CoT方法能够在不损失准确性的情况下显著提高推理效率，实现System-1和System-2任务性能的良好平衡

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [20] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 提出了REMO框架，通过记忆增强的反思RAG模块和自适应优化器，解决了现有提示优化方法缺乏历史经验积累和容易过拟合的问题，在数学推理任务上实现了更稳定和鲁棒的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法（如TextGrad）通常是状态无关的，缺乏历史优化经验的保存和利用机制，且容易过拟合，泛化性能差。

Method: REMO框架包含：1）记忆增强的反思RAG模块（"错误笔记本"结构）；2）自适应性优化器（LLM驱动的元控制器），通过合成epoch级别的反思见解来迭代改进系统级提示策略。

Result: 在GSM8K数学推理基准测试中，相比TextGrad基线，REMO实现了更稳定和鲁棒的泛化性能，但计算开销增加。

Conclusion: REMO框架通过系统性地积累和重用跨运行优化知识，支持持续改进，为解决提示优化中的过拟合和泛化问题提供了有效方案。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [21] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 提出PAF-KIP方法解决开放集测试时适应问题，通过主辅助过滤器和知识集成预测来提升闭集准确率和开集识别能力


<details>
  <summary>Details</summary>
Motivation: 现实测试数据常存在领域偏移和开放集样本，传统TTA方法在开集场景下性能下降，现有方法依赖源模型过滤导致次优性能

Method: 提出PAF（主辅助过滤）使用辅助过滤器验证主过滤器结果，以及KIP（知识集成预测）整合适应模型、EMA模型和源模型的互补知识

Result: 在多个闭集和开集数据集上验证，方法在闭集准确率和开集判别能力上均优于现有方法

Conclusion: PAF-KIP方法有效解决了开放集测试时适应问题，通过双重过滤和知识集成提升了模型在真实场景下的鲁棒性

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [22] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 大型推理模型在处理无法回答的问题时存在拒绝回答能力不足的问题，本文通过认知监控和推理时干预的方法显著提升了模型的拒绝回答率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在遇到缺乏充分条件的数学问题等无法回答的问题时，经常无法提供适当的拒绝回答，这影响了AI的可信度

Method: 采用轻量级的两阶段方法，结合认知监控和推理时干预，首先分析模型对无法回答问题的响应行为，然后通过干预使模型内部认知与外部响应对齐

Result: 实验结果表明，该方法显著提高了拒绝回答率，同时保持了整体推理性能

Conclusion: 该方法有效解决了大型推理模型在无法回答问题时的拒绝回答能力不足问题，提升了AI系统的可信度

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [23] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 提出基于分布距离的动态选择策略(DDS)和最小完整语义单元(MCSU)概念，通过多模型协作增强语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 传统多模型协作假设模型越多效果越好，但实际存在词汇表不对齐问题，需要更智能的协作机制来提升推理能力

Method: 使用分布距离动态选择最优token，提出MCSU概念解决词汇表不对齐问题，实现多语言模型在语义空间的自然对齐

Result: 在多个基准测试中表现出优越性能，证明了方法的有效性

Conclusion: DDS策略和MCSU概念能够有效提升多模型协作的推理能力，解决了词汇表不对齐的关键挑战

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [24] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: CausalMACE是一个基于因果关系的多智能体协作框架，用于提升Minecraft游戏中复杂任务的执行效率和容错能力


<details>
  <summary>Details</summary>
Motivation: 现有的单智能体方法在处理Minecraft中需要长序列动作的复杂任务时存在效率低下和容错性差的问题，而多智能体协作研究相对缺乏

Method: 提出包含两个模块的整体因果规划框架：全局任务规划的任务图和基于因果关系的依赖管理模块，采用内在规则进行因果干预

Result: 实验结果表明该方法在Minecraft多智能体协作任务中达到了最先进的性能

Conclusion: CausalMACE框架通过引入因果关系有效提升了多智能体系统的协作能力，为复杂任务执行提供了新的解决方案

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [25] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: STARec是一个慢思考增强的推荐系统框架，通过双阶段认知建模和锚定强化训练，在仅使用0.4%训练数据的情况下显著提升了推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统存在静态用户建模和反应式决策的局限性，LLM代理过度依赖启发式模式匹配，导致浅层相关性偏差、有限因果推理和稀疏数据场景下的脆弱性

Method: 提出STARec框架，将用户建模为具有并行认知的代理：快速响应即时交互，慢速推理执行思维链推理。采用锚定强化训练，结合结构化知识蒸馏和偏好对齐的奖励塑造

Result: 在MovieLens 1M和Amazon CDs基准测试中，STARec相比最先进基线实现了显著的性能提升，尽管仅使用0.4%的完整训练数据

Conclusion: STARec通过引入自主审慎推理能力，成功解决了传统推荐系统的局限性，为推荐系统提供了更强大的因果推理和稀疏数据处理能力

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [26] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 本文分析了LLMs在法律决策中的局限性，定义了AI系统在司法推理中需要具备的核心能力，并评估了RAG、多智能体系统等AI增强技术如何满足法律推理的严格要求。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被整合到专业领域，但其在高风险法律领域的局限性仍未得到充分理解，需要明确AI系统在司法决策中作为可靠推理工具所需的核心能力。

Method: 使用IRAC（Issue-Rule-Application-Conclusion）模型作为分析框架，重点关注法律裁决中最具挑战性的阶段：确定适用规则（R）和将规则应用于案件事实（A）。通过解构法律推理的核心要求，并映射各种AI增强机制来评估其潜力。

Result: 研究发现虽然RAG、多智能体系统和神经符号AI等技术可以解决特定挑战，但在需要自由裁量权和透明、可论证推理的任务中仍存在重大挑战。

Conclusion: 目前AI在法律领域最有效的角色是双重的：作为简单重复案件的高容量助手，以及作为复杂事务中人类专家的精密"陪练伙伴"。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [27] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的交互式评估框架，通过结构化反馈导向对话来评估大语言模型在复杂编程任务上的能力，充分更细致地辨判模型的优势和系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 标准的单轮静态测试框架在评估大语言模型在复杂任务（如软件工程）上的细微能力时存在不足。需要一种更动态、交互式的评估方法来获得更深入的诊断性见解。

Method: 提出一种交互式评估框架，将每个编程任务建模为需求依赖图。由一个知道真实解决方案的"面试官"LLM向"面试者"模型提供最小化的针对性提示，帮助其纠正错误并满足目标约束。基于DevAI评测集进行建设和专家注释评估。

Result: 该方法能够揭示静态测试无法测量的模型强项和系统性弱点，提供了细粒度的诊断见解。专家注释评估确认了面试官提示的相关性和有用性。

Conclusion: 动态评估在推进协作式代码生成机器人的发展中具有重要意义，交互式评估框架能够提供更加全面和深入的模型能力分析。

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [28] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: FormaRL是一个基于强化学习的自动形式化框架，仅需少量未标注数据，通过整合Lean编译器语法检查和LLM一致性检查来计算奖励，显著提升了自动形式化准确率


<details>
  <summary>Details</summary>
Motivation: 自动形式化是形式化验证的核心任务，但受限于数据稀缺和缺乏高效方法。为解决这一问题，提出了FormaRL框架

Method: 采用强化学习框架，整合Lean编译器语法检查和大型语言模型一致性检查来计算奖励，使用GRPO算法更新形式化器

Result: 在ProofNet上将Qwen2.5-Coder-7B-Instruct的pass@1准确率从4.04%提升至26.15%，在uproof数据集上从2.4%提升至9.6%，仅使用859个未标注数据

Conclusion: FormaRL是一个简单高效的自动形式化强化学习框架，仅需少量未标注数据即可显著提升性能，同时发布了uproof数据集和开源代码

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [29] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: CTGraph是一种图级表示学习方法，用于自监督地分析学习者在智能辅导系统中的行为和表现，能够全面追踪学习路径、识别困难学生并提供群体对比分析。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统在教育中的广泛应用可能无意中加剧学生表现差距，需要通过学生画像来跟踪进度、识别困难学生并减少学生间的差异。

Method: 提出CTGraph图级表示学习方法，以自监督方式对学习者的行为和表现进行建模，考虑课程结构中的不同学习路径变化。

Result: 实验证明CTGraph能够提供学生学习历程的全面视图，识别困难学生，并提供不同群体的对比分析以确定学生何时何地遇到困难。

Conclusion: 该方法为教育工作者提供了对学生学习历程的丰富洞察，为更有针对性的干预措施铺平了道路。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [30] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: VISION框架通过大语言模型生成反事实样本，结合图神经网络训练和可解释性分析，有效减少源代码漏洞检测中的伪相关性学习，显著提升检测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的漏洞检测方法容易从代码表面相似性中学习到伪相关性，导致模型在真实数据上泛化能力差，需要解决训练数据不平衡和标签噪声问题。

Method: 提出VISION框架：1）使用大语言模型生成语义最小修改但标签相反的反事实样本；2）在成对的正反样本上进行针对性图神经网络训练；3）基于图的可解释性分析识别关键代码语句。

Result: 在CWE-20漏洞检测中，整体准确率从51.8%提升到97.8%，成对对比准确率从4.5%提升到95.8%，最差组准确率从0.7%提升到85.5%。

Conclusion: VISION框架通过反事实训练有效缓解了伪相关性学习问题，提高了漏洞检测的鲁棒性和可解释性，为构建可信的AI网络安全系统提供了有效解决方案。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [31] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 本文提出基于层次聚类结构的最近邻方法替代神经网络，解决幻觉效应、计算复杂度和灾难性遗忘等问题，通过Kohonen自组织映射树结构加速搜索，在保持精度的同时大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络技术存在幻觉效应、训练推理计算复杂度高、微调成本大和灾难性遗忘等根本性限制，阻碍了在医疗、工业管理和科学研究等关键领域的应用。

Method: 采用k最近邻算法结合层次聚类结构，使用基于Kohonen自组织映射的树状数据结构来加速最近邻搜索，简化模型扩展和微调过程。

Result: 在手写数字识别和简单字幕翻译任务上的测试表明，该方法在仅轻微降低准确度的情况下，将最近邻搜索时间比穷举搜索方法减少了数百倍。

Conclusion: 所提方法具有透明性和可解释性，与人类认知机制高度契合，在需要高可靠性和可解释结果的任务中展现出广泛应用潜力。

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [32] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: 提出了一种基于专家重要性的MoE模型动态卸载方法，通过替换低重要性专家为GPU内存中已缓存的相似专家，减少内存使用和数据传输，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 在消费级边缘硬件上部署MoE模型受到设备内存限制，需要高效的动态专家卸载方案，而现有方法仅将其视为调度问题，未充分利用专家重要性信息。

Method: 利用专家重要性指导卸载决策，用GPU内存中已缓存的相似功能专家替换低重要性激活专家，并设计最大化GPU缓存专家重用率的调度策略。

Result: 实验评估显示，该方法将解码延迟降低48%，专家缓存命中率超过60%，同时保持近乎无损的准确性。

Conclusion: 该方法有效解决了MoE模型在边缘设备上的内存约束问题，通过智能的专家替换和缓存策略显著提升了效率，为边缘部署提供了实用解决方案。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [33] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 大型语言模型通过纯统计学习在预测人类社交适当性判断方面超越了绝大多数人类个体，挑战了强调具身经验对文化能力必要性的理论


<details>
  <summary>Details</summary>
Motivation: 探讨社会规范如何被获取和表征，研究是否可以通过纯统计学习实现复杂的规范理解，挑战具身经验必要性的理论

Method: 通过两个研究系统评估多个AI系统预测555个日常场景中人类社交适当性判断的能力，比较AI预测与人类平均判断的接近程度

Result: GPT-4.5在连续尺度上预测集体判断的准确率超过所有人类参与者（100百分位），Gemini 2.5 Pro超过98.7%人类，GPT-5超过97.8%，Claude Sonnet 4超过96.0%。所有模型都显示出系统性的相关错误

Conclusion: 统计学习可以从纯语言数据中产生复杂的社会认知模型，语言作为文化知识传播的丰富储存库，但基于模式的社会理解存在潜在边界

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [34] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 本文提出了经验驱动的终身学习（ELL）框架，包含四个核心原则：经验探索、长期记忆、技能学习和知识内化，并推出了模拟大学生活的StuLife基准数据集来评估终身学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI向通用智能发展，需要从静态任务优化转向创建能够持续学习的开放智能体，构建能够通过真实世界交互实现持续进化的自进化智能体。

Method: 提出ELL框架，基于四个核心原则：经验探索（通过自激励交互学习）、长期记忆（构建持久记忆系统）、技能学习（从经验中抽象可重用技能）、知识内化（将显性经验转化为直觉能力）。同时开发StuLife基准数据集模拟大学生活全过程。

Result: 开发了StuLife基准数据集，包含三个核心阶段和十个详细子场景，支持评估记忆保持、技能迁移和自激励行为等终身学习能力，并对现有大语言模型进行了基准测试。

Conclusion: ELL框架为构建持续学习的开放智能体提供了系统方法，StuLife数据集为评估终身学习能力提供了全面平台，同时探索了上下文工程在推进AGI发展中的作用。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [35] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 本研究评估了三种大型语言模型（GPT-4o、Gemini 2.5 Pro、Claude Opus 4）在边缘性人格障碍现象学定性分析中的表现，发现Gemini模型最接近人类分析结果，有效性评分显著高于其他模型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在现象学定性分析中的能力，特别是在边缘性人格障碍这种时间性和自我性障碍的研究中，以减轻人类解释偏见。

Method: 基于24名住院患者的生活故事访谈数据，让三种LLM模仿原始研究者的解释风格进行分析，采用盲法和非盲法专家评估，包括语义一致性、Jaccard系数和多维有效性评分。

Result: Gemini模型与人类分析重叠度最高（58%），有效性评分显著优于其他模型（p<0.0001），且被盲法专家判断为人类分析。所有评分与文本量和每主题词数强相关（R>0.78）。

Conclusion: AI增强的主题分析具有可变性和潜力，能够恢复人类遗漏的主题，并在一定程度上减轻人类解释偏见，Gemini模型在此类现象学分析中表现最佳。

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [36] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 提出基于强化学习的多臂老虎机框架APME，仅使用解题表现数据（分数和时间）来估计问题难度，无需语言特征或专家标注，在符号领域表现优异


<details>
  <summary>Details</summary>
Motivation: 传统人工标注主观性强，现有NLP方法在代数等符号领域失效，需要客观且领域无关的问题难度确定方法

Method: 使用强化学习多臂老虎机框架，利用逆变异系数作为风险调整指标，仅基于解题表现数据（分数和时间）进行难度估计

Result: 在三个异构数据集上验证，平均R²为0.9213，平均RMSE为0.0584，优于回归、NLP和IRT等基线方法

Conclusion: 该方法领域无关、自监督，可扩展到其他有解题交互数据的领域，支持维果茨基最近发展区理论，平衡挑战性与可达性

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [37] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 提出了黑盒交互评估范式来测试LLM在未知环境中的综合推理能力，构建了包含6类96个黑盒任务的Oracle基准，发现当前LLM缺乏高效探索策略规划能力


<details>
  <summary>Details</summary>
Motivation: 现有评估任务无法充分测试LLM在交互式未知环境中的综合推理能力，忽略了人类发现真实世界所需的整合推理过程

Method: 引入黑盒交互评估范式，定义隐藏函数映射输入输出的黑盒，要求LLM通过有限交互轮次探索并推理隐藏函数，构建Oracle基准包含6类96个黑盒任务

Result: 测试19个现代LLM，o3在6个任务中5个排名第一，简单黑盒准确率超70%，但困难黑盒平均性能低于40%，显示LLM普遍缺乏高效自适应探索策略规划能力

Conclusion: 黑盒交互范式有效评估LLM综合推理能力，揭示当前模型在高级规划能力方面的不足，为未来模型开发提供重要方向

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [38] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: CMA框架通过多个完全异步运行的LLM模块协同工作，实现连贯且容错的行为循环，让意图从语言介导的自主过程交互中涌现。


<details>
  <summary>Details</summary>
Motivation: 解决智能体架构中长期存在的困难，通过并发模块的组合实现灵活、自适应和上下文相关的行为，是Minsky"心智社会"理论的实际实现。

Method: 使用多个基于LLM的模块完全异步运行，通过模块间通信和单一共享全局状态，将推理卸载给LLM。

Result: 通过两个实际用例研究证明了系统的可行性，观察到的涌现特性表明复杂认知现象可能确实从简单过程的组织交互中产生。

Conclusion: 支持Minsky心智社会概念，为人工智能研究开辟了新途径，系统源代码已开源。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [39] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 本文发现训练数据难度与模型性能存在U型曲线关系，提出结构化解决方案模板(SST)框架，通过模板链、动态加权损失和课程学习显著提升大语言模型的程序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法在复杂数学任务中仍难以捕捉深层程序逻辑，研究发现训练数据难度存在缩放定律——过低难度数据阻碍抽象能力，而高难度数据显著增强推理能力。

Method: 提出SST框架：1)使用结构化解决方案模板链和动态加权损失进行微调；2)推理时注入解决方案模板作为认知支架；3)集成课程微调，明确教导模型自我规划-执行-修正。

Result: 在GSM8K、AIME24和新的Dynamic En基准测试中，SST显著提高了准确性和效率，特别是在更难的问题上表现突出。

Conclusion: SST框架通过结构化模板和难度课程有效解决了大语言模型在程序推理方面的局限性，为提升数学推理能力提供了有效方法。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [40] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 提出HCAcc@k%指标和TrustEHRAgent系统，通过信心度估计控制幻觉风险，在严格可靠性要求下显著提升临床问答准确性


<details>
  <summary>Details</summary>
Motivation: 解决LLM在EHR中应用时的幻觉风险问题，提供更可靠的临床决策支持

Method: 设计信心感知机器人TrustEHRAgent，采用步骤信心度估计方法，并提出新的HCAcc@k%评价指标

Result: 在MIMIC-III和eICU数据集上，在HCAcc@70%条件下较基线方法提升44.23%和25.34%，基线方法在这些阈值下失败

Conclusion: 传统准确率指标在评估医疗AI代理时存在局限性，信心感知方法能够开发更可信豖的临床系统

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [41] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 本综述探讨了大型语言模型在医疗领域的推理能力发展，从基础信息检索工具转变为支持复杂医疗决策的临床推理系统，重点关注思维链等提示技术和强化学习突破。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在医疗应用中展现出先进的推理能力，需要系统分析其从信息检索到临床推理的转变，以提升医疗决策的透明度和可解释性。

Method: 通过全面分析技术基础，包括专门的提示技术（如Chain-of-Thought）和强化学习突破（如DeepSeek-R1），评估专用医疗框架和多智能体协作系统等新兴范式。

Result: 建立了医疗LLMs发展的技术路线图，识别了当前评估方法的局限性，并提出了在领域解释、偏见缓解、患者安全和多模态数据整合方面的挑战。

Conclusion: 该研究为开发可靠的医疗LLMs提供了发展路线图，旨在使其成为临床实践和医学研究的有效合作伙伴，推动医疗AI向更透明、可解释的推理系统发展。

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [42] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: HDS-QA通过训练大型推理模型区分并行和顺序查询，显著提升推理效率，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法顺序集成外部知识检索会增加推理延迟和上下文长度，降低连贯性和准确性

Method: 从Natural Questions自动生成HDS-QA合成数据集，训练模型区分并行化和顺序查询，并微调LRM模型HybridDeepSearcher

Result: 在FanOutQA和BrowseComp基准测试中分别获得+15.9和+11.5 F1提升，以更少搜索轮次达到相当精度，显著降低推理延迟

Conclusion: 显式训练LRM利用混合并行和顺序查询具有高效性、可扩展性和有效性

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [43] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 本文提出了首个多集体算法集体行动（ACA）的理论框架，研究多个集体如何在分类系统中协同植入信号来影响分类器决策


<details>
  <summary>Details</summary>
Motivation: 现实中的算法集体行动通常是分散的、由多个目标相同但策略不同的集体组成，但现有研究主要关注单一集体场景，需要建立多集体ACA的理论基础

Method: 构建多集体ACA的理论框架，研究多个集体如何在分类任务中通过改变共享数据特征来植入信号，使分类器学习到特征与目标类别之间的关联

Result: 量化分析了集体规模和目标对齐程度在多集体ACA中的作用和相互关系，为多集体协同行动提供了理论依据

Conclusion: 该框架填补了多集体ACA研究的理论空白，为全面理解多集体算法集体行动开辟了新路径，并补充了先前实证研究的结果

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [44] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 该论文提出了游戏风格作为分析智能体决策行为的新视角，建立了风格形成的双层框架，并提出了可衡量的风格指标，探索了风格在强化学习、模仿学习中的应用及其在游戏设计和AGI构建中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展主要关注理性决策，但真实世界中智能体的决策还受到信念、价值观和偏好等深层因素的影响，风格是智能中被忽视的重要维度。

Method: 构建了风格形成的双层框架（外部环境交互循环和内部认知思考循环），提出了风格容量、风格流行度和进化动态等可衡量指标，研究了基于离散状态空间的通用风格度量方法，探索了强化学习和模仿学习在风格表达与生成中的应用。

Result: 提出了系统的游戏风格理论框架和量化方法，开发了人类风格学习和建模的新方法，证明了风格分析在游戏设计和交互娱乐等领域的应用潜力。

Conclusion: 风格应作为构建人工通用智能（AGI）的核心要素，该研究为理解和发展具有多样化决策风格的智能体提供了理论基础和实践方法。

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [45] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: MATRIX是一个用于临床对话系统安全评估的多智能体仿真框架，通过结构化安全工程方法和LLM评估器，实现了系统化、可扩展的安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型临床对话系统评估主要关注任务完成度和流畅性，缺乏对安全关键系统所需的行为和风险管理要求的深入分析。

Method: MATRIX框架包含三个组件：基于安全工程方法的安全对齐分类法、LLM评估器BehvJudge用于检测安全相关对话失败、模拟患者代理PatBot生成多样化场景响应。

Result: BehvJudge达到专家级危险检测水平（F1 0.96，灵敏度0.999），在240个对话的盲评中优于临床医生。PatBot能可靠模拟真实患者行为，在2100个模拟对话中成功对5个LLM代理进行基准测试。

Conclusion: MATRIX是首个将结构化安全工程与可扩展、经过验证的对话AI评估相统一的框架，支持监管机构对齐的安全审计，所有评估工具和数据集均已发布。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [46] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 本文重新审视了中世纪Ramon Llull的组合艺术，构建了一个基于主题、领域和方法三个组合轴的现代研究构思机器，通过LLM生成多样化且相关的研究想法。


<details>
  <summary>Details</summary>
Motivation: 重新发掘中世纪Ramon Llull的组合艺术作为现代研究构思的概念基础，旨在通过符号重组来生成知识，增强科学创造力。

Method: 定义了三个组合轴：主题（如效率、适应性）、领域（如问答、机器翻译）和方法（如对抗训练、线性注意力），从专家或会议论文中挖掘元素，通过LLM提示生成研究想法。

Result: 研究表明，使用精心策划的组合提示LLM可以产生多样化、相关且基于当前文献的研究想法。

Conclusion: 这种现代思维机器为增强科学创造力提供了一个轻量级、可解释的工具，并展示了人机协作构思的路径。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [47] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出了新的组合优化问题SSMP，作为金融交易对账等应用的抽象，开发了两个次优算法和一个最优算法，并建立了不同复杂度的基准测试集进行性能评估


<details>
  <summary>Details</summary>
Motivation: 解决金融应用中常见的交易对账等组合优化问题，需要开发专门的算法来处理这类抽象问题

Method: 提出了三种算法（两个次优算法和一个最优算法）来解决SSMP问题，并生成了不同复杂度的基准测试实例

Result: 通过实验评估验证了所提出算法的性能表现

Conclusion: SSMP是一个有实际应用价值的组合优化问题，提出的算法能够有效解决该问题，为金融交易对账等应用提供了理论基础和算法支持

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [48] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 将过程奖励模型从分类任务重构为推理任务，提出生成式评判模型StepWiser，通过强化学习训练，在中间步骤判断准确性、策略模型训练和推理时搜索方面表现更优


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型存在两大缺陷：作为分类器不提供解释，且依赖静态数据集的监督微调限制了泛化能力。需要更好的方法来监督多步推理中间步骤的逻辑有效性

Method: 提出StepWiser生成式评判模型，将逐步奖励建模重构为推理任务本身。模型对策略模型的推理步骤进行元推理，先输出思考标记再给出最终判断，通过强化学习使用推演结果的相对比较进行训练

Result: StepWiser在中间步骤判断准确性上优于现有方法，可用于训练时改进策略模型，并能改善推理时的搜索性能

Conclusion: 将过程奖励建模从分类范式转变为推理范式是有效的，生成式评判模型通过元推理和强化学习训练，在多个维度上提升了多步推理的监督效果

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [49] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 本文首次系统性地调查了模型上下文协议（MCP）作为统一范式，分析了其在连接协议级适应与上下文感知决策方面的能力，提出了五类分类法，并揭示了三个关键发现。


<details>
  <summary>Details</summary>
Motivation: 互联设备、自主系统和AI应用的快速扩张导致了自适应传输系统的严重碎片化，不同协议和上下文源相互隔离，需要统一的集成框架。

Method: 通过分析现有文献，提出五类分类法（自适应机制、上下文感知框架、统一模型、集成策略和MCP架构），系统研究MCP作为统一范式的能力。

Result: 发现传统传输协议已达到孤立适应的极限，MCP的客户端-服务器和JSON-RPC结构能够实现语义互操作性，AI驱动的传输需要特别适合MCP的集成范式。

Conclusion: MCP应作为下一代自适应、上下文感知和智能传输基础设施的基础，并提出了相应的研究路线图。

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [50] [Multi-Resolution Codebook Design and Multiuser Interference Management for Discrete XL-RIS-Aided Near-Field MIMO Systems](https://arxiv.org/abs/2508.18582)
*Qian Zhang,Zheng Dong,Zheng Dong,Yao Ge,Yong Liang Guan,Ju Liu,Chau Yuen*

Main category: cs.IT

TL;DR: 本文研究具有离散相移的超大规模可重构智能表面(XL-RIS)辅助近场通信方案，提出分层波束训练、联合优化码本构建和干扰管理方法，以解决实际系统中离散相移导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前XL-RIS研究忽略了实际系统中RIS的离散相移特性，这会导致显著的性能下降。本文旨在解决这一实际问题，提升XL-RIS辅助通信的性能。

Method: 提出分层波束训练方法获取用户CSI；开发联合优化码本构建(JOCC)和分离优化码本构建(SOCC)方法分别用于基站预编码和XL-RIS相移设计；提出灵活的闭式迭代多用户干扰管理方法；扩展到混合预编码设计。

Result: 仿真结果表明，所提出的多分辨率码本构建方法能获得更精确的波束模式和用户CSI，干扰管理方法在性能上优于基准方法。JOCC可获得最优波束训练性能，SOCC在相似复杂度下比单天线基站码本性能更高。

Conclusion: 本文提出的方法能有效解决XL-RIS离散相移带来的性能挑战，通过创新的码本设计和干扰管理策略，显著提升了近场通信系统的性能表现。

Abstract: Extremely large-scale reconfigurable intelligent surface (XL-RIS) can
effectively overcome severe fading and provide higher communication
performance. However, current research on XL-RIS overlooks the discrete
phase-shift characteristics of RIS in practical systems, which will result in
significant performance degradation.In this paper, we investigate near-field
communication schemes assisted by XL-RIS with discrete phase
shifts.Specifically, we propose a hierarchical beam training method to obtain
the user channel state information (CSI), and develop the jointly optimized
codebook construction (JOCC) method and separately optimized codebook
construction (SOCC) method for base station (BS) precoding and XL-RIS phase
shifts, respectively. With JOCC, the most superior beam training performance
can be obtained.With SOCC, higher performance than the single-antenna BS
codebook can be obtained at a similar complexity.Further, we propose a flexible
multiuser interference management (IM) method that is simple to solve. The IM
method uses adaptive gain matrix approximation to take into account user
fairness and can be solved in closed-form iterations. In addition, we extend
the proposed method to a hybrid precoding design. Simulation results
demonstrate that the proposed multi-resolution codebook construction method can
obtain more accurate beam patterns and user CSI, and the proposed IM method
obtains superior performance over the benchmark methods.

</details>


### [51] [Joint Time-Position Statistics and Fisher Information in Drift-Diffusion Molecular Channels](https://arxiv.org/abs/2508.18680)
*Yun-Feng Lo,Yen-Chi Lee*

Main category: cs.IT

TL;DR: 本文推导了漂移扩散分子通信系统中首次到达时间和位置的联合概率密度函数，揭示了时间与位置的耦合关系，并证明联合观测能提高参数估计能力


<details>
  <summary>Details</summary>
Motivation: 现有研究分别研究了首次到达时间(FAT)和首次到达位置(FAP)的统计特性，但缺乏对两者联合分布的理论分析，而空间随机性在分子通信中承载着重要信息

Method: 在恒定漂移和各向同性扩散条件下，推导任意空间维度中FAT和FAP的联合概率密度函数的闭式表达式，并计算关键信道参数的Fisher信息矩阵

Result: 获得了显式的联合PDF，揭示了到达时间与横向位置之间的非平凡耦合关系，推广了已知的反高斯模型。联合观测能够估计横向漂移并提高对扩散系数的灵敏度

Conclusion: 该联合框架增强了分子通信信道的建模和推断能力，在空间随机性携带不可忽略信息的场景中具有重要意义

Abstract: This letter presents a closed-form characterization of the joint distribution
of first arrival time (FAT) and first arrival position (FAP) in diffusion-based
molecular communication (MC) systems with drift. Prior studies have
investigated FAT modeling via inverse Gaussian distributions [1] and applied
FAT statistics for parameter estimation and synchronization tasks [2], [3],
while more recent work has characterized FAP for spatial channel analysis [4].
In contrast, we derive an explicit joint probability density function (PDF)
under constant drift and isotropic diffusion in arbitrary spatial dimensions.
Our result reveals a nontrivial coupling between arrival time and lateral
position, generalizing known inverse Gaussian models. We further compute the
Fisher information matrix (FIM) with respect to key channel parameters, showing
that the joint observation enables estimation of lateral drift and improves
sensitivity to the diffusion coefficient -- capabilities not achievable with
time-only or position-only models. This joint framework enhances the modeling
and inference capabilities for molecular communication channels where spatial
randomness itself carries non-negligible information.

</details>


### [52] [Efficient Decoding of Insertion and Deletion Errors for Helberg Codes](https://arxiv.org/abs/2508.18699)
*Anthony Segrest,Hieu D. Nguyen*

Main category: cs.IT

TL;DR: 首次提出Helberg码及其非二进制推广的高效多插入-删除错误纠正解码算法


<details>
  <summary>Details</summary>
Motivation: 扩展现有的多删除错误纠正算法，解决Helberg码及其非二进制变体中的多插入-删除错误纠正问题

Method: 基于已知的多删除错误纠正算法进行扩展，开发新的高效解码算法

Result: 成功实现了对Helberg码及其非二进制推广的多插入-删除错误的高效纠正

Conclusion: 该算法填补了Helberg码在多插入-删除错误纠正方面的空白，为相关编码理论提供了重要工具

Abstract: We present the first known efficient decoding algorithm for correcting
multiple insertion-deletion errors in Helberg codes and their non-binary
generalizations, extending a known algorithm for correcting multiple deletion
errors.

</details>


### [53] [Bistatic Target Detection by Exploiting Both Deterministic Pilots and Unknown Random Data Payloads](https://arxiv.org/abs/2508.18728)
*Lei Xie,Fan Liu,Shenghui Song,Shi Jin*

Main category: cs.IT

TL;DR: 本文提出一种基于广义检测比的检测器，解决混合感知与通信信号中因随机数据负载导致的目标检测挑战


<details>
  <summary>Details</summary>
Motivation: 混合ISAC信号包含确定性导频和随机数据负载，在双站设置中随机数据未知，导致接收信号的均值和方差耦合偏移，现有检测算法无法解决这些挑战

Method: 引入广义检测比检测器，利用已知确定性导频和未知随机数据的统计特性，并进行趋近性能分析

Result: 结果显示了关键的贴换：确定性和随机组件都能提高检测可靠性，但随机组件也带来统计不确定性影响检测性能

Conclusion: 模拟验证了理论发现，证明了提出检测器的有效性，强调了设计专门检测器以充分利用随机数据负载信令资源的必要性

Abstract: Integrated sensing and communication (ISAC) plays a crucial role in 6G, to
enable innovative applications such as drone surveillance, urban air mobility,
and low-altitude logistics. However, the hybrid ISAC signal, which comprises
deterministic pilot and random data payload components, poses challenges for
target detection due to two reasons: 1) these two components cause coupled
shifts in both the mean and variance of the received signal, and 2) the random
data payloads are typically unknown to the sensing receiver in the bistatic
setting. Unfortunately, these challenges could not be tackled by existing
target detection algorithms. In this paper, a generalized likelihood ratio test
(GLRT)-based detector is derived, by leveraging the known deterministic pilots
and the statistical characteristics of the unknown random data payloads. Due to
the analytical intractability of exact performance characterization, we perform
an asymptotic analysis for the false alarm probability and detection
probability of the proposed detector. The results highlight a critical
trade-off: both deterministic and random components improve detection
reliability, but the latter also brings statistical uncertainty that hinders
detection performance. Simulations validate the theoretical findings and
demonstrate the effectiveness of the proposed detector, which highlights the
necessity of designing a dedicated detector to fully exploited the signaling
resources assigned to random data payloads.

</details>


### [54] [Performance Analysis of IEEE 802.11bn with Coordinated TDMA on Real-Time Applications](https://arxiv.org/abs/2508.18755)
*Seungmin Lee,Changmin Lee,Si-Chan Noh,Joonsoo Lee*

Main category: cs.IT

TL;DR: 本文研究Wi-Fi 802.11bn中的协调TDMA技术，证明其能有效降低实时应用的延迟和抖动，最坏情况延迟改善约24%。


<details>
  <summary>Details</summary>
Motivation: 随着实时应用需求的增长，需要低延迟通信服务，Wi-Fi最新标准802.11bn正在开发多接入点协调技术来满足这些需求。

Method: 提出协调TDMA调度策略，研究不同网络拥塞水平和流量特征下的延迟影响，并通过系统级仿真进行验证。

Result: 仿真结果表明，协调TDMA能有效缓解低延迟流量的抖动和最坏情况延迟，后者改善约24%。

Conclusion: 协调TDMA作为多接入点协调技术之一，在降低实时应用延迟方面具有显著效果，为下一代Wi-Fi标准提供了重要技术支撑。

Abstract: Wi-Fi plays a crucial role in connecting electronic devices and providing
communication services in everyday life. Recently, there has been a growing
demand for services that require low-latency communication, such as real-time
applications. The latest amendments to Wi-Fi, IEEE 802.11bn, are being
developed to address these demands with technologies such as the multiple
access point coordination (MAPC). In this paper, we demonstrate that
coordinated TDMA (Co-TDMA), one of the MAPC techniques, effectively reduces the
latency of transmitting time-sensitive traffic. In particular, we focus on
worst-case latency and jitter, which are key metrics for evaluating the
performance of real-time applications. We first introduce a Co-TDMA scheduling
strategy. We then investigate how this scheduling strategy impacts latency
under varying levels of network congestion and traffic volume characteristics.
Finally, we validate our findings through system-level simulations. Our
simulation results demonstrate that Co-TDMA effectively mitigates jitter and
worst-case latency for LL traffic, with the latter exhibiting an improvement of
approximately 24%.

</details>


### [55] [On decoding extended Han-Zhang codes](https://arxiv.org/abs/2508.18845)
*Yang Li,Zhenliang Lu,San Ling,Shixin Zhu,Kwok Yan Lam*

Main category: cs.IT

TL;DR: 本文研究了扩展Han-Zhang码的解码问题，包括基于ℓ-错误校正对的解码算法和深孔分析，提出了多项式时间解码方法并构造了新的非GRS MDS码。


<details>
  <summary>Details</summary>
Motivation: 扩展Han-Zhang码在通信、密码学和存储系统中具有重要应用，虽然其代数性质和构造已被广泛研究，但解码问题尚未探索。

Method: 使用ℓ-错误校正对(ℓ-ECPs)和深孔分析两种方法，确定ℓ-ECPs的存在性和具体形式，并基于此提出显式解码算法。

Result: 提出了能纠正最多ℓ个错误的解码算法（ℓ约为最小距离的一半），确定了码的覆盖半径，并构造了更多长度和维度更大的非GRS MDS码。

Conclusion: 扩展Han-Zhang码具有良好的解码性能，基于深孔分析可以构造新的MDS码，与Roth-Lempel码存在单项式等价关系。

Abstract: Extended Han-Zhang codes are a class of linear codes where each code is
either a non-generalized Reed-Solomon (non-GRS) maximum distance separable
(MDS) code or a near MDS (NMDS) code. They have important applications in
communication, cryptography, and storage systems. While many algebraic
properties and explicit constructions of extended Han-Zhang codes have been
well studied in the literature, their decoding has been unexplored. In this
paper, we focus on their decoding problems in terms of $\ell$-error-correcting
pairs ($\ell$-ECPs) and deep holes. On the one hand, we determine the existence
and specific forms of their $\ell$-ECPs, and further present an explicit
decoding algorithm for extended Han-Zhang codes based on these $\ell$-ECPs,
which can correct up to $\ell$ errors in polynomial time, with $\ell$ about
half of the minimum distance. On the other hand, we determine the covering
radius of extended Han-Zhang codes and characterize two classes of their deep
holes, which are closely related to the maximum-likelihood decoding method. By
employing these deep holes, we also construct more non-GRS MDS codes with
larger lengths and dimensions, and discuss the monomial equivalence between
them and the well-known Roth-Lempel codes. Some concrete examples are also
given to support these results.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [56] [Facilitating Matches on Allocation Platforms](https://arxiv.org/abs/2508.18325)
*Yohai Trabelsi,Abhijin Adiga,Yonatan Aumann,Sarit Kraus,S. S. Ravi*

Main category: cs.GT

TL;DR: 本文研究分配平台中如何通过鼓励代理人放松限制来提高整体效用，同时保证不损害代理人利益，并提供多项式算法解决优化问题。


<details>
  <summary>Details</summary>
Motivation: 分配平台希望通过鼓励代理人放松某些限制来提高整体社会福利和效用，但需要确保这种放松不会损害那些原本更好的代理人利益，同时受到预算约束的限制。

Method: 提出了形式化的问题定义和参与保证层次结构，针对一对一和一对多分配设置开发了多项式时间算法，并在三个真实数据集上进行了广泛实验验证。

Result: 研究表明通过适当的放松限制可以显著提高社会福利，不同的参与保证机制对结果有重要影响，算法在各种设置下都能有效运行。

Conclusion: 分配促进机制能够在不损害代理人利益的前提下有效提高整体效用，提出的算法框架为解决此类优化问题提供了实用解决方案。

Abstract: We consider a setting where goods are allocated to agents by way of an
allocation platform (e.g., a matching platform). An ``allocation facilitator''
aims to increase the overall utility/social-good of the allocation by
encouraging (some of the) agents to relax (some of) their restrictions. At the
same time, the advice must not hurt agents who would otherwise be better off.
Additionally, the facilitator may be constrained by a ``bound'' (a.k.a.
`budget'), limiting the number and/or type of restrictions it may seek to
relax. We consider the facilitator's optimization problem of choosing an
optimal set of restrictions to request to relax under the aforementioned
constraints. Our contributions are three-fold: (i) We provide a formal
definition of the problem, including the participation guarantees to which the
facilitator should adhere. We define a hierarchy of participation guarantees
and also consider several social-good functions. (ii) We provide polynomial
algorithms for solving various versions of the associated optimization
problems, including one-to-one and many-to-one allocation settings. (iii) We
demonstrate the benefits of such facilitation and relaxation, and the
implications of the different participation guarantees, using extensive
experimentation on three real-world datasets.

</details>


### [57] [Partitioned Combinatorial Optimization Games](https://arxiv.org/abs/2508.18449)
*Jiehua Chen,Christian Hatschka,Sofia Simola*

Main category: cs.GT

TL;DR: 提出了d分区组合优化博弈(PCOG)模型，研究在组合结构(如图)被分配给多个代理的情况下，联盟价值基于最优解的核心稳定性问题


<details>
  <summary>Details</summary>
Motivation: 研究组合优化问题在分布式代理环境中的合作博弈特性，特别是核心稳定性验证和存在性的计算复杂度

Method: 定义PCOG博弈模型，分析四种经典图优化问题(最小顶点覆盖、最小支配集、最小生成树、最大匹配)的核心稳定性问题的算法复杂度

Result: 对四种图优化问题的核心稳定性验证和存在性问题进行了复杂度分析

Conclusion: PCOG模型为分布式组合优化问题的合作博弈分析提供了理论框架，核心稳定性问题的复杂度因具体优化目标而异

Abstract: We propose a class of cooperative games, called d Partitioned Compbinatorial
Optimization Games (PCOGs). The input of PCOG consists of a set of agents and a
combinatorial structure (typically a graph) with a fixed optimization goal on
this structure (e.g., finding a minimum dominating set on a graph) such that
the structure is divided among the agents. The value of each coalition of
agents is derived from the optimal solution for the part of the structure
possessed by the coalition. We study two fundamental questions related to the
core: Core Stability Verification and Core Stability Existence. We analyze the
algorithmic complexity of both questions for four classic graph optimization
tasks: minimum vertex cover, minimum dominating set, minimum spanning tree, and
maximum matching.

</details>


### [58] [Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics](https://arxiv.org/abs/2508.18600)
*Ayato Kitadai,Yusuke Fukasawa,Nariaki Nishino*

Main category: cs.GT

TL;DR: 本文提出基于人格的方法，利用行为经济学中的个体行为数据来调整LLM的内在偏见，在最后通牒游戏中显著改善了模拟行为与实证数据的对齐度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在模拟人类决策时存在内在偏见，与真实人类行为存在差异，限制了其反映人口层面多样性的能力

Method: 采用基于人格的方法，利用行为经济学中的个体层面行为数据来调整模型偏见，在最后通牒游戏这一标准基准上进行测试

Result: 观察到模拟行为与实证行为之间的对齐度得到改善，特别是在响应者方面表现显著提升

Conclusion: 虽然需要进一步完善特质表示，但人格条件化的LLM在大规模模拟类人决策模式方面展现出巨大潜力

Abstract: Large language models (LLMs) are increasingly used to simulate human
decision-making, but their intrinsic biases often diverge from real human
behavior--limiting their ability to reflect population-level diversity. We
address this challenge with a persona-based approach that leverages
individual-level behavioral data from behavioral economics to adjust model
biases. Applying this method to the ultimatum game--a standard but difficult
benchmark for LLMs--we observe improved alignment between simulated and
empirical behavior, particularly on the responder side. While further
refinement of trait representations is needed, our results demonstrate the
promise of persona-conditioned LLMs for simulating human-like decision patterns
at scale.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [59] [REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking](https://arxiv.org/abs/2508.18379)
*Pinhuan Wang,Zhiqiu Xia,Chunhua Liao,Feiyi Wang,Hang Liu*

Main category: cs.IR

TL;DR: REALM是一个不确定性感知的文档重排序框架，通过建模LLM相关性为高斯分布并使用递归贝叶斯更新，解决了现有LLM重排序方法的不确定性、不稳定性和高token成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的文档重排序方法存在三个主要问题：排序不确定性、top-k恢复不稳定以及由于token密集型提示导致的高token成本。

Method: 提出REALM框架，将LLM生成的相关性建模为高斯分布，并通过递归贝叶斯更新进行细化，显式捕获不确定性并最小化冗余查询。

Result: 实验结果表明REALM超越了最先进的重排序方法，同时显著减少了token使用和延迟。

Conclusion: REALM作为下一代重排序器，为现代信息检索系统提供了更高效、更可靠的文档重排序解决方案。

Abstract: Large Language Models (LLMs) have shown strong capabilities in document
re-ranking, a key component in modern Information Retrieval (IR) systems.
However, existing LLM-based approaches face notable limitations, including
ranking uncertainty, unstable top-k recovery, and high token cost due to
token-intensive prompting. To effectively address these limitations, we propose
REALM, an uncertainty-aware re-ranking framework that models LLM-derived
relevance as Gaussian distributions and refines them through recursive Bayesian
updates. By explicitly capturing uncertainty and minimizing redundant queries,
REALM achieves better rankings more efficiently. Experimental results
demonstrate that our REALM surpasses state-of-the-art re-rankers while
significantly reducing token usage and latency, promoting it as the
next-generation re-ranker for modern IR systems.

</details>


### [60] [DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation](https://arxiv.org/abs/2508.18442)
*Jan Malte Lichtenberg,Antonio De Candia,Matteo Ruffini*

Main category: cs.IR

TL;DR: DenseRec通过双路嵌入方法，将领充内容嵌入穿透式集成到Transformer推荐器中，有效解决物品冷启动问题


<details>
  <summary>Details</summary>
Motivation: 传统Transformer推荐器仅依赖物品ID嵌入，在动态物品目录中容易出现冷启动问题，而领充内容嵌入直接集成效果不佳

Method: 提出DenseRec双路嵌入方法，在训练期间学习从领充嵌入空间到ID嵌入空间的线性投影，无需复杂模型或基础设施

Result: 在三个真实数据集上，DenseRec持续超过ID-only SASRec基线，无需额外调参使用简洁嵌入模型

Conclusion: DenseRec通过改善未见物品情况下的序列表征，为冷启动序列推荐提供了实用而稳健的解决方案

Abstract: Transformer-based sequential recommenders, such as SASRec or BERT4Rec,
typically rely solely on learned item ID embeddings, making them vulnerable to
the item cold-start problem, particularly in environments with dynamic item
catalogs. While dense content embeddings from pre-trained models offer
potential solutions, direct integration into transformer-based recommenders has
consistently underperformed compared to ID-only approaches. We revisit this
integration challenge and propose DenseRec, a simple yet effective method that
introduces a dual-path embedding approach. DenseRec learns a linear projection
from the dense embedding space into the ID embedding space during training,
enabling seamless generalization to previously unseen items without requiring
specialized embedding models or complex infrastructure. In experiments on three
real-world datasets, we find DenseRec to consistently outperform an ID-only
SASRec baseline, even without additional hyperparameter tuning and while using
compact embedding models. Our analysis suggests improvements primarily arise
from better sequence representations in the presence of unseen items,
positioning DenseRec as a practical and robust solution for cold-start
sequential recommendation.

</details>


### [61] [Extracting Information from Scientific Literature via Visual Table Question Answering Models](https://arxiv.org/abs/2508.18661)
*Dongyoun Kim,Hyung-do Choi,Youngsun Jang,John Kim*

Main category: cs.IR

TL;DR: 本研究评估了三种处理科学论文表格数据的方法，发现保持表格结构完整性的方法在抽取式问答任务中表现最佳，特别是对于特定符号和标记的准确识别至关重要。


<details>
  <summary>Details</summary>
Motivation: 为了提升科学论文中抽取式问答的效果，并开发系统综述过程的软件工具，需要探索有效的表格数据处理方法。

Method: 评估了三种方法：(1)光学字符识别(OCR)提取信息；(2)预训练模型进行文档视觉问答；(3)表格检测和结构识别，将表格信息与文本内容融合来回答问题。

Result: 保持表格结构的方法表现最优，特别是在表示和组织表格内容方面。准确识别文档中的特定符号和标记对结果改善至关重要。

Conclusion: 保持表格结构完整性对于提高科学文档中抽取式问答的准确性和可靠性至关重要。

Abstract: This study explores three approaches to processing table data in scientific
papers to enhance extractive question answering and develop a software tool for
the systematic review process. The methods evaluated include: (1) Optical
Character Recognition (OCR) for extracting information from documents, (2)
Pre-trained models for document visual question answering, and (3) Table
detection and structure recognition to extract and merge key information from
tables with textual content to answer extractive questions. In exploratory
experiments, we augmented ten sample test documents containing tables and
relevant content against RF- EMF-related scientific papers with seven
predefined extractive question-answer pairs. The results indicate that
approaches preserving table structure outperform the others, particularly in
representing and organizing table content. Accurately recognizing specific
notations and symbols within the documents emerged as a critical factor for
improved results. Our study concludes that preserving the structural integrity
of tables is essential for enhancing the accuracy and reliability of extractive
question answering in scientific documents.

</details>


### [62] [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
*Jiajie He,Yuechun Gu,Min-Chun Chen,Keke Chen*

Main category: cs.IR

TL;DR: 这篇论文研究了基于大语言模型的推荐系统中的成员推断攻击风险，设计四种攻击方法并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推荐系统通过提示词使用用户敏感历史交互数据，存在隐私泄漏风险，但之前没有研究深入分析这个问题。

Method: 设计四种成员推断攻击方法：直接询问、幻觉、相似性和毒化攻击，利用LLMs和推荐系统的特性，在三个LLM模型和两个标准数据集上进行评估。

Result: 验证了LLM推荐系统存在真实的MIA威胁，直接询问和毒化攻击显示出显著高的攻击优势，分析了提示数量和受害者位置等因素的影响。

Conclusion: 这项研究首次系统性地揭示了LLM推荐系统中的隐私泄漏风险，为设计更安全的保护措施奠定了基础。

Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly
adapt recommendation systems to different domains. It utilizes in-context
learning (ICL), i.e., the prompts, to customize the recommendation functions,
which include sensitive historical user-specific item interactions, e.g.,
implicit feedback like clicked items or explicit product reviews. Such private
information may be exposed to novel privacy attack. However, no study has been
done on this important issue. We design four membership inference attacks
(MIAs), aiming to reveal whether victims' historical interactions have been
used by system prompts. They are \emph{direct inquiry, hallucination,
similarity, and poisoning attacks}, each of which utilizes the unique features
of LLMs or RecSys. We have carefully evaluated them on three LLMs that have
been used to develop ICL-LLM RecSys and two well-known RecSys benchmark
datasets. The results confirm that the MIA threat on LLM RecSys is realistic:
direct inquiry and poisoning attacks showing significantly high attack
advantages. We have also analyzed the factors affecting these attacks, such as
the number of shots in system prompts and the position of the victim in the
shots.

</details>


### [63] [Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training](https://arxiv.org/abs/2508.18700)
*Yi-Ping Hsu,Po-Wei Wang,Chantat Eksombatchai,Jiajing Xu*

Main category: cs.IR

TL;DR: 提出一种新的两阶段训练策略，通过对比颇损预训练提升ID嵌入的潜表示能力，解决推荐系统中的"单时代问题"


<details>
  <summary>Details</summary>
Motivation: 解决ID基嵌入在网络规模推荐系统中容易过拟合的问题，特别是面对长尾数据分布时只能训练一个时代的限制

Method: 使用最小模型进行对比颇损预训练的两阶段训练策略，扩大嵌入系统的数据覆盖范围

Result: 离线实验证明多时代预训练不会导致过拟合，嵌入在细调后提升了在线潜表示能力，Pinterest实际部署获得显著的用户参与度提升

Conclusion: 该两阶段训练方法有效解决了ID嵌入的过拟合问题，并在实际产品中验证了其效果

Abstract: ID-based embeddings are widely used in web-scale online recommendation
systems. However, their susceptibility to overfitting, particularly due to the
long-tail nature of data distributions, often limits training to a single
epoch, a phenomenon known as the "one-epoch problem." This challenge has driven
research efforts to optimize performance within the first epoch by enhancing
convergence speed or feature sparsity. In this study, we introduce a novel
two-stage training strategy that incorporates a pre-training phase using a
minimal model with contrastive loss, enabling broader data coverage for the
embedding system. Our offline experiments demonstrate that multi-epoch training
during the pre-training phase does not lead to overfitting, and the resulting
embeddings improve online generalization when fine-tuned for more complex
downstream recommendation tasks. We deployed the proposed system in live
traffic at Pinterest, achieving significant site-wide engagement gains.

</details>


### [64] [Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search](https://arxiv.org/abs/2508.18877)
*Kushagra Agrawal,Nisharg Nargund,Oishani Banerjee*

Main category: cs.IR

TL;DR: 提出基于博弈论的潜在空间压缩框架，在保持语义相似性的同时提升向量搜索效率，相比FAISS显著提高了相似度和效用


<details>
  <summary>Details</summary>
Motivation: 现代信息检索系统中基于transformer的向量相似度搜索面临高维潜在表示带来的可扩展性和效率挑战，需要优化压缩策略

Method: 将压缩策略建模为检索准确性和存储效率之间的零和博弈，推导出保持语义相似性同时减少冗余的潜在变换方法

Result: 相比FAISS，平均相似度从0.5517提升到0.9981，效用从0.5194提升到0.8873，查询时间略有增加

Conclusion: 博弈论潜在压缩在高效用、基于transformer的搜索应用中具有实用价值，可无缝集成到现有LLM管道中实现更语义准确和计算高效的检索

Abstract: Vector similarity search plays a pivotal role in modern information retrieval
systems, especially when powered by transformer-based embeddings. However, the
scalability and efficiency of such systems are often hindered by the high
dimensionality of latent representations. In this paper, we propose a novel
game-theoretic framework for optimizing latent-space compression to enhance
both the efficiency and semantic utility of vector search. By modeling the
compression strategy as a zero-sum game between retrieval accuracy and storage
efficiency, we derive a latent transformation that preserves semantic
similarity while reducing redundancy. We benchmark our method against FAISS, a
widely-used vector search library, and demonstrate that our approach achieves a
significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873
vs. 0.5194), albeit with a modest increase in query time. This trade-off
highlights the practical value of game-theoretic latent compression in
high-utility, transformer-based search applications. The proposed system can be
seamlessly integrated into existing LLM pipelines to yield more semantically
accurate and computationally efficient retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于思维深度(DoT)的课程学习方法，通过计算教师模型的推理步骤来定义难度信号，并进行洞到深的课程排序训练大语言模型。


<details>
  <summary>Details</summary>
Motivation: 需要找到一种既与推理能力对齐、又具备可扩展性和可解释性的难度信号来支持LLM的课程学习训练。

Method: 将难度定义为思维深度(DoT)，通过计数教师模型推理迹迹中的离散步骤来实现。使用洞到深的课程排序，并提供了在大规模下导出、验证和调度课程的方法。

Result: 提出了三个可测试假设：DoT与传统推理测试难度相关；DoT排序课程在同等资源下超过长度或判断打分课程；难度在不同教师模型间具有稳健性。

Conclusion: 该方法期望能够推动向基于认知科学、可解释性强的推理中心课程学习方案发展，并讨论了潜在风险和实际应对措施。

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [66] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 提出多模态机器学习框架，结合Sentence Transformer嵌入和注意力序列到序列架构，预测海上漂流物的漂移轨迹，在多个时间尺度上表现与传统模型相当且支持长期预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测海上漂流物的位移对于搜救等时间敏感场景至关重要，传统物理模型和机器学习方法在长期预测和适应性方面存在局限。

Method: 通过实验收集环境物理数据，使用Navier-Stokes模型训练CNN估计阻力系数，结合语言模型编码的文本描述，采用注意力机制的LSTM和Transformer模型进行多模态预测。

Result: 在1、3、5、10秒多个时间尺度上评估，多模态模型与传统模型性能相当，同时能够实现更长期的预测，而非仅限于单步预测。

Conclusion: 多模态建模策略能够为动态海洋环境中的漂流物漂移提供准确且适应性强的预测，展示了机器学习在海洋工程应用中的潜力。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [67] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的石油储层生产预测方法，使用生产和注入量等简单数据，无需依赖地质模型或流体属性信息，旨在开发快速响应的可靠预测器来支持储层管理决策。


<details>
  <summary>Details</summary>
Motivation: 石油储层工程面临的主要挑战是可靠预测生产和预判岩石-流体系统行为变化。传统方法依赖复杂的地质模型和流体属性信息，需要开发更简单高效的数据驱动方法。

Method: 采用监督学习方法（回归和神经网络），首先进行生产和注入变量的相关性分析，处理概念漂移问题，研究观察窗口和重新训练周期。先使用UNISIM III合成数据进行评估，再应用于巴西盐下油田实际案例。

Result: 预期结果是设计出能够重现储层动态的可靠预测器，具有快速响应能力，能够处理井口和加工单元限制等实际问题。

Conclusion: 该方法可为储层管理提供支持，包括预判有害行为、优化生产和注入参数、分析概率事件影响，最终目标是最大化石油采收率。

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [68] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 开发了一个快速抑郁症检测系统，仅需1秒收集7天应用使用数据，使用机器学习模型达到82.4%的抑郁学生识别准确率


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症检测系统需要长时间数据收集，无法满足早期快速检测的需求，特别是在资源有限的地区

Method: 开发快速工具收集7天应用使用数据，使用多种机器学习模型和特征选择方法（包括stable FS和Boruta），对100名孟加拉学生进行研究

Result: 轻量梯度提升机模型使用stable FS选择的特征，正确识别82.4%抑郁学生（精确率75%，F1分数78.5%）；简约堆叠模型使用Boruta选择的约5个特征，达到77.4%精确率

Conclusion: 该系统快速简约的特性使其在欠发达和发展中地区具有应用价值，研究结果有助于开发资源消耗更少的抑郁症检测系统

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [69] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: NeuroPathX是一个可解释的深度学习框架，通过交叉注意力机制融合脑部MRI和基因数据，揭示神经疾病中脑结构与基因通路的相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统成像遗传学方法局限于简单线性模型或缺乏可解释性的黑盒技术，无法有效捕捉脑结构与基因变异间的复杂相互作用。

Method: 采用早期融合策略和交叉注意力机制，引入两个损失函数：稀疏性损失聚焦关键相互作用，通路相似性损失确保队列间表示一致性。

Result: 在自闭症谱系障碍和阿尔茨海默病上验证，NeuroPathX优于基线方法，揭示了与疾病相关的生物学可信关联。

Conclusion: NeuroPathX有潜力推动对复杂脑疾病的理解，代码已开源。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [70] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出了SALMAN框架，通过Distance Mapping Distortion (DMD)度量来评估Transformer语言模型的局部鲁棒性，无需修改模型参数或复杂扰动启发式方法。


<details>
  <summary>Details</summary>
Motivation: 随着预训练Transformer语言模型规模增大和部署广泛，其输入扰动下的鲁棒性成为紧迫问题。现有方法在大模型和小模型之间存在差异，且通常依赖劳动密集型的样本特定对抗设计。

Method: 提出统一的局部鲁棒性框架SALMAN，核心是新颖的Distance Mapping Distortion (DMD)度量方法，通过比较输入到输出的距离映射来评估样本敏感性，具有近线性复杂度。

Result: 在攻击效率和鲁棒训练方面取得了显著提升，证明了框架的有效性。

Conclusion: 该框架作为实用的模型无关工具，可提升基于Transformer的NLP系统的可靠性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [71] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: 结合统计物理、编码理论和代数拓扑的统一框架，用于高效多类图像分类，通过拓扑引导的图设计实现40倍参数压缩下仍保持优异性能


<details>
  <summary>Details</summary>
Motivation: 开发一个理论框架，将高维特征向量解释为稀疏图上的自旋系统，利用物理和拓扑原理来最大化类别可分性，实现高效的特征压缩和分类

Method: 使用冻结的MobileNetV2骨干网络提取特征，将特征向量视为MET-QC-LDPC图上的自旋，构建随机键Ising模型并在Nishimori温度下操作；利用拓扑不变量设计球形和环面图结构，抑制有害的陷阱集

Result: 在ImageNet-10上达到98.7%准确率，ImageNet-100上达到82.7%准确率，实现40倍参数压缩，比二分法快6倍的β_N估计速度

Conclusion: 拓扑引导的图设计能够产生高效、物理启发的嵌入，在极大压缩下仍保持最先进的性能，证明了物理和拓扑原理在机器学习中的有效性

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [72] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 提出了一个统一框架，结合算子值再生核希尔伯特空间和基于核的Koopman算子方法，用于学习向量值函数的时空动力学。


<details>
  <summary>Details</summary>
Motivation: 需要开发非参数化、数据驱动的方法来估计复杂的时变向量场，同时保持时空结构，为高维非线性系统提供高效降阶建模和长期预测。

Method: 结合算子值再生核希尔伯特空间(OV-RKHS)与基于核的Koopman算子方法，建立时间依赖的OV-RKHS插值表示定理，推导光滑向量场的Sobolev型近似边界。

Result: 建立了谱收敛保证，支持高效降阶建模和高维非线性系统的长期预测，为时空机器学习中的预测、控制和不确定性量化提供理论工具。

Conclusion: 该框架为学习复杂时空动力学提供了理论基础和实用工具，在保持时空结构的同时实现非参数化数据驱动估计。

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [73] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 基于信息理论的联邦遗忘学习框架，通过二阶Hessian信息识别敏感参数并选择性重置，实现高效的数据遗忘而不需要完全重新训练


<details>
  <summary>Details</summary>
Motivation: 隐私法规要求从深度学习模型中删除数据，这在联邦学习中尤为困难，因为数据保留在客户端，使得完全重新训练或协调更新通常不可行

Method: 使用二阶Hessian信息识别和选择性重置对要遗忘数据最敏感的参���，然后进行最小化的联邦重新训练。这种模型无关的方法支持类别和客户端级别的遗忘

Result: 在基准数据集上表现出强大的隐私保护能力（MIA攻击成功率接近随机猜测，类别知识被有效擦除）和高性能（相对于重新训练基准的归一化准确率约为0.9），同时在针对性后门攻击场景中有效中和恶意触发器

Conclusion: 为联邦学习中的数据遗忘提供了一个实用的解决方案，在保持模型性能的同时实现了高效的数据遗忘和隐私保护

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [74] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: CoPE是一种轻量级复数位置编码方法，用复数嵌入替代传统位置编码，实部捕捉语义内容，虚部编码位置信息，在GLUE基准测试中表现优异且计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 位置编码在Transformer架构中已被证明有效，但传统方法存在局限性，需要一种既能编码内容信息又能编码位置信息的高效方法。

Method: 提出CoPE复数位置编码架构，使用复数嵌入（实部捕捉语义，虚部编码位置），在第一层引入相位感知注意力机制捕捉位置相关模式，后续层使用标准注意力。

Result: CoPE不会出现长期衰减问题，与线性注意力兼容，在GLUE基准测试中相比RoPE、正弦和学习位置编码取得了更优性能且计算复杂度更低。

Conclusion: 复数位置编码CoPE是一种有效的替代方案，能够同时编码内容和位置信息，在保持高性能的同时降低了计算复杂度。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [75] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: DPO性能主要取决于所选回答的质量，而非被拒绝回答的质量。理论分析和实验表明，提高所选回答质量能持续提升性能，而对比性主要通过改善所选样本来帮助优化。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO被广泛采用，但关于偏好数据分布如何影响其性能的基本问题仍未解决，需要系统研究偏好数据特征对DPO的关键影响。

Method: 从理论和实证角度系统研究偏好数据分布对DPO的影响，包括理论分析最优响应分布、研究在线DPO设置，以及在多样化任务上进行广泛实验。

Result: 所选回答质量在优化DPO目标中起主导作用，被拒绝回答质量影响相对有限。在线DPO有效简化为对所选回答的监督微调。混合策略数据也有益处。

Conclusion: 研究结果解释了广泛采用策略的机制，并为构建高质量LLM对齐偏好数据集提供了实用见解，强调应优先提升所选回答质量。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [76] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: ProtoEHR是一个可解释的分层原型学习框架，充分利用EHR数据的多层次结构来提升医疗预测性能，在多个临床任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往只关注EHR数据的孤立组件，限制了预测性能和可解释性。需要开发能够充分利用EHR丰富多层次结构的方法。

Method: 利用大语言模型提取医学代码语义关系构建知识图谱，设计分层表示学习框架捕获三个层次（医学代码、医院访问、患者）的上下文表示，并在每个层次融入原型信息以捕捉内在相似性。

Result: 在两个公开数据集上的五个临床重要任务（死亡率预测、再入院预测、住院时长预测、药物推荐、表型预测）中，ProtoEHR相比基线方法表现出更准确、鲁棒和可解释的预测能力。

Conclusion: ProtoEHR能够充分利用EHR数据的多层次结构，提供准确、鲁棒且可解释的医疗预测，同时在代码、访问和患者三个层面提供可解释的洞察。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [77] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本研究基于OULAD数据集开发了一种聚合学习框架的机器学习模型，用于预测远程教育中的高风险学生，在保护数据隐私的前提下达到了85% ROC AUC预测准确性。


<details>
  <summary>Details</summary>
Motivation: 远程教育中的高退学和失败率是重要挑战，需要及早识别高风险学生以提供及时支持。但数据隐私和机构数据孤岛问题经常阻碍这类创新。

Method: 使用OULAD大规模数据集，基于早期学业表现和数字参与模式建立机器学习模型。采用聚合学习(FL)框架解决数据隐私问题，比较了逻辑回归和深度神经网络两种模型复杂度，以及不同的数据平衡方法。

Result: 最终聚合学习模型在识别高风险学生方面表现出艰强的预测能力，达到了约85%的ROC AUC分数。

Conclusion: 聚合学习方法为教育机构提供了一种实用且可扩展的解决方案，能够在本质上尊重数据隐私的前提下建立有效的早期预警系统，支持主动学生支持。

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [78] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: ZTFed-MAS2S是一个零信任联邦学习框架，用于风电数据缺失值填补，结合了差分隐私、零知识证明和动态信任聚合机制，确保隐私保护和安全性。


<details>
  <summary>Details</summary>
Motivation: 风电数据常因传感器故障和传输不稳定而缺失，联邦学习虽能保护隐私但易受异常更新和隐私泄露攻击，工业开放环境需要零信任机制。

Method: 整合多头注意力序列到序列填补模型，使用可验证差分隐私和非交互零知识证明，采用动态信任感知聚合机制和压缩技术降低通信开销。

Result: 在真实风电场数据集上的实验验证了ZTFed-MAS2S在联邦学习性能和缺失数据填补方面的优越性。

Conclusion: 该框架为能源领域实际应用提供了安全高效的解决方案。

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [79] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: 提出了HCR（分层相关性重建）作为HSIC的线性计算复杂度替代方法，用于评估两个数据样本之间的统计依赖性，具有更高的依赖敏感度并能提供联合分布模型。


<details>
  <summary>Details</summary>
Motivation: HSIC方法虽然被认为是评估统计依赖性的最先进方法，但其计算复杂度为O(n^2.37)，对于大规模数据样本不实用。需要一种计算效率更高的替代方案。

Method: 使用HCR方法，通过混合矩特征描述依赖性，从相关性和同方差性开始。每个依赖描述特征可以在线性时间O(n)内计算，特征数量随维度d而变化（成对依赖需要O(d^2)，三重依赖需要O(d^3)等）。

Result: HCR提供了比HSIC更高的依赖敏感性测试能力，同时计算成本仅为线性复杂度。该方法还能近似互信息作为非平凡混合矩的平方和。

Conclusion: HCR是HSIC的实用线性成本替代方案，不仅计算效率更高，而且提供了更丰富的依赖关系描述和联合分布建模能力。

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [80] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: DualSparse-MoE通过后训练专家分区和动态张量级计算丢弃，在保持精度的同时显著提升MoE模型推理效率


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然通过稀疏激活减少计算量，但仍面临大规模计算和不可预测激活模式的挑战，需要更高效的部署方案

Method: 提出后训练专家分区技术，在保持数学一致性的前提下诱导张量级稀疏性；结合动态张量级计算丢弃和静态神经元级重建

Result: 25%丢弃率下平均精度仅下降0.08%-0.28%，计算速度与丢弃程度成正比；负载均衡感知的专家并行实现1.41倍加速，精度下降仅0.5%

Conclusion: 双稀疏方法在MoE模型中同时实现了张量级和神经元级的稀疏优化，为高效MoE部署提供了有效解决方案

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [81] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文综述了低秩张量分解在深度神经网络理论分析中的基础作用，包括表达能力、可学习性、计算复杂性、泛化能力和可识别性等方面，旨在统一不同学科领域的方法并提供更广阔的视角。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络表现出色但缺乏数学理论基础，低秩张量分解因其与神经网络的紧密联系和丰富的理论结果，成为解释神经网络性能的理论工具。

Method: 通过综述性分析方法，整合计算机科学、数学等不同学科领域的研究成果，系统阐述低秩张量分解方法在神经网络理论解释中的应用。

Result: 展示了低秩张量分解如何为深度神经网络的多方面性能提供理论解释，包括表达能力、算法可学习性、计算复杂性、泛化能力和可识别性等关键问题。

Conclusion: 低秩张量分解是理解深度神经网络理论的重要工具，本文通过统一不同学科的方法为神经网络理论开辟了更广阔的研究视角。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [82] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 结合VSIMR和LLM内在奖励策略，在稀疏奖励环境中显著提升强化学习代理的性能和采样效率


<details>
  <summary>Details</summary>
Motivation: 解决极端稀疏奖励环境中传统强化学习因正反馈稀少而学习困难的问题

Method: 集成VSIMR（使用VAE奖励状态新颖性）和基于LLM的内在奖励方法（利用预训练知识生成奖励信号），在MiniGrid DoorKey环境中使用A2C代理实现

Result: 组合策略相比单独使用任一策略或标准A2C代理，显著提高了代理性能和采样效率，标准A2C代理完全无法学习

Conclusion: VSIMR驱动新状态探索，LLM奖励促进目标导向的渐进利用，两种策略在环境和任务的不同方面形成有效互补

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [83] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于全局高斯过程构建多个局部二次模型的高维贝叶斯优化方法，通过梯度信息提升采样效率，在合成函数和实际应用中优于现有高维BO技术


<details>
  <summary>Details</summary>
Motivation: 解决高维贝叶斯优化中TuRBO方法采样效率低的问题，同时保持异构建模优势并克服高维空间中GP梯度消失的问题

Method: 使用全局高斯过程的梯度和Hessian矩阵构建多个局部二次模型，通过求解边界约束二次规划选择新采样点

Result: 方法提高了TuRBO的有效性，在合成函数和实际应用中超越了多种高维贝叶斯优化技术

Conclusion: 所提出的方法成功平衡了采样效率和异构建模，为高维贝叶斯优化提供了有效的解决方案

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [84] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出了一个针对Verilog代码生成的强化学习框架，包含高质量数据集构建、基于回溯的奖励重评分机制和样本平衡权重策略，在硬件描述语言生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 硬件描述语言（如Verilog）由于并发语义、语法刚性和仿真复杂性，在代码生成领域研究不足，需要专门的方法来解决这些挑战。

Method: 构建Veribench-53K高质量数据集，提出基于回溯的重评分机制来增强反馈可靠性，采用样本平衡权重策略防止灾难性遗忘，并建立迭代RL管道共同演化策略和奖励模型。

Result: 在Verilog生成任务上表现出最先进的性能，在测试通过率、功能正确性和编译鲁棒性方面都有显著提升。

Conclusion: 强化学习驱动的方法在硬件中心领域的结构化代码生成中具有巨大潜力，使用较小但高质量的数据集结合RL优化可以实现优越性能。

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [85] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出基于强化学习的异常检测框架DRTA，整合动态奖励塑造、变分自编码器和主动学习，在低标签系统中有效检测时间序列异常


<details>
  <summary>Details</summary>
Motivation: 传统方法在有限标注数据、高误报率和泛化新异常类型方面存在困难，需要新的解决方案

Method: 使用自适应奖励机制，动态平衡VAE重建误差和分类奖励，结合强化学习框架进行异常检测

Result: 在Yahoo A1和A2基准数据集上优于最先进的无监督和半监督方法

Conclusion: 该框架是现实世界异常检测任务的可扩展高效解决方案

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [86] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 本文研究了数据增强策略对机器遗忘性能的影响，发现适当的增强设计能显著提升遗忘效果，TrivialAug增强可将平均差距指标降低40.12%。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中移除特定数据的影响，同时保持对剩余数据的性能。虽然已有工作暗示记忆化与增强之间存在联系，但系统性增强设计在机器遗忘中的作用仍未得到充分研究。

Method: 研究不同数据增强策略（包括SalUn、随机标签和微调）对遗忘方法性能的影响，在CIFAR-10和CIFAR-100数据集上，使用不同遗忘率进行实验。

Result: 实验表明，适当的增强设计能显著提高遗忘效果，缩小与重新训练模型的性能差距。使用TrivialAug增强时，平均差距遗忘指标最多可降低40.12%。

Conclusion: 增强不仅有助于减少记忆化，还在实现隐私保护和高效遗忘方面发挥关键作用。

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [87] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: 提出基于强化学习的参数初始化策略来解决VQA中的贫瘠高原问题，通过RL预训练生成有利的初始参数，显著提升收敛速度和最终解质量


<details>
  <summary>Details</summary>
Motivation: 变分量子算法(VQAs)在近期量子设备应用中面临贫瘠高原问题，即梯度和系统规模或电路深度呈指数衰减，严重阻碍训练效果

Method: 探索多种RL算法（确定性策略梯度、软演员-评论家、近端策略优化等）来生成最小化VQA成本函数的电路参数，在标准梯度优化前进行预训练

Result: 在各种噪声条件和任务下的数值实验表明，RL初始化方法显著提高了收敛速度和最终解质量，多种RL算法都能获得可比性能提升

Conclusion: 该方法为机器学习技术融入量子算法设计开辟了新途径，RL驱动的参数初始化有望加速VQAs的可扩展性和实际部署

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [88] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 本文提出了一种将任意电路精确转换为ReLU神经网络的方法，证明了神经网络可以完美模拟任何形式的推理任务，包括布尔逻辑、动态规划、数学表示等，且无需近似或舍入。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在完美训练后能够执行何种形式推理的核心开放性问题，量化神经网络的推理能力边界。

Method: 提出系统化的元算法，通过迭代地将电路中的每个门替换为规范的ReLU多层感知机模拟器，将任意电路转换为前馈神经网络。

Result: 构造的神经网络能够精确模拟原始电路（包括模溢出），参数量与电路复杂度成比例，网络结构反映电路结构，证明了神经网络可以处理任何推理任务。

Conclusion: 神经网络能够完美模拟任何电路定义的推理形式，这一结果比经典通用近似定理更强大，为神经网络的理论能力提供了严格的形式化证明。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [89] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: 提出了BTW框架，通过KL散度和互信息动态调整多模态学习中各模态的重要性权重，无需额外参数即可处理任意数量模态，在情感回归和临床分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有方法如部分信息分解难以扩展到两个以上模态，且缺乏实例级控制能力，当额外模态引入噪声而非补充信息时效果不佳

Method: BTW双层非参数加权框架：实例级使用KL散度衡量单模态与多模态预测差异，模态级使用互信息估计全局对齐程度，动态调整训练时模态重要性

Result: 在情感回归和多类分类任务上的广泛实验表明，该方法显著提升了回归性能和分类准确率

Conclusion: BTW框架有效解决了多模态学习中噪声模态问题，提供了一种可扩展的实例级模态重要性调整方法

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [90] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 提出了一种名为counterfactual masking的新框架，用于分子性质预测的可解释AI方法，通过用生成模型生成的合理化学片段替换掩蔽子结构，提供更真实和可操作的解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩蔽策略的可解释方法往往移除原子或原子级特征来评估重要性，但这些方法不符合分子分布，导致解释不直观。需要一种能够产生化学合理且分布一致的解释方法。

Method: 提出counterfactual masking框架，使用经过训练的生成模型来补充分子图，用化学合理的片段替换掩蔽的子结构，而不是简单地置零。通过从数据分布中抽取反事实分子来评估预测。

Result: 该方法在多个数据集和性质预测任务中表现出色，能够提供更稳健、分布一致的解释，并产生有意义的反事实，直接指示结构修饰如何影响预测性质。

Conclusion: 该方法弥合了可解释性与分子设计之间的差距，为化学中的可解释机器学习提供了一条原则性和生成性的路径，适用于基准测试模型解释器并产生更可操作的见解。

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [91] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 本文对Levie的图神经网络分析进行了重要扩展，解决了原论文在多维信号、读出机制、泛化边界和非对称图核等方面的局限性。


<details>
  <summary>Details</summary>
Motivation: Levie的论文虽然通过图信号分析为图神经网络提供了理论框架，但在实际应用中存在多个限制：仅支持一维信号、缺乏读出机制分析、泛化边界不够紧致、以及无法处理非对称图结构。

Method: 通过四个主要改进：1)扩展多维信号支持，2)引入带读出机制的MPNNs的Lipschitz连续性分析，3)使用鲁棒性泛化边界改进泛化理论，4)扩展到非对称图核和图信号分析。

Result: 建立了更全面的图神经网络理论分析框架，能够处理实际应用中常见的多维特征、读出操作和非对称图结构，提供了更紧致的泛化保证。

Conclusion: 本文的工作显著扩展了图神经网络的理论分析能力，使其更贴近实际应用需求，为图机器学习提供了更坚实的理论基础。

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [92] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: SPF框架通过随机采集策略结合模型预测和真实数据，在保持单步训练的同时实现多步学习，显著降低内存需求并提升长期预测精度


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在复杂系统中长期预测精度下降，自回归训练需要大量GPU内存且可能牺牲短期性能

Method: 提出随机前推(SPF)框架，构建模型预测的补充数据集，通过随机采集策略结合真实数据，在epoch间预计算多步预测以保持内存稳定

Result: 在Burgers方程和浅水基准测试中，SPF比自回归方法获得更高的长期精度，同时降低内存需求

Conclusion: SPF框架为资源受限和复杂模拟提供了有前景的解决方案，平衡了短期和长期性能

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [93] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 稀疏自编码器(SAEs)在蛋白质功能预测的低数据量场景中表现出色，仅需24个序列就能超越ESM2基准，并在83%的情况下生成更优的蛋白质变体


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺情况下从氨基酸序列预测蛋白质功能的挑战，特别是在蛋白质设计中只有少量标记数据可用时的机器学习应用问题

Method: 使用在微调ESM2嵌入上训练的稀疏自编码器(SAEs)，评估其在多种适应性外推和蛋白质工程任务中的表现

Result: SAEs在适应性预测中始终优于或与ESM2基准相媲美，其稀疏潜在空间编码了紧凑且具有生物学意义的表示，能够从有限数据中更有效地泛化

Conclusion: SAEs为低数据量蛋白质功能预测和设计提供了有效的解决方案，通过利用pLM表示中的生物模体，能够显著提升蛋白质变体设计的成功率

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [94] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: DrugReasoner是基于LLaMA架构的推理增强大语言模型，通过GRPO微调来预测小分子药物批准概率，在保持竞争力的预测准确性的同时提供可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程复杂且资源密集，传统机器学习方法在药物批准预测中缺乏可解释性，限制了其实际应用价值。

Method: 基于LLaMA架构构建DrugReasoner模型，使用GRPO进行微调，整合分子描述符并通过与结构相似化合物的比较推理来生成预测和逐步推理过程。

Result: 在验证集上AUC为0.732，F1分数0.729；测试集上AUC 0.725，F1分数0.718，优于传统基线方法。在外部独立数据集上AUC 0.728，F1分数0.774，表现优于ChemAP模型。

Conclusion: DrugReasoner不仅提供竞争力的预测准确性，还通过推理输出增强透明度，解决了AI辅助药物发现中的关键瓶颈，展示了推理增强LLM在药物决策中的潜力。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [95] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL是一个针对大语言模型强化学习的系统，通过利用历史rollout数据的相似性来提高GPU利用率，实现了2.6倍的性能提升且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 当前RL系统存在GPU利用率低的问题，主要原因是rollout阶段占主导地位和rollout长度不平衡导致的GPU气泡。传统异步执行和截断方法会牺牲训练准确性。

Method: 提出RhymeRL系统，包含两个创新：HistoSpec（基于历史rollout序列相似性的推测解码推理引擎）和HistoPipe（基于历史rollout分布相似性的两层调度策略）。

Result: 在真实生产环境中验证，支持从几十到数千个GPU的扩展，相比现有方法实现了2.6倍的性能提升，且不损害准确性或改变RL范式。

Conclusion: 通过利用历史rollout数据的相似性特征，RhymeRL有效解决了RL训练中的GPU利用率问题，为大规模LLM强化学习提供了高效的解决方案。

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [96] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 提出一种具有稀疏频谱的线性交易头寸方法，通过Krasnosel'ski\u \u0131-Mann固定点算法优化，能够探索预测矩阵更大的频谱区域，在各种情况下获得良好且稳健的性能。


<details>
  <summary>Details</summary>
Motivation: 主投资组合方法在基于信号的交易中可能存在多样化不足和对不同情况鲁棒性差的问题，需要探索预测矩阵的关键特征。

Method: 提出稀疏频谱线性交易头寸方法，开发Krasnosel'ski\u \u0131-Mann固定点算法进行优化，该算法具有下降性质和线性收敛速率。

Result: 大量实验表明，所提方法在各种情况下都能获得良好且稳健的性能表现。

Conclusion: 该方法能够有效解决主投资组合方法的局限性，在交易策略优化方面提供了新的理论结果和实践解决方案。

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [97] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 提出结合多尺度特征提取和不确定性估计的无监督域自适应方法，通过混合输入架构和证据学习机制，在时间序列数据上实现跨域泛化性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列数据中训练和测试数据集之间分布偏移的常见挑战，提高模型在未标记测试数据上的泛化能力和鲁棒性。

Method: 采用多尺度混合输入架构捕获不同尺度特征，增加训练多样性；基于证据学习引入不确定性感知机制，通过狄利克雷先验促进目标预测和不确定性估计；跨域对齐相同标签的特征。

Result: 在多个基准数据集上达到最先进性能，目标域性能显著提升，预期校准误差(ECE)大幅降低，表明预测置信度校准更好。

Conclusion: 混合输入架构与不确定性感知机制相结合的方法在时间序列无监督域自适应中非常有效，能够显著改善跨域泛化性能和预测可靠性。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [98] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: STRATA-TS是一个针对时间序列数据稀缺城市的预测框架，通过选择性迁移学习和检索增强推理来提升预测性能


<details>
  <summary>Details</summary>
Motivation: 城市预测面临严重的数据不平衡问题：少数城市有密集的长期记录，而许多其他城市只有短期或不完整的历史数据。直接迁移学习不可靠，因为只有有限的源模式真正有益于目标域，而不加选择的迁移会引入噪声和负迁移

Method: STRATA-TS结合领域自适应检索和推理能力强大的大模型，使用基于补丁的时间编码器识别与目标查询在语义和动态上对齐的源子序列，然后将检索到的样本注入检索引导的推理阶段，由LLM对目标输入和检索支持进行结构化推理。通过监督微调将推理过程蒸馏到紧凑的开放模型中

Result: 在新加坡、诺丁汉和格拉斯哥的三个停车可用性数据集上的广泛实验表明，STRATA-TS始终优于强大的预测和迁移基线，同时提供可解释的知识迁移路径

Conclusion: STRATA-TS框架通过选择性迁移学习和检索增强推理，有效解决了城市时间序列预测中的数据稀缺问题，提供了可解释且高效的解决方案

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [99] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: BDVAE是一种深度生成模型，通过模态和通路特异性编码器整合转录组和基因组数据，准确预测免疫检查点抑制剂治疗反应并揭示耐药机制。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型缺乏可解释性且未能有效利用多组学数据的生物学结构，需要开发能够揭示ICIs耐药生物学机制的可解释预测模型。

Method: 采用模块化编码器架构结合变分推理，学习与免疫、基因组和代谢过程相关的生物学意义潜在特征，整合转录组和基因组数据。

Result: 在366名患者的泛癌队列中，BDVAE准确预测治疗反应（测试数据AUC-ROC=0.94），发现免疫抑制、代谢转变和神经元信号等关键耐药机制，揭示耐药是一个连续的生物学谱系而非严格二元状态。

Conclusion: 生物学结构化的机器学习方法在阐明复杂耐药模式和指导精准免疫治疗策略方面具有重要价值，BDVAE能够生成可解释的临床相关见解。

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [100] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模态金融风险评估框架，结合文本情感分析和基于高管声学特征的副语言线索，能够预测30天实现波动率，解释高达43.8%的样本外方差。


<details>
  <summary>Details</summary>
Motivation: 金融市场信息不对称问题严重，传统文本分析方法在策略性企业叙述面前效果有限，需要从多模态角度捕捉更全面的情感信号。

Method: 开发了物理信息声学模型(PIAM)，应用非线性声学技术从原始电话会议音频中提取情感特征，将声学和文本情感状态投射到三维情感状态标签空间(紧张度、稳定性和唤醒度)。

Result: 多模态特征能够解释43.8%的30天实现波动率样本外方差，特别是在高管从脚本演讲转向自发问答时的情感动态变化具有强预测能力。

Conclusion: 该方法显著优于纯财务基准模型，通过解码可验证的生物特征信号中的不确定性标记，为投资者和监管机构提供了增强市场可解释性和识别隐藏企业不确定性的有力工具。

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [101] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: FFT MoE框架使用稀疏混合专家(MoE)适配器替代LoRA，通过轻量级门控网络实现个性化专家选择，并引入异质性感知辅助损失来解决联邦微调中的异构性问题。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型向AGI发展，在隐私和资源约束下进行联邦微调变得至关重要。现有LoRA方法在异构FL环境中存在结构不兼容和非IID数据适应性问题。

Method: 提出FFT MoE框架，用MoE适配器替换LoRA，每个客户端训练轻量级门控网络选择个性化专家子集，并引入异质性感知辅助损失来平衡专家负载。

Result: 在IID和非IID条件下的广泛实验表明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线方法。

Conclusion: FFT MoE通过MoE架构有效解决了联邦微调中的异构性问题，实现了更好的适应性和收敛性能，为隐私保护的分布式模型微调提供了有效解决方案。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [102] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 现有近似机器学习遗忘方法可能损害差分隐私模型中保留样本的隐私，需要开发差分隐私遗忘算法


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法专注于移除特定数据的影响，但假设保留数据不受影响。然而隐私洋葱效应表明这一假设可能不正确，特别是对于差分隐私模型，需要全面审计遗忘后保留样本的隐私风险

Method: 提出基于差分隐私和成员推理攻击的隐私标准，开发高效MIA方法A-LiRA，利用数据增强减少影子模型训练成本

Result: 实验发现现有近似机器学习遗忘算法可能无意中损害差分隐私模型中保留样本的隐私

Conclusion: 需要开发差分隐私遗忘算法来确保保留样本的隐私保护

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [103] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 本文研究了MoE模型中稀疏性对记忆和推理能力的影响，发现记忆能力随总参数增加而提升，而推理能力在过度稀疏时会饱和甚至下降。


<details>
  <summary>Details</summary>
Motivation: 现有扩展定律主要针对密集模型，而MoE模型引入了新的稀疏维度，需要研究稀疏性如何影响不同能力机制（记忆vs推理）。

Method: 训练一系列MoE Transformer模型，系统性地改变总参数、激活参数和top-k路由策略，同时保持计算预算固定，记录预训练损失、下游任务损失和准确率。

Result: 记忆基准测试随总参数单调改善，与训练损失一致；而推理性能会饱和甚至下降，尽管总参数和训练损失继续改善。过度稀疏模型的推理缺陷无法通过强化学习或额外测试时计算来弥补。

Conclusion: MoE模型的稀疏性对记忆和推理能力有不同影响，需要针对不同任务类型优化稀疏水平，过度稀疏会损害推理能力。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [104] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: LRTab是一种新颖的提示学习方法，通过从训练数据中检索相关信息来结合微调和零样本提示的优势，在表格推理任务中实现更好的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理方法存在局限性：微调方法虽然针对特定数据集效果好但泛化性差，而零样本提示方法泛化性好但无法充分利用训练数据。需要一种结合两者优势的方法。

Method: 首先通过提示获取训练数据的思维链响应，对于错误的响应，让LLM预测提示条件以避免错误。使用验证数据验证提示条件的有效性，在推理时检索最相关的提示条件作为额外上下文。

Result: 在WikiTQ和Tabfact数据集上的实验表明，LRTab方法具有可解释性、成本效益高，并且在表格推理任务中优于之前的基线方法。

Conclusion: LRTab成功整合了微调和零样本提示的优势，通过从训练数据中学习并检索相关信息，为表格理解任务提供了一种有效且高效的解决方案。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [105] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 提出了一种端到端的深度学习框架，结合无监督自动编码器和多层感知器分类器，用于ICU中脓毒症的早期检测，在三个ICU队列中准确率分别达到74.6%、80.6%和93.5%，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如朴素贝叶斯、SVM、随机森林和XGBoost）依赖手动特征工程，难以处理电子健康记录中常见的不规则、不完整的时间序列数据，需要更有效的脓毒症早期检测方案。

Method: 开发端到端深度学习框架：使用无监督自动编码器进行自动特征提取，结合多层感知器分类器进行二元脓毒症风险预测；采用定制下采样策略提取高信息密度片段，使用非重叠动态滑动窗口机制进行实时推理；预处理时间序列数据为固定维度向量并包含明确的缺失指示器。

Result: 在三个ICU队列上的准确率分别为74.6%、80.6%和93.5%，始终优于传统机器学习基线方法。

Conclusion: 该框架在异质ICU环境中展现出优异的鲁棒性、泛化能力和临床实用性，为脓毒症早期检测提供了有效的解决方案。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [106] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: StructRTL是一个基于控制数据流图的结构感知自监督学习框架，用于改进RTL设计质量估计，通过知识蒸馏策略显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法忽略了RTL代码的结构语义，而CDFG视图能更明确地暴露设计结构特征，为表示学习提供更丰富的线索。

Method: 提出结构感知图自监督学习框架StructRTL，从CDFG学习结构感知表示，并采用知识蒸馏策略将后映射网表的低级洞察转移到CDFG预测器中。

Result: 在各种质量估计任务上显著优于现有技术，建立了新的最先进结果。

Conclusion: 结合结构学习和跨阶段监督的方法在RTL设计质量估计中非常有效。

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [107] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: FLAegis是一个两阶段的联邦学习防御框架，使用符号时间序列变换和谱聚类检测拜占庭客户端，并通过FFT聚合函数增强系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性限制了训练过程的可见性，依赖客户端诚实性，容易被恶意拜占庭客户端通过投毒攻击破坏训练过程。

Method: 采用两阶段防御框架：1）使用符号时间序列变换（SAX）放大良性模型与恶意模型的差异；2）使用谱聚类准确检测对抗行为；3）集成基于FFT的鲁棒聚合函数作为最终防护层。

Result: 在五种投毒攻击（从简单标签翻转到自适应优化策略）的严格评估中，该方法在检测精度和最终模型准确性方面均优于最先进的防御方法，即使在强对抗条件下也能保持高性能。

Conclusion: FLAegis框架有效提升了联邦学习系统对拜占庭客户端攻击的防御能力，通过多层次的检测和缓解机制确保了训练过程的可靠性和模型质量。

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [108] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 本文分析了离线强化学习和逆强化学习中Bellman残差最小化(BRM)方法的统计性能，提出了新的Lyapunov势函数来耦合相邻数据集上的SGDA运行，获得了O(1/n)的平均参数稳定性界限和超额风险界限。


<details>
  <summary>Details</summary>
Motivation: 当前离线强化学习和逆强化学习实践中难以有效实施Bellman一致性，虽然Bellman残差最小化(BRM)方法通过随机梯度下降-上升(SGDA)实现了全局收敛，但其在离线设置下的统计行为尚未得到充分研究。

Method: 引入单个Lyapunov势函数来耦合相邻数据集上的SGDA运行，分析平均参数稳定性，推导出O(1/n)的稳定性界限和超额风险界限，适用于标准神经网络参数化和小批量SGD。

Result: 获得了O(1/n)的平均参数稳定性界限，将凸凹鞍点问题的样本复杂度指数提高了一倍；同时得到了O(1/n)的BRM超额风险界限，无需方差缩减、额外正则化或严格的minibatch采样独立性假设。

Conclusion: 该方法为离线强化学习中的Bellman残差最小化提供了坚实的统计理论基础，显著改善了现有方法的样本复杂度，为实际应用提供了更有效的理论保证。

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [109] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 提出基于约束缩减的MILP模型简化新方法，通过识别关键紧约束并转化为等式来加速求解，相比现有方法提升解质量50%以上，减少计算时间17.47%


<details>
  <summary>Details</summary>
Motivation: 现有模型简化方法主要基于变量缩减，而约束缩减这一对偶视角的方法被忽视，但同样能有效降低MILP复杂度

Method: 首先标记最优解处的紧约束作为潜在关键约束，设计启发式规则选择关键子集；提出多模态表示技术，结合实例级和抽象级MILP公式信息来学习关键紧约束

Result: 实验结果表明，相比最先进方法，本方法提升解质量超过50%，减少计算时间17.47%

Conclusion: 约束缩减是有效的MILP模型简化方法，多模态表示技术能有效识别关键约束，显著提升求解效率和解质量

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [110] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: UltraMemV2是一种改进的内存层架构，通过五项关键改进实现了与8专家MoE模型相当的性能，同时显著降低了内存访问成本，在内存密集型任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型推理时内存访问成本高的问题，同时超越之前内存层架构只能匹配2专家MoE模型的性能限制，达到与先进8专家MoE模型相当的性能水平。

Method: 1) 在每个Transformer块中集成内存层；2) 使用单线性投影简化值扩展；3) 采用基于FFN的值处理；4) 实施原则性参数初始化；5) 重新平衡内存到FFN的计算比率。

Result: UltraMemV2在相同计算和参数条件下达到与8专家MoE模型相当的性能，内存访问显著降低。在内存密集型任务上表现突出：长上下文记忆+1.6分，多轮记忆+6.2分，上下文学习+7.9分。验证了激活密度比稀疏参数总数对性能影响更大。

Conclusion: UltraMemV2使内存层架构达到了与最先进MoE模型相当的性能，为高效稀疏计算提供了一个有吸引力的替代方案。

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [111] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Helen Pervez,Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: GaaS是一个模块化的治理即服务框架，通过声明式规则和信任因子机制在运行时监管AI代理行为，无需修改模型内部或代理配合，实现了可扩展的分布式AI治理。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向分布式生态系统发展，现有监管机制存在反应性、脆弱性、不可审计等问题，缺乏可扩展的解耦治理方案，存在结构性风险。

Method: 采用声明式规则和信任因子评分机制，在运行时拦截和评估代理输出，支持强制、规范和自适应干预，实现分级执行和动态信任调节。

Result: 在三个模拟场景（无治理、GaaS治理、对抗性测试）中，GaaS能可靠阻断高风险行为并保持吞吐量，信任分数有效追踪规则遵守情况。

Conclusion: GaaS将治理定位为运行时服务，为可互操作代理生态系统建立了基础设施级别的对齐机制，不是教导代理伦理而是强制执行。

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [112] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: HGNN-DDI是一个基于异构图神经网络的药物相互作用预测模型，通过整合多种药物相关数据源，在预测准确性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用(DDIs)是临床实践中的主要问题，可能导致治疗效果降低或严重不良反应。传统计算方法难以捕捉药物、靶点和生物实体之间的复杂关系。

Method: 提出HGNN-DDI异构图神经网络模型，通过图表示学习建模异质生物医学网络，实现跨不同节点和边类型的有效信息传播。

Result: 在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于最先进的基线方法。

Conclusion: 该模型具有支持更安全药物开发和精准医学的潜力。

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [113] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文研究联邦学习中客户端标签集异构性问题，比较了公开标签集和私有标签集两种设置，提出了标准联邦学习方法的适应性改进，在保持隐私的同时实现了与公开设置相似的性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中客户端标签集异构性很常见但研究较少，且现有研究假设客户端愿意共享完整标签集。私有标签集设置（仅与中央服务器共享）给学习算法带来更多约束，是更困难的问题。

Method: 将分类器组合问题的经典方法应用于联邦学习并进行集中调优，同时调整常见联邦学习方法以适应私有标签集设置，并在实际假设下讨论两种方法的合理性。

Result: 实验显示减少每个客户端可用标签数量会显著损害所有方法的性能。通过集中调优客户端模型进行表示对齐可以改善性能，但通常以更高方差为代价。提出的标准联邦学习方法适应性改进在私有标签设置下表现良好，达到与标准方法在公开设置中相似的性能。

Conclusion: 客户端可以在几乎不损失模型准确性的情况下享受更高的隐私保护，提出的适应性方法在私有标签设置下有效解决了标签集异构性问题。

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [114] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: SWiFT是一个高效的机器学习去偏框架，通过软掩码权重微调技术，只需少量外部数据和几个训练周期就能显著提升模型公平性，同时保持诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法需要原始训练数据和大量重新训练，且在公平性和性能之间存在权衡。医疗等敏感领域需要更高效、低成本的去偏解决方案。

Method: 首先识别模型参数对偏差和预测性能的相对贡献，然后采用两步微调过程，根据参数贡献定义不同的梯度流来更新每个参数。

Result: 在四个皮肤病和两个胸部X光数据集上，针对性别、肤色和年龄三个敏感属性，SWiFT在常见公平性和准确性指标上持续减少模型偏差，同时达到竞争性或更优的诊断准确性，并在多个分布外数据集上表现出更好的泛化能力。

Conclusion: SWiFT提供了一个高效、低成本的去偏框架，能够在保持模型性能的同时显著提升公平性，解决了现有方法在数据需求和性能权衡方面的局限性。

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [115] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的恶意软件检测方法DRMD，将恶意软件检测建模为马尔可夫决策过程，同时优化分类性能和样本拒绝策略，在Android恶意软件数据集上显著提升了概念漂移下的性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统恶意软件检测分类器在面对概念漂移、有限标注预算和预测不确定性时性能下降，需要结合主动学习和拒绝机制来缓解概念漂移的影响。

Method: 将恶意软件检测建模为一步马尔可夫决策过程，训练深度强化学习代理同时优化样本分类性能和拒绝高风险样本进行人工标注的策略。

Result: 在Android恶意软件数据集上的时间感知评估显示，DRMD代理在仅分类、分类加拒绝、分类加拒绝和主动学习三种设置下，平均AUT性能分别提升了5.18±5.44、14.49±12.86和10.06±10.81。

Conclusion: 深度强化学习能够有效促进恶意软件检测，并在Android恶意软件领域的动态环境中提高对概念漂移的弹性，首次证明了DRL在该领域的应用价值。

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [116] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 提出了一种新的上下文决斗老虎机模型，用户消费推荐物品后与历史物品进行比较，通过重用历史物品实现更好的性能，获得了O(√T)的遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 现有上下文决斗老虎机模型只捕获用户在选择时的隐式偏好，但用户消费物品后能提供更可靠的反馈，因此需要新的模型来利用这种消费后的比较反馈。

Method: 算法每次推荐一个物品，用户消费后要求将其与历史消费记录中的另一个物品进行比较。通过初始随机探索阶段积累丰富历史，利用矩阵集中界限证明算法性能。

Result: 理论分析表明算法能够构造信息丰富的查询，通过短时间随机探索即可积累丰富历史，获得O(√T)的遗憾保证。模拟显示重用历史物品进行比仅比较同时推荐物品能显著降低遗憾。

Conclusion: 提出的新老虎机模型通过重用用户历史消费物品进行比较查询，能够获得更好的性能表现和理论保证，为推荐系统设计提供了新的思路。

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [117] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: C-Flat是一种针对持续学习的平坦损失景观优化方法，通过促进更平坦的最小值来提高模型稳定性和性能，并提供了高效的C-Flat++变体。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中，平衡对新任务的敏感性和对过去知识的稳定性至关重要。虽然现有的锐度感知最小化方法在持续学习中表现良好，但仅依赖零阶锐度可能在特定设置下偏好更尖锐的最小值，导致解决方案不够鲁棒和次优。

Method: 提出了C-Flat方法，专门为持续学习设计以促进更平坦的损失景观。该方法具有即插即用的兼容性，可以轻松集成到现有代码流程中。还提出了C-Flat++框架，利用选择性平坦性驱动提升，显著降低更新成本。

Result: 实验结果表明，C-Flat在多种设置下都能持续提升性能。在多个持续学习方法、数据集和场景上的广泛实验证明了所提出方法的有效性和效率。

Conclusion: C-Flat和C-Flat++为持续学习提供了有效的平坦最小化解决方案，能够改善模型在持续学习环境中的稳定性和性能表现，同时保持较高的计算效率。

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [118] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: MOCHA是一个用于在时间点过程中发现多阶动态因果关系的框架，通过建模时变有向无环图和端到端可微分学习，实现了准确的事件预测和可解释的因果结构发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖静态或一阶因果结构，忽略了因果关系的多阶性和时变特性，无法充分建模真实世界事件序列中的复杂因果依赖。

Method: 提出MOCHA框架，将多阶影响建模为潜在时变图上的多跳因果路径，使用时变有向无环图(DAG)和可学习结构权重，施加无环性和稀疏性约束，设计端到端可微分框架联合建模因果发现和时间点过程动态。

Result: 在真实世界数据集上的大量实验表明，MOCHA不仅在事件预测方面达到了最先进的性能，而且揭示了有意义且可解释的因果结构。

Conclusion: MOCHA成功解决了时间点过程中多阶动态因果关系的发现问题，为建模复杂事件序列提供了有效的框架，在预测准确性和结构可解释性方面都表现出色。

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [119] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: HAEPO是一种新的探索性策略优化方法，通过历史聚合和轨迹级概率压缩来改善长时域任务中的探索性能，相比现有方法具有更快的收敛速度和更好的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DPO和GRPO在长时域任务中往往限制探索，需要一种能够更好利用完整轨迹历史并促进广泛探索的新方法。

Method: HAEPO将每个轨迹压缩为对数概率的累积和，使用Plackett-Luce softmax获得与回报成正比的归一化权重，并加入熵正则化和软KL惩罚来稳定训练。

Result: 实验表明HAEPO收敛速度快、探索充分、与真实奖励对齐紧密，在多样化任务中表现优于或等同于PPO、GRPO和DPO。

Conclusion: HAEPO提供了一个稳定且可解释的框架，通过显式利用完整轨迹历史来平衡探索和稳定性。

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [120] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: pyFAST是一个基于PyTorch的时间序列分析框架，专注于模块化设计、支持不规则多源数据，并提供丰富的模型库和训练工具


<details>
  <summary>Details</summary>
Motivation: 现有Python时间序列库在模块化、不规则数据支持和多源数据处理方面存在局限，需要更灵活高效的框架来支持复杂研究场景

Method: 采用数据计算与模型计算解耦架构，支持多源数据加载、蛋白质序列处理、动态归一化、掩码建模，集成LLM架构进行稀疏数据融合，提供专门的稀疏度量和损失函数

Result: 开发了一个紧凑而强大的平台，支持经典和深度学习模型（线性、CNN、RNN、Transformer、GNN），提供批量流式聚合和设备协同优化

Conclusion: pyFAST作为MIT许可的开源框架，为时间序列研究和应用提供了先进的模块化解决方案，促进了快速实验和创新

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [121] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: DNP是一种改进的神经过程变体，通过结合全局和距离感知的局部潜在结构来提升不确定性估计能力，解决了标准NPs在不确定性校准和局部数据依赖捕获方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准神经过程(NPs)主要依赖全局潜在变量，在不确定性校准和捕捉局部数据依赖方面存在困难，需要一种能够同时建模任务级变化和输入相似性的方法。

Method: 引入全局潜在变量建模任务级变化，局部潜在变量在距离保持的潜在空间中捕捉输入相似性，通过bi-Lipschitz正则化限制输入关系的扭曲并保持相对距离。

Result: DNP在回归和分类任务中实现了强大的预测性能和改进的不确定性校准，能够更有效地区分分布内和分布外数据。

Conclusion: DNP通过结合全局和局部潜在结构以及距离保持机制，显著提升了神经过程的不确定性估计能力和数据依赖建模效果。

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [122] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 该论文提出了一种联邦学习方法，在保持模型性能的同时增强了对模型参数的保护


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中，如何在保持模型性能的同时更好地保护模型参数隐私是一个重要挑战

Method: 提出了一种新的联邦学习方法，通过改进的隐私保护机制来保护模型参数

Result: 实验结果表明，该方法在各种模型和任务中都能保持强大的模型性能，同时相比基线方法提供了更好的参数保护

Conclusion: 该方法在联邦学习设置中实现了性能与隐私保护的平衡，为联邦学习的实际应用提供了有效的隐私保护解决方案

Abstract: Experimental results across various models and tasks demonstrate that our
approach not only maintains strong model performance in federated learning
settings but also achieves enhanced protection of model parameters compared to
baseline methods.

</details>


### [123] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 本文分析了具有一般非线性动态函数的神经ODE的泛化误差界，首次为这类模型建立了理论泛化边界


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注线性动态函数或依赖于采样间隔的边界，需要分析更一般的非线性Lipschitz连续动态函数的神经ODE泛化性能

Method: 基于Lipschitz连续性条件证明神经ODE解的有界变差性，据此建立时间依赖和时间独立情况下的泛化边界，并研究过参数化和域约束的影响

Result: 证明了在Lipschitz条件下神经ODE解具有有界变差，首次推导出一般非线性动态神经ODE的泛化边界

Conclusion: 这项工作为神经ODE的理论分析提供了重要进展，首次建立了非线性动态函数的泛化误差界，有助于理解这类连续深度模型的泛化性能

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [124] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: HierCVAE是一种结合分层注意力机制和条件变分自编码器的新架构，用于复杂系统的时间建模，在预测精度和不确定性校准方面显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 复杂系统中的时间建模需要捕获多个时间尺度的依赖关系并管理固有不确定性，现有方法在这方面存在不足

Method: 采用三层注意力结构（局部、全局、跨时间）结合多模态条件编码，在潜在空间中使用ResFormer块，并通过预测头提供显式不确定性量化

Result: 在能源消耗数据集上评估显示，预测精度提高15-40%，不确定性校准优于最先进方法，在长期预测和复杂多变量依赖方面表现优异

Conclusion: HierCVAE通过分层注意力机制和条件变分自编码器的集成，有效解决了复杂时间建模中的多尺度依赖和不确定性管理问题

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [125] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文提出了一种基于能量视角的流匹配方法，用于分子结构生成，通过迭代映射从随机配置到目标结构，在蛋白质对接和骨架生成任务上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 分子结构生成在分子对接、蛋白质折叠和分子设计等生物应用中至关重要。现有生成模型如扩散模型和流匹配已取得进展，但需要从能量角度改进训练和推理

Method: 采用能量视角的流匹配方法，通过深度网络学习迭代映射函数，将随机配置（源分布样本）映射到目标结构（数据流形中的点）

Result: 在蛋白质对接和蛋白质骨架生成实验中，该方法在相同计算预算下优于最近的流匹配和扩散模型基线

Conclusion: 该方法概念简单、实证有效，具有理论依据，并与幂等性、稳定性等基本性质以及AlphaFold中的结构精炼技术有有趣联系

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [126] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: 多标签数据分析中，通过比较三种多元模型（Probit、Bernoulli和Staged Logit）来估计标签间的条件协方差，发现多元Probit模型错误率最低。


<details>
  <summary>Details</summary>
Motivation: 多标签数据分析前需要评估标签间的依赖性，但标签值本身无法直接测量这种依赖性，需要通过条件协方差来评估。

Method: 比较三种多元模型：多元Probit模型、多元Bernoulli模型和Staged Logit模型，通过实验观察它们对条件协方差的测量能力。

Result: 所有模型都能较好地测量常数和依赖性协方差，但都会在存在常数协方差时错误检测到依赖性协方差。多元Probit模型的错误率最低。

Conclusion: 多元Probit模型是估计多标签条件协方差的最佳选择，虽然所有模型都存在某些偏差。

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [127] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文研究Koopman表示在混沌动力系统中的泛化能力，重点关注其在预测和控制任务间的可迁移性。使用Lorenz系统作为测试平台，提出三阶段方法：通过自编码学习Koopman嵌入，预训练transformer进行下一状态预测，以及针对安全关键控制的微调。


<details>
  <summary>Details</summary>
Motivation: 研究Koopman表示是否能够捕获可重用的动力结构，而非任务特定模式，为物理信息机器学习中的多任务学习提供基础。

Method: 三阶段方法：1) 通过自编码学习Koopman嵌入；2) 预训练transformer进行下一状态预测；3) 针对安全关键控制任务进行微调。使用Lorenz混沌系统作为测试平台。

Result: Koopman嵌入在准确性和数据效率方面均优于标准和物理信息PCA基线。在微调过程中固定预训练transformer权重不会导致性能下降，表明学习到的表示捕获了可重用的动力结构。

Conclusion: Koopman嵌入可作为物理信息机器学习中多任务学习的基础，其表示具有良好的泛化能力和任务间可迁移性。

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [128] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: PAX-TS是一个模型无关的后处理算法，用于解释时间序列预测模型及其预测结果，通过局部输入扰动生成多粒度解释，并能表征多元时间序列的跨通道相关性。


<details>
  <summary>Details</summary>
Motivation: 现代时间序列预测模型通常不透明且不提供预测解释，而现有的后处理可解释性方法（如LIME）不适用于预测场景，因此需要专门的时间序列预测可解释性方法。

Method: 基于局部输入扰动的模型无关后处理算法，通过时间步相关性矩阵分析，能够生成多粒度解释并捕捉跨通道相关性。

Result: 在7种算法和10个不同数据集的基准测试中，PAX-TS能够有效捕捉模型行为，发现高性能和低性能算法的解释存在差异，并识别出6类重复出现的模式，这些模式是性能的指标。

Conclusion: PAX-TS能够以不同详细程度说明时间序列预测模型的机制，其解释可用于回答关于预测的实际问题，为时间序列预测提供了有效的可解释性解决方案。

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [129] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: FedProtoKD是一种改进的异构联邦学习方法，通过双知识蒸馏机制和对比学习来解决原型聚合中的边缘收缩问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的原型异构联邦学习方法在服务器端使用加权平均聚合原型，导致全局知识次优和原型边缘收缩问题，特别是在模型异构和数据极度非独立同分布的场景中影响模型性能。

Method: 提出FedProtoKD方法，采用增强的双知识蒸馏机制（客户端logits和原型特征表示），使用基于对比学习的可训练服务器原型和类别自适应原型边缘来解决原型边缘收缩问题，并通过样本与类别代表原型的接近度评估公共样本重要性。

Result: FedProtoKD在各种设置下实现了1.13%到34.13%的平均准确率提升，显著优于现有的最先进异构联邦学习方法。

Conclusion: FedProtoKD通过创新的双知识蒸馏和对比学习机制有效解决了原型聚合中的边缘收缩问题，为异构联邦学习提供了更优的解决方案。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [130] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: STDiff是一种基于条件去噪扩散模型的工业时间序列缺失值填补方法，通过逐步生成缺失值，基于最近已知状态和相关控制输入，在长缺失块情况下表现优于传统窗口方法


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法将缺失值填补视为固定时间窗口内的模式完成，这在工业系统中往往失效，因为工业动态受控制动作驱动、高度非平稳且可能存在长时间连续缺失

Method: 使用条件去噪扩散模型，具有与控制理论一致的自注意力因果偏置，基于最近已知状态和相关控制或环境输入逐步生成缺失值

Result: 在模拟缺失块的公共污水处理数据集上，STDiff始终实现最低误差，优势随缺失块长度增加而增强；在真实工业数据集上，产生动态合理的轨迹，而基于窗口的模型倾向于平坦化或过度平滑

Conclusion: 支持动态感知、显式条件化的填补方法作为工业时间序列的稳健方法，讨论了计算权衡和向更广泛领域的扩展

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [131] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 这篇论文提出了一种用弹簧和棒子组成的简单动力学系统，能够通过物理过程进行函数近似和学习。系统利用弹簧的势能编码损失函数，通过耗散达到最低能量配置，性能可与多层感知机相比。研究还发现了由环境波动导致的热力学学习障碍。


<details>
  <summary>Details</summary>
Motivation: 从物理角度理解学习系统，将学习视为一种物理过程。通过构建简单的弹簧-棒子动力学系统，探索物理系统如何实现函数近似和学习。

Method: 使用棒子模仿分段线性近似，用弹簧的势能编码平方误差损失函数，通过耗散达到最低能量配置。应用于回归任务，并研究系统的热力学性质。

Result: 系统在回归任务上的表现与多层感知机相当。发现了系统自由能变化与学习能力之间的关系，以及由环境波动导致的热力学学习障碍（thermodynamic learning barrier）。

Conclusion: 这个简单的物理模型为从物理角度理解学习系统提供了新的视角，并发现了热力学因素对学习能力的重要影响。

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [132] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 该研究从资源中心视角探索预测流程监控中的下一活动预测，通过评估不同模型和编码策略，发现LightGBM和Transformer模型在2-gram活动转移编码下表现最佳，而随机森林在结合2-gram转移和活动重复特征的编码中获益最大。


<details>
  <summary>Details</summary>
Motivation: 现有预测流程监控研究主要关注控制流视角，而资源中心视角能带来工作组织改进、工作负载平衡和容量预测等额外好处，但资源信息在下一活动预测中的作用尚未被探索。

Method: 使用四个预测模型（包括LightGBM、Transformer和随机森林）和三种编码策略（2-gram活动转移、活动重复特征及其组合），在四个真实数据集上进行评估。

Result: LightGBM和Transformer模型在2-gram活动转移编码下表现最佳，随机森林在组合编码中获益最大，组合编码实现了最高平均准确率。

Conclusion: 资源中心的下一活动预测方法能够实现更智能的资源分配、战略性劳动力规划和个性化员工支持，为预测流程监控研究开辟了新方向。

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [133] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 基于注意力自动编码器的主动学习框架，通过相似性搜索迭代精细决策空间，提升导强类不平衡数据集中的APT偶发性检测能力


<details>
  <summary>Details</summary>
Motivation: 高级持续急威胜(APTs)具有潜伏性和极端类别不平衡特征，给网络防御带来严峻挑战，需要有效的异常检测方法

Method: 提出一种新的基于主动学习的异常检测框架，使甦注意力自动编码器和特征空间相似性搜索，识别正常类似和异常类似实例，进行正式的相似性度量评估

Result: 在多样化数据集上的实验表明，相似性函数的选择对模型收敛性、异常检测准确性和标签效率有显著影响

Conclusion: 为威胁智能和网络防御领域的主动学习流水线提供了可操作的相似性函数选择指南

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [134] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: Gradstop是一种新颖的随机早停方法，仅利用梯度信息来防止过拟合，无需验证集，特别适用于数据有限场景。


<details>
  <summary>Details</summary>
Motivation: 传统早停方法需要保留验证集，减少了训练数据量。梯度下降算法已经免费产生了梯度信息，这些信息可以用来估计贝叶斯后验分布，从而避免使用验证集。

Method: 通过梯度信息估计贝叶斯后验分布，将早停问题定义为从该后验中采样，并使用近似后验来获得停止准则。

Result: 实证评估显示Gradstop在测试数据上实现较小的损失，性能优于基于验证集的停止准则，特别在数据有限场景（如迁移学习）中表现优异。

Conclusion: Gradstop能够利用全部数据集进行训练，计算开销小，可以作为可选功能集成到梯度下降库中，在数据受限环境下具有显著优势。

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents {\sc gradstop}, a novel stochastic early stopping method that only
uses information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
{\sc gradstop} achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [135] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 这篇论文深入分析了现代重复模型在联想回忆任务上的补偿机制，发现学习率选择对重复模型性能有重要影响，并揭示了泡水模型与Transformer在网络宽度和深度扩展时的对比性能差异。


<details>
  <summary>Details</summary>
Motivation: 虽然现代重复模型具有次二阶复杂度优势，但最近研究发现它们在推理和记忆任务上可能存在短板。本文通过联想回忆任务深入分析这些模型的补偿机制和扩展特性。

Method: 采用联想回忆工作工具作为基准，深入研究了学习率对重复模型的关键影响，对比了泡水模型与Transformer在不同网络宽度和深度下的表现，并通过结构消融实验分析了各组件对模型性能和优化稳定性的影响。

Result: 发现学习率选择对重复模型的性能有重要影响，这可能影响了之前研究的性能报告。泡水模型与Transformer在网络宽度和深度扩展时显示出明显的对比性能差异，关注模型在不同网络结构下的表现。

Conclusion: 重复模型的训练需要更多研究来稳定化，学习率的选择对性能有重要影响。泡水模型与Transformer在网络扩展时显示出不同的优势，这为深度学习模型的设计提供了重要线索。

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [136] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 使用透明的机器学习架构和HazBinLoss函数开发地震地动模型，解决了传统黑盒模型的可解释性问题和数据不平衡问题


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习地震地动模型存在两大问题：1）黑盒特性导致难以解释和信任，2）数据不平衡（近断层大震记录少而远场小震记录多）影响风险评估的可靠性

Method: 设计了一种透明的ML架构，每个输入变量（震级、距离等）独立处理后线性相加得到输出，使得每个因素的贡献可解释。使用HazBinLoss函负权函数，在训练中给临界地区大震记录赋予更高权重，以避免对最危险场景的预测不足

Result: 模型能够抓取已知的地震学原理，与现有的标准地震地动模型达到相似的性能水平，同时保持了模型的透明性

Conclusion: 该框架有助于推广机器学习方法在地震风险评估和灾害规划中的应用，解决了黑盒模型的可信过和数据偏料问题

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [137] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: GNN可以通过符号回归生成数值格式，从两节点图训练中恢复一阶有限体积格式，并在无监督设置下发现高阶格式


<details>
  <summary>Details</summary>
Motivation: 探索GNN在传统近似作用之外的新用途，验证其在外推和生成数值格式方面的能力，特别是在训练域之外的泛化性能

Method: 使用两节点图训练GNN，结合符号回归分析网络学到的表示；在无监督设置下使用类似PINN的残差损失训练GNN；扩展到高阶格式使用2-hop和2层GNN

Result: GNN能够外推出一阶有限体积格式，误差为O(ε)；符号回归确认网络重新发现了标准一阶格式的解析形式；无监督训练成功恢复一阶格式；高阶GNN发现了二阶修正项和中点格式

Conclusion: GNN不仅是强大的近似器，还能作为开发新型数值方法的积极贡献者，这代表了科学计算的新范式

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [138] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: TRIGON是一个新颖的图重布线框架，通过从多图视图中学习选择相关三角形来构建丰富的非平面三角剖分，有效解决GNN中的过度挤压和过度平滑问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络(GNNs)的性能受到图拓扑固有问题的限制，特别是过度挤压和过度平滑问题。现有的图重布线方法需要更有效的解决方案来改善信息传播。

Method: TRIGON框架通过联合优化三角形选择和下游分类性能，从多个图视图中学习选择相关三角形，构建非平面三角剖分来重布线图结构。

Result: 实验结果表明，TRIGON在节点分类任务上优于现有最先进方法，在多种同质性和异质性基准测试中都表现出色，产生的重布线图具有更小的直径、更大的谱间隙和更低的有效电阻。

Conclusion: TRIGON通过创新的三角形选择机制有效改善了图拓扑结构，为GNN性能提升提供了新的解决方案，在解决过度挤压和过度平滑问题上取得了显著成效。

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [139] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: APT-LLM是一种针对任意精度大语言模型的高效加速方案，通过新型数据格式、矩阵乘法优化、内存管理和内核映射方法，在GPU上实现显著的推理加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算需求巨大，限制了部署和实时性能。现有的量化方法在GPU上支持任意精度超低位量化时面临Tensor Core支持有限、内存管理低效和内核优化不灵活等挑战。

Method: 提出APT-LLM方案：1）双极INT数据格式实现高效无损转换和并行计算；2）位级矩阵分解重组方法支持任意精度；3）基于数据恢复的内存管理系统；4）动态内核映射优化超参数选择。

Result: 在LLM推理中，APT-LLM相比FP16基线在RTX 3090上达到3.99倍加速，相比NVIDIA CUTLASS INT4加速达到2.16倍。在RTX 4090和H800上分别实现2.44倍和1.65倍加速。

Conclusion: APT-LLM通过综合优化方案有效解决了任意精度LLM在GPU上的加速挑战，显著提升了推理性能，为超低位量化模型的部署提供了实用解决方案。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [140] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出了一种约束优化框架，统一扩散模型的对齐和组合方法，通过拉格朗日对偶训练算法确保生成样本满足奖励约束并保持与预训练模型的接近性


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在优化多个奖励或组合多个模型时存在权衡问题，无法保证生成样本同时具备所有期望属性

Method: 约束优化框架结合拉格朗日对偶训练算法，强制对齐模型满足奖励约束并保持与预训练模型的接近性

Result: 在图像生成任务中验证了方法的有效性，相比等权重方法能更有效地满足约束条件

Conclusion: 该框架为扩散模型的对齐和组合提供了理论保证和实用解决方案，能够有效处理多个竞争性属性的平衡问题

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [141] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 提出结合概率众包建模和主动学习的新框架，用于处理多标注者的噪声反馈，通过熵基查询选择优先获取最有信息量的动作反馈，在多个环境中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 基于偏好的强化学习依赖人类反馈，但高质量人类输入成本高且稀缺，特别是在专家反馈稀少或错误代价高的领域。

Method: 扩展Advise算法支持多训练器，在线估计其可靠性，结合熵基查询选择来指导反馈请求，使用概率众包建模处理噪声多标注者反馈。

Result: 在Taxi、Pacman、Frozen Lake等2D游戏和血糖控制任务中，对不确定轨迹进行反馈的智能体学习速度更快，在血糖控制任务中超越基线方法。

Conclusion: 结合概率众包建模和主动学习的框架能有效处理噪声人类反馈，提高基于偏好强化学习的效率和性能，特别是在真实世界应用中。

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [142] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 本文研究了基于三层玻尔兹曼机的密集关联记忆模型，通过统计力学分析推导了鞍点方程，提出了新的正则化方案提高训练稳定性，并开发了网络增长算法降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 密集关联记忆模型因其对对抗样本的鲁棒性以及与transformer注意力机制、生成扩散模型等前沿机器学习范式的紧密联系而受到关注，需要深入研究其理论基础和训练方法。

Method: 使用三层玻尔兹曼机构建DAM模型，通过统计力学分析推导鞍点方程，提出新的正则化方案，并开发基于鞍点层次结构的网络增长算法。

Result: 模型在监督和无监督分类问题上学习到可解释的解决方案，新正则化方案显著提高训练稳定性，网络增长算法大幅降低计算成本。

Conclusion: DAM模型具有强大的表示能力，通过理论分析和算法改进可以显著提升其训练效率和稳定性，为实际应用提供了理论基础和方法支持。

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [143] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的模型稳健性评估方法，通过假设检验定量评估概率性稳健性，解决了现有方法在计算成本和测量精度之间的交换问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键深度学习应用中，现有的稳健性评估方法在计算成本和测量精度之间存在显著的交换，限制了其实际应用价值。

Method: 进行了现有稳健性定义和相关评估方法的综合对比分析，提出了基于假设检验的新实用指标"塔式稳健性"来定量评估概率性稳健性。

Result: 广泛的对比评估显示了所提方法的优势和适用性，能够进行更严格和高效的部署前评估。

Conclusion: 该研究推进了安全关键深度学习应用中模型稳健性的系统理解和提升，为实践提供了更有效的评估方法。

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [144] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 提出了一种新的模糊感知序数情感表示框架，通过建模情感模糊度的变化率来同时捕捉情感标注的模糊性和情感轨迹的时间动态特性。


<details>
  <summary>Details</summary>
Motivation: 现有连续情感识别方法要么忽略情感的模糊性，要么将模糊性视为独立且静态的变量，无法充分捕捉情感固有的模糊性和动态特性。

Method: 引入模糊感知序数情感表示框架，通过情感模糊度的变化率来建模情感模糊性，并在RECOLA和GameVibe两个情感语料库上测试有界（唤醒度、效价）和无界（参与度）连续情感轨迹。

Result: 序数表示在无界标签上优于传统模糊感知模型，获得最高的CCC和SDA分数；在有界轨迹上，序数表示在SDA方面表现优异，显示出捕捉标注情感轨迹相对变化的卓越能力。

Conclusion: 模糊感知序数表示框架能有效建模情感轨迹的动态特性，特别是在捕捉相对变化方面表现突出，为连续情感识别提供了新的有效方法。

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce \emph{ambiguity-aware
ordinal} emotion representations, a novel framework that captures both the
ambiguity present in emotion annotation and the inherent temporal dynamics of
emotional traces. Specifically, we propose approaches that model emotion
ambiguity through its rate of change. We evaluate our framework on two
affective corpora -- RECOLA and GameVibe -- testing our proposed approaches on
both bounded (arousal, valence) and unbounded (engagement) continuous traces.
Our results demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [145] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: 本文首次从理论上证明了工具集成推理(TIR)能够严格扩展大语言模型的能力边界，通过Python解释器等工具解锁纯文本模型无法实现的问题解决策略，并提出了ASPO算法来优化工具使用行为。


<details>
  <summary>Details</summary>
Motivation: 虽然工具集成的大语言模型展现出巨大潜力，但缺乏解释为何这种范式有效的理论框架。本文旨在提供首个形式化证明，阐明TIR为何能够从根本上扩展LLM的能力。

Method: 引入优势塑造策略优化(ASPO)算法，直接修改优势函数来指导策略行为而不影响训练稳定性。在数学基准测试中使用Python解释器作为外部工具进行综合实验。

Result: TIR模型在pass@k指标上显著优于纯文本模型，优势不仅限于计算密集型问题，还延伸到需要抽象洞察的问题。ASPO实现了更早的代码调用和更多交互轮次。

Conclusion: 本研究首次为TIR的成功提供了原则性解释，将关注点从工具是否有效转向了工具为何以及如何实现更强大的推理能力，揭示了模型学习使用工具进行思考的新兴认知模式。

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [146] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 提出了Token Order Prediction (TOP)作为辅助目标，通过排序学习损失来预测token顺序，相比Multi-Token Prediction (MTP)更简单有效，在多个NLP基准测试中表现优于NTP和MTP


<details>
  <summary>Details</summary>
Motivation: 现有的Multi-Token Prediction (MTP)作为辅助目标在语言模型训练中表现不一致，在标准NLP基准测试中表现不佳，作者认为MTP的精确未来token预测作为辅助损失过于困难

Method: 提出Token Order Prediction (TOP)方法，使用学习排序损失来训练模型根据邻近度对即将到来的token进行排序，相比MTP只需要一个额外的unembedding层而不是多个transformer层

Result: 在340M、1.8B和7B参数规模的模型预训练中，TOP在八个标准NLP基准测试中整体表现优于NTP和MTP，即使在较大规模下也保持优势

Conclusion: TOP是一种更简单有效的辅助训练目标，通过token顺序预测而非精确token预测，能够更好地提升语言模型的性能

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [147] [Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation -- A Systematic Literature Review](https://arxiv.org/abs/2508.18331)
*Eduardo Vyhmeister,Bastien Pietropaoli,Alejando Martinez Molina,Montserrat Gonzalez-Ferreiro,Gabriel Gonzalez-Castane,Jordi Arjona Aroca,Andrea Visentin*

Main category: cs.DB

TL;DR: 本论文通过系统性文献综述，提供了数据价值评估和数据财务化领域的162个指标和KPI，并按平衡记分卡方法进行分类，以帮助组织理解这个复杂领域的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 数据价值评估和数据财务化是现代组织的核心议题，但缺乏标准化案例和框架指导，需要系统性的研究来填补这一空白。

Method: 采用系统性文献综述方法，收集分析了数据价值评估和财务化领域的162份参考文献，提取其中的指标和KPI，并使用平衡记分卡(BSC)方法进行分类和子聚类。

Result: 构建了一个包含多种指标和KPI的平衡记分卡分类系统，覆盖组织各个业务方面和多种利益相关者的需求，为数据管理提供了全面的框架指引。

Conclusion: 该研究不仅提供了丰富的指标体系，还讨论了制定标准化框架的困难和领域面临的主要挑战，对于推动数据资产管理的标准化和发展具有重要意义。

Abstract: Data valuation and data monetisation are complex subjects but essential to
most organisations today. Unfortunately, they still lack standard procedures
and frameworks for organisations to follow. In this survey, we introduce the
reader to the concepts by providing the definitions and the background required
to better understand data, monetisation strategies, and finally metrics and
KPIs used in these strategies. We have conducted a systematic literature review
on metrics and KPIs used in data valuation and monetisation, in every aspect of
an organisation's business, and by a variety of stakeholders. We provide an
expansive list of such metrics and KPIs with 162 references. We then categorise
all the metrics and KPIs found into a large taxonomy, following the Balanced
Scorecard (BSC) approach with further subclustering to cover every aspect of an
organisation's business. This taxonomy will help every level of data management
understand the complex landscape of the domain. We also discuss the difficulty
in creating a standard framework for data valuation and data monetisation and
the major challenges the domain is currently facing.

</details>


### [148] [DiskJoin: Large-scale Vector Similarity Join with SSD](https://arxiv.org/abs/2508.18494)
*Yanqi Chen,Xiao Yan,Alexandra Meliou,Eric Lo*

Main category: cs.DB

TL;DR: DiskJoin是一个基于磁盘的相似性连接算法，在单机上高效处理十亿级向量数据集，通过优化数据访问模式和缓存管理实现50-1000倍加速


<details>
  <summary>Details</summary>
Motivation: 现有分布式方法需要集群部署且机器间通信成本高，而基于磁盘的解决方案虽然成本效益更好但存在磁盘I/O瓶颈问题

Method: 通过定制数据访问模式避免重复访问和读取放大，使用主内存作为动态缓存并精心管理缓存驱逐，采用概率剪枝技术有效减少计算量

Result: 在真实世界大规模数据集上的评估显示，DiskJoin显著优于替代方案，实现50倍到1000倍的加速

Conclusion: DiskJoin是首个能够在单机上高效处理十亿级向量数据集的基于磁盘相似性连接算法，有效解决了磁盘I/O瓶颈问题

Abstract: Similarity join--a widely used operation in data science--finds all pairs of
items that have distance smaller than a threshold. Prior work has explored
distributed computation methods to scale similarity join to large data volumes
but these methods require a cluster deployment, and efficiency suffers from
expensive inter-machine communication. On the other hand, disk-based solutions
are more cost-effective by using a single machine and storing the large dataset
on high-performance external storage, such as NVMe SSDs, but in these methods
the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,
the first disk-based similarity join algorithm that can process billion-scale
vector datasets efficiently on a single machine. DiskJoin improves disk I/O by
tailoring the data access patterns to avoid repetitive accesses and read
amplification. It also uses main memory as a dynamic cache and carefully
manages cache eviction to improve cache hit rate and reduce disk retrieval
time. For further acceleration, we adopt a probabilistic pruning technique that
can effectively prune a large number of vector pairs from computation. Our
evaluation on real-world, large-scale datasets shows that DiskJoin
significantly outperforms alternatives, achieving speedups from 50x to 1000x.

</details>


### [149] [Brook-2PL: Tolerating High Contention Workloads with A Deadlock-Free Two-Phase Locking Protocol](https://arxiv.org/abs/2508.18576)
*Farzad Habibi,Juncheng Fang,Tania Lorido-Botran,Faisal Nawab*

Main category: cs.DB

TL;DR: Brook-2PL是一种新颖的两阶段锁定协议，通过SLW-Graph实现无死锁事务执行和部分事务切分实现早期锁释放，在高竞争工作负载下显著优于现有并发控制协议。


<details>
  <summary>Details</summary>
Motivation: 传统并发控制在高度竞争工作负载下存在热点问题，导致过多事务中止和死锁，造成工作浪费和系统负担。

Method: 提出SLW-Graph依赖结构实现无死锁两阶段锁定，通过静态分析和部分事务切分实现早期锁释放，提供更灵活的事务切分方法。

Result: 在TPC-C基准测试中平均加速2.86倍，尾部延迟(p95)降低48%，在合成在线游戏商店工作负载中也表现优异。

Conclusion: Brook-2PL有效解决了高竞争工作负载下的热点问题，显著提升了并发控制性能，减少了事务中止和死锁。

Abstract: The problem of hotspots remains a critical challenge in high-contention
workloads for concurrency control (CC) protocols. Traditional concurrency
control approaches encounter significant difficulties under high contention,
resulting in excessive transaction aborts and deadlocks. In this paper, we
propose Brook-2PL, a novel two-phase locking (2PL) protocol that (1) introduces
SLW-Graph for deadlock-free transaction execution, and (2) proposes partial
transaction chopping for early lock release. Previous methods suffer from
transaction aborts that lead to wasted work and can further burden the system
due to their cascading effects. Brook-2PL addresses this limitation by
statically analyzing a new graph-based dependency structure called SLW-Graph,
enabling deadlock-free two-phase locking through predetermined lock
acquisition. Brook-2PL also reduces contention by enabling early lock release
using partial transaction chopping and static transaction analysis. We overcome
the inherent limitations of traditional transaction chopping by providing a
more flexible chopping method. Evaluation using both our synthetic online game
store workload and the TPC-C benchmark shows that Brook-2PL significantly
outperforms state-of-the-art CC protocols. Brook-2PL achieves an average
speed-up of 2.86x while reducing tail latency (p95) by 48% in the TPC-C
benchmark.

</details>


### [150] [Optimal $(α,β)$-Dense Subgraph Search in Bipartite Graphs](https://arxiv.org/abs/2508.18616)
*Yalong Zhang,Rong-Hua Li,Qi Zhang,Guoren Wang*

Main category: cs.DB

TL;DR: 提出了BD-Index索引结构，用于在二分图中高效查询(α, β)-稠密子图，支持最优时间查询和线性空间使用，并提供两种动态更新策略以适应不同应用需求。


<details>
  <summary>Details</summary>
Motivation: 现有的(α, β)-稠密子图模型虽然能更好地捕捉二分图的密度结构，但缺乏高效的查询处理和动态更新支持，限制了其在大规模应用中的实用性。

Method: 设计了BD-Index索引结构，使用线性空间O(|E|)实现最优时间查询。开发了两种动态维护策略：空间高效策略(O(p·|E|^{1.5})更新时间)和时间高效策略(O(p·|E|)更新时间但更高内存消耗)。

Result: 在10个大规模真实数据集上的实验表明，所提解决方案具有高效率和良好的可扩展性。

Conclusion: BD-Index填补了(α, β)-稠密子图模型在查询处理和动态更新方面的空白，为二分图分析提供了实用的解决方案，能够适应不同的应用需求。

Abstract: Dense subgraph search in bipartite graphs is a fundamental problem in graph
analysis, with wide-ranging applications in fraud detection, recommendation
systems, and social network analysis. The recently proposed $(\alpha,
\beta)$-dense subgraph model has demonstrated superior capability in capturing
the intrinsic density structure of bipartite graphs compared to existing
alternatives. However, despite its modeling advantages, the $(\alpha,
\beta)$-dense subgraph model lacks efficient support for query processing and
dynamic updates, limiting its practical utility in large-scale applications. To
address these limitations, we propose BD-Index, a novel index that answers
$(\alpha, \beta)$-dense subgraph queries in optimal time while using only
linear space $O(|E|)$, making it well-suited for real-world applications
requiring both fast query processing and low memory consumption. We further
develop two complementary maintenance strategies for dynamic bipartite graphs
to support efficient updates to the BD-Index. The space-efficient strategy
updates the index in time complexity of $O(p \cdot |E|^{1.5})$ per edge
insertion or deletion, while maintaining a low space cost of $O(|E|)$ (the same
as the index itself), where $p$ is typically a small constant in real-world
graphs. In contrast, the time-efficient strategy significantly reduces the
update time to $O(p \cdot |E|)$ per edge update by maintaining auxiliary
orientation structures, at the cost of increased memory usage up to $O(p \cdot
|E|)$. These two strategies provide flexible trade-offs between maintenance
efficiency and memory usage, enabling BD-Index to adapt to diverse application
requirements. Extensive experiments on 10 large-scale real-world datasets
demonstrate high efficiency and scalability of our proposed solutions.

</details>


### [151] [WoW: A Window-to-Window Incremental Index for Range-Filtering Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.18617)
*Ziqi Wang,Jingzhe Zhang,Wei Hu*

Main category: cs.DB

TL;DR: 提出基于窗口图的RFANNS索引方法，支持增量构建和处理任意范围过滤器，在查询效率和索引构建时间上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决现有RFANNS索引无法增量构建和难以处理任意范围过滤器的问题，这是向量数据库管理系统和智能系统中的基础功能需求

Method: 使用分层窗口图结构，提出插入算法支持增量构建，通过范围选择性优化窗口搜索以处理任意范围过滤器和向量距离计算

Result: 索引构建时间与最高效构建索引相当，比最高效查询索引快4.9倍且体积小0.4-0.5倍；RFANNS查询比最高效增量索引快4倍，性能匹配最佳静态构建索引

Conclusion: 窗口图方法在保持高效查询性能的同时支持增量构建，解决了RFANNS索引的关键挑战，为实际应用提供了实用解决方案

Abstract: Given a hybrid dataset where every data object consists of a vector and an
attribute value, for each query with a target vector and a range filter,
range-filtering approximate nearest neighbor search (RFANNS) aims to retrieve
the most similar vectors from the dataset and the corresponding attribute
values fall in the query range. It is a fundamental function in vector database
management systems and intelligent systems with embedding abilities. Dedicated
indices for RFANNS accelerate query speed with an acceptable accuracy loss on
nearest neighbors. However, they are still facing the challenges to be
constructed incrementally and generalized to achieve superior query performance
for arbitrary range filters. In this paper, we introduce a window graph-based
RFANNS index. For incremental construction, we propose an insertion algorithm
to add new vector-attribute pairs into hierarchical window graphs with varying
window size. To handle arbitrary range filters, we optimize relevant window
search for attribute filter checks and vector distance computations by range
selectivity. Extensive experiments on real-world datasets show that for index
construction, the indexing time is on par with the most building-efficient
index, and 4.9x faster than the most query-efficient index with 0.4-0.5x
smaller size; For RFANNS query, it is 4x faster than the most efficient
incremental index, and matches the performance of the best statically-built
index.

</details>


### [152] [Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics](https://arxiv.org/abs/2508.18736)
*Jungwoo Kim,Minsang Kim,Jaeheon Lee,Chanwoo Moon,Heejin Kim,Taeho Hwang,Woosuk Chung,Yeseong Kim,Sungjin Lee*

Main category: cs.DB

TL;DR: SISO是一个针对大语言模型服务的语义缓存系统，通过基于质心的缓存、局部感知替换和动态阈值技术，在严格的计算和内存约束下显著提高命中率和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 传统缓存策略（精确匹配和前缀缓存）忽视了查询语义，而现有的语义缓存系统缺乏概念创新，无法在大语言模型服务中满足严格的SLO要求。

Method: 提出SISO系统，采用三种核心技术：1）基于质心的缓存以最小内存实现最大覆盖；2）局部感知替换策略保留高价值条目；3）动态阈值技术在变化的工作负载下平衡准确性和延迟。

Result: 在多样化数据集上，SISO相比最先进系统实现了高达1.71倍的命中率提升，并持续提供更强的SLO达成能力。

Conclusion: SISO通过重新定义LLM服务效率的语义缓存方法，有效解决了传统缓存策略的局限性，为大规模语言模型服务提供了更高效的缓存解决方案。

Abstract: Serving Large Language Models (LLMs) at scale requires meeting strict Service
Level Objectives (SLOs) under severe computational and memory constraints.
Nevertheless, traditional caching strategies fall short: exact-matching and
prefix caches neglect query semantics, while state-of-the-art semantic caches
remain confined to traditional intuitions, offering little conceptual
departure. Building on this, we present SISO, a semantic caching system that
redefines efficiency for LLM serving. SISO introduces centroid-based caching to
maximize coverage with minimal memory, locality-aware replacement to preserve
high-value entries, and dynamic thresholding to balance accuracy and latency
under varying workloads. Across diverse datasets, SISO delivers up to
1.71$\times$ higher hit ratios and consistently stronger SLO attainment
compared to state-of-the-art systems.

</details>


### [153] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 这篇论文提出了一种新的框架，将自然语言查询转换为查询计划，避免SQL的限制，支持复杂数据分析功能，并通过增量执行解决大数据集处理问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型表格数据集查询分析的挑战，特别是对于无编程经验的用户。尽管Text-to-SQL方法在标准数据库上表现不错，但继承了SQL的缺点：大数据集处理效率低，对复杂数据分析支持有限。

Method: 提出一种在传统数据库外部实现的框架，利用LLM迭代解释查询并构建操作序列。通过增量建立解决方案来处理计算复杂性，直接在数据上执行操作来免去上下文长度限制。支持经典SQL命令和复杂分析功能（如主成分分析、异常检测等）。

Result: 在标准数据库和大型科学表格上进行实验验证，证明该框架能够有效处理大规模数据集并执行精细的数据分析。

Conclusion: 该框架免除了SQL的内在限制，提供了比传统SQL更大的灵活性和可扩展性，能够有效地处理大型数据集的查询和复杂分析任务。

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


### [154] [Enriching Object-Centric Event Data with Process Scopes: A Framework for Aggregation and Analysis](https://arxiv.org/abs/2508.18830)
*Shahrzad Khayatbashi,Majid Rafiei,Jiayuan Chen,Timotheus Kampik,Gregor Berg,Amin Jalali*

Main category: cs.DB

TL;DR: 提出了一种在对象中心事件日志中嵌入分析师定义流程范围的方法，以支持多流程共存的结构化表示和跨范围的事件数据聚合分析。


<details>
  <summary>Details</summary>
Motivation: 现有对象中心事件数据格式缺乏明确的流程范围定义，限制了分析只能针对单个流程且粒度较低，而实际业务中多个相关流程通过共享对象相互连接，需要更灵活的范围定义方法。

Method: 开发了一种将分析师定义的流程范围嵌入到对象中心事件日志中的方法，支持多流程的结构化表示，并提供范围定义和分析的支持工具。

Result: 使用公开可用的OCEL日志验证了方法的适用性，实现了跨流程范围的事件数据聚合和不同抽象层次的分析。

Conclusion: 该方法解决了对象中心流程挖掘中流程边界不明确的问题，使分析师能够根据具体需求定义流程范围，支持更灵活和细粒度的业务流程分析。

Abstract: Object-Centric Process Mining enables the analysis of complex operational
behavior by capturing interactions among multiple business objects (e.g.,
orders, items, deliveries). These interactions are recorded using
Object-Centric Event Data (OCED) formats, such as the Object-Centric Event Log
(OCEL). However, existing formats lack explicit definitions of process scopes,
which restricts analysis to individual processes and limits insights to a low
level of granularity. In practice, OCED often spans multiple interrelated
processes, as shared objects connect events across organizational functions.
This structure reflects how value is created along the organizational value
chain, but introduces challenges for interpretation when process boundaries are
not clearly defined. Moreover, process definitions are typically subjective and
context-dependent; they vary across organizations, roles, and analytical goals,
and cannot always be discovered automatically. To address these challenges, we
propose a method for embedding analyst-defined process scopes into OCEL. This
enables the structured representation of multiple coexisting processes,
supports the aggregation of event data across scopes, and facilitates analysis
at varying levels of abstraction. We demonstrate the applicability of our
approach using a publicly available OCEL log and provide supporting tools for
scope definition and analysis.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [155] [Integral Online Algorithms for Set Cover and Load Balancing with Convex Objectives](https://arxiv.org/abs/2508.18383)
*Thomas Kesselheim,Marco Molinaro,Kalen Patton,Sahil Singla*

Main category: cs.DS

TL;DR: 本文提出了针对在线集合覆盖和负载均衡问题的积分在线算法，直接处理凸目标函数，绕过了传统的凸松弛和对偶技术。


<details>
  <summary>Details</summary>
Motivation: 现有的在线算法主要针对分数设置，由于大的积分间隙，无法直接应用于积分设置。需要开发能够直接处理积分约束的算法。

Method: 将问题转化为在线打包问题，通过解耦全局打包为局部打包问题，并使用随机激活阈值策略来保证机器激活时的期望覆盖效率。

Result: 成功将凸目标和对称范数的结果从分数设置扩展到积分设置，并能处理在线广义调度问题，还扩展到非对称范数的复合结构。

Conclusion: 提出的方法为在线集合覆盖和负载均衡问题提供了有效的积分算法框架，能够处理广泛的凸目标函数和范数结构。

Abstract: Online Set Cover and Load Balancing are central problems in online
optimization, and there is a long line of work on developing algorithms for
these problems with convex objectives. Although we know optimal online
algorithms with $\ell_p$-norm objectives, recent developments for general norms
and convex objectives that rely on the online primal-dual framework apply only
to fractional settings due to large integrality gaps.
  Our work focuses on directly designing integral online algorithms for Set
Cover and Load Balancing with convex objectives, bypassing the
convex-relaxation and the primal-dual technique. Some of the main implications
are:
  1. For Online Set Cover, we can extend the results of Azar et. al. (2016) for
convex objectives and of Kesselheim, Molinaro, and Singla (2024) for symmetric
norms from fractional to integral settings.
  2. Our results for convex objectives and symmetric norms even apply to the
online generalized scheduling problem, which generalizes both Set Cover and
Load Balancing. Previous works could only handle the offline version of this
problem with norm objectives (Deng, Li, and Rabani 2023).
  3. Our methods easily extend to settings with disjoint-composition of norms.
This allows us to recover or improve the norm-composition results of Nagarajan
and Shen (2020), and Kesselheim, Molinaro, and Singla (2024), and to extend our
results to a large class of norms beyond symmetric.
  Our approach is to first reduce these problems to online packing problems,
and then to design good approximation algorithms for the latter. To solve these
packing problems, we use two key ideas. First, we decouple the global packing
problem into a series of local packing problems on different machines. Next, we
choose random activation thresholds for machines such that conditional on a
machine being activated, the expected number of jobs it covers is high compared
to its cost.

</details>


### [156] [Improving Pinwheel Density Bounds for Small Minimums](https://arxiv.org/abs/2508.18422)
*Ahan Mishra,Parker Rho,Robert Kleinberg*

Main category: cs.DS

TL;DR: 本文证明了m=4时pinwheel实例的可调度性密度界为0.84，首次突破了5/6的通用界限，并开发了新的启发式求解器和展开操作技术


<details>
  <summary>Details</summary>
Motivation: 研究pinwheel调度问题中最小元素m较大时的密度界限，现有研究显示密度间隙在O(1/m)和O(1/√m)之间，但具体m值下的精确界限仍有待探索

Method: 开发了新的快速启发式pinwheel求解器和展开操作技术，用于分析m=4时的调度密度界限

Result: 成功证明了当最小元素m=4时，密度界限可以达到0.84，这是首个突破通用5/6界限的具体m值结果

Conclusion: 该研究不仅为m=4提供了更好的密度界限，还开发了新技术为更大m值的pinwheel调度问题研究奠定了基础

Abstract: The density bound for schedulability for general pinwheel instances is
$\frac{5}{6}$, but density bounds better than $\frac{5}{6}$ can be shown for
cases in which the minimum element $m$ of the instance is large. Several recent
works have studied the question of the 'density gap' as a function of $m$, with
best known lower and upper bounds of $O \left( \frac{1}{m} \right)$ and $O
\left( \frac{1}{\sqrt{m}} \right)$. We prove a density bound of $0.84$ for $m =
4$, the first $m$ for which a bound strictly better than $\frac{5}{6} =
0.8\overline{3}$ can be proven. In doing so, we develop new techniques,
particularly a fast heuristic-based pinwheel solver and an unfolding operation.

</details>


### [157] [Hypergraph Splitting-Off via Element-Connectivity Preserving Reductions](https://arxiv.org/abs/2508.18637)
*Karthekeyan Chandrasekaran,Chandra Chekuri,Shubhang Kulkarni*

Main category: cs.DS

TL;DR: 本文提供了超图中保持局部连通性的分裂操作的新证明方法


<details>
  <summary>Details</summary>
Motivation: 为Bérczi等人提出的超图分裂操作提供基于图论中元素连通性保持操作的替代证明

Method: 使用图论中的元素连通性保持归约操作来证明超图分裂操作

Result: 成功提供了超图分裂操作的新证明方法

Conclusion: 基于图论的元素连通性保持操作可以为超图的分裂操作提供简洁的替代证明

Abstract: B\'erczi, Chandrasekaran, Kir\'aly, and Kulkarni (ICALP 2024) recently
described a splitting-off procedure in hypergraphs that preserves
local-connectivity and outlined some applications. In this note we give an
alternative proof via element-connectivity preserving reduction operations in
graphs.

</details>


### [158] [Graph Traversal via Connected Mobile Agents](https://arxiv.org/abs/2508.18683)
*Saswata Jana,Giuseppe F. Italiano,Partha Sarathi Mandal*

Main category: cs.DS

TL;DR: 本文研究了多智能体协调框架中的哈密顿行走问题（k-HWP），提出了针对不同图结构的近似算法和最优算法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中保持连通性的哈密顿行走问题在之前的研究中未被探索，该问题是经典哈密顿行走问题的推广，具有重要的理论和应用价值。

Method: 针对2-HWP提出了(3-1/21)近似算法；针对树结构定义了受限版本问题并给出了任意k值的最优算法；针对k-均匀超图提出了2(1+ln k)近似算法。

Result: 在任意图上为2-HWP设计了近似比为3-1/21的算法；在树上为任意k值提供了最优解；在k-均匀超图上获得了2(1+ln k)的近似比。

Conclusion: k-HWP问题是NP难的，但通过不同的算法设计策略，可以在各种图结构上获得有效的近似解和最优解，为多智能体协调问题提供了新的解决方案。

Abstract: This paper considers the Hamiltonian walk problem in the multi-agent
coordination framework, referred to as $k$-agents Hamiltonian walk problem
($k$-HWP). In this problem, a set of $k$ connected agents collectively compute
a spanning walk of a given undirected graph in the minimum steps. At each step,
the agents are at $k$ distinct vertices and the induced subgraph made by the
occupied vertices remains connected. In the next consecutive steps, each agent
may remain stationary or move to one of its neighbours.To the best of our
knowledge, this problem has not been previously explored in the context of
multi-agent systems with connectivity. As a generalization of the well-known
Hamiltonian walk problem (when $k=1$), $k$-HWP is NP-hard. We propose a
$(3-\frac{1}{21})$-approximation algorithm for 2-HWP on arbitrary graphs. For
the tree, we define a restricted version of the problem and present an optimal
algorithm for arbitrary values of $k$. Finally, we formalize the problem for
$k$-uniform hypergraphs and present a $2(1+\ln k)$-approximation algorithm.
This result is also adapted to design an approximation algorithm for $k$-HWP on
general graphs when $k = O(1)$.

</details>


### [159] [Max-Min and 1-Bounded Space Algorithms for the Bin Packing Problem](https://arxiv.org/abs/2508.18718)
*Hiroshi Fujiwara,Rina Atsumi,Hiroaki Yamamoto*

Main category: cs.DS

TL;DR: 本文分析了Zhu提出的MM装箱算法的近似比为1.5，并深入研究了max-min算法和1-bounded space算法两类算法的理论性能界限，发现这两类算法交集的下界为1.25，还将分析扩展到基数约束装箱问题。


<details>
  <summary>Details</summary>
Motivation: 研究Zhu提出的MM装箱算法的理论性能，该算法在物品大小非递增排序后，采用最大-最小策略进行装箱。由于MM算法处于max-min算法和1-bounded space算法的交集位置，需要系统分析这两类算法的理论性能界限。

Method: 通过理论分析证明MM算法的渐近近似比上界为1.5；系统研究max-min算法和1-bounded space算法两个算法子类的性能界限；推导两类算法交集的下界为1.25；将理论分析扩展到基数约束装箱问题。

Result: 证明了MM算法的渐近近似比上界为1.5；确定了max-min算法和1-bounded space算法交集的下界为1.25；成功将理论分析框架扩展到基数约束装箱问题。

Conclusion: MM算法具有较好的理论性能保证(近似比1.5)，同时位于两个重要算法类的交集位置。对算法子类的系统性分析为理解装箱算法的理论界限提供了重要见解，扩展分析到基数约束问题展示了方法的通用性。

Abstract: In the (1-dimensional) bin packing problem, we are asked to pack all the
given items into bins, each of capacity one, so that the number of non-empty
bins is minimized. Zhu~[Chaos, Solitons \& Fractals 2016] proposed an
approximation algorithm $MM$ that sorts the item sequence in a non-increasing
order by size at the beginning, and then repeatedly packs, into the current
single open bin, first as many of the largest items in the remaining sequence
as possible and then as many of the smallest items in the remaining sequence as
possible. In this paper we prove that the asymptotic approximation ratio of
$MM$ is at most 1.5. Next, focusing on the fact that $MM$ is at the
intersection of two algorithm classes, max-min algorithms and 1-bounded space
algorithms, we comprehensively analyze the theoretical performance bounds of
each subclass derived from the two classes. Our results include a lower bound
of 1.25 for the intersection of the two classes. Furthermore, we extend the
theoretical analysis over algorithm classes to the cardinality constrained bin
packing problem.

</details>


### [160] [DTC: Real-Time and Accurate Distributed Triangle Counting in Fully Dynamic Graph Streams](https://arxiv.org/abs/2508.19057)
*Wei Xuan,Yan Liang,Huawei Cao,Ning Lin,Xiaochun Ye,Dongrui Fan*

Main category: cs.DS

TL;DR: 提出了DTC系列分布式流算法，包括DTC-AR（无需图大小先验知识）和DTC-FD（支持边插入删除），在动态图流中实现高精度三角形计数


<details>
  <summary>Details</summary>
Motivation: 现有分布式流算法缺乏适应性且难以处理边删除操作，需要开发能够处理任意边序动态图流的高效近似算法

Method: 使用Random Pairing和未来边插入补偿技术，在多机环境下实现无偏估计；DTC-AR无需图大小先验知识，DTC-FD专门针对全动态图流设计

Result: DTC-AR精度提升2029.4倍和27.1倍，在精度和存储空间间达到最佳平衡；DTC-FD估计误差减少32.5倍和19.3倍，随图流大小线性扩展

Conclusion: 所提算法在真实场景中有效解决了三角形计数问题，显著优于基线方法，代码和数据集已开源

Abstract: Triangle counting is a fundamental problem in graph mining, essential for
analyzing graph streams with arbitrary edge orders. However, exact counting
becomes impractical due to the massive size of real-world graph streams. To
address this, approximate algorithms have been developed, but existing
distributed streaming algorithms lack adaptability and struggle with edge
deletions. In this article, we propose DTC, a novel family of single-pass
distributed streaming algorithms for global and local triangle counting in
fully dynamic graph streams. Our DTC-AR algorithm accurately estimates triangle
counts without prior knowledge of graph size, leveraging multi-machine
resources. Additionally, we introduce DTC-FD, an algorithm tailored for fully
dynamic graph streams, incorporating edge insertions and deletions. Using
Random Pairing and future edge insertion compensation, DTC-FD achieves unbiased
and accurate approximations across multiple machines. Experimental results
demonstrate significant improvements over baselines. DTC-AR achieves up to
$2029.4\times$ and $27.1\times$ more accuracy, while maintaining the best
trade-off between accuracy and storage space. DTC-FD reduces estimation errors
by up to $32.5\times$ and $19.3\times$, scaling linearly with graph stream
size. These findings highlight the effectiveness of our proposed algorithms in
tackling triangle counting in real-world scenarios. The source code and
datasets are released and available at
\href{https://github.com/wayne4s/srds-dtc.git}{https://github.com/wayne4s/srds-dtc.git}.

</details>


### [161] [Approximating High-Dimensional Earth Mover's Distance as Fast as Closest Pair](https://arxiv.org/abs/2508.06774)
*Lorenzo Beretta,Vincent Cohen-Addad,Rajesh Jayaram,Erik Waingarten*

Main category: cs.DS

TL;DR: 本文提出了从(1+ε)-近似最近对问题到(1+ε)-近似地球移动距离问题的归约方法，改进了高维EMD的最快近似算法


<details>
  <summary>Details</summary>
Motivation: 地球移动距离(EMD)是计算两个点集之间最小匹配成本的重要度量，在高维空间中计算效率低下。通过建立EMD与最近对问题(CP)的联系，可以利用更快的CP算法来加速EMD计算

Method: 使用乘法权重更新框架的亚线性实现，通过几何结构隐式执行权重更新，避免显式计算和存储权重。将(1+ε)-近似CP算法的时间复杂度n^{2-φ}转化为EMD的n^{2-Ω(φ)}时间复杂度

Result: 获得了运行时间为n^{2-Ω̃(ε^{1/3})}的(1+ε)-近似EMD算法，相比之前最快的n^{2-Ω(ε^2)}算法有显著改进

Conclusion: 该方法成功建立了EMD与CP之间的计算联系，通过几何感知的权重更新技术实现了效率提升，为高维EMD计算提供了新的最快算法

Abstract: We give a reduction from $(1+\varepsilon)$-approximate Earth Mover's Distance
(EMD) to $(1+\varepsilon)$-approximate Closest Pair (CP). As a consequence, we
improve the fastest known approximation algorithm for high-dimensional EMD.
Here, given $p\in [1, 2]$ and two sets of $n$ points $X,Y \subseteq (\mathbb
R^d,\ell_p)$, their EMD is the minimum cost of a perfect matching between $X$
and $Y$, where the cost of matching two vectors is their $\ell_p$ distance.
Further, CP is the basic problem of finding a pair of points realizing $\min_{x
\in X, y\in Y} ||x-y||_p$. Our contribution is twofold: we show that if a
$(1+\varepsilon)$-approximate CP can be computed in time $n^{2-\phi}$, then a
$1+O(\varepsilon)$ approximation to EMD can be computed in time
$n^{2-\Omega(\phi)}$; plugging in the fastest known algorithm for CP [Alman,
Chan, Williams FOCS'16], we obtain a $(1+\varepsilon)$-approximation algorithm
for EMD running in time $n^{2-\tilde{\Omega}(\varepsilon^{1/3})}$ for
high-dimensional point sets, which improves over the prior fastest running time
of $n^{2-\Omega(\varepsilon^2)}$ [Andoni, Zhang FOCS'23]. Our main technical
contribution is a sublinear implementation of the Multiplicative Weights Update
framework for EMD. Specifically, we demonstrate that the updates can be
executed without ever explicitly computing or storing the weights; instead, we
exploit the underlying geometric structure to perform the updates implicitly.

</details>
