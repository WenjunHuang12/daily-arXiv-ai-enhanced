<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.IT](#cs.IT) [Total: 12]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Mechanism Design for Federated Learning with Non-Monotonic Network Effects](https://arxiv.org/abs/2601.04648)
*Xiang Li,Bing Luo,Jianwei Huang,Yuan Luo*

Main category: cs.GT

TL;DR: 提出SWAN机制，通过模型交易与共享框架解决联邦学习中网络效应和个性化性能需求的激励问题，显著提升社会福利并降低激励成本。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习激励机制忽略网络效应和不同应用对模型性能的差异化需求，导致激励效果不佳、社会福利低下，甚至在实际部署中不适用。

Method: 提出模型交易与共享框架，允许客户端通过参与训练或购买模型获取FL模型；设计SWAN机制，利用模型客户支付进行激励，最大化社会福利。

Result: 在硬件原型实验中，SWAN机制相比现有FL机制提升社会福利高达352.42%，减少额外激励成本93.07%。

Conclusion: 考虑网络效应和个性化性能需求的激励机制能显著提升联邦学习的社会福利和激励效率，为实际部署提供有效解决方案。

Abstract: Mechanism design is pivotal to federated learning (FL) for maximizing social welfare by coordinating self-interested clients. Existing mechanisms, however, often overlook the network effects of client participation and the diverse model performance requirements (i.e., generalization error) across applications, leading to suboptimal incentives and social welfare, or even inapplicability in real deployments. To address this gap, we explore incentive mechanism design for FL with network effects and application-specific requirements of model performance. We develop a theoretical model to quantify the impact of network effects on heterogeneous client participation, revealing the non-monotonic nature of such effects. Based on these insights, we propose a Model Trading and Sharing (MoTS) framework, which enables clients to obtain FL models through either participation or purchase. To further address clients' strategic behaviors, we design a Social Welfare maximization with Application-aware and Network effects (SWAN) mechanism, exploiting model customer payments for incentivization. Experimental results on a hardware prototype demonstrate that our SWAN mechanism outperforms existing FL mechanisms, improving social welfare by up to $352.42\%$ and reducing extra incentive costs by $93.07\%$.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [2] [Learning Multinomial Logits in $O(n \log n)$ time](https://arxiv.org/abs/2601.04423)
*Flavio Chierichetti,Mirko Giacchini,Ravi Kumar,Silvio Lattanzi,Alessandro Panconesi,Erasmo Tani,Andrew Tomkins*

Main category: cs.DS

TL;DR: 本文研究了从成对比较查询中学习MNL模型权重的问题，提出了自适应和非自适应两种算法，分别达到O(n/ε³ log n)和O(n²/ε³ log n log(n/ε))的查询复杂度，并给出了相应的下界证明。


<details>
  <summary>Details</summary>
Motivation: MNL模型在推荐系统中广泛应用，但如何通过查询学习模型权重的基本计算问题尚未解决。现有研究缺乏对查询复杂度的系统分析，特别是从成对比较中学习MNL模型的理论保证。

Method: 提出了两种算法：1）自适应算法通过顺序查询成对比较来学习权重；2）非自适应算法通过批量查询成对比较来学习权重。两种算法都只使用大小为2的slate查询。

Result: 自适应算法查询复杂度为O(n/ε³ log n)，非自适应算法为O(n²/ε³ log n log(n/ε))。同时证明了自适应查询的下界为Ω(n/ε² log n)，非自适应查询的下界为Ω(n²/ε² log n)。

Conclusion: 自适应算法在支持大小n的依赖上是最优的，非自适应算法在log n因子内是紧的。这为MNL模型学习提供了理论保证，对推荐系统界面设计有直接意义。

Abstract: A Multinomial Logit (MNL) model is composed of a finite universe of items $[n]=\{1,..., n\}$, each assigned a positive weight. A query specifies an admissible subset -- called a slate -- and the model chooses one item from that slate with probability proportional to its weight. This query model is also known as the Plackett-Luce model or conditional sampling oracle in the literature. Although MNLs have been studied extensively, a basic computational question remains open: given query access to slates, how efficiently can we learn weights so that, for every slate, the induced choice distribution is within total variation distance $\varepsilon$ of the ground truth? This question is central to MNL learning and has direct implications for modern recommender system interfaces.
  We provide two algorithms for this task, one with adaptive queries and one with non-adaptive queries. Each algorithm outputs an MNL $M'$ that induces, for each slate $S$, a distribution $M'_S$ on $S$ that is within $\varepsilon$ total variation distance of the true distribution. Our adaptive algorithm makes $O\left(\frac{n}{\varepsilon^{3}}\log n\right)$ queries, while our non-adaptive algorithm makes $O\left(\frac{n^{2}}{\varepsilon^{3}}\log n \log\frac{n}{\varepsilon}\right)$ queries. Both algorithms query only slates of size two and run in time proportional to their query complexity.
  We complement these upper bounds with lower bounds of $Ω\left(\frac{n}{\varepsilon^{2}}\log n\right)$ for adaptive queries and $Ω\left(\frac{n^{2}}{\varepsilon^{2}}\log n\right)$ for non-adaptive queries, thus proving that our adaptive algorithm is optimal in its dependence on the support size $n$, while the non-adaptive one is tight within a $\log n$ factor.

</details>


### [3] [Using Ray-shooting Queries for Sublinear Algorithms for Dominating Sets in RDV Graphs](https://arxiv.org/abs/2601.04626)
*Therese Biedl,Prashant Gokhale*

Main category: cs.DS

TL;DR: 在RDV图中，通过结合水平-垂直线段相交查询和射线射击数据结构，可以在O(n log n)时间内找到最小支配集。


<details>
  <summary>Details</summary>
Motivation: RDV图是介于区间图和弦图之间的图类，之前的研究已经展示了如何在O(n log n)时间内找到最大匹配。本文旨在探索是否能在类似时间复杂度内解决更复杂的支配集问题。

Method: 利用RDV图的特殊结构：顶点可表示为有根树中向下路径的交集图。结合两种数据结构：1) 水平-垂直线段相交查询（用于邻接关系），2) 射线射击数据结构。通过这种组合方法高效处理支配集问题。

Result: 成功在O(n log n)时间内找到RDV图的最小支配集（假设给定图的线性大小表示）。该方法还可为区间图的最小支配集问题提供新的O(n)时间证明。

Conclusion: 通过巧妙结合线段相交查询和射线射击数据结构，可以在接近线性的时间内解决RDV图的最小支配集问题，扩展了之前仅用于最大匹配的技术，并为区间图提供了新的算法视角。

Abstract: In this paper, we study the dominating set problem in \emph{RDV graphs}, a graph class that lies between interval graphs and chordal graphs and is defined as the \textbf{v}ertex-intersection graphs of \textbf{d}ownward paths in a \textbf{r}ooted tree. It was shown in a previous paper that adjacency queries in an RDV graph can be reduced to the question whether a horizontal segment intersects a vertical segment. This was then used to find a maximum matching in an $n$-vertex RDV graph, using priority search trees, in $O(n\log n)$ time, i.e., without even looking at all edges. In this paper, we show that if additionally we also use a ray shooting data structure, we can also find a minimum dominating set in an RDV graph $O(n\log n)$ time (presuming a linear-sized representation of the graph is given). The same idea can also be used for a new proof to find a minimum dominating set in an interval graph in $O(n)$ time.

</details>


### [4] [Branch-width of connectivity functions is fixed-parameter tractable](https://arxiv.org/abs/2601.04756)
*Tuukka Korhonen,Sang-il Oum*

Main category: cs.DS

TL;DR: 该论文提出了一个固定参数可处理算法，用于在给定预言机的情况下，为连通函数找到宽度至多为k的分支分解，运行时间为2^{O(k^2)}γn^6 log n，改进了之前Oum和Seymour的γn^{O(k)}算法。


<details>
  <summary>Details</summary>
Motivation: 解决Hliněný提出的开放问题：对于由秩预言机给出的拟阵分支宽度，是否具有固定参数可处理性。同时改进图分支宽度、秩宽度和雕刻宽度等FPT算法中对参数k的依赖关系。

Method: 设计了一个新的固定参数可处理算法，通过连通函数预言机计算f(X)，算法核心思想是利用分支分解的结构特性，通过系统搜索和优化技术来找到宽度不超过k的分支分解。

Result: 成功证明了在给定预言机的情况下，寻找宽度至多为k的分支分解是固定参数可处理的，运行时间为2^{O(k^2)}γn^6 log n，显著改进了先前结果，并解决了相关开放问题。

Conclusion: 该算法不仅解决了拟阵分支宽度的固定参数可处理性问题，还统一改进了图分支宽度、秩宽度和雕刻宽度的FPT算法，在参数k的依赖关系上取得了指数级改进。

Abstract: A connectivity function on a finite set $V$ is a symmetric submodular function $f \colon 2^V \to \mathbb{Z}$ with $f(\emptyset)=0$. We prove that finding a branch-decomposition of width at most $k$ for a connectivity function given by an oracle is fixed-parameter tractable (FPT), by providing an algorithm of running time $2^{O(k^2)} γn^6 \log n$, where $γ$ is the time to compute $f(X)$ for any set $X$, and $n = |V|$. This improves the previous algorithm by Oum and Seymour [J. Combin. Theory Ser.~B, 2007], which runs in time $γn^{O(k)}$. Our algorithm can be applied to rank-width of graphs, branch-width of matroids, branch-width of (hyper)graphs, and carving-width of graphs. This resolves an open problem asked by Hliněný [SIAM J. Comput., 2005], who asked whether branch-width of matroids given by the rank oracle is fixed-parameter tractable. Furthermore, our algorithm improves the best known dependency on $k$ in the running times of FPT algorithms for graph branch-width, rank-width, and carving-width.

</details>


### [5] [An Invitation to "Fine-grained Complexity of NP-Complete Problems"](https://arxiv.org/abs/2601.05044)
*Jesper Nederlof*

Main category: cs.DS

TL;DR: 该调查论文探讨NP完全问题的细粒度复杂度，研究是否存在比暴力搜索更快的算法，以及暴力搜索是否已经是最优解。


<details>
  <summary>Details</summary>
Motivation: 虽然已知P≠NP意味着NP完全问题的最坏情况运行时间必须是超多项式的，但具体能达到多快的运行时间仍是一个开放问题。许多问题的朴素暴力搜索算法仍然是目前最快的，这引发了一个根本性问题：这些暴力搜索算法是否已经是最优的？

Method: 通过调查论文的形式，综合运用代数、复杂性理论、极值与加法组合学、密码学等多学科方法，分析NP完全问题的细粒度复杂度。研究经典结果和最新进展，探索算法设计的边界。

Result: 该领域的研究表明，对于许多NP完全问题，暴力搜索算法可能确实是最优的，或者至少存在条件性下界证明支持这一结论。细粒度复杂度理论为理解计算问题的精确时间界限提供了框架。

Conclusion: 细粒度复杂度研究揭示了NP完全问题计算复杂性的深层结构，表明许多问题的朴素算法可能已经达到最优。这一领域连接了多个数学和计算机科学分支，为算法设计提供了重要见解。

Abstract: Assuming that P is not equal to NP, the worst-case run time of any algorithm solving an NP-complete problem must be super-polynomial. But what is the fastest run time we can get? Before one can even hope to approach this question, a more provocative question presents itself: Since for many problems the naive brute-force baseline algorithms are still the fastest ones, maybe their run times are already optimal?
  The area that we call in this survey "fine-grained complexity of NP-complete problems" studies exactly this question. We invite the reader to catch up on selected classic results as well as delve into exciting recent developments in a riveting tour through the area passing by (among others) algebra, complexity theory, extremal and additive combinatorics, cryptography, and, of course, last but not least, algorithm design.

</details>


### [6] [Learning Mixture Models via Efficient High-dimensional Sparse Fourier Transforms](https://arxiv.org/abs/2601.05157)
*Alkis Kalavasis,Pravesh K. Kothari,Shuchen Li,Manolis Zampetakis*

Main category: cs.DS

TL;DR: 提出首个能有效学习重尾分布混合模型的多项式时间算法，无需最小均值分离，突破了传统矩方法的限制。


<details>
  <summary>Details</summary>
Motivation: 传统混合模型学习方法都依赖低阶矩，无法处理重尾分布（如拉普拉斯分布）或无限协方差的分布。需要开发能绕过矩方法限制的新技术。

Method: 基于高效高维稀疏傅里叶变换的新方法，利用分布特征函数的重尾特性，无需最小均值分离条件。

Result: 实现了在d维空间中学习k个球形重尾分布混合的多项式时间和样本复杂度算法，适用于拉普拉斯分布等重尾分布，但不适用于高斯分布。

Conclusion: 该方法突破了传统矩方法的局限性，为统计估计开辟了新途径，并能与现有技术结合实现"两全其美"的保证，在鲁棒均值估计等应用中具有潜力。

Abstract: In this work, we give a ${\rm poly}(d,k)$ time and sample algorithm for efficiently learning the parameters of a mixture of $k$ spherical distributions in $d$ dimensions. Unlike all previous methods, our techniques apply to heavy-tailed distributions and include examples that do not even have finite covariances. Our method succeeds whenever the cluster distributions have a characteristic function with sufficiently heavy tails. Such distributions include the Laplace distribution but crucially exclude Gaussians.
  All previous methods for learning mixture models relied implicitly or explicitly on the low-degree moments. Even for the case of Laplace distributions, we prove that any such algorithm must use super-polynomially many samples. Our method thus adds to the short list of techniques that bypass the limitations of the method of moments.
  Somewhat surprisingly, our algorithm does not require any minimum separation between the cluster means. This is in stark contrast to spherical Gaussian mixtures where a minimum $\ell_2$-separation is provably necessary even information-theoretically [Regev and Vijayaraghavan '17]. Our methods compose well with existing techniques and allow obtaining ''best of both worlds" guarantees for mixtures where every component either has a heavy-tailed characteristic function or has a sub-Gaussian tail with a light-tailed characteristic function.
  Our algorithm is based on a new approach to learning mixture models via efficient high-dimensional sparse Fourier transforms. We believe that this method will find more applications to statistical estimation. As an example, we give an algorithm for consistent robust mean estimation against noise-oblivious adversaries, a model practically motivated by the literature on multiple hypothesis testing. It was formally proposed in a recent Master's thesis by one of the authors, and has already inspired follow-up works.

</details>


### [7] [Inapproximability of Counting Permutation Patterns](https://arxiv.org/abs/2601.05166)
*Michal Opler*

Main category: cs.DS

TL;DR: 该论文强烈反驳了近似计数比精确计数更容易的猜想，证明了在指数时间假说下，长度k模式的近似计数与精确计数具有相同的计算复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明对于长度不超过5的模式，近似计数可以在近线性时间内完成，这引发了近似计数是否普遍比精确计数更容易的猜想。本文旨在检验这一猜想。

Method: 通过理论分析，在指数时间假说（ETH）下，证明了对于长度k的模式，任何在时间f(k)·n^{o(k/log k)}内运行的算法都无法将模式副本数量近似到n^{(1/2-ε)k}的乘法因子内。

Result: 证明了近似计数与精确计数具有相同的条件复杂度下界，即都需要f(k)·n^{Ω(k/log k)}时间，且误差因子下界n^{(1/2-ε)k}本质上是紧的，因为n^{k/2}近似可以在2^{O(k²)}·n时间内完成。

Conclusion: 近似计数并不比精确计数更容易，两者在计算复杂度上具有相同的下界，这强烈反驳了先前关于近似计数普遍更简单的猜想。

Abstract: Detecting and counting copies of permutation patterns are fundamental algorithmic problems, with applications in the analysis of rankings, nonparametric statistics, and property testing tasks such as independence and quasirandomness testing. From an algorithmic perspective, there is a sharp difference in complexity between detecting and counting the copies of a given length-$k$ pattern in a length-$n$ permutation. The former admits a $2^{\mathcal{O}(k^2)} \cdot n$ time algorithm (Guillemot and Marx, 2014) while the latter cannot be solved in time $f(k)\cdot n^{o(k/\log k)}$ unless the Exponential Time Hypothesis (ETH) fails (Berendsohn, Kozma, and Marx, 2021). In fact already for patterns of length 4, exact counting is unlikely to admit near-linear time algorithms under standard fine-grained complexity assumptions (Dudek and Gawrychowski, 2020).
  Recently, Ben-Eliezer, Mitrović and Sristava (2026) showed that for patterns of length up to 5, a $(1+\varepsilon)$-approximation of the pattern count can be computed in near-linear time, yielding a separation between exact and approximate counting for small patterns, and conjectured that approximate counting is asymptotically easier than exact counting in general. We strongly refute their conjecture by showing that, under ETH, no algorithm running in time $f(k)\cdot n^{o(k/\log k)}$ can approximate the number of copies of a length-$k$ pattern within a multiplicative factor $n^{(1/2-\varepsilon)k}$. The lower bound on runtime matches the conditional lower bound for exact pattern counting, and the obtained bound on the multiplicative error factor is essentially tight, as an $n^{k/2}$-approximation can be computed in $2^{\mathcal{O}(k^2)}\cdot n$ time using an algorithm for pattern detection.

</details>


### [8] [Concurrent Balanced Augmented Trees](https://arxiv.org/abs/2601.05225)
*Evan Wrench,Ajay Singh,Younghun Roh,Panagiota Fatourou,Siddhartha Jayanti,Eric Ruppert,Yuanhao Wei*

Main category: cs.DS

TL;DR: 首个无锁增强平衡搜索树，支持高效聚合查询、顺序统计查询和范围查询，性能比非平衡增强树快2.2-30倍


<details>
  <summary>Details</summary>
Motivation: 增强搜索树能支持聚合查询、顺序统计查询和范围查询等多种操作，但现有无锁增强搜索树都是非平衡的，性能受限。需要设计无锁增强平衡搜索树来获得更好的性能

Method: 基于Fatourou和Ruppert的无锁增强非平衡搜索树，设计首个无锁增强平衡搜索树。使用委托机制优化版本以提高可扩展性和性能，并解决内存回收问题

Result: 优化版本性能提升超过2倍，增强平衡树比非平衡增强树快2.2-30倍，在120线程上比非增强树快几个数量级

Conclusion: 成功实现首个无锁增强平衡搜索树，通过委托优化显著提升性能，为并发数据结构提供高效的多功能查询支持

Abstract: Augmentation makes search trees tremendously more versatile, allowing them to support efficient aggregation queries, order-statistic queries, and range queries in addition to insertion, deletion, and lookup. In this paper, we present the first lock-free augmented balanced search tree. Our algorithmic ideas build upon a recent augmented unbalanced search tree presented by Fatourou and Ruppert [DISC, 2024]. We implement both data structures, solving some memory reclamation challenges in the process, and provide an experimental performance analysis of them. We also present optimized versions of our balanced tree that use delegation to achieve better scalability and performance (by more than 2x in some workloads). Our experiments show that our augmented balanced tree is 2.2 to 30 times faster than the unbalanced augmented tree, and up to several orders of magnitude faster than unaugmented trees on 120 threads.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [9] [Achievable Rate and Coding Principle for MIMO Multicarrier Systems With Cross-Domain MAMP Receiver Over Doubly Selective Channels](https://arxiv.org/abs/2601.04433)
*Yuhao Chi,Zhiyuan Peng,Lei Liu,Ying Li,Yao Ge,Chau Yuen*

Main category: cs.IT

TL;DR: 提出MS-CD-MAMP接收机用于MIMO多载波系统，分析其可达速率极限和最优编码原则，在双选择性信道中OFDM/OTFS/AFDM可达相同最大速率


<details>
  <summary>Details</summary>
Motivation: 现有研究显示OTFS和AFDM在非编码双选择性信道中优于OFDM，但编码系统中这些优势是否延续尚不明确，且编码MIMO多载波系统的信息论极限分析和低复杂度接收机设计仍不清楚

Method: 提出多时隙跨域记忆近似消息传递（MS-CD-MAMP）接收机，利用时域信道稀疏性降低复杂度，利用符号域星座约束提升性能；提出简化的单输入单输出变分状态演化分析可达速率和最优编码原则

Result: 编码MIMO-OFDM/OTFS/AFDM在双选择性信道中可达相同最大速率，实际优化的LDPC码性能距理论极限仅0.5-1.8dB，比精心设计的点对点LDPC码有0.8-4.4dB增益

Conclusion: MS-CD-MAMP接收机为MIMO多载波系统提供了统一的信息论极限分析和低复杂度接收方案，解决了编码系统中OTFS/AFDM优势延续性的不确定问题

Abstract: The integration of multicarrier modulation and multiple-input-multiple-output (MIMO) is critical for reliable transmission of wireless signals in complex environments, which significantly improve spectrum efficiency. Existing studies have shown that popular orthogonal time frequency space (OTFS) and affine frequency division multiplexing (AFDM) offer significant advantages over orthogonal frequency division multiplexing (OFDM) in uncoded doubly selective channels. However, it remains uncertain whether these benefits extend to coded systems. Meanwhile, the information-theoretic limit analysis of coded MIMO multicarrier systems and the corresponding low-complexity receiver design remain unclear. To overcome these challenges, this paper proposes a multi-slot cross-domain memory approximate message passing (MS-CD-MAMP) receiver as well as develops its information-theoretic (i.e., achievable rate) limit and optimal coding principle for MIMO-multicarrier modulation (e.g., OFDM, OTFS, and AFDM) systems. The proposed MS-CD-MAMP receiver can exploit not only the time domain channel sparsity for low complexity but also the corresponding symbol domain constellation constraints for performance enhancement. Meanwhile, limited by the high-dimensional complex state evolution (SE), a simplified single-input single-output variational SE is proposed to derive the achievable rate of MS-CD-MAMP and the optimal coding principle with the goal of maximizing the achievable rate. Numerical results show that coded MIMO-OFDM/OTFS/AFDM with MS-CD-MAMP achieve the same maximum achievable rate in doubly selective channels, whose finite-length performance with practical optimized low-density parity-check (LDPC) codes is only 0.5 $\sim$ 1.8 dB away from the associated theoretical limit, and has 0.8 $\sim$ 4.4 dB gain over the well-designed point-to-point LDPC codes.

</details>


### [10] [Bridging Distance and Spectral Positional Encodings via Anchor-Based Diffusion Geometry Approximation](https://arxiv.org/abs/2601.04517)
*Zimo Yan,Zheng Xie,Runfan Duan,Chang Liu,Wumei Du*

Main category: cs.IT

TL;DR: 该论文揭示了距离编码与扩散几何之间的理论联系，提出了一种三边测量映射方法，并在分子图学习任务中验证了位置编码的重要性。


<details>
  <summary>Details</summary>
Motivation: 分子图学习需要位置信号来捕捉局部邻域和全局拓扑结构。虽然谱编码和基于锚点的距离编码被广泛使用，但两者之间的精确关系尚未被充分理解。本文旨在揭示距离编码作为扩散几何低秩替代的本质，并建立两者之间的理论联系。

Method: 1. 将距离编码解释为扩散几何的低秩替代；2. 推导出显式的三边测量映射，从变换后的锚点距离和锚点谱位置重建截断的扩散坐标；3. 在随机正则图上提供点态和Frobenius间隙保证；4. 在DrugBank分子图上使用基于GNP的DDI预测骨干，通过距离驱动的Nyström方案恢复扩散几何。

Result: 1. 理论分析表明距离编码可以准确重建扩散几何；2. 在DrugBank分子图的DDI预测任务中，距离驱动的Nyström方案能紧密恢复扩散几何；3. 拉普拉斯编码和距离编码都显著优于无编码基线。

Conclusion: 该研究建立了距离编码与扩散几何之间的理论联系，证明了距离编码可以作为扩散几何的有效低秩替代。在分子图学习任务中，位置编码（无论是谱编码还是距离编码）对性能提升至关重要，为图神经网络中的位置编码设计提供了理论指导。

Abstract: Molecular graph learning benefits from positional signals that capture both local neighborhoods and global topology. Two widely used families are spectral encodings derived from Laplacian or diffusion operators and anchor-based distance encodings built from shortest-path information, yet their precise relationship is poorly understood. We interpret distance encodings as a low-rank surrogate of diffusion geometry and derive an explicit trilateration map that reconstructs truncated diffusion coordinates from transformed anchor distances and anchor spectral positions, with pointwise and Frobenius-gap guarantees on random regular graphs. On DrugBank molecular graphs using a shared GNP-based DDI prediction backbone, a distance-driven Nyström scheme closely recovers diffusion geometry, and both Laplacian and distance encodings substantially outperform a no-encoding baseline.

</details>


### [11] [Air-to-Ground Communications for Internet of Things: UAV-based Coverage Hole Detection and Recovery](https://arxiv.org/abs/2601.04665)
*Xiao Fan,Wenkun Wen,Peiran Wu,Junhui Zhao,Minghua Xia*

Main category: cs.IT

TL;DR: 提出基于无人机群的物联网网络覆盖空洞实时检测与恢复框架，通过巡逻无人机检测空洞，然后部署无人机作为空中基站恢复连接，采用Delaunay三角剖分组织无人机群，并包含防撞机制。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖空洞检测方法（如最小化路测）成本高、耗时长、依赖过时数据，不适合实时应用。无人机在物联网连接中发挥关键作用，特别是在地面网络受限或暂时不可用的情况下。

Method: 1) 派遣巡逻无人机检测地面基站状态不确定区域的覆盖空洞；2) 检测到空洞后，通过卫星或附近运行基站部署一个或多个无人机作为空中基站恢复连接；3) 基于Delaunay三角剖分组织无人机群，实现可扩展部署和可分析的随机几何特征；4) 基于多智能体系统理论设计防撞机制，确保多无人机安全协调运动。

Result: 仿真结果表明，该框架在覆盖空洞检测和按需连接恢复方面均实现高效率，同时显著降低运营成本和时间。

Conclusion: 提出的无人机辅助框架能够有效解决物联网网络覆盖空洞的实时检测和恢复问题，相比传统方法具有成本低、实时性强、可扩展性好等优势。

Abstract: Uncrewed aerial vehicles (UAVs) play a pivotal role in ensuring seamless connectivity for Internet of Things (IoT) devices, particularly in scenarios where conventional terrestrial networks are constrained or temporarily unavailable. However, traditional coverage-hole detection approaches, such as minimizing drive tests, are costly, time-consuming, and reliant on outdated radio-environment data, making them unsuitable for real-time applications. To address these limitations, this paper proposes a UAV-assisted framework for real-time detection and recovery of coverage holes in IoT networks. In the proposed scheme, a patrol UAV is first dispatched to identify coverage holes in regions where the operational status of terrestrial base stations (BSs) is uncertain. Once a coverage hole is detected, one or more UAVs acting as aerial BSs are deployed by a satellite or nearby operational BSs to restore connectivity. The UAV swarm is organized based on Delaunay triangulation, enabling scalable deployment and tractable analytical characterization using stochastic geometry. Moreover, a collision-avoidance mechanism grounded in multi-agent system theory ensures safe and coordinated motion among multiple UAVs. Simulation results demonstrate that the proposed framework achieves high efficiency in both coverage-hole detection and on-demand connectivity restoration while significantly reducing operational cost and time.

</details>


### [12] [Feasibility Study Regarding Self-sustainable Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2601.04723)
*Zhenyu Li,Ozan Alp Topal,Özlem Tuğfe Demir,Emil Björnson,Cicek Cavdar*

Main category: cs.IT

TL;DR: 论文评估了自供能可重构智能表面（ssRIS）的部署可行性，分析了两种能量收集与反射方案：元件分割（ES）和时间分割（TS），比较了它们在LOS和NLOS信道下的性能差异。


<details>
  <summary>Details</summary>
Motivation: ssRIS无需布线供电即可实现可重构相移，能部署在传统中继或基站无法覆盖的区域，为增强无线覆盖提供了新方案。但需要评估其实际部署可行性，特别是不同能量收集方案在不同环境下的适用性。

Method: 分析了两种能量收集与反射方案：元件分割（ES）和时间分割（TS）。研究了元件需求如何随系统参数（发射功率、数据速率需求、中断约束）变化，并在LOS和NLOS ssRIS到用户设备的信道条件下进行了分析。

Result: TS方案具有更好的信道硬化增益，元件需求在不同中断裕度下保持稳定，适合室内部署和中等数据速率场景。但TS的元件需求随能量收集难度和数据速率呈指数增长。ES方案仅随能量收集难度线性增长，在恶劣户外场景下更具可行性。

Conclusion: TS在环境良好的场景中表现优异，优先考虑可靠性；而ES在需要操作鲁棒性的苛刻条件下更优。这为ssRIS在不同部署环境下的方案选择提供了指导。

Abstract: Without requiring operational costs such as cabling and powering while maintaining reconfigurable phase-shift capability, self-sustainable reconfigurable intelligent surfaces (ssRISs) can be deployed in locations inaccessible to conventional relays or base stations, offering a novel approach to enhance wireless coverage. This study assesses the feasibility of ssRIS deployment by analyzing two harvest-and-reflect (HaR) schemes: element-splitting (ES) and time-splitting (TS). We examine how element requirements scale with key system parameters, transmit power, data rate demands, and outage constraints under both line-of-sight (LOS) and non-line-of-sight (NLOS) ssRIS-to-user equipment (UE) channels. Analytical and numerical results reveal distinct feasibility characteristics. The TS scheme demonstrates better channel hardening gain, maintaining stable element requirements across varying outage margins, making it advantageous for indoor deployments with favorable harvesting conditions and moderate data rates. However, TS exhibits an element requirement that exponentially scales to harvesting difficulty and data rate. Conversely, the ES scheme shows only linear growth with harvesting difficulty, providing better feasibility under challenging outdoor scenarios. These findings establish that TS excels in benign environments, prioritizing reliability, while ES is preferable for demanding conditions requiring operational robustness.

</details>


### [13] [Privacy-Utility Trade-offs Under Multi-Level Point-Wise Leakage Constraints](https://arxiv.org/abs/2601.04815)
*Amirreza Zamani,Parastoo Sadeghi,Mikael Skoglund*

Main category: cs.IT

TL;DR: 提出基于信息论的隐私机制设计，在无法直接访问私有数据X的情况下，通过设计隐私机制生成披露数据U，在满足不同隐私泄露预算的点对点约束下最大化关于有用数据Y的信息披露。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常对每个数据点使用相同的泄露水平，但实际应用中不同数据点可能需要不同的隐私保护级别，有些甚至需要完全隐私（零泄露）。需要一种更灵活的隐私机制，能够为不同数据点分配不同的泄露预算。

Method: 引入多级点对点泄露度量，允许为U的不同实现施加不同的泄露水平。当泄露足够小时，利用信息几何概念对互信息进行局部近似，将问题转化为可求解的二次优化问题。证明在可逆泄露矩阵条件下，仅需考虑二元U即可达到最优效用。

Result: 当泄露矩阵可逆时，利用局部近似可将问题转化为具有闭式解的二次优化问题。特别地，证明仅需二元U即可达到最优效用，这导致基于矩阵最大奇异值和奇异向量的低复杂度隐私设计。

Conclusion: 提出的多级点对点泄露框架提供了更灵活的隐私保护机制，能够同时处理完全隐私和非零泄露的情况。通过信息几何近似和二元U的充分性证明，实现了简单有效的隐私设计，降低了计算复杂度。

Abstract: An information-theoretic privacy mechanism design is studied, where an agent observes useful data $Y$ which is correlated with the private data $X$. The agent wants to reveal the information to a user, hence, the agent utilizes a privacy mechanism to produce disclosed data $U$ that can be revealed. We assume that the agent has no direct access to $X$, i.e., the private data is hidden. We study privacy mechanism design that maximizes the disclosed information about $Y$, measured by the mutual information between $Y$ and $U$, while satisfying a point-wise constraint with different privacy leakage budgets. We introduce a new measure, called the \emph{multi-level point-wise leakage}, which allows us to impose different leakage levels for different realizations of $U$. In contrast to previous studies on point-wise measures, which use the same leakage level for each realization, we consider a more general scenario in which each data point can leak information up to a different threshold. As a result, this concept also covers cases in which some data points should not leak any information about the private data, i.e., they must satisfy perfect privacy. In other words, a combination of perfect privacy and non-zero leakage can be considered. When the leakage is sufficiently small, concepts from information geometry allow us to locally approximate the mutual information. We show that when the leakage matrix $P_{X|Y}$ is invertible, utilizing this approximation leads to a quadratic optimization problem that has closed-form solution under some constraints. In particular, we show that it is sufficient to consider only binary $U$ to attain the optimal utility. This leads to simple privacy designs with low complexity which are based on finding the maximum singular value and singular vector of a matrix.

</details>


### [14] [Stability of Constrained Optimization Models for Structured Signal Recovery](https://arxiv.org/abs/2601.04849)
*Yijun Zhong,Yi Shen*

Main category: cs.IT

TL;DR: 该论文研究了利用信号结构先验知识（如稀疏性）进行信号恢复的约束优化模型，分析了模型对噪声的鲁棒性和参数稳定性，并建立了样本复杂度与失配误差之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 从测量中恢复未知但结构化的信号是成像恢复、无线通信和信号处理等领域的重要挑战性问题。信号结构先验知识（如稀疏性）对于信号恢复模型至关重要，但参数选择往往困难，需要理论分析支持实际应用。

Method: 提出了三种约束优化模型，每种模型利用不同形式的结构先验来正则化解空间。进行了理论分析，证明这些模型对噪声具有鲁棒性，并且对调谐参数保持稳定性。

Result: 理论分析表明，这些模型在测量不完美和模型不确定性不可避免的场景中具有实用价值。在温和条件下，建立了样本复杂度与失配误差之间的权衡关系。

Conclusion: 通过提供理论基础，该工作支持这些模型在实际应用中的使用，特别是在测量不完美和模型不确定性不可避免的场景中。建立的权衡关系为实际系统设计提供了理论指导。

Abstract: Recovering an unknown but structured signal from its measurements is a challenging problem with significant applications in fields such as imaging restoration, wireless communications, and signal processing. In this paper, we consider the inherent problem stems from the prior knowledge about the signal's structure, such as sparsity which is critical for signal recovery models. We investigate three constrained optimization models that effectively address this challenge, each leveraging distinct forms of structural priors to regularize the solution space. Our theoretical analysis demonstrates that these models exhibit robustness to noise while maintaining stability with respect to tuning parameters that is a crucial property for practical applications, when the parameter selection is often nontrivial. By providing theoretical foundations, our work supports their practical use in scenarios where measurement imperfections and model uncertainties are unavoidable. Furthermore, under mild conditions, we establish tradeoff between the sample complexity and the mismatch error.

</details>


### [15] [Wireless Communication with Cross-Linked Rotatable Antenna Array: Architecture Design and Rotation Optimization](https://arxiv.org/abs/2601.04862)
*Ailing Zheng,Qingqing Wu,Ziyuan Zheng,Qiaoyan Peng,Yanze Zhu,Honghao Wang,Wen Chen,Guoying Zhang*

Main category: cs.IT

TL;DR: 提出了一种交叉链接可旋转天线结构，通过协调多个天线旋转降低成本，在单用户和多用户场景下优化接收波束成形和旋转角度，性能接近灵活天线方向方案。


<details>
  <summary>Details</summary>
Motivation: 传统可旋转天线系统的硬件成本和控制复杂度与天线数量成正比，需要一种成本效益更高的解决方案来利用空间自由度。

Method: 采用交叉链接可旋转天线结构，建立天线单元级和天线面板级旋转的系统模型，通过交替优化算法联合优化基站接收波束成形和旋转角度，使用最小均方误差方法和可行方向法求解。

Result: 通过精心设计行列分区方案，CL-RA架构性能接近灵活天线方向方案；CL天线单元级方案比面板级方案性能提升25%，比传统固定方向天线性能提升128%。

Conclusion: 交叉链接可旋转天线结构提供了一种成本效益高的解决方案，能够有效利用空间自由度提升系统性能，在硬件限制下通过遗传算法处理离散旋转角度选择问题。

Abstract: Rotatable antenna (RA) technology can harness additional spatial degrees of freedom by enabling the dynamic three-dimensional orientation control of each antenna. Unfortunately, the hardware cost and control complexity of traditional RA systems is proportional to the number of RAs. To address the issue, we consider a cross-linked (CL) RA structure, which enables the coordinated rotation of multiple antennas, thereby offering a cost-effective solution. To evaluate the performance of the CL-RA array, we investigate a CL-RA-aided uplink system. Specifically, we first establish system models for both antenna element-level and antenna panel-level rotation. Then, we formulate a sum rate maximization problem by jointly optimizing the receive beamforming at the base station and the rotation angles. For the antenna element-level rotation, we derive the optimal solution of the CL-RA array under the single-user case. Subsequently, for two rotation schemes, we propose an alternating optimization algorithm to solve the formulated problem in the multi-user case, where the receive beamforming and the antenna rotation angles are obtained by applying the minimum mean square error method and feasible direction method, respectively. In addition, considering the hardware limitations, we apply the genetic algorithm to address the discrete rotation angles selection problem. Simulation results show that by carefully designing the row-column partition scheme, the performance of the CL-RA architecture is quite close to that of the flexible antenna orientation scheme. Moreover, the CL antenna element-level scheme surpasses the CL antenna panel-level scheme by 25% and delivers a 128% performance improvement over conventional fixed-direction antennas.

</details>


### [16] [Learning Sparsifying Transforms for mmWave Communication via $\ell^4$-Norm Maximization](https://arxiv.org/abs/2601.04980)
*Sueda Taner,Christoph Studer*

Main category: cs.IT

TL;DR: 该论文提出通过ℓ⁴范数最大化学习复数域稀疏变换，评估DFT在毫米波大规模MIMO系统中的最优性，并学习改进的稀疏变换。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段波传播的高方向性导致用户设备与基站之间只有少量显著传输路径，这种稀疏特性在波束空间域中体现。传统上使用DFT作为稀疏变换，但尚不清楚DFT是否是有限维天线阵列的最优稀疏变换。

Method: 将Zhai等人的ℓ⁴范数最大化完全字典学习框架扩展到复数域，提出两种学习算法，用于评估DFT的最优性并学习改进的稀疏变换。

Result: 建立了ℓ⁴范数最大化的理论基础，开发了两种学习算法，通过理论和仿真评估DFT的最优性，并为实际和合成信道向量学习到改进的稀疏变换。

Conclusion: DFT可能不是毫米波大规模MIMO系统中最优的稀疏变换，通过ℓ⁴范数最大化学习的复数域稀疏变换能够提供更好的稀疏表示性能。

Abstract: The high directionality of wave propagation at millimeter-wave (mmWave) carrier frequencies results in only a small number of significant transmission paths between user equipments and the basestation (BS). This sparse nature of wave propagation is revealed in the beamspace domain, which is traditionally obtained by taking the spatial discrete Fourier transform (DFT) across a uniform linear antenna array at the BS, where each DFT output is associated with a distinct beam. In recent years, beamspace processing has emerged as a promising technique to reduce baseband complexity and power consumption in all-digital massive multiuser (MU) multiple-input multiple-output (MIMO) systems operating at mmWave frequencies. However, it remains unclear whether the DFT is the optimal sparsifying transform for finite-dimensional antenna arrays. In this paper, we extend the framework of Zhai et al. for complete dictionary learning via $\ell^4$-norm maximization to the complex case in order to learn new sparsifying transforms. We provide a theoretical foundation for $\ell^4$-norm maximization and propose two suitable learning algorithms. We then utilize these algorithms (i) to assess the optimality of the DFT for sparsifying channel vectors theoretically and via simulations and (ii) to learn improved sparsifying transforms for real-world and synthetically generated channel vectors.

</details>


### [17] [Refinements of Jensen's Inequality for Twice-Differentiable Convex Functions with Bounded Hessian](https://arxiv.org/abs/2601.05030)
*Sambhab Mishra*

Main category: cs.IT

TL;DR: 本文针对具有有界Hessian矩阵的二阶可微函数，建立了Jensen不等式的严格改进，通过积分余项的泰勒展开和Gruss型不等式，将偏度和峰度纳入误差项，改进了连续分布香农熵和瑞利衰落信道遍历容量的估计。


<details>
  <summary>Details</summary>
Motivation: Jensen不等式是凸分析中的基本结果，但传统方差界精度有限。作者希望建立更精确的改进，特别是针对具有有界Hessian的二阶可微函数，以填补经典方差界与高精度估计之间的差距。

Method: 使用积分余项的泰勒展开，结合Gruss型不等式推导显式误差项，将偏度和峰度等高阶矩信息纳入边界估计中。

Result: 建立了Jensen不等式的严格改进形式，获得了由Gruss型不等式控制的显式误差项，成功改进了连续分布香农熵和瑞利衰落信道遍历容量的现有估计。

Conclusion: 通过引入高阶矩信息（偏度和峰度）到Jensen不等式的改进中，获得了更精确的估计方法，并在信息论和通信理论的实际应用中验证了其有效性。

Abstract: Jensen's inequality, attributed to Johan Jensen -- a Danish mathematician and engineer noted for his contributions to the theory of functions -- is a ubiquitous result in convex analysis, providing a fundamental lower bound for the expectation of a convex function. In this paper, we establish rigorous refinements of this inequality specifically for twice-differentiable functions with bounded Hessians. By utilizing Taylor expansions with integral remainders, we tried to bridge the gap between classical variance-based bounds and higher-precision estimates. We also discover explicit error terms governed by Gruss-type inequalities, allowing for the incorporation of skewness and kurtosis into the bound. Using these new theoretical tools, we improve upon existing estimates for the Shannon entropy of continuous distributions and the ergodic capacity of Rayleigh fading channels, demonstrating the practical efficacy of our refinements.

</details>


### [18] [Precoding Matrix Indicator in the 5G NR Protocol: A Tutorial on 3GPP Beamforming Codebooks](https://arxiv.org/abs/2601.05092)
*Boyu Ning,Haifan Yin,Sixu Liu,Hao Deng,Songjie Yang,Yuchen Zhang,Weidong Mei,David Gesbert,Jaebum Park,Robert W. Heath,Emil Björnson*

Main category: cs.IT

TL;DR: 该论文系统性地分析了5G NR中的波束赋形码本技术（PMI），从理论、标准化和实现三个角度全面探讨了3GPP标准中码本的设计原理、演进历程和实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有标准化文档中对波束赋形码本技术的描述往往晦涩难懂，缺乏系统的理论解释和直观的物理意义阐释，这给研究人员和工程师在实际应用中带来了困难。论文旨在填补这一空白，为学术界和工业界提供一个统一的理解框架。

Method: 论文采用多角度分析方法：1) 介绍MIMO系统中波束赋形的背景和5G系统中的码本信令流程；2) 建立3GPP标准中常规码本和端口选择码本的基础理论；3) 对3GPP Release 15-18的码本演进进行严格技术分析，包括设计原理、符号变量的物理解释和可视化说明；4) 通过数学建模、性能基准测试、反馈比较和场景适用性分析进行综合评估。

Result: 论文提供了对5G NR波束赋形码本技术的系统性理解，包括：1) 清晰解释了码本设计的核心原理；2) 以表格形式总结了码本公式中每个符号变量的物理含义；3) 提供了直观的可视化说明来解释码本参数如何传递信息；4) 建立了实际系统中波束赋形码本的统一理解框架。

Conclusion: 该工作既是一个信息丰富的教程，也是未来研究的指导，有助于促进学术界和工业界在无线通信技术发展方面更有效的合作。论文还指出了未来研究方向和其他波束赋形场景，为持续的研究和开发工作提供了方向。

Abstract: This paper bridges this critical gap by providing a systematic examination of the beamforming codebook technology, i.e., precoding matrix indicator (PMI), in the 5G NR from theoretical, standardization, and implementation perspectives. We begin by introducing the background of beamforming in multiple-input multiple-output (MIMO) systems and the signaling procedures for codebook-based beamforming in practical 5G systems. Then, we establish the fundamentals of regular codebooks and port-selection codebooks in 3GPP standards. Next, we provide rigorous technical analysis of 3GPP codebook evolution spanning Releases 15-18, with particular focus on: 1) We elucidate the core principles underlying codebook design, 2) provide clear physical interpretations for each symbolic variable in the codebook formulas, summarized in tabular form, and 3) offer intuitive visual illustrations to explain how codebook parameters convey information. These essential pedagogical elements are almost entirely absent in the often-obscure standardization documents. Through mathematical modeling, performance benchmarking, feedback comparisons, and scenario-dependent applicability analysis, we provide researchers and engineers with a unified understanding of beamforming codebooks in real-world systems. Furthermore, we identify future directions and other beamforming scenarios for ongoing research and development efforts. This work serves as both an informative tutorial and a guidance for future research, facilitating more effective collaboration between academia and industry in advancing wireless communication technologies.

</details>


### [19] [Fundamental Tradeoffs for ISAC Multiple Access in Finite-Blocklength Regime](https://arxiv.org/abs/2601.05165)
*Zhentian Zhang,Christos Masouros,Kai-Kit Wong,Jian Dang,Zaichen Zhang,Kaitao Meng,Farshad Rostami Ghadi,Mohammad Javad Ahmadi*

Main category: cs.IT

TL;DR: 研究有限块长约束下上行链路双功能ISAC多址接入的通信-感知权衡，建立了感知误差的几何分解，推导了FBL体制下的可达性和逆界，并通过数值验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 传统渐近分析无法准确描述短包和低延迟传输下的有限块长约束，需要研究双功能ISAC在FBL体制下的通信-感知基本权衡关系。

Method: 通过无偏信道状态感知估计器建立感知误差的几何分解，推导通信码率与感知精度权衡的可达性和逆界，并建立信道估计精度与实际感知参数的通用Cramér-Rao界联系。

Result: 感知误差由信噪比和信息码本相关结构共同决定，揭示了码本几何中活跃用户间的互相关如何约束双功能ISAC性能，数值结果验证了块长、天线维度和感知需求的影响。

Conclusion: 在FBL约束下，双功能ISAC性能受码本几何相关结构显著影响，建立了通信-感知权衡的理论框架，为实际系统设计提供了指导。

Abstract: This paper investigates the fundamental communication--sensing tradeoffs of uplink dual-functional integrated sensing and communication (ISAC) multiple access under finite blocklength (FBL) constraints. Unlike conventional asymptotic analyses, we explicitly account for the limitations under FBL constraints imposed by short packets and low-latency transmission. By examining the unbiased channel state sensing estimator, we establish a geometric decomposition of the sensing error, indicating that it is jointly determined by the signal-to-noise ratio and the correlation structure of the information codebook. This insight reveals how cross-correlation among active users in the codebook geometry fundamentally constrains dual-functional ISAC performance. Consequently, we derive achievability and converse bounds that characterize the tradeoff between communication code rate and sensing accuracy in the FBL regime, with the converse further bounded by Shannon capacity. Moreover, by treating channel state sensing as a high-level sensing objective, a universal Cramér--Rao bound is derived to link channel estimation accuracy to practical sensing parameters. Examples of parameter sensing are also provided based on 3GPP standard. Numerical results validate the theoretical analysis and demonstrate the impact of blocklength, antenna dimensions, and sensing requirements.

</details>


### [20] [Information-Theoretic Limits on Exact Subgraph Alignment Problem](https://arxiv.org/abs/2601.05173)
*Chun Hei Michael Shiu,Hei Victor Cheng,Lele Wang*

Main category: cs.IT

TL;DR: 本文提出了子图对齐问题，旨在从大图中恢复小图模式的顶点集和对应关系，解决了现有图对齐算法无法直接处理子图定位场景的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图对齐研究主要关注两个图共享相同顶点集的情况，但在计算机视觉、社交网络分析和生物信息学等实际应用中，经常需要在大图中定位小图模式。现有算法无法直接处理这种子图定位场景，因此需要新的理论框架。

Method: 提出了子图对齐问题的正式表述，引入了Erdos-Renyi子图对模型，并建立了适当的重建准则。建立了几乎严格的信息论结果，并提出了一些新颖的分析方法。

Result: 建立了子图对齐问题的几乎严格的信息论结果，表明在某些条件下可以恢复子图的顶点集和对应关系。当小图模式是大图的诱导子图且需要同时恢复顶点集和对应关系时，问题简化为NP完全的图同构问题。

Conclusion: 子图对齐问题为实际应用中的图模式定位提供了理论框架，填补了现有图对齐研究的空白。提出的信息论结果为该问题的可解性提供了理论基础，并为未来算法设计提供了指导。

Abstract: The graph alignment problem aims to identify the vertex correspondence between two correlated graphs. Most existing studies focus on the scenario in which the two graphs share the same vertex set. However, in many real-world applications, such as computer vision, social network analysis, and bioinformatics, the task often involves locating a small graph pattern within a larger graph. Existing graph alignment algorithms and analysis cannot directly address these scenarios because they are not designed to identify the specific subset of vertices where the small graph pattern resides within the larger graph. Motivated by this limitation, we introduce the subgraph alignment problem, which seeks to recover both the vertex set and/or the vertex correspondence of a small graph pattern embedded in a larger graph. In the special case where the small graph pattern is an induced subgraph of the larger graph and both the vertex set and correspondence are to be recovered, the problem reduces to the subgraph isomorphism problem, which is NP-complete in the worst case. In this paper, we formally formulate the subgraph alignment problem by proposing the Erdos-Renyi subgraph pair model together with some appropriate recovery criterion. We then establish almost-tight information-theoretic results for the subgraph alignment problem and present some novel approaches for the analysis.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [AHA: Scalable Alternative History Analysis for Operational Timeseries Applications](https://arxiv.org/abs/2601.04432)
*Harshavardhan Kamarthi,Harshil Shah,Henry Milner,Sayan Sinha,Yan Li,B. Aditya Prakash,Vyas Sekar*

Main category: cs.DB

TL;DR: AHA系统为高维时序数据的替代历史分析提供低成本高保真解决方案，相比传统方法降低85%总拥有成本


<details>
  <summary>Details</summary>
Motivation: 运营商和分析师需要对高维时序数据（如用户体验指标）进行回顾性分析，但传统数据处理方案要么运营成本高，要么无法保证准确重放

Method: 设计AHA系统，基于三个关键洞察：1) 底层统计的可分解性；2) 属性值组合中子群活跃度的稀疏性；3) 现代分析数据库中聚合操作的高效结构

Result: 在多个真实数据集和大型视频分析公司生产流水线的案例研究中，AHA为广泛下游任务提供100%准确度，相比传统方法降低高达85%的总拥有成本（计算+存储）

Conclusion: AHA系统成功解决了高维时序数据替代历史分析的成本效率和保真度问题，为运营商提供了实用的解决方案

Abstract: Many operational systems collect high-dimensional timeseries data about users/systems on key performance metrics. For instance, ISPs, content distribution networks, and video delivery services collect quality of experience metrics for user sessions associated with metadata (e.g., location, device, ISP). Over such historical data, operators and data analysts often need to run retrospective analysis; e.g., analyze anomaly detection algorithms, experiment with different configurations for alerts, evaluate new algorithms, and so on. We refer to this class of workloads as alternative history analysis for operational datasets. We show that in such settings, traditional data processing solutions (e.g., data warehouses, sampling, sketching, big-data systems) either pose high operational costs or do not guarantee accurate replay. We design and implement a system, called AHA (Alternative History Analytics), that overcomes both challenges to provide cost efficiency and fidelity for high-dimensional data. The design of AHA is based on analytical and empirical insights about such workloads: 1) the decomposability of underlying statistics; 2) sparsity in terms of active number of subpopulations over attribute-value combinations; and 3) efficiency structure of aggregation operations in modern analytics databases. Using multiple real-world datasets and as well as case-studies on production pipelines at a large video analytics company, we show that AHA provides 100% accuracy for a broad range of downstream tasks and up to 85x lower total cost of ownership (i.e., compute + storage) compared to conventional methods.

</details>


### [22] [Does Provenance Interact?](https://arxiv.org/abs/2601.04722)
*Chrysanthi Kosyfaki,Ruiyuan Zhang,Nikos Mamoulis,Xiaofang Zhou*

Main category: cs.DB

TL;DR: 本文提出使用时间交互网络(TINs)高效表示时间溯源信息，解决流式系统中溯源图随数据量超线性增长的可扩展性问题，为大规模数据流提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 数据溯源在数据库查询解释和科学工作流审计等领域有广泛应用，但传统溯源追踪面临计算成本和存储开销的挑战。在流式系统中，溯源图会随数据量超线性增长，存在显著的可扩展性问题。现有时间溯源方法主要关注系统级调试，在数据管理应用方面存在空白。

Method: 提出使用时间交互网络(TINs)表示时间溯源信息，将数据分为离散型和流动型两种类型，定义五种时间溯源查询类型，并提出基于状态的索引方法。该方法适用于流式系统、交通网络和金融网络等多种场景。

Result: 展示了TINs在多个领域的适用性，包括流式系统、交通网络和金融网络。提出的分类、查询类型和索引方法为时间溯源的实际应用提供了框架。

Conclusion: 本文提出了一个研究议程，旨在使时间溯源成为大规模数据流的实用工具。通过TINs表示、数据分类、查询定义和索引方法，为解决溯源追踪的可扩展性挑战提供了方向。

Abstract: Data provenance (the process of determining the origin and derivation of data outputs) has applications across multiple domains including explaining database query results and auditing scientific workflows. Despite decades of research, provenance tracing remains challenging due to computational costs and storage overhead. In streaming systems such as Apache Flink, provenance graphs can grow super-linearly with data volume, posing significant scalability challenges. Temporal provenance is a promising direction, attaching timestamps to provenance information, enabling time-focused queries without maintaining complete historical records. However, existing temporal provenance methods primarily focus on system-level debugging, leaving a gap in data management applications. This paper proposes an agenda that uses Temporal Interaction Networks (TINs) to represent temporal provenance efficiently. We demonstrate TINs' applicability across streaming systems, transportation networks, and financial networks. We classify data into discrete and liquid types, define five temporal provenance query types, and propose a state-based indexing approach. Our vision outlines research directions toward making temporal provenance a practical tool for large-scale dataflows.

</details>


### [23] [Structural Indexing of Relational Databases for the Evaluation of Free-Connex Acyclic Conjunctive Queries](https://arxiv.org/abs/2601.04757)
*Cristian Riveros,Benjamin Scheidt,Nicole Schweikardt*

Main category: cs.DB

TL;DR: 提出基于结构对称性的索引方法，用于高效评估自由连接无环连接查询，预处理时间与辅助数据库大小线性相关，后者通常远小于原数据库。


<details>
  <summary>Details</summary>
Motivation: 现有索引方法主要基于值或顺序（如B+树），无法有效利用数据库内部的结构对称性。需要一种能利用结构对称性来提升自由连接无环连接查询评估效率的索引方法。

Method: 构建基于结构对称性的索引结构，核心是构建辅助数据库D_col，其大小与Scheidt和Schweikardt的"关系颜色细化"算法分配的颜色数量相关。在图中，这对应于图的最小公平划分大小。

Result: 对于任何自由连接无环连接查询，可以在与D_col大小线性的预处理时间后，以常数延迟计数或枚举查询结果。D_col大小通常远小于原数据库，如二叉树中对数大小，正则图中常数大小。

Conclusion: 这是首个基于数据库内部结构对称性的索引基础结果，能够以可能严格小于数据库大小的性能评估所有自由连接无环连接查询，为高效查询处理提供了新方向。

Abstract: We present an index structure to boost the evaluation of free-connex acyclic conjunctive queries (fc-ACQs) over relational databases. The main ingredient of the index associated with a given database $D$ is an auxiliary database $D_{col}$. Our main result states that for any fc-ACQ $Q$ over $D$, we can count the number of answers of $Q$ or enumerate them with constant delay after a preprocessing phase that takes time linear in the size of $D_{col}$.
  Unlike previous indexing methods based on values or order (e.g., B+ trees), our index is based on structural symmetries among tuples in a database, and the size of $D_{col}$ is related to the number of colors assigned to $D$ by Scheidt and Schweikardt's "relational color refinement" (2025). In the particular case of graphs, this coincides with the minimal size of an equitable partition of the graph. For example, the size of $D_{col}$ is logarithmic in the case of binary trees and constant for regular graphs. Even in the worst-case that $D$ has no structural symmetries among tuples at all, the size of $D_{col}$ is still linear in the size of $D$.
  Given that the size of $D_{col}$ is bounded by the size of $D$ and can be much smaller (even constant for some families of databases), our index is the first foundational result on indexing internal structural symmetries of a database to evaluate all fc-ACQs with performance potentially strictly smaller than the database size.

</details>


### [24] [LGTD: Local-Global Trend Decomposition for Season-Length-Free Time Series Analysis](https://arxiv.org/abs/2601.04820)
*Chotanansub Sophaken,Thanadej Rattanakornphan,Piyanon Charoenpoonpanich,Thanapol Phungtua-eng,Chainarong Amornbunchornvej*

Main category: cs.DB

TL;DR: LGTD是一种无需指定季节长度的时序分解框架，将时间序列表示为全局趋势、自适应局部趋势（其重复出现形成隐含的季节结构）和残差项的和，适用于不规则、漂移或弱周期性结构的时间序列。


<details>
  <summary>Details</summary>
Motivation: 现有季节-趋势分解方法大多依赖用户指定或估计的季节长度，并隐含假设稳定的周期结构，这限制了在大型异构集合中的鲁棒性和可部署性，因为重复模式可能漂移、间歇出现或存在于多个时间尺度。

Method: LGTD首先估计捕获长期演化的全局趋势，然后应用AutoTrend（自适应误差驱动的局部线性趋势推断模块）将去趋势信号分割为短期分段线性区间，通过局部趋势的重复出现形成隐含的季节结构，最后获得去除全局和局部趋势后的残差。

Result: LGTD在计算复杂度上在线性条件下随序列长度线性扩展，在合成基准测试中，在固定、过渡和可变季节长度设置下表现出稳健且平衡的分解性能，特别是在基于周期的方法性能下降的情况下。

Conclusion: 通过消除手动指定季节长度的需求，LGTD支持在具有不规则、漂移或弱周期性结构的时间序列上实现自动化、低接触部署，将季节性结构视为局部趋势区间重复出现而产生的涌现属性。

Abstract: Time series decomposition into trend, seasonal structure, and residual components is a core primitive for downstream analytics such as anomaly detection, change-point detection, and forecasting. However, most existing seasonal-trend decomposition methods rely on user-specified or estimated season lengths and implicitly assume stable periodic structure. These assumptions limit robustness and deployability in large, heterogeneous collections where recurring patterns may drift, appear intermittently, or exist at multiple time scales.
  We propose LGTD (Local-Global Trend Decomposition), a season-length-free decomposition framework that represents a time series as the sum of a smooth global trend, adaptive local trends whose recurrence induces implicit (emergent) seasonal structure, and a residual component. Rather than explicitly modeling seasonality through a fixed or estimated period, LGTD treats seasonal structure as an emergent property arising from repeated local trend regimes. Concretely, LGTD first estimates a global trend capturing long-term evolution, then applies AutoTrend, an adaptive error-driven local linear trend inference module, to segment the detrended signal into short-lived piecewise-linear regimes. Residuals are obtained after removing both global and local trends.
  By eliminating manual season-length specification, LGTD supports automated, low-touch deployment across time series with irregular, drifting, or weakly periodic structure. We analyze computational complexity and show that LGTD scales linearly with series length under mild conditions. Experiments on synthetic benchmarks demonstrate robust and balanced decomposition performance across fixed, transitive, and variable season-length settings, especially where period-based methods degrade.

</details>


### [25] [Responsibility Measures for Conjunctive Queries with Negation](https://arxiv.org/abs/2601.04868)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.DB

TL;DR: 该论文研究数据库查询中事实对查询结果贡献的责任度量，特别针对带否定原子的联合合取查询(UCQ¬)，提出了两种责任度量方法，并分析了其计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有责任度量研究主要集中在单调查询上，缺乏对带否定原子的联合合取查询(UCQ¬)的责任度量方法。需要定义合理的解释或相关性概念来处理包含否定原子的查询。

Method: 提出了两种方法：一种仅对正数据库事实分配分数，另一种同时考虑否定事实。这些方法可以将先前研究的drastic Shapley和WSMS分数从单调查询扩展到UCQ¬查询。

Result: 分析了所得度量的数据和组合复杂度，特别证明了WSMS度量对所有UCQ¬查询在数据复杂度上是可处理的，并对合适的带否定的合取查询类在组合复杂度上也是可处理的。

Conclusion: 该研究扩展了责任度量到带否定原子的查询，提供了两种实用的方法，并建立了重要的计算复杂度结果，为UCQ¬查询的解释性分析提供了理论基础。

Abstract: We contribute to the recent line of work on responsibility measures that quantify the contributions of database facts to obtaining a query result. In contrast to existing work which has almost exclusively focused on monotone queries, here we explore how to define responsibility measures for unions of conjunctive queries with negated atoms (UCQ${}^\lnot$). Starting from the question of what constitutes a reasonable notion of explanation or relevance for queries with negated atoms, we propose two approaches, one assigning scores to (positive) database facts and the other also considering negated facts. Our approaches, which are orthogonal to the previously studied score of Reshef et al., can be used to lift previously studied scores for monotone queries, known as drastic Shapley and weighted sums of minimal supports (WSMS), to UCQ$^\lnot$. We investigate the data and combined complexity of the resulting measures, notably showing that the WSMS measures are tractable in data complexity for all UCQ${}^\lnot$ queries and further establishing tractability in combined complexity for suitable classes of conjunctive queries with negation.

</details>


### [26] [Rule Rewriting Revisited: A Fresh Look at Static Filtering for Datalog and ASP](https://arxiv.org/abs/2601.05108)
*Philipp Hanisch,Markus Krötzsch*

Main category: cs.DB

TL;DR: 静态过滤是一种数据无关的Datalog优化方法，推广了关系数据库中的代数查询重写技术。本文回顾并扩展了该方法，使其适用于答案集编程，提出了可处理的近似算法来改善实际性能。


<details>
  <summary>Details</summary>
Motivation: 尽管静态过滤方法早在1986年就被Kifer和Lozinskii发现，但该方法在近期研究和系统开发中被忽视，其特例被独立重新发现。因此需要回顾原始方法，使用更新术语和更通用的过滤器谓词来捕捉现代系统特性，并将其扩展到答案集编程。

Method: 使用更新术语和更通用的过滤器谓词来重新表述原始静态过滤方法，将其扩展到答案集编程。由于扩展后的方法复杂度较高（双指数级），提出了可处理的近似算法，这些近似算法在典型情况下仍能显著改进逻辑程序性能。

Result: 扩展后的方法比经典方法更通用但也更复杂：一般情况下是双指数级复杂度，即使对于有界元数的谓词也是单指数级。提出的可处理近似算法在实际数据上的规则系统性能可以提升一个数量级。

Conclusion: 静态过滤作为一种数据无关的优化方法，通过适当的扩展和近似处理，可以显著改善Datalog和答案集编程系统的性能，值得在现代系统中重新关注和应用。

Abstract: Static filtering is a data-independent optimisation method for Datalog, which generalises algebraic query rewriting techniques from relational databases. In spite of its early discovery by Kifer and Lozinskii in 1986, the method has been overlooked in recent research and system development, and special cases are being rediscovered independently. We therefore recall the original approach, using updated terminology and more general filter predicates that capture features of modern systems, and we show how to extend its applicability to answer set programming (ASP). The outcome is strictly more general but also more complex than the classical approach: double exponential in general and single exponential even for predicates of bounded arity. As a solution, we propose tractable approximations of the algorithm that can still yield much improved logic programs in typical cases, e.g., it can improve the performance of rule systems over real-world data in the order of magnitude.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [27] [Correct and Weight: A Simple Yet Effective Loss for Implicit Feedback Recommendation](https://arxiv.org/abs/2601.04291)
*Minglei Yin,Chuanbo Hu,Bin Liu,Neil Zhenqiang Gong,Yanfang,Ye,Xin Li*

Main category: cs.IR

TL;DR: 提出名为CW损失的新损失函数，通过修正负样本分布和动态重加权机制，解决推荐系统中隐式反馈的假阴性问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统普遍采用隐式反馈学习，但面临假阴性的持续挑战——未观测到的用户-物品交互不一定表示负面偏好。现有方法未能系统性地在训练目标中修正假阴性的影响。

Method: 提出CW损失函数，包含两个关键技术：1) 基于正-未标记学习思想，通过可观测的通用数据分布和正交互分布来近似真实负分布，修正负采样过程的偏差；2) 引入动态重加权机制，根据模型当前预测调整每个负样本的重要性，对置信度高的负样本施加更大排序间隔，同时降低对可能是假阴性的不确定负样本的惩罚。

Result: 在四个大规模稀疏基准数据集上的广泛实验表明，该方法在多个排序导向指标上一致且显著优于一系列最先进的损失函数。

Conclusion: CW损失函数优雅高效，无需对数据采样过程进行复杂修改或显著增加计算开销，可广泛应用于现有推荐模型，有效解决隐式反馈中的假阴性问题。

Abstract: Learning from implicit feedback has become the standard paradigm for modern recommender systems. However, this setting is fraught with the persistent challenge of false negatives, where unobserved user-item interactions are not necessarily indicative of negative preference. To address this issue, this paper introduces a novel and principled loss function, named Corrected and Weighted (CW) loss, that systematically corrects for the impact of false negatives within the training objective. Our approach integrates two key techniques. First, inspired by Positive-Unlabeled learning, we debias the negative sampling process by re-calibrating the assumed negative distribution. By theoretically approximating the true negative distribution (p-) using the observable general data distribution (p) and the positive interaction distribution (p^+), our method provides a more accurate estimate of the likelihood that a sampled unlabeled item is truly negative. Second, we introduce a dynamic re-weighting mechanism that modulates the importance of each negative instance based on the model's current prediction. This scheme encourages the model to enforce a larger ranking margin between positive items and confidently predicted (i.e., easy) negative items, while simultaneously down-weighting the penalty on uncertain negatives that have a higher probability of being false negatives. A key advantage of our approach is its elegance and efficiency; it requires no complex modifications to the data sampling process or significant computational overhead, making it readily applicable to a wide array of existing recommendation models. Extensive experiments conducted on four large-scale, sparse benchmark datasets demonstrate the superiority of our proposed loss. The results show that our method consistently and significantly outperforms a suite of state-of-the-art loss functions across multiple ranking-oriented metrics.

</details>


### [28] [The Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval](https://arxiv.org/abs/2601.04395)
*Tomer Wullach,Ori Shapira,Amir DN Cohen*

Main category: cs.IR

TL;DR: 该研究分析了多语言密集检索中，如何利用分级相关性分数及其二值化阈值来优化模型性能，发现最优阈值因语言和任务而异，合理选择阈值可提升效果、减少训练数据需求并缓解标注噪声。


<details>
  <summary>Details</summary>
Motivation: 密集检索模型通常使用需要二值相关性判断的对比学习目标进行微调，但相关性本质上是分级的。目前分级相关性信号在密集检索中未被充分利用，且二值化阈值的选择对多语言检索性能的影响尚不明确。

Method: 使用带有LLM标注的分级相关性分数的多语言数据集，分析分级相关性分数及其二值化阈值对单语言、多语言混合和跨语言检索场景的影响，系统研究不同阈值选择对模型性能的影响。

Result: 研究发现最优阈值在不同语言和任务中系统性变化，通常反映资源水平的差异。合理选择的阈值可以提升检索效果、减少微调所需数据量并缓解标注噪声，而不当阈值则会降低性能。

Conclusion: 分级相关性是密集检索中宝贵但未被充分利用的信号，阈值校准应被视为微调流程中一个原则性的组成部分，需要针对不同语言和任务进行系统优化。

Abstract: Dense retrieval models are typically fine-tuned with contrastive learning objectives that require binary relevance judgments, even though relevance is inherently graded. We analyze how graded relevance scores and the threshold used to convert them into binary labels affect multilingual dense retrieval. Using a multilingual dataset with LLM-annotated relevance scores, we examine monolingual, multilingual mixture, and cross-lingual retrieval scenarios. Our findings show that the optimal threshold varies systematically across languages and tasks, often reflecting differences in resource level. A well-chosen threshold can improve effectiveness, reduce the amount of fine-tuning data required, and mitigate annotation noise, whereas a poorly chosen one can degrade performance. We argue that graded relevance is a valuable but underutilized signal for dense retrieval, and that threshold calibration should be treated as a principled component of the fine-tuning pipeline.

</details>


### [29] [Re-Rankers as Relevance Judges](https://arxiv.org/abs/2601.04455)
*Chuan Meng,Jiqun Liu,Mohammad Aliannejadi,Fengran Mo,Jeff Dalton,Maarten de Rijke*

Main category: cs.IR

TL;DR: 研究探索将重排序器用作相关性判断工具，发现重排序器在40-50%情况下能超越现有LLM相关性判断方法，但存在自偏好和跨家族偏见。


<details>
  <summary>Details</summary>
Motivation: 目前使用LLM预测相关性判断的研究大多将其视为独立任务，专注于提示设计。然而，预测相关性判断本质上是相关性预测问题，这在重排序任务中已有深入研究。现有研究很少探索重用或调整成熟的重排序方法来预测相关性判断，导致资源浪费和重复开发。

Method: 在重排序器作为相关性判断器的框架下，设计了两种适应策略：(1) 使用重排序器生成的二元标记（如"true"和"false"）作为直接判断；(2) 通过阈值化将连续的重排序分数转换为二元标签。在TREC-DL 2019-2023数据集上进行了广泛实验，使用了来自3个家族的8个重排序器，参数量从220M到32B不等。

Result: 结果显示，基于重排序器的相关性判断器在两种策略下，在约40%到50%的情况下能够超越UMBRELA（最先进的基于LLM的相关性判断器）。同时，这些判断器表现出强烈的自偏好（偏向自己和同家族的重排序器）以及跨家族偏见。

Conclusion: 重排序器可以作为有效的相关性判断工具，在多数情况下性能优于专门的LLM方法。然而，其存在的自偏好和偏见问题需要在应用中加以考虑。这项研究为相关性判断任务提供了新的视角，减少了资源浪费和重复开发。

Abstract: Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., "true" and "false") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.

</details>


### [30] [Self-MedRAG: a Self-Reflective Hybrid Retrieval-Augmented Generation Framework for Reliable Medical Question Answering](https://arxiv.org/abs/2601.04531)
*Jessica Ryan,Alexander I. Gumilang,Robert Wiliam,Derwin Suhartono*

Main category: cs.IR

TL;DR: Self-MedRAG：结合混合检索与自我反思的临床推理框架，通过迭代假设验证减少幻觉，在医学QA任务中显著提升准确性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学问答中存在幻觉和缺乏依据的推理问题，传统单次检索无法处理需要多步推理的复杂生物医学查询，限制了临床场景下的可靠性

Method: 提出Self-MedRAG框架：1）混合检索策略结合稀疏检索（BM25）和密集检索（Contriever），通过RRF融合；2）生成器产生答案和推理依据；3）轻量级自我反思模块使用NLI或LLM验证；4）若依据不足则自主重写查询并迭代优化

Result: 在MedQA和PubMedQA基准测试中，混合检索显著优于单检索器基线。自我反思循环带来显著提升：MedQA准确率从80.00%提升至83.33%，PubMedQA从69.10%提升至79.82%

Conclusion: 混合检索与基于证据的迭代自我反思相结合，能有效减少无依据的断言，增强基于LLM系统的临床可靠性，为高风险的临床场景提供了更可靠的解决方案

Abstract: Large Language Models (LLMs) have demonstrated significant potential in medical Question Answering (QA), yet they remain prone to hallucinations and ungrounded reasoning, limiting their reliability in high-stakes clinical scenarios. While Retrieval-Augmented Generation (RAG) mitigates these issues by incorporating external knowledge, conventional single-shot retrieval often fails to resolve complex biomedical queries requiring multi-step inference. To address this, we propose Self-MedRAG, a self-reflective hybrid framework designed to mimic the iterative hypothesis-verification process of clinical reasoning. Self-MedRAG integrates a hybrid retrieval strategy, combining sparse (BM25) and dense (Contriever) retrievers via Reciprocal Rank Fusion (RRF) to maximize evidence coverage. It employs a generator to produce answers with supporting rationales, which are then assessed by a lightweight self-reflection module using Natural Language Inference (NLI) or LLM-based verification. If the rationale lacks sufficient evidentiary support, the system autonomously reformulates the query and iterates to refine the context. We evaluated Self-MedRAG on the MedQA and PubMedQA benchmarks. The results demonstrate that our hybrid retrieval approach significantly outperforms single-retriever baselines. Furthermore, the inclusion of the self-reflective loop yielded substantial gains, increasing accuracy on MedQA from 80.00% to 83.33% and on PubMedQA from 69.10% to 79.82%. These findings confirm that integrating hybrid retrieval with iterative, evidence-based self-reflection effectively reduces unsupported claims and enhances the clinical reliability of LLM-based systems.

</details>


### [31] [Exploring Recommender System Evaluation: A Multi-Modal User Agent Framework for A/B Testing](https://arxiv.org/abs/2601.04554)
*Wenlin Zhang,Xiangyang Li,Qiyuan Ge,Kuicai Dong,Pengyue Jia,Xiaopeng Li,Zijian Zhang,Maolin Wang,Yichao Wang,Huifeng Guo,Ruiming Tang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出A/B Agent多模态用户代理，通过构建推荐沙箱环境和模拟真实用户行为，替代传统在线A/B测试，降低经济成本和时间需求


<details>
  <summary>Details</summary>
Motivation: 传统在线A/B测试存在经济成本高、用户体验下降和时间需求大等问题，而现有LLM代理缺乏真实环境和视觉感知能力，无法模拟真实用户交互

Method: 构建推荐沙箱环境支持多模态多页面交互，设计代理具备多模态信息感知、细粒度用户偏好、档案整合、行动记忆检索和疲劳系统，模拟复杂人类决策

Result: 从模型、数据和特征三个角度验证了A/B Agent替代传统A/B测试的潜力，发现其生成的数据能有效增强推荐模型能力

Conclusion: A/B Agent为解决传统在线A/B测试挑战提供了有效方案，展示了LLM代理在推荐系统评估中的实际应用价值

Abstract: In recommender systems, online A/B testing is a crucial method for evaluating the performance of different models. However, conducting online A/B testing often presents significant challenges, including substantial economic costs, user experience degradation, and considerable time requirements. With the Large Language Models' powerful capacity, LLM-based agent shows great potential to replace traditional online A/B testing. Nonetheless, current agents fail to simulate the perception process and interaction patterns, due to the lack of real environments and visual perception capability. To address these challenges, we introduce a multi-modal user agent for A/B testing (A/B Agent). Specifically, we construct a recommendation sandbox environment for A/B testing, enabling multimodal and multi-page interactions that align with real user behavior on online platforms. The designed agent leverages multimodal information perception, fine-grained user preferences, and integrates profiles, action memory retrieval, and a fatigue system to simulate complex human decision-making. We validated the potential of the agent as an alternative to traditional A/B testing from three perspectives: model, data, and features. Furthermore, we found that the data generated by A/B Agent can effectively enhance the capabilities of recommendation models. Our code is publicly available at https://github.com/Applied-Machine-Learning-Lab/ABAgent.

</details>


### [32] [Adaptive Retrieval for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.04618)
*Jongho Kim,Jaeyoung Kim,Seung-won Hwang,Jihyuk Kim,Yu Jin Kim,Moontae Lee*

Main category: cs.IR

TL;DR: REPAIR框架通过将推理计划重新用作自适应检索的密集反馈信号，解决推理密集型检索中"桥梁文档"召回不足的问题，相比基线提升5.6%


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的重新排序管道试图在排序中呈现"桥梁文档"（对推理过程有贡献但不直接相关初始查询的文档），但存在召回率受限的问题。朴素的自适应检索解决方案常导致规划错误传播。

Method: 提出REPAIR框架，通过将推理计划重新用作密集反馈信号进行自适应检索。关键区别在于通过选择性自适应检索实现重新排序过程中的中途修正，检索支持关键计划的文档。

Result: 在推理密集型检索和复杂QA任务上的实验结果表明，该方法优于现有基线5.6个百分点。

Conclusion: REPAIR框架通过利用推理计划作为反馈信号进行选择性自适应检索，有效解决了推理密集型检索中桥梁文档召回不足的问题，显著提升了检索性能。

Abstract: We study leveraging adaptive retrieval to ensure sufficient "bridge" documents are retrieved for reasoning-intensive retrieval. Bridge documents are those that contribute to the reasoning process yet are not directly relevant to the initial query. While existing reasoning-based reranker pipelines attempt to surface these documents in ranking, they suffer from bounded recall. Naive solution with adaptive retrieval into these pipelines often leads to planning error propagation. To address this, we propose REPAIR, a framework that bridges this gap by repurposing reasoning plans as dense feedback signals for adaptive retrieval. Our key distinction is enabling mid-course correction during reranking through selective adaptive retrieval, retrieving documents that support the pivotal plan. Experimental results on reasoning-intensive retrieval and complex QA tasks demonstrate that our method outperforms existing baselines by 5.6%pt.

</details>


### [33] [Succeeding at Scale: Automated Multi-Retriever Fusion and Query-Side Adaptation for Multi-Tenant Search](https://arxiv.org/abs/2601.04646)
*Prateek Jain,Shabari S Nair,Ritesh Goru,Prakhar Agarwal,Ajay Yadav,Yoga Sri Varshan Varadharajan,Constantine Caramanis*

Main category: cs.IR

TL;DR: 提出DevRev Search基准测试和Index-Preserving Adaptation方法，通过仅微调查询编码器（保持文档索引冻结）解决大规模多租户检索系统中的"暗数据"问题和模型更新成本挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模多租户检索系统面临两个关键挑战：1) 缺乏标注数据的"暗数据"问题；2) 联合微调查询和文档编码器需要重新索引整个语料库，这在多租户环境中成本过高。

Method: 1) 构建DevRev Search基准测试：使用融合候选生成策略（结合稀疏和密集检索器）和LLM-as-a-Judge进行一致性过滤和相关性标注；2) 提出Index-Preserving Adaptation策略：仅通过LoRA微调查询编码器，保持文档索引冻结，并针对特定transformer层进行优化。

Result: 在DevRev Search和SciFact数据集上的实验表明，仅微调查询编码器的方法能够获得有竞争力的性能提升，同时保持文档索引不变，实现了质量与效率的最佳权衡。

Conclusion: 提出的Index-Preserving Adaptation策略为个性化企业搜索提供了一条可扩展的路径，通过仅微调查询编码器解决了多租户环境中的模型更新成本问题。

Abstract: Large-scale multi-tenant retrieval systems amass vast user query logs yet critically lack the curated relevance labels required for effective domain adaptation. This "dark data" problem is exacerbated by the operational cost of model updates: jointly fine-tuning query and document encoders requires re-indexing the entire corpus, which is prohibitive in multi-tenant environments with thousands of isolated indices. To address these dual challenges, we introduce \textbf{DevRev Search}, a passage retrieval benchmark for technical customer support constructed through a fully automatic pipeline. We employ a \textbf{fusion-based candidate generation} strategy, pooling results from diverse sparse and dense retrievers, and utilize an LLM-as-a-Judge to perform rigorous \textbf{consistency filtering} and relevance assignment. We further propose a practical \textbf{Index-Preserving Adaptation} strategy: by fine-tuning only the query encoder via Low-Rank Adaptation (LoRA), we achieve competitive performance improvements while keeping the document index frozen. Our experiments on DevRev Search and SciFact demonstrate that targeting specific transformer layers in the query encoder yields optimal quality-efficiency trade-offs, offering a scalable path for personalized enterprise search.

</details>


### [34] [PROMISE: Process Reward Models Unlock Test-Time Scaling Laws in Generative Recommendations](https://arxiv.org/abs/2601.04674)
*Chengcheng Guo,Kuo Cai,Yu Zhou,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: Promise框架通过过程奖励模型和引导波束搜索解决生成式推荐中的语义漂移问题，实现推理时计算扩展，让小模型达到或超越大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法存在"语义漂移"问题，即早期高层级token的错误会不可逆转地将生成轨迹导向不相关的语义子空间，严重影响推荐准确性。

Method: 提出Promise框架：1) 轻量级过程奖励模型评估中间推理步骤质量；2) PRM引导的波束搜索策略，利用密集反馈动态剪枝错误分支；3) 实现推荐系统的推理时扩展定律。

Result: 大规模平台上的离线实验和在线A/B测试表明，Promise有效缓解语义漂移，显著提高推荐准确性，同时实现高效部署，小模型通过增加推理计算可匹配或超越大模型。

Conclusion: Promise框架成功解决了生成式推荐中的语义漂移问题，通过密集逐步验证和推理时计算扩展，为推荐系统提供了更准确、高效的生成式解决方案。

Abstract: Generative Recommendation has emerged as a promising paradigm, reformulating recommendation as a sequence-to-sequence generation task over hierarchical Semantic IDs. However, existing methods suffer from a critical issue we term Semantic Drift, where errors in early, high-level tokens irreversibly divert the generation trajectory into irrelevant semantic subspaces. Inspired by Process Reward Models (PRMs) that enhance reasoning in Large Language Models, we propose Promise, a novel framework that integrates dense, step-by-step verification into generative models. Promise features a lightweight PRM to assess the quality of intermediate inference steps, coupled with a PRM-guided Beam Search strategy that leverages dense feedback to dynamically prune erroneous branches. Crucially, our approach unlocks Test-Time Scaling Laws for recommender systems: by increasing inference compute, smaller models can match or surpass larger models. Extensive offline experiments and online A/B tests on a large-scale platform demonstrate that Promise effectively mitigates Semantic Drift, significantly improving recommendation accuracy while enabling efficient deployment.

</details>


### [35] [Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective](https://arxiv.org/abs/2601.04918)
*Ziwen Wang,Shangshang Yang,Xiaoshan Yu,Haiping Ma,Xingyi Zhang*

Main category: cs.IR

TL;DR: 提出OSCD方法，通过进化多目标一次性神经架构搜索，为认知诊断任务寻找最优模型架构，解决现有方法忽视数据噪声和依赖专家设计的问题。


<details>
  <summary>Details</summary>
Motivation: 现有认知诊断研究过于关注模型性能提升，忽视了观测响应数据中普遍存在的噪声污染问题，这严重阻碍了实际部署。同时，当前认知诊断模型严重依赖研究人员的领域专业知识进行结构设计，无法充分探索架构可能性，未能发挥模型架构的全部潜力。

Method: 提出OSCD方法，包含训练和搜索两个阶段：1）训练阶段构建包含多样化架构组合的搜索空间，训练基于完全二叉树拓扑的权重共享超网络；2）搜索阶段将异构噪声场景下的最优架构搜索建模为多目标优化问题，开发集成帕累托最优解搜索策略和跨场景性能评估的优化框架。

Result: 在真实世界教育数据集上的大量实验验证了OSCD模型发现的认知诊断任务最优架构的有效性和鲁棒性。

Conclusion: OSCD方法能够高效且鲁棒地提升模型评估学习者熟练度的能力，解决了现有认知诊断模型忽视数据噪声和依赖专家设计的问题，为智能辅导系统中的认知诊断任务提供了更优的解决方案。

Abstract: With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.

</details>


### [36] [Dynamics in Search Engine Query Suggestions for European Politicians](https://arxiv.org/abs/2601.05081)
*Franziska Pradel,Fabian Haak,Sven-Oliver Proksch,Philipp Schaer*

Main category: cs.IR

TL;DR: 研究分析了欧洲各国Google搜索中政治人物查询建议的差异，发现查询建议在政治人物本国、担任超国家角色及女性政治人物中更不稳定，而政治领袖和男性政治人物的查询建议在各国间更相似。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎是获取在线政治信息的主要渠道，但人们对政治搜索查询建议如何反映互联网用户的潜在兴趣，以及这些建议在不同国家和时间上的变化了解不足。特别是针对欧洲政治人物的搜索查询建议缺乏系统性分析。

Method: 使用在十个国家收集的欧洲政治人物搜索查询建议原始数据集，分析查询建议的稳定性。研究考察了查询建议在政治人物本国、超国家角色和性别等因素下的时间稳定性，以及政治领袖和男性政治人物查询建议的跨国相似性。

Result: 查询建议在政治人物本国、担任超国家角色的政治人物以及女性政治人物中更不稳定。相反，政治领袖和男性政治人物的查询建议在不同国家间表现出更高的相似性。

Conclusion: 研究揭示了政治人物搜索查询建议的时空变化模式，为理解在线政治信息搜索提供了新视角。未来研究可进一步探讨欧洲政治人物在线搜索信息的相关方向。

Abstract: Search engines are commonly used for online political information seeking. Yet, it remains unclear how search query suggestions for political searches that reflect the latent interest of internet users vary across countries and over time. We provide a systematic analysis of Google search engine query suggestions for European and national politicians. Using an original dataset of search query suggestions for European politicians collected in ten countries, we find that query suggestions are less stable over time in politicians' countries of origin, when the politicians hold a supranational role, and for female politicians. Moreover, query suggestions for political leaders and male politicians are more similar across countries. We conclude by discussing possible future directions for studying information search about European politicians in online search.

</details>


### [37] [Multivector Reranking in the Era of Strong First-Stage Retrievers](https://arxiv.org/abs/2601.05200)
*Silvio Martinico,Franco Maria Nardini,Cosimo Rulli,Rossano Venturini*

Main category: cs.IR

TL;DR: 提出两阶段检索架构，用单向量检索器替代多向量检索的token级收集阶段，结合推理无关的稀疏检索和候选剪枝技术，实现24倍加速且保持检索质量


<details>
  <summary>Details</summary>
Motivation: 多向量表示检索效果好但计算成本高，现有gather-and-refine策略需要昂贵的token级索引搜索且可能错过最优文档，需要更高效的检索方案

Method: 1) 用学习稀疏检索器(LSR)替代token级收集阶段；2) 集成推理无关LSR方法减少查询编码时间；3) 引入两种早期剪枝低质量候选的优化技术

Result: 两阶段方法比现有最佳多向量检索系统快24倍以上，检索效率提升1.8倍且无质量损失，保持可比或更优的检索质量

Conclusion: 将多向量检索重新构建为两阶段架构，结合高效单向量检索和优化技术，能在显著提升效率的同时保持检索效果，为实际部署提供可行方案

Abstract: Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever -- specifically, a learned sparse retriever (LSR) -- produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality.

</details>
