<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了一种区间二型贝叶斯定理方法，通过将专家提供的区间估计编码为区间二型模糊隶属函数来处理输入概率的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值，但现实应用中专家通常只能提供区间范围估计，需要扩展贝叶斯定理来处理这种不确定性。

Method: 开发了区间二型贝叶斯定理版本，使用保守方法避免输入不一致；提出新颖算法将专家提供的区间编码为区间二型模糊隶属函数。

Result: 该方法能够有效处理输入概率的区间不确定性，避免产生无效输出结果。

Conclusion: 提出的区间二型贝叶斯定理扩展了传统方法，为处理现实世界中的不确定性提供了更实用的工具。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 提出基于NLP和多模态LLM的自动化游戏模板生成框架，将游戏设计文档转换为Unity可执行代码，显著提升游戏开发效率


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中的关键空白，简化从游戏设计到实现的过程，利用LLM技术加速游戏原型开发

Method: 使用微调的LLaMA-3模型专门用于Unity代码生成，结合自定义Unity集成包，构建端到端系统解析GDD并生成结构化游戏规范

Result: 微调模型在编译成功率、GDD遵循度、最佳实践采用和代码模块化等指标上表现优异（平均得分4.8/5.0），显著优于基线模型

Conclusion: 该系统有效证明了LLM在游戏开发流程中的价值，能够高效地将设计文档转化为功能完整的游戏原型，为自动化游戏开发提供了可行方案

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 使用多个专门化的LLM代理分解MiniZinc模型建模任务，每个代理处理特定类型的全局约束，最后组装成完整模型


<details>
  <summary>Details</summary>
Motivation: 自然语言描述向MiniZinc模型的转换需要逻辑推理和约束编程专业知识，这是一个具有挑战性的任务

Method: 提出一个多代理框架，每个LLM代理专门检测和生成特定类型的全局约束代码，最后通过组装代理集成完整模型

Result: 初步实验显示该方法在多个LLM上都表现更好，超过了一次性提示和思维链提示等基线方法

Conclusion: 通过任务分解和专门化代理的方式可以有效处理复杂的模型建立任务，并提出了未来工作的完整路线图

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出了一种截断交叉熵(TCE)损失函数来缓解生成模型在合成数据递归训练中的模型崩溃问题，通过降低对高置信度预测的权重来延迟模型崩溃


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型生成合成数据的速率加快，预计到203年大部分训练数据将是机器生成的。在合成数据上重复训练会导致模型崩溃现象，使模型性能逐渐退化。现有缓解策略有限，需要新的解决方案

Method: 识别模型对其自生成数据的过度自信是模型崩溃的关键驱动因素，提出置信度感知的损失函数TCE，在训练过程中降低高置信度预测的权重

Result: TCE显著延迟了递归训练中的模型崩溃，可以将模型崩溃前的保真度间隔延长2.3倍以上，且该方法在不同模态上都具有通用性

Conclusion: 损失函数设计为在合成数据日益增多的时代保持生成模型质量提供了一个简单而强大的工具

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 在动态多治理者协商任务中比较了人类、大语言模型和贝叶斯模型的表现，发现性能相似但行为差异显著


<details>
  <summary>Details</summary>
Motivation: 随着协调任务趋向自动化，需要评估不仅是自主治理者的绩效，还有它们在动态多治理者环境中的协商过程和行为动态

Method: 在动态协商设置中进行直接比较，涉及216名人类、GPT-4o、Gemini 1.5 Pro和贝叶斯模型，捕获结果和行为动态

Result: 贝叶斯模型通过敌对性优化获得最高剩余，但抱死率高；人类和LLM总体剩余相似，但LLM偏向保守进退的交易方式，人类则更具战略性、冒险性和公平性

Conclusion: 绩效平等指标可能隐藏了过程和对齐方面的根本差异，这对实际部署至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [6] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文研究可解释AI中的不确定性解释和全局解释，通过测试算法在信任校准、用户满意度和可解释性方面的表现，为XAI方案提供通用指导原则。


<details>
  <summary>Details</summary>
Motivation: 虽然可解释AI(XAI)已被广泛研究，但不确定性解释和全局解释这两个领域相对较少被关注。研究者希望构建XAI方案的通用指导原则，并特别关注算法在提供直观视觉理解方面的能力。

Method: 选择了一个能够同时涵盖不确定性、鲁棒性和全局XAI等多个概念的算法进行测试，评估其在信任校准方面的能力，并检验复杂但提供直观视觉理解的算法是否能带来更高的用户满意度和人类可解释性。

Result: 研究得出了XAI方案的通用指导原则，但具体测试结果未在摘要中详细说明。

Conclusion: 该研究为可解释AI领域提供了重要的指导原则，特别强调了不确定性解释和全局解释的重要性，以及直观视觉理解在提高用户满意度和可解释性方面的潜在价值。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [7] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来优化大语言模型在冷启动推荐任务中的表现，通过最优示例注入和指令结构调整显著提升了few-shot设置下的推荐精度。


<details>
  <summary>Details</summary>
Motivation: 冷启动用户问题限制了推荐系统的有效性，因为缺乏历史行为信息。需要一种有效的方法来优化大语言模型在推荐任务中的指令提示。

Method: 提出了上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户画像，Ds是精选支持集，R̂是预测的物品排序列表。使用基于transformer的自回归大语言模型（BioGPT、LLaMA-2、GPT-4），采用词元级对齐和嵌入空间正则化技术。

Result: 实验证明最优示例注入和指令结构能显著提高precision@k和NDCG分数，特别是在低数据设置下。提示的及时组合不仅影响语法结构，还直接控制注意力规模和推理过程中的解码器行为。

Conclusion: 基于提示的适应方法可以视为解决基于大语言模型的推荐系统中冷启动问题的一种有效途径。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱风险识别的机器学习管道，在IMI大数据与人工智能竞赛中获得第二名，AUROC达到0.961


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构的优先事项，机器学习在此领域具有巨大潜力，需要开发系统化的方法来识别高风险银行客户

Method: 采用16步设计和统计分析流程，构建SQLite数据库，开发基于SQL的特征工程算法，集成预训练模型和可解释AI模块

Result: 管道在包含195,789个客户ID的数据集上实现了0.961的平均AUROC（标准差0.005），在竞赛中获得第二名

Conclusion: 提出的综合系统化方法能够有效构建稳健的反洗钱机器学习管道，具有高准确性和可解释性

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一个基于神经科学原理的计算框架，用于提升机器人等自主系统的空间智能，包含六个核心模块：生物受作多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。


<details>
  <summary>Details</summary>
Motivation: 当前自主AI系统在空间推理能力上存在显著缺口，主要限于符号和序列处理，而人类空间智能基于多感官知觉、空间记忆和认知地图，能够在非结构化环境中进行灵活的上下文感知决策。缩小这一差距对于提升机器人等自主系统与物理世界的交互至关重要。

Method: 从计算神经科学中的空间神经模型出发，将核心生物功能映射到六个计算模块：生物受作多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。基于这个框架对现有方法进行分析评估，识别关键缺口。

Result: 提出了一个结构化的神经科学基础计算框架，为自主空间智能研究提供了清晰的研究路径。对现有方法的分析显示了当前技术在生物受作多模态感知、多感官整合等关键模块上的不足。

Conclusion: 该框架为提升自主AI系统的空间智能提供了有价值的研究方向，特别是在动态和非结构化环境中的空间推理能力。希望通过神经科学基础的视角和结构化路径，推动代理空间智能领域的发展。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和渐进多尺度解码策略，解决多智能体运动预测中交互关系动态演化的问题，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了智能体间交互关系的动态演化特性，无法有效处理未来运动中的不确定性

Method: 使用动态异构图进行场景建模，设计渐进式架构处理时空依赖关系，结合多尺度解码逐步消除未来运动不确定性

Result: 在INTERACTION多智能体预测基准排名第一，在Argoverse 2多世界预测基准也达到最优性能

Conclusion: ProgD方法通过显式建模动态交互关系和多尺度渐进解码，显著提升了多智能体运动预测的准确性和一致性

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多代理系统监管架构，通过行为追踪、动态声誉和恶意行为预测三个核心模块实现自动化负责和监管


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的自治代理在金融、医疗等领域带来机遇的同时，其不可预测行为和异构能力也带来了重大的监管和负责挑战

Method: 提出一种区块链启用的层状监管架构，包含代理层、区块链数据层和监管应用层，设计了行为追踪仲裁、动态声誉评估和恶意行为预测三个核心模块

Result: 为大规模代理生态系统建立了可信、弹性和可扩展的监管机制系统基础

Conclusion: 该框架为多代理系统中的区块链监管提供了系统化的基础，并提出了未来研究方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了NbQA数据集和Jupiter框架，通过从真实Jupyter笔记本提取高质量工具使用任务和解决方案，结合MCTS搜索增强多步推理能力，在数据科学任务上达到或超越GPT-4o性能


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步推理和工具使用方面存在局限，限制了在复杂数据分析任务中的有效性

Method: 从真实Jupyter笔记本提取标准化任务-解决方案对构建NbQA数据集；提出Jupiter框架，将数据分析建模为搜索问题，使用蒙特卡洛树搜索生成多样化解决方案轨迹进行价值模型学习

Result: Qwen2.5-7B和14B模型在NbQA上分别解决77.82%和86.38%的任务，匹配或超越GPT-4o和先进代理框架；在多步推理任务上展现出更好的泛化能力和工具使用推理

Conclusion: 通过构建高质量数据集和搜索增强推理框架，显著提升了LLM在数据科学工作流中的多步推理和工具使用能力

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 这篇论文对三种知识图构建方法（spaCy、Stanford CoreNLP-OpenIE、GraphRAG）进行了技术比较研究，分析它们在基于大语言模型的问题回答系统中的效果、可行性和适应性。实验结果显示OpenIE在三元组覆盖面方面最优，而GraphRAG在推理能力方面表现最好。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法在处理复杂、长文本的主题性和整体理解时有限制，需要对文本和上下文进行更深入的分析。知识图作为结构化信息的强大工具，最近成为增强问题回答系统的新优势。

Method: 进行了一项综合性的技术比较研究，对三种不同的知识图三元组构建方法（spaCy、Stanford CoreNLP-OpenIE和GraphRAG）进行评估，并将它们与大语言模型（LLMs）集成用于问题回答。所有方法都利用了开源技术。

Result: 实验结果表明，虽然OpenIE提供了最全面的三元组覆盖范围，但GraphRAG在三种方法中展示了最优异的推理能力。

Conclusion: 论文最后讨论了每种方法的优势和局限性，并为改进基于知识图的问题回答系统提供了未来发展方向的见解。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS生成的高质量推理轨迹用于改进基于偏好的强化学习策略优化，提出分阶段GRPO训练范式，通过树状结构优势估计来提升策略学习效果


<details>
  <summary>Details</summary>
Motivation: 利用MCTS在数学和符号领域生成高质量中间轨迹的有效性，探索如何将这些轨迹重新用于改进基于偏好的强化学习中的策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全，引入新颖的树状结构优势估计方法，产生前缀条件奖励信号

Result: 结构化优势估计可以稳定更新并更好地反映组合推理质量，但仍面临优势饱和和奖励信号崩溃等挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放性挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了内存、工具和思维树等核心功能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得显著进展，但设计多功能、鲁棒且高效的智能体部署平台仍面临重大挑战

Method: 提出LightAgent框架，集成Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，保持极轻量级结构，作为完全开源解决方案与主流聊天平台无缝集成

Result: 开发人员可以轻松构建自学习智能体，框架已在GitHub上发布

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统提供了轻量级但功能强大的部署解决方案

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究锦标赛中获胜者的认证解释问题，通过识别最小支持子锦标赛来解释为什么某个候选人在不同锦标赛规则下必然获胜。


<details>
  <summary>Details</summary>
Motivation: 锦标赛模型广泛应用于表示候选人或团队之间的两两优势关系，需要为获胜者提供形式化的可解释AI认证解释。

Method: 识别最小支持子锦标赛（候选人在其中是必要获胜者），针对多种锦标赛规则（top cycle、uncovered set、Copeland、Borda、maximin、weighted uncovered set）进行分析。

Result: 确定了各规则的最小支持子锦标赛大小，提出了多项式时间算法（除weighted uncovered set外，该问题是NP完全的）。

Conclusion: 最小支持子锦标赛能够提供紧凑、认证且直观的解释，为锦标赛获胜者的形式化解释提供了有效方法。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究空间协调三个维度（探索多样性、移动专业化、自适应空间接近度）对团队在限制显性沟通的搜救任务中表现的影响


<details>
  <summary>Details</summary>
Motivation: 许多团队（消防员、军事、执法、应急响应）需要在没有视觉线索或大量显性沟通的情况下协调物理空间中的移动，但现有研究主要关注共址同步团队或知识工作协调

Method: 分析34个四人团队（136名参与者）在搜救任务中的空间接近度、分布模式和移动对齐等空间协调指标数据

Result: 空间专业化正向预测绩效，自适应空间接近度呈现边际倒U型关系（适度适应最优），这些指标的时间动态能区分高低绩效团队

Conclusion: 研究揭示了基于角色的团队工作中隐式空间协调机制，强调了平衡自适应策略的重要性，对培训和AI辅助团队支持系统有重要意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的AutoML代理的多样化、真实结构化基准测试，通过自动收集Kaggle等平台的ML挑战任务，提供多维度评估框架和难度分级机制。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法充分评估LLM代理在真实AutoML工作流中的完整能力。

Method: 1) 基于浏览器自动化和LLM的任务采集系统自动收集多平台ML挑战；2) 基于排行榜的难度建模机制；3) 包含性能、格式合规性、约束遵循和任务泛化的多维度评估框架

Result: 构建了包含150个AutoML任务的基准测试，提供Lite(18任务)、Medium和Full三个不同规模的版本，涵盖多种数据模态和难度级别。

Conclusion: TAM Bench为解决现有基准测试局限性提供了全面解决方案，为评估LLM-based AutoML代理在真实场景中的能力提供了标准化测试平台。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过集成视觉-语言模型和层状奖励函数的新题DRL架构，实现了资源效率高的语义探索，代理能够策略性地查询VLM获取外部指导


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在语义探索中平衡效率和语义理解方面遇到困难，需要更高级的认知能力和理性决策能力

Method: 集成视觉-语言模型通过层状奖励函数，将VLM查询模型化为专门动作，结合课程学习策略在不同复杂度级别指导学习

Result: 代理在物体发现率方面显著提升，能够有效导航到语义丰富区域，并掌握策略性查询外部环境信息的能力

Conclusion: 该研究提供了一种将常识性语义推理嵌入自主代理的实用可扩展方法，为完全智能自主探索提供了新方向

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过模板导向推理，无需人工构建few-shot示例即可激发LLM的内在推理能力，在多种任务上实现优异性能


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供的示例，限制了模型内在推理能力的利用，且任务特定的few-shot提示构建成本高且存在任务间不一致问题

Method: 提出Template-Oriented Reasoning (TORSO)方法，通过模板导向的方式激发模型利用内部推理能力生成适当响应，无需人工构建few-shot示例

Result: 实验结果表明TORSO在多样化LLM基准测试中实现了强劲性能，并生成合理的推理过程

Conclusion: TORSO提供了一种有效的方法来激发LLM的内在推理能力，无需依赖成本高昂的人工few-shot示例构建，在多个任务上展现出优越性能

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 该技术报告分析了LLM在法律应用中产生幻觉（虚假信息）的问题，研究了其原因、表现以及RAG缓解策略的有效性，指出了RAG的局限性并提出了整体优化方案。


<details>
  <summary>Details</summary>
Motivation: 解决法律领域LLM产生虚假信息的严重问题，因为法律应用中对准确性和真实性的要求极高，幻觉可能导致严重后果。

Method: 通过分析LLM幻觉的成因和表现形式，评估RAG（检索增强生成）策略的缓解效果，并探索伦理和监管层面的影响。

Result: 发现RAG策略虽然有效但存在局限性，需要采用更全面的优化方法。人类监督在法律AI应用中具有不可替代的作用。

Conclusion: 解决方案不在于渐进改进生成模型，而应采用"咨询式"AI范式，优先考虑真实性和可追溯性，作为增强而非替代专业判断的工具。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化的分布式内存框架，通过可验证写入、动态内存调度和跨域知识扩散来解决多智能体系统中的内存管理问题，提高推理准确性并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互数据，现有内存管理方法存在噪声积累、内存膨胀和跨域泛化能力有限等问题，需要更高效的内存管理机制。

Method: SEDM框架包含三个核心组件：基于可重现回放的可验证写入准入机制、根据经验效用动态排序和整合条目的自调度内存控制器、以及抽象可重用见解支持跨异构任务迁移的跨域知识扩散。

Result: 在基准数据集上的评估显示，SEDM相比强基线方法提高了推理准确性，减少了token开销，并且能够将从事实验证中提取的知识用于增强多跳推理能力。

Conclusion: SEDM为开放式多智能体协作提供了一个可扩展且可持续的内存机制，将内存从被动存储转变为主动自优化的组件。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 该论文探索使用量子变分电路学习组合张量表示，在需要组合泛化的图像描述任务中，量子模型相比经典组合模型表现更好，特别是在多热编码图像向量上取得了良好的概念验证结果。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的关键能力，但当前AI工具（如视觉语言模型）缺乏这种能力。先前基于组合张量语义的研究结果不佳，作者推测量子模型更高的训练效率可能改善这类任务的性能。

Method: 将组合张量模型的表示解释在希尔伯特空间中，训练变分量子电路来学习这些表示。使用两种图像编码技术：二进制图像向量的多热编码（MHE）和基于CLIP视觉语言模型的图像向量的角度/幅度编码。

Result: 在使用噪声MHE编码时取得了良好的概念验证结果。在CLIP图像向量上的性能表现较为复杂，但仍然优于经典组合模型。

Conclusion: 量子模型在组合泛化任务中显示出潜力，特别是在特定编码方式下能够超越经典方法，为量子计算在自然语言处理和计算机视觉交叉领域的应用提供了新的可能性。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并实现管道并行化，显著提升具身AI系统的推理频率和吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统在动态环境中需要处理高频输入输出，传统顺序计算模式在保证准确性的同时难以达到实际应用所需的"思考"频率。

Method: 解耦感知和生成模块，提供受控的管道并行化，建立共享的公共上下文来解决数据陈旧性问题。

Result: 平均吞吐量提升2.54倍，同时达到原始准确性的102.7%。

Conclusion: Auras有效克服了顺序计算的限制，在保持准确性的同时提供了高吞吐量，适用于具身AI系统的实时推理需求。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 这篇论文探讨大语言模型扩大规模是否存在效益逐渐减少的问题。研究发现单步准确性的小幅提升可以带来长任务完成能力的指数增长。模型在长任务中失败的主要原因是执行错误，而非思维能力不足。研究还发现了自我条件化效应，即模型在上一步出错后更容易继续出错。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂思维问题上表现良好，但在简单任务加长后却失败的矛盾现象，通过重点研究执行能力来调和这种辩论。

Method: 通过明确提供知识和计划来隔离执行能力，测试不同规模模型在长轴任务中的表现，分析步长增加时的每步准确率变化。

Result: 较大模型能够正确执行更多步骤，即使小模型在单步任务上有100%准确率。模型的每步准确率随步长增加而下降，并发现自我条件化效应，而最新的思维模型不会出现这种自我条件化问题。

Conclusion: 通过重点关注执行能力，可以解释LLM在复杂思维问题和简单长任务上的表现差异，并强调扩大模型规模和序列测试时计算对长轴任务的巨大好处。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出了一种基于类概率分布信器比的不确定性量化框架，通过汇聚模型的预测异象来评估每个样本的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的加性不确定性分解方法受到质疑，需要更直观有效的方法来量化神经网络预测的每个样本不确定性。

Method: 提出基于类概率分布信器比的框架，使用方差门控测量来根据汇聚模型的信心因子缩放预测结果。

Result: 该方法能够有效地量化和分解预测不确定性，并讨论了委员机器多样性出现累套的问题。

Conclusion: 新框架为神经网络每个样本的不确定性量化提供了更直观有效的方法，适用于高风险应用的决策支持。

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [27] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: 提出了一个改进的矩阵乘性权重更新算法，在矩阵学习专家建议问题中实现了实例最优的遗憾界O(√(T·S(X||d⁻¹I_d)))，同时保持与原始MMWU相同的计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有MMWU算法在矩阵LEA问题上只能达到极小极大最优遗憾界O(√(Tlog d))，但无法实现实例最优性能。本文旨在开发一个既能保持计算效率又能获得更好遗憾界的算法

Method: 开发了基于势函数的矩阵LEA通用框架，MMWU是其特例。通过拉普拉斯变换技术构建新的"单边"Jensen迹不等式，应用最优势函数（基于虚误差函数）来推导新算法

Result: 新算法实现了实例最优遗憾界O(√(T·S(X||d⁻¹I_d)))，计算复杂度与MMWU相同。在量子态学习、去极化噪声、随机量子态和Gibbs态学习等应用中优于现有技术

Conclusion: 该工作提供了矩阵在线学习的改进算法，在保持计算效率的同时显著提升了性能，在量子学习理论中有重要应用价值，能够预测非线性量子性质

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [28] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出了一种针对奖励信号存在对抗性污染的鲁棒Q学习算法，在异步采样模型下证明了有限时间收敛性，收敛速率与非对抗情况相近，仅增加与污染样本比例成正比的附加项。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励信号可能受到极端噪声、传感器故障或恶意攻击的对抗性污染，这会严重降低经典算法（如Q学习）的性能。

Method: 提出了新的鲁棒Q学习变体算法，在异步采样模型下运行，即使部分观测奖励被对手任意扰动也能有效工作。还提出了无需先验知道真实奖励分布统计信息的算法变体。

Result: 证明了算法在对抗性污染下的有限时间收敛速率与非对抗情况匹配，仅增加与污染样本比例成正比的附加项。推导了信息论下界证明该附加项是不可避免的。

Conclusion: 提供了异步Q学习的首个有限时间鲁棒性保证，填补了鲁棒强化学习的重要空白。开发的技术工具（改进的Azuma-Hoeffding不等式）可能具有独立价值。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [29] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出基于Wasserstein距离的分布鲁棒优化框架，解决多源异构数据中群体分布不确定性下的最差群体性能优化问题


<details>
  <summary>Details</summary>
Motivation: 传统方法假设群体分布可准确估计，但在噪声、非平稳和动态环境中该假设常被违反，导致模型在非典型或欠代表群体上性能下降

Method: 使用Wasserstein距离的分布鲁棒优化(DRO)方法，同时考虑群体内分布不确定性和最差群体性能优化，开发梯度下降-上升算法求解

Result: 在真实世界数据上验证了方法的有效性

Conclusion: 所提框架能有效处理群体分布不确定性，提升最差群体性能，适用于噪声和非平稳环境

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [30] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: FoundationalECGNet是一个用于自动ECG分类的基础框架，通过多技术融合实现心电信号分析，在正常/异常分类和多类疾病检测中达到99% F1分数。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死亡原因，现有ECG分析方法面临噪声、类别不平衡和数据集异质性等挑战，需要开发更准确和可扩展的诊断系统。

Method: 整合双阶段小波去噪（Morlet和Daubechies）、CBAM注意力模块、图注意力网络和时间序列变换器，共同捕捉多通道ECG信号的空间和时间依赖性。

Result: 在多个数据集上，正常vs异常分类达到99% F1分数，多类疾病检测表现优异：传导障碍和肥大99% F1分数，心律失常98.9% F1分数。

Conclusion: FoundationalECGNet提供了一个可扩展、可解释和通用的自动ECG分析解决方案，有望提高医疗环境中的诊断精度和患者预后。

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [31] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: 该论文通过将LRP类归因方法表示为修正梯度矩阵的乘积，建立了与雅可比矩阵乘法的类比，推导了奇异值上界和归因图值的分量界，并获得了控制经验均值收敛到期望的乘性常数，揭示了LRP-beta方法的常数与权重范数无关的重要特性。


<details>
  <summary>Details</summary>
Motivation: 分析LRP类归因方法的数值特性，特别是研究归因值的分布规律和收敛性质，为多数据增强场景和Smoothgrad类方法提供理论指导。

Method: 将LRP归因方法表示为修正梯度矩阵的乘积形式，建立与链式法则中雅可比矩阵乘法的类比，推导奇异值上界和分量界，进而获得控制收敛的乘性常数。

Result: 获得了控制经验均值收敛到期望的乘性常数，发现LRP-beta方法的常数与权重范数无关，这与基于梯度的方法和LRP-epsilon形成显著区别。

Conclusion: 该分析对多非几何数据增强场景和Smoothgrad类归因方法具有重要意义，揭示了LRP-beta方法在数值稳定性方面的优势特性。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [32] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: 该论文研究如何通过碳感知的客户端选择和训练调度来减少联邦学习中的碳排放，利用碳强度时空变化特性，在保证模型精度的同时显著降低碳排放。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习训练产生大量碳排放，联邦学习天然具备利用地理分散客户端碳强度时空变化的优势，需要研究如何通过调度策略降低碳排放。

Method: 提出碳感知调度器，整合松弛时间（允许适度延长训练时间）、α-公平碳分配和全局微调阶段，利用碳强度变化特性选择低碳时段进行训练。

Result: 在真实碳强度数据上的实验表明，该调度器优于不考虑松弛时间的基线方法，在广泛碳预算范围内实现更高模型精度，在严格碳约束下表现尤为突出。

Conclusion: 碳感知调度策略能有效减少联邦学习的碳排放，通过合理利用碳强度时空变化和适度延长训练时间，可以在不牺牲模型性能的前提下实现显著的碳减排效果。

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [33] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 提出了一个结合主动帕累托前沿学习算法(PyePAL)、可视化技术和可解释AI的框架，用于优化聚合物薄膜旋涂工艺参数，实现硬度和弹性的多目标优化。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜实现特定机械性能本质上是一个多目标优化问题，需要同时优化硬度、弹性等多个性能指标。

Method: 使用PyePAL算法结合高斯过程模型预测目标值，通过UMAP进行二维可视化，并采用模糊语言总结将参数与性能关系转化为语言描述。

Result: 实验结果表明该方法能有效识别有前景的聚合物设计，视觉和语言解释有助于专家驱动的分析和知识发现。

Conclusion: 该框架成功实现了聚合物薄膜旋涂工艺的多目标优化，并通过可视化与可解释AI技术增强了优化结果的可理解性和专家分析能力。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [34] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: ANNA注意力机制在保持Transformer表达能力的同时，实现了亚二次时间复杂度，能够高效模拟MPC算法并解决关键推理任务


<details>
  <summary>Details</summary>
Motivation: 标准Transformer虽然具有模拟大规模并行计算(MPC)算法的表示能力，但其二次时间复杂度严重限制了可扩展性，需要开发更高效的注意力机制

Method: 提出了近似最近邻注意力(ANNA)机制，具有亚二次时间复杂度，通过理论证明ANNA-transformers在保持表达能力的同时能够高效解决推理任务

Result: 证明ANNA-transformers：(1)保持标准注意力的表达能力，匹配MPC算法能力；(2)能近乎最优深度解决Match2和k-hop等关键推理任务；(3)常数深度ANNA-transformers可模拟常数深度低秩transformers

Conclusion: ANNA提供了一种统一的方式来推理广泛的注意力近似方法，在保持理论表达能力的同时显著提升了计算效率

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [35] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过知识蒸馏将LLM的领域知识和推理能力转移到传统模型中，在保持快速推理的同时提升问答模拟器的性能


<details>
  <summary>Details</summary>
Motivation: 现有问答模拟器方法存在性能与效率的权衡：LLM-free方法推理快但性能次优，LLM-based方法性能好但推理慢且资源消耗大。需要一种既能保持高效推理又能获得良好性能的方法

Method: LDSim（LLM Distillation based Simulator），使用知识蒸馏技术从大型语言模型（LLM）中提取领域知识和推理能力，来辅助传统模型的预测

Result: 大量实验表明，LDSim在模拟任务和知识追踪（KT）任务上都取得了强劲的结果

Conclusion: LDSim通过知识蒸馏成功平衡了性能与效率，为教育推荐系统提供了高质量的模拟数据生成方案，同时保持了较快的推理速度

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [36] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref是一个密集Transformer模型家族，在多个参数规模(0.13B-1.7B)和token规模(最高1T)上训练，在8个开放参考数据集上建立研究基线，为比较不同训练方法提供参考标准。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供标准化的参考基线，以便评估不同训练方法的有效性和质量，特别是在不同规模和数据集上的表现。

Method: 使用密集Transformer架构，在8个开放参考数据集上进行大规模训练，涵盖不同参数规模和token规模，并保存中间检查点。

Result: NemoTron-CC HQ数据集表现最佳，其次是DCLM-baseline和FineWeb-Edu。提供了完整的训练检查点、日志、代码和下游评估。

Conclusion: 建立了可复现的研究基线，标准化了比较方法，为未来研究提供了便利，特别是在训练动态研究和扩展趋势分析方面。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [37] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 基于深度自动编码器的上下文条件异常检测框架，能够自动识别上下文特征并模型条件分布，在多个表格数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测在象网络安全和金融等领域具有重要意义，但真实表格数据存在异质性上下文，单一全局分布方法容易忽略上下文细节，影响检测性能。

Method: 提出上下文条件异常检测框架，自动识别上下文特征，使用简单深度自动编码器模型条件数据分布。

Result: 在多个表格数据集上进行的广泛实验表明，该方法超过了现有的最先进方法。

Conclusion: 突出了上下文在准确区分异常与正常实例中的重要性，为表格数据异常检测提供了有效解决方案。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [38] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出混合专家模型(MoWE)，通过动态组合现有天气模型的输出来提升预测精度，相比最佳单一模型降低10%的RMSE误差


<details>
  <summary>Details</summary>
Motivation: 数据驱动的天气模型性能近年来趋于平缓，需要新的范式突破现有局限，而不是重新开发新的预测器

Method: 使用基于Vision Transformer的门控网络，根据预测提前期动态学习多个专家模型在每个网格点的权重贡献，以较低计算资源训练

Result: 在2天预测范围内，RMSE比最佳AI天气模型降低10%，显著优于单个专家模型和简单平均组合

Conclusion: 提供了一种计算高效且可扩展的策略，通过充分利用现有高质量预测模型来推动数据驱动天气预测的技术进步

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [39] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 这篇范围综述分析了机器学习在电力系统保护和扰动管理中的应用，发现ML模型在仿真数据上表现良好但缺乏真实世界验证，存在方法不一致、数据集质量差等问题，提出了标准化框架和改进建议。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和分布式能源的集成改变了现代电力系统，对传统保护方案构成挑战，需要评估机器学习在电力系统保护中的应用现状和潜力。

Method: 采用PRISMA范围综述框架，基于100多篇文献进行系统性分析，评估ML在保护任务中的研究范围、性能表现和适用方法。

Result: ML模型在仿真数据集上表现出高准确性，但在真实条件下的性能验证不足；现有文献存在方法严谨性、数据集质量和评估指标不一致的问题；缺乏标准化阻碍了结果的可比性。

Conclusion: 需要优先考虑公共基准数据集、现实验证方法和先进ML架构，推动ML保护从理论承诺走向实际部署，以应对日益动态和分散的电力系统挑战。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [40] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE是一个可扩展的XAI框架，通过RKHS中的正交函数分解避免特征子集枚举，提供功能组件而非标量归因，在保持高保真度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决现有XAI框架的两个主要限制：特征子集推理的指数级计算成本和将效应总结为单一标量值的表达能力不足问题。

Method: 基于再生核希尔伯特空间(RKHS)的正交函数分解方法，通过递归核中心化程序的分析投影方案计算功能组件，避免显式子集枚举。

Result: 在10个数据集上实现中位数约3倍的加速(0.6-9.7倍范围)，保持高保真度(R²在0.81-0.999之间)，并在大多数数据集上具有显著的排序一致性。

Conclusion: STRIDE通过提供结构化功能视角补充标量归因方法，支持新颖的诊断功能如'组件手术'来量化特定交互的影响。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [41] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 提出Rashomon Ensemble方法，通过基于性能和解释对模型进行分组，从多样化的高性能解决方案中选择模型构建集成，提高泛化能力，在Rashomon比例较大的场景中AUROC提升可达0.20+


<details>
  <summary>Details</summary>
Motivation: 机器学习中选择泛化能力好的模型仍然具有挑战性。Rashomon效应指多个模型在给定学习问题上表现相似的情况，这在现实场景中经常发生。需要一种方法能够从这些多样化高性能解决方案中战略性地选择模型来改善泛化

Method: 提出Rashomon Ensemble方法，基于模型的性能和解释对模型进行分组，构建集成以最大化多样性同时保持预测准确性。确保每个模型覆盖解决方案空间的不同区域

Result: 在开放和专有的协作现实世界数据集上验证了方法，在Rashomon比例较大的场景中AUROC提升可达0.20+。展示了在各种现实世界应用中对企业的实际益处

Conclusion: 该方法具有鲁棒性、实用性和有效性，能够提高模型对分布偏移和未见数据变化的鲁棒性

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [42] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 通过量子力学视角研究深度线性网络的几何结构，使用群作用和鲁默奇下洒技术分析过参数化，并在平衡流形上定义波茨端曼熵


<details>
  <summary>Details</summary>
Motivation: 为学习过程提供热力学描述，通过量子力学视角探索深度线性网络的几何结构特性

Method: 使用群作用分析过参数化，采用鲁默奇下洒技术从参数空间映射到观测量空间，通过Jacobi矩阵理论构造平衡流形切空间的正交基

Result: 完成了平衡流形切空间正交基的显式构造，通过群轨道定义并计算波茨端曼熵，证明观测量空间的鲁默奇几何可通过平衡流形的鲁默奇下洒获得

Conclusion: 量子力学方法为深度网络学习提供了新的理论基础，通过几何结构分析可以更深入理解过参数化和学习过程的热力学特性

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [43] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: Sensitivity-LoRA是一种高效的LLM微调方法，通过基于权重矩阵的全局和局部敏感度动态分配秩，解决了传统LoRA方法统一秩分配的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法对所有增量矩阵使用统一秩分配，而现有的秩分配技术计算效率低、复杂且不稳定，限制了实际应用。需要一种更高效的秩分配方法。

Method: 利用损失函数的二阶导数（Hessian矩阵）捕捉权重敏感度，基于全局和局部敏感度动态分配秩，实现最优秩分配且计算开销最小。

Result: 实验结果表明，Sensitivity-LoRA在不同任务和基准测试中表现出强大的有效性、效率和稳定性。

Conclusion: Sensitivity-LoRA通过动态秩分配解决了LoRA的局限性，为资源受限环境下的LLM专业化提供了高效稳定的微调方法。

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [44] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 通过结合多元格兰杰因果性和PCMCI+算法的因果性深度学习框架，提升了北极海冰预测的准确性和可解释性，减少偏差并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的相关性学习模型无法区分真实因果关系与偏偏相关，影响模型的稳健性、可解释性和沿时外推能力。

Method: 提出一种因果性深度学习框架，集成多元格兰杰因果性(MVGC)和PCMCI+算法进行因果特征选择，并在混合神经网络架构中实现。使用1979-2021年43年的北极海冰范围数据和相关海洋-大气变量。

Result: 实验结果显示，结合因果输入能够提高不同预测时间段的预测准确性和可解释性，识别出对SIE动态有直接因果关系的预测因子，减少不必要特征并提高计算效率。

Conclusion: 该框架虽然在北极海冰预测中录得成效，但具有广泛的适用性，可应用于其他动态、高维度预测领域，为因果性预测模型提供了可扩展的理论基础和实践性能力。

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [45] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 提出了一个基于物理信息神经网络(PINNs)的连续时间多智能体强化学习框架(CT-MARL)，通过值梯度迭代(VGI)模块解决HJB方程在高维多智能体系统中的计算难题


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以处理高频或非规则时间间隔交互的复杂动态系统，连续时间RL在多智能体领域面临维度灾难和集中值函数近似困难两大挑战

Method: 使用PINNs近似HJB基值函数，引入VGI模块迭代优化轨迹上的值梯度，确保值与微分结构的一致性，提高梯度保真度

Result: 在连续时间版本的多智能体粒子环境和MuJoCo基准测试中，该方法 consistently优于现有连续时间RL基线，并能扩展到复杂多智能体动态系统

Conclusion: 提出的CT-MARL框架成功解决了连续时间多智能体强化学习中的关键挑战，为复杂动态系统的多智能体控制提供了有效的解决方案

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [46] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 使用机器学习模型预测ISP对等互联伙伴选择，XGBoost模型在公开数据上达到98%准确率


<details>
  <summary>Details</summary>
Motivation: ISP之间的对等互联过程复杂耗时，自动化选择对等伙伴可以提高全球互联网生态系统的效率

Method: 从PeeringDB、CAIDA等公开数据库收集ISP数据，评估树基模型、神经网络和Transformer三类机器学习模型

Result: 树基模型（特别是XGBoost）表现最佳，准确率达98%，对时间、空间变化和缺失数据具有强鲁棒性

Conclusion: 该方法可实现ISP对等伙伴选择的全自动化，推动互联网生态系统向更高效优化方向发展

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [47] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: HISPASpoof是首个大规模西班牙语合成语音检测数据集，包含6种口音的真实语音和6种零样本TTS系统生成的合成语音，用于解决西班牙语在语音取证领域的不足。


<details>
  <summary>Details</summary>
Motivation: 虽然英语和中文的合成语音检测器发展迅速，但拥有6亿多使用者的西班牙语在语音取证领域代表性不足，需要专门的检测数据集和方法。

Method: 构建HISPASpoof数据集，包含真实语音和多种零样本TTS系统生成的合成语音，评估了5种代表性检测方法在西班牙语上的性能。

Result: 基于英语训练的检测器无法有效泛化到西班牙语，而使用HISPASpoof训练能显著提升检测性能，同时也能有效进行合成语音来源溯源。

Conclusion: HISPASpoof为西班牙语语音取证提供了重要基准，有助于推进可靠和包容的语音取证技术发展。

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [48] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 这篇论文提出了一种无需训练的适配性token合并框架，用于在6G网络中减少视觉Transformer模型的计算开销和传输资源消耗


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型在语义通信中展现强大能力，但计算需求高，在资源受限的6G网络中部署困难

Method: 使用高斯过程贝叶斯优化来解决层级token合并比例的多目标优化问题，构建Pareto最优配置前沿

Result: 方法在各种信器比特性条件下都较基线更优，显著减少浮点运算量的同时保持竞争性的准确性

Conclusion: 该研究为未来边缘智能系统中部署Transformer基于语义通信提供了可扩展和高效的方法

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [49] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 量子启发的QLSTM与QA3C强化学习模型在USD/TWD交易中实现11.87%收益，最大回撤仅0.92%，优于传统货币ETF


<details>
  <summary>Details</summary>
Motivation: 结合量子启发神经网络与深度强化学习，为金融交易提供新途径，特别是在外汇市场趋势预测和风险控制方面

Method: 集成量子长短期记忆(QLSTM)进行短期趋势预测，结合量子异步优势行动者-评论家(QA3C)算法，使用多核训练，状态设计包含QLSTM特征和技术指标

Result: 在2000-2025年数据上训练测试(80/20分割)，多头策略5年收益11.87%，最大回撤0.92%，超越多个货币ETF表现

Conclusion: 混合模型在外汇交易中表现竞争力，QLSTM适合小利润紧风险交易，未来需改进量子模拟和策略复杂度

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [50] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间实施长度公平裁剪来解决PPO/GRPO方法中的长度偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式裁剪时存在长度偏差问题，固定裁剪范围会系统性地对长短响应进行不同权重调整，扭曲有效目标。

Method: 提出FSPO方法，在序列对数IS比率上应用高斯启发的裁剪带，包含KL校正漂移项并按√L缩放，确保长度公平性。

Result: FSPO在不同长度区间内平坦化裁剪率，稳定训练过程，在多个评估数据集上优于所有基线方法。

Conclusion: FSPO通过理论分析和实证验证，有效解决了序列级RL中的长度公平性问题，提供了更稳定和优越的性能。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [51] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 深度学习天气预测模型的当前评估指标存在"统计相似性陷阱"，奖励模糊预测却漏掉稀有高影响事件。本文提出DART框架，通过双解码器结构和专门为极端对流检测优化的设计，显著提升了危险对流检测的精确度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习天气模型的评估指标存在显著缺陷，导致模糊预测在统计指标上表现良好，却无法检测关键的极端天气事件。这称为"统计相似性陷阱"，对极端天气预防造成严重影响。

Method: 提出DART框架，采用双解码器简化架构，包括：1)显式的背景/极端分解；2)物理动机过采样；3)任务特定损失函数。该框架将粗糕大气预报转换为高分辨率卫星亮温场，专门优化于检测下限230K的极端对流。

Result: 1)实证验证了统计相似性陷阱的存在；2)发现"IVT矛盾"，移除总水气运输反而提升极端对流检测效果270%；3)DART在相同CSI值下偏差从6.72降至2.52，CSI达到0.273；4)通过2023年8月吉大钢洪洋灾害实际验证。

Conclusion: DART框架系统性解决了混合转换-分割-下量缩任务的挑战，为极端天气预防提供了可靠的AI解决方案。该框架训练时间短（<10分钟），无缝集成到现有气象工作流程，显示了AI在极端天气准备中的应用潜力。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [52] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O算法，通过自适应激励机制和渐进惩罚策略解决约束强化学习中的边界稳定性问题，在连续控制任务中优于现有安全RL方法。


<details>
  <summary>Details</summary>
Motivation: 约束强化学习需要在最大化回报的同时满足安全约束，但在连续控制设置中，策略优化方法在约束边界附近表现不稳定，导致训练性能不佳。

Method: 引入自适应激励机制，在接近约束边界前保持约束范围内；提出IP3O算法，采用渐进增加的惩罚机制来稳定训练动态。

Result: 在基准环境上的实证评估显示，IP3O相比最先进的安全RL算法表现出更好的性能，并提供了最坏情况最优性误差的理论保证。

Conclusion: IP3O算法有效解决了约束强化学习中的边界稳定性问题，通过渐进惩罚机制实现了更好的训练稳定性和性能表现。

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [53] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 该研究通过机器学习方法确定了农业旅游发展的关键指标，LASSO算法组合逻辑回归模型达到最高几何准确率


<details>
  <summary>Details</summary>
Motivation: 农业旅游作为促进农村发展的经济模式，需要详细研究其增长策略和关键指标

Method: 两阶段研究方法：通过文献综述识别关键指标，然后使用LASSO算法组合多种机器学习分类器进行特征选择

Result: LASSO算法组合逻辑回归模型在70-30%数据分割下达到98%几何准确率，在80-20%数据分割下达到99%几何准确率

Conclusion: 机器学习方法可以有效识别农业旅游发展的关键因素，为政策制定者提供科学依据

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [54] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde框架结合数据抽象、图神经网络和强化学习，为具有丰富结构化状态的决策问题生成归纳策略函数，在未见过的测试实例上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决具有丰富结构化状态（如对象类和关系）的决策问题，需要能够处理不同规模和结构问题的归纳策略函数。

Method: 将MDP状态表示为实体事实数据库，转换为二分图，通过神经消息传递映射到潜在状态。使用监督学习和强化学习训练策略，在训练和测试实例分离的设置下评估泛化能力。

Result: Vejde策略在未见过的测试实例上平均泛化良好，得分损失不显著，且平均得分接近针对特定实例训练的MLP代理。

Conclusion: Vejde框架能够有效处理结构化状态决策问题，展现出强大的泛化能力，为复杂决策问题的解决提供了有效途径。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [55] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 提出MMT-FD方法解决旋转机械故障诊断中标注数据稀缺和模型泛化能力不足的问题，通过多注意力元变换器和对比学习实现少样本无监督诊断


<details>
  <summary>Details</summary>
Motivation: 工业应用中获取大量标注故障数据困难且成本高，不同机械设备需要单独训练诊断模型，缺乏通用性

Method: 结合时频域编码器和元学习模型，通过随机增强生成状态表示，使用对比学习迭代优化，最后用少量标注数据微调

Result: 在轴承故障数据集和转子试验台数据上验证，仅用1%标注数据就达到99%的诊断准确率，展现强大泛化能力

Conclusion: MMT-FD框架有效解决了少样本条件下的旋转机械故障诊断问题，具有高效率和强泛化能力，适用于多种机械设备

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [56] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 提出EMPG框架解决LLM智能体在长时任务中因稀疏奖励导致的信用分配问题，通过熵调制策略梯度来稳定学习动态，在多个任务上显著超越基线方法


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长时任务中面临稀疏奖励导致的中期步骤信用分配困难，传统方法主要关注创建密集奖励信号，但忽略了学习动态中策略梯度幅度与熵的耦合问题

Method: 提出熵调制策略梯度(EMPG)框架，基于步骤不确定性和最终任务结果重新校准学习信号，包括放大自信正确动作的更新、惩罚自信错误、衰减不确定步骤的更新，并引入未来清晰度奖励项

Result: 在WebShop、ALFWorld和DeepSearch三个挑战性智能体任务上的综合实验表明，EMPG实现了显著的性能提升，显著优于强策略梯度基线方法

Conclusion: EMPG通过调制策略梯度解决了LLM学习动态中的根本问题，为长时任务中的信用分配提供了有效解决方案，在多个基准任务上表现出优越性能

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [57] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 通过数据驱动的DRSALife模型学习Soft人工生命规则集，用于反应-正分体系统的出现动力学识别，达到74%准确率且具有稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决在无物理先验知识情况下的系统识别挑战，通过学习Soft人工生命模型来探索反应-正分体系统的出现动力学。

Method: 使用DRSALife概念框架学习Soft人工生命规则集，包括组织自动机和细胞自动机模型，并在噪声和稀疏数据集上进行实验。

Result: 学习到的模型能够准确预测出现动力学（74%准确率），具有良好的稳健性，并成功识别了基础偏微方程的结构和参数。

Conclusion: DRSALife模型为无先验知识情况下的系统识别提供了有效方法，在噪声和稀疏数据环境中仍保持良好性能。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [58] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: 提出了MoSE框架，通过匿名游走提取子图并动态路由到专家网络，增强GNN的结构表达能力，在理论和实验上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖局部消息传递，难以捕捉复杂高阶子图模式，现有基于随机游走的方法局限于图级任务且缺乏灵活性。

Method: 使用匿名游走提取信息子图，基于结构语义动态路由到专门化专家网络，在Subgraph Weisfeiler-Lehman测试框架下理论证明其表达能力。

Result: 在多个图任务上优于竞争基线，可视化显示模型能学习到可解释的结构模式。

Conclusion: MoSE框架提供了灵活且可解释的子图表示学习方法，在结构表达能力和任务适用性方面都有显著提升。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [59] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出了一种基于多项式核的新HGR相关系数计算方法，相比现有方法具有更好的鲁棒性和确定性，适用于实际机器学习应用


<details>
  <summary>Details</summary>
Motivation: HGR相关系数作为皮尔逊相关系数的非线性扩展，在算法公平性、科学分析和因果发现中有重要应用，但现有计算方法存在偏差-方差权衡问题，影响实际应用的鲁棒性

Method: 使用用户可配置的多项式核来计算HGR相关系数，提供了更快的计算速度和几乎同等有效的限制条件

Result: 新方法在鲁棒性和确定性方面具有显著优势，实验验证了其在约束机器学习框架中的适用性，计算得到的次梯度可作为损失正则化器

Conclusion: 该方法为HGR相关系数的计算提供了更可靠的解决方案，特别适合实际应用场景中的约束机器学习任务

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [60] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，结合元学习、可解释AI和高效LLM推理，无需额外试验即可推荐最优超参数和预训练模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AutoML方法依赖试错和昂贵API，缺乏可解释性和泛化性，需要更高效的自动化深度学习模型选择方案。

Method: 利用历史实验结果的SHAP解释，结合元学习和LLM推理，采用LLM-as-judge评估控制输出质量。

Result: 在8个医学影像数据集上，MetaLLMiX达到或超越传统HPO方法性能，计算成本大幅降低，响应时间减少99.6-99.9%，在5/8任务上获得最优结果。

Conclusion: MetaLLMiX提供了一种高效、可解释的超参数优化方案，显著降低了计算开销，在保持精度的同时实现了快速部署。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [61] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现大型语言模型生成的自我反事实解释存在有效性与最小性之间的权衡问题，通常要么修改过多要么修改不足，无法提供可靠的模型决策解释


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何通过自我生成的反事实解释来向人类解释其决策，这对于模型在关键场景中的可信部署至关重要

Method: 评估LLMs生成的反事实解释的有效性（是否达到预期结果）和最小性（是否只做必要修改），在多个LLMs、数据集和评估设置中进行测试

Result: LLMs通常能生成有效的反事实解释但远非最小化，而在要求最小化时又往往修改过少导致预测不变，存在明显的有效性-最小性权衡

Conclusion: 自我反事实解释作为可解释性工具效果有限甚至可能产生误导，在关键场景部署LLMs时必须考虑不可靠自我解释对下游决策的影响

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [62] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习与地质统计学的混合框架KpR，通过空间滞后特征增强ML模型，在土壤属性预测中显著提升精度30%


<details>
  <summary>Details</summary>
Motivation: 机器学习和地质统计学在土壤属性预测中各有优势但方法不同，需要结合两者优点来提升预测性能

Method: 使用Kriging Prior Regression (KpR)方法，通过普通克里金法生成空间滞后特征来丰富机器学习模型的空间上下文信息

Result: KpR与TabPFN模型结合在六个田间数据集上表现出可靠的预测性能和不确定性估计，平均R2比无空间上下文的机器学习算法提高约30%

Conclusion: KpR与TabPFN相结合为精准农业中的数字土壤制图提供了一个强大且通用的建模框架

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [63] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 本研究提出了fuser算法和SAC验证框架，用于分析微生物群落在不同环境条件下的动态关联网络，解决了传统方法在跨环境预测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统微生物共现网络推断算法通常只分析单一环境样本，无法捕捉微生物群落在不同生态条件下的动态适应过程，且常将不同环境样本混合分析而忽略了环境特异性。

Method: 提出Same-All Cross-validation (SAC)框架评估算法性能，并开发fuser算法，该算法在训练时保留子样本特异性信号的同时跨环境共享相关信息，生成环境特异性的预测网络。

Result: fuser在相同环境(Same)场景下与glmnet等现有算法性能相当，在跨环境(All)场景下显著降低了测试误差。

Conclusion: fuser算法能够有效处理微生物群落的时空动态变化，为跨环境微生物网络推断提供了新的解决方案，在异质性环境条件下表现出优越的预测性能。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [64] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: CSGD是首个通过concrete scores将score matching扩展到离散图的可控分子图生成模型，实现了多条件灵活控制，在四个分子数据集上取得了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 解决现有图扩散模型在多条件设置下效果有限的问题，传统方法依赖联合条件或连续松弛会损害保真度

Method: 提出CSGD模型，基于concrete scores扩展score matching到离散图；引入Composable Guidance (CoG)实现细粒度条件控制，以及Probability Calibration (PC)缓解训练-测试不匹配

Result: 在四个分子数据集上平均可控性比之前方法提升15.3%，同时保持高有效性和分布保真度

Conclusion: 基于score的建模在离散图生成中具有实际优势，能够实现灵活的多属性分子设计

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [65] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: 提出了AquaCast深度学习模型，通过多输入多输出架构结合内源变量和外源因素来预测城市水动力学，在真实和合成数据集上都表现出优于现有基准的性能。


<details>
  <summary>Details</summary>
Motivation: 解决城市水动力学预测的挑战，传统方法难以同时捕捉变量间和时间依赖关系，需要有效整合内源变量（如水高、流量）和外源因素（如降水历史和预报）。

Method: 开发AquaCast多输入多输出深度学习模型，使用嵌入层融合外源输入，避免对外源变量进行预测，专注于内源变量的预测，同时捕捉所有输入的变量间和时间依赖关系。

Result: 在LausanneCity数据集上达到最先进性能，仅使用内源变量时表现优异，加入外源变量和预报后性能进一步提升。在三个大规模合成数据集上的测试证实了模型的泛化能力和可扩展性。

Conclusion: AquaCast模型能够稳健准确地预测城市水动力学，在真实和合成数据集中 consistently 优于现有基线方法，证明了其有效性和通用性。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [66] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 开发了一个名为Agent-E的AI代理系统，能够自动从会议论文中识别特定地区的论文，并通过RPA完成预定操作（如提交提名表），在586篇论文测试中达到100%召回率和99.4%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献快速增长带来的发现挑战，减少学术发现所需的手动工作量，为研究人员、资助机构和学术团体提供自动化解决方案。

Method: 构建了一个完全自动化的系统管道，使用专门的AI代理Agent-E来识别特定地理区域的会议论文，然后通过机器人流程自动化（RPA）执行预定操作。

Result: 在5个不同会议的586篇论文上验证，系统成功识别所有目标论文，召回率达到100%，准确率接近完美（99.4%）。

Conclusion: 任务导向的AI代理不仅能够过滤信息，还能积极参与并加速学术社区的工作流程，展示了其在学术自动化方面的巨大潜力。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [67] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 基于时间规则的可解释时态知识图预测方法，在9个数据集上达到或超过现有方法的性能，且提供完全可解释的预测结果


<details>
  <summary>Details</summary>
Motivation: 解决时态知识图预测任务，受到最近使用重复事实的基准方法的启发，提出完全可解释的方法

Method: 学习四种简单类型的时间规则，使用考虑到最近性和频率的信心函数

Result: 在9个数据集上评估，该方法达到或超过八个最新模型和两个基准方法的性能

Conclusion: 该方法在保持高性能的同时提供了完全可解释的预测，为时态知识图预测领域提供了有效的解决方案

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [68] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 提出了一种新颖的图对齐框架，通过双通道编码器和几何感知功能映射模块同时增强节点区分度并确保跨图潜在空间几何一致性，在无监督图对齐任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图对齐方法存在两个关键问题：GNN嵌入中的过度平滑导致节点区分度降低，以及结构噪声、特征异质性和训练不稳定性导致的跨图潜在空间错位，最终导致不可靠的节点对应关系。

Method: 使用双通道编码器结合低通和高通谱滤波器生成既结构感知又高度区分的嵌入；引入几何感知功能映射模块学习图嵌入之间的双射等距变换，确保跨表示的一致几何关系。

Result: 在图基准测试中 consistently 优于现有无监督对齐基线，对结构不一致性和挑战性对齐场景表现出卓越的鲁棒性；在视觉-语言基准测试中能有效泛化，实现视觉和语言表示的无监督对齐。

Conclusion: 该框架通过同时处理节点区分度和跨图几何一致性，显著提升了无监督图对齐的性能和鲁棒性，并展示了跨领域的泛化能力。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [69] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 提出了D2P2-SGD优化器，结合动态差分隐私和随机投影技术，在保护隐私的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有DPSGD的静态噪声机制影响模型性能，且随着模型参数指数增长，随机优化器的效率面临挑战

Method: 结合动态差分隐私（自动梯度裁剪）和随机投影SGD，动态调整效用与隐私的权衡

Result: 在不同目标函数上表现出可证明的次线性收敛率，实验显示在保持隐私的同时显著提高准确率

Conclusion: DDP以隐私为代价带来更好的效用，随机投影使模型学习更高效，D2P2-SGD在隐私和性能间取得更好平衡

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [70] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 通过分析组基反事实解释的时间演化来解释概念漏流，提出三层框架进行全面诊断


<details>
  <summary>Details</summary>
Motivation: 动态环境中的机器学习模型遇到概念漏流问题，需要解释模型决策逻辑如何和为什么发生变化

Method: 跟踪组基反事实解释(GCEs)的聚类质心和反事实动作向量的演化，通过数据层、模型层和解释层的三层框架进行分析

Result: 该方法能够更全面地诊断漏流，区分不同根本原因，如数据分布移动与概念重新标签

Conclusion: 通过时间演化的反事实解释可以作为可解释的代理工具，揭示模型决策边界和逻辑的结构性变化

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [71] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个针对机器学习算法选择问题的大规模实验数据集，包含9,408个管道在300个数据集上的实验结果，旨在解决OpenML等现有平台在数据预处理管道多样性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的OpenML等实验共享平台存在管道多样性不足的问题，主要集中在少数流行技术上，导致算法选择问题的样本不平衡，无法充分代表各种数据预处理技术的组合。

Method: 构建PIPES数据集，通过系统性地组合选定的技术集合，创建9,408个不同的机器学习管道，并在300个数据集上执行实验，记录详细的管道块信息、训练测试时间、预测结果、性能指标和错误信息。

Result: 成功创建了一个包含大量多样化管道实验结果的综合数据集，涵盖了广泛的数据预处理技术组合，为元学习研究提供了更全面和平衡的样本基础。

Conclusion: PIPES数据集有效解决了现有实验平台的局限性，为算法选择问题的研究提供了更加多样化和完整的实验数据支持，并具有进一步扩展的潜力。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [72] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 使用原型网络和谱图表示进行少量样本学习，在呼吸音分类中实现了竞争性的准确率，多分类模型与二元模型性能相当


<details>
  <summary>Details</summary>
Motivation: 解决医疗诊断中标签数据有限的挑战，探索少量样本学习在COVID-19、流感和健康状态的咳喷声音分类中的可行性

Method: 采用原型网络(Prototypical Networks)结合咳喷声音的谱图表示，进行少量样本学习，比较多类和二元分类模型的性能

Result: 多类分类准确率达74.87%（每类15个支持示例），二元分类均超70%以上，流感最易区分，健康类最难，统计测试显示多类与二元模型性能无显著差异

Conclusion: 少量样本学习在医疗音响诊断中具有可行性，特别适用于标签数据稀缺的情况，多类分类模型与二元分类具有相当性能

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [73] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准不同保护子组的安全阈值来减少伤害并实现公平性目标（覆盖率或伤害均等化）


<details>
  <summary>Details</summary>
Motivation: 为了解决离线强化学习中的安全性和公平性问题，特别是在医疗保健等敏感领域需要减少伤害并确保决策对不同保护子组的公平性

Method: 使用可行性引导的公平自适应强化学习（FG-FARL），校准每个子组的安全阈值，并与行为克隆（BC）和HACO（混合自适应符合离线RL）基线进行比较

Result: FG-FARL在保持与基线相当的价值估计的同时，显著改善了公平性指标，通过Medicaid人群健康管理项目的真实数据验证了效果

Conclusion: FG-FARL提供了一种实用的方法来实现更安全和更公平的决策支持，特别是在医疗保健等需要平衡效果与公平性的领域

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [74] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出ProDiGy联邦学习算法，通过双重评分系统评估客户端梯度，在非IID数据下有效防御拜占庭攻击


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构环境下容易受到对抗攻击，现有防御机制在非IID数据分布下效果不佳

Method: 基于梯度接近性和差异性的联合双重评分系统来评估客户端梯度

Result: 在多种场景下优于现有防御方法，特别是在非IID数据分布时仍能保持强大的防御能力和模型精度

Conclusion: 双重视角方法能有效促进诚实客户端间的自然相似性，同时检测可疑一致性作为攻击指标

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [75] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: 使用可学习的蝶蝴变换替代固定哈达玛矩阵，通过层适配旋转来提升2比特量化的性能，在LLaMA-2-7B上将困惑度从22.1降低到15.4


<details>
  <summary>Details</summary>
Motivation: 现有的固定旋转方法（如QuIP和QuaRot）使用一成不变的哈达玛矩阵，无法适应不同转换器层的特定异常值模式，导致极端2比特量化时性能严重下降

Method: 提出ButterflyQuant方法，用可学习的蝶蝴变换替代固定旋转，通过连续的Givens旋转角参数化保证正交性，并为激活值添加均匀性正则化以促进量化友好的分布

Result: 在LLaMA-2-7B模型上进行2比特量化，ButterflyQuant将困惑度从QuaRot的22.1显著提升到15.4，学习仅需128个校准样本和单GPU分钟级别的计算成本

Conclusion: 层适配学习旋转能够有效提升极端量化的性能，蝶蝴变换的连续参数化为正交变换提供了可学习且高效的方案

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


### [76] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出一种针对随机和混沌时空系统的深度学习模拟器，能够根据PDE参数值进行条件化模拟，通过预训练和微调实现参数泛化，并提供计算加速和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统数值积分方法计算成本高，难以高效探索参数空间。需要开发能够处理不同参数值、域大小和分辨率的计算高效模拟器，同时提供不确定性量化。

Method: 采用预训练+微调策略，在单一参数域预训练后，使用多样化小数据集微调以实现参数泛化。引入局部注意力机制处理可变域大小和分辨率，支持从小域预训练扩展到更大域。

Result: 在混沌Kuramoto-Sivashinsky方程和随机强迫beta-plane湍流上验证，模型能够捕捉插值参数值的现象，相比传统数值积分提供显著计算加速，概率变体提供不确定性量化。

Conclusion: 该方法成功实现了参数条件化的时空系统模拟，提供计算高效的参数空间探索能力，并通过概率变体支持罕见事件的统计研究，为复杂系统模拟提供了有效工具。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [77] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO是一种新颖的数据稀疏算子学习算法，通过贪婪算法构建网络结构，实现离散化不变性，显著优于现有算子学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决多输入PDE组的算子学习问题，减少泛化差距，实现严格的离散化不变性。

Method: 结合降基方法和生成预训练物理信息神经网络，使用贪婪算法离线构建网络结构，通过任务特定激活函数进行知识蒸馏。

Result: 在分布内和分布外测试中显著优于PCA-Net、DeepONet、FNO和CNO等先进算法，是唯一实现严格离散化不变性的算子学习算法。

Conclusion: ReBaNO通过数学严谨的贪婪算法和紧凑架构，在计算成本最小化的同时实现了优异的性能和离散化不变性。

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [78] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: 提出基于功能基团的分子表示框架FGR，结合化学知识定义和模式挖掘的功能基团，在33个基准数据集上实现最先进性能，同时保持化学可解释性


<details>
  <summary>Details</summary>
Motivation: 深度学习分子预测模型缺乏可解释性，阻碍化学家采用。需要开发既高性能又化学可解释的分子表示方法

Method: 使用功能基团(FG)概念构建分子表示，整合化学知识定义的功能基团和从大规模分子语料库中挖掘的模式功能基团(MFG)，通过预训练编码到低维潜在空间，并可包含2D结构描述符

Result: 在物理化学、生物物理、量子力学、生物活性和药代动力学等33个多样化基准数据集上达到最先进性能

Conclusion: FGR框架是开发高性能、化学可解释深度学习模型的重要进展，使化学家能够直接将预测性质与特定功能基团关联，促进对结构-性质关系的新见解

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 本文通过大规模实验对比了AI搜索和传统网络搜索，提出了生成式引擎优化(GEO)新概念，为在生成式AI搜索时代获得可见性提供实践指南。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索引擎的快速普及正在改变信息检索方式，从传统的排名列表转向综合性答案，这对传统SEO实践构成挑战，需要新的优化框架。

Method: 进行大规模、受控实验，涵盖多个垂直领域、语言和查询重写，定量分析AI搜索和Google在信息来源方面的关键差异。

Result: 发现AI搜索存在系统性偏见，偏好权威第三方来源，而非品牌自有内容和社交内容；不同AI搜索服务在域名多样性、新鲜度、跨语言稳定性和语法敏感性方面存在显著差异。

Conclusion: 提出了GEO战略议程，包括：(1)为机器扫描和证成设计内容，(2)控制权威媒体建立AI认知权威，(3)采用引擎特定和语言敏感策略，(4)克服大品牌偏见。为生成式搜索时代的可见性提供基础框架。

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [80] [Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation](https://arxiv.org/abs/2509.09037)
*Amanda Aird,Ben Armstrong,Nicholas Mattei,Robin Burke*

Main category: cs.IR

TL;DR: 本文认为嫉妒自由性(EF)和EF-1在个性化推荐系统中不适合作为公平性衡量标准


<details>
  <summary>Details</summary>
Motivation: 嫉妒自由性概念在经济学和推荐系统中被广泛使用，但作者认为在个性化场景下这一标准并不适用

Method: 通过概述嫉妒自由性在经济学和推荐系统中的应用，分析其在个性化环境中的局限性

Result: 论证了嫉妒自由性不适用于需要个性化处理的场景

Conclusion: 在个性化推荐系统中，需要重新考虑和定义公平性标准，而不是简单套用传统的嫉妒自由性概念

Abstract: Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have
been used as fairness concepts in the economics, game theory, and social choice
literatures since the 1960s, and have recently gained popularity within the
recommendation systems communities. In this short position paper we will give
an overview of envy-freeness and its use in economics and recommendation
systems; and illustrate why envy is not appropriate to measure fairness for use
in settings where personalization plays a role.

</details>


### [81] [Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation](https://arxiv.org/abs/2509.09114)
*Kelin Ren,Chan-Yang Ju,Dong-Ho Lee*

Main category: cs.IR

TL;DR: MambaRec是一个新颖的多模态推荐框架，通过注意力引导学习整合局部特征对齐和全局分布正则化，解决了现有方法在细粒度跨模态关联建模和全局一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统主要依赖静态融合策略或基于图的局部交互建模，存在两个关键局限：(1) 细粒度跨模态关联建模能力不足，导致融合质量欠佳；(2) 缺乏全局分布级一致性，造成表示偏差。

Method: 提出MambaRec框架，核心是Dilated Refinement Attention Module (DREAM)，使用多尺度扩张卷积结合通道和空间注意力来对齐视觉和文本模态的细粒度语义模式。同时应用最大均值差异(MMD)和对比损失函数约束全局模态对齐，并采用降维策略降低计算成本。

Result: 在真实电商数据集上的大量实验表明，MambaRec在融合质量、泛化能力和效率方面均优于现有方法。

Conclusion: MambaRec通过局部特征对齐和全局分布正则化的双重机制，有效提升了多模态推荐系统的性能，为解决跨模态语义建模和表示一致性问题提供了有效解决方案。

Abstract: Multimodal recommendation systems are increasingly becoming foundational
technologies for e-commerce and content platforms, enabling personalized
services by jointly modeling users' historical behaviors and the multimodal
features of items (e.g., visual and textual). However, most existing methods
rely on either static fusion strategies or graph-based local interaction
modeling, facing two critical limitations: (1) insufficient ability to model
fine-grained cross-modal associations, leading to suboptimal fusion quality;
and (2) a lack of global distribution-level consistency, causing
representational bias. To address these, we propose MambaRec, a novel framework
that integrates local feature alignment and global distribution regularization
via attention-guided learning. At its core, we introduce the Dilated Refinement
Attention Module (DREAM), which uses multi-scale dilated convolutions with
channel-wise and spatial attention to align fine-grained semantic patterns
between visual and textual modalities. This module captures hierarchical
relationships and context-aware associations, improving cross-modal semantic
modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive
loss functions to constrain global modality alignment, enhancing semantic
consistency. This dual regularization reduces mode-specific deviations and
boosts robustness. To improve scalability, MambaRec employs a dimensionality
reduction strategy to lower the computational cost of high-dimensional
multimodal features. Extensive experiments on real-world e-commerce datasets
show that MambaRec outperforms existing methods in fusion quality,
generalization, and efficiency. Our code has been made publicly available at
https://github.com/rkl71/MambaRec.

</details>


### [82] [CESRec: Constructing Pseudo Interactions for Sequential Recommendation via Conversational Feedback](https://arxiv.org/abs/2509.09342)
*Yifan Wang,Shen Gao,Jiabao Fang,Rui Yan,Billy Chiu,Shuo Shang*

Main category: cs.IR

TL;DR: CESRec是一个新颖的推荐框架，将序列推荐系统的长期偏好建模与对话推荐系统的实时偏好获取相结合，通过语义伪交互构建和双重对齐异常项掩码实现更好的推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法依赖协同过滤信号但无法捕捉实时用户偏好，而对话推荐系统擅长获取即时兴趣但忽略历史行为。需要弥合这一差距。

Method: 提出语义伪交互构建动态更新用户历史交互序列，生成结合长期和实时偏好的伪交互序列；使用双重对齐异常项掩码识别和掩码偏离用户核心偏好的历史项。

Result: 大量实验表明CESRec通过增强强SRS模型实现了最先进的性能，验证了将对话反馈整合到SRS中的有效性。

Conclusion: CESRec成功整合了序列推荐和对话推荐的优点，为推荐系统提供了更全面的用户偏好建模能力。

Abstract: Sequential Recommendation Systems (SRS) have become essential in many
real-world applications. However, existing SRS methods often rely on
collaborative filtering signals and fail to capture real-time user preferences,
while Conversational Recommendation Systems (CRS) excel at eliciting immediate
interests through natural language interactions but neglect historical
behavior. To bridge this gap, we propose CESRec, a novel framework that
integrates the long-term preference modeling of SRS with the real-time
preference elicitation of CRS. We introduce semantic-based pseudo interaction
construction, which dynamically updates users'historical interaction sequences
by analyzing conversational feedback, generating a pseudo-interaction sequence
that seamlessly combines long-term and real-time preferences. Additionally, we
reduce the impact of outliers in historical items that deviate from users'core
preferences by proposing dual alignment outlier items masking, which identifies
and masks such items using semantic-collaborative aligned representations.
Extensive experiments demonstrate that CESRec achieves state-of-the-art
performance by boosting strong SRS models, validating its effectiveness in
integrating conversational feedback into SRS.

</details>


### [83] [We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later](https://arxiv.org/abs/2509.09414)
*Alan Said,Maria Soledad Pera,Michael D. Ekstrand*

Main category: cs.IR

TL;DR: 该论文批判推荐系统研究领域长期存在的根本性问题，指出2011年Amatriain提出的警告至今仍未解决，反而在错误基础上增加了更多复杂性。作者呼吁需要从根本上重新定义研究目标、服务对象和知识生产方式。


<details>
  <summary>Details</summary>
Motivation: 重新审视Amatriain 2011年对推荐系统研究的批评，证明这些概念性、认识论和基础设施方面的失败至今仍然存在，只是以更微妙或系统性的形式持续。

Method: 基于可重复性研究、评估方法学、环境影响和参与式设计等近期工作，分析领域复杂性增长与反思不足的矛盾，并展示社区主导的范式转变倡议。

Result: 发现推荐系统研究的加速复杂性已经超过了其自我反思能力，许多根本问题仍然存在，需要根本性的重新框架而不仅仅是技术改进。

Conclusion: 有意义的变革不仅需要新指标或更好工具，更需要从根本上重新定义推荐系统研究的目的、服务对象和知识生产验证方式，建立基于认识谦逊、人类影响和可持续实践的研究议程。

Abstract: In 2011, Xavier Amatriain sounded the alarm: recommender systems research was
"doing it all wrong" [1]. His critique, rooted in statistical misinterpretation
and methodological shortcuts, remains as relevant today as it was then. But
rather than correcting course, we added new layers of sophistication on top of
the same broken foundations. This paper revisits Amatriain's diagnosis and
argues that many of the conceptual, epistemological, and infrastructural
failures he identified still persist, in more subtle or systemic forms. Drawing
on recent work in reproducibility, evaluation methodology, environmental
impact, and participatory design, we showcase how the field's accelerating
complexity has outpaced its introspection. We highlight ongoing community-led
initiatives that attempt to shift the paradigm, including workshops, evaluation
frameworks, and calls for value-sensitive and participatory research. At the
same time, we contend that meaningful change will require not only new metrics
or better tooling, but a fundamental reframing of what recommender systems
research is for, who it serves, and how knowledge is produced and validated.
Our call is not just for technical reform, but for a recommender systems
research agenda grounded in epistemic humility, human impact, and sustainable
practice.

</details>


### [84] [Boosting Data Utilization for Multilingual Dense Retrieval](https://arxiv.org/abs/2509.09459)
*Chao Huang,Fengran Mo,Yufeng Chen,Changhao Guan,Zhenrui Yue,Xinyu Wang,Jinan Xu,Kaiyu Huang*

Main category: cs.IR

TL;DR: 通过提高负样本质量和批处理效果来改善多语言密集检索的方法，在16种语言的MIRACL数据集上超越了多个基线模型


<details>
  <summary>Details</summary>
Motivation: 多语言密集检索的挑战在于将不同语言的表示对齐到同一向量空间中，现有方法依赖于负样本质量和批处理效果

Method: 提出了一种提高数据利用率的方法，通过获取高质量的难负样本和有效的小批处数据来改善多语言密集检索

Result: 在包含16种语言的MIRACL多语言检索基准数据集上进行了涉及广泛的实验，结果显示方法有效，超越了多个现有强基线模型

Conclusion: 通过重点关注数据利用率而非模型架构的复杂性，可以有效提升多语言密集检索的性能

Abstract: Multilingual dense retrieval aims to retrieve relevant documents across
different languages based on a unified retriever model. The challenge lies in
aligning representations of different languages in a shared vector space. The
common practice is to fine-tune the dense retriever via contrastive learning,
whose effectiveness highly relies on the quality of the negative sample and the
efficacy of mini-batch data. Different from the existing studies that focus on
developing sophisticated model architecture, we propose a method to boost data
utilization for multilingual dense retrieval by obtaining high-quality hard
negative samples and effective mini-batch data. The extensive experimental
results on a multilingual retrieval benchmark, MIRACL, with 16 languages
demonstrate the effectiveness of our method by outperforming several existing
strong baselines.

</details>


### [85] [AskDoc -- Identifying Hidden Healthcare Disparities](https://arxiv.org/abs/2509.09622)
*Shashank Gupta*

Main category: cs.IR

TL;DR: 这项研究分析了Reddit AskDoc社区的在线医生咨询服务，发现该平台上白人男性占主导地位，且医生参与度较低，存在一些人口统计学特征的差异参与情况。


<details>
  <summary>Details</summary>
Motivation: 研究线上"Ask the Doctor"服务是否反映了现实医疗体系中的障碍和偏见，以及这些服务在不同人口统计学组之间的可达性和参与情况。

Method: 从2020年1月到2022年5月下载AskDoc子红板数据，使用正则表达式识别自我报告的人口统计学特征（性别、种族、年龄），并进行统计分析以了解同伴和医生与发帖者的互动情况。

Result: 50%的帖子没有收到评论；90%以上用户披露性别和年龄，80%用户不披露种族；社区以成年白人男性为主；不同人口统计学特征的发帖者在互动中存在差异；医生参与度较低。

Conclusion: 社交媒体可能缩短病人与提供者之间的距离，但目前医生的参与度低于发帖者。

Abstract: The objective of this study is to understand the online Ask the Doctor
services medical advice on internet platforms via AskDoc, a Reddit community
that serves as a public AtD platform and study if platforms mirror existing
hurdles and partiality in healthcare across various demographic groups. We
downloaded data from January 2020 to May 2022 from AskDoc -- a subreddit, and
created regular expressions to identify self-reported demographics (Gender,
Race, and Age) from the posts, and performed statistical analysis to understand
the interaction between peers and physicians with the posters. Half of the
posts did not receive comments from peers or physicians. At least 90% of the
people disclose their gender and age, and 80% of the people do not disclose
their race. It was observed that the subreddit is dominated by adult (age group
20-39) white males. Some disparities were observed in the engagement between
the users and the posters with certain demographics. Beyond the confines of
clinics and hospitals, social media could bring patients and providers closer
together, however, as observed, current physicians participation is low
compared to posters.

</details>


### [86] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: 该论文提出了一个针对无线电法规领域的RAG（检索增强生成）管道，创建了首个该领域的选择题评估集，在检索质量上达到97%准确率，并使GPT-4o的生成准确率相对提升近12%。


<details>
  <summary>Details</summary>
Motivation: 无线电法规是一个法律敏感且高风险领域，需要准确的问题回答系统。现有方法在该领域缺乏专门评估和优化方案。

Method: 构建电信专用的RAG管道，从权威来源自动筛选并人工验证创建选择题评估集，定义领域特定的检索指标，优化检索和生成过程。

Result: 检索器达到约97%的准确率，RAG管道使所有测试模型的生成准确率均得到提升，GPT-4o相对提升近12%（从不足1%到显著改善）。

Conclusion: 精心设计的领域针对性基础方法为监管问答提供了简单而强大的基准解决方案，证明了结构化检索在敏感领域的重要性。

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [87] [Koza and Koza-Hub for born-interoperable knowledge graph generation using KGX](https://arxiv.org/abs/2509.09096)
*Daniel R Korn,Patrick Golden,Aaron Odell,Katherina Cortes,Shilpa Sundar,Kevin Schaper,Sarah Gehrke,Corey Cox,Harry Caufield,Justin Reese,Evan Morris,Christopher J Mungall,Melissa Haendel*

Main category: cs.DB

TL;DR: Koza是一个Python软件包，通过KGX标准简化生物医学知识图谱构建，提供30个标准数据源的转换流程，将知识图谱摄入转化为基本操作并通过YAML配置实现。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学知识图谱构建方法存在大量冗余劳动，主要原因是缺乏数据标准和"知识图谱就绪"的数据源。

Method: 使用KGX标准，将知识图谱摄入转化为一组基本操作，通过YAML文件提供配置，并强制遵守选定的数据模式。

Result: 开发了Koza软件包和Koza-Hub，为30个黄金标准生物医学数据源提供了转换流程。

Conclusion: 该方法通过标准化和自动化解决了生物医学知识图谱构建中的冗余劳动问题。

Abstract: Knowledge graph construction has become an essential domain for the future of
biomedical research. But current approaches demand a high amount of redundant
labor. These redundancies are the result of the lack of data standards and
"knowledge-graph ready" data from sources. Using the KGX standard, we aim to
solve these issues. Herein we introduce Koza and the Koza-Hub, a Python
software package which streamlines ingesting raw biomedical information into
the KGX format, and an associated set of conversion processes for thirty gold
standard biomedical data sources. Our approach is to turn knowledge graph
ingests into a set of primitive operations, provide configuration through YAML
files, and enforce compliance with the chosen data schema.

</details>


### [88] [Let's Simply Count: Quantifying Distributional Similarity Between Activities in Event Data](https://arxiv.org/abs/2509.09440)
*Henrik Kirchmann,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Matthias Weidlich*

Main category: cs.DB

TL;DR: 本文主张在过程挖掘中使用基于计数的简单嵌入方法来建模活动分布相似性，替代复杂的神经网络方法，在保持效果的同时提高计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有过程挖掘中的分布相似性方法采用神经网络（如word2vec和自编码器），虽然有效但存在计算成本高和可解释性差的问题。本文提倡简化建模方法。

Method: 引入基于计数的嵌入方法，避免复杂训练过程，提供直接可解释的表示。建立了全面的基准测试框架，评估嵌入的内在质量、下游应用性能和计算效率。

Result: 实验表明，基于计数的嵌入在事件数据中活动间的分布相似性建模方面提供了高效且有效的解决方案，与现有技术相比表现优异。

Conclusion: 基于计数的简单嵌入方法在过程挖掘中是一个有前途的替代方案，能够在保持性能的同时显著提高计算效率和模型可解释性。

Abstract: To obtain insights from event data, advanced process mining methods assess
the similarity of activities to incorporate their semantic relations into the
analysis. Here, distributional similarity that captures similarity from
activity co-occurrences is commonly employed. However, existing work for
distributional similarity in process mining adopt neural network-based
approaches as developed for natural language processing, e.g., word2vec and
autoencoders. While these approaches have been shown to be effective, their
downsides are high computational costs and limited interpretability of the
learned representations.
  In this work, we argue for simplicity in the modeling of distributional
similarity of activities. We introduce count-based embeddings that avoid a
complex training process and offer a direct interpretable representation. To
underpin our call for simple embeddings, we contribute a comprehensive
benchmarking framework, which includes means to assess the intrinsic quality of
embeddings, their performance in downstream applications, and their
computational efficiency. In experiments that compare against the state of the
art, we demonstrate that count-based embeddings provide a highly effective and
efficient basis for distributional similarity between activities in event data.

</details>


### [89] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 提出了一种新的机器学习模型解释框架，通过视图定义突出数据库中对预测贡献最大的部分，以解释关系数据库上深度学习模型的决策过程。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络等深度学习模型在关系数据库上的复杂性使得模型预测难以人类理解，需要一种可解释的方法来说明模型如何使用数据进行决策。

Method: 基于Nash等人(2010)的决定性概念，通过视图定义突出数据库的关键部分。提出了模型无关和专门为异构GNN设计的算法，包括可学习遮置技术，避免在所有数据库空间中的穷举搜索。

Result: 在RelBench集合上进行了涉及多个领域和不同记录级任务的广泛实验研究，结果表明所提出的解释方法具有实用性，且生成效率高。

Conclusion: 该框架为关系数据库上的深度学习模型提供了有效的解释方法，通过控制决定性与简洁性的交换以及不同粒度级别的视图定义，实现了可理解的全局解释。

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [90] [Improved Approximation Guarantees and Hardness Results for MNL-Driven Product Ranking](https://arxiv.org/abs/2509.09180)
*Danny Segev,Gidi Steinberg*

Main category: cs.DS

TL;DR: 本文针对市场占有率排名问题的计算复杂性进行研究，提出了多项式时间近似方案(PTAS)，证明了问题的强NP-hard性质，并引入了黑盒约简方法。


<details>
  <summary>Details</summary>
Motivation: 解决Derakhshan等人(2022)提出的市场占有率排名问题的开放计算问题，该问题结合了多项Logit选择模型和基于搜索的考虑-选择范式。

Method: 利用新技术发展和分析思路，结合修订原始见解，设计了多项式时间近似方案。引入黑盒约简方法，将一般实例映射到"有界比率"实例，获得准PTAS。

Result: 成功开发了多项式时间近似方案，证明了问题的强NP-hard性质，提出了易于实现的准PTAS算法。

Conclusion: 本文为市场占有率排名问题提供了完整的计算特征描述，既证明了计算困难性，又提供了有效的近似解决方案。

Abstract: In this paper, we address open computational questions regarding the market
share ranking problem, recently introduced by Derakhshan et al. (2022). Their
modelling framework incorporates the extremely popular Multinomial Logit (MNL)
choice model, along with a novel search-based consider-then-choose paradigm. In
a nutshell, the authors devised a Pandora's-Box-type search model, where
different customer segments sequentially screen through a ranked list of
products, one position after the other, forming their consideration set by
including all products viewed up until terminating their inspection procedure.
Subsequently, a purchasing decision out of this set is made based on a joint
MNL choice model.
  Our main contribution consists in devising a polynomial-time approximation
scheme for the market share ranking problem, utilizing fresh technical
developments and analytical ideas, in conjunction with revising the original
insights of Derakhshan et al. (2022). Along the way, we introduce a black-box
reduction, mapping general instances of the market share ranking problem into
``bounded ratio'' instances, showing that this result directly leads to an
elegant and easily-implementable quasi-PTAS. Finally, to provide a complete
computational characterization, we prove that the market share ranking problem
is strongly $\mathrm{NP}$-hard.

</details>


### [91] [Additive Approximation Schemes for Low-Dimensional Embeddings](https://arxiv.org/abs/2509.09652)
*Prashanti Anderson,Ainesh Bakshi,Samuel B. Hopkins*

Main category: cs.DS

TL;DR: 本文提出了第一个针对k维欧几里得度量违反问题（k-EMV）的多项式时间加法近似方案，解决了高维数据低维嵌入的优化问题


<details>
  <summary>Details</summary>
Motivation: k-EMV问题在统计学中已有70多年研究历史（称为多维缩放），但除了k=1的情况外，没有已知的高效近似算法。Dhamdhere提出了O(log(n))近似算法，但寻找PTAS一直是一个开放性问题

Method: 使用Sherali-Adams/Sum-of-Squares松弛的相关舍入新分析技术，专门针对低维嵌入问题进行优化

Result: 获得了目标值为OPT_EMV + ε‖D‖₂²的嵌入，时间复杂度为(n·B)^poly(k, ε⁻¹)，其中B是每个条目的比特数

Conclusion: 这是实现k-EMV问题PTAS的关键第一步，相关技术还可应用于加权k-EMV变体和ℓ_p低秩近似问题（p>2）

Abstract: We consider the task of fitting low-dimensional embeddings to
high-dimensional data. In particular, we study the $k$-Euclidean Metric
Violation problem ($\textsf{$k$-EMV}$), where the input is $D \in
\mathbb{R}^{\binom{n}{2}}_{\geq 0}$ and the goal is to find the closest vector
$X \in \mathbb{M}_{k}$, where $\mathbb{M}_k \subset
\mathbb{R}^{\binom{n}{2}}_{\geq 0}$ is the set of all $k$-dimensional Euclidean
metrics on $n$ points, and closeness is formulated as the following
optimization problem, where $\| \cdot \|$ is the entry-wise $\ell_2$ norm: \[
  \textsf{OPT}_{\textrm{EMV}} = \min_{X \in \mathbb{M}_{k} } \Vert D - X
\Vert_2^2\,.\] Cayton and Dasgupta [CD'06] showed that this problem is NP-Hard,
even when $k=1$. Dhamdhere [Dha'04] obtained a $O(\log(n))$-approximation for
$\textsf{$1$-EMV}$ and leaves finding a PTAS for it as an open question
(reiterated recently by Lee [Lee'25]). Although $\textsf{$k$-EMV}$ has been
studied in the statistics community for over 70 years, under the name
"multi-dimensional scaling", there are no known efficient approximation
algorithms for $k > 1$, to the best of our knowledge.
  We provide the first polynomial-time additive approximation scheme for
$\textsf{$k$-EMV}$. In particular, we obtain an embedding with objective value
$\textsf{OPT}_{\textrm{EMV}} + \varepsilon \Vert D\Vert_2^2$ in $(n\cdot
B)^{\mathsf{poly}(k, \varepsilon^{-1})}$ time, where each entry in $D$ can be
represented by $B$ bits. We believe our algorithm is a crucial first step
towards obtaining a PTAS for $\textsf{$k$-EMV}$. Our key technical contribution
is a new analysis of correlation rounding for Sherali-Adams / Sum-of-Squares
relaxations, tailored to low-dimensional embeddings. We also show that our
techniques allow us to obtain additive approximation schemes for two related
problems: a weighted variant of $\textsf{$k$-EMV}$ and $\ell_p$ low-rank
approximation for $p>2$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [92] [Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance](https://arxiv.org/abs/2509.08976)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 本文提出使用博弈论作为统一框架来整合网络战中的攻防技术，通过AI增强的博弈模型实现从政策到技术实施的多层次战略优化，并以RedCyber案例验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现代网络战作为多域作战的核心领域，需要将分散的攻防技术整合为连贯战略，而现有研究往往关注孤立战术，缺乏整体性理解。

Method: 采用博弈论框架建模攻防交互，结合现代AI技术进行均衡分析、风险评估和战略推理，实现多层次网络战策略的设计与优化。

Result: 博弈论模型能够捕捉冲突的悖论逻辑和非线性动态，通过RedCyber合成网络冲突案例验证了该方法对网络作战相互依赖性的建模能力。

Conclusion: 博弈论为网络战提供了量化分析工具，未来研究方向包括韧性建设、跨层级规划以及AI在网络战中不断演变的角色。

Abstract: Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.

</details>


### [93] [Persuasion Gains and Losses from Peer Communication](https://arxiv.org/abs/2509.09099)
*Toygar T. Kerman,Anastas P. Tenev,Konstantin Zabarnyi*

Main category: cs.GT

TL;DR: 贝叶斯说服模型中，发送者通过扩展接收者通信网络可以获得严格收益，但收益并不随网络密度单调增加，某些网络结构扩展甚至能达到空网络的最优收益上限。


<details>
  <summary>Details</summary>
Motivation: 研究在通信网络中，发送者如何通过部分信息揭示来说服临界质量的接收者，以及接收者间增加的通信如何影响发送者的期望效用。

Method: 分析同质二元行动接收者在通信网络中的行为，每个接收者观察自己和邻居的私人消息，研究网络扩展对发送者效用的影响。

Result: 网络扩展可以严格有利于发送者，发送者从说服中获得的收益不随网络密度单调变化；许多网络扩展能达到所有网络中发送者期望效用的上界（空网络收益）；某些网络结构的微小修改会阻止这种有益扩展。

Conclusion: 研究结果警示不要假定更多通信必然导致更好的集体结果，网络结构对信息传播效果有重要影响。

Abstract: We study a Bayesian persuasion setting in which a sender wants to persuade a
critical mass of receivers by revealing partial information about the state to
them. The homogeneous binary-action receivers are located on a communication
network, and each observes the private messages sent to them and their
immediate neighbors. We examine how the sender's expected utility varies with
increased communication among receivers. We show that for general families of
networks, extending the network can strictly benefit the sender. Thus, the
sender's gain from persuasion is not monotonic in network density. Moreover,
many network extensions can achieve the upper bound on the sender's expected
utility among all networks, which corresponds to the payoff in an empty
network. This is the case in networks reflecting a clear informational
hierarchy (e.g., in global corporations), as well as in decentralized networks
in which information originates from multiple sources (e.g., influencers in
social media). Finally, we show that a slight modification to the structure of
some of these networks precludes the possibility of such beneficial extensions.
Overall, our results caution against presuming that more communication
necessarily leads to better collective outcomes.

</details>


### [94] [Mechanism Design with Outliers and Predictions](https://arxiv.org/abs/2509.09561)
*Argyrios Deligkas,Eduard Eiben,Sophie Klumper,Guido Schäfer,Artem Tsikiridis*

Main category: cs.GT

TL;DR: 本文研究了带异常值的机制设计问题，在线性设施选址中允许排除z个最远代理，发现排除异常值反而可能降低效率，并给出了确定性策略证明机制在功利主义和平均主义目标下的紧界。


<details>
  <summary>Details</summary>
Motivation: 研究当存在极端或非典型偏好的代理时，通过排除z个异常值来优化社会福利目标，在线性设施选址这一经典问题上探索异常值排除对机制设计的影响。

Method: 在线性设施选址问题中，允许机制排除z个距离设施最远的代理，分析确定性策略证明机制在功利主义和平均主义社会福利目标下的近似比性能。

Result: 当z≥n/2时，无法获得有界近似比；对于平均主义成本，选择第(z+1)顺序统计量是策略证明且2-近似的，且这是最优的；功利主义成本下近似保证随异常值增加而恶化，但通过预测可获得最优的一致性-鲁棒性权衡。

Conclusion: 排除异常值并不总是提高效率，在某些情况下反而会恶化性能，但通过合理设计机制（如利用预测信息）可以在异常值设置下获得良好的性能保证。

Abstract: We initiate the study of mechanism design with outliers, where the designer
can discard $z$ agents from the social cost objective. This setting is
particularly relevant when some agents exhibit extreme or atypical preferences.
As a natural case study, we consider facility location on the line: $n$
strategic agents report their preferred locations, and a mechanism places a
facility to minimize a social cost function. In our setting, the $z$ agents
farthest from the chosen facility are excluded from the social cost. While it
may seem intuitive that discarding outliers improves efficiency, our results
reveal that the opposite can hold.
  We derive tight bounds for deterministic strategyproof mechanisms under the
two most-studied objectives: utilitarian and egalitarian social cost. Our
results offer a comprehensive view of the impact of outliers. We first show
that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded
approximation for either objective. For egalitarian cost, selecting the $(z +
1)$-th order statistic is strategyproof and 2-approximate. In fact, we show
that this is best possible by providing a matching lower bound. Notably, this
lower bound of 2 persists even when the mechanism has access to a prediction of
the optimal location, in stark contrast to the setting without outliers. For
utilitarian cost, we show that strategyproof mechanisms cannot effectively
exploit outliers, leading to the counterintuitive outcome that approximation
guarantees worsen as the number of outliers increases. However, in this case,
access to a prediction allows us to design a strategyproof mechanism achieving
the best possible trade-off between consistency and robustness. Finally, we
also establish lower bounds for randomized mechanisms that are truthful in
expectation.

</details>


### [95] [Maximizing social welfare among EF1 allocations at the presence of two types of agents](https://arxiv.org/abs/2509.09641)
*Jiaxuan Ma,Yong Chen,Guangting Chen,Mingyang Gong,Guohui Lin,An Zhang*

Main category: cs.GT

TL;DR: 该论文研究在只有两种效用函数的情况下，最大化社会福利的公平分配问题，提出了改进的近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究在只有两种不同效用函数的代理之间分配不可分割物品的问题，目标是最大化功利社会福利，同时满足envy-free up to one item的公平标准。

Method: 提出了针对两种标准化效用函数的2-近似算法，改进了之前16√n的近似比。对于3个代理的情况，分别针对标准化和非标准化效用函数提出了更优的近似算法。

Result: 对于两种标准化效用函数的情况，获得了2-近似算法；对于3个代理，标准化效用函数获得了5/3-近似算法，非标准化效用函数获得了2-近似算法，这些结果都是紧的。

Conclusion: 该研究在特殊效用函数设置下显著改进了公平分配的近似比，证实了APX-完全性，并为该问题提供了最优的近似算法。

Abstract: We study the fair allocation of indivisible items to $n$ agents to maximize
the utilitarian social welfare, where the fairness criterion is envy-free up to
one item and there are only two different utility functions shared by the
agents. We present a $2$-approximation algorithm when the two utility functions
are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for
general normalized utility functions; thus this constant ratio approximation
algorithm confirms the APX-completeness in this special case previously shown
APX-hard. When there are only three agents, i.e., $n = 3$, the previous best
ratio is $3$ shown for general utility functions, and we present an improved
and tight $\frac 53$-approximation algorithm when the two utility functions are
normalized, and a best possible and tight $2$-approximation algorithm when the
two utility functions are unnormalized.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [96] [Improved Receiver Chain Performance via Error Location Inference](https://arxiv.org/abs/2509.08869)
*Michael Greenwood,Robert Hunter*

Main category: cs.IT

TL;DR: 使用机器学习模型估计字节级坏死概率，通过标记删除来提升Reed-Solomon解码能力，在不改变太空舱硬件的情况下获得0.3dB收益


<details>
  <summary>Details</summary>
Motivation: 现代太空舱通信系统依赖于卷积码和Reed-Solomon码的约束编码方案，需要在不改变硬件和编码标准的前提下提升数据恢复能力

Method: 在解码端使用机器学习模型估计接收数据帧中字节级坏死的概率，并将这些估计用于在RS解码前标记删除

Result: 方法能够在信号条件恶劣时提高数据恢复能力，获得0.3分贝的收益

Conclusion: 该机器学习方法可以有效提升太空舱通信系统的错误纠正能力，而无需改变现有硬件或编码标准

Abstract: Modern spacecraft communication systems rely on concatenated error correction
schemes, typically combining convolutional and Reed-Solomon (RS) codes. This
paper presents a decoder-side method that uses a machine learning model to
estimate the likelihood of byte-level corruption in received data frames. These
estimates are used to mark erasures prior to RS decoding, enhancing its
correction capacity without requiring changes to spacecraft hardware or
encoding standards. The approach enables improved data recovery under degraded
signal conditions at a gain of 0.3 decibels.

</details>


### [97] [Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?](https://arxiv.org/abs/2509.09411)
*Rui Xu,Yinghui Ye,Xiaoli Chu,Guangyue Lu,Farshad Rostami Ghadi,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本文探讨在流体天线系统(FAS)中使用频布相关矩阵替代以往的系数相关矩阵来计算高斯套利的优势，并在Nakagami-m衰落下验证了其更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往研究使用Jake模型的通道系数相关矩阵来近似高斯套利中的协方差矩阵，但多元正态随机变量实际上是通过变换相关频布生成的，因此需要考虑使用频布相关矩阵的准确性问题。

Method: 在完全相关的Nakagami-m衰落环境下，开发了一种生成这种衰落通道的方法用于Monte Carlo模拟，作为验证理论结果的基准。

Result: 模拟结果证实了所提出的通道建模方法的有效性，并证明使用频布相关矩阵具有更高的准确性，特别是在稀疏端口部署和低中断时。

Conclusion: 使用频布相关矩阵在评估FAS性能时比使用系数相关矩阵更准确，特别在关键性能情况下显示出显著优势。

Abstract: Gaussian copula has been employed to evaluate the outage performance of Fluid
Antenna Systems (FAS), with the covariance matrix reflecting the dependence
among multivariate normal random variables (RVs). While prior studies
approximate this matrix using the channel coefficient correlation matrix from
Jake's model, this work instead employs the channel envelope correlation
matrix, motivated by the fact that the multivariate normal RVs are generated by
transforming correlated channel envelopes. This raises an open question of
whether using the coefficient- or envelope-level correlation matrix yields
better accuracy in accessing FAS performance. Toward this end, this paper
explores the benefits of using the envelope-level correlation matrix under
fully correlated Nakagami-m fading, and develops a method for generating such
fading channels for Monte Carlo simulations, which serve as a benchmark for
validating the theoretical results. Simulation results confirm the
effectiveness of the proposed channel modeling approach and demonstrate the
superior accuracy of using the envelope-level correlation matrix, particularly
in sparse port deployment and low-outage regime.

</details>


### [98] [Mixture of Semantics Transmission for Generative AI-Enabled Semantic Communication Systems](https://arxiv.org/abs/2509.09499)
*Junjie Ni,Tong Wu,Zhiyong Chen,Yin Xu,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出基于生成式AI的语义混合传输策略(MoS)，通过ROI/RONI分区处理实现无线语义通信中带宽资源的高效分配和图像重建


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的语义通信方法在信道资源利用效率方面存在不足，需要在视觉保真度和语义相关性之间取得更好平衡

Method: 在发送端将图像分为感兴趣区域(ROI)和非感兴趣区域(RONI)，分别提取语义信息并分配不同带宽；在接收端使用扩散模型基于接收到的语义信息重建完整图像

Result: 实验结果表明适当的ROI-RONI分配至关重要，MoS在ROI的峰值信噪比(PSNR)和RONI的CLIP分数方面取得了显著性能提升

Conclusion: MoS策略通过语义混合传输实现了更高效的信道资源利用，在保持语义相关性的同时提升了视觉质量

Abstract: In this paper, we propose a mixture of semantics (MoS) transmission strategy
for wireless semantic communication systems based on generative artificial
intelligence (AI). At the transmitter, we divide an image into regions of
interest (ROI) and reigons of non-interest (RONI) to extract their semantic
information respectively. Semantic information of ROI can be allocated more
bandwidth, while RONI can be represented in a compact form for transmission. At
the receiver, a diffusion model reconstructs the full image using the received
semantic information of ROI and RONI. Compared to existing generative AI-based
methods, MoS enables more efficient use of channel resources by balancing
visual fidelity and semantic relevance. Experimental results demonstrate that
appropriate ROI-RONI allocation is critical. The MoS achieves notable
performance gains in peak signal-to-noise ratio (PSNR) of ROI and CLIP score of
RONI.

</details>


### [99] [Fast Polarisation-Aware Decoder for Non-Binary Polar Codes](https://arxiv.org/abs/2509.09554)
*Joseph Jabbour,Ali Chamas Al-Ghouwayel,Emmanuel Boutillon*

Main category: cs.IT

TL;DR: 非二进制极化码低复杂度解码器研究，通过核心自定义和极化感知优化，实现了计算复杂度显著降低


<details>
  <summary>Details</summary>
Motivation: 降低非二进制极化码解码器的计算复杂度，提高解码效率

Method: 提出FSC-PA算法，通过进行离线分析来自定义每个核心，最小化具有相同输入极化水平的奇偶检查节点的计算负荷

Result: 与现有扩展最小和算法相比，FSC-PA算法实现了域加法减少60%和实数加法减少30%，性能损失仅为0.2dB以下

Conclusion: 该方法能够在几乎不损失性能的前提下，显著降低非二进制极化码解码器的计算复杂度

Abstract: The paper investigates the emerging field of low-complexity non-binary polar
code (NB-PC) decoders. It shows that customizing each kernel of an NB-PC
decoder through offline analysis can significantly reduce the overall decoding
complexity. The proposed decoder, referred to as the Fast Successive
Cancellation-Polarization Aware (FSC-PA) scheme, achieves this by minimizing
the computational load of parity-check nodes that share the same level of input
polarization. The NB polar decoder is developed for both BPSK and CCSK
modulations. Compared to the state-of-the-art extended min-sum algorithm, the
FSC-PA algorithm achieves an overall reduction of 60 percents in field
additions and 30 percents in real additions, while incurring only a negligible
performance loss (less than 0.2 dB degradation).

</details>


### [100] [RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems](https://arxiv.org/abs/2509.09644)
*Chunjie Wang,Xuhui Zhang,Wenchao Liu,Jinke Ren,Shuqiang Wang,Yanyan Shen,Kejiang Ye,Kim Fung Tsang*

Main category: cs.IT

TL;DR: 提出了一种RIS赋能的智能交通系统数据收集增强框架，通过联合优化RIS相移、功率分配、计算资源和时隙分配，最大化RSU对的最小处理数据量


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中数据收集和处理效率问题，利用RIS技术增强通信性能，提高数据处理能力

Method: 采用混合RSMA和TDMA协议，提出基于交替优化和顺序秩一约束松弛的高效迭代算法

Result: 大量仿真表明，所提算法在不同场景下显著优于基线方案，有效提升了智能交通应用的数据处理性能

Conclusion: 该框架为RIS赋能的智能交通系统提供了有效的数据收集和处理解决方案，具有实际应用价值

Abstract: This paper investigates the data collection enhancement problem in a
reconfigurable intelligent surface (RIS)-empowered intelligent consumer
transportation system (ICTS). We propose a novel framework where a data center
(DC) provides energy to pre-configured roadside unit (RSU) pairs during the
downlink stage. While in the uplink stage, these RSU pairs utilize a hybrid
rate-splitting multiple access (RSMA) and time-division multiple access (TDMA)
protocol to transmit the processed data to the DC, while simultaneously
performing local data processing using the harvested energy. Our objective is
to maximize the minimal processed data volume of the RSU pairs by jointly
optimizing the RIS downlink and uplink phase shifts, the transmit power of the
DC and RSUs, the RSU computation resource allocation, and the time slot
allocation. To address the formulated non-convex problem, we develop an
efficient iterative algorithm integrating alternating optimization and
sequential rank-one constraint relaxation methods. Extensive simulations
demonstrate that the proposed algorithm significantly outperforms baseline
schemes under diverse scenarios, validating its effectiveness in enhancing the
data processing performance for intelligent transportation applications.

</details>
